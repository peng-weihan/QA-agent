{"question": "What is SQLFluff's approach to handling complex SQL constructs?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff handles complex SQL constructs through a multi-stage approach: 1) Templating preprocessing - for templated SQL (Jinja, Python format strings, placeholders), SQLFluff first converts raw/pre-templated code into valid SQL using dummy values, then backports rule violations to the original templated sections; 2) Grammar-based parsing - uses dialect-specific grammars to parse complex constructs by breaking them down into simpler components using grammar classes like Sequence, OneOf, Delimited, Bracketed, AnyNumberOf, AnySetOf, Ref, and Conditional; 3) Tree-like structure - creates a hierarchical parse tree where complex statements are decomposed into nested segments, with FileSegment as root containing StatementSegments and their sub-components; 4) Recursive matching - segments recursively match based on their respective grammars until reaching raw segments with no children; 5) Template-aware handling - for complex templated cases where template tags cut across the parse tree, SQLFluff treats template tags as being outside the SQL structure (similar to C preprocessor directives) and pulls all tags to the least indented level; 6) Implicit indentation support - handles complex WHERE clauses and CASE expressions that can be left un-closed before line end; 7) Error recovery - if no match is found for a segment, contents are wrapped in an UnparsableSegment for graceful handling.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "sequence.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/grammar", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Sequence and Bracketed Grammars.\"\"\"\n\n# NOTE: We rename the typing.Sequence here so it doesn't collide\n# with the grammar class that we're defining.\nfrom collections.abc import Sequence as SequenceType\nfrom os import getenv\nfrom typing import Optional, Union, cast\n\nfrom sqlfluff.core.errors import SQLParseError\nfrom sqlfluff.core.helpers.slice import is_zero_slice\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.grammar.base import (\n    BaseGrammar,\n    cached_method_for_parse_context,\n)\nfrom sqlfluff.core.parser.grammar.conditional import Conditional\nfrom sqlfluff.core.parser.match_algorithms import (\n    skip_start_index_forward_to_code,\n    skip_stop_index_backward_to_code,\n    trim_to_terminator,\n)\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.segments import (\n    BaseSegment,\n    BracketedSegment,\n    Dedent,\n    Indent,\n    MetaSegment,\n    TemplateSegment,\n    UnparsableSegment,\n)\nfrom sqlfluff.core.parser.types import ParseMode, SimpleHintType\n\n\ndef _flush_metas(\n    pre_nc_idx: int,\n    post_nc_idx: int,\n    meta_buffer: SequenceType[type[\"MetaSegment\"]],\n    segments: SequenceType[BaseSegment],\n) -> tuple[tuple[int, type[MetaSegment]], ...]:\n    \"\"\"Position any new meta segments relative to the non code section.\n\n    It's important that we position the new meta segments appropriately\n    around any templated sections and any whitespace so that indentation\n    behaviour works as expected.\n\n    There are four valid locations (which may overlap).\n    1. Before any non-code\n    2. Before the first block templated section (if it's a block opener).\n    3. After the last block templated section (if it's a block closer).\n    4. After any non code.\n\n    If all the metas have a positive indent value then they should go in\n    position 1 or 3, otherwise we're in position 2 or 4. Within each of\n    those scenarios it depends on whether an appro"}, {"start_line": 70000, "end_line": 72000, "belongs_to": {"file_name": "dialect_ansi.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "use in join_clauses:\n            aliases: list[tuple[BaseSegment, AliasInfo]] = cast(\n                JoinClauseSegment, clause\n            ).get_eventual_aliases()\n            # Only append if non-null. A None reference, may\n            # indicate a generator expression or similar.\n            if aliases:\n                buff = buff + aliases\n        return buff\n\n\nclass WhenClauseSegment(BaseSegment):\n    \"\"\"A 'WHEN' clause for a 'CASE' statement.\"\"\"\n\n    type = \"when_clause\"\n    match_grammar: Matchable = Sequence(\n        \"WHEN\",\n        # NOTE: The nested sequence here is to ensure the correct\n        # placement of the meta segments when templated elements\n        # are present.\n        # https://github.com/sqlfluff/sqlfluff/issues/3988\n        Sequence(\n            ImplicitIndent,\n            Ref(\"ExpressionSegment\"),\n            Dedent,\n        ),\n        Conditional(Indent, indented_then=True),\n        \"THEN\",\n        Conditional(ImplicitIndent, indented_then_contents=True),\n        Ref(\"ExpressionSegment\"),\n        Conditional(Dedent, indented_then_contents=True),\n        Conditional(Dedent, indented_then=True),\n    )\n\n\nclass ElseClauseSegment(BaseSegment):\n    \"\"\"An 'ELSE' clause for a 'CASE' statement.\"\"\"\n\n    type = \"else_clause\"\n    match_grammar: Matchable = Sequence(\n        \"ELSE\", ImplicitIndent, Ref(\"ExpressionSegment\"), Dedent\n    )\n\n\nclass CaseExpressionSegment(BaseSegment):\n    \"\"\"A `CASE WHEN` clause.\"\"\"\n\n    type = \"case_expression\"\n    match_grammar: Matchable = OneOf(\n        Sequence(\n            \"CASE\",\n            ImplicitIndent,\n            AnyNumberOf(\n                Ref(\"WhenClauseSegment\"),\n                reset_terminators=True,\n                terminators=[Ref.keyword(\"ELSE\"), Ref.keyword(\"END\")],\n            ),\n            Ref(\n                \"ElseClauseSegment\",\n                optional=True,\n                reset_terminators=True,\n                terminators=[Ref.keyword(\"END\")],\n            ),\n            Dedent,\n            \""}, {"start_line": 0, "end_line": 38, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.core.parser.\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "lexer.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The code for the Lexer.\"\"\"\n\nimport logging\nfrom collections.abc import Iterator\nfrom typing import Any, NamedTuple, Optional, Union\nfrom uuid import UUID, uuid4\n\nimport regex\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLLexError\nfrom sqlfluff.core.helpers.slice import is_zero_slice, offset_slice, to_tuple\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments import (\n    BaseSegment,\n    Dedent,\n    EndOfFile,\n    Indent,\n    MetaSegment,\n    RawSegment,\n    TemplateLoop,\n    TemplateSegment,\n    UnlexableSegment,\n)\nfrom sqlfluff.core.templaters import TemplatedFile\nfrom sqlfluff.core.templaters.base import TemplatedFileSlice\n\n# Instantiate the lexer logger\nlexer_logger = logging.getLogger(\"sqlfluff.lexer\")\n\n\nclass BlockTracker:\n    \"\"\"This is an object for keeping track of templating blocks.\n\n    Using the .enter() and .exit() methods on opening and closing\n    blocks, we can match up tags of the same level so that later\n    it's easier to treat them the same way in the linting engine.\n\n    In case looping means that we encounter the same block more\n    than once, we use cache uuids against their source location\n    so that if we try to re-enter the block again, it will get\n    the same uuid on the second pass.\n    \"\"\"\n\n    _stack: list[UUID] = []\n    _map: dict[tuple[int, int], UUID] = {}\n\n    def enter(self, src_slice: slice) -> None:\n        \"\"\"Add a block to the stack.\"\"\"\n        key = to_tuple(src_slice)\n        uuid = self._map.get(key, None)\n\n        if not uuid:\n            uuid = uuid4()\n            self._map[key] = uuid\n            lexer_logger.debug(\n                \"        Entering block stack @ %s: %s (fresh)\",\n                src_slice,\n                uuid,\n            )\n        else:\n            lexer_logger.debug(\n                \"        Entering block stack @ %s: %s (cached)\",\n                src_slice,\n                uuid,\n            )\n\n        self._stack.append(uui"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "ST04.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Implementation of Rule ST04.\"\"\"\n\nfrom sqlfluff.core.parser import BaseSegment, Indent, NewlineSegment, WhitespaceSegment\nfrom sqlfluff.core.rules import BaseRule, LintFix, LintResult, RuleContext\nfrom sqlfluff.core.rules.crawlers import SegmentSeekerCrawler\nfrom sqlfluff.utils.functional import FunctionalContext, Segments, sp\nfrom sqlfluff.utils.reflow.reindent import construct_single_indent\n\n\nclass Rule_ST04(BaseRule):\n    \"\"\"Nested ``CASE`` statement in ``ELSE`` clause could be flattened.\n\n    **Anti-pattern**\n\n    In this example, the outer ``CASE``'s ``ELSE`` is an unnecessary, nested ``CASE``.\n\n    .. code-block:: sql\n\n        SELECT\n          CASE\n            WHEN species = 'Cat' THEN 'Meow'\n            ELSE\n            CASE\n               WHEN species = 'Dog' THEN 'Woof'\n            END\n          END as sound\n        FROM mytable\n\n    **Best practice**\n\n    Move the body of the inner ``CASE`` to the end of the outer one.\n\n    .. code-block:: sql\n\n        SELECT\n          CASE\n            WHEN species = 'Cat' THEN 'Meow'\n            WHEN species = 'Dog' THEN 'Woof'\n          END AS sound\n        FROM mytable\n\n    \"\"\"\n\n    name = \"structure.nested_case\"\n    aliases = (\"L058\",)\n    groups = (\"all\", \"structure\")\n    crawl_behaviour = SegmentSeekerCrawler({\"case_expression\"})\n    is_fix_compatible = True\n\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Nested CASE statement in ELSE clause could be flattened.\"\"\"\n        segment = FunctionalContext(context).segment\n        assert segment.select(sp.is_type(\"case_expression\"))\n        case1_children = segment.children()\n        case1_first_case = case1_children.first(sp.is_keyword(\"CASE\")).get()\n        assert case1_first_case\n        case1_first_when = case1_children.first(\n            sp.is_type(\"when_clause\", \"else_clause\")\n        ).get()\n        case1_last_when = case1_children.last(sp.is_type(\"when_clause\")).get()\n        case1_else_clause = case1_children.select(sp.is_type(\"else_clause\"))"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "context.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The parser context.\n\nThis mirrors some of the same design of the flask\ncontext manager. https://flask.palletsprojects.com/en/1.1.x/\n\nThe context acts as a way of keeping track of state, references\nto common configuration and dialects, logging and also the parse\nand match depth of the current operation.\n\"\"\"\n\nimport logging\nimport uuid\nfrom collections import defaultdict\nfrom collections.abc import Iterator, Sequence\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, NoReturn, Optional\n\nfrom tqdm import tqdm\n\nfrom sqlfluff.core.config import progress_bar_configuration\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects.base import Dialect\n    from sqlfluff.core.parser.match_result import MatchResult\n    from sqlfluff.core.parser.matchable import Matchable\n\n# Get the parser logger\nparser_logger = logging.getLogger(\"sqlfluff.parser\")\n\n\nclass ParseContext:\n    \"\"\"Object to handle the context at hand during parsing.\n\n    Holds two tiers of references.\n    1. Persistent config, like references to the dialect or\n       the current verbosity and logger.\n    2. Stack config, like the parse and match depth.\n\n    The manipulation of the stack config is done using a context\n    manager and layered config objects inside the context.\n\n    NOTE: We use context managers here to avoid _copying_\n    the context, just to mutate it safely. This is significantly\n    more performant than the copy operation, but does require some\n    care to use properly.\n\n    When fetching elements from the context, we first look\n    at the top level stack config object and the persistent\n    config values (stored as attributes of the ParseContext\n    itself).\n    \"\"\"\n\n    def __init__(\n        self,\n        dialect: \"Dialect\",\n        indentation_config: Optional[dict[str, Any]] = None,\n    ) -> None:\n        \"\"\"Initialize a new instance of the class.\n\n        Args:\n            dialect (Dialect): The dialect "}, {"start_line": 72000, "end_line": 74000, "belongs_to": {"file_name": "dialect_ansi.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "END\",\n        ),\n        Sequence(\n            \"CASE\",\n            Ref(\"ExpressionSegment\"),\n            ImplicitIndent,\n            AnyNumberOf(\n                Ref(\"WhenClauseSegment\"),\n                reset_terminators=True,\n                terminators=[Ref.keyword(\"ELSE\"), Ref.keyword(\"END\")],\n            ),\n            Ref(\n                \"ElseClauseSegment\",\n                optional=True,\n                reset_terminators=True,\n                terminators=[Ref.keyword(\"END\")],\n            ),\n            Dedent,\n            \"END\",\n        ),\n        terminators=[\n            Ref(\"ComparisonOperatorGrammar\"),\n            Ref(\"CommaSegment\"),\n            Ref(\"BinaryOperatorGrammar\"),\n        ],\n    )\n\n\nansi_dialect.add(\n    # Expression_A_Grammar\n    # https://www.cockroachlabs.com/docs/v20.2/sql-grammar.html#a_expr\n    # The upstream grammar is defined recursively, which if implemented naively\n    # will cause SQLFluff to overflow the stack from recursive function calls.\n    # To work around this, the a_expr grammar is reworked a bit into sub-grammars\n    # that effectively provide tail recursion.\n    Expression_A_Unary_Operator_Grammar=OneOf(\n        # This grammar corresponds to the unary operator portion of the initial\n        # recursive block on the Cockroach Labs a_expr grammar.  It includes the\n        # unary operator matching sub-block, but not the recursive call to a_expr.\n        Ref(\n            \"SignedSegmentGrammar\",\n            exclude=Sequence(Ref(\"QualifiedNumericLiteralSegment\")),\n        ),\n        Ref(\"TildeSegment\"),\n        Ref(\"NotOperatorGrammar\"),\n        # used in CONNECT BY clauses (EXASOL, Snowflake, Postgres...)\n        \"PRIOR\",\n    ),\n    Tail_Recurse_Expression_A_Grammar=Sequence(\n        # This should be used instead of a recursive call to Expression_A_Grammar\n        # whenever the repeating element in Expression_A_Grammar makes a recursive\n        # call to itself at the _end_.  If it's in the middle then you still need\n       "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "python.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the templaters.\"\"\"\n\nimport ast\nimport re\nfrom collections.abc import Iterable, Iterator\nfrom string import Formatter\nfrom typing import Any, Callable, NamedTuple, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLTemplaterError\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.helpers.slice import offset_slice, zero_slice\nfrom sqlfluff.core.helpers.string import findall\nfrom sqlfluff.core.templaters.base import (\n    RawFileSlice,\n    RawTemplater,\n    TemplatedFile,\n    TemplatedFileSlice,\n    large_file_check,\n    templater_logger,\n)\n\n\nclass IntermediateFileSlice(NamedTuple):\n    \"\"\"An intermediate representation of a partially sliced File.\"\"\"\n\n    intermediate_type: str\n    source_slice: slice\n    templated_slice: slice\n    slice_buffer: list[RawFileSlice]\n\n    def _trim_end(\n        self, templated_str: str, target_end: str = \"head\"\n    ) -> tuple[\"IntermediateFileSlice\", list[TemplatedFileSlice]]:\n        \"\"\"Trim the ends of a intermediate segment.\"\"\"\n        target_idx = 0 if target_end == \"head\" else -1\n        terminator_types = (\"block_start\") if target_end == \"head\" else (\"block_end\")\n        main_source_slice = self.source_slice\n        main_templated_slice = self.templated_slice\n        slice_buffer = self.slice_buffer\n\n        end_buffer = []\n\n        # Yield any leading literals, comments or blocks.\n        while len(slice_buffer) > 0 and slice_buffer[target_idx].slice_type in (\n            \"literal\",\n            \"block_start\",\n            \"block_end\",\n            \"comment\",\n        ):\n            focus = slice_buffer[target_idx]\n            templater_logger.debug(\"            %s Focus: %s\", target_end, focus)\n            # Is it a zero length item?\n            if focus.slice_type in (\"block_start\", \"block_end\", \"comment\"):\n                # Only add the length in the source space.\n                templated_len = 0\n            else:\n                # Assume it's a literal, chec"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "match_algorithms.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Matching algorithms.\n\nThese are mostly extracted from the body of either BaseSegment\nor BaseGrammar to un-bloat those classes.\n\"\"\"\n\nfrom collections import defaultdict\nfrom collections.abc import Sequence\nfrom typing import DefaultDict, Optional, cast\n\nfrom sqlfluff.core.errors import SQLParseError\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.segments import BaseSegment, BracketedSegment, Dedent, Indent\n\n\ndef skip_start_index_forward_to_code(\n    segments: Sequence[BaseSegment], start_idx: int, max_idx: Optional[int] = None\n) -> int:\n    \"\"\"Move an index forward through segments until segments[index] is code.\"\"\"\n    if max_idx is None:\n        max_idx = len(segments)\n    for _idx in range(start_idx, max_idx):\n        if segments[_idx].is_code:\n            break\n    else:\n        _idx = max_idx\n    return _idx\n\n\ndef skip_stop_index_backward_to_code(\n    segments: Sequence[BaseSegment], stop_idx: int, min_idx: int = 0\n) -> int:\n    \"\"\"Move an index backward through segments until segments[index - 1] is code.\"\"\"\n    for _idx in range(stop_idx, min_idx, -1):\n        if segments[_idx - 1].is_code:\n            break\n    else:\n        _idx = min_idx\n    return _idx\n\n\ndef first_trimmed_raw(seg: BaseSegment) -> str:\n    \"\"\"Trim whitespace off a whole element raw.\n\n    Used as a helper function in BaseGrammar._look_ahead_match.\n\n    For existing compound segments, we should assume that within\n    that segment, things are internally consistent, that means\n    rather than enumerating all the individual segments of a longer\n    one we just dump out the whole segment, but splitting off the\n    first element separated by whitespace. This is a) faster and\n    also b) prevents some really horrible bugs with bracket matching.\n    See https://github.com/sqlfluff/sqlfluff/issues/433\n\n    This fetches the _whole_ raw of a potentially compou"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Implements the base rule class.\n\nRules crawl through the trees returned by the parser and evaluate particular\nrules.\n\nThe intent is that it should be possible for the rules to be expressed\nas simply as possible, with as much of the complexity abstracted away.\n\nThe evaluation function should take enough arguments that it can evaluate\nthe position of the given segment in relation to its neighbors, and that\nthe segment which finally \"triggers\" the error, should be the one that would\nbe corrected OR if the rule relates to something that is missing, then it\nshould flag on the segment FOLLOWING, the place that the desired element is\nmissing.\n\"\"\"\n\nimport bdb\nimport copy\nimport fnmatch\nimport logging\nimport pathlib\nimport re\nfrom collections import defaultdict, namedtuple\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    ClassVar,\n    DefaultDict,\n    Optional,\n    Union,\n)\n\nimport regex\n\nfrom sqlfluff.core.errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a "}], "retrieved_count": 10, "cost_time": 1.012056589126587}
{"question": "What is the relationship between BaseSegment and RawSegment in SQLFluff's parser?", "answer": null, "relative_code_list": null, "ground_truth": "BaseSegment and RawSegment have a hierarchical inheritance relationship in SQLFluff's parser architecture. BaseSegment is the abstract base class that serves as the foundation for all segments, while RawSegment is a concrete implementation that represents the leaf nodes of the parse tree. Key aspects of their relationship include: 1) Inheritance hierarchy - RawSegment inherits from BaseSegment (class RawSegment(BaseSegment)), making it a specialized type of segment; 2) Structural differences - BaseSegment is designed to contain other subsegments and represents composite elements in the parse tree, while RawSegment has no children (self.segments = ()) and represents atomic tokens; 3) Lexer output - RawSegment is the primary output of the lexer, representing individual tokens like keywords, identifiers, literals, and whitespace; 4) Type system - BaseSegment has a class-level 'type' attribute and _class_types, while RawSegment has instance-level instance_types that can be set during initialization; 5) Content representation - BaseSegment's raw content is computed by concatenating its child segments' raw content, while RawSegment stores its raw content directly in the _raw attribute; 6) Position tracking - both use PositionMarker for location information, but RawSegment requires it during initialization; 7) Specialization - RawSegment includes additional properties like is_code, is_comment, is_whitespace, and methods like normalize() for token-specific behavior; 8) Tree structure - BaseSegments form the internal nodes of the parse tree, while RawSegments form the leaves, creating a complete hierarchical representation of SQL code.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 966, "belongs_to": {"file_name": "segments_raw_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test the RawSegment class.\"\"\"\n\nfrom sqlfluff.core.parser.segments.base import PathStep\n\n\ndef test__parser__raw_get_raw_segments(raw_segments):\n    \"\"\"Test niche case of calling get_raw_segments on a raw segment.\"\"\"\n    for s in raw_segments:\n        assert s.get_raw_segments() == [s]\n\n\ndef test__parser__raw_segments_with_ancestors(\n    raw_segments, DummySegment, DummyAuxSegment\n):\n    \"\"\"Test raw_segments_with_ancestors.\n\n    This is used in the reflow module to assess parse depth.\n    \"\"\"\n    test_seg = DummySegment([DummyAuxSegment(raw_segments[:1]), raw_segments[1]])\n    # Result should be the same raw segment, but with appropriate parents\n    assert test_seg.raw_segments_with_ancestors == [\n        (\n            raw_segments[0],\n            [\n                PathStep(test_seg, 0, 2, (0, 1)),\n                PathStep(test_seg.segments[0], 0, 1, (0,)),\n            ],\n        ),\n        (raw_segments[1], [PathStep(test_seg, 1, 2, (0, 1))]),\n    ]\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "raw.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Raw segment definitions.\n\nThis is designed to be the root segment, without\nany children, and the output of the lexer.\n\"\"\"\n\nfrom typing import Any, Callable, Optional, Union, cast\nfrom uuid import uuid4\n\nimport regex as re\n\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\n\n\nclass RawSegment(BaseSegment):\n    \"\"\"This is a segment without any subsegments.\"\"\"\n\n    type = \"raw\"\n    _is_code = True\n    _is_comment = False\n    _is_whitespace = False\n    # Classes inheriting from RawSegment may provide a _default_raw\n    # to enable simple initialisation.\n    _default_raw = \"\"\n\n    def __init__(\n        self,\n        raw: Optional[str] = None,\n        pos_marker: Optional[PositionMarker] = None,\n        # For legacy and syntactic sugar we allow the simple\n        # `type` argument here, but for more precise inheritance\n        # we suggest using the `instance_types` option.\n        type: Optional[str] = None,\n        instance_types: tuple[str, ...] = (),\n        trim_start: Optional[tuple[str, ...]] = None,\n        trim_chars: Optional[tuple[str, ...]] = None,\n        source_fixes: Optional[list[SourceFix]] = None,\n        uuid: Optional[int] = None,\n        quoted_value: Optional[tuple[str, Union[int, str]]] = None,\n        escape_replacements: Optional[list[tuple[str, str]]] = None,\n        casefold: Optional[Callable[[str], str]] = None,\n    ):\n        \"\"\"Initialise raw segment.\n\n        If raw is not provided, we default to _default_raw if present.\n        If pos_marker is not provided, it is assume that this will be\n        inserted later as part of a reposition phase.\n        \"\"\"\n        if raw is not None:  # NB, raw *can* be an empty string and be valid\n            self._raw = raw\n        else:\n            self._raw = self._default_raw\n        self._raw_upper = self._raw.upper()\n        # pos marker is required here. We ignore the typing initially\n        # because it might *initially* b"}, {"start_line": 0, "end_line": 433, "belongs_to": {"file_name": "segments_file_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test the BaseFileSegment class.\"\"\"\n\nfrom sqlfluff.core.parser import BaseFileSegment\n\n\ndef test__parser__base_segments_file(raw_segments):\n    \"\"\"Test BaseFileSegment to behave as expected.\"\"\"\n    base_seg = BaseFileSegment(raw_segments, fname=\"/some/dir/file.sql\")\n    assert base_seg.type == \"file\"\n    assert base_seg.file_path == \"/some/dir/file.sql\"\n    assert base_seg.can_start_end_non_code\n    assert base_seg.allow_empty\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "segments_base_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test the BaseSegment class.\"\"\"\n\nimport pickle\n\nimport pytest\n\nfrom sqlfluff.core.parser import BaseSegment, PositionMarker, RawSegment\nfrom sqlfluff.core.parser.segments.base import PathStep\nfrom sqlfluff.core.rules.base import LintFix\nfrom sqlfluff.core.templaters import TemplatedFile\n\n\ndef test__parser__base_segments_type(DummySegment):\n    \"\"\"Test the .is_type() method.\"\"\"\n    assert BaseSegment.class_is_type(\"base\")\n    assert not BaseSegment.class_is_type(\"foo\")\n    assert not BaseSegment.class_is_type(\"foo\", \"bar\")\n    assert DummySegment.class_is_type(\"dummy\")\n    assert DummySegment.class_is_type(\"base\")\n    assert DummySegment.class_is_type(\"base\", \"foo\", \"bar\")\n\n\ndef test__parser__base_segments_class_types(DummySegment):\n    \"\"\"Test the metaclass ._class_types attribute.\"\"\"\n    assert DummySegment._class_types == {\"dummy\", \"base\"}\n\n\ndef test__parser__base_segments_descendant_type_set(\n    raw_segments, DummySegment, DummyAuxSegment\n):\n    \"\"\"Test the .descendant_type_set() method.\"\"\"\n    test_seg = DummySegment([DummyAuxSegment(raw_segments)])\n    assert test_seg.descendant_type_set == {\"raw\", \"base\", \"dummy_aux\"}\n\n\ndef test__parser__base_segments_direct_descendant_type_set(\n    raw_segments, DummySegment, DummyAuxSegment\n):\n    \"\"\"Test the .direct_descendant_type_set() method.\"\"\"\n    test_seg = DummySegment([DummyAuxSegment(raw_segments)])\n    assert test_seg.direct_descendant_type_set == {\"base\", \"dummy_aux\"}\n\n\ndef test__parser__base_segments_to_tuple_a(raw_segments, DummySegment, DummyAuxSegment):\n    \"\"\"Test the .to_tuple() method.\"\"\"\n    test_seg = DummySegment([DummyAuxSegment(raw_segments)])\n    assert test_seg.to_tuple() == (\n        \"dummy\",\n        ((\"dummy_aux\", ((\"raw\", ()), (\"raw\", ()))),),\n    )\n\n\ndef test__parser__base_segments_to_tuple_b(raw_segments, DummySegment, DummyAuxSegment):\n    \"\"\"Test the .to_tuple() method.\"\"\"\n    test_seg = DummySegment(\n        [DummyAuxSegment(raw_segments + (DummyAuxSegment(raw_segments[:1]),))]\n    )\n    "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "common.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Common segment types used as building blocks of dialects.\n\nThe expectation for these segments is that they have no additional\nlogic (or very minimal logic).\n\"\"\"\n\nfrom sqlfluff.core.parser.segments.base import BaseSegment\nfrom sqlfluff.core.parser.segments.raw import RawSegment\n\n\nclass CodeSegment(RawSegment):\n    \"\"\"An alias for RawSegment.\n\n    This has a more explicit name for segment creation.\n    \"\"\"\n\n    pass\n\n\nclass UnlexableSegment(CodeSegment):\n    \"\"\"A placeholder to unlexable sections.\n\n    This otherwise behaves exactly like a code section.\n    \"\"\"\n\n    type = \"unlexable\"\n\n\nclass CommentSegment(RawSegment):\n    \"\"\"Segment containing a comment.\"\"\"\n\n    type = \"comment\"\n    _is_code = False\n    _is_comment = True\n\n\nclass WhitespaceSegment(RawSegment):\n    \"\"\"Segment containing whitespace.\"\"\"\n\n    type = \"whitespace\"\n    _is_whitespace = True\n    _is_code = False\n    _is_comment = False\n    _default_raw = \" \"\n\n\nclass NewlineSegment(RawSegment):\n    \"\"\"Segment containing a newline.\n\n    NOTE: NewlineSegment does not inherit from WhitespaceSegment.\n    Therefore NewlineSegment.is_type('whitespace') returns False.\n\n    This is intentional and convenient for rules. If users want\n    to match on both, call .is_type('whitespace', 'newline')\n    \"\"\"\n\n    type = \"newline\"\n    _is_whitespace = True\n    _is_code = False\n    _is_comment = False\n    _default_raw = \"\\n\"\n\n\nclass SymbolSegment(CodeSegment):\n    \"\"\"A segment used for matching single entities which aren't keywords.\n\n    We rename the segment class here so that descendants of\n    _ProtoKeywordSegment can use the same functionality\n    but don't end up being labelled as a `keyword` later.\n    \"\"\"\n\n    type = \"symbol\"\n\n\nclass IdentifierSegment(CodeSegment):\n    \"\"\"An identifier segment.\n\n    Defined here for type inheritance.\n    \"\"\"\n\n    type = \"identifier\"\n\n\nclass LiteralSegment(CodeSegment):\n    \"\"\"A literal segment.\n\n    Defined here for type inheritance.\n    \"\"\"\n\n    type = \"literal\"\n\n\nclass BinaryOpera"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "segments_base_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ize=2)\n        == \"[L:  1, P:  1]      |  raw:                                                \"\n        \"        'foobar'\\n\"\n    )\n    # Check tuple\n    assert raw_seg.to_tuple() == (\"raw\", ())\n    # Check tuple\n    assert raw_seg.to_tuple(show_raw=True) == (\"raw\", \"foobar\")\n\n\ndef test__parser__base_segments_base(raw_segments, fresh_ansi_dialect, DummySegment):\n    \"\"\"Test base segments behave as expected.\"\"\"\n    base_seg = DummySegment(raw_segments)\n    # Check we assume the position correctly\n    assert (\n        base_seg.pos_marker.start_point_marker()\n        == raw_segments[0].pos_marker.start_point_marker()\n    )\n    assert (\n        base_seg.pos_marker.end_point_marker()\n        == raw_segments[-1].pos_marker.end_point_marker()\n    )\n\n    # Check that we correctly reconstruct the raw\n    assert base_seg.raw == \"foobar.barfoo\"\n    # Check tuple\n    assert base_seg.to_tuple() == (\n        \"dummy\",\n        (raw_segments[0].to_tuple(), raw_segments[1].to_tuple()),\n    )\n    # Check Formatting and Stringification\n    assert str(base_seg) == repr(base_seg) == \"<DummySegment: ([L:  1, P:  1])>\"\n    assert base_seg.stringify(ident=1, tabsize=2) == (\n        \"[L:  1, P:  1]      |  dummy:\\n\"\n        \"[L:  1, P:  1]      |    raw:                                                 \"\n        \"     'foobar'\\n\"\n        \"[L:  1, P:  7]      |    raw:                                                 \"\n        \"     '.barfoo'\\n\"\n    )\n\n\ndef test__parser__base_segments_raw_compare():\n    \"\"\"Test comparison of raw segments.\"\"\"\n    template = TemplatedFile.from_string(\"foobar\")\n    rs1 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n    rs2 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n    assert rs1 == rs2\n\n\ndef test__parser__base_segments_base_compare(DummySegment, DummyAuxSegment):\n    \"\"\"Test comparison of base segments.\"\"\"\n    template = TemplatedFile.from_string(\"foobar\")\n    rs1 = RawSegment(\"foobar\", PositionMarker(sli"}, {"start_line": 0, "end_line": 1715, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Definitions of the segment classes.\"\"\"\n\nfrom sqlfluff.core.parser.segments.base import (\n    BaseSegment,\n    SourceFix,\n    UnparsableSegment,\n)\nfrom sqlfluff.core.parser.segments.bracketed import BracketedSegment\nfrom sqlfluff.core.parser.segments.common import (\n    BinaryOperatorSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    IdentifierSegment,\n    LiteralSegment,\n    NewlineSegment,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.segments.file import BaseFileSegment\nfrom sqlfluff.core.parser.segments.generator import SegmentGenerator\nfrom sqlfluff.core.parser.segments.keyword import KeywordSegment, LiteralKeywordSegment\nfrom sqlfluff.core.parser.segments.meta import (\n    Dedent,\n    EndOfFile,\n    ImplicitIndent,\n    Indent,\n    MetaSegment,\n    TemplateLoop,\n    TemplateSegment,\n)\nfrom sqlfluff.core.parser.segments.raw import RawSegment\n\n__all__ = (\n    \"BaseSegment\",\n    \"BaseFileSegment\",\n    \"UnparsableSegment\",\n    \"BracketedSegment\",\n    \"SegmentGenerator\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"LiteralKeywordSegment\",\n    \"SymbolSegment\",\n    \"MetaSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"TemplateSegment\",\n    \"EndOfFile\",\n    \"TemplateLoop\",\n    \"SourceFix\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n)\n"}, {"start_line": 1000, "end_line": 2225, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ",\n    LiteralSegment,\n    NewlineSegment,\n    RawSegment,\n    SegmentGenerator,\n    SourceFix,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.types import ParseMode\n\n__all__ = (\n    \"BaseSegment\",\n    \"SourceFix\",\n    \"BaseFileSegment\",\n    \"BracketedSegment\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"SymbolSegment\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"LiteralKeywordSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"SegmentGenerator\",\n    \"Sequence\",\n    \"OneOf\",\n    \"Delimited\",\n    \"Bracketed\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\",\n    \"Conditional\",\n    \"StringParser\",\n    \"MultiStringParser\",\n    \"TypedParser\",\n    \"RegexParser\",\n    \"PositionMarker\",\n    \"Lexer\",\n    \"StringLexer\",\n    \"RegexLexer\",\n    \"Parser\",\n    \"Matchable\",\n    \"ParseMode\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Base segment definitions.\n\nHere we define:\n- BaseSegment. This is the root class for all segments, and is\n  designed to hold other subsegments.\n- UnparsableSegment. A special wrapper to indicate that the parse\n  function failed on this block of segments and to prevent further\n  analysis.\n\"\"\"\n\n# Import annotations for py 3.7 to allow `weakref.Referencetype[\"BaseSegment\"]`\nfrom __future__ import annotations\n\nimport logging\nimport weakref\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom functools import cached_property\nfrom io import StringIO\nfrom itertools import chain\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    ClassVar,\n    Optional,\n    Union,\n    cast,\n)\nfrom uuid import uuid4\n\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import trim_non_code_segments\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.types import SimpleHintType\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.raw import RawSegment\n\n# Instantiate the linter logger (only for use in methods involved with fixing.)\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\nTupleSerialisedSegment = tuple[str, Union[str, tuple[\"TupleSerialisedSegment\", ...]]]\nRecordSerialisedSegment = dict[\n    str, Union[None, str, \"RecordSerialisedSegment\", list[\"RecordSerialisedSegment\"]]\n]\n\n\n@dataclass(frozen=True)\nclass SourceFix:\n    \"\"\"A stored reference to a fix in the non-templated file.\"\"\"\n\n    edit: str\n    source_slice: slice\n    # TODO: It might be possible to refactor this to not require\n    # a templated_slice (because in theory it's unnecessary).\n    # However much of the fix handling code assumes we need\n    # a position in the templated file to interpret it.\n    # More work required to ach"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "segments_base_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "gments[0]) == [\n        PathStep(test_seg_b, 0, 1, (0,)),\n        PathStep(test_seg_a, 0, 2, (0, 1)),\n    ]\n    assert test_seg_b.path_to(raw_segments[1]) == [\n        PathStep(test_seg_b, 0, 1, (0,)),\n        PathStep(test_seg_a, 1, 2, (0, 1)),\n    ]\n\n\ndef test__parser__base_segments_stubs():\n    \"\"\"Test stub methods that have no implementation in base class.\"\"\"\n    template = TemplatedFile.from_string(\"foobar\")\n    rs1 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n    base_segment = BaseSegment(segments=[rs1])\n\n    with pytest.raises(NotImplementedError):\n        base_segment.edit(\"foo\")\n\n\ndef test__parser__base_segments_raw(raw_seg):\n    \"\"\"Test raw segments behave as expected.\"\"\"\n    # Check Segment Return\n    assert raw_seg.segments == ()\n    assert raw_seg.raw == \"foobar\"\n    # Check Formatting and Stringification\n    assert str(raw_seg) == repr(raw_seg) == \"<CodeSegment: ([L:  1, P:  1]) 'foobar'>\"\n    assert (\n        raw_seg.stringify(ident=1, tabsize=2)\n        == \"[L:  1, P:  1]      |  raw:                                                \"\n        \"        'foobar'\\n\"\n    )\n    # Check tuple\n    assert raw_seg.to_tuple() == (\"raw\", ())\n    # Check tuple\n    assert raw_seg.to_tuple(show_raw=True) == (\"raw\", \"foobar\")\n\n\ndef test__parser__base_segments_base(raw_segments, fresh_ansi_dialect, DummySegment):\n    \"\"\"Test base segments behave as expected.\"\"\"\n    base_seg = DummySegment(raw_segments)\n    # Check we assume the position correctly\n    assert (\n        base_seg.pos_marker.start_point_marker()\n        == raw_segments[0].pos_marker.start_point_marker()\n    )\n    assert (\n        base_seg.pos_marker.end_point_marker()\n        == raw_segments[-1].pos_marker.end_point_marker()\n    )\n\n    # Check that we correctly reconstruct the raw\n    assert base_seg.raw == \"foobar.barfoo\"\n    # Check tuple\n    assert base_seg.to_tuple() == (\n        \"dummy\",\n        (raw_segments[0].to_tuple(), raw_segments[1].to_tuple()),\n    )\n    # Check "}], "retrieved_count": 10, "cost_time": 1.0178494453430176}
{"question": "What is the purpose of the RuleSet class in SQLFluff's rule management?", "answer": null, "relative_code_list": null, "ground_truth": "The RuleSet class serves as the central registry and management system for SQLFluff's linting rules. Its key purposes include: 1) Rule registration - provides a decorator-based registration system (@ruleset.register) that adds rule classes to the ruleset and performs validation checks; 2) Rule storage - maintains an internal _register dictionary that maps rule codes to RuleManifest objects containing rule metadata (code, name, description, groups, aliases, rule_class); 3) Configuration validation - validates rule configuration options against predefined validation rules through _validate_config_options() method; 4) Rule filtering - handles rule allowlisting and denylisting through the get_rulelist() method that filters rules based on configuration settings; 5) Runtime instantiation - rules are registered as classes at module load time but instantiated at runtime, allowing configuration values to be passed dynamically and respond to path-specific configuration changes; 6) Naming convention enforcement - enforces the Rule_XXXX naming convention where XXXX follows the LNNN pattern (letter + three digits); 7) Code collision prevention - prevents duplicate rule codes from being registered; 8) Group validation - ensures all rules belong to the 'all' group as a requirement; 9) Plugin integration - supports plugin-based rule registration through optional plugin parameter; 10) Configuration integration - works with FluffConfig to provide rule-specific configuration sections and validation.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 1349, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Configuration and examples for individual rules.\"\"\"\n\nfrom sqlfluff.core.plugin.host import get_plugin_manager\nfrom sqlfluff.core.rules.base import (\n    BaseRule,\n    EvalResultType,\n    LintResult,\n    RuleGhost,\n    RulePack,\n    RuleSet,\n)\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.fix import LintFix\n\n\ndef _load_standard_rules() -> RuleSet:\n    \"\"\"Initialise the standard ruleset.\n\n    We do this on each call so that dynamic rules changes\n    are possible.\n    \"\"\"\n    std_rule_set = RuleSet(name=\"standard\", config_info=get_config_info())\n\n    # Iterate through the rules list and register each rule with the standard set.\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            std_rule_set.register(rule)\n\n    return std_rule_set\n\n\ndef get_ruleset(name: str = \"standard\") -> RuleSet:\n    \"\"\"Get a ruleset by name.\"\"\"\n    std_rules = _load_standard_rules()\n    lookup = {std_rules.name: std_rules}\n    # Return a copy in case someone modifies the register.\n    return lookup[name].copy()\n\n\n__all__ = (\n    \"get_ruleset\",\n    \"RuleSet\",\n    \"RulePack\",\n    \"BaseRule\",\n    \"LintResult\",\n    \"LintFix\",\n    \"RuleContext\",\n    \"RuleGhost\",\n    \"EvalResultType\",\n    \"ConfigInfo\",\n)\n"}, {"start_line": 35000, "end_line": 37000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "the keys) may be codes, groups, aliases or names. The values\n            of the mapping are sets of rule codes *only*. This object acts as\n            a lookup to be able to translate selectors (which may contain\n            diverse references) into a consolidated list of rule codes. This\n            mapping contains the full set of rules, rather than just the filtered\n            set present in the `rules` attribute.\n    \"\"\"\n\n    rules: list[BaseRule]\n    reference_map: dict[str, set[str]]\n\n    def codes(self) -> Iterator[str]:\n        \"\"\"Returns an iterator through the codes contained in the pack.\"\"\"\n        return (r.code for r in self.rules)\n\n\nclass RuleSet:\n    \"\"\"Class to define a ruleset.\n\n    A rule set is instantiated on module load, but the references\n    to each of its classes are instantiated at runtime. This means\n    that configuration values can be passed to those rules live\n    and be responsive to any changes in configuration from the\n    path that the file is in.\n\n    Rules should be fetched using the :meth:`get_rulelist` command which\n    also handles any filtering (i.e. allowlisting and denylisting).\n\n    New rules should be added to the instance of this class using the\n    :meth:`register` decorator. That decorator registers the class, but also\n    performs basic type and name-convention checks.\n\n    The code for the rule will be parsed from the name, the description\n    from the docstring. The eval function is assumed that it will be\n    overridden by the subclass, and the parent class raises an error on\n    this function if not overridden.\n\n    \"\"\"\n\n    def __init__(self, name: str, config_info: dict[str, ConfigInfo]) -> None:\n        self.name = name\n        self.config_info = config_info\n        self._register: dict[str, RuleManifest] = {}\n\n    def _validate_config_options(\n        self, config: \"FluffConfig\", rule_ref: Optional[str] = None\n    ) -> None:\n        \"\"\"Ensure that all config options are valid.\n\n        Config options can also b"}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tion for the base classes\n        if name not in (\"BaseRule\",):\n            class_dict = RuleMetaclass._populate_code_and_description(name, class_dict)\n        # Validate rule names\n        rule_name = class_dict.get(\"name\", \"\")\n        if rule_name:\n            if not RuleMetaclass._valid_rule_name_regex.match(rule_name):\n                raise SQLFluffUserError(\n                    f\"Tried to define rule with unexpected \"\n                    f\"name format: {rule_name}. Rule names should be lowercase \"\n                    \"and snake_case with optional `.` characters to indicate \"\n                    \"a namespace or grouping. e.g. `layout.spacing`.\"\n                )\n\n        # Use the stock __new__ method now we've adjusted the docstring.\n        # There are no overload variants of type.__new__ that are compatible, so\n        # we ignore type checking in this case.\n        return super().__new__(mcs, name, bases, class_dict)  # type: ignore\n\n\nclass BaseRule(metaclass=RuleMetaclass):\n    \"\"\"The base class for a rule.\n\n    Args:\n        code (:obj:`str`): The identifier for this rule, used in inclusion\n            or exclusion.\n        description (:obj:`str`): A human readable description of what this\n            rule does. It will be displayed when any violations are found.\n\n    \"\"\"\n\n    _check_docstring = True\n    _works_on_unparsable = True\n    _adjust_anchors = False\n    targets_templated = False\n    # Some fix routines do their own checking for whether their fixes\n    # are safe around templated elements. For those - the default\n    # safety checks might be inappropriate. In those cases, set\n    # template_safe_fixes to True.\n    template_safe_fixes = False\n\n    # Config settings supported for this rule.\n    # See config_info.py for supported values.\n    config_keywords: list[str] = []\n    # Lint loop / crawl behavior. When appropriate, rules can (and should)\n    # override these values to make linting faster.\n    crawl_behaviour: BaseCrawler\n    # Rules can ove"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# names, aliases and description are less appropriate to inherit.\n        # NOTE: This applies in particular to CP02, which inherits all groups\n        # from CP01. If we don't do this, those groups don't show in the docs.\n        for base in reversed(bases):\n            if \"groups\" in class_dict:\n                break\n            elif base.groups:\n                class_dict[\"groups\"] = base.groups\n                break\n\n        # If the rule doesn't itself define `config_keywords`, check the parent\n        # classes for them. If we don't do this then they'll still be available to\n        # the rule, but they won't appear in the docs.\n        for base in reversed(bases):\n            if \"config_keywords\" in class_dict:\n                break\n            elif base.config_keywords:\n                class_dict[\"config_keywords\"] = base.config_keywords\n                break\n\n        class_dict = RuleMetaclass._populate_docstring(name, class_dict)\n        # Don't try and infer code and description for the base classes\n        if name not in (\"BaseRule\",):\n            class_dict = RuleMetaclass._populate_code_and_description(name, class_dict)\n        # Validate rule names\n        rule_name = class_dict.get(\"name\", \"\")\n        if rule_name:\n            if not RuleMetaclass._valid_rule_name_regex.match(rule_name):\n                raise SQLFluffUserError(\n                    f\"Tried to define rule with unexpected \"\n                    f\"name format: {rule_name}. Rule names should be lowercase \"\n                    \"and snake_case with optional `.` characters to indicate \"\n                    \"a namespace or grouping. e.g. `layout.spacing`.\"\n                )\n\n        # Use the stock __new__ method now we've adjusted the docstring.\n        # There are no overload variants of type.__new__ that are compatible, so\n        # we ignore type checking in this case.\n        return super().__new__(mcs, name, bases, class_dict)  # type: ignore\n\n\nclass BaseRule(metaclass=RuleMetaclass):\n  "}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "layed in the sqlfluff\n    docs.\n    \"\"\"\n\n    # Precompile the regular expressions\n    _doc_search_regex: ClassVar = re.compile(\n        \"(\\\\s{4}\\\\*\\\\*Anti-pattern\\\\*\\\\*|\\\\s{4}\\\\.\\\\. note::|\"\n        \"\\\\s\\\\s{4}\\\\*\\\\*Configuration\\\\*\\\\*)\",\n        flags=re.MULTILINE,\n    )\n    _valid_classname_regex: ClassVar = regex.compile(\n        r\"Rule_?([A-Z]{1}[a-zA-Z]+)?_([A-Z0-9]{4})\"\n    )\n    _valid_rule_name_regex: ClassVar = regex.compile(r\"[a-z][a-z\\.\\_]+\")\n\n    @staticmethod\n    def _populate_code_and_description(\n        name: str, class_dict: dict[str, Any]\n    ) -> dict[str, Any]:\n        \"\"\"Extract and validate the rule code & description.\n\n        We expect that rules are defined as classes with the name `Rule_XXXX`\n        where `XXXX` is of the form `LLNN`, where L is a letter and N is a\n        two digit number. For backward compatibility we also still support\n        the legacy format of LNNN i.e. a single letter and three digit number.\n\n        The two letters should be indicative of the grouping and focus of\n        the rule. e.g. capitalisation rules have the code CP for CaPitalisation.\n\n        If this receives classes by any other name, then it will raise a\n        :exc:`ValueError`.\n        \"\"\"\n        rule_name_match = RuleMetaclass._valid_classname_regex.match(name)\n        # Validate the name\n        if not rule_name_match:  # pragma: no cover\n            raise SQLFluffUserError(\n                f\"Tried to define rule class with \"\n                f\"unexpected format: {name}. Format should be: \"\n                \"'Rule_PluginName_LL23' (for plugins) or \"\n                \"`Rule_LL23` (for core rules).\"\n            )\n\n        plugin_name, code = rule_name_match.groups()\n        # If the docstring is multiline, then we extract just summary.\n        description = class_dict[\"__doc__\"].replace(\"``\", \"'\").split(\"\\n\")[0]\n        if plugin_name:\n            code = f\"{plugin_name}_{code}\"\n\n        class_dict[\"code\"] = code\n        class_dict[\"description\"] = desc"}, {"start_line": 34000, "end_line": 36000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n        return anchor\n\n\n@dataclass(frozen=True)\nclass RuleManifest:\n    \"\"\"Element in the rule register.\"\"\"\n\n    code: str\n    name: str\n    description: str\n    groups: tuple[str, ...]\n    aliases: tuple[str, ...]\n    rule_class: type[BaseRule]\n\n\n@dataclass\nclass RulePack:\n    \"\"\"A bundle of rules to be applied.\n\n    This contains a set of rules, post filtering but also contains the mapping\n    required to interpret any noqa messages found in files.\n\n    The reason for this object is that rules are filtered and instantiated\n    into this pack in the main process when running in multi-processing mode so\n    that user defined rules can be used without reference issues.\n\n    Attributes:\n        rules (:obj:`list` of :obj:`BaseRule`): A filtered list of instantiated\n            rules to be applied to a given file.\n        reference_map (:obj:`dict`): A mapping of rule references to the codes\n            they refer to, e.g. `{\"my_ref\": {\"LT01\", \"LT02\"}}`. The references\n            (i.e. the keys) may be codes, groups, aliases or names. The values\n            of the mapping are sets of rule codes *only*. This object acts as\n            a lookup to be able to translate selectors (which may contain\n            diverse references) into a consolidated list of rule codes. This\n            mapping contains the full set of rules, rather than just the filtered\n            set present in the `rules` attribute.\n    \"\"\"\n\n    rules: list[BaseRule]\n    reference_map: dict[str, set[str]]\n\n    def codes(self) -> Iterator[str]:\n        \"\"\"Returns an iterator through the codes contained in the pack.\"\"\"\n        return (r.code for r in self.rules)\n\n\nclass RuleSet:\n    \"\"\"Class to define a ruleset.\n\n    A rule set is instantiated on module load, but the references\n    to each of its classes are instantiated at runtime. This means\n    that configuration values can be passed to those rules live\n    and be responsive to any changes in configuration from the\n    path that the file is in.\n\n   "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`LintFix`, optional): An array of any\n            fixes which would correct this issue. If not present then it's\n            assumed that this issue will have to manually fixed.\n        memory (:obj:`dict`, optional): An object which stores any working\n            memory for the rule. The `memory` returned in any `LintResult`\n            will be passed as an input to the next segment to be crawled.\n        description (:obj:`str`, optional): A description of the problem\n            identified as part of this result. This will override the\n            description of the rule as what gets reported to the user\n            with the problem if provided.\n        source (:obj:`str`, optional): A string identifier for what\n            generated the result. Within larger libraries like reflow this\n            can be useful for tracking where a result came from.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        anchor: Optional[BaseSegment] = None,\n        fixes: Optional[list[\"LintFix\"]] = None,\n  "}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      return f\"LintResult({self.description}: {self.anchor}{fix_coda})\"\n        return f\"LintResult({self.anchor}{fix_coda})\"\n\n    def to_linting_error(self, rule: \"BaseRule\") -> Optional[SQLLintError]:\n        \"\"\"Convert a linting result to a :exc:`SQLLintError` if appropriate.\"\"\"\n        if self.anchor:\n            # Allow description override from the LintResult\n            description = self.description or rule.description\n            return SQLLintError(\n                rule=rule,\n                segment=self.anchor,\n                fixes=self.fixes,\n                description=description,\n            )\n\n        return None\n\n\nEvalResultType = Union[LintResult, list[LintResult], None]\n\n\nclass RuleMetaclass(type):\n    \"\"\"The metaclass for rules.\n\n    This metaclass provides provides auto-enrichment of the\n    rule docstring so that examples, groups, aliases and\n    names are added.\n\n    The reason we enrich the docstring is so that it can be\n    picked up by autodoc and all be displayed in the sqlfluff\n    docs.\n    \"\"\"\n\n    # Precompile the regular expressions\n    _doc_search_regex: ClassVar = re.compile(\n        \"(\\\\s{4}\\\\*\\\\*Anti-pattern\\\\*\\\\*|\\\\s{4}\\\\.\\\\. note::|\"\n        \"\\\\s\\\\s{4}\\\\*\\\\*Configuration\\\\*\\\\*)\",\n        flags=re.MULTILINE,\n    )\n    _valid_classname_regex: ClassVar = regex.compile(\n        r\"Rule_?([A-Z]{1}[a-zA-Z]+)?_([A-Z0-9]{4})\"\n    )\n    _valid_rule_name_regex: ClassVar = regex.compile(r\"[a-z][a-z\\.\\_]+\")\n\n    @staticmethod\n    def _populate_code_and_description(\n        name: str, class_dict: dict[str, Any]\n    ) -> dict[str, Any]:\n        \"\"\"Extract and validate the rule code & description.\n\n        We expect that rules are defined as classes with the name `Rule_XXXX`\n        where `XXXX` is of the form `LLNN`, where L is a letter and N is a\n        two digit number. For backward compatibility we also still support\n        the legacy format of LNNN i.e. a single letter and three digit number.\n\n        The two letters should be indicativ"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      rules: Optional[list[str]] = None,\n        user_rules: Optional[list[type[BaseRule]]] = None,\n        exclude_rules: Optional[list[str]] = None,\n    ) -> None:\n        if config and (dialect or rules or exclude_rules):\n            raise ValueError(  # pragma: no cover\n                \"Linter does not support setting both `config` and any of \"\n                \"`dialect`, `rules` or `exclude_rules`. The latter are \"\n                \"provided as convenience methods to avoid needing to \"\n                \"set the `config` object. If using `config`, please \"\n                \"provide all the other values within that object.\"\n            )\n        # Use the provided config or create one from the kwargs.\n        self.config = config or FluffConfig.from_kwargs(\n            dialect=dialect,\n            rules=rules,\n            exclude_rules=exclude_rules,\n            # Don't require a dialect to be provided yet. Defer this until we\n            # are actually linting something, since the directory we are linting\n            # from may provide additional configuration, including a dialect.\n            require_dialect=False,\n        )\n        # Get the dialect and templater\n        self.dialect: \"Dialect\" = cast(\"Dialect\", self.config.get(\"dialect_obj\"))\n        self.templater: \"RawTemplater\" = cast(\n            \"RawTemplater\", self.config.get(\"templater_obj\")\n        )\n        # Store the formatter for output\n        self.formatter = formatter\n        # Store references to user rule classes\n        self.user_rules = user_rules or []\n\n    def get_rulepack(self, config: Optional[FluffConfig] = None) -> RulePack:\n        \"\"\"Get hold of a set of rules.\"\"\"\n        rs = get_ruleset()\n        # Register any user rules\n        for rule in self.user_rules:\n            rs.register(rule)\n        cfg = config or self.config\n        return rs.get_rulepack(config=cfg)\n\n    def rule_tuples(self) -> list[RuleTuple]:\n        \"\"\"A simple pass through to access the rule tuples of the rule se"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "rules_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the standard set of rules.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import Linter\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.linter import RuleTuple\nfrom sqlfluff.core.parser import WhitespaceSegment\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.rules import BaseRule, LintFix, LintResult, get_ruleset\nfrom sqlfluff.core.rules.crawlers import RootOnlyCrawler, SegmentSeekerCrawler\nfrom sqlfluff.core.rules.doc_decorators import (\n    document_configuration,\n    document_fix_compatible,\n    document_groups,\n)\nfrom sqlfluff.core.rules.loader import get_rules_from_path\nfrom sqlfluff.core.templaters.base import TemplatedFile\nfrom sqlfluff.utils.testing.logging import fluff_log_catcher\nfrom sqlfluff.utils.testing.rules import get_rule_from_set\nfrom test.fixtures.rules.custom.L000 import Rule_L000\nfrom test.fixtures.rules.custom.S000 import Rule_S000\n\n\nclass Rule_T042(BaseRule):\n    \"\"\"A dummy rule.\"\"\"\n\n    groups = (\"all\",)\n\n    def _eval(self, context):\n        pass\n\n\nclass Rule_T001(BaseRule):\n    \"\"\"A deliberately malicious rule.\n\n    **Anti-pattern**\n\n    Blah blah\n    \"\"\"\n\n    groups = (\"all\",)\n    crawl_behaviour = SegmentSeekerCrawler({\"whitespace\"})\n    is_fix_compatible = True\n\n    def _eval(self, context):\n        \"\"\"Stars make newlines.\"\"\"\n        if context.segment.is_type(\"whitespace\"):\n            return LintResult(\n                anchor=context.segment,\n                fixes=[\n                    LintFix.replace(\n                        context.segment, [WhitespaceSegment(context.segment.raw + \" \")]\n                    )\n                ],\n            )\n\n\nclass Rule_T002(BaseRule):\n    \"\"\"A rule which says all raw code segments are bad.\n\n    This is used for testing unparsable code.\n    \"\"\"\n\n    groups = (\"all\",)\n    # Root only crawler so that the in-rule filters don't kick in.\n    crawl_behaviour = RootOnlyCrawler()\n\n    def _eval(s"}], "retrieved_count": 10, "cost_time": 1.0290520191192627}
{"question": "How does SQLFluff implement its plugin system for custom rules?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff implements its plugin system for custom rules through a modular architecture that allows third-party rule development and integration. The plugin system works as follows: 1) Plugin Discovery - SQLFluff uses Python's entry point system to discover plugins, with plugins registering themselves through setuptools entry points; 2) Rule Registration - Custom rules inherit from BaseRule and are automatically registered through the RuleMetaclass when imported; 3) Plugin Loading - The plugin system loads custom rules from external packages and integrates them into the main rule registry; 4) Configuration Integration - Custom rules integrate with the configuration system, allowing users to configure plugin rules through standard configuration files; 5) Rule Validation - Plugin rules are validated against the BaseRule interface and must implement required methods like _eval(); 6) Metadata Management - Plugin rules can define their own metadata (code, name, description, groups) that gets integrated into the rule system; 7) Fix Generation - Plugin rules can generate fixes using the same LintFix system as built-in rules; 8) Error Handling - Plugin rules are wrapped with error handling to prevent plugin errors from crashing the main application; 9) Documentation Integration - Plugin rules can provide documentation that gets integrated into SQLFluff's help system; 10) Testing Support - The plugin system provides utilities for testing custom rules and integrating them into SQLFluff's test suite.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 1269, "belongs_to": {"file_name": "lib.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/plugin", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Base implementation for the plugin.\"\"\"\n\nfrom typing import Any\n\nfrom sqlfluff.core.config import load_config_resource\nfrom sqlfluff.core.plugin import hookimpl\nfrom sqlfluff.core.rules import BaseRule, ConfigInfo\nfrom sqlfluff.core.rules.config_info import STANDARD_CONFIG_INFO_DICT\nfrom sqlfluff.core.rules.loader import get_rules_from_path\nfrom sqlfluff.core.templaters import RawTemplater, core_templaters\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: All standard rules will eventually be loaded as\n    plugins and so before 2.0.0, once all legacy plugin definitions\n    are migrated, this function will be amended to return no rules.\n    \"\"\"\n    return get_rules_from_path()\n\n\n@hookimpl\ndef get_templaters() -> list[type[RawTemplater]]:\n    \"\"\"Get templaters.\"\"\"\n    templaters = list(t for t in core_templaters())\n    return templaters\n\n\n@hookimpl\ndef load_default_config() -> dict[str, Any]:\n    \"\"\"Loads the default configuration for the plugin.\"\"\"\n    return load_config_resource(\n        package=\"sqlfluff.core\",\n        file_name=\"default_config.cfg\",\n    )\n\n\n@hookimpl\ndef get_configs_info() -> dict[str, ConfigInfo]:\n    \"\"\"Get rule config validations and descriptions.\"\"\"\n    return STANDARD_CONFIG_INFO_DICT\n"}, {"start_line": 0, "end_line": 1183, "belongs_to": {"file_name": "hookspecs.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/plugin", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the specification to implement a plugin.\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import TYPE_CHECKING, Any\n\nimport pluggy\n\nfrom sqlfluff.core.plugin import plugin_base_name\n\nif TYPE_CHECKING:  # pragma: no cover\n    # NOTE: This import is against the normal import rules, but is here for strict\n    # type checking. We have an exception for this in the import linter.\n    from sqlfluff.core.rules.base import BaseRule\n\nhookspec = pluggy.HookspecMarker(plugin_base_name)\n\n\nclass PluginSpec:\n    \"\"\"Defines the method signatures for plugin implementations.\"\"\"\n\n    @hookspec\n    @abstractmethod\n    def get_rules(self) -> list[type[\"BaseRule\"]]:\n        \"\"\"Get plugin rules.\"\"\"\n\n    @hookspec\n    @abstractmethod\n    def load_default_config(self) -> dict[str, Any]:\n        \"\"\"Loads the default configuration for the plugin.\"\"\"\n\n    @hookspec\n    @abstractmethod\n    # TODO: This type annotation could probably be more specific but that would\n    # require making the config info object something more like a namedTuple rather\n    # than a dict.\n    def get_configs_info(self) -> dict[str, dict[str, Any]]:\n        \"\"\"Get rule config validations and descriptions.\"\"\"\n"}, {"start_line": 1000, "end_line": 2364, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/aliasing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tion\": \"How to handle comparison casefolding in an alias.\",\n        },\n        \"min_alias_length\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The minimum length of an alias to allow without raising a violation.\"\n            ),\n        },\n        \"max_alias_length\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The maximum length of an alias to allow without raising a violation.\"\n            ),\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.aliasing.AL01 import Rule_AL01\n    from sqlfluff.rules.aliasing.AL02 import Rule_AL02\n    from sqlfluff.rules.aliasing.AL03 import Rule_AL03\n    from sqlfluff.rules.aliasing.AL04 import Rule_AL04\n    from sqlfluff.rules.aliasing.AL05 import Rule_AL05\n    from sqlfluff.rules.aliasing.AL06 import Rule_AL06\n    from sqlfluff.rules.aliasing.AL07 import Rule_AL07\n    from sqlfluff.rules.aliasing.AL08 import Rule_AL08\n    from sqlfluff.rules.aliasing.AL09 import Rule_AL09\n\n    return [\n        Rule_AL01,\n        Rule_AL02,\n        Rule_AL03,\n        Rule_AL04,\n        Rule_AL05,\n        Rule_AL06,\n        Rule_AL07,\n        Rule_AL08,\n        Rule_AL09,\n    ]\n"}, {"start_line": 0, "end_line": 378, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/jinja", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The jinja rules plugin bundle.\"\"\"\n\nfrom sqlfluff.core.plugin import hookimpl\nfrom sqlfluff.core.rules import BaseRule\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.jinja.JJ01 import Rule_JJ01\n\n    return [Rule_JJ01]\n"}, {"start_line": 1000, "end_line": 2820, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e that currently, the gap _before_ and _after_ the semicolon \"\n                \"is considered 'between' statements.\"\n            ),\n        },\n        \"maximum_empty_lines_inside_statements\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The maximum number of empty lines allowed inside statements.\"\n            ),\n        },\n        \"wildcard_policy\": {\n            \"validation\": [\"single\", \"multiple\"],\n            \"definition\": \"Treatment of wildcards. Defaults to ``single``.\",\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.layout.LT01 import Rule_LT01\n    from sqlfluff.rules.layout.LT02 import Rule_LT02\n    from sqlfluff.rules.layout.LT03 import Rule_LT03\n    from sqlfluff.rules.layout.LT04 import Rule_LT04\n    from sqlfluff.rules.layout.LT05 import Rule_LT05\n    from sqlfluff.rules.layout.LT06 import Rule_LT06\n    from sqlfluff.rules.layout.LT07 import Rule_LT07\n    from sqlfluff.rules.layout.LT08 import Rule_LT08\n    from sqlfluff.rules.layout.LT09 import Rule_LT09\n    from sqlfluff.rules.layout.LT10 import Rule_LT10\n    from sqlfluff.rules.layout.LT11 import Rule_LT11\n    from sqlfluff.rules.layout.LT12 import Rule_LT12\n    from sqlfluff.rules.layout.LT13 import Rule_LT13\n    from sqlfluff.rules.layout.LT14 import Rule_LT14\n    from sqlfluff.rules.layout.LT15 import Rule_LT15\n\n    return [\n        Rule_LT01,\n        Rule_LT02,\n        Rule_LT03,\n        Rule_LT04,\n        Rule_LT05,\n        Rule_LT06,\n        Rule_LT07,\n        Rule_LT08,\n        Rule_LT09,\n        Rule_LT10,\n        Rule_LT11,\n        Rule_LT12,\n        Rule_LT13,\n        Rule_LT14,\n        Rule_LT15,\n    ]\n"}, {"start_line": 0, "end_line": 1987, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/plugins/sqlfluff-plugin-example/src/sqlfluff_plugin_example", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"An example of a custom rule implemented through the plugin system.\n\nThis uses the rules API supported from 0.4.0 onwards.\n\"\"\"\n\nfrom typing import Any\n\nfrom sqlfluff.core.config import load_config_resource\nfrom sqlfluff.core.plugin import hookimpl\nfrom sqlfluff.core.rules import BaseRule, ConfigInfo\n\n# For backward compatibility we still support importing\n# rules within the body of the root plugin module. This is included\n# here for illustration, but also such that support for this import\n# order can be tested in the test suite (and that the associated\n# warning is triggered).\n# See note below in `get_rules()` for more details.\n# i.e. we DO NOT recommend importing here:\nfrom sqlfluff_plugin_example.rules import Rule_Example_L001  # noqa: F401\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: It is much better that we only import the rule on demand.\n    The root module of the plugin (i.e. this file which contains\n    all of the hook implementations) should have fully loaded before\n    we try and import the rules. This is partly for performance\n    reasons - but more because the definition of a BaseRule requires\n    that all of the get_configs_info() methods have both been\n    defined _and have run_ before so all the validation information\n    is available for the validation steps in the meta class.\n    \"\"\"\n    # i.e. we DO recommend importing here:\n    from sqlfluff_plugin_example.rules import Rule_Example_L001  # noqa: F811\n\n    return [Rule_Example_L001]\n\n\n@hookimpl\ndef load_default_config() -> dict[str, Any]:\n    \"\"\"Loads the default configuration for the plugin.\"\"\"\n    return load_config_resource(\n        package=\"sqlfluff_plugin_example\",\n        file_name=\"plugin_default_config.cfg\",\n    )\n\n\n@hookimpl\ndef get_configs_info() -> dict[str, dict[str, ConfigInfo]]:\n    \"\"\"Get rule config validations and descriptions.\"\"\"\n    return {\n        \"forbidden_columns\": {\"definition\": \"A list of column to forbid\"},\n    }\n"}, {"start_line": 0, "end_line": 393, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/plugin", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Marker to be imported and used in plugins (and for own implementations).\"\"\"\n\nfrom typing import Any, Callable, TypeVar, cast\n\nimport pluggy\n\n# Improvement suggested by @oremanj on python/typing gitter\nF = TypeVar(\"F\", bound=Callable[..., Any])\n\nproject_name = \"sqlfluff\"\nplugin_base_name = f\"{project_name}-plugin\"\nhookimpl = cast(Callable[[F], F], pluggy.HookimplMarker(plugin_base_name))\n"}, {"start_line": 0, "end_line": 1349, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Configuration and examples for individual rules.\"\"\"\n\nfrom sqlfluff.core.plugin.host import get_plugin_manager\nfrom sqlfluff.core.rules.base import (\n    BaseRule,\n    EvalResultType,\n    LintResult,\n    RuleGhost,\n    RulePack,\n    RuleSet,\n)\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.fix import LintFix\n\n\ndef _load_standard_rules() -> RuleSet:\n    \"\"\"Initialise the standard ruleset.\n\n    We do this on each call so that dynamic rules changes\n    are possible.\n    \"\"\"\n    std_rule_set = RuleSet(name=\"standard\", config_info=get_config_info())\n\n    # Iterate through the rules list and register each rule with the standard set.\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            std_rule_set.register(rule)\n\n    return std_rule_set\n\n\ndef get_ruleset(name: str = \"standard\") -> RuleSet:\n    \"\"\"Get a ruleset by name.\"\"\"\n    std_rules = _load_standard_rules()\n    lookup = {std_rules.name: std_rules}\n    # Return a copy in case someone modifies the register.\n    return lookup[name].copy()\n\n\n__all__ = (\n    \"get_ruleset\",\n    \"RuleSet\",\n    \"RulePack\",\n    \"BaseRule\",\n    \"LintResult\",\n    \"LintFix\",\n    \"RuleContext\",\n    \"RuleGhost\",\n    \"EvalResultType\",\n    \"ConfigInfo\",\n)\n"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "plugin_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "         \"sqlfluff.core.plugin.lib\",\n        }\n    )\n\n    # At this stage we should also check that the example plugin\n    # also raises a warning for it's import location.\n    assert (\n        \"Rule 'Rule_Example_L001' has been imported before all plugins \"\n        \"have been fully loaded\"\n    ) in caplog.text\n\n\n@pytest.mark.parametrize(\n    \"rule_ref\",\n    # Check both V1 plugin\n    [\"Rule_Example_L001\"],\n)\ndef test__plugin_example_rules_returned(rule_ref):\n    \"\"\"Test that the example rules from the plugin are returned.\"\"\"\n    plugin_manager = get_plugin_manager()\n    # The plugin import order is non-deterministic\n    rule_names = [\n        rule.__name__ for rules in plugin_manager.hook.get_rules() for rule in rules\n    ]\n    print(f\"Rule names: {rule_names}\")\n    assert rule_ref in rule_names\n\n\n@pytest.mark.parametrize(\n    \"rule_ref,config_option\",\n    # Check both V1 and V2 rule plugins.\n    [(\"Example_L001\", \"forbidden_columns\")],\n)\ndef test__plugin_default_config_read(rule_ref, config_option):\n    \"\"\"Test that the example plugin default config is merged into FluffConfig.\"\"\"\n    fluff_config = FluffConfig(overrides={\"dialect\": \"ansi\"})\n    # The plugin import order is non-deterministic\n    print(f\"Detected config sections: {fluff_config._configs['rules'].keys()}\")\n    # Check V1\n    assert config_option in fluff_config._configs[\"rules\"][rule_ref]\n\n\nclass MockEntryPoint(importlib.metadata.EntryPoint):\n    \"\"\"Fake Entry Point which just raises an exception on load.\"\"\"\n\n    def load(self):\n        \"\"\"Raise an exception on load.\"\"\"\n        raise ValueError(\"TEST ERROR\")\n\n\ndef test__plugin_handle_bad_load():\n    \"\"\"Test that we can safely survive a plugin which fails to load.\"\"\"\n    # Mock fake plugin\n    ep = MockEntryPoint(\"test_name\", \"test_value\", \"sqlfluff\")\n\n    plugin_manager = get_plugin_manager()\n    with fluff_log_catcher(logging.WARNING, \"sqlfluff.plugin\") as caplog:\n        _load_plugin(plugin_manager, ep, \"plugin_name\", \"v1.2.3\")\n    # Assert that the"}, {"start_line": 0, "end_line": 1573, "belongs_to": {"file_name": "rules.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/plugins/sqlfluff-plugin-example/src/sqlfluff_plugin_example", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"An example of a custom rule implemented through the plugin system.\n\nThis uses the rules API supported from 0.4.0 onwards.\n\"\"\"\n\nfrom sqlfluff.core.rules import (\n    BaseRule,\n    LintResult,\n    RuleContext,\n)\nfrom sqlfluff.core.rules.crawlers import SegmentSeekerCrawler\n\n\n# These two decorators allow plugins\n# to be displayed in the sqlfluff docs\nclass Rule_Example_L001(BaseRule):\n    \"\"\"ORDER BY on these columns is forbidden!\n\n    **Anti-pattern**\n\n    Using ``ORDER BY`` one some forbidden columns.\n\n    .. code-block:: sql\n\n        SELECT *\n        FROM foo\n        ORDER BY\n            bar,\n            baz\n\n    **Best practice**\n\n    Do not order by these columns.\n\n    .. code-block:: sql\n\n        SELECT *\n        FROM foo\n        ORDER BY bar\n    \"\"\"\n\n    groups = (\"all\",)\n    config_keywords = [\"forbidden_columns\"]\n    crawl_behaviour = SegmentSeekerCrawler({\"orderby_clause\"})\n    is_fix_compatible = True\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Overwrite __init__ to set config.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.forbidden_columns = [\n            col.strip() for col in self.forbidden_columns.split(\",\")\n        ]\n\n    def _eval(self, context: RuleContext):\n        \"\"\"We should not ORDER BY forbidden_columns.\"\"\"\n        for seg in context.segment.segments:\n            col_name = seg.raw.lower()\n            if col_name in self.forbidden_columns:\n                return LintResult(\n                    anchor=seg,\n                    description=f\"Column `{col_name}` not allowed in ORDER BY.\",\n                )\n"}], "retrieved_count": 10, "cost_time": 1.0672438144683838}
{"question": "What are the core components of SQLFluff's parser?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's parser consists of several core components: 1) Lexer - separates SQL into segments of whitespace and code, producing typed segments (subclasses of RawSegment); 2) Parser - the most complex component that applies dialect-specific grammars to lexed segments, creating a tree-like structure with FileSegment as the root containing StatementSegments; 3) Grammar system - defines the shape of SQL statements using classes like Sequence, OneOf, Delimited, Bracketed, AnyNumberOf, AnySetOf, Ref, and Conditional; 4) Segment system - includes BaseSegment (abstract base class), RawSegment (raw tokens), and various specialized segments like KeywordSegment, IdentifierSegment, LiteralSegment, SymbolSegment, etc.; 5) Position markers - track location information for segments; 6) Parse context - manages parsing state and configuration. The parser uses a single-pass approach where segments recursively match based on their respective grammars until reaching raw segments with no children.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 38, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.core.parser.\"\"\"\n"}, {"start_line": 1000, "end_line": 2225, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ",\n    LiteralSegment,\n    NewlineSegment,\n    RawSegment,\n    SegmentGenerator,\n    SourceFix,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.types import ParseMode\n\n__all__ = (\n    \"BaseSegment\",\n    \"SourceFix\",\n    \"BaseFileSegment\",\n    \"BracketedSegment\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"SymbolSegment\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"LiteralKeywordSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"SegmentGenerator\",\n    \"Sequence\",\n    \"OneOf\",\n    \"Delimited\",\n    \"Bracketed\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\",\n    \"Conditional\",\n    \"StringParser\",\n    \"MultiStringParser\",\n    \"TypedParser\",\n    \"RegexParser\",\n    \"PositionMarker\",\n    \"Lexer\",\n    \"StringLexer\",\n    \"RegexLexer\",\n    \"Parser\",\n    \"Matchable\",\n    \"ParseMode\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"init file for the parser.\"\"\"\n\nfrom sqlfluff.core.parser.grammar import (\n    AnyNumberOf,\n    AnySetOf,\n    Anything,\n    Bracketed,\n    Conditional,\n    Delimited,\n    Nothing,\n    OneOf,\n    OptionallyBracketed,\n    OptionallyDelimited,\n    Ref,\n    Sequence,\n)\nfrom sqlfluff.core.parser.lexer import Lexer, RegexLexer, StringLexer\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.parser import Parser\nfrom sqlfluff.core.parser.parsers import (\n    MultiStringParser,\n    RegexParser,\n    StringParser,\n    TypedParser,\n)\nfrom sqlfluff.core.parser.segments import (\n    BaseFileSegment,\n    BaseSegment,\n    BinaryOperatorSegment,\n    BracketedSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    Dedent,\n    IdentifierSegment,\n    ImplicitIndent,\n    Indent,\n    KeywordSegment,\n    LiteralKeywordSegment,\n    LiteralSegment,\n    NewlineSegment,\n    RawSegment,\n    SegmentGenerator,\n    SourceFix,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.types import ParseMode\n\n__all__ = (\n    \"BaseSegment\",\n    \"SourceFix\",\n    \"BaseFileSegment\",\n    \"BracketedSegment\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"SymbolSegment\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"LiteralKeywordSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"SegmentGenerator\",\n    \"Sequence\",\n    \"OneOf\",\n    \"Delimited\",\n    \"Bracketed\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\","}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parser.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the Parser class.\"\"\"\n\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import check_still_complete\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.parser.segments import BaseFileSegment, BaseSegment\n\n\nclass Parser:\n    \"\"\"Instantiates parsed queries from a sequence of lexed raw segments.\"\"\"\n\n    def __init__(\n        self, config: Optional[FluffConfig] = None, dialect: Optional[str] = None\n    ):\n        if config and dialect:\n            raise ValueError(  # pragma: no cover\n                \"Parser does not support setting both `config` and `dialect`.\"\n            )\n        # Use the provided config or create one from the dialect.\n        self.config = config or FluffConfig.from_kwargs(dialect=dialect)\n        self.RootSegment: type[BaseFileSegment] = self.config.get(\n            \"dialect_obj\"\n        ).get_root_segment()\n\n    def parse(\n        self,\n        segments: Sequence[\"BaseSegment\"],\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> Optional[\"BaseSegment\"]:\n        \"\"\"Parse a series of lexed tokens using the current dialect.\"\"\"\n        if not segments:  # pragma: no cover\n            # This should normally never happen because there will usually\n            # be an end_of_file segment. It would probably only happen in\n            # api use cases.\n            return None\n\n        # NOTE: This is the only time we use the parse context not in the\n        # context of a context manager. That's because it's the initial\n        # instantiation.\n        ctx = ParseContext.from_config(config=self.config)\n        # Kick off parsing with the root segment. The BaseFileSegment has\n        # a unique entry point to facilitate exactly this. All other segments\n        # will use the standard .match() route.\n        root = self.RootSegment.root_pars"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "dialect_ansi.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The core ANSI dialect.\n\nThis is the core SQL grammar. We'll probably extend this or make it pluggable\nfor other dialects. Here we encode the structure of the language.\n\nThere shouldn't be any underlying \"machinery\" here, that should all\nbe defined elsewhere.\n\nA lot of the inspiration for this sql grammar is taken from the cockroach\nlabs full sql grammar. In particular their way for dividing up the expression\ngrammar. Check out their docs, they're awesome.\nhttps://www.cockroachlabs.com/docs/stable/sql-grammar.html#select_stmt\n\"\"\"\n\nfrom collections.abc import Generator\nfrom enum import Enum\nfrom typing import NamedTuple, Optional, Union, cast\n\nfrom sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.dialects.common import AliasInfo, ColumnAliasInfo\nfrom sqlfluff.core.parser import (\n    AnyNumberOf,\n    AnySetOf,\n    Anything,\n    BaseFileSegment,\n    BaseSegment,\n    BinaryOperatorSegment,\n    Bracketed,\n    BracketedSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    Conditional,\n    Dedent,\n    Delimited,\n    IdentifierSegment,\n    ImplicitIndent,\n    Indent,\n    KeywordSegment,\n    LiteralKeywordSegment,\n    LiteralSegment,\n    Matchable,\n    MultiStringParser,\n    NewlineSegment,\n    Nothing,\n    OneOf,\n    OptionallyBracketed,\n    ParseMode,\n    RawSegment,\n    Ref,\n    RegexLexer,\n    RegexParser,\n    SegmentGenerator,\n    Sequence,\n    StringLexer,\n    StringParser,\n    SymbolSegment,\n    TypedParser,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.dialects.dialect_ansi_keywords import (\n    ansi_reserved_keywords,\n    ansi_unreserved_keywords,\n)\n\nansi_dialect = Dialect(\n    \"ansi\",\n    root_segment_name=\"FileSegment\",\n    formatted_name=\"ANSI\",\n    docstring=\"\"\"**Default Casing**: ``UPPERCASE``\n\n**Quotes**: String Literals: ``''``, Identifiers: ``\"\"``\n\nThis is the base dialect which holds most of the definitions of common\nSQL commands "}, {"start_line": 0, "end_line": 664, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/grammar", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Definitions of grammars.\"\"\"\n\nfrom sqlfluff.core.parser.grammar.anyof import (\n    AnyNumberOf,\n    AnySetOf,\n    OneOf,\n    OptionallyBracketed,\n)\nfrom sqlfluff.core.parser.grammar.base import Anything, Nothing, Ref\nfrom sqlfluff.core.parser.grammar.conditional import Conditional\nfrom sqlfluff.core.parser.grammar.delimited import Delimited, OptionallyDelimited\nfrom sqlfluff.core.parser.grammar.sequence import Bracketed, Sequence\n\n__all__ = (\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"OneOf\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\",\n    \"Delimited\",\n    \"Sequence\",\n    \"Bracketed\",\n    \"Conditional\",\n)\n"}, {"start_line": 0, "end_line": 1715, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Definitions of the segment classes.\"\"\"\n\nfrom sqlfluff.core.parser.segments.base import (\n    BaseSegment,\n    SourceFix,\n    UnparsableSegment,\n)\nfrom sqlfluff.core.parser.segments.bracketed import BracketedSegment\nfrom sqlfluff.core.parser.segments.common import (\n    BinaryOperatorSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    IdentifierSegment,\n    LiteralSegment,\n    NewlineSegment,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.segments.file import BaseFileSegment\nfrom sqlfluff.core.parser.segments.generator import SegmentGenerator\nfrom sqlfluff.core.parser.segments.keyword import KeywordSegment, LiteralKeywordSegment\nfrom sqlfluff.core.parser.segments.meta import (\n    Dedent,\n    EndOfFile,\n    ImplicitIndent,\n    Indent,\n    MetaSegment,\n    TemplateLoop,\n    TemplateSegment,\n)\nfrom sqlfluff.core.parser.segments.raw import RawSegment\n\n__all__ = (\n    \"BaseSegment\",\n    \"BaseFileSegment\",\n    \"UnparsableSegment\",\n    \"BracketedSegment\",\n    \"SegmentGenerator\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"LiteralKeywordSegment\",\n    \"SymbolSegment\",\n    \"MetaSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"TemplateSegment\",\n    \"EndOfFile\",\n    \"TemplateLoop\",\n    \"SourceFix\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n)\n"}, {"start_line": 0, "end_line": 1589, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The core elements of sqlfluff.\"\"\"\n\nimport tblib.pickling_support\n\n# Config objects\nfrom sqlfluff.core.config import FluffConfig\n\n# Dialect introspection\nfrom sqlfluff.core.dialects import dialect_readout, dialect_selector\n\n# All of the errors.\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLFluffUserError,\n    SQLLexError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\n\n# Public classes\nfrom sqlfluff.core.linter import Linter\nfrom sqlfluff.core.parser import Lexer, Parser\n\n# Timing objects\nfrom sqlfluff.core.timing import TimingSummary\n\n__all__ = (\n    \"FluffConfig\",\n    \"Linter\",\n    \"Lexer\",\n    \"Parser\",\n    \"dialect_selector\",\n    \"dialect_readout\",\n    \"SQLBaseError\",\n    \"SQLTemplaterError\",\n    \"SQLLexError\",\n    \"SQLParseError\",\n    \"SQLLintError\",\n    \"SQLFluffUserError\",\n    \"TimingSummary\",\n)\n\n# This is for \"sqlfluff lint\" and \"sqlfluff fix\" multiprocessing (--processes)\n# support. If an exception (i.e. runtime error) occurs in a worker process, we\n# want to return the tracebook to the main process and report it there, as part\n# of the normal output. However, anything returned from a multiprocessing.Pool\n# worker must be serializable using \"pickle\". By default, Python traceback\n# objects cannot be pickled. The tblib package addresses this limitation; we\n# simply need to install it before creating the worker pool. See these links for\n# additional context:\n# https://pypi.org/project/tblib/\n# https://stackoverflow.com/questions/6126007/python-getting-a-traceback-from-a-multiprocessing-process\ntblib.pickling_support.install()\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parsers.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Individual segment parsers.\n\nMatchable objects which return individual segments.\n\"\"\"\n\nfrom abc import abstractmethod\nfrom collections.abc import Collection, Sequence\nfrom typing import Any, Callable, Optional\nfrom uuid import uuid4\n\nimport regex\n\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.segments import BaseSegment, RawSegment\nfrom sqlfluff.core.parser.types import SimpleHintType\n\n\nclass BaseParser(Matchable):\n    \"\"\"An abstract class from which other Parsers should inherit.\"\"\"\n\n    # Meta segments are handled separately. All Parser elements\n    # are assumed to be not meta.\n    is_meta: bool = False\n\n    @abstractmethod\n    def __init__(\n        self,\n        raw_class: type[RawSegment],\n        type: Optional[str] = None,\n        optional: bool = False,\n        # The following kwargs are passed on to the segment:\n        trim_chars: Optional[tuple[str, ...]] = None,\n        casefold: Optional[Callable[[str], str]] = None,\n    ) -> None:\n        self.raw_class = raw_class\n        # Store instance_types rather than just type to allow\n        # for multiple possible types to be supported in derivative\n        # classes.\n        self._instance_types: tuple[str, ...] = (type or raw_class.type,)\n        self.optional = optional\n        self._trim_chars = trim_chars\n        self.casefold = casefold\n        # Generate a cache key\n        self._cache_key = uuid4().hex\n\n    def cache_key(self) -> str:\n        \"\"\"Get the cache key for this parser.\n\n        For parsers, they're unique per-instance.\n        \"\"\"\n        return self._cache_key\n\n    def is_optional(self) -> bool:\n        \"\"\"Return whether this element is optional.\"\"\"\n        return self.optional\n\n    def segment_kwargs(self) -> dict[str, Any]:\n        \"\"\"Generates the segment_kwargs package for generating a matched segment.\"\"\"\n        segment_kwargs: dict[str, Any"}, {"start_line": 0, "end_line": 1849, "belongs_to": {"file_name": "helpers.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Helpers for the parser module.\"\"\"\n\nfrom typing import TYPE_CHECKING\n\nfrom sqlfluff.core.errors import SQLParseError\n\nif TYPE_CHECKING:\n    from sqlfluff.core.parser.segments import BaseSegment  # pragma: no cover\n\n\ndef join_segments_raw(segments: tuple[\"BaseSegment\", ...]) -> str:\n    \"\"\"Make a string from the joined `raw` attributes of an iterable of segments.\"\"\"\n    return \"\".join(s.raw for s in segments)\n\n\ndef check_still_complete(\n    segments_in: tuple[\"BaseSegment\", ...],\n    matched_segments: tuple[\"BaseSegment\", ...],\n    unmatched_segments: tuple[\"BaseSegment\", ...],\n) -> bool:\n    \"\"\"Check that the segments in are the same as the segments out.\"\"\"\n    initial_str = join_segments_raw(segments_in)\n    current_str = join_segments_raw(matched_segments + unmatched_segments)\n\n    if initial_str != current_str:  # pragma: no cover\n        segment = unmatched_segments[0] if unmatched_segments else None\n        raise SQLParseError(\n            f\"Parse completeness check fail: {current_str!r} != {initial_str!r}\",\n            segment=segment,\n        )\n    return True\n\n\ndef trim_non_code_segments(\n    segments: tuple[\"BaseSegment\", ...],\n) -> tuple[\n    tuple[\"BaseSegment\", ...], tuple[\"BaseSegment\", ...], tuple[\"BaseSegment\", ...]\n]:\n    \"\"\"Take segments and split off surrounding non-code segments as appropriate.\n\n    We use slices to avoid creating too many unnecessary tuples.\n    \"\"\"\n    pre_idx = 0\n    seg_len = len(segments)\n    post_idx = seg_len\n\n    if segments:\n        seg_len = len(segments)\n\n        # Trim the start\n        while pre_idx < seg_len and not segments[pre_idx].is_code:\n            pre_idx += 1\n\n        # Trim the end\n        while post_idx > pre_idx and not segments[post_idx - 1].is_code:\n            post_idx -= 1\n\n    return segments[:pre_idx], segments[pre_idx:post_idx], segments[post_idx:]\n"}], "retrieved_count": 10, "cost_time": 1.06748628616333}
{"question": "What is the purpose of the BaseRule class in SQLFluff?", "answer": null, "relative_code_list": null, "ground_truth": "The BaseRule class serves as the foundational abstract base class for all SQLFluff linting rules. Its key purposes include: 1) Rule definition framework - provides the base structure and interface that all rules must implement, including the essential _eval() method that evaluates rules against parse tree segments; 2) Configuration management - handles rule-specific configuration through config_keywords list, validates configuration options during initialization, and provides access to configuration values as class attributes; 3) Metadata management - manages rule metadata including code (unique identifier), description (human-readable explanation), name, groups (categorization), and aliases (backward compatibility); 4) Crawling behavior - defines crawl_behaviour attribute that controls how rules traverse the parse tree (e.g., RootOnlyCrawler, SegmentSeekerCrawler); 5) Linting phases - supports different linting phases (main, post) for rules that need to run at specific times; 6) Template awareness - provides flags like targets_templated and template_safe_fixes to control how rules interact with templated code; 7) Error handling - includes _works_on_unparsable flag to control whether rules should process unparsable segments; 8) Logging integration - provides custom logging with rule code context; 9) Fix compatibility - supports automatic code fixing through is_fix_compatible flag and LintFix objects; 10) Metaclass integration - uses RuleMetaclass to automatically populate documentation, code, and description from class names and docstrings.", "score": null, "retrieved_content": [{"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tion for the base classes\n        if name not in (\"BaseRule\",):\n            class_dict = RuleMetaclass._populate_code_and_description(name, class_dict)\n        # Validate rule names\n        rule_name = class_dict.get(\"name\", \"\")\n        if rule_name:\n            if not RuleMetaclass._valid_rule_name_regex.match(rule_name):\n                raise SQLFluffUserError(\n                    f\"Tried to define rule with unexpected \"\n                    f\"name format: {rule_name}. Rule names should be lowercase \"\n                    \"and snake_case with optional `.` characters to indicate \"\n                    \"a namespace or grouping. e.g. `layout.spacing`.\"\n                )\n\n        # Use the stock __new__ method now we've adjusted the docstring.\n        # There are no overload variants of type.__new__ that are compatible, so\n        # we ignore type checking in this case.\n        return super().__new__(mcs, name, bases, class_dict)  # type: ignore\n\n\nclass BaseRule(metaclass=RuleMetaclass):\n    \"\"\"The base class for a rule.\n\n    Args:\n        code (:obj:`str`): The identifier for this rule, used in inclusion\n            or exclusion.\n        description (:obj:`str`): A human readable description of what this\n            rule does. It will be displayed when any violations are found.\n\n    \"\"\"\n\n    _check_docstring = True\n    _works_on_unparsable = True\n    _adjust_anchors = False\n    targets_templated = False\n    # Some fix routines do their own checking for whether their fixes\n    # are safe around templated elements. For those - the default\n    # safety checks might be inappropriate. In those cases, set\n    # template_safe_fixes to True.\n    template_safe_fixes = False\n\n    # Config settings supported for this rule.\n    # See config_info.py for supported values.\n    config_keywords: list[str] = []\n    # Lint loop / crawl behavior. When appropriate, rules can (and should)\n    # override these values to make linting faster.\n    crawl_behaviour: BaseCrawler\n    # Rules can ove"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Implements the base rule class.\n\nRules crawl through the trees returned by the parser and evaluate particular\nrules.\n\nThe intent is that it should be possible for the rules to be expressed\nas simply as possible, with as much of the complexity abstracted away.\n\nThe evaluation function should take enough arguments that it can evaluate\nthe position of the given segment in relation to its neighbors, and that\nthe segment which finally \"triggers\" the error, should be the one that would\nbe corrected OR if the rule relates to something that is missing, then it\nshould flag on the segment FOLLOWING, the place that the desired element is\nmissing.\n\"\"\"\n\nimport bdb\nimport copy\nimport fnmatch\nimport logging\nimport pathlib\nimport re\nfrom collections import defaultdict, namedtuple\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    ClassVar,\n    DefaultDict,\n    Optional,\n    Union,\n)\n\nimport regex\n\nfrom sqlfluff.core.errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a "}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# names, aliases and description are less appropriate to inherit.\n        # NOTE: This applies in particular to CP02, which inherits all groups\n        # from CP01. If we don't do this, those groups don't show in the docs.\n        for base in reversed(bases):\n            if \"groups\" in class_dict:\n                break\n            elif base.groups:\n                class_dict[\"groups\"] = base.groups\n                break\n\n        # If the rule doesn't itself define `config_keywords`, check the parent\n        # classes for them. If we don't do this then they'll still be available to\n        # the rule, but they won't appear in the docs.\n        for base in reversed(bases):\n            if \"config_keywords\" in class_dict:\n                break\n            elif base.config_keywords:\n                class_dict[\"config_keywords\"] = base.config_keywords\n                break\n\n        class_dict = RuleMetaclass._populate_docstring(name, class_dict)\n        # Don't try and infer code and description for the base classes\n        if name not in (\"BaseRule\",):\n            class_dict = RuleMetaclass._populate_code_and_description(name, class_dict)\n        # Validate rule names\n        rule_name = class_dict.get(\"name\", \"\")\n        if rule_name:\n            if not RuleMetaclass._valid_rule_name_regex.match(rule_name):\n                raise SQLFluffUserError(\n                    f\"Tried to define rule with unexpected \"\n                    f\"name format: {rule_name}. Rule names should be lowercase \"\n                    \"and snake_case with optional `.` characters to indicate \"\n                    \"a namespace or grouping. e.g. `layout.spacing`.\"\n                )\n\n        # Use the stock __new__ method now we've adjusted the docstring.\n        # There are no overload variants of type.__new__ that are compatible, so\n        # we ignore type checking in this case.\n        return super().__new__(mcs, name, bases, class_dict)  # type: ignore\n\n\nclass BaseRule(metaclass=RuleMetaclass):\n  "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`LintFix`, optional): An array of any\n            fixes which would correct this issue. If not present then it's\n            assumed that this issue will have to manually fixed.\n        memory (:obj:`dict`, optional): An object which stores any working\n            memory for the rule. The `memory` returned in any `LintResult`\n            will be passed as an input to the next segment to be crawled.\n        description (:obj:`str`, optional): A description of the problem\n            identified as part of this result. This will override the\n            description of the rule as what gets reported to the user\n            with the problem if provided.\n        source (:obj:`str`, optional): A string identifier for what\n            generated the result. Within larger libraries like reflow this\n            can be useful for tracking where a result came from.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        anchor: Optional[BaseSegment] = None,\n        fixes: Optional[list[\"LintFix\"]] = None,\n  "}, {"start_line": 15000, "end_line": 17000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  \"\"\"The base class for a rule.\n\n    Args:\n        code (:obj:`str`): The identifier for this rule, used in inclusion\n            or exclusion.\n        description (:obj:`str`): A human readable description of what this\n            rule does. It will be displayed when any violations are found.\n\n    \"\"\"\n\n    _check_docstring = True\n    _works_on_unparsable = True\n    _adjust_anchors = False\n    targets_templated = False\n    # Some fix routines do their own checking for whether their fixes\n    # are safe around templated elements. For those - the default\n    # safety checks might be inappropriate. In those cases, set\n    # template_safe_fixes to True.\n    template_safe_fixes = False\n\n    # Config settings supported for this rule.\n    # See config_info.py for supported values.\n    config_keywords: list[str] = []\n    # Lint loop / crawl behavior. When appropriate, rules can (and should)\n    # override these values to make linting faster.\n    crawl_behaviour: BaseCrawler\n    # Rules can override this to specify \"post\". \"Post\" rules are those that are\n    # not expected to trigger any downstream rules, e.g. capitalization fixes.\n    # They run on two occasions:\n    # - On the first pass of the main phase\n    # - In a second linter pass after the main phase\n    lint_phase = \"main\"\n    # Groups attribute to be overwritten.\n    groups: tuple[str, ...] = ()\n    # Name attribute to be overwritten.\n    # NOTE: for backward compatibility we should handle the case\n    # where no name is set gracefully.\n    name: str = \"\"\n    # Optional set of aliases for the rule. Most often used for old codes which\n    # referred to this rule.\n    aliases: tuple[str, ...] = ()\n\n    # NOTE: code and description are provided here as hints, but should not\n    # be set directly. They are set automatically by the metaclass based on\n    # the class _name_ when defined.\n    code: str\n    description: str\n\n    # Should we document this rule as fixable? Used by the metaclass to add\n    # a line to the do"}, {"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rride this to specify \"post\". \"Post\" rules are those that are\n    # not expected to trigger any downstream rules, e.g. capitalization fixes.\n    # They run on two occasions:\n    # - On the first pass of the main phase\n    # - In a second linter pass after the main phase\n    lint_phase = \"main\"\n    # Groups attribute to be overwritten.\n    groups: tuple[str, ...] = ()\n    # Name attribute to be overwritten.\n    # NOTE: for backward compatibility we should handle the case\n    # where no name is set gracefully.\n    name: str = \"\"\n    # Optional set of aliases for the rule. Most often used for old codes which\n    # referred to this rule.\n    aliases: tuple[str, ...] = ()\n\n    # NOTE: code and description are provided here as hints, but should not\n    # be set directly. They are set automatically by the metaclass based on\n    # the class _name_ when defined.\n    code: str\n    description: str\n\n    # Should we document this rule as fixable? Used by the metaclass to add\n    # a line to the docstring.\n    is_fix_compatible = False\n\n    # Add comma separated string to Base Rule to ensure that it uses the same\n    # Configuration that is defined in the Config.py file\n    split_comma_separated_string = staticmethod(split_comma_separated_string)\n\n    def __init__(self, code: str, description: str, **kwargs: Any) -> None:\n        self.description = description\n        self.code = code\n        # kwargs represents the config passed to the rule. Add all kwargs as class\n        # attributes so they can be accessed in rules which inherit from this class\n        for key, value in kwargs.items():\n            self.__dict__[key] = value\n\n        # We also define a custom logger here, which also includes the code\n        # of the rule in the logging.\n        self.logger = RuleLoggingAdapter(rules_logger, {\"code\": code})\n        # Validate that declared configuration options exist\n        for keyword in self.config_keywords:\n            if keyword not in kwargs.keys():\n                rais"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      return f\"LintResult({self.description}: {self.anchor}{fix_coda})\"\n        return f\"LintResult({self.anchor}{fix_coda})\"\n\n    def to_linting_error(self, rule: \"BaseRule\") -> Optional[SQLLintError]:\n        \"\"\"Convert a linting result to a :exc:`SQLLintError` if appropriate.\"\"\"\n        if self.anchor:\n            # Allow description override from the LintResult\n            description = self.description or rule.description\n            return SQLLintError(\n                rule=rule,\n                segment=self.anchor,\n                fixes=self.fixes,\n                description=description,\n            )\n\n        return None\n\n\nEvalResultType = Union[LintResult, list[LintResult], None]\n\n\nclass RuleMetaclass(type):\n    \"\"\"The metaclass for rules.\n\n    This metaclass provides provides auto-enrichment of the\n    rule docstring so that examples, groups, aliases and\n    names are added.\n\n    The reason we enrich the docstring is so that it can be\n    picked up by autodoc and all be displayed in the sqlfluff\n    docs.\n    \"\"\"\n\n    # Precompile the regular expressions\n    _doc_search_regex: ClassVar = re.compile(\n        \"(\\\\s{4}\\\\*\\\\*Anti-pattern\\\\*\\\\*|\\\\s{4}\\\\.\\\\. note::|\"\n        \"\\\\s\\\\s{4}\\\\*\\\\*Configuration\\\\*\\\\*)\",\n        flags=re.MULTILINE,\n    )\n    _valid_classname_regex: ClassVar = regex.compile(\n        r\"Rule_?([A-Z]{1}[a-zA-Z]+)?_([A-Z0-9]{4})\"\n    )\n    _valid_rule_name_regex: ClassVar = regex.compile(r\"[a-z][a-z\\.\\_]+\")\n\n    @staticmethod\n    def _populate_code_and_description(\n        name: str, class_dict: dict[str, Any]\n    ) -> dict[str, Any]:\n        \"\"\"Extract and validate the rule code & description.\n\n        We expect that rules are defined as classes with the name `Rule_XXXX`\n        where `XXXX` is of the form `LLNN`, where L is a letter and N is a\n        two digit number. For backward compatibility we also still support\n        the legacy format of LNNN i.e. a single letter and three digit number.\n\n        The two letters should be indicativ"}, {"start_line": 0, "end_line": 1740, "belongs_to": {"file_name": "context.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Define RuleContext class.\"\"\"\n\nimport pathlib\nfrom dataclasses import dataclass, field\nfrom typing import Any, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.dialects import Dialect\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n\n@dataclass\nclass RuleContext:\n    \"\"\"Class for holding the context passed to rule eval functions.\"\"\"\n\n    # These don't change within a file.\n    dialect: Dialect\n    fix: bool\n    templated_file: Optional[TemplatedFile]\n    path: Optional[pathlib.Path]\n    config: FluffConfig\n\n    # These change within a file.\n    # segment: The segment in question\n    segment: BaseSegment\n    # parent_stack: A tuple of the path from the root to this segment.\n    parent_stack: tuple[BaseSegment, ...] = field(default=tuple())\n    # raw_stack: All of the raw segments so far in the file\n    raw_stack: tuple[RawSegment, ...] = field(default=tuple())\n    # memory: Arbitrary storage for the rule\n    memory: Any = field(default_factory=dict)\n    # segment_idx: The index of this segment in the parent\n    segment_idx: int = field(default=0)\n\n    @property\n    def siblings_pre(self) -> tuple[BaseSegment, ...]:  # pragma: no cover\n        \"\"\"Return sibling segments prior to self.segment.\"\"\"\n        if self.parent_stack:\n            return self.parent_stack[-1].segments[: self.segment_idx]\n        else:\n            return tuple()\n\n    @property\n    def siblings_post(self) -> tuple[BaseSegment, ...]:\n        \"\"\"Return sibling segments after self.segment.\"\"\"\n        if self.parent_stack:\n            return self.parent_stack[-1].segments[self.segment_idx + 1 :]\n        else:\n            return tuple()  # pragma: no cover\n"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "layed in the sqlfluff\n    docs.\n    \"\"\"\n\n    # Precompile the regular expressions\n    _doc_search_regex: ClassVar = re.compile(\n        \"(\\\\s{4}\\\\*\\\\*Anti-pattern\\\\*\\\\*|\\\\s{4}\\\\.\\\\. note::|\"\n        \"\\\\s\\\\s{4}\\\\*\\\\*Configuration\\\\*\\\\*)\",\n        flags=re.MULTILINE,\n    )\n    _valid_classname_regex: ClassVar = regex.compile(\n        r\"Rule_?([A-Z]{1}[a-zA-Z]+)?_([A-Z0-9]{4})\"\n    )\n    _valid_rule_name_regex: ClassVar = regex.compile(r\"[a-z][a-z\\.\\_]+\")\n\n    @staticmethod\n    def _populate_code_and_description(\n        name: str, class_dict: dict[str, Any]\n    ) -> dict[str, Any]:\n        \"\"\"Extract and validate the rule code & description.\n\n        We expect that rules are defined as classes with the name `Rule_XXXX`\n        where `XXXX` is of the form `LLNN`, where L is a letter and N is a\n        two digit number. For backward compatibility we also still support\n        the legacy format of LNNN i.e. a single letter and three digit number.\n\n        The two letters should be indicative of the grouping and focus of\n        the rule. e.g. capitalisation rules have the code CP for CaPitalisation.\n\n        If this receives classes by any other name, then it will raise a\n        :exc:`ValueError`.\n        \"\"\"\n        rule_name_match = RuleMetaclass._valid_classname_regex.match(name)\n        # Validate the name\n        if not rule_name_match:  # pragma: no cover\n            raise SQLFluffUserError(\n                f\"Tried to define rule class with \"\n                f\"unexpected format: {name}. Format should be: \"\n                \"'Rule_PluginName_LL23' (for plugins) or \"\n                \"`Rule_LL23` (for core rules).\"\n            )\n\n        plugin_name, code = rule_name_match.groups()\n        # If the docstring is multiline, then we extract just summary.\n        description = class_dict[\"__doc__\"].replace(\"``\", \"'\").split(\"\\n\")[0]\n        if plugin_name:\n            code = f\"{plugin_name}_{code}\"\n\n        class_dict[\"code\"] = code\n        class_dict[\"description\"] = desc"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`Lin"}], "retrieved_count": 10, "cost_time": 1.0617756843566895}
{"question": "What is the structure of SQLFluff's configuration system?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's configuration system is built around the FluffConfig class and supports multiple configuration sources with a hierarchical nesting structure. The system includes: 1) Configuration files - supports multiple file formats including setup.cfg, tox.ini, pep8.ini, .sqlfluff, and pyproject.toml, with later files overriding earlier ones; 2) Hierarchical nesting - configuration is loaded from multiple locations in order: default config, user's app config directory (~/.config/sqlfluff), home directory (~), directories between home and working directory, current working directory, subdirectories between working directory and file directory, and the file's containing directory; 3) FluffConfig class - the main configuration object that combines defaults, user configs, and overrides using nested_combine(), validates configuration, and manages long-lived objects like dialects and templaters; 4) Configuration sections - uses colon-delimited sections (e.g., [sqlfluff:rules:capitalisation.keywords]) in cfg files and dot-delimited sections (e.g., [tool.sqlfluff.rules.capitalisation.keywords]) in pyproject.toml; 5) Override system - supports command-line overrides that take precedence over file-based configuration; 6) Plugin integration - uses a plugin manager to load default configurations from various plugins; 7) Special handling - includes logic for comma-separated values, dialect initialization, and templater object creation.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fluffconfig.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Module for loading config.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom collections.abc import Iterable\nfrom copy import copy, deepcopy\nfrom itertools import chain\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nimport pluggy\n\nfrom sqlfluff.core.config.ini import coerce_value\nfrom sqlfluff.core.config.loader import load_config_string, load_config_up_to_path\nfrom sqlfluff.core.config.validate import validate_config_dict\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.helpers.dict import (\n    dict_diff,\n    iter_records_from_nested_dict,\n    nested_combine,\n    records_to_nested_dict,\n)\nfrom sqlfluff.core.helpers.string import (\n    split_colon_separated_string,\n    split_comma_separated_string,\n)\nfrom sqlfluff.core.plugin.host import get_plugin_manager\nfrom sqlfluff.core.types import ConfigMappingType, ConfigValueOrListType\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.templaters.base import RawTemplater\n\n# Instantiate the config logger\nconfig_logger = logging.getLogger(\"sqlfluff.config\")\n\n\nclass FluffConfig:\n    \"\"\"The persistent object for internal methods to access configuration.\n\n    This class is designed to be instantiated once for each file and then be\n    reused by each part of the process. For multiple files in the same path, a\n    parent object will be created for the each path and then variants of it\n    are created *for each file*. The object itself contains the references\n    to any long lived objects which might be used by multiple parts of the\n    codebase such as the dialect and the templater (both of which can be\n    resource intensive to load & instantiate), which allows (for example),\n    multiple files to reuse the same instance of the relevant dialect.\n\n    It is also designed to pickle well for use in parallel operations.\n\n    Args:\n        configs (ConfigMappingType, optional): A nested dict of config\n            values from which to construct the config.\n        extra_config_path"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fluffconfig_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the configuration routines.\"\"\"\n\nimport logging\nimport os\n\nimport pytest\n\nimport sqlfluff\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.templaters import (\n    JinjaTemplater,\n    PlaceholderTemplater,\n    PythonTemplater,\n    RawTemplater,\n)\nfrom sqlfluff.utils.testing.logging import fluff_log_catcher\n\nconfig_b = {\n    \"core\": {\"rules\": \"LT03\", \"dialect\": \"ansi\"},\n    \"layout\": {\n        \"type\": {\"comma\": {\"line_position\": \"trailing\", \"spacing_before\": \"touch\"}}\n    },\n}\n\nconfig_c = {\n    \"core\": {\"rules\": \"LT03\", \"dialect\": \"ansi\"},\n    # NOTE:\n    # - NOT_A_RULE doesn't match anything.\n    # - L001 is an alias, but no longer a rule.\n    # - layout is a group and but doesn't match any individual rule.\n    \"rules\": {\n        \"NOT_A_RULE\": {\"foo\": \"bar\"},\n        \"L001\": {\"foo\": \"bar\"},\n        \"layout\": {\"foo\": \"bar\"},\n    },\n}\n\n\ndef test__config__from_strings():\n    \"\"\"Test loading config from multiple strings.\"\"\"\n    strings = [\n        \"[sqlfluff]\\ndialect=mysql\\ntesting_val=foobar\",\n        \"[sqlfluff]\\ndialect=postgres\\ntesting_val2=bar\",\n        \"[sqlfluff]\\ndialect=mysql\\ntesting_val=foo\",\n    ]\n    cfg = FluffConfig.from_strings(*strings)\n    assert cfg.get(\"dialect\") == \"mysql\"\n    assert cfg.get(\"testing_val2\") == \"bar\"\n    assert cfg.get(\"testing_val\") == \"foo\"\n\n\ndef test__config__nested_config_tests():\n    \"\"\"Test linting with overridden config in nested paths.\n\n    This looks like a linter test but it's actually a config\n    test.\n    \"\"\"\n    lntr = Linter(\n        # Exclude CP02 in overrides (similar to cli --exclude-rules)\n        config=FluffConfig(overrides=dict(exclude_rules=\"CP02\", dialect=\"ansi\"))\n    )\n    lnt = lntr.lint_path(\"test/fixtures/config/inheritance_b\")\n    violations = lnt.check_tuples_by_path()\n    for k in violations:\n        if k.endswith(\"nested\\\\example.sql\"):\n            # CP01 is enabled in the .sqlfluff file and not excluded.\n            assert"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "fluffconfig.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " config logger\nconfig_logger = logging.getLogger(\"sqlfluff.config\")\n\n\nclass FluffConfig:\n    \"\"\"The persistent object for internal methods to access configuration.\n\n    This class is designed to be instantiated once for each file and then be\n    reused by each part of the process. For multiple files in the same path, a\n    parent object will be created for the each path and then variants of it\n    are created *for each file*. The object itself contains the references\n    to any long lived objects which might be used by multiple parts of the\n    codebase such as the dialect and the templater (both of which can be\n    resource intensive to load & instantiate), which allows (for example),\n    multiple files to reuse the same instance of the relevant dialect.\n\n    It is also designed to pickle well for use in parallel operations.\n\n    Args:\n        configs (ConfigMappingType, optional): A nested dict of config\n            values from which to construct the config.\n        extra_config_path (str, optional): An optional additional path\n            to load config files from. These are loaded last if found\n            and take precedence over any pre-existing config values.\n            Note that when provided directly to the class, this path\n            is not loaded for the class in question (it's assumed that\n            has already been done, and the results are incorporated in\n            the `configs` argument), but it *is* passed onward to child\n            config instances, which will use it.\n        ignore_local_config (bool, optional, defaults to False): If set to\n            True, this skips loading configuration from the user home\n            directory (``~``) or ``appdir`` path.\n        overrides (ConfigMappingType, optional): A additional set of\n            configs to merge into the ``core`` section of the config\n            object at the end. These values take precedence over all\n            other provided values and are inherited by child configs.\n           "}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "fluffconfig.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "= \"rule_denylist\", \"rule_allowlist\", \"dialect_obj\", \"templater_obj\"\n\n    def __init__(\n        self,\n        configs: Optional[ConfigMappingType] = None,\n        extra_config_path: Optional[str] = None,\n        ignore_local_config: bool = False,\n        overrides: Optional[ConfigMappingType] = None,\n        plugin_manager: Optional[pluggy.PluginManager] = None,\n        # Ideally a dialect should be set when config is read but sometimes\n        # it might only be set in nested .sqlfluff config files, so allow it\n        # to be not required.\n        require_dialect: bool = True,\n    ) -> None:\n        self._extra_config_path = (\n            extra_config_path  # We only store this for child configs\n        )\n        self._ignore_local_config = (\n            ignore_local_config  # We only store this for child configs\n        )\n        # If overrides are provided, validate them early.\n        if overrides:\n            overrides = {\"core\": overrides}\n            validate_config_dict(overrides, \"<provided overrides>\")\n        # Stash overrides so we can pass them to child configs\n        core_overrides = overrides[\"core\"] if overrides else None\n        assert isinstance(core_overrides, dict) or core_overrides is None\n        self._overrides = core_overrides\n\n        # Fetch a fresh plugin manager if we weren't provided with one\n        self._plugin_manager = plugin_manager or get_plugin_manager()\n\n        defaults = nested_combine(*self._plugin_manager.hook.load_default_config())\n        # If any existing configs are provided. Validate them:\n        if configs:\n            validate_config_dict(configs, \"<provided configs>\")\n        self._configs = nested_combine(\n            defaults, configs or {\"core\": {}}, overrides or {}\n        )\n        # Some configs require special treatment\n        self._configs[\"core\"][\"color\"] = (\n            False if self._configs[\"core\"].get(\"nocolor\", False) else None\n        )\n        # Handle inputs which are potentially comma separated st"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Init file for the config module.\n\nThis holds all the methods and classes for configuration.\n\"\"\"\n\nfrom typing import Optional\n\nfrom sqlfluff.core.config.file import (\n    load_config_file_as_dict,\n    load_config_string_as_dict,\n)\nfrom sqlfluff.core.config.fluffconfig import FluffConfig\nfrom sqlfluff.core.config.loader import (\n    ConfigLoader,\n    load_config_at_path,\n    load_config_file,\n    load_config_resource,\n    load_config_string,\n    load_config_up_to_path,\n)\n\n__all__ = (\n    \"FluffConfig\",\n    \"ConfigLoader\",\n    \"load_config_file\",\n    \"load_config_resource\",\n    \"load_config_string\",\n    \"load_config_at_path\",\n    \"load_config_up_to_path\",\n    \"progress_bar_configuration\",\n    \"clear_config_caches\",\n)\n\n\ndef clear_config_caches() -> None:\n    \"\"\"Clear any of the cached config methods.\n\n    This is primarily used during testing where the cache may be be rendered unreliable\n    by using moving around files while setting up tests. Some of the cached methods\n    rely on *filename* caching, and so we may break one of the assumptions of the\n    caching routines (that files aren't modified while SQLFluff is running) during\n    the test suite. That means we need to clear the cache during those times to\n    get reliable results.\n\n    NOTE: You may not notice those results when running tests individually locally\n    as they may only be visible when running the whole test suite.\n    \"\"\"\n    load_config_file_as_dict.cache_clear()\n    load_config_at_path.cache_clear()\n    load_config_string_as_dict.cache_clear()\n    pass\n\n\nclass ProgressBarConfiguration:\n    \"\"\"Singleton-esque progress bar configuration.\n\n    It's expected to be set during starting with parameters coming from commands\n    parameters, then to be just utilized as just\n    ```\n    from sqlfluff.core.config import progress_bar_configuration\n    is_progressbar_disabled = progress_bar_configuration.disable_progress_bar\n    ```\n    \"\"\"\n\n    _disable_progress_bar: Optional[bool] = True\n\n    @property\n    "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "loader.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Config loading methods and helpers.\n\nThis is designed to house the main functions which are exposed by the\noverall config module. There is some caching in this module, which\nis designed around caching the configuration loaded at *specific paths*\nrather than the individual file caching in the `file` module.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport os\nimport os.path\nimport sys\nfrom functools import cache\nfrom importlib.resources import files\nfrom pathlib import Path\nfrom typing import (\n    Optional,\n)\n\nimport platformdirs\nimport platformdirs.macos\nimport platformdirs.unix\n\nfrom sqlfluff.core.config.file import (\n    load_config_file_as_dict,\n    load_config_string_as_dict,\n)\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.helpers.dict import nested_combine\nfrom sqlfluff.core.helpers.file import iter_intermediate_paths\nfrom sqlfluff.core.types import ConfigMappingType\n\n# Instantiate the config logger\nconfig_logger = logging.getLogger(\"sqlfluff.config\")\n\nglobal_loader = None\n\"\"\":obj:`ConfigLoader`: A variable to hold the single module loader when loaded.\n\nWe define a global loader, so that between calls to load config, we\ncan still cache appropriately\n\"\"\"\n\n\ndef _get_user_config_dir_path(sys_platform: str) -> str:\n    \"\"\"Get the user config dir for this system.\n\n    Args:\n        sys_platform (str): The result of ``sys.platform()``. Provided\n            as an argument here for ease of testing. In normal usage\n            it should only be  called with ``sys.platform()``. This\n            argument only applies to switching between linux and macos.\n            Win32 detection still uses the underlying ``sys.platform()``\n            methods.\n    \"\"\"\n    appname = \"sqlfluff\"\n    appauthor = \"sqlfluff\"\n\n    # First try the default SQLFluff specific cross-platform config path.\n    cross_platform_path = os.path.expanduser(\"~/.config/sqlfluff\")\n    if os.path.exists(cross_platform_path):\n        return cross_platform_path\n\n    # Th"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "fluffconfig.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " For example, override values provided in the CLI use this\n            method to apply to all files in a linting operation. Note\n            that this mapping dict *only* applies to the ``core``\n            section and so cannot be used for all values.\n        plugin_manager (PluginManager, optional): Optional pre-loaded\n            config manager. Generally users should not need to provide\n            this, as the class will fetch it's own if not provided.\n            This argument is used when creating new class instances to\n            avoid reloading the manager.\n\n    .. note::\n       Methods for accessing internal properties on the config are not particularly\n       standardised as the project currently assumes that few other tools are using\n       this interface directly. If you or your project would like more formally\n       supported methods for access to the config object, raise an issue on GitHub\n       with the kind of things you'd like to achieve.\n    \"\"\"\n\n    private_vals = \"rule_denylist\", \"rule_allowlist\", \"dialect_obj\", \"templater_obj\"\n\n    def __init__(\n        self,\n        configs: Optional[ConfigMappingType] = None,\n        extra_config_path: Optional[str] = None,\n        ignore_local_config: bool = False,\n        overrides: Optional[ConfigMappingType] = None,\n        plugin_manager: Optional[pluggy.PluginManager] = None,\n        # Ideally a dialect should be set when config is read but sometimes\n        # it might only be set in nested .sqlfluff config files, so allow it\n        # to be not required.\n        require_dialect: bool = True,\n    ) -> None:\n        self._extra_config_path = (\n            extra_config_path  # We only store this for child configs\n        )\n        self._ignore_local_config = (\n            ignore_local_config  # We only store this for child configs\n        )\n        # If overrides are provided, validate them early.\n        if overrides:\n            overrides = {\"core\": overrides}\n            validate_config_dict(overrid"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "loader_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the configuration routines.\"\"\"\n\nimport os\nimport sys\nfrom contextlib import contextmanager\nfrom unittest.mock import call, patch\n\nimport pytest\n\nfrom sqlfluff.core import FluffConfig\nfrom sqlfluff.core.config import (\n    load_config_at_path,\n    load_config_file,\n    load_config_string,\n    load_config_up_to_path,\n)\nfrom sqlfluff.core.config.loader import (\n    _get_user_config_dir_path,\n    _load_user_appdir_config,\n)\nfrom sqlfluff.core.errors import SQLFluffUserError\n\nconfig_a = {\n    \"core\": {\"testing_val\": \"foobar\", \"testing_int\": 4, \"dialect\": \"mysql\"},\n    \"bar\": {\"foo\": \"barbar\"},\n}\n\n\n@pytest.fixture\ndef mock_xdg_home(monkeypatch):\n    \"\"\"Sets the XDG_CONFIG_HOME variable.\"\"\"\n    monkeypatch.setenv(\"XDG_CONFIG_HOME\", \"~/.config/my/special/path\")\n\n\ndef test__config__load_file_dir():\n    \"\"\"Test loading config from a directory path.\"\"\"\n    cfg = load_config_at_path(\n        os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\")\n    )\n    assert cfg == config_a\n\n\ndef test__config__load_from_string():\n    \"\"\"Test loading config from a string.\"\"\"\n    # Load a string\n    with open(\n        os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\", \".sqlfluff\")\n    ) as f:\n        config_string = f.read()\n    cfg = load_config_string(config_string)\n    assert cfg == config_a\n\n\ndef test__config__load_file_f():\n    \"\"\"Test loading config from a file path.\"\"\"\n    cfg = load_config_at_path(\n        os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\", \"testing.sql\")\n    )\n    assert cfg == config_a\n\n\ndef test__config__load_file_missing_extra():\n    \"\"\"Test loading config from a file path if extra path is not found.\"\"\"\n    with pytest.raises(SQLFluffUserError):\n        load_config_up_to_path(\n            os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\", \"testing.sql\"),\n            extra_config_path=\"non/existent/path\",\n        )\n\n\ndef test__config__load_nested():\n    \"\"\"Test nested overwrite and order of precedence of config files.\"\"\"\n"}, {"start_line": 0, "end_line": 1349, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Configuration and examples for individual rules.\"\"\"\n\nfrom sqlfluff.core.plugin.host import get_plugin_manager\nfrom sqlfluff.core.rules.base import (\n    BaseRule,\n    EvalResultType,\n    LintResult,\n    RuleGhost,\n    RulePack,\n    RuleSet,\n)\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.fix import LintFix\n\n\ndef _load_standard_rules() -> RuleSet:\n    \"\"\"Initialise the standard ruleset.\n\n    We do this on each call so that dynamic rules changes\n    are possible.\n    \"\"\"\n    std_rule_set = RuleSet(name=\"standard\", config_info=get_config_info())\n\n    # Iterate through the rules list and register each rule with the standard set.\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            std_rule_set.register(rule)\n\n    return std_rule_set\n\n\ndef get_ruleset(name: str = \"standard\") -> RuleSet:\n    \"\"\"Get a ruleset by name.\"\"\"\n    std_rules = _load_standard_rules()\n    lookup = {std_rules.name: std_rules}\n    # Return a copy in case someone modifies the register.\n    return lookup[name].copy()\n\n\n__all__ = (\n    \"get_ruleset\",\n    \"RuleSet\",\n    \"RulePack\",\n    \"BaseRule\",\n    \"LintResult\",\n    \"LintFix\",\n    \"RuleContext\",\n    \"RuleGhost\",\n    \"EvalResultType\",\n    \"ConfigInfo\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "toml.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Methods for loading config from pyproject.toml files.\"\"\"\n\nimport sys\nfrom typing import Any, TypeVar\n\nif sys.version_info >= (3, 11):\n    import tomllib\nelse:  # pragma: no cover\n    import tomli as tomllib\n\nfrom sqlfluff.core.helpers.dict import (\n    NestedDictRecord,\n    iter_records_from_nested_dict,\n    records_to_nested_dict,\n)\nfrom sqlfluff.core.types import ConfigMappingType\n\nT = TypeVar(\"T\")\n\n\ndef _condense_rule_record(record: NestedDictRecord[T]) -> NestedDictRecord[T]:\n    \"\"\"Helper function to condense the rule section of a toml config.\"\"\"\n    key, value = record\n    if len(key) > 2:\n        key = (\".\".join(key[:-1]), key[-1])\n    return key, value\n\n\ndef _validate_structure(raw_config: dict[str, Any]) -> ConfigMappingType:\n    \"\"\"Helper function to narrow types for use by SQLFluff.\n\n    This is a recursive function on any dict keys found.\n    \"\"\"\n    validated_config: ConfigMappingType = {}\n    for key, value in raw_config.items():\n        if isinstance(value, dict):\n            validated_config[key] = _validate_structure(value)\n        elif isinstance(value, list):\n            # Coerce all list items to strings, to be in line\n            # with the behaviour of ini configs.\n            validated_config[key] = [str(item) for item in value]\n        elif isinstance(value, (str, int, float, bool)) or value is None:\n            validated_config[key] = value\n        else:  # pragma: no cover\n            # Whatever we found, make it into a string.\n            # This is very unlikely to happen and is more for completeness.\n            validated_config[key] = str(value)\n    return validated_config\n\n\ndef load_toml_file_config(filepath: str) -> ConfigMappingType:\n    \"\"\"Read the SQLFluff config section of a pyproject.toml file.\n\n    We don't need to change any key names here, because the root\n    section of the toml file format is `tool.sqlfluff.core`.\n\n    NOTE: Toml files are always encoded in UTF-8. That is a necessary\n    part of the toml spec: https://toml"}], "retrieved_count": 10, "cost_time": 1.0770995616912842}
{"question": "What dependencies exist between SQLFluff's BaseSegment and RawSegment classes?", "answer": null, "relative_code_list": null, "ground_truth": "BaseSegment and RawSegment have a hierarchical dependency relationship where RawSegment depends on BaseSegment through inheritance. Key dependencies include: 1) Inheritance dependency - RawSegment inherits from BaseSegment (class RawSegment(BaseSegment)), making BaseSegment a required dependency for RawSegment; 2) Type system dependency - RawSegment depends on BaseSegment's type system, including the _class_types attribute and class_types property; 3) Position marker dependency - RawSegment uses BaseSegment's pos_marker attribute for location tracking; 4) Segment structure dependency - RawSegment depends on BaseSegment's segments attribute (though RawSegment sets it to empty tuple); 5) Method inheritance - RawSegment inherits and can override BaseSegment methods like raw, raw_upper, and other utility methods; 6) Metaclass dependency - RawSegment uses BaseSegment's SegmentMetaclass for class creation and type validation; 7) Tree traversal dependency - RawSegment participates in BaseSegment's tree traversal methods and parent-child relationships; 8) Serialization dependency - RawSegment uses BaseSegment's serialization methods for JSON and string representation; 9) UUID system dependency - RawSegment uses BaseSegment's UUID system for unique identification; 10) Cache key dependency - RawSegment depends on BaseSegment's cache key system for performance optimization. The dependency is unidirectional - BaseSegment does not depend on RawSegment, but RawSegment cannot exist without BaseSegment.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 966, "belongs_to": {"file_name": "segments_raw_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test the RawSegment class.\"\"\"\n\nfrom sqlfluff.core.parser.segments.base import PathStep\n\n\ndef test__parser__raw_get_raw_segments(raw_segments):\n    \"\"\"Test niche case of calling get_raw_segments on a raw segment.\"\"\"\n    for s in raw_segments:\n        assert s.get_raw_segments() == [s]\n\n\ndef test__parser__raw_segments_with_ancestors(\n    raw_segments, DummySegment, DummyAuxSegment\n):\n    \"\"\"Test raw_segments_with_ancestors.\n\n    This is used in the reflow module to assess parse depth.\n    \"\"\"\n    test_seg = DummySegment([DummyAuxSegment(raw_segments[:1]), raw_segments[1]])\n    # Result should be the same raw segment, but with appropriate parents\n    assert test_seg.raw_segments_with_ancestors == [\n        (\n            raw_segments[0],\n            [\n                PathStep(test_seg, 0, 2, (0, 1)),\n                PathStep(test_seg.segments[0], 0, 1, (0,)),\n            ],\n        ),\n        (raw_segments[1], [PathStep(test_seg, 1, 2, (0, 1))]),\n    ]\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "raw.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Raw segment definitions.\n\nThis is designed to be the root segment, without\nany children, and the output of the lexer.\n\"\"\"\n\nfrom typing import Any, Callable, Optional, Union, cast\nfrom uuid import uuid4\n\nimport regex as re\n\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\n\n\nclass RawSegment(BaseSegment):\n    \"\"\"This is a segment without any subsegments.\"\"\"\n\n    type = \"raw\"\n    _is_code = True\n    _is_comment = False\n    _is_whitespace = False\n    # Classes inheriting from RawSegment may provide a _default_raw\n    # to enable simple initialisation.\n    _default_raw = \"\"\n\n    def __init__(\n        self,\n        raw: Optional[str] = None,\n        pos_marker: Optional[PositionMarker] = None,\n        # For legacy and syntactic sugar we allow the simple\n        # `type` argument here, but for more precise inheritance\n        # we suggest using the `instance_types` option.\n        type: Optional[str] = None,\n        instance_types: tuple[str, ...] = (),\n        trim_start: Optional[tuple[str, ...]] = None,\n        trim_chars: Optional[tuple[str, ...]] = None,\n        source_fixes: Optional[list[SourceFix]] = None,\n        uuid: Optional[int] = None,\n        quoted_value: Optional[tuple[str, Union[int, str]]] = None,\n        escape_replacements: Optional[list[tuple[str, str]]] = None,\n        casefold: Optional[Callable[[str], str]] = None,\n    ):\n        \"\"\"Initialise raw segment.\n\n        If raw is not provided, we default to _default_raw if present.\n        If pos_marker is not provided, it is assume that this will be\n        inserted later as part of a reposition phase.\n        \"\"\"\n        if raw is not None:  # NB, raw *can* be an empty string and be valid\n            self._raw = raw\n        else:\n            self._raw = self._default_raw\n        self._raw_upper = self._raw.upper()\n        # pos marker is required here. We ignore the typing initially\n        # because it might *initially* b"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "segments_base_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test the BaseSegment class.\"\"\"\n\nimport pickle\n\nimport pytest\n\nfrom sqlfluff.core.parser import BaseSegment, PositionMarker, RawSegment\nfrom sqlfluff.core.parser.segments.base import PathStep\nfrom sqlfluff.core.rules.base import LintFix\nfrom sqlfluff.core.templaters import TemplatedFile\n\n\ndef test__parser__base_segments_type(DummySegment):\n    \"\"\"Test the .is_type() method.\"\"\"\n    assert BaseSegment.class_is_type(\"base\")\n    assert not BaseSegment.class_is_type(\"foo\")\n    assert not BaseSegment.class_is_type(\"foo\", \"bar\")\n    assert DummySegment.class_is_type(\"dummy\")\n    assert DummySegment.class_is_type(\"base\")\n    assert DummySegment.class_is_type(\"base\", \"foo\", \"bar\")\n\n\ndef test__parser__base_segments_class_types(DummySegment):\n    \"\"\"Test the metaclass ._class_types attribute.\"\"\"\n    assert DummySegment._class_types == {\"dummy\", \"base\"}\n\n\ndef test__parser__base_segments_descendant_type_set(\n    raw_segments, DummySegment, DummyAuxSegment\n):\n    \"\"\"Test the .descendant_type_set() method.\"\"\"\n    test_seg = DummySegment([DummyAuxSegment(raw_segments)])\n    assert test_seg.descendant_type_set == {\"raw\", \"base\", \"dummy_aux\"}\n\n\ndef test__parser__base_segments_direct_descendant_type_set(\n    raw_segments, DummySegment, DummyAuxSegment\n):\n    \"\"\"Test the .direct_descendant_type_set() method.\"\"\"\n    test_seg = DummySegment([DummyAuxSegment(raw_segments)])\n    assert test_seg.direct_descendant_type_set == {\"base\", \"dummy_aux\"}\n\n\ndef test__parser__base_segments_to_tuple_a(raw_segments, DummySegment, DummyAuxSegment):\n    \"\"\"Test the .to_tuple() method.\"\"\"\n    test_seg = DummySegment([DummyAuxSegment(raw_segments)])\n    assert test_seg.to_tuple() == (\n        \"dummy\",\n        ((\"dummy_aux\", ((\"raw\", ()), (\"raw\", ()))),),\n    )\n\n\ndef test__parser__base_segments_to_tuple_b(raw_segments, DummySegment, DummyAuxSegment):\n    \"\"\"Test the .to_tuple() method.\"\"\"\n    test_seg = DummySegment(\n        [DummyAuxSegment(raw_segments + (DummyAuxSegment(raw_segments[:1]),))]\n    )\n    "}, {"start_line": 0, "end_line": 433, "belongs_to": {"file_name": "segments_file_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test the BaseFileSegment class.\"\"\"\n\nfrom sqlfluff.core.parser import BaseFileSegment\n\n\ndef test__parser__base_segments_file(raw_segments):\n    \"\"\"Test BaseFileSegment to behave as expected.\"\"\"\n    base_seg = BaseFileSegment(raw_segments, fname=\"/some/dir/file.sql\")\n    assert base_seg.type == \"file\"\n    assert base_seg.file_path == \"/some/dir/file.sql\"\n    assert base_seg.can_start_end_non_code\n    assert base_seg.allow_empty\n"}, {"start_line": 0, "end_line": 1715, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Definitions of the segment classes.\"\"\"\n\nfrom sqlfluff.core.parser.segments.base import (\n    BaseSegment,\n    SourceFix,\n    UnparsableSegment,\n)\nfrom sqlfluff.core.parser.segments.bracketed import BracketedSegment\nfrom sqlfluff.core.parser.segments.common import (\n    BinaryOperatorSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    IdentifierSegment,\n    LiteralSegment,\n    NewlineSegment,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.segments.file import BaseFileSegment\nfrom sqlfluff.core.parser.segments.generator import SegmentGenerator\nfrom sqlfluff.core.parser.segments.keyword import KeywordSegment, LiteralKeywordSegment\nfrom sqlfluff.core.parser.segments.meta import (\n    Dedent,\n    EndOfFile,\n    ImplicitIndent,\n    Indent,\n    MetaSegment,\n    TemplateLoop,\n    TemplateSegment,\n)\nfrom sqlfluff.core.parser.segments.raw import RawSegment\n\n__all__ = (\n    \"BaseSegment\",\n    \"BaseFileSegment\",\n    \"UnparsableSegment\",\n    \"BracketedSegment\",\n    \"SegmentGenerator\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"LiteralKeywordSegment\",\n    \"SymbolSegment\",\n    \"MetaSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"TemplateSegment\",\n    \"EndOfFile\",\n    \"TemplateLoop\",\n    \"SourceFix\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Base segment definitions.\n\nHere we define:\n- BaseSegment. This is the root class for all segments, and is\n  designed to hold other subsegments.\n- UnparsableSegment. A special wrapper to indicate that the parse\n  function failed on this block of segments and to prevent further\n  analysis.\n\"\"\"\n\n# Import annotations for py 3.7 to allow `weakref.Referencetype[\"BaseSegment\"]`\nfrom __future__ import annotations\n\nimport logging\nimport weakref\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom functools import cached_property\nfrom io import StringIO\nfrom itertools import chain\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    ClassVar,\n    Optional,\n    Union,\n    cast,\n)\nfrom uuid import uuid4\n\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import trim_non_code_segments\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.types import SimpleHintType\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.raw import RawSegment\n\n# Instantiate the linter logger (only for use in methods involved with fixing.)\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\nTupleSerialisedSegment = tuple[str, Union[str, tuple[\"TupleSerialisedSegment\", ...]]]\nRecordSerialisedSegment = dict[\n    str, Union[None, str, \"RecordSerialisedSegment\", list[\"RecordSerialisedSegment\"]]\n]\n\n\n@dataclass(frozen=True)\nclass SourceFix:\n    \"\"\"A stored reference to a fix in the non-templated file.\"\"\"\n\n    edit: str\n    source_slice: slice\n    # TODO: It might be possible to refactor this to not require\n    # a templated_slice (because in theory it's unnecessary).\n    # However much of the fix handling code assumes we need\n    # a position in the templated file to interpret it.\n    # More work required to ach"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "segments_base_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "gments[0]) == [\n        PathStep(test_seg_b, 0, 1, (0,)),\n        PathStep(test_seg_a, 0, 2, (0, 1)),\n    ]\n    assert test_seg_b.path_to(raw_segments[1]) == [\n        PathStep(test_seg_b, 0, 1, (0,)),\n        PathStep(test_seg_a, 1, 2, (0, 1)),\n    ]\n\n\ndef test__parser__base_segments_stubs():\n    \"\"\"Test stub methods that have no implementation in base class.\"\"\"\n    template = TemplatedFile.from_string(\"foobar\")\n    rs1 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n    base_segment = BaseSegment(segments=[rs1])\n\n    with pytest.raises(NotImplementedError):\n        base_segment.edit(\"foo\")\n\n\ndef test__parser__base_segments_raw(raw_seg):\n    \"\"\"Test raw segments behave as expected.\"\"\"\n    # Check Segment Return\n    assert raw_seg.segments == ()\n    assert raw_seg.raw == \"foobar\"\n    # Check Formatting and Stringification\n    assert str(raw_seg) == repr(raw_seg) == \"<CodeSegment: ([L:  1, P:  1]) 'foobar'>\"\n    assert (\n        raw_seg.stringify(ident=1, tabsize=2)\n        == \"[L:  1, P:  1]      |  raw:                                                \"\n        \"        'foobar'\\n\"\n    )\n    # Check tuple\n    assert raw_seg.to_tuple() == (\"raw\", ())\n    # Check tuple\n    assert raw_seg.to_tuple(show_raw=True) == (\"raw\", \"foobar\")\n\n\ndef test__parser__base_segments_base(raw_segments, fresh_ansi_dialect, DummySegment):\n    \"\"\"Test base segments behave as expected.\"\"\"\n    base_seg = DummySegment(raw_segments)\n    # Check we assume the position correctly\n    assert (\n        base_seg.pos_marker.start_point_marker()\n        == raw_segments[0].pos_marker.start_point_marker()\n    )\n    assert (\n        base_seg.pos_marker.end_point_marker()\n        == raw_segments[-1].pos_marker.end_point_marker()\n    )\n\n    # Check that we correctly reconstruct the raw\n    assert base_seg.raw == \"foobar.barfoo\"\n    # Check tuple\n    assert base_seg.to_tuple() == (\n        \"dummy\",\n        (raw_segments[0].to_tuple(), raw_segments[1].to_tuple()),\n    )\n    # Check "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "depthmap.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The DepthMap class is an enriched sequence of raw segments.\"\"\"\n\nimport logging\nfrom collections.abc import Sequence\nfrom dataclasses import dataclass\n\nfrom sqlfluff.core.parser import BaseSegment\nfrom sqlfluff.core.parser.segments.base import PathStep\nfrom sqlfluff.core.parser.segments.raw import RawSegment\n\nreflow_logger = logging.getLogger(\"sqlfluff.rules.reflow\")\n\n\n@dataclass(frozen=True)\nclass StackPosition:\n    \"\"\"An element of the stack_positions property of DepthInfo.\"\"\"\n\n    idx: int\n    len: int\n    type: str\n\n    @staticmethod\n    def _stack_pos_interpreter(path_step: PathStep) -> str:\n        \"\"\"Interpret a path step for stack_positions.\"\"\"\n        # If no code, then no.\n        if not path_step.code_idxs:\n            return \"\"\n        # If there's only one code element, this must be it.\n        elif len(path_step.code_idxs) == 1:\n            return \"solo\"\n        # Check for whether first or last code element.\n        # NOTE: code_idxs is always sorted because of how it's constructed.\n        # That means the lowest is always as the start and the highest at the end.\n        elif path_step.idx == path_step.code_idxs[0]:\n            return \"start\"\n        elif path_step.idx == path_step.code_idxs[-1]:\n            return \"end\"\n        else:\n            return \"\"  # NOTE: Empty string evaluates as falsy.\n\n    @classmethod\n    def from_path_step(cls, path_step: PathStep) -> \"StackPosition\":\n        \"\"\"Interpret a PathStep to construct a StackPosition.\n\n        The reason we don't just use the same object is partly\n        to interpret it a little more, but also to drop the reference\n        to a specific segment which could induce bugs at a later\n        stage if used.\n        \"\"\"\n        return cls(path_step.idx, path_step.len, cls._stack_pos_interpreter(path_step))\n\n\n@dataclass(frozen=True)\nclass DepthInfo:\n    \"\"\"An object to hold the depth information for a specific raw segment.\"\"\"\n\n    stack_depth: int\n    stack_hashes: tuple[int, ...]\n    # This is a"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "segments_base_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ize=2)\n        == \"[L:  1, P:  1]      |  raw:                                                \"\n        \"        'foobar'\\n\"\n    )\n    # Check tuple\n    assert raw_seg.to_tuple() == (\"raw\", ())\n    # Check tuple\n    assert raw_seg.to_tuple(show_raw=True) == (\"raw\", \"foobar\")\n\n\ndef test__parser__base_segments_base(raw_segments, fresh_ansi_dialect, DummySegment):\n    \"\"\"Test base segments behave as expected.\"\"\"\n    base_seg = DummySegment(raw_segments)\n    # Check we assume the position correctly\n    assert (\n        base_seg.pos_marker.start_point_marker()\n        == raw_segments[0].pos_marker.start_point_marker()\n    )\n    assert (\n        base_seg.pos_marker.end_point_marker()\n        == raw_segments[-1].pos_marker.end_point_marker()\n    )\n\n    # Check that we correctly reconstruct the raw\n    assert base_seg.raw == \"foobar.barfoo\"\n    # Check tuple\n    assert base_seg.to_tuple() == (\n        \"dummy\",\n        (raw_segments[0].to_tuple(), raw_segments[1].to_tuple()),\n    )\n    # Check Formatting and Stringification\n    assert str(base_seg) == repr(base_seg) == \"<DummySegment: ([L:  1, P:  1])>\"\n    assert base_seg.stringify(ident=1, tabsize=2) == (\n        \"[L:  1, P:  1]      |  dummy:\\n\"\n        \"[L:  1, P:  1]      |    raw:                                                 \"\n        \"     'foobar'\\n\"\n        \"[L:  1, P:  7]      |    raw:                                                 \"\n        \"     '.barfoo'\\n\"\n    )\n\n\ndef test__parser__base_segments_raw_compare():\n    \"\"\"Test comparison of raw segments.\"\"\"\n    template = TemplatedFile.from_string(\"foobar\")\n    rs1 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n    rs2 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n    assert rs1 == rs2\n\n\ndef test__parser__base_segments_base_compare(DummySegment, DummyAuxSegment):\n    \"\"\"Test comparison of base segments.\"\"\"\n    template = TemplatedFile.from_string(\"foobar\")\n    rs1 = RawSegment(\"foobar\", PositionMarker(sli"}, {"start_line": 1000, "end_line": 2225, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ",\n    LiteralSegment,\n    NewlineSegment,\n    RawSegment,\n    SegmentGenerator,\n    SourceFix,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.types import ParseMode\n\n__all__ = (\n    \"BaseSegment\",\n    \"SourceFix\",\n    \"BaseFileSegment\",\n    \"BracketedSegment\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"SymbolSegment\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"LiteralKeywordSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"SegmentGenerator\",\n    \"Sequence\",\n    \"OneOf\",\n    \"Delimited\",\n    \"Bracketed\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\",\n    \"Conditional\",\n    \"StringParser\",\n    \"MultiStringParser\",\n    \"TypedParser\",\n    \"RegexParser\",\n    \"PositionMarker\",\n    \"Lexer\",\n    \"StringLexer\",\n    \"RegexLexer\",\n    \"Parser\",\n    \"Matchable\",\n    \"ParseMode\",\n)\n"}], "retrieved_count": 10, "cost_time": 1.062525749206543}
{"question": "Why does SQLFluff's rule caching mechanism optimize repeated linting operations?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's rule caching mechanism optimizes repeated linting operations by storing and reusing rule evaluation results to avoid redundant computations. Key benefits include: 1) Result caching - Rule evaluation results are cached to avoid re-computing the same checks on identical code segments; 2) Grammar caching - Frequently used grammar patterns and parse tree structures are cached for faster rule matching; 3) Configuration caching - Rule configurations and settings are cached to avoid repeated validation and processing; 4) Segment caching - Parsed segments and their metadata are cached to avoid re-parsing identical structures; 5) Context caching - Rule evaluation context and state information are cached for reuse; 6) Memory efficiency - Caching reduces memory allocation and deallocation overhead; 7) CPU optimization - Avoids redundant CPU-intensive operations like regex matching and tree traversal; 8) Incremental updates - Only changed segments trigger rule re-evaluation, while cached results are reused for unchanged code; 9) Batch processing - Multiple files can benefit from shared cached results; 10) Performance scaling - Caching improves performance more significantly as the number of rules and file size increases.", "score": null, "retrieved_content": [{"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "t import LintingResult\nfrom sqlfluff.core.parser import Lexer, Parser\nfrom sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\nfrom sqlfluff.core.rules import BaseRule, RulePack, get_ruleset\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.rules.noqa import IgnoreMask\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.meta import MetaSegment\n    from sqlfluff.core.templaters import RawTemplater, TemplatedFile\n\n\nRuleTimingsType = list[tuple[str, str, float]]\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass Linter:\n    \"\"\"The interface class to interact with the linter.\"\"\"\n\n    # Default to allowing process parallelism\n    allow_process_parallelism = True\n\n    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n        dialect: Optional[str] = None,\n        rules: Optional[list[str]] = None,\n        user_rules: Optional[list[type[BaseRule]]] = None,\n        exclude_rules: Optional[list[str]] = None,\n    ) -> None:\n        if config and (dialect or rules or exclude_rules):\n            raise ValueError(  # pragma: no cover\n                \"Linter does not support setting both `config` and any of \"\n                \"`dialect`, `rules` or `exclude_rules`. The latter are \"\n                \"provided as convenience methods to avoid needing to \"\n                \"set the `config` object. If using `config`, please \"\n                \"provide all the other values within that object.\"\n            )\n        # Use the provided config or create one from the kwargs.\n        self.config = config or FluffConfig.from_kwargs(\n            dialect=dialect,\n            rules=rules,\n            exclude_rules=exclude_rules,\n            # Don't require a dialect to be provided yet. Defer this until we\n            # are actually linting something, since the dir"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`LintFix`, optional): An array of any\n            fixes which would correct this issue. If not present then it's\n            assumed that this issue will have to manually fixed.\n        memory (:obj:`dict`, optional): An object which stores any working\n            memory for the rule. The `memory` returned in any `LintResult`\n            will be passed as an input to the next segment to be crawled.\n        description (:obj:`str`, optional): A description of the problem\n            identified as part of this result. This will override the\n            description of the rule as what gets reported to the user\n            with the problem if provided.\n        source (:obj:`str`, optional): A string identifier for what\n            generated the result. Within larger libraries like reflow this\n            can be useful for tracking where a result came from.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        anchor: Optional[BaseSegment] = None,\n        fixes: Optional[list[\"LintFix\"]] = None,\n  "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "rules_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the standard set of rules.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import Linter\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.linter import RuleTuple\nfrom sqlfluff.core.parser import WhitespaceSegment\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.rules import BaseRule, LintFix, LintResult, get_ruleset\nfrom sqlfluff.core.rules.crawlers import RootOnlyCrawler, SegmentSeekerCrawler\nfrom sqlfluff.core.rules.doc_decorators import (\n    document_configuration,\n    document_fix_compatible,\n    document_groups,\n)\nfrom sqlfluff.core.rules.loader import get_rules_from_path\nfrom sqlfluff.core.templaters.base import TemplatedFile\nfrom sqlfluff.utils.testing.logging import fluff_log_catcher\nfrom sqlfluff.utils.testing.rules import get_rule_from_set\nfrom test.fixtures.rules.custom.L000 import Rule_L000\nfrom test.fixtures.rules.custom.S000 import Rule_S000\n\n\nclass Rule_T042(BaseRule):\n    \"\"\"A dummy rule.\"\"\"\n\n    groups = (\"all\",)\n\n    def _eval(self, context):\n        pass\n\n\nclass Rule_T001(BaseRule):\n    \"\"\"A deliberately malicious rule.\n\n    **Anti-pattern**\n\n    Blah blah\n    \"\"\"\n\n    groups = (\"all\",)\n    crawl_behaviour = SegmentSeekerCrawler({\"whitespace\"})\n    is_fix_compatible = True\n\n    def _eval(self, context):\n        \"\"\"Stars make newlines.\"\"\"\n        if context.segment.is_type(\"whitespace\"):\n            return LintResult(\n                anchor=context.segment,\n                fixes=[\n                    LintFix.replace(\n                        context.segment, [WhitespaceSegment(context.segment.raw + \" \")]\n                    )\n                ],\n            )\n\n\nclass Rule_T002(BaseRule):\n    \"\"\"A rule which says all raw code segments are bad.\n\n    This is used for testing unparsable code.\n    \"\"\"\n\n    groups = (\"all\",)\n    # Root only crawler so that the in-rule filters don't kick in.\n    crawl_behaviour = RootOnlyCrawler()\n\n    def _eval(s"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`Lin"}, {"start_line": 25000, "end_line": 27000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "                tree = new_tree\n                                previous_versions.add(loop_check_tuple)\n                                changed = True\n                                continue\n                            else:\n                                # Applying these fixes took us back to a state\n                                # which we've seen before. We're in a loop, so\n                                # we want to stop.\n                                cls._warn_unfixable(crawler.code)\n\n                    # Record rule timing\n                    rule_timings.append(\n                        (crawler.code, crawler.name, time.monotonic() - t0)\n                    )\n\n                if fix and not changed:\n                    # We did not change the file. Either the file is clean (no\n                    # fixes), or any fixes which are present will take us back\n                    # to a previous state.\n                    linter_logger.info(\n                        f\"Fix loop complete for {phase} phase. Stability \"\n                        f\"achieved after {loop}/{loop_limit} loops.\"\n                    )\n                    break\n            else:\n                if fix:\n                    # The linter loop hit the limit before reaching a stable point\n                    # (i.e. free of lint errors). If this happens, it's usually\n                    # because one or more rules produced fixes which did not address\n                    # the original issue **or** created new issues.\n                    linter_logger.warning(\n                        f\"Loop limit on fixes reached [{loop_limit}].\"\n                    )\n\n                    # Discard any fixes for the linting errors, since they caused a\n                    # loop. IMPORTANT: By doing this, we are telling SQLFluff that\n                    # these linting errors are \"unfixable\". This is important,\n                    # because when \"sqlfluff fix\" encounters unfixable lint errors,\n                   "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Implements the base rule class.\n\nRules crawl through the trees returned by the parser and evaluate particular\nrules.\n\nThe intent is that it should be possible for the rules to be expressed\nas simply as possible, with as much of the complexity abstracted away.\n\nThe evaluation function should take enough arguments that it can evaluate\nthe position of the given segment in relation to its neighbors, and that\nthe segment which finally \"triggers\" the error, should be the one that would\nbe corrected OR if the rule relates to something that is missing, then it\nshould flag on the segment FOLLOWING, the place that the desired element is\nmissing.\n\"\"\"\n\nimport bdb\nimport copy\nimport fnmatch\nimport logging\nimport pathlib\nimport re\nfrom collections import defaultdict, namedtuple\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    ClassVar,\n    DefaultDict,\n    Optional,\n    Union,\n)\n\nimport regex\n\nfrom sqlfluff.core.errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a "}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "RF05.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "          # Runtime configurations:\n                # https://spark.apache.org/docs/latest/configuration.html#application-properties\n                # Example configurations for table:\n                # https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#configuration\n                #\n                if context.parent_stack[-1].is_type(\"property_name_identifier\"):\n                    identifier = identifier.replace(\".\", \"\")\n\n            # Strip spaces if allowed (note a separate config as only valid for quoted\n            # identifiers)\n            if self.allow_space_in_identifier:\n                identifier = identifier.replace(\" \", \"\")\n\n        # We always allow underscores so strip them out\n        identifier = identifier.replace(\"_\", \"\")\n\n        # redshift allows a # at the beginning of temporary table names\n        if (\n            context.dialect.name == \"redshift\"\n            and identifier[0] == \"#\"\n            and context.parent_stack\n            and context.parent_stack[-1].is_type(\"table_reference\")\n        ):\n            identifier = identifier[1:]\n\n        # Set the identified minus the allowed characters\n        additional_allowed_characters = self._get_additional_allowed_characters(\n            context.dialect.name\n        )\n        if additional_allowed_characters:\n            identifier = identifier.translate(\n                str.maketrans(\"\", \"\", additional_allowed_characters)\n            )\n\n        # Finally test if the remaining identifier is only made up of alphanumerics\n        if identifiers_policy_applicable(policy, context.parent_stack) and not (\n            identifier.isalnum()\n        ):\n            return LintResult(anchor=context.segment)\n\n        return None\n\n    def _init_ignore_words_list(self) -> list[str]:\n        \"\"\"Called first time rule is evaluated to fetch & cache the policy.\"\"\"\n        ignore_words_config: str = str(getattr(self, \"ignore_words\"))\n        if ignore_words_config and ignore_words_config != \"None"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      return f\"LintResult({self.description}: {self.anchor}{fix_coda})\"\n        return f\"LintResult({self.anchor}{fix_coda})\"\n\n    def to_linting_error(self, rule: \"BaseRule\") -> Optional[SQLLintError]:\n        \"\"\"Convert a linting result to a :exc:`SQLLintError` if appropriate.\"\"\"\n        if self.anchor:\n            # Allow description override from the LintResult\n            description = self.description or rule.description\n            return SQLLintError(\n                rule=rule,\n                segment=self.anchor,\n                fixes=self.fixes,\n                description=description,\n            )\n\n        return None\n\n\nEvalResultType = Union[LintResult, list[LintResult], None]\n\n\nclass RuleMetaclass(type):\n    \"\"\"The metaclass for rules.\n\n    This metaclass provides provides auto-enrichment of the\n    rule docstring so that examples, groups, aliases and\n    names are added.\n\n    The reason we enrich the docstring is so that it can be\n    picked up by autodoc and all be displayed in the sqlfluff\n    docs.\n    \"\"\"\n\n    # Precompile the regular expressions\n    _doc_search_regex: ClassVar = re.compile(\n        \"(\\\\s{4}\\\\*\\\\*Anti-pattern\\\\*\\\\*|\\\\s{4}\\\\.\\\\. note::|\"\n        \"\\\\s\\\\s{4}\\\\*\\\\*Configuration\\\\*\\\\*)\",\n        flags=re.MULTILINE,\n    )\n    _valid_classname_regex: ClassVar = regex.compile(\n        r\"Rule_?([A-Z]{1}[a-zA-Z]+)?_([A-Z0-9]{4})\"\n    )\n    _valid_rule_name_regex: ClassVar = regex.compile(r\"[a-z][a-z\\.\\_]+\")\n\n    @staticmethod\n    def _populate_code_and_description(\n        name: str, class_dict: dict[str, Any]\n    ) -> dict[str, Any]:\n        \"\"\"Extract and validate the rule code & description.\n\n        We expect that rules are defined as classes with the name `Rule_XXXX`\n        where `XXXX` is of the form `LLNN`, where L is a letter and N is a\n        two digit number. For backward compatibility we also still support\n        the legacy format of LNNN i.e. a single letter and three digit number.\n\n        The two letters should be indicativ"}, {"start_line": 0, "end_line": 38, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.core.linter.\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the linter class.\"\"\"\n\nimport fnmatch\nimport logging\nimport os\nimport time\nfrom collections.abc import Iterator, Sequence\nfrom typing import TYPE_CHECKING, Optional, cast\n\nimport regex\nfrom tqdm import tqdm\n\nfrom sqlfluff.core.config import FluffConfig, progress_bar_configuration\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLFluffSkipFile,\n    SQLLexError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.helpers.file import get_encoding\nfrom sqlfluff.core.linter.common import (\n    ParsedString,\n    ParsedVariant,\n    RenderedFile,\n    RuleTuple,\n)\nfrom sqlfluff.core.linter.discovery import paths_from_path\nfrom sqlfluff.core.linter.fix import apply_fixes, compute_anchor_edit_info\nfrom sqlfluff.core.linter.linted_dir import LintedDir\nfrom sqlfluff.core.linter.linted_file import (\n    TMP_PRS_ERROR_TYPES,\n    FileTimings,\n    LintedFile,\n)\nfrom sqlfluff.core.linter.linting_result import LintingResult\nfrom sqlfluff.core.parser import Lexer, Parser\nfrom sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\nfrom sqlfluff.core.rules import BaseRule, RulePack, get_ruleset\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.rules.noqa import IgnoreMask\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.meta import MetaSegment\n    from sqlfluff.core.templaters import RawTemplater, TemplatedFile\n\n\nRuleTimingsType = list[tuple[str, str, float]]\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass Linter:\n    \"\"\"The interface class to interact with the linter.\"\"\"\n\n    # Default to allowing process parallelism\n    allow_process_parallelism = True\n\n    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n        dialect: Optional[str] = None,\n  "}], "retrieved_count": 10, "cost_time": 1.0429797172546387}
{"question": "What dependencies exist between the Linter class and the Parser and Rule classes?", "answer": null, "relative_code_list": null, "ground_truth": "The Linter class has several key dependencies on both Parser and Rule classes, forming the core of SQLFluff's linting pipeline. Key dependencies include: 1) Parser dependency - Linter imports and uses Parser class for creating parse trees from lexed segments, calling parser.parse() to convert raw segments into structured parse trees; 2) Rule system dependency - Linter depends on BaseRule class and RulePack for rule execution, using get_rulepack() to obtain filtered rule sets and executing rules against parse trees; 3) Import dependencies - Linter imports from sqlfluff.core.parser (Parser, Lexer) and sqlfluff.core.rules (BaseRule, RulePack, get_ruleset) to access these core components; 4) Configuration integration - Linter uses FluffConfig to configure both Parser and Rule behavior, passing configuration to both components; 5) Error handling - Linter processes errors from both Parser (SQLParseError) and Rules (SQLLintError), aggregating them into unified violation reports; 6) Pipeline coordination - Linter orchestrates the flow from parsing (via Parser) to rule evaluation (via BaseRule instances), managing the complete linting workflow; 7) Fix system integration - Linter coordinates with both Parser and Rules for automatic code fixing, applying LintFix objects generated by rules; 8) Template handling - Linter works with both Parser and Rules to handle templated SQL, ensuring proper source mapping between raw and rendered code; 9) Segment processing - Linter depends on BaseSegment from parser for tree traversal and rule evaluation; 10) Architecture layering - According to pyproject.toml dependency layers, linter references many things including rules, while rules should be independent from linter but can reference parser components.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 489, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Linter class and helper classes.\"\"\"\n\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.linter.common import ParsedString, RenderedFile, RuleTuple\nfrom sqlfluff.core.linter.linted_file import LintedFile\nfrom sqlfluff.core.linter.linter import Linter\nfrom sqlfluff.core.linter.linting_result import LintingResult\n\n__all__ = (\n    \"FormatterInterface\",\n    \"RuleTuple\",\n    \"ParsedString\",\n    \"LintedFile\",\n    \"LintingResult\",\n    \"Linter\",\n    \"RenderedFile\",\n)\n"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "rules_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\")\n        name = \"fake_basic\"\n        aliases = (\"fb1\", \"foo\")  # NB: Foo is a group on another rule.\n        crawl_behaviour = RootOnlyCrawler()\n\n        def _eval(self, **kwargs):\n            pass\n\n    class Rule_T011(Rule_T010):\n        \"\"\"Fake Basic Rule.\n\n        NOTE: We inherit crawl behaviour and _eval from above.\n        \"\"\"\n\n        groups = (\"all\", \"test\", \"foo\")\n        name = \"fake_other\"\n        aliases = (\"fb2\",)\n\n    class Rule_T012(Rule_T010):\n        \"\"\"Fake Basic Rule.\n\n        NOTE: We inherit crawl behaviour and _eval from above.\n        \"\"\"\n\n        # NB: \"fake_other\" is the name of another rule.\n        groups = (\"all\", \"foo\", \"fake_other\")\n        # No aliases, Name collides with the alias of another rule.\n        name = \"fake_again\"\n        aliases = ()\n\n    cfg = FluffConfig(\n        overrides={\"rules\": rules, \"exclude_rules\": exclude_rules, \"dialect\": \"ansi\"}\n    )\n    linter = Linter(config=cfg, user_rules=[Rule_T010, Rule_T011, Rule_T012])\n    # Get the set of selected codes:\n    selected_codes = set(tpl[0] for tpl in linter.rule_tuples())\n    # Check selected rules\n    assert selected_codes == resulting_codes\n\n\ndef test__rules__filter_unparsable():\n    \"\"\"Test that rules that handle their own crawling respect unparsable.\"\"\"\n    # Set up a linter with the user rule\n    linter = Linter(user_rules=[Rule_T002], dialect=\"ansi\", rules=[\"T002\"])\n    # Lint a simple parsable file and check we do get issues\n    # It's parsable, so we should get issues.\n    res = linter.lint_string(\"SELECT 1\")\n    assert any(v.rule_code() == \"T002\" for v in res.violations)\n    # Lint an unparsable file. Check we don't get any violations.\n    # It's not parsable so we shouldn't get issues.\n    res = linter.lint_string(\"asd asdf sdfg\")\n    assert not any(v.rule_code() == \"T002\" for v in res.violations)\n\n\ndef test__rules__result_unparsable():\n    \"\"\"Test that the linter won't allow rules which make the file unparsable.\"\"\"\n    # Set up a linter with the user rule\n  "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      rules: Optional[list[str]] = None,\n        user_rules: Optional[list[type[BaseRule]]] = None,\n        exclude_rules: Optional[list[str]] = None,\n    ) -> None:\n        if config and (dialect or rules or exclude_rules):\n            raise ValueError(  # pragma: no cover\n                \"Linter does not support setting both `config` and any of \"\n                \"`dialect`, `rules` or `exclude_rules`. The latter are \"\n                \"provided as convenience methods to avoid needing to \"\n                \"set the `config` object. If using `config`, please \"\n                \"provide all the other values within that object.\"\n            )\n        # Use the provided config or create one from the kwargs.\n        self.config = config or FluffConfig.from_kwargs(\n            dialect=dialect,\n            rules=rules,\n            exclude_rules=exclude_rules,\n            # Don't require a dialect to be provided yet. Defer this until we\n            # are actually linting something, since the directory we are linting\n            # from may provide additional configuration, including a dialect.\n            require_dialect=False,\n        )\n        # Get the dialect and templater\n        self.dialect: \"Dialect\" = cast(\"Dialect\", self.config.get(\"dialect_obj\"))\n        self.templater: \"RawTemplater\" = cast(\n            \"RawTemplater\", self.config.get(\"templater_obj\")\n        )\n        # Store the formatter for output\n        self.formatter = formatter\n        # Store references to user rule classes\n        self.user_rules = user_rules or []\n\n    def get_rulepack(self, config: Optional[FluffConfig] = None) -> RulePack:\n        \"\"\"Get hold of a set of rules.\"\"\"\n        rs = get_ruleset()\n        # Register any user rules\n        for rule in self.user_rules:\n            rs.register(rule)\n        cfg = config or self.config\n        return rs.get_rulepack(config=cfg)\n\n    def rule_tuples(self) -> list[RuleTuple]:\n        \"\"\"A simple pass through to access the rule tuples of the rule se"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "t import LintingResult\nfrom sqlfluff.core.parser import Lexer, Parser\nfrom sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\nfrom sqlfluff.core.rules import BaseRule, RulePack, get_ruleset\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.rules.noqa import IgnoreMask\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.meta import MetaSegment\n    from sqlfluff.core.templaters import RawTemplater, TemplatedFile\n\n\nRuleTimingsType = list[tuple[str, str, float]]\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass Linter:\n    \"\"\"The interface class to interact with the linter.\"\"\"\n\n    # Default to allowing process parallelism\n    allow_process_parallelism = True\n\n    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n        dialect: Optional[str] = None,\n        rules: Optional[list[str]] = None,\n        user_rules: Optional[list[type[BaseRule]]] = None,\n        exclude_rules: Optional[list[str]] = None,\n    ) -> None:\n        if config and (dialect or rules or exclude_rules):\n            raise ValueError(  # pragma: no cover\n                \"Linter does not support setting both `config` and any of \"\n                \"`dialect`, `rules` or `exclude_rules`. The latter are \"\n                \"provided as convenience methods to avoid needing to \"\n                \"set the `config` object. If using `config`, please \"\n                \"provide all the other values within that object.\"\n            )\n        # Use the provided config or create one from the kwargs.\n        self.config = config or FluffConfig.from_kwargs(\n            dialect=dialect,\n            rules=rules,\n            exclude_rules=exclude_rules,\n            # Don't require a dialect to be provided yet. Defer this until we\n            # are actually linting something, since the dir"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ectory we are linting\n            # from may provide additional configuration, including a dialect.\n            require_dialect=False,\n        )\n        # Get the dialect and templater\n        self.dialect: \"Dialect\" = cast(\"Dialect\", self.config.get(\"dialect_obj\"))\n        self.templater: \"RawTemplater\" = cast(\n            \"RawTemplater\", self.config.get(\"templater_obj\")\n        )\n        # Store the formatter for output\n        self.formatter = formatter\n        # Store references to user rule classes\n        self.user_rules = user_rules or []\n\n    def get_rulepack(self, config: Optional[FluffConfig] = None) -> RulePack:\n        \"\"\"Get hold of a set of rules.\"\"\"\n        rs = get_ruleset()\n        # Register any user rules\n        for rule in self.user_rules:\n            rs.register(rule)\n        cfg = config or self.config\n        return rs.get_rulepack(config=cfg)\n\n    def rule_tuples(self) -> list[RuleTuple]:\n        \"\"\"A simple pass through to access the rule tuples of the rule set.\"\"\"\n        rs = self.get_rulepack()\n        return [\n            RuleTuple(rule.code, rule.name, rule.description, rule.groups, rule.aliases)\n            for rule in rs.rules\n        ]\n\n    # #### Static methods\n    # These are the building blocks of the linting process.\n\n    @staticmethod\n    def load_raw_file_and_config(\n        fname: str, root_config: FluffConfig\n    ) -> tuple[str, FluffConfig, str]:\n        \"\"\"Load a raw file and the associated config.\"\"\"\n        file_config = root_config.make_child_from_path(fname)\n        config_encoding: str = file_config.get(\"encoding\", default=\"autodetect\")\n        encoding = get_encoding(fname=fname, config_encoding=config_encoding)\n        # Check file size before loading.\n        limit = file_config.get(\"large_file_skip_byte_limit\")\n\n        if limit:\n            # make sure the limit becomes an integer\n            try:\n                limit = int(limit)\n            except ValueError:\n                raise ValueError(\n                 "}, {"start_line": 0, "end_line": 38, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.core.linter.\"\"\"\n"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "rules_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "t of selected codes:\n    selected_codes = set(tpl[0] for tpl in linter.rule_tuples())\n    # Check selected rules\n    assert selected_codes == resulting_codes\n\n\ndef test__rules__filter_unparsable():\n    \"\"\"Test that rules that handle their own crawling respect unparsable.\"\"\"\n    # Set up a linter with the user rule\n    linter = Linter(user_rules=[Rule_T002], dialect=\"ansi\", rules=[\"T002\"])\n    # Lint a simple parsable file and check we do get issues\n    # It's parsable, so we should get issues.\n    res = linter.lint_string(\"SELECT 1\")\n    assert any(v.rule_code() == \"T002\" for v in res.violations)\n    # Lint an unparsable file. Check we don't get any violations.\n    # It's not parsable so we shouldn't get issues.\n    res = linter.lint_string(\"asd asdf sdfg\")\n    assert not any(v.rule_code() == \"T002\" for v in res.violations)\n\n\ndef test__rules__result_unparsable():\n    \"\"\"Test that the linter won't allow rules which make the file unparsable.\"\"\"\n    # Set up a linter with the user rule\n    linter = Linter(user_rules=[Rule_T003], dialect=\"ansi\", rules=[\"T003\"])\n    # Lint a simple parsable file and check we do get issues\n    # It's parsable, so we should get issues.\n    raw_sql = \"SELECT 1 FROM a\"\n    with fluff_log_catcher(logging.WARNING, \"sqlfluff\") as caplog:\n        res = linter.lint_string(raw_sql, fix=True)\n    # Check we got the warning.\n    assert \"would result in an unparsable file\" in caplog.text\n    # Check we get the violation.\n    assert any(v.rule_code() == \"T003\" for v in res.violations)\n    # The resulting file should be _the same_ because it would have resulted\n    # in an unparsable file if applied.\n    assert res.tree.raw == raw_sql\n\n\n@pytest.mark.parametrize(\n    \"sql_query, check_tuples\",\n    [\n        (\n            \"SELECT * FROM foo\",\n            # Even though there's a runaway fix, we should still\n            # find each issue once and not duplicates of them.\n            [\n                (\"T001\", 1, 7),\n                (\"T001\", 1, 9),\n         "}, {"start_line": 29000, "end_line": 31000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "          root_variant = variant\n                break\n        else:\n            linter_logger.info(\n                \"lint_parsed found no valid root variant for %s\", parsed.fname\n            )\n\n        # If there is a root variant, handle that first.\n        if root_variant:\n            linter_logger.info(\"lint_parsed - linting root variant (%s)\", parsed.fname)\n            assert root_variant.tree  # We just checked this.\n            (\n                fixed_tree,\n                initial_linting_errors,\n                ignore_mask,\n                rule_timings,\n            ) = cls.lint_fix_parsed(\n                root_variant.tree,\n                config=parsed.config,\n                rule_pack=rule_pack,\n                fix=fix,\n                fname=parsed.fname,\n                templated_file=variant.templated_file,\n                formatter=formatter,\n            )\n\n            # Set legacy variables for now\n            # TODO: Revise this\n            templated_file = variant.templated_file\n            tree = fixed_tree\n\n            # We're only going to return the *initial* errors, rather\n            # than any generated during the fixing cycle.\n            violations += initial_linting_errors\n\n            # Attempt to lint other variants if they exist.\n            # TODO: Revise whether this is sensible...\n            for idx, alternate_variant in enumerate(parsed.parsed_variants):\n                if alternate_variant is variant or not alternate_variant.tree:\n                    continue\n                linter_logger.info(\"lint_parsed - linting alt variant (%s)\", idx)\n                (\n                    _,  # Fixed Tree\n                    alt_linting_errors,\n                    _,  # Ignore Mask\n                    _,  # Timings\n                ) = cls.lint_fix_parsed(\n                    alternate_variant.tree,\n                    config=parsed.config,\n                    rule_pack=rule_pack,\n                    fix=fix,\n                    fname=parsed.fn"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "linter_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " unexpected happened\")\n    lntr = Linter()\n    lntr.lint_paths((\"test/fixtures/linter/passing.sql\",))\n    assert (\n        \"Unable to lint test/fixtures/linter/passing.sql due to an internal error.\"\n        # NB: Replace is to handle windows-style paths.\n        in patched_logger.warning.call_args[0][0].replace(\"\\\\\", \"/\")\n        and \"Exception: Something unexpected happened\"\n        in patched_logger.warning.call_args[0][0]\n    )\n\n\ndef test__linter__empty_file():\n    \"\"\"Test linter behaves nicely with an empty string.\n\n    Much of this test is about making sure that ParsedString is\n    instantiated appropriately.\n    \"\"\"\n    lntr = Linter(dialect=\"ansi\")\n    # Make sure no exceptions raised and no violations found in empty file.\n    parsed = lntr.parse_string(\"\")\n    # There should still be a parsed variant\n    assert parsed.parsed_variants\n    assert len(parsed.parsed_variants) == 1\n    root_variant = parsed.parsed_variants[0]\n    # That root variant should still have a templated file and a parsed tree\n    # (although that parsed tree will likely just be an end of file marker).\n    assert root_variant.templated_file\n    assert root_variant.tree\n    # No violations\n    assert not parsed.violations\n\n\ndef test__linter__parse_fail():\n    \"\"\"Test linter behaves as expected with an unparsable string.\n\n    Much of this test is about making sure that ParsedString is\n    instantiated appropriately.\n    \"\"\"\n    lntr = Linter(dialect=\"ansi\")\n    # Try and parse something which obviously isn't SQL\n    parsed = lntr.parse_string(\"THIS IS NOT SQL\")\n    # There should still be a parsed variant\n    assert parsed.parsed_variants\n    assert len(parsed.parsed_variants) == 1\n    root_variant = parsed.parsed_variants[0]\n    # That root variant should still have a templated file and a parsed tree...\n    assert root_variant.templated_file\n    assert root_variant.tree\n    # ...but that tree should contain an unparsable segment.\n    assert \"unparsable\" in root_variant.tree.type_set()\n    "}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "linter_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e and a parsed tree\n    # (although that parsed tree will likely just be an end of file marker).\n    assert root_variant.templated_file\n    assert root_variant.tree\n    # No violations\n    assert not parsed.violations\n\n\ndef test__linter__parse_fail():\n    \"\"\"Test linter behaves as expected with an unparsable string.\n\n    Much of this test is about making sure that ParsedString is\n    instantiated appropriately.\n    \"\"\"\n    lntr = Linter(dialect=\"ansi\")\n    # Try and parse something which obviously isn't SQL\n    parsed = lntr.parse_string(\"THIS IS NOT SQL\")\n    # There should still be a parsed variant\n    assert parsed.parsed_variants\n    assert len(parsed.parsed_variants) == 1\n    root_variant = parsed.parsed_variants[0]\n    # That root variant should still have a templated file and a parsed tree...\n    assert root_variant.templated_file\n    assert root_variant.tree\n    # ...but that tree should contain an unparsable segment.\n    assert \"unparsable\" in root_variant.tree.type_set()\n    # There *should* be violations because there should be a parsing fail.\n    assert parsed.violations\n    assert any(isinstance(v, SQLParseError) for v in parsed.violations)\n\n\ndef test__linter__templating_fail():\n    \"\"\"Test linter behaves as expected with invalid jinja template.\n\n    Much of this test is about making sure that ParsedString is\n    instantiated appropriately.\n    \"\"\"\n    lntr = Linter(dialect=\"ansi\")\n    # Try and parse something which breaks Jinja templating.\n    parsed = lntr.parse_string(\"{% if foo %}\")\n    # For a templating fail, there won't be a parsed variant.\n    assert not parsed.parsed_variants\n    # There *should* be violations because there should be a templating fail.\n    assert parsed.violations\n    assert any(isinstance(v, SQLTemplaterError) for v in parsed.violations)\n\n\n@pytest.mark.parametrize(\n    \"path,rules,ignore_templated_areas,check_tuples\",\n    [\n        (\n            \"test/fixtures/templater/jinja_h_macros/jinja.sql\",\n            \"L006\",\n         "}], "retrieved_count": 10, "cost_time": 1.0929434299468994}
{"question": "What is the role of the Lexer class in SQLFluff's parsing pipeline?", "answer": null, "relative_code_list": null, "ground_truth": "The Lexer class serves as the second stage in SQLFluff's parsing pipeline, responsible for breaking down SQL input into atomic tokens. Its key roles include: 1) Tokenization - takes SQL input (either raw strings or TemplatedFile objects) and separates it into individual segments of whitespace and code, producing a flat sequence of typed segments (all subclasses of RawSegment); 2) Template integration - handles templated SQL by mapping lexed elements to template slices, allowing rule violations to be backported to original templated sections; 3) Dialect-specific lexing - uses dialect-specific lexer matchers (StringLexer and RegexLexer) to recognize tokens according to the specified SQL dialect; 4) Error handling - identifies unlexable content and packages it as UnlexableSegment, generating SQLLexError violations for problematic input; 5) Block tracking - manages templating blocks using BlockTracker to match opening and closing tags for proper template processing; 6) Position tracking - maintains position markers for each token to enable accurate error reporting and source mapping; 7) Matcher coordination - coordinates multiple lexer matchers in priority order, using a last-resort lexer (RegexLexer) for unmatched content; 8) Segment generation - converts lexed elements into RawSegment objects that form the input for the parser stage; 9) Template slice mapping - maps lexed elements to their corresponding template slices for proper source location tracking; 10) Pipeline integration - provides the foundation for the parser stage by creating the atomic building blocks that will be assembled into the parse tree.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "lexer.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The code for the Lexer.\"\"\"\n\nimport logging\nfrom collections.abc import Iterator\nfrom typing import Any, NamedTuple, Optional, Union\nfrom uuid import UUID, uuid4\n\nimport regex\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLLexError\nfrom sqlfluff.core.helpers.slice import is_zero_slice, offset_slice, to_tuple\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments import (\n    BaseSegment,\n    Dedent,\n    EndOfFile,\n    Indent,\n    MetaSegment,\n    RawSegment,\n    TemplateLoop,\n    TemplateSegment,\n    UnlexableSegment,\n)\nfrom sqlfluff.core.templaters import TemplatedFile\nfrom sqlfluff.core.templaters.base import TemplatedFileSlice\n\n# Instantiate the lexer logger\nlexer_logger = logging.getLogger(\"sqlfluff.lexer\")\n\n\nclass BlockTracker:\n    \"\"\"This is an object for keeping track of templating blocks.\n\n    Using the .enter() and .exit() methods on opening and closing\n    blocks, we can match up tags of the same level so that later\n    it's easier to treat them the same way in the linting engine.\n\n    In case looping means that we encounter the same block more\n    than once, we use cache uuids against their source location\n    so that if we try to re-enter the block again, it will get\n    the same uuid on the second pass.\n    \"\"\"\n\n    _stack: list[UUID] = []\n    _map: dict[tuple[int, int], UUID] = {}\n\n    def enter(self, src_slice: slice) -> None:\n        \"\"\"Add a block to the stack.\"\"\"\n        key = to_tuple(src_slice)\n        uuid = self._map.get(key, None)\n\n        if not uuid:\n            uuid = uuid4()\n            self._map[key] = uuid\n            lexer_logger.debug(\n                \"        Entering block stack @ %s: %s (fresh)\",\n                src_slice,\n                uuid,\n            )\n        else:\n            lexer_logger.debug(\n                \"        Entering block stack @ %s: %s (cached)\",\n                src_slice,\n                uuid,\n            )\n\n        self._stack.append(uui"}, {"start_line": 29000, "end_line": 31000, "belongs_to": {"file_name": "lexer.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "template slice\n                        continue\n\n            raise NotImplementedError(\n                f\"Unable to process slice: {tfs}\"\n            )  # pragma: no cover\n\n    # If templated elements are left, yield them.\n    # We can assume they're all zero length if we're here.\n    for tfs_idx, tfs in enumerate(templated_file_slices[tfs_idx:], tfs_idx):\n        next_tfs = (\n            templated_file_slices[tfs_idx + 1]\n            if tfs_idx + 1 < len(templated_file_slices)\n            else None\n        )\n        yield from _handle_zero_length_slice(\n            tfs, next_tfs, block_stack, templated_file, add_indents\n        )\n\n\nclass Lexer:\n    \"\"\"The Lexer class actually does the lexing step.\"\"\"\n\n    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        last_resort_lexer: Optional[StringLexer] = None,\n        dialect: Optional[str] = None,\n    ):\n        if config and dialect:\n            raise ValueError(  # pragma: no cover\n                \"Lexer does not support setting both `config` and `dialect`.\"\n            )\n        # Use the provided config or create one from the dialect.\n        self.config = config or FluffConfig.from_kwargs(dialect=dialect)\n        # Store the matchers\n        self.lexer_matchers = self.config.get(\"dialect_obj\").get_lexer_matchers()\n\n        self.last_resort_lexer = last_resort_lexer or RegexLexer(\n            \"<unlexable>\",\n            r\"[^\\t\\n\\ ]*\",\n            UnlexableSegment,\n        )\n\n    def lex(\n        self, raw: Union[str, TemplatedFile]\n    ) -> tuple[tuple[BaseSegment, ...], list[SQLLexError]]:\n        \"\"\"Take a string or TemplatedFile and return segments.\n\n        If we fail to match the *whole* string, then we must have\n        found something that we cannot lex. If that happens we should\n        package it up as unlexable and keep track of the exceptions.\n        \"\"\"\n        # Make sure we've got a string buffer and a template\n        # regardless of what was passed in.\n        if isinsta"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "             )\n        with open(fname, encoding=encoding, errors=\"backslashreplace\") as target_file:\n            raw_file = target_file.read()\n        # Scan the raw file for config commands.\n        file_config.process_raw_file_for_config(raw_file, fname)\n        # Return the raw file and config\n        return raw_file, file_config, encoding\n\n    @staticmethod\n    def _normalise_newlines(string: str) -> str:\n        \"\"\"Normalise newlines to unix-style line endings.\"\"\"\n        return regex.sub(r\"\\r\\n|\\r\", \"\\n\", string)\n\n    @staticmethod\n    def _lex_templated_file(\n        templated_file: \"TemplatedFile\", config: FluffConfig\n    ) -> tuple[Optional[Sequence[BaseSegment]], list[SQLLexError]]:\n        \"\"\"Lex a templated file.\"\"\"\n        violations = []\n        linter_logger.info(\"LEXING RAW (%s)\", templated_file.fname)\n        # Get the lexer\n        lexer = Lexer(config=config)\n        # Lex the file and log any problems\n        try:\n            segments, lex_vs = lexer.lex(templated_file)\n            # NOTE: There will always be segments, even if it's\n            # just an end of file marker.\n            assert segments, \"The token sequence should never be empty.\"\n            # We might just get the violations as a list\n            violations += lex_vs\n            linter_logger.info(\"Lexed segments: %s\", [seg.raw for seg in segments])\n        except SQLLexError as err:  # pragma: no cover\n            linter_logger.info(\"LEXING FAILED! (%s): %s\", templated_file.fname, err)\n            violations.append(err)\n            return None, violations\n\n        # Check that we've got sensible indentation from the lexer.\n        # We might need to suppress if it's a complicated file.\n        templating_blocks_indent = config.get(\"template_blocks_indent\", \"indentation\")\n        if isinstance(templating_blocks_indent, str):\n            force_block_indent = templating_blocks_indent.lower().strip() == \"force\"\n        else:\n            force_block_indent = False\n        templating"}, {"start_line": 0, "end_line": 38, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.core.parser.\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "raw.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Raw segment definitions.\n\nThis is designed to be the root segment, without\nany children, and the output of the lexer.\n\"\"\"\n\nfrom typing import Any, Callable, Optional, Union, cast\nfrom uuid import uuid4\n\nimport regex as re\n\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\n\n\nclass RawSegment(BaseSegment):\n    \"\"\"This is a segment without any subsegments.\"\"\"\n\n    type = \"raw\"\n    _is_code = True\n    _is_comment = False\n    _is_whitespace = False\n    # Classes inheriting from RawSegment may provide a _default_raw\n    # to enable simple initialisation.\n    _default_raw = \"\"\n\n    def __init__(\n        self,\n        raw: Optional[str] = None,\n        pos_marker: Optional[PositionMarker] = None,\n        # For legacy and syntactic sugar we allow the simple\n        # `type` argument here, but for more precise inheritance\n        # we suggest using the `instance_types` option.\n        type: Optional[str] = None,\n        instance_types: tuple[str, ...] = (),\n        trim_start: Optional[tuple[str, ...]] = None,\n        trim_chars: Optional[tuple[str, ...]] = None,\n        source_fixes: Optional[list[SourceFix]] = None,\n        uuid: Optional[int] = None,\n        quoted_value: Optional[tuple[str, Union[int, str]]] = None,\n        escape_replacements: Optional[list[tuple[str, str]]] = None,\n        casefold: Optional[Callable[[str], str]] = None,\n    ):\n        \"\"\"Initialise raw segment.\n\n        If raw is not provided, we default to _default_raw if present.\n        If pos_marker is not provided, it is assume that this will be\n        inserted later as part of a reposition phase.\n        \"\"\"\n        if raw is not None:  # NB, raw *can* be an empty string and be valid\n            self._raw = raw\n        else:\n            self._raw = self._default_raw\n        self._raw_upper = self._raw.upper()\n        # pos marker is required here. We ignore the typing initially\n        # because it might *initially* b"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "lexer_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "st the StringLexer.\"\"\"\n    matcher = StringLexer(\"dot\", \".\", CodeSegment)\n    assert_matches(raw, matcher, res)\n\n\n@pytest.mark.parametrize(\n    \"raw,reg,res\",\n    [\n        (\"fsaljk\", \"f\", \"f\"),\n        (\"fsaljk\", r\"f\", \"f\"),\n        (\"fsaljk\", r\"[fas]*\", \"fsa\"),\n        # Matching whitespace segments\n        (\"   \\t   fsaljk\", r\"[^\\S\\r\\n]*\", \"   \\t   \"),\n        # Matching whitespace segments (with a newline)\n        (\"   \\t \\n  fsaljk\", r\"[^\\S\\r\\n]*\", \"   \\t \"),\n        # Matching quotes containing stuff\n        (\"'something boring'   \\t \\n  fsaljk\", r\"'[^']*'\", \"'something boring'\"),\n        (\n            \"' something exciting \\t\\n '   \\t \\n  fsaljk\",\n            r\"'[^']*'\",\n            \"' something exciting \\t\\n '\",\n        ),\n    ],\n)\ndef test__parser__lexer_regex(raw, reg, res, caplog):\n    \"\"\"Test the RegexLexer.\"\"\"\n    matcher = RegexLexer(\"test\", reg, CodeSegment)\n    with caplog.at_level(logging.DEBUG):\n        assert_matches(raw, matcher, res)\n\n\ndef test__parser__lexer_lex_match(caplog):\n    \"\"\"Test the RepeatedMultiMatcher.\"\"\"\n    matchers = [\n        StringLexer(\"dot\", \".\", CodeSegment),\n        RegexLexer(\"test\", r\"#[^#]*#\", CodeSegment),\n    ]\n    with caplog.at_level(logging.DEBUG):\n        res = Lexer.lex_match(\"..#..#..#\", matchers)\n        assert res.forward_string == \"#\"  # Should match right up to the final element\n        assert len(res.elements) == 5\n        assert res.elements[2].raw == \"#..#\"\n\n\ndef test__parser__lexer_fail():\n    \"\"\"Test the how the lexer fails and reports errors.\"\"\"\n    lex = Lexer(config=FluffConfig(overrides={\"dialect\": \"ansi\"}))\n\n    _, vs = lex.lex(\"Select \\u0394\")\n\n    assert len(vs) == 1\n    err = vs[0]\n    assert isinstance(err, SQLLexError)\n    assert err.line_pos == 8\n\n\ndef test__parser__lexer_fail_via_parse():\n    \"\"\"Test the how the parser fails and reports errors while lexing.\"\"\"\n    lexer = Lexer(config=FluffConfig(overrides={\"dialect\": \"ansi\"}))\n    _, vs = lexer.lex(\"Select \\u0394\")\n    assert vs\n    assert l"}, {"start_line": 30000, "end_line": 32000, "belongs_to": {"file_name": "lexer.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "s not support setting both `config` and `dialect`.\"\n            )\n        # Use the provided config or create one from the dialect.\n        self.config = config or FluffConfig.from_kwargs(dialect=dialect)\n        # Store the matchers\n        self.lexer_matchers = self.config.get(\"dialect_obj\").get_lexer_matchers()\n\n        self.last_resort_lexer = last_resort_lexer or RegexLexer(\n            \"<unlexable>\",\n            r\"[^\\t\\n\\ ]*\",\n            UnlexableSegment,\n        )\n\n    def lex(\n        self, raw: Union[str, TemplatedFile]\n    ) -> tuple[tuple[BaseSegment, ...], list[SQLLexError]]:\n        \"\"\"Take a string or TemplatedFile and return segments.\n\n        If we fail to match the *whole* string, then we must have\n        found something that we cannot lex. If that happens we should\n        package it up as unlexable and keep track of the exceptions.\n        \"\"\"\n        # Make sure we've got a string buffer and a template\n        # regardless of what was passed in.\n        if isinstance(raw, str):\n            template = TemplatedFile.from_string(raw)\n            str_buff = raw\n        else:\n            template = raw\n            str_buff = str(template)\n\n        # Lex the string to get a tuple of LexedElement\n        element_buffer: list[LexedElement] = []\n        while True:\n            res = self.lex_match(str_buff, self.lexer_matchers)\n            element_buffer += res.elements\n            if res.forward_string:\n                resort_res = self.last_resort_lexer.match(res.forward_string)\n                if not resort_res:  # pragma: no cover\n                    # If we STILL can't match, then just panic out.\n                    raise SQLLexError(\n                        \"Fatal. Unable to lex characters: {0!r}\".format(\n                            res.forward_string[:10] + \"...\"\n                            if len(res.forward_string) > 9\n                            else res.forward_string\n                        )\n                    )\n                str_buff = "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "lexer_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ent \\nblah*/\",\n            [\"abc\", \" \", \"/* comment\", \" \", \"\\n\", \"blah*/\", \"\"],\n        ),\n        (\"abc /*\\n\\t\\n*/\", [\"abc\", \" \", \"/*\", \"\\n\", \"\\t\", \"\\n\", \"*/\", \"\"]),\n        # Test strings\n        (\"*-+bd/\", [\"*\", \"-\", \"+\", \"bd\", \"/\", \"\"]),\n        # Test Negatives and Minus\n        (\"2+4 -5\", [\"2\", \"+\", \"4\", \" \", \"-\", \"5\", \"\"]),\n        (\"when 'Spec\\\\'s 23' like\", [\"when\", \" \", \"'Spec\\\\'s 23'\", \" \", \"like\", \"\"]),\n        ('when \"Spec\\\\\"s 23\" like', [\"when\", \" \", '\"Spec\\\\\"s 23\"', \" \", \"like\", \"\"]),\n    ],\n)\ndef test__parser__lexer_obj(raw, res, caplog):\n    \"\"\"Test the lexer splits as expected in a selection of cases.\"\"\"\n    lex = Lexer(config=FluffConfig(overrides={\"dialect\": \"ansi\"}))\n    with caplog.at_level(logging.DEBUG):\n        lexing_segments, _ = lex.lex(raw)\n        assert [seg.raw for seg in lexing_segments] == res\n\n\n@pytest.mark.parametrize(\n    \"raw,res\",\n    [\n        (\".fsaljk\", \".\"),\n        (\"fsaljk\", None),\n    ],\n)\ndef test__parser__lexer_string(raw, res):\n    \"\"\"Test the StringLexer.\"\"\"\n    matcher = StringLexer(\"dot\", \".\", CodeSegment)\n    assert_matches(raw, matcher, res)\n\n\n@pytest.mark.parametrize(\n    \"raw,reg,res\",\n    [\n        (\"fsaljk\", \"f\", \"f\"),\n        (\"fsaljk\", r\"f\", \"f\"),\n        (\"fsaljk\", r\"[fas]*\", \"fsa\"),\n        # Matching whitespace segments\n        (\"   \\t   fsaljk\", r\"[^\\S\\r\\n]*\", \"   \\t   \"),\n        # Matching whitespace segments (with a newline)\n        (\"   \\t \\n  fsaljk\", r\"[^\\S\\r\\n]*\", \"   \\t \"),\n        # Matching quotes containing stuff\n        (\"'something boring'   \\t \\n  fsaljk\", r\"'[^']*'\", \"'something boring'\"),\n        (\n            \"' something exciting \\t\\n '   \\t \\n  fsaljk\",\n            r\"'[^']*'\",\n            \"' something exciting \\t\\n '\",\n        ),\n    ],\n)\ndef test__parser__lexer_regex(raw, reg, res, caplog):\n    \"\"\"Test the RegexLexer.\"\"\"\n    matcher = RegexLexer(\"test\", reg, CodeSegment)\n    with caplog.at_level(logging.DEBUG):\n        assert_matches(raw, matcher, res)\n\n\ndef test__parser__lexer_lex_m"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parser.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the Parser class.\"\"\"\n\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import check_still_complete\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.parser.segments import BaseFileSegment, BaseSegment\n\n\nclass Parser:\n    \"\"\"Instantiates parsed queries from a sequence of lexed raw segments.\"\"\"\n\n    def __init__(\n        self, config: Optional[FluffConfig] = None, dialect: Optional[str] = None\n    ):\n        if config and dialect:\n            raise ValueError(  # pragma: no cover\n                \"Parser does not support setting both `config` and `dialect`.\"\n            )\n        # Use the provided config or create one from the dialect.\n        self.config = config or FluffConfig.from_kwargs(dialect=dialect)\n        self.RootSegment: type[BaseFileSegment] = self.config.get(\n            \"dialect_obj\"\n        ).get_root_segment()\n\n    def parse(\n        self,\n        segments: Sequence[\"BaseSegment\"],\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> Optional[\"BaseSegment\"]:\n        \"\"\"Parse a series of lexed tokens using the current dialect.\"\"\"\n        if not segments:  # pragma: no cover\n            # This should normally never happen because there will usually\n            # be an end_of_file segment. It would probably only happen in\n            # api use cases.\n            return None\n\n        # NOTE: This is the only time we use the parse context not in the\n        # context of a context manager. That's because it's the initial\n        # instantiation.\n        ctx = ParseContext.from_config(config=self.config)\n        # Kick off parsing with the root segment. The BaseFileSegment has\n        # a unique entry point to facilitate exactly this. All other segments\n        # will use the standard .match() route.\n        root = self.RootSegment.root_pars"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "t import LintingResult\nfrom sqlfluff.core.parser import Lexer, Parser\nfrom sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\nfrom sqlfluff.core.rules import BaseRule, RulePack, get_ruleset\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.rules.noqa import IgnoreMask\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.meta import MetaSegment\n    from sqlfluff.core.templaters import RawTemplater, TemplatedFile\n\n\nRuleTimingsType = list[tuple[str, str, float]]\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass Linter:\n    \"\"\"The interface class to interact with the linter.\"\"\"\n\n    # Default to allowing process parallelism\n    allow_process_parallelism = True\n\n    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n        dialect: Optional[str] = None,\n        rules: Optional[list[str]] = None,\n        user_rules: Optional[list[type[BaseRule]]] = None,\n        exclude_rules: Optional[list[str]] = None,\n    ) -> None:\n        if config and (dialect or rules or exclude_rules):\n            raise ValueError(  # pragma: no cover\n                \"Linter does not support setting both `config` and any of \"\n                \"`dialect`, `rules` or `exclude_rules`. The latter are \"\n                \"provided as convenience methods to avoid needing to \"\n                \"set the `config` object. If using `config`, please \"\n                \"provide all the other values within that object.\"\n            )\n        # Use the provided config or create one from the kwargs.\n        self.config = config or FluffConfig.from_kwargs(\n            dialect=dialect,\n            rules=rules,\n            exclude_rules=exclude_rules,\n            # Don't require a dialect to be provided yet. Defer this until we\n            # are actually linting something, since the dir"}], "retrieved_count": 10, "cost_time": 1.0992491245269775}
{"question": "Why does SQLFluff use a plugin-based architecture for custom rules instead of requiring users to modify the core codebase?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff uses a plugin-based architecture for custom rules to provide several key benefits: 1) Separation of concerns - Keeps the core codebase clean and focused on standard functionality while allowing custom rules to be developed independently; 2) Maintainability - Prevents the core codebase from becoming bloated with organization-specific or project-specific rules that aren't universally applicable; 3) Version control - Allows custom rules to be versioned and managed separately from the main SQLFluff releases; 4) Distribution - Enables custom rules to be packaged and distributed independently, making them easier to share across teams or organizations; 5) Stability - Reduces the risk of breaking changes in the core codebase when custom rules are added or modified; 6) Flexibility - Allows different projects or teams to use different sets of custom rules without affecting others; 7) Testing - Enables custom rules to be tested independently of the core SQLFluff functionality; 8) Upgradability - Makes it easier to upgrade SQLFluff without losing custom rule functionality; 9) Community contribution - Encourages community contributions without requiring changes to the main codebase; 10) Enterprise adoption - Makes SQLFluff more suitable for enterprise environments where custom rules are often required for compliance or organizational standards.", "score": null, "retrieved_content": [{"start_line": 1000, "end_line": 2820, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e that currently, the gap _before_ and _after_ the semicolon \"\n                \"is considered 'between' statements.\"\n            ),\n        },\n        \"maximum_empty_lines_inside_statements\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The maximum number of empty lines allowed inside statements.\"\n            ),\n        },\n        \"wildcard_policy\": {\n            \"validation\": [\"single\", \"multiple\"],\n            \"definition\": \"Treatment of wildcards. Defaults to ``single``.\",\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.layout.LT01 import Rule_LT01\n    from sqlfluff.rules.layout.LT02 import Rule_LT02\n    from sqlfluff.rules.layout.LT03 import Rule_LT03\n    from sqlfluff.rules.layout.LT04 import Rule_LT04\n    from sqlfluff.rules.layout.LT05 import Rule_LT05\n    from sqlfluff.rules.layout.LT06 import Rule_LT06\n    from sqlfluff.rules.layout.LT07 import Rule_LT07\n    from sqlfluff.rules.layout.LT08 import Rule_LT08\n    from sqlfluff.rules.layout.LT09 import Rule_LT09\n    from sqlfluff.rules.layout.LT10 import Rule_LT10\n    from sqlfluff.rules.layout.LT11 import Rule_LT11\n    from sqlfluff.rules.layout.LT12 import Rule_LT12\n    from sqlfluff.rules.layout.LT13 import Rule_LT13\n    from sqlfluff.rules.layout.LT14 import Rule_LT14\n    from sqlfluff.rules.layout.LT15 import Rule_LT15\n\n    return [\n        Rule_LT01,\n        Rule_LT02,\n        Rule_LT03,\n        Rule_LT04,\n        Rule_LT05,\n        Rule_LT06,\n        Rule_LT07,\n        Rule_LT08,\n        Rule_LT09,\n        Rule_LT10,\n        Rule_LT11,\n        Rule_LT12,\n        Rule_LT13,\n        Rule_LT14,\n        Rule_LT15,\n    ]\n"}, {"start_line": 0, "end_line": 1269, "belongs_to": {"file_name": "lib.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/plugin", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Base implementation for the plugin.\"\"\"\n\nfrom typing import Any\n\nfrom sqlfluff.core.config import load_config_resource\nfrom sqlfluff.core.plugin import hookimpl\nfrom sqlfluff.core.rules import BaseRule, ConfigInfo\nfrom sqlfluff.core.rules.config_info import STANDARD_CONFIG_INFO_DICT\nfrom sqlfluff.core.rules.loader import get_rules_from_path\nfrom sqlfluff.core.templaters import RawTemplater, core_templaters\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: All standard rules will eventually be loaded as\n    plugins and so before 2.0.0, once all legacy plugin definitions\n    are migrated, this function will be amended to return no rules.\n    \"\"\"\n    return get_rules_from_path()\n\n\n@hookimpl\ndef get_templaters() -> list[type[RawTemplater]]:\n    \"\"\"Get templaters.\"\"\"\n    templaters = list(t for t in core_templaters())\n    return templaters\n\n\n@hookimpl\ndef load_default_config() -> dict[str, Any]:\n    \"\"\"Loads the default configuration for the plugin.\"\"\"\n    return load_config_resource(\n        package=\"sqlfluff.core\",\n        file_name=\"default_config.cfg\",\n    )\n\n\n@hookimpl\ndef get_configs_info() -> dict[str, ConfigInfo]:\n    \"\"\"Get rule config validations and descriptions.\"\"\"\n    return STANDARD_CONFIG_INFO_DICT\n"}, {"start_line": 0, "end_line": 1573, "belongs_to": {"file_name": "rules.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/plugins/sqlfluff-plugin-example/src/sqlfluff_plugin_example", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"An example of a custom rule implemented through the plugin system.\n\nThis uses the rules API supported from 0.4.0 onwards.\n\"\"\"\n\nfrom sqlfluff.core.rules import (\n    BaseRule,\n    LintResult,\n    RuleContext,\n)\nfrom sqlfluff.core.rules.crawlers import SegmentSeekerCrawler\n\n\n# These two decorators allow plugins\n# to be displayed in the sqlfluff docs\nclass Rule_Example_L001(BaseRule):\n    \"\"\"ORDER BY on these columns is forbidden!\n\n    **Anti-pattern**\n\n    Using ``ORDER BY`` one some forbidden columns.\n\n    .. code-block:: sql\n\n        SELECT *\n        FROM foo\n        ORDER BY\n            bar,\n            baz\n\n    **Best practice**\n\n    Do not order by these columns.\n\n    .. code-block:: sql\n\n        SELECT *\n        FROM foo\n        ORDER BY bar\n    \"\"\"\n\n    groups = (\"all\",)\n    config_keywords = [\"forbidden_columns\"]\n    crawl_behaviour = SegmentSeekerCrawler({\"orderby_clause\"})\n    is_fix_compatible = True\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Overwrite __init__ to set config.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.forbidden_columns = [\n            col.strip() for col in self.forbidden_columns.split(\",\")\n        ]\n\n    def _eval(self, context: RuleContext):\n        \"\"\"We should not ORDER BY forbidden_columns.\"\"\"\n        for seg in context.segment.segments:\n            col_name = seg.raw.lower()\n            if col_name in self.forbidden_columns:\n                return LintResult(\n                    anchor=seg,\n                    description=f\"Column `{col_name}` not allowed in ORDER BY.\",\n                )\n"}, {"start_line": 0, "end_line": 1183, "belongs_to": {"file_name": "hookspecs.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/plugin", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the specification to implement a plugin.\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import TYPE_CHECKING, Any\n\nimport pluggy\n\nfrom sqlfluff.core.plugin import plugin_base_name\n\nif TYPE_CHECKING:  # pragma: no cover\n    # NOTE: This import is against the normal import rules, but is here for strict\n    # type checking. We have an exception for this in the import linter.\n    from sqlfluff.core.rules.base import BaseRule\n\nhookspec = pluggy.HookspecMarker(plugin_base_name)\n\n\nclass PluginSpec:\n    \"\"\"Defines the method signatures for plugin implementations.\"\"\"\n\n    @hookspec\n    @abstractmethod\n    def get_rules(self) -> list[type[\"BaseRule\"]]:\n        \"\"\"Get plugin rules.\"\"\"\n\n    @hookspec\n    @abstractmethod\n    def load_default_config(self) -> dict[str, Any]:\n        \"\"\"Loads the default configuration for the plugin.\"\"\"\n\n    @hookspec\n    @abstractmethod\n    # TODO: This type annotation could probably be more specific but that would\n    # require making the config info object something more like a namedTuple rather\n    # than a dict.\n    def get_configs_info(self) -> dict[str, dict[str, Any]]:\n        \"\"\"Get rule config validations and descriptions.\"\"\"\n"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "definition\": (\"Should count(0) be preferred over count(*) and count(1)?\"),\n        },\n        \"multiline_newline\": {\n            \"validation\": [True, False],\n            \"definition\": (\n                \"Should semi-colons be placed on a new line after multi-line \"\n                \"statements?\"\n            ),\n        },\n        \"require_final_semicolon\": {\n            \"validation\": [True, False],\n            \"definition\": (\n                \"Should final semi-colons be required? \"\n                \"(N.B. forcing trailing semi-colons is not recommended for dbt users \"\n                \"as it can cause issues when wrapping the query within other SQL \"\n                \"queries).\"\n            ),\n        },\n        \"preferred_quoted_literal_style\": {\n            \"validation\": [\"consistent\", \"single_quotes\", \"double_quotes\"],\n            \"definition\": (\n                \"Preferred quoting style to use for the quoted literals. If set to \"\n                \"``consistent`` quoting style is derived from the first quoted literal \"\n                \"in the file.\"\n            ),\n        },\n        \"preferred_type_casting_style\": {\n            \"validation\": [\"consistent\", \"shorthand\", \"convert\", \"cast\"],\n            \"definition\": (\"The expectation for using sql type casting\"),\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.convention.CV01 import Rule_CV01\n    from sqlfluff.rules.convention.CV02 import Rule_CV02\n    from sqlfluff.rules.convention.CV03 import Rule_CV03\n    from sqlfluff.rules.convention.CV04 import Rule_CV04\n    from sqlfluff.rules.convention.CV05 import Rule_CV05\n    from sqlfluff.rules.convention.CV06 import Rule_CV06\n    from sqlfluff.rules.convention.CV07 import Rule_CV07\n    from sqlfluff.rules.convention.CV08 import Rule_CV08\n    from sqlfluff.rules.convention.CV09 import Rule_CV09\n    from sql"}, {"start_line": 1000, "end_line": 2364, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/aliasing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tion\": \"How to handle comparison casefolding in an alias.\",\n        },\n        \"min_alias_length\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The minimum length of an alias to allow without raising a violation.\"\n            ),\n        },\n        \"max_alias_length\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The maximum length of an alias to allow without raising a violation.\"\n            ),\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.aliasing.AL01 import Rule_AL01\n    from sqlfluff.rules.aliasing.AL02 import Rule_AL02\n    from sqlfluff.rules.aliasing.AL03 import Rule_AL03\n    from sqlfluff.rules.aliasing.AL04 import Rule_AL04\n    from sqlfluff.rules.aliasing.AL05 import Rule_AL05\n    from sqlfluff.rules.aliasing.AL06 import Rule_AL06\n    from sqlfluff.rules.aliasing.AL07 import Rule_AL07\n    from sqlfluff.rules.aliasing.AL08 import Rule_AL08\n    from sqlfluff.rules.aliasing.AL09 import Rule_AL09\n\n    return [\n        Rule_AL01,\n        Rule_AL02,\n        Rule_AL03,\n        Rule_AL04,\n        Rule_AL05,\n        Rule_AL06,\n        Rule_AL07,\n        Rule_AL08,\n        Rule_AL09,\n    ]\n"}, {"start_line": 0, "end_line": 1987, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/plugins/sqlfluff-plugin-example/src/sqlfluff_plugin_example", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"An example of a custom rule implemented through the plugin system.\n\nThis uses the rules API supported from 0.4.0 onwards.\n\"\"\"\n\nfrom typing import Any\n\nfrom sqlfluff.core.config import load_config_resource\nfrom sqlfluff.core.plugin import hookimpl\nfrom sqlfluff.core.rules import BaseRule, ConfigInfo\n\n# For backward compatibility we still support importing\n# rules within the body of the root plugin module. This is included\n# here for illustration, but also such that support for this import\n# order can be tested in the test suite (and that the associated\n# warning is triggered).\n# See note below in `get_rules()` for more details.\n# i.e. we DO NOT recommend importing here:\nfrom sqlfluff_plugin_example.rules import Rule_Example_L001  # noqa: F401\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: It is much better that we only import the rule on demand.\n    The root module of the plugin (i.e. this file which contains\n    all of the hook implementations) should have fully loaded before\n    we try and import the rules. This is partly for performance\n    reasons - but more because the definition of a BaseRule requires\n    that all of the get_configs_info() methods have both been\n    defined _and have run_ before so all the validation information\n    is available for the validation steps in the meta class.\n    \"\"\"\n    # i.e. we DO recommend importing here:\n    from sqlfluff_plugin_example.rules import Rule_Example_L001  # noqa: F811\n\n    return [Rule_Example_L001]\n\n\n@hookimpl\ndef load_default_config() -> dict[str, Any]:\n    \"\"\"Loads the default configuration for the plugin.\"\"\"\n    return load_config_resource(\n        package=\"sqlfluff_plugin_example\",\n        file_name=\"plugin_default_config.cfg\",\n    )\n\n\n@hookimpl\ndef get_configs_info() -> dict[str, dict[str, ConfigInfo]]:\n    \"\"\"Get rule config validations and descriptions.\"\"\"\n    return {\n        \"forbidden_columns\": {\"definition\": \"A list of column to forbid\"},\n    }\n"}, {"start_line": 0, "end_line": 1349, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Configuration and examples for individual rules.\"\"\"\n\nfrom sqlfluff.core.plugin.host import get_plugin_manager\nfrom sqlfluff.core.rules.base import (\n    BaseRule,\n    EvalResultType,\n    LintResult,\n    RuleGhost,\n    RulePack,\n    RuleSet,\n)\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.fix import LintFix\n\n\ndef _load_standard_rules() -> RuleSet:\n    \"\"\"Initialise the standard ruleset.\n\n    We do this on each call so that dynamic rules changes\n    are possible.\n    \"\"\"\n    std_rule_set = RuleSet(name=\"standard\", config_info=get_config_info())\n\n    # Iterate through the rules list and register each rule with the standard set.\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            std_rule_set.register(rule)\n\n    return std_rule_set\n\n\ndef get_ruleset(name: str = \"standard\") -> RuleSet:\n    \"\"\"Get a ruleset by name.\"\"\"\n    std_rules = _load_standard_rules()\n    lookup = {std_rules.name: std_rules}\n    # Return a copy in case someone modifies the register.\n    return lookup[name].copy()\n\n\n__all__ = (\n    \"get_ruleset\",\n    \"RuleSet\",\n    \"RulePack\",\n    \"BaseRule\",\n    \"LintResult\",\n    \"LintFix\",\n    \"RuleContext\",\n    \"RuleGhost\",\n    \"EvalResultType\",\n    \"ConfigInfo\",\n)\n"}, {"start_line": 1000, "end_line": 2695, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      \"validation\": [\"all\", \"aliases\", \"column_aliases\", \"table_aliases\", \"none\"],\n            \"definition\": \"Types of quoted identifiers to flag violations for.\",\n        },\n        \"allow_space_in_identifier\": {\n            \"validation\": [True, False],\n            \"definition\": (\"Should spaces in identifiers be allowed?\"),\n        },\n        \"additional_allowed_characters\": {\n            \"definition\": (\n                \"Optional list of extra allowed characters, \"\n                \"in addition to alphanumerics (A-Z, a-z, 0-9) and underscores.\"\n            ),\n        },\n        \"prefer_quoted_identifiers\": {\n            \"validation\": [True, False],\n            \"definition\": (\n                \"If ``True``, requires every identifier to be quoted. \"\n                \"Defaults to ``False``.\"\n            ),\n        },\n        \"prefer_quoted_keywords\": {\n            \"validation\": [True, False],\n            \"definition\": (\n                \"If ``True``, requires every keyword used as an identifier to be \"\n                \"quoted. Defaults to ``False``.\"\n            ),\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.references.RF01 import Rule_RF01\n    from sqlfluff.rules.references.RF02 import Rule_RF02\n    from sqlfluff.rules.references.RF03 import Rule_RF03\n    from sqlfluff.rules.references.RF04 import Rule_RF04\n    from sqlfluff.rules.references.RF05 import Rule_RF05\n    from sqlfluff.rules.references.RF06 import Rule_RF06\n\n    return [Rule_RF01, Rule_RF02, Rule_RF03, Rule_RF04, Rule_RF05, Rule_RF06]\n"}, {"start_line": 2000, "end_line": 3407, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "om the first quoted literal \"\n                \"in the file.\"\n            ),\n        },\n        \"preferred_type_casting_style\": {\n            \"validation\": [\"consistent\", \"shorthand\", \"convert\", \"cast\"],\n            \"definition\": (\"The expectation for using sql type casting\"),\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.convention.CV01 import Rule_CV01\n    from sqlfluff.rules.convention.CV02 import Rule_CV02\n    from sqlfluff.rules.convention.CV03 import Rule_CV03\n    from sqlfluff.rules.convention.CV04 import Rule_CV04\n    from sqlfluff.rules.convention.CV05 import Rule_CV05\n    from sqlfluff.rules.convention.CV06 import Rule_CV06\n    from sqlfluff.rules.convention.CV07 import Rule_CV07\n    from sqlfluff.rules.convention.CV08 import Rule_CV08\n    from sqlfluff.rules.convention.CV09 import Rule_CV09\n    from sqlfluff.rules.convention.CV10 import Rule_CV10\n    from sqlfluff.rules.convention.CV11 import Rule_CV11\n    from sqlfluff.rules.convention.CV12 import Rule_CV12\n\n    return [\n        Rule_CV01,\n        Rule_CV02,\n        Rule_CV03,\n        Rule_CV04,\n        Rule_CV05,\n        Rule_CV06,\n        Rule_CV07,\n        Rule_CV08,\n        Rule_CV09,\n        Rule_CV10,\n        Rule_CV11,\n        Rule_CV12,\n    ]\n"}], "retrieved_count": 10, "cost_time": 1.111750841140747}
{"question": "Why does SQLFluff implement a noqa comment system for rule suppression?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff implements a noqa comment system for rule suppression to provide fine-grained control over linting behavior and accommodate legitimate exceptions to rules. Key reasons include: 1) Selective suppression - Allows developers to suppress specific rules for specific lines or sections where the rule doesn't apply; 2) False positive handling - Enables suppression of rules that generate false positives in legitimate edge cases; 3) Legacy code support - Allows gradual adoption of SQLFluff by suppressing rules for existing code that doesn't meet new standards; 4) Context-specific exceptions - Enables suppression when business logic or external constraints require deviations from standard formatting; 5) Gradual migration - Supports incremental adoption of SQLFluff by allowing selective rule suppression during migration; 6) Documentation - Noqa comments serve as documentation explaining why certain rules are suppressed; 7) Team flexibility - Allows teams to maintain their own standards while using SQLFluff's core functionality; 8) Integration support - Enables SQLFluff to work with existing codebases that may have legitimate deviations from standard formatting; 9) Rule testing - Allows developers to test specific rules by suppressing others; 10) Compliance requirements - Enables compliance with organizational or regulatory requirements that may conflict with certain linting rules.", "score": null, "retrieved_content": [{"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "noqa_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "           \"exclude_rules\": \"LT13\",\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    sql = \"\"\"\nSELECT {{ col_a }} AS a -- noqa: TMP,PRS\nFROM foo;\n\"\"\"\n    result = lntr.lint_string(sql)\n    print(result.tree.stringify())\n    violations = result.get_violations()\n    assert not violations\n\n\ndef test_linter_noqa_disable():\n    \"\"\"Test \"noqa\" comments can be disabled via the config.\"\"\"\n    lntr_noqa_enabled = Linter(\n        config=FluffConfig(\n            overrides={\n                \"rules\": \"AL02\",\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    lntr_noqa_disabled = Linter(\n        config=FluffConfig(\n            overrides={\n                \"disable_noqa\": True,\n                \"rules\": \"AL02\",\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    # This query raises AL02, but it is being suppressed by the inline noqa comment.\n    # We can ignore this comment by setting disable_noqa = True in the config\n    # or by using the --disable-noqa flag in the CLI.\n    sql = \"\"\"\n    SELECT col_a a --noqa: AL02\n    FROM foo\n    \"\"\"\n\n    # Verify that noqa works as expected with disable_noqa = False (default).\n    result_noqa_enabled = lntr_noqa_enabled.lint_string(sql)\n    violations_noqa_enabled = result_noqa_enabled.get_violations()\n    assert len(violations_noqa_enabled) == 0\n\n    # Verify that noqa comment is ignored with disable_noqa = True.\n    result_noqa_disabled = lntr_noqa_disabled.lint_string(sql)\n    violations_noqa_disabled = result_noqa_disabled.get_violations()\n    assert len(violations_noqa_disabled) == 1\n    assert violations_noqa_disabled[0].rule.code == \"AL02\"\n\n\ndef test_linter_disable_noqa_except():\n    \"\"\"Test \"noqa\" comments can be disabled via the config.\"\"\"\n    lntr_disable_noqa_except_al02 = Linter(\n        config=FluffConfig(\n            overrides={\n                \"disable_noqa_except\": \"AL02\",\n                \"rules\": \"AL02, CP01\",\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "noqa_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for applying noqa directives and the IgnoreMask.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.errors import SQLBaseError, SQLParseError\nfrom sqlfluff.core.rules.noqa import IgnoreMask, NoQaDirective\n\n# noqa tests require a rule_set, therefore we construct dummy rule set for glob matching.\ndummy_rule_map = Linter().get_rulepack().reference_map\n\n\nclass DummyLintError(SQLBaseError):\n    \"\"\"Fake lint error used by tests, similar to SQLLintError.\"\"\"\n\n    def __init__(self, line_no: int, code: str = \"LT01\"):\n        self._code = code\n        super().__init__(line_no=line_no)\n\n\ndef test__linter__raises_malformed_noqa():\n    \"\"\"A badly formatted noqa gets raised as a parsing error.\"\"\"\n    lntr = Linter(dialect=\"ansi\")\n    result = lntr.lint_string_wrapped(\"select 1 --noqa missing semicolon\")\n\n    with pytest.raises(SQLParseError):\n        result.check_tuples()\n\n\n@pytest.mark.parametrize(\n    \"input,expected\",\n    [\n        (\"\", None),\n        (\"noqa\", NoQaDirective(0, 0, None, None, \"noqa\")),\n        (\"noqa?\", SQLParseError),\n        (\"noqa:\", NoQaDirective(0, 0, None, None, \"noqa:\")),\n        (\n            \"noqa:LT01,LT02\",\n            NoQaDirective(0, 0, (\"LT01\", \"LT02\"), None, \"noqa:LT01,LT02\"),\n        ),\n        (\n            \"noqa: enable=LT01\",\n            NoQaDirective(0, 0, (\"LT01\",), \"enable\", \"noqa: enable=LT01\"),\n        ),\n        (\n            \"noqa: disable=CP01\",\n            NoQaDirective(0, 0, (\"CP01\",), \"disable\", \"noqa: disable=CP01\"),\n        ),\n        (\n            \"noqa: disable=all\",\n            NoQaDirective(0, 0, None, \"disable\", \"noqa: disable=all\"),\n        ),\n        (\"noqa: disable\", SQLParseError),\n        (\n            \"Inline comment before inline ignore -- noqa:LT01,LT02\",\n            NoQaDirective(0, 0, (\"LT01\", \"LT02\"), None, \"noqa:LT01,LT02\"),\n        ),\n        # Test selection with rule globs\n        (\n            \"noqa:L04*\",\n            NoQaDirective(\n                0,\n"}, {"start_line": 17000, "end_line": 19000, "belongs_to": {"file_name": "noqa_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e-noqa flag in the CLI.\n    sql = \"\"\"\n    SELECT col_a a --noqa: AL02\n    FROM foo\n    \"\"\"\n\n    # Verify that noqa works as expected with disable_noqa = False (default).\n    result_noqa_enabled = lntr_noqa_enabled.lint_string(sql)\n    violations_noqa_enabled = result_noqa_enabled.get_violations()\n    assert len(violations_noqa_enabled) == 0\n\n    # Verify that noqa comment is ignored with disable_noqa = True.\n    result_noqa_disabled = lntr_noqa_disabled.lint_string(sql)\n    violations_noqa_disabled = result_noqa_disabled.get_violations()\n    assert len(violations_noqa_disabled) == 1\n    assert violations_noqa_disabled[0].rule.code == \"AL02\"\n\n\ndef test_linter_disable_noqa_except():\n    \"\"\"Test \"noqa\" comments can be disabled via the config.\"\"\"\n    lntr_disable_noqa_except_al02 = Linter(\n        config=FluffConfig(\n            overrides={\n                \"disable_noqa_except\": \"AL02\",\n                \"rules\": \"AL02, CP01\",\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    lntr_disable_noqa_except_core = Linter(\n        config=FluffConfig(\n            overrides={\n                \"disable_noqa_except\": \"core\",\n                \"rules\": \"AL02, CP01\",\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    # This query raises AL02 and CP01 but it is being suppressed by the inline noqa\n    # comments. We can partially ignore these comment by setting\n    # disable_noqa_except = rule_list in the config or by using the\n    # --disable-noqa-except option in the CLI.\n    sql = \"\"\"\n    SELECT\n    col_a a, --noqa: AL02\n    col_b b --noqa: aliasing\n    from foo; --noqa: CP01\n    \"\"\"\n\n    # Verify that noqa comment is ignored with\n    # disable_noqa_except = AL02 (base rule name).\n    result_disable_noqa_except_al02 = lntr_disable_noqa_except_al02.lint_string(sql)\n    violations_disable_noqa_except_al02 = (\n        result_disable_noqa_except_al02.get_violations()\n    )\n    assert len(violations_disable_noqa_except_al02) == 1\n    assert violations_disa"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "noqa.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines container classes for handling noqa comments.\"\"\"\n\nimport fnmatch\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Optional, Union, cast\n\nfrom sqlfluff.core.errors import SQLBaseError, SQLParseError, SQLUnusedNoQaWarning\nfrom sqlfluff.core.parser import BaseSegment, RawSegment, RegexLexer\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\n@dataclass\nclass NoQaDirective:\n    \"\"\"Parsed version of a 'noqa' comment.\"\"\"\n\n    line_no: int  # Source line number\n    line_pos: int  # Source line position\n    rules: Optional[tuple[str, ...]]  # Affected rule names\n    action: Optional[str]  # \"enable\", \"disable\", or \"None\"\n    raw_str: str = \"\"  # The raw representation of the directive for warnings.\n    used: bool = False  # Has it been used.\n\n    def _filter_violations_single_line(\n        self, violations: list[SQLBaseError]\n    ) -> list[SQLBaseError]:\n        \"\"\"Filter a list of violations based on this single line noqa.\n\n        Also record whether this class was _used_ in any of that filtering.\n\n        The \"ignore\" list is assumed to ONLY contain NoQaDirectives with\n        action=None.\n        \"\"\"\n        assert not self.action\n        matched_violations = [\n            v\n            for v in violations\n            if (\n                v.line_no == self.line_no\n                and (self.rules is None or v.rule_code() in self.rules)\n            )\n        ]\n        if matched_violations:\n            # Successful match, mark ignore as used.\n            self.used = True\n            return [v for v in violations if v not in matched_violations]\n        else:\n            return violations\n\n\nclass IgnoreMask:\n    \"\"\"Structure to hold a set of 'noqa' directives.\"\"\"\n\n    def __init__(self, ignores: list[NoQaDirective]):\n        self._ignore_list = ignores\n\n    def __repr__(self) -> str:  # pragma: no cover\n        return \"<IgnoreMask>\"\n\n    # ### Construction class methods.\n\n    @staticmethod\n   "}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "noqa.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "          if comment_remainder:\n                    action: Optional[str]\n                    if \"=\" in comment_remainder:\n                        action, rule_part = comment_remainder.split(\"=\", 1)\n                        if action not in {\"disable\", \"enable\"}:  # pragma: no cover\n                            return SQLParseError(\n                                \"Malformed 'noqa' section. \"\n                                \"Expected 'noqa: enable=<rule>[,...] | all' \"\n                                \"or 'noqa: disable=<rule>[,...] | all\",\n                                line_no=line_no,\n                            )\n                    else:\n                        action = None\n                        rule_part = comment_remainder\n                        if rule_part in {\"disable\", \"enable\"}:\n                            return SQLParseError(\n                                \"Malformed 'noqa' section. \"\n                                \"Expected 'noqa: enable=<rule>[,...] | all' \"\n                                \"or 'noqa: disable=<rule>[,...] | all\",\n                                line_no=line_no,\n                            )\n                    rules: Optional[tuple[str, ...]]\n                    if rule_part != \"all\":\n                        # Rules can be globs therefore we compare to the rule_set to\n                        # expand the globs.\n                        unexpanded_rules = tuple(\n                            r.strip() for r in rule_part.split(\",\")\n                        )\n                        # We use a set to do natural deduplication.\n                        expanded_rules: set[str] = set()\n                        for r in unexpanded_rules:\n                            matched = False\n                            for expanded in (\n                                reference_map[x]\n                                for x in fnmatch.filter(reference_map.keys(), r)\n                            ):\n                                expanded_rules |= expanded\n "}, {"start_line": 15000, "end_line": 17000, "belongs_to": {"file_name": "noqa_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\" feature to ignore PRS or TMP at the higher \"Linter\" level.\n\n    Because templating and parsing failures prevent a fully formed parse tree\n    to be formed the rely on slightly different routines to ensure ignores are\n    still applied.\n    \"\"\"\n    lntr = Linter(\n        config=FluffConfig(\n            overrides={\n                \"disable_noqa\": disable_noqa,\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.linter\"):\n        result = lntr.lint_string(sql)\n    violations = result.get_violations()\n    # In both the templating fail and parsing fail cases, the failures should be\n    # ignored because of the inline ignore, _unless_ `disable_noqa`` is set.\n    if disable_noqa:\n        assert violations\n    else:\n        assert not violations\n\n\ndef test_linter_noqa_tmp():\n    \"\"\"Test \"noqa\" feature to ignore TMP at the higher \"Linter\" level.\"\"\"\n    lntr = Linter(\n        config=FluffConfig(\n            overrides={\n                \"exclude_rules\": \"LT13\",\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    sql = \"\"\"\nSELECT {{ col_a }} AS a -- noqa: TMP,PRS\nFROM foo;\n\"\"\"\n    result = lntr.lint_string(sql)\n    print(result.tree.stringify())\n    violations = result.get_violations()\n    assert not violations\n\n\ndef test_linter_noqa_disable():\n    \"\"\"Test \"noqa\" comments can be disabled via the config.\"\"\"\n    lntr_noqa_enabled = Linter(\n        config=FluffConfig(\n            overrides={\n                \"rules\": \"AL02\",\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    lntr_noqa_disabled = Linter(\n        config=FluffConfig(\n            overrides={\n                \"disable_noqa\": True,\n                \"rules\": \"AL02\",\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    # This query raises AL02, but it is being suppressed by the inline noqa comment.\n    # We can ignore this comment by setting disable_noqa = True in the config\n    # or by using the --disabl"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "noqa_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ne),\n        (\"noqa\", NoQaDirective(0, 0, None, None, \"noqa\")),\n        (\"noqa?\", SQLParseError),\n        (\"noqa:\", NoQaDirective(0, 0, None, None, \"noqa:\")),\n        (\n            \"noqa:LT01,LT02\",\n            NoQaDirective(0, 0, (\"LT01\", \"LT02\"), None, \"noqa:LT01,LT02\"),\n        ),\n        (\n            \"noqa: enable=LT01\",\n            NoQaDirective(0, 0, (\"LT01\",), \"enable\", \"noqa: enable=LT01\"),\n        ),\n        (\n            \"noqa: disable=CP01\",\n            NoQaDirective(0, 0, (\"CP01\",), \"disable\", \"noqa: disable=CP01\"),\n        ),\n        (\n            \"noqa: disable=all\",\n            NoQaDirective(0, 0, None, \"disable\", \"noqa: disable=all\"),\n        ),\n        (\"noqa: disable\", SQLParseError),\n        (\n            \"Inline comment before inline ignore -- noqa:LT01,LT02\",\n            NoQaDirective(0, 0, (\"LT01\", \"LT02\"), None, \"noqa:LT01,LT02\"),\n        ),\n        # Test selection with rule globs\n        (\n            \"noqa:L04*\",\n            NoQaDirective(\n                0,\n                0,\n                (\n                    \"AM04\",  # L044 is an alias of AM04\n                    \"CP04\",  # L040 is an alias of CP04\n                    \"CV04\",  # L047 is an alias of CV04\n                    \"CV05\",  # L049 is an alias of CV05\n                    \"JJ01\",  # L046 is an alias of JJ01\n                    \"LT01\",  # L048 is an alias of LT01\n                    \"LT10\",  # L041 is an alias of LT10\n                    \"ST02\",  # L043 is an alias of ST02\n                    \"ST03\",  # L045 is an alias of ST03\n                    \"ST05\",  # L042 is an alias of ST05\n                ),\n                None,\n                \"noqa:L04*\",\n            ),\n        ),\n        # Test selection with aliases.\n        (\n            \"noqa:L002\",\n            NoQaDirective(0, 0, (\"LT02\",), None, \"noqa:L002\"),\n        ),\n        # Test selection with alias globs.\n        (\n            \"noqa:L00*\",\n            NoQaDirective(\n                0,\n                0,\n                "}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "noqa_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "u u -- Some comment --noqa: L01*\n        , col_v v -- We can ignore both AL02 and LT04 -- noqa: L01[29]\n    FROM foo\n        \"\"\"\n    result = lntr.lint_string(sql)\n    violations = result.get_violations()\n    assert {3, 6, 7, 8, 10, 12, 13, 14, 15, 18} == {v.line_no for v in violations}\n\n\ndef test_linter_noqa_with_templating():\n    \"\"\"Similar to test_linter_noqa, but uses templating (Jinja).\"\"\"\n    lntr = Linter(\n        config=FluffConfig(\n            overrides={\n                \"dialect\": \"bigquery\",  # Use bigquery to allow hash comments.\n                \"templater\": \"jinja\",\n                \"rules\": \"LT05\",\n            }\n        )\n    )\n    sql = \"\\n\"\n    '\"{%- set a_var = [\"1\", \"2\"] -%}\\n'\n    \"SELECT\\n\"\n    \"  this_is_just_a_very_long_line_for_demonstration_purposes_of_a_bug_involving_\"\n    \"templated_sql_files, --noqa: LT05\\n\"\n    \"  this_is_not_so_big a, --Inline comment --noqa: AL02\\n\"\n    \"  this_is_not_so_big b, /* Block comment */ --noqa: AL02\\n\"\n    \"  this_is_not_so_big c, # hash comment --noqa: AL02\\n\"\n    \"  this_is_just_a_very_long_line_for_demonstration_purposes_of_a_bug_involving_\"\n    \"templated_sql_files, --noqa: L01*\\n\"\n    \"FROM\\n\"\n    \"  a_table\\n\"\n    \"    \"\n    result = lntr.lint_string(sql)\n    assert not result.get_violations()\n\n\ndef test_linter_noqa_template_errors():\n    \"\"\"Similar to test_linter_noqa, but uses templating (Jinja).\"\"\"\n    lntr = Linter(\n        config=FluffConfig(\n            overrides={\n                \"templater\": \"jinja\",\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    sql = \"\"\"select * --noqa: TMP\nfrom raw\nwhere\n    balance_date >= {{ execution_date - macros.timedelta() }}  --noqa: TMP\n\"\"\"\n    result = lntr.lint_string(sql)\n    assert not result.get_violations()\n\n\n@pytest.mark.parametrize(\"disable_noqa\", [True, False])\n@pytest.mark.parametrize(\n    \"sql\", [\"SELEC * FROM foo -- noqa: PRS\\n\", \"{% if 1 > '2' %} -- noqa: TMP\\n\"]\n)\ndef test_linter_noqa_prs(sql, disable_noqa, caplog):\n    \"\"\"Test \"noqa"}, {"start_line": 18000, "end_line": 19381, "belongs_to": {"file_name": "noqa_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    lntr_disable_noqa_except_core = Linter(\n        config=FluffConfig(\n            overrides={\n                \"disable_noqa_except\": \"core\",\n                \"rules\": \"AL02, CP01\",\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    # This query raises AL02 and CP01 but it is being suppressed by the inline noqa\n    # comments. We can partially ignore these comment by setting\n    # disable_noqa_except = rule_list in the config or by using the\n    # --disable-noqa-except option in the CLI.\n    sql = \"\"\"\n    SELECT\n    col_a a, --noqa: AL02\n    col_b b --noqa: aliasing\n    from foo; --noqa: CP01\n    \"\"\"\n\n    # Verify that noqa comment is ignored with\n    # disable_noqa_except = AL02 (base rule name).\n    result_disable_noqa_except_al02 = lntr_disable_noqa_except_al02.lint_string(sql)\n    violations_disable_noqa_except_al02 = (\n        result_disable_noqa_except_al02.get_violations()\n    )\n    assert len(violations_disable_noqa_except_al02) == 1\n    assert violations_disable_noqa_except_al02[0].rule.code == \"CP01\"\n\n    # Verify that noqa works as expected with disable_noqa_except = core (rule alias).\n    result_disable_noqa_except_core = lntr_disable_noqa_except_core.lint_string(sql)\n    violations_disable_noqa_except_core = (\n        result_disable_noqa_except_core.get_violations()\n    )\n    assert len(violations_disable_noqa_except_core) == 0\n"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "noqa_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "eature at the higher \"Linter\" level.\"\"\"\n    lntr = Linter(\n        config=FluffConfig(\n            overrides={\n                \"dialect\": \"bigquery\",  # Use bigquery to allow hash comments.\n                \"rules\": \"AL02, LT04\",\n            }\n        )\n    )\n    sql = \"\"\"\n    SELECT\n        col_a a,\n        col_b b, --noqa: disable=AL02\n        col_c c,\n        col_d d, --noqa: enable=AL02\n        col_e e,\n        col_f f,\n        col_g g,  --noqa\n        col_h h,\n        col_i i, --noqa:AL02\n        col_j j,\n        col_k k, --noqa:AL03\n        col_l l,\n        col_m m,\n        col_n n, --noqa: disable=all\n        col_o o,\n        col_p p, --noqa: enable=all\n        col_q q, --Inline comment --noqa: AL02\n        col_r r, /* Block comment */ --noqa: AL02\n        col_s s # hash comment --noqa: AL02\n        -- We trigger both AL02 (implicit aliasing)\n        -- and LT04 (leading commas) here to\n        -- test glob ignoring of multiple rules.\n        , col_t t --noqa: L01*\n        , col_u u -- Some comment --noqa: L01*\n        , col_v v -- We can ignore both AL02 and LT04 -- noqa: L01[29]\n    FROM foo\n        \"\"\"\n    result = lntr.lint_string(sql)\n    violations = result.get_violations()\n    assert {3, 6, 7, 8, 10, 12, 13, 14, 15, 18} == {v.line_no for v in violations}\n\n\ndef test_linter_noqa_with_templating():\n    \"\"\"Similar to test_linter_noqa, but uses templating (Jinja).\"\"\"\n    lntr = Linter(\n        config=FluffConfig(\n            overrides={\n                \"dialect\": \"bigquery\",  # Use bigquery to allow hash comments.\n                \"templater\": \"jinja\",\n                \"rules\": \"LT05\",\n            }\n        )\n    )\n    sql = \"\\n\"\n    '\"{%- set a_var = [\"1\", \"2\"] -%}\\n'\n    \"SELECT\\n\"\n    \"  this_is_just_a_very_long_line_for_demonstration_purposes_of_a_bug_involving_\"\n    \"templated_sql_files, --noqa: LT05\\n\"\n    \"  this_is_not_so_big a, --Inline comment --noqa: AL02\\n\"\n    \"  this_is_not_so_big b, /* Block comment */ --noqa: AL02\\n\"\n    \"  this_is_not_so_big c"}], "retrieved_count": 10, "cost_time": 1.094604253768921}
{"question": "What dependencies exist between SQLFluff's LintedFile and LintedDir classes?", "answer": null, "relative_code_list": null, "ground_truth": "LintedFile and LintedDir have a container-content dependency relationship where LintedDir depends on LintedFile. Key dependencies include: 1) Container dependency - LintedDir contains a list of LintedFile objects (self.files: list[LintedFile]) and manages them as a collection; 2) Import dependency - LintedDir imports LintedFile from sqlfluff.core.linter.linted_file module; 3) Add method dependency - LintedDir.add() method takes a LintedFile parameter and processes it to extract metadata and statistics; 4) Data extraction dependency - LintedDir extracts data from LintedFile objects including violations, statistics, timings, and file status; 5) Statistics aggregation - LintedDir aggregates statistics across multiple LintedFile objects (num_files, num_clean, num_unclean, num_violations); 6) Record management - LintedDir creates LintingRecord objects from LintedFile data for serialization and reporting; 7) Memory management - LintedDir can optionally retain or discard LintedFile objects based on retain_files parameter to manage memory usage; 8) Error tracking - LintedDir tracks template and parsing errors from LintedFile objects; 9) Timing aggregation - LintedDir aggregates timing information from multiple LintedFile objects; 10) Path management - LintedDir manages files that share a common root path, while each LintedFile represents a single file. The dependency is unidirectional - LintedFile does not depend on LintedDir, but LintedDir cannot function without LintedFile objects.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linted_dir.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the LintedDir class.\n\nThis stores the idea of a collection of linted files at a single start path\n\n\"\"\"\n\nfrom collections.abc import Iterable\nfrom typing import Optional, TypedDict, Union\n\nfrom sqlfluff.core.errors import (\n    CheckTuple,\n    SerializedObject,\n    SQLBaseError,\n    SQLLintError,\n)\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.linter.linted_file import TMP_PRS_ERROR_TYPES, LintedFile\nfrom sqlfluff.core.parser.segments.base import BaseSegment\n\n\nclass LintingRecord(TypedDict):\n    \"\"\"A class to store the linted file statistics.\"\"\"\n\n    filepath: str\n    violations: list[SerializedObject]\n    # Things like file length\n    statistics: dict[str, int]\n    # Raw timings, in seconds, for both rules and steps\n    timings: dict[str, float]\n\n\nclass LintedDir:\n    \"\"\"A class to store the idea of a collection of linted files at a single start path.\n\n    A LintedDir may contain files in subdirectories, but they all share\n    a common root.\n\n    Importantly, this class also abstracts away from the given LintedFile\n    object and allows us to either _keep_ those objects for later use, or\n    extract the results from them and allow the original object to be discarded\n    and save memory overhead if not required.\n    \"\"\"\n\n    def __init__(self, path: str, retain_files: bool = True) -> None:\n        self.files: list[LintedFile] = []\n        self.path: str = path\n        self.retain_files: bool = retain_files\n        # Records\n        self._records: list[LintingRecord] = []\n        # Stats\n        self._num_files: int = 0\n        self._num_clean: int = 0\n        self._num_unclean: int = 0\n        self._num_violations: int = 0\n        self.num_unfiltered_tmp_prs_errors: int = 0\n        self._unfiltered_tmp_prs_errors_map: dict[str, int] = {}\n        self.num_tmp_prs_errors: int = 0\n        self.num_unfixable_lint_errors: int = 0\n        # Timing\n        self.step_timings: list[dict[str, float]] = []\n        self.rule_timings: list[t"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linting_result.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the linter class.\"\"\"\n\nimport csv\nimport time\nfrom collections.abc import Iterable, Mapping\nfrom typing import TYPE_CHECKING, Any, Optional, TypeVar, Union\n\nfrom sqlfluff.core.errors import CheckTuple, SQLBaseError\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.linter.linted_dir import LintedDir, LintingRecord\nfrom sqlfluff.core.timing import RuleTimingSummary, TimingSummary\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.parser.segments.base import BaseSegment\n\n\ndef sum_dicts(d1: Mapping[str, int], d2: Mapping[str, int]) -> dict[str, int]:\n    \"\"\"Take the keys of two dictionaries and add their values.\"\"\"\n    keys = set(d1.keys()) | set(d2.keys())\n    return {key: d1.get(key, 0) + d2.get(key, 0) for key in keys}\n\n\nT = TypeVar(\"T\")\n\n\ndef combine_dicts(*d: dict[str, T]) -> dict[str, T]:\n    \"\"\"Take any set of dictionaries and combine them.\"\"\"\n    dict_buffer: dict[str, T] = {}\n    for dct in d:\n        dict_buffer.update(dct)\n    return dict_buffer\n\n\nclass LintingResult:\n    \"\"\"A class to represent the result of a linting operation.\n\n    Notably this might be a collection of paths, all with multiple\n    potential files within them.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.paths: list[LintedDir] = []\n        self._start_time: float = time.monotonic()\n        self.total_time: float = 0.0\n\n    def add(self, path: LintedDir) -> None:\n        \"\"\"Add a new `LintedDir` to this result.\"\"\"\n        self.paths.append(path)\n\n    def stop_timer(self) -> None:\n        \"\"\"Stop the linting timer.\"\"\"\n        self.total_time = time.monotonic() - self._start_time\n\n    def check_tuples(\n        self, raise_on_non_linting_violations: bool = True\n    ) -> list[CheckTuple]:\n        \"\"\"Fetch all check_tuples from all contained `LintedDir` objects.\n\n        Returns:\n            A list of check tuples.\n        \"\"\"\n        return [\n            t\n            for path in self.paths\n            for t in path.check_tuples(\n            "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linter_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/test", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The Test file for the linter class.\"\"\"\n\nimport os\nimport os.path\nimport shutil\n\nimport pytest\n\nfrom sqlfluff.cli.commands import lint\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.utils.testing.cli import invoke_assert_code\n\n\n@pytest.mark.parametrize(\n    \"path\", [\"models/my_new_project/disabled_model.sql\", \"macros/echo.sql\"]\n)\ndef test__linter__skip_file(path, project_dir, dbt_fluff_config):  # noqa\n    \"\"\"Test that the linter skips disabled dbt models and macros.\"\"\"\n    conf = FluffConfig(configs=dbt_fluff_config)\n    lntr = Linter(config=conf)\n    model_file_path = os.path.join(project_dir, path)\n    linted_path = lntr.lint_path(path=model_file_path)\n    # Check that the file is still there\n    assert len(linted_path.files) == 1\n    linted_file = linted_path.files[0]\n    # Normalise paths to control for OS variance\n    assert os.path.normpath(linted_file.path) == os.path.normpath(model_file_path)\n    assert not linted_file.templated_file\n    assert not linted_file.tree\n\n\ndef test__linter__lint_ephemeral_3_level(project_dir, dbt_fluff_config):\n    \"\"\"Test linter can lint a project with 3-level ephemeral dependencies.\"\"\"\n    # This was previously crashing inside dbt, in a function named\n    # inject_ctes_into_sql(). (issue 2671).\n    conf = FluffConfig(configs=dbt_fluff_config)\n    lntr = Linter(config=conf)\n    model_file_path = os.path.join(project_dir, \"models/ephemeral_3_level\")\n    lntr.lint_path(path=model_file_path)\n\n\ndef test__linter__config_pairs(dbt_fluff_config):  # noqa\n    \"\"\"Test that the dbt templater returns version information in it's config.\"\"\"\n    conf = FluffConfig(configs=dbt_fluff_config)\n    lntr = Linter(config=conf)\n    # NOTE: This method is called within the config readout.\n    assert lntr.templater.config_pairs() == [\n        (\"templater\", \"dbt\"),\n        (\"dbt\", lntr.templater.dbt_version),\n    ]\n\n\ndef test_dbt_target_dir(tmpdir, dbt_project_folder, profiles_dir):\n    \"\"\"Test with dbt project in subdir that target/ is c"}, {"start_line": 0, "end_line": 489, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Linter class and helper classes.\"\"\"\n\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.linter.common import ParsedString, RenderedFile, RuleTuple\nfrom sqlfluff.core.linter.linted_file import LintedFile\nfrom sqlfluff.core.linter.linter import Linter\nfrom sqlfluff.core.linter.linting_result import LintingResult\n\n__all__ = (\n    \"FormatterInterface\",\n    \"RuleTuple\",\n    \"ParsedString\",\n    \"LintedFile\",\n    \"LintingResult\",\n    \"Linter\",\n    \"RenderedFile\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linter_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the Linter class and LintingResult class.\"\"\"\n\nimport logging\nimport os\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom sqlfluff.cli.formatters import OutputStreamFormatter\nfrom sqlfluff.cli.outputstream import make_output_stream\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLFluffSkipFile,\n    SQLLexError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.linter import runner\nfrom sqlfluff.core.linter.linting_result import combine_dicts, sum_dicts\nfrom sqlfluff.core.linter.runner import get_runner\nfrom sqlfluff.utils.testing.logging import fluff_log_catcher\n\n\nclass DummyLintError(SQLBaseError):\n    \"\"\"Fake lint error used by tests, similar to SQLLintError.\"\"\"\n\n    def __init__(self, line_no: int, code: str = \"LT01\"):\n        self._code = code\n        super().__init__(line_no=line_no)\n\n\ndef normalise_paths(paths):\n    \"\"\"Test normalising paths.\n\n    NB Paths on difference platforms might look different, so this\n    makes them comparable.\n    \"\"\"\n    return {pth.replace(\"/\", \".\").replace(\"\\\\\", \".\") for pth in paths}\n\n\n@pytest.mark.parametrize(\"filesize,raises_skip\", [(0, False), (5, True), (2000, False)])\ndef test__linter__skip_large_bytes(filesize, raises_skip):\n    \"\"\"Test extracting paths from a file path.\"\"\"\n    config = FluffConfig(\n        overrides={\"large_file_skip_byte_limit\": filesize, \"dialect\": \"ansi\"}\n    )\n    # First check the function directly\n    if raises_skip:\n        with pytest.raises(SQLFluffSkipFile) as excinfo:\n            Linter.load_raw_file_and_config(\n                \"test/fixtures/linter/indentation_errors.sql\", config\n            )\n        assert \"Skipping\" in str(excinfo.value)\n        assert f\"over the limit of {filesize}\" in str(excinfo.value)\n    # If NOT raises, then we'll catch the raise an error and the test will fail.\n\n    # Then check that it either is or isn't linted appropriately via lint_paths.\n    lntr = Linter(config)\n"}, {"start_line": 0, "end_line": 38, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.core.linter.\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the linter class.\"\"\"\n\nimport fnmatch\nimport logging\nimport os\nimport time\nfrom collections.abc import Iterator, Sequence\nfrom typing import TYPE_CHECKING, Optional, cast\n\nimport regex\nfrom tqdm import tqdm\n\nfrom sqlfluff.core.config import FluffConfig, progress_bar_configuration\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLFluffSkipFile,\n    SQLLexError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.helpers.file import get_encoding\nfrom sqlfluff.core.linter.common import (\n    ParsedString,\n    ParsedVariant,\n    RenderedFile,\n    RuleTuple,\n)\nfrom sqlfluff.core.linter.discovery import paths_from_path\nfrom sqlfluff.core.linter.fix import apply_fixes, compute_anchor_edit_info\nfrom sqlfluff.core.linter.linted_dir import LintedDir\nfrom sqlfluff.core.linter.linted_file import (\n    TMP_PRS_ERROR_TYPES,\n    FileTimings,\n    LintedFile,\n)\nfrom sqlfluff.core.linter.linting_result import LintingResult\nfrom sqlfluff.core.parser import Lexer, Parser\nfrom sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\nfrom sqlfluff.core.rules import BaseRule, RulePack, get_ruleset\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.rules.noqa import IgnoreMask\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.meta import MetaSegment\n    from sqlfluff.core.templaters import RawTemplater, TemplatedFile\n\n\nRuleTimingsType = list[tuple[str, str, float]]\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass Linter:\n    \"\"\"The interface class to interact with the linter.\"\"\"\n\n    # Default to allowing process parallelism\n    allow_process_parallelism = True\n\n    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n        dialect: Optional[str] = None,\n  "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linted_file.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the LintedFile class.\n\nThis holds linting results for a single file, and also\ncontains all of the routines to apply fixes to that file\npost linting.\n\"\"\"\n\nimport logging\nimport os\nimport shutil\nimport stat\nimport tempfile\nfrom collections import defaultdict\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass\nfrom typing import NamedTuple, Optional, Union\n\nfrom sqlfluff.core.errors import (\n    CheckTuple,\n    SQLBaseError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.linter.patch import FixPatch, generate_source_patches\n\n# Classes needed only for type checking\nfrom sqlfluff.core.parser.segments import BaseSegment\nfrom sqlfluff.core.rules.noqa import IgnoreMask\nfrom sqlfluff.core.templaters import RawFileSlice, TemplatedFile\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\nTMP_PRS_ERROR_TYPES = (SQLTemplaterError, SQLParseError)\n\n\n@dataclass\nclass FileTimings:\n    \"\"\"A dataclass for holding the timings information for a file.\"\"\"\n\n    step_timings: dict[str, float]\n    # NOTE: Because rules may run more than once for any\n    # given file we record each run and then we can post\n    # process this as we wish later.\n    rule_timings: list[tuple[str, str, float]]\n\n    def __repr__(self) -> str:  # pragma: no cover\n        return \"<FileTimings>\"\n\n    def get_rule_timing_dict(self) -> dict[str, float]:\n        \"\"\"Generate a summary to total time in each rule.\n\n        This is primarily for csv export.\n        \"\"\"\n        total_times: dict[str, float] = defaultdict(float)\n\n        for code, _, time in self.rule_timings:\n            total_times[code] += time\n\n        # Return as plain dict\n        return dict(total_times.items())\n\n\nclass LintedFile(NamedTuple):\n    \"\"\"A class to store the idea of a linted file.\"\"\"\n\n    path: str\n    violations: list[SQLBaseError]\n    timings: Optional[FileTimings]\n    tre"}, {"start_line": 35000, "end_line": 37000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "the excluded rules\n        return {k: v.intersection(noqa_set) for k, v in output_map.items()}\n\n    @classmethod\n    def lint_rendered(\n        cls,\n        rendered: RenderedFile,\n        rule_pack: RulePack,\n        fix: bool = False,\n        formatter: Optional[FormatterInterface] = None,\n    ) -> LintedFile:\n        \"\"\"Take a RenderedFile and return a LintedFile.\"\"\"\n        parsed = cls.parse_rendered(rendered)\n        return cls.lint_parsed(\n            parsed,\n            rule_pack=rule_pack,\n            fix=fix,\n            formatter=formatter,\n            encoding=rendered.encoding,\n        )\n\n    # ### Instance Methods\n    # These are tied to a specific instance and so are not necessarily\n    # safe to use in parallel operations.\n\n    def render_string(\n        self, in_str: str, fname: str, config: FluffConfig, encoding: str\n    ) -> RenderedFile:\n        \"\"\"Template the file.\"\"\"\n        linter_logger.info(\"Rendering String [%s] (%s)\", self.templater.name, fname)\n\n        # Start the templating timer\n        t0 = time.monotonic()\n\n        # Newlines are normalised to unix-style line endings (\\n).\n        # The motivation is that Jinja normalises newlines during templating and\n        # we want consistent mapping between the raw and templated slices.\n        in_str = self._normalise_newlines(in_str)\n\n        # Since Linter.__init__() does not require a dialect to be specified,\n        # check for one now. (We're processing a string, not a file, so we're\n        # not going to pick up a .sqlfluff or other config file to provide a\n        # missing dialect at this point.)\n        config.verify_dialect_specified()\n        if not config.get(\"templater_obj\") == self.templater:\n            linter_logger.warning(\n                f\"Attempt to set templater to {config.get('templater_obj').name} \"\n                f\"failed. Using {self.templater.name} templater. Templater cannot \"\n                \"be set in a .sqlfluff file in a subdirectory of the current \"\n         "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "linting_result.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "dict_buffer\n\n\nclass LintingResult:\n    \"\"\"A class to represent the result of a linting operation.\n\n    Notably this might be a collection of paths, all with multiple\n    potential files within them.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.paths: list[LintedDir] = []\n        self._start_time: float = time.monotonic()\n        self.total_time: float = 0.0\n\n    def add(self, path: LintedDir) -> None:\n        \"\"\"Add a new `LintedDir` to this result.\"\"\"\n        self.paths.append(path)\n\n    def stop_timer(self) -> None:\n        \"\"\"Stop the linting timer.\"\"\"\n        self.total_time = time.monotonic() - self._start_time\n\n    def check_tuples(\n        self, raise_on_non_linting_violations: bool = True\n    ) -> list[CheckTuple]:\n        \"\"\"Fetch all check_tuples from all contained `LintedDir` objects.\n\n        Returns:\n            A list of check tuples.\n        \"\"\"\n        return [\n            t\n            for path in self.paths\n            for t in path.check_tuples(\n                raise_on_non_linting_violations=raise_on_non_linting_violations\n            )\n        ]\n\n    def check_tuples_by_path(self) -> dict[str, list[CheckTuple]]:\n        \"\"\"Fetch all check_tuples from all contained `LintedDir` objects.\n\n        Returns:\n            A dict, with lists of tuples grouped by path.\n        \"\"\"\n        buff: dict[str, list[CheckTuple]] = {}\n        for path in self.paths:\n            buff.update(path.check_tuples_by_path())\n        return buff\n\n    def num_violations(\n        self,\n        types: Optional[Union[type[SQLBaseError], Iterable[type[SQLBaseError]]]] = None,\n        fixable: Optional[bool] = None,\n    ) -> int:\n        \"\"\"Count the number of violations in the result.\"\"\"\n        return sum(\n            path.num_violations(types=types, fixable=fixable) for path in self.paths\n        )\n\n    def get_violations(\n        self, rules: Optional[Union[str, tuple[str, ...]]] = None\n    ) -> list[SQLBaseError]:\n        \"\"\"Return a list of violations in the res"}], "retrieved_count": 10, "cost_time": 1.1232070922851562}
{"question": "Why does SQLFluff provide a fix generation system for automatic code correction?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff provides a fix generation system for automatic code correction to enhance developer productivity and ensure consistent SQL formatting. Key reasons include: 1) Developer productivity - Automatically fixing common formatting issues saves developers time and reduces manual formatting work; 2) Consistency enforcement - Automatic fixes ensure consistent SQL formatting across teams and projects without requiring manual intervention; 3) Error reduction - Automated fixes reduce the chance of human error in manual formatting; 4) Immediate feedback - Developers can see and apply fixes immediately during development rather than waiting for code reviews; 5) CI/CD integration - Automatic fixes can be integrated into continuous integration pipelines to maintain code quality; 6) Learning tool - The fix system helps developers learn proper SQL formatting by showing them what changes are needed; 7) Rule compliance - Fixes ensure that SQL code complies with configured linting rules automatically; 8) Batch processing - Multiple files can be automatically fixed in a single operation; 9) Selective fixing - Developers can choose which rules to apply fixes for, allowing for gradual adoption; 10) Quality assurance - Automatic fixes help maintain high code quality standards across large codebases.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "patch.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Helpers for generating patches to fix files.\"\"\"\n\nimport logging\nfrom collections.abc import Iterator\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nfrom sqlfluff.core.parser import BaseSegment\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.templaters import TemplatedFile\n\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\n\n@dataclass\nclass FixPatch:\n    \"\"\"An edit patch for a source file.\"\"\"\n\n    templated_slice: slice\n    fixed_raw: str\n    # The patch category, functions mostly for debugging and explanation\n    # than for function. It allows traceability of *why* this patch was\n    # generated. It has no significance for processing.\n    patch_category: str\n    source_slice: slice\n    templated_str: str\n    source_str: str\n\n    def dedupe_tuple(self) -> tuple[slice, str]:\n        \"\"\"Generate a tuple of this fix for deduping.\"\"\"\n        return (self.source_slice, self.fixed_raw)\n\n\ndef _iter_source_fix_patches(\n    segment: BaseSegment, templated_file: TemplatedFile\n) -> Iterator[FixPatch]:\n    \"\"\"Yield any source patches as fixes now.\n\n    NOTE: This yields source fixes for the segment and any of its\n    children, so it's important to call it at the right point in\n    the recursion to avoid yielding duplicates.\n    \"\"\"\n    for source_fix in segment.source_fixes:\n        yield FixPatch(\n            source_fix.templated_slice,\n            source_fix.edit,\n            patch_category=\"source\",\n            source_slice=source_fix.source_slice,\n            templated_str=templated_file.templated_str[source_fix.templated_slice],\n            source_str=templated_file.source_str[source_fix.source_slice],\n        )\n\n\ndef _iter_templated_patches(\n    segment: BaseSegment, templated_file: TemplatedFile\n) -> Iterator[FixPatch]:\n    \"\"\"Iterate through the segments generating fix patches.\n\n    The patches are generated in TEMPLATED space. This is important\n    so that we defer dealing with any loops until later. At this stage\n    ev"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fix.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the LintFix class, returned by rules when recommending a fix.\"\"\"\n\nimport logging\nfrom collections.abc import Iterable, Sized\nfrom itertools import chain\nfrom typing import Any, Optional, cast\n\nfrom sqlfluff.core.parser import BaseSegment, PositionMarker, RawSegment, SourceFix\nfrom sqlfluff.core.templaters import RawFileSlice, TemplatedFile\n\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\n\nclass LintFix:\n    \"\"\"A class to hold a potential fix to a linting violation.\n\n    Args:\n        edit_type (:obj:`str`): One of `create_before`, `create_after`,\n            `replace`, `delete` to indicate the kind of fix this represents.\n        anchor (:obj:`BaseSegment`): A segment which represents\n            the *position* that this fix should be applied at. For deletions\n            it represents the segment to delete, for creations it implies the\n            position to create at (with the existing element at this position\n            to be moved *after* the edit), for a `replace` it implies the\n            segment to be replaced.\n        edit (iterable of :obj:`BaseSegment`, optional): For `replace` and\n            `create` fixes, this holds the iterable of segments to create\n            or replace at the given `anchor` point.\n        source (iterable of :obj:`BaseSegment`, optional): For `replace` and\n            `create` fixes, this holds iterable of segments that provided\n            code. IMPORTANT: The linter uses this to prevent copying material\n            from templated areas.\n    \"\"\"\n\n    def __init__(\n        self,\n        edit_type: str,\n        anchor: BaseSegment,\n        edit: Optional[Iterable[BaseSegment]] = None,\n        source: Optional[Iterable[BaseSegment]] = None,\n    ) -> None:\n        if edit_type not in (\n            \"create_before\",\n            \"create_after\",\n            \"replace\",\n            \"delete\",\n        ):  # pragma: no cover\n            raise ValueError(f\"Unexpected edit_type: {edit_type}\")\n        self.edit_type = edit_type\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fix.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Helper classes & methods for applying fixes to segments.\"\"\"\n\nimport logging\nfrom collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlfluff.core.parser import BaseSegment, SourceFix\nfrom sqlfluff.core.rules.fix import LintFix\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n\n\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\n\n@dataclass\nclass AnchorEditInfo:\n    \"\"\"For a given fix anchor, count of the fix edit types and fixes for it.\"\"\"\n\n    delete: int = field(default=0)\n    replace: int = field(default=0)\n    create_before: int = field(default=0)\n    create_after: int = field(default=0)\n    fixes: list[\"LintFix\"] = field(default_factory=list)\n    source_fixes: list[SourceFix] = field(default_factory=list)\n    # First fix of edit_type \"replace\" in \"fixes\"\n    _first_replace: Optional[\"LintFix\"] = field(default=None)\n\n    def add(self, fix: \"LintFix\") -> None:\n        \"\"\"Adds the fix and updates stats.\n\n        We also allow potentially multiple source fixes on the same\n        anchor by condensing them together here.\n        \"\"\"\n        if fix in self.fixes:\n            # Deduplicate fixes in case it's already in there.\n            return\n\n        if fix.is_just_source_edit():\n            assert fix.edit\n            # is_just_source_edit confirms there will be a list\n            # so we can hint that to mypy.\n            self.source_fixes += fix.edit[0].source_fixes\n            # is there already a replace?\n            if self._first_replace:\n                assert self._first_replace.edit\n                # is_just_source_edit confirms there will be a list\n                # and that's the only way to get into _first_replace\n                # if it's populated so we can hint that to mypy.\n                linter_logger.info(\n                    \"Multiple edits detected, condensing %s onto %s\",\n                    fix,\n                    self._first_repl"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fix_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test routines for fixing errors.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core.linter.fix import compute_anchor_edit_info\nfrom sqlfluff.core.linter.patch import FixPatch, generate_source_patches\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments import (\n    BaseSegment,\n    RawSegment,\n    TemplateSegment,\n)\nfrom sqlfluff.core.parser.segments.raw import SourceFix\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters import RawFileSlice, TemplatedFile\nfrom sqlfluff.core.templaters.base import TemplatedFileSlice\n\n\n@pytest.fixture(scope=\"module\")\ndef raw_segments(generate_test_segments):\n    \"\"\"Construct a list of raw segments as a fixture.\"\"\"\n    return generate_test_segments([\"foobar\", \".barfoo\"])\n\n\ndef test__rules_base_segments_compute_anchor_edit_info(raw_segments):\n    \"\"\"Test BaseSegment.compute_anchor_edit_info().\"\"\"\n    # Construct a fix buffer, intentionally with:\n    # - one duplicate.\n    # - two different incompatible fixes on the same segment.\n    fixes = [\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"a\")]),\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"a\")]),\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"b\")]),\n    ]\n    anchor_info_dict = compute_anchor_edit_info(fixes)\n    # Check the target segment is the only key we have.\n    assert list(anchor_info_dict.keys()) == [raw_segments[0].uuid]\n    anchor_info = anchor_info_dict[raw_segments[0].uuid]\n    # Check that the duplicate as been deduplicated.\n    # i.e. this isn't 3.\n    assert anchor_info.replace == 2\n    # Check the fixes themselves.\n    # NOTE: There's no duplicated first fix.\n    assert anchor_info.fixes == [\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"a\")]),\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"b\")]),\n    ]\n    # Check the first replace\n    assert anchor_info._first_replace == LintFix.replace(\n        r"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "std_fix_auto_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Automated tests for fixing violations.\n\nAny files in the test/fixtures/linter/autofix directory will be picked up\nand automatically tested against the appropriate dialect.\n\"\"\"\n\nimport json\nimport logging\nimport os\nimport shutil\nimport tempfile\nfrom typing import Optional\n\nimport pytest\nimport yaml\n\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.config import clear_config_caches\n\n# Construct the tests from the filepath\ntest_cases = []\nbase_auto_fix_path = (\"test\", \"fixtures\", \"linter\", \"autofix\")\n\n# Generate the filenames for each dialect from the parser test directory\nfor dialect in os.listdir(os.path.join(*base_auto_fix_path)):\n    # Ignore documentation\n    if dialect.endswith(\".md\"):\n        continue\n    # assume that d is now the name of a dialect\n    dirlist = os.listdir(os.path.join(*base_auto_fix_path, dialect))\n    for test_case in dirlist:\n        test_cases.append(\n            (\n                # The dialect\n                dialect,\n                # The directory name\n                test_case,\n            )\n        )\n\n\ndef make_dialect_path(dialect, fname):\n    \"\"\"Work out how to find paths given a dialect and a file name.\"\"\"\n    return os.path.join(\"test\", \"fixtures\", \"dialects\", dialect, fname)\n\n\ndef auto_fix_test(dialect, folder, caplog):\n    \"\"\"A test for roundtrip testing, take a file buffer, lint, fix and lint.\n\n    This is explicitly different from the linter version of this, in that\n    it uses the command line rather than the direct api.\n    \"\"\"\n    # Log just the rules logger for this test.\n    # NOTE: In debugging it may be instructive to enable some of\n    # the other loggers listed here to debug particular issues.\n    # Enabling all of them results in very long logs so use\n    # wisely.\n    # caplog.set_level(logging.DEBUG, logger=\"sqlfluff.templater\")\n    # caplog.set_level(logging.DEBUG, logger=\"sqlfluff.lexer\")\n    caplog.set_level(logging.DEBUG, logger=\"sqlfluff.linter\")\n    caplog.set_level(logging.DEBUG, logger=\"sq"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "re.parser.matchable import Matchable\nfrom sqlfluff.core.parser.types import SimpleHintType\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.raw import RawSegment\n\n# Instantiate the linter logger (only for use in methods involved with fixing.)\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\nTupleSerialisedSegment = tuple[str, Union[str, tuple[\"TupleSerialisedSegment\", ...]]]\nRecordSerialisedSegment = dict[\n    str, Union[None, str, \"RecordSerialisedSegment\", list[\"RecordSerialisedSegment\"]]\n]\n\n\n@dataclass(frozen=True)\nclass SourceFix:\n    \"\"\"A stored reference to a fix in the non-templated file.\"\"\"\n\n    edit: str\n    source_slice: slice\n    # TODO: It might be possible to refactor this to not require\n    # a templated_slice (because in theory it's unnecessary).\n    # However much of the fix handling code assumes we need\n    # a position in the templated file to interpret it.\n    # More work required to achieve that if desired.\n    templated_slice: slice\n\n    def __hash__(self) -> int:\n        # Only hash based on the source slice, not the\n        # templated slice (which might change)\n        return hash((self.edit, self.source_slice.start, self.source_slice.stop))\n\n\n@dataclass(frozen=True)\nclass PathStep:\n    \"\"\"An element of the response to BaseSegment.path_to().\n\n    Attributes:\n        segment (:obj:`BaseSegment`): The segment in the chain.\n        idx (int): The index of the target within its `segment`.\n        len (int): The number of children `segment` has.\n        code_idxs (:obj:`tuple` of int): The indices which contain code.\n    \"\"\"\n\n    segment: BaseSegment\n    idx: int\n    len: int\n    code_idxs: tuple[int, ...]\n\n\ndef _iter_base_types(\n    new_type: Optional[str], bases: tuple[type[BaseSegment]]\n) -> Iterator[str]:\n    \"\"\"Iterate types for a new segment class.\n\n    This is a helper method used within in the construction of\n    SegmentMetaclass so that we can construct a fro"}, {"start_line": 36000, "end_line": 38000, "belongs_to": {"file_name": "commands.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/cli", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "=True,\n    help=(\n        \"[DEPRECATED - From 3.0 onward this is the default behaviour] \"\n        \"Apply fixes will also be applied file by file, during the \"\n        \"linting process, rather than waiting until all files are \"\n        \"linted before fixing.\"\n    ),\n)\n@click.option(\n    \"--check\",\n    is_flag=True,\n    help=(\n        \"Analyse all files and ask for confirmation before applying \"\n        \"any fixes. Fixes will be applied all together at the end of \"\n        \"the operation.\"\n    ),\n)\n@click.option(\n    \"-q\",\n    \"--quiet\",\n    is_flag=True,\n    help=(\n        \"Reduces the amount of output to stdout to a minimal level. \"\n        \"This is effectively the opposite of -v. NOTE: It will only \"\n        \"take effect if -f/--force is also set.\"\n    ),\n)\n@click.option(\n    \"-x\",\n    \"--fixed-suffix\",\n    default=None,\n    help=\"An optional suffix to add to fixed files.\",\n)\n@click.option(\n    \"--FIX-EVEN-UNPARSABLE\",\n    is_flag=True,\n    default=None,\n    help=(\n        \"Enables fixing of files that have templating or parse errors. \"\n        \"Note that the similar-sounding '--ignore' or 'noqa' features merely \"\n        \"prevent errors from being *displayed*. For safety reasons, the 'fix'\"\n        \"command will not make any fixes in files that have templating or parse \"\n        \"errors unless '--FIX-EVEN-UNPARSABLE' is enabled on the command line\"\n        \"or in the .sqlfluff config file.\"\n    ),\n)\n@click.option(\n    \"--show-lint-violations\",\n    is_flag=True,\n    help=\"Show lint violations\",\n)\n@click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\ndef fix(\n    force: bool,\n    paths: tuple[str],\n    disregard_sqlfluffignores: bool,\n    check: bool = False,\n    bench: bool = False,\n    quiet: bool = False,\n    fixed_suffix: str = \"\",\n    logger: Optional[logging.Logger] = None,\n    processes: Optional[int] = None,\n    disable_progress_bar: Optional[bool] = False,\n    persist_timing: Optional[str] = None,\n    extra_config_path: Optional[str] = None,\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linted_file_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests covering the LintedFile class and it's methods.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core.linter import LintedFile\nfrom sqlfluff.core.linter.patch import FixPatch\nfrom sqlfluff.core.templaters import RawFileSlice\n\n\n@pytest.mark.parametrize(\n    \"source_slices,source_patches,raw_source_string,expected_result\",\n    # NOTE: For all of these examples we're not setting the patch_category\n    # of the fix patches. They're not used at this step so irrelevant for\n    # testing.\n    [\n        # Trivial example\n        ([slice(0, 1)], [], \"a\", \"a\"),\n        # Simple replacement\n        (\n            [slice(0, 1), slice(1, 2), slice(2, 3)],\n            [FixPatch(slice(1, 2), \"d\", \"\", slice(1, 2), \"b\", \"b\")],\n            \"abc\",\n            \"adc\",\n        ),\n        # Simple insertion\n        (\n            [slice(0, 1), slice(1, 1), slice(1, 2)],\n            [FixPatch(slice(1, 1), \"b\", \"\", slice(1, 1), \"\", \"\")],\n            \"ac\",\n            \"abc\",\n        ),\n        # Simple deletion\n        (\n            [slice(0, 1), slice(1, 2), slice(2, 3)],\n            [FixPatch(slice(1, 2), \"\", \"\", slice(1, 2), \"b\", \"b\")],\n            \"abc\",\n            \"ac\",\n        ),\n        # Illustrative templated example (although practically at\n        # this step, the routine shouldn't care if it's templated).\n        (\n            [slice(0, 2), slice(2, 7), slice(7, 9)],\n            [FixPatch(slice(2, 3), \"{{ b }}\", \"\", slice(2, 7), \"b\", \"{{b}}\")],\n            \"a {{b}} c\",\n            \"a {{ b }} c\",\n        ),\n    ],\n)\ndef test__linted_file__build_up_fixed_source_string(\n    source_slices, source_patches, raw_source_string, expected_result, caplog\n):\n    \"\"\"Test _build_up_fixed_source_string.\n\n    This is part of fix_string().\n    \"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.linter\"):\n        result = LintedFile._build_up_fixed_source_string(\n            source_slices, source_patches, raw_source_string\n        )\n    assert result == expected_result\n\n\n@pytest.m"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "std_fix_auto_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "'s not worry\n        violations = None\n\n    # Run the fix command\n    overrides = {\"dialect\": dialect}\n    if rules:\n        overrides[\"rules\"] = rules\n\n    # Clear config caches before loading. The way we move files around\n    # makes the filepath based caching inaccurate, which leads to unstable\n    # test cases unless we regularly clear the cache.\n    clear_config_caches()\n    cfg = FluffConfig.from_root(overrides=overrides)\n    lnt = Linter(config=cfg)\n    res = lnt.lint_path(filepath, fix=True)\n\n    if not res.files:\n        raise ValueError(\"LintedDir empty: Parsing likely failed.\")\n    print(f\"## Templated file:\\n{res.tree.raw}\")\n\n    # We call the check_tuples here, even to makes sure any non-linting\n    # violations are raised, and the test fails.\n    vs = set(\n        res.check_tuples(\n            raise_on_non_linting_violations=raise_on_non_linting_violations\n        )\n    )\n    # If we have a violations structure, let's enforce it.\n    if violations:\n        # Format the violations file\n        expected_vs = set()\n        for rule_key in violations[\"violations\"][\"linting\"]:\n            for elem in violations[\"violations\"][\"linting\"][rule_key]:\n                expected_vs.add((rule_key, *elem))\n        assert expected_vs == vs\n\n    # Actually do the fixes\n    res = res.persist_changes()\n    # Read the fixed file\n    with open(filepath) as fixed_file:\n        fixed_buff = fixed_file.read()\n    # Clear up once read\n    shutil.rmtree(tempdir_path)\n    # Also clear the config cache again so it's not polluted for later tests.\n    clear_config_caches()\n    # Read the comparison file\n    with open(cmp_filepath) as comp_file:\n        comp_buff = comp_file.read()\n\n    # Make sure we were successful\n    assert res\n    # Assert that we fixed as expected\n    assert fixed_buff == comp_buff\n\n\n@pytest.mark.parametrize(\"dialect,folder\", test_cases)\ndef test__std_fix_auto(dialect, folder, caplog):\n    \"\"\"Automated Fixing Tests.\"\"\"\n    auto_fix_test(dialect=dialect, folder"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "simple.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/api", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " not already provided.\n            Defaults to None.\n        fix_even_unparsable (:obj:`bool`, optional): Optional override for the\n            corresponding SQLFluff configuration value.\n\n    Returns:\n        :obj:`str` for the fixed SQL if possible.\n    \"\"\"\n    cfg = config or get_simple_config(\n        dialect=dialect,\n        rules=rules,\n        exclude_rules=exclude_rules,\n        config_path=config_path,\n    )\n    linter = Linter(config=cfg)\n\n    result = linter.lint_string_wrapped(sql, fix=True)\n    if fix_even_unparsable is None:\n        fix_even_unparsable = cfg.get(\"fix_even_unparsable\")\n    should_fix = True\n    if not fix_even_unparsable:\n        # If fix_even_unparsable wasn't set, check for templating or parse\n        # errors and suppress fixing if there were any.\n        _, num_filtered_errors = result.count_tmp_prs_errors()\n        if num_filtered_errors > 0:\n            should_fix = False\n    if should_fix:\n        sql = result.paths[0].files[0].fix_string()[0]\n    return sql\n\n\ndef parse(\n    sql: str,\n    dialect: Optional[str] = None,\n    config: Optional[FluffConfig] = None,\n    config_path: Optional[str] = None,\n) -> dict[str, Any]:\n    \"\"\"Parse a SQL string.\n\n    Args:\n        sql (:obj:`str`): The SQL to be parsed.\n        dialect (:obj:`Optional[str]`, optional): A reference to the dialect of the SQL\n            to be parsed. Defaults to `ansi`.\n        config (:obj:`Optional[FluffConfig]`, optional): A configuration object\n            to use for the operation. Defaults to None.\n        config_path (:obj:`Optional[str]`, optional): A path to a .sqlfluff config,\n            which is only used if a `config` is not already provided.\n            Defaults to None.\n\n    Returns:\n        :obj:`Dict[str, Any]` JSON containing the parsed structure.\n\n    Note:\n        In the case of multiple potential variants from the raw source file\n        only the first variant is returned by the simple API. For access to\n        the other variants, use the under"}], "retrieved_count": 10, "cost_time": 1.1240434646606445}
{"question": "Why does SQLFluff's parallel processing reduce execution time for multiple files?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's parallel processing reduces execution time for multiple files by utilizing multiple CPU cores to process files concurrently. Key benefits include: 1) CPU utilization - Parallel processing takes advantage of multi-core systems to process multiple files simultaneously; 2) I/O optimization - Multiple files can be read and processed concurrently, reducing I/O wait times; 3) Scalability - Performance scales with the number of available CPU cores; 4) Resource efficiency - Better utilization of system resources by distributing work across multiple processes; 5) Reduced wall-clock time - Overall execution time is significantly reduced compared to sequential processing; 6) Batch processing - Large numbers of files can be processed efficiently in parallel batches; 7) Process isolation - Each file is processed in its own process, preventing memory leaks and improving stability; 8) Error isolation - Errors in one file don't affect processing of other files; 9) Configuration sharing - Common configuration and dialect objects can be shared across processes; 10) Progress tracking - Parallel processing supports progress bars and real-time feedback for long-running operations.", "score": null, "retrieved_content": [{"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "runner.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "one:\n        if isinstance(e, IOError):\n            # IOErrors are caught in commands.py, so propagate it\n            raise (e)  # pragma: no cover\n        linter_logger.warning(\n            f\"\"\"Unable to lint {fname} due to an internal error. \\\nPlease report this as an issue with your query's contents and stacktrace below!\nTo hide this warning, add the failing file to .sqlfluffignore\n{traceback.format_exc()}\"\"\",\n        )\n\n\nclass SequentialRunner(BaseRunner):\n    \"\"\"Simple runner that does sequential processing.\"\"\"\n\n    def run(self, fnames: list[str], fix: bool) -> Iterator[LintedFile]:\n        \"\"\"Sequential implementation.\"\"\"\n        for fname, partial in self.iter_partials(fnames, fix=fix):\n            try:\n                yield partial()\n            except (bdb.BdbQuit, KeyboardInterrupt):  # pragma: no cover\n                raise\n            except Exception as e:\n                self._handle_lint_path_exception(fname, e)\n\n\nclass ParallelRunner(BaseRunner):\n    \"\"\"Base class for parallel runner implementations (process or thread).\"\"\"\n\n    POOL_TYPE: Callable[..., multiprocessing.pool.Pool]\n    # Don't pass the formatter in a parallel world, they\n    # don't pickle well.\n    pass_formatter = False\n\n    def __init__(self, linter: Linter, config: FluffConfig, processes: int) -> None:\n        super().__init__(linter, config)\n        self.processes = processes\n\n    def run(self, fnames: list[str], fix: bool) -> Iterator[LintedFile]:\n        \"\"\"Parallel implementation.\n\n        Note that the partials are generated one at a time then\n        passed directly into the pool as they're ready. This means\n        the main thread can do the IO work while passing the parsing\n        and linting work out to the threads.\n        \"\"\"\n        with self._create_pool(\n            self.processes,\n            self._init_global,\n        ) as pool:\n            try:\n                for lint_result in self._map(\n                    pool,\n                    self._apply,\n                "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "runner.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Implements runner classes used internally by the Linter class.\n\nImplements various runner types for SQLFluff:\n- Serial\n- Parallel\n  - Multiprocess\n  - Multithread (used only by automated tests)\n\"\"\"\n\nimport bdb\nimport functools\nimport logging\nimport multiprocessing\nimport multiprocessing.dummy\nimport multiprocessing.pool\nimport signal\nimport sys\nimport traceback\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Iterable, Iterator\nfrom types import TracebackType\nfrom typing import Callable, Optional, Union\n\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.errors import SQLFluffSkipFile\nfrom sqlfluff.core.linter import LintedFile, RenderedFile\nfrom sqlfluff.core.plugin.host import is_main_process\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\nPartialLintCallable = Callable[[], LintedFile]\n\n\nclass BaseRunner(ABC):\n    \"\"\"Base runner class.\"\"\"\n\n    def __init__(\n        self,\n        linter: Linter,\n        config: FluffConfig,\n    ) -> None:\n        self.linter = linter\n        self.config = config\n\n    pass_formatter = True\n\n    def iter_rendered(self, fnames: list[str]) -> Iterator[tuple[str, RenderedFile]]:\n        \"\"\"Iterate through rendered files ready for linting.\"\"\"\n        for fname in self.linter.templater.sequence_files(\n            fnames, config=self.config, formatter=self.linter.formatter\n        ):\n            try:\n                yield fname, self.linter.render_file(fname, self.config)\n            except SQLFluffSkipFile as s:\n                linter_logger.warning(str(s))\n\n    def iter_partials(\n        self,\n        fnames: list[str],\n        fix: bool = False,\n    ) -> Iterator[tuple[str, PartialLintCallable]]:\n        \"\"\"Iterate through partials for linted files.\n\n        Generates filenames and objects which return LintedFiles.\n        \"\"\"\n        for fname, rendered in self.iter_rendered(fnames):\n            # Generate a fresh ruleset\n            rule_pack = self.linter.get_rulepack(config=rend"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "linter_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ",\n                \"violations\": 5,\n            },\n        ),\n        (\n            \"multifile_b\",\n            {\n                \"avg per file\": 2.0,\n                \"clean\": 0,\n                \"clean files\": 0,\n                \"exit code\": 111,\n                \"files\": 2,\n                \"status\": \"FAIL\",\n                \"unclean\": 2,\n                \"unclean files\": 2,\n                \"unclean rate\": 1.0,\n                \"violations\": 4,\n            },\n        ),\n    ],\n)\ndef test__linter__linting_result_stats(path, stats):\n    \"\"\"Test that a LintingResult can get the right stats with multiple files.\n\n    https://github.com/sqlfluff/sqlfluff/issues/5673\n    \"\"\"\n    lntr = Linter()\n    result = lntr.lint_paths((f\"test/fixtures/linter/exit_codes/{path}\",))\n    # NOTE: We're using fake return codes for testing purposes.\n    assert result.stats(111, 222) == stats\n\n\n@pytest.mark.parametrize(\"processes\", [1, 2])\ndef test__linter__linting_result_get_violations(processes):\n    \"\"\"Test that we can get violations from a LintingResult.\"\"\"\n    lntr = Linter()\n    result = lntr.lint_paths(\n        (\n            \"test/fixtures/linter/comma_errors.sql\",\n            \"test/fixtures/linter/whitespace_errors.sql\",\n        ),\n        processes=processes,\n    )\n\n    all([isinstance(v, SQLLintError) for v in result.get_violations()])\n\n\n@pytest.mark.parametrize(\"force_error\", [False, True])\ndef test__linter__linting_parallel_thread(force_error, monkeypatch):\n    \"\"\"Run linter in parallel mode using threads.\n\n    Similar to test__linter__linting_result_get_violations but uses a thread\n    pool of 1 worker to test parallel mode without subprocesses. This lets the\n    tests capture code coverage information for the backend parts of parallel\n    execution without having to jump through hoops.\n    \"\"\"\n    if not force_error:\n        monkeypatch.setattr(Linter, \"allow_process_parallelism\", False)\n\n    else:\n\n        def _create_pool(*args, **kwargs):\n            class ErrorPool:\n              "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "runner.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ered.config)\n            yield (\n                fname,\n                functools.partial(\n                    self.linter.lint_rendered,\n                    rendered,\n                    rule_pack,\n                    fix,\n                    # Formatters may or may not be passed. They don't pickle\n                    # nicely so aren't appropriate in a multiprocessing world.\n                    self.linter.formatter if self.pass_formatter else None,\n                ),\n            )\n\n    @abstractmethod\n    def run(self, fnames: list[str], fix: bool) -> Iterator[LintedFile]:\n        \"\"\"Run linting on the specified list of files.\"\"\"\n        ...\n\n    @classmethod\n    def _init_global(cls) -> None:\n        \"\"\"Initializes any global state.\n\n        May be overridden by subclasses to apply global configuration, initialize\n        logger state in child processes, etc.\n        \"\"\"\n        pass\n\n    @staticmethod\n    def _handle_lint_path_exception(fname: Optional[str], e: BaseException) -> None:\n        if isinstance(e, IOError):\n            # IOErrors are caught in commands.py, so propagate it\n            raise (e)  # pragma: no cover\n        linter_logger.warning(\n            f\"\"\"Unable to lint {fname} due to an internal error. \\\nPlease report this as an issue with your query's contents and stacktrace below!\nTo hide this warning, add the failing file to .sqlfluffignore\n{traceback.format_exc()}\"\"\",\n        )\n\n\nclass SequentialRunner(BaseRunner):\n    \"\"\"Simple runner that does sequential processing.\"\"\"\n\n    def run(self, fnames: list[str], fix: bool) -> Iterator[LintedFile]:\n        \"\"\"Sequential implementation.\"\"\"\n        for fname, partial in self.iter_partials(fnames, fix=fix):\n            try:\n                yield partial()\n            except (bdb.BdbQuit, KeyboardInterrupt):  # pragma: no cover\n                raise\n            except Exception as e:\n                self._handle_lint_path_exception(fname, e)\n\n\nclass ParallelRunner(BaseRunner):\n    \"\"\"Base class for "}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "linter_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " can get violations from a LintingResult.\"\"\"\n    lntr = Linter()\n    result = lntr.lint_paths(\n        (\n            \"test/fixtures/linter/comma_errors.sql\",\n            \"test/fixtures/linter/whitespace_errors.sql\",\n        ),\n        processes=processes,\n    )\n\n    all([isinstance(v, SQLLintError) for v in result.get_violations()])\n\n\n@pytest.mark.parametrize(\"force_error\", [False, True])\ndef test__linter__linting_parallel_thread(force_error, monkeypatch):\n    \"\"\"Run linter in parallel mode using threads.\n\n    Similar to test__linter__linting_result_get_violations but uses a thread\n    pool of 1 worker to test parallel mode without subprocesses. This lets the\n    tests capture code coverage information for the backend parts of parallel\n    execution without having to jump through hoops.\n    \"\"\"\n    if not force_error:\n        monkeypatch.setattr(Linter, \"allow_process_parallelism\", False)\n\n    else:\n\n        def _create_pool(*args, **kwargs):\n            class ErrorPool:\n                def __enter__(self):\n                    return self\n\n                def __exit__(self, exc_type, exc_val, exc_tb):\n                    pass\n\n                def imap_unordered(self, *args, **kwargs):\n                    yield runner.DelayedException(ValueError())\n\n            return ErrorPool()\n\n        monkeypatch.setattr(runner.MultiProcessRunner, \"_create_pool\", _create_pool)\n\n    config = FluffConfig(overrides={\"dialect\": \"ansi\"})\n    output_stream = make_output_stream(config, None, os.devnull)\n    lntr = Linter(\n        formatter=OutputStreamFormatter(output_stream, False, verbosity=0),\n        dialect=\"ansi\",\n    )\n    result = lntr.lint_paths(\n        # NOTE: Lint more than one file to make sure we enabled the multithreaded\n        # code path.\n        (\n            \"test/fixtures/linter/comma_errors.sql\",\n            \"test/fixtures/linter/whitespace_errors.sql\",\n        ),\n        processes=2,\n    )\n\n    all([isinstance(v, SQLLintError) for v in result.get_violations()])\n\n\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "generate_parse_fixture_yml.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Utility to generate yml files for all the parsing examples.\"\"\"\n\nimport fnmatch\nimport multiprocessing\nimport os\nimport re\nimport sys\nimport time\nfrom collections import defaultdict\nfrom typing import Callable, Optional, TypeVar\n\nimport click\nimport yaml\nfrom conftest import (\n    ParseExample,\n    compute_parse_tree_hash,\n    get_parse_fixtures,\n    parse_example_file,\n)\n\nfrom sqlfluff.core.errors import SQLParseError\n\nS = TypeVar(\"S\", bound=\"ParseExample\")\n\n\ndef distribute_work(work_items: list[S], work_fn: Callable[[S], None]) -> None:\n    \"\"\"Distribute work keep track of progress.\"\"\"\n    # Build up a dict of sets, where the key is the dialect and the set\n    # contains all the expected cases. As cases return we'll check them\n    # off.\n    success_map = {}\n\n    expected_cases = defaultdict(set)\n    for case in work_items:\n        expected_cases[case.dialect].add(case)\n\n    errors = []\n\n    with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n        for example, result in pool.imap_unordered(work_fn, work_items):\n            if result is not None:\n                errors.append(result)\n                success_map[example] = False\n            else:\n                success_map[example] = True\n\n            expected_cases[example.dialect].remove(example)\n            # Check to see whether a dialect is complete\n            if not expected_cases[example.dialect]:\n                # It's done. Report success rate.\n                local_success_map = {\n                    k: v for k, v in success_map.items() if k.dialect == example.dialect\n                }\n                if all(local_success_map.values()):\n                    print(f\"{example.dialect!r} complete.\\t\\tAll Success \")\n                else:\n                    fail_files = [\n                        k.sqlfile for k, v in local_success_map.items() if not v\n                    ]\n                    print(\n                        f\"{example.dialect!r} complete.\\t\\t{len(fail_files)} fails. \"\n     "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the linter class.\"\"\"\n\nimport fnmatch\nimport logging\nimport os\nimport time\nfrom collections.abc import Iterator, Sequence\nfrom typing import TYPE_CHECKING, Optional, cast\n\nimport regex\nfrom tqdm import tqdm\n\nfrom sqlfluff.core.config import FluffConfig, progress_bar_configuration\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLFluffSkipFile,\n    SQLLexError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.helpers.file import get_encoding\nfrom sqlfluff.core.linter.common import (\n    ParsedString,\n    ParsedVariant,\n    RenderedFile,\n    RuleTuple,\n)\nfrom sqlfluff.core.linter.discovery import paths_from_path\nfrom sqlfluff.core.linter.fix import apply_fixes, compute_anchor_edit_info\nfrom sqlfluff.core.linter.linted_dir import LintedDir\nfrom sqlfluff.core.linter.linted_file import (\n    TMP_PRS_ERROR_TYPES,\n    FileTimings,\n    LintedFile,\n)\nfrom sqlfluff.core.linter.linting_result import LintingResult\nfrom sqlfluff.core.parser import Lexer, Parser\nfrom sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\nfrom sqlfluff.core.rules import BaseRule, RulePack, get_ruleset\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.rules.noqa import IgnoreMask\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.meta import MetaSegment\n    from sqlfluff.core.templaters import RawTemplater, TemplatedFile\n\n\nRuleTimingsType = list[tuple[str, str, float]]\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass Linter:\n    \"\"\"The interface class to interact with the linter.\"\"\"\n\n    # Default to allowing process parallelism\n    allow_process_parallelism = True\n\n    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n        dialect: Optional[str] = None,\n  "}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "runner.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "parallel runner implementations (process or thread).\"\"\"\n\n    POOL_TYPE: Callable[..., multiprocessing.pool.Pool]\n    # Don't pass the formatter in a parallel world, they\n    # don't pickle well.\n    pass_formatter = False\n\n    def __init__(self, linter: Linter, config: FluffConfig, processes: int) -> None:\n        super().__init__(linter, config)\n        self.processes = processes\n\n    def run(self, fnames: list[str], fix: bool) -> Iterator[LintedFile]:\n        \"\"\"Parallel implementation.\n\n        Note that the partials are generated one at a time then\n        passed directly into the pool as they're ready. This means\n        the main thread can do the IO work while passing the parsing\n        and linting work out to the threads.\n        \"\"\"\n        with self._create_pool(\n            self.processes,\n            self._init_global,\n        ) as pool:\n            try:\n                for lint_result in self._map(\n                    pool,\n                    self._apply,\n                    self.iter_partials(fnames, fix=fix),\n                ):\n                    if isinstance(lint_result, DelayedException):\n                        try:\n                            lint_result.reraise()\n                        except Exception as e:\n                            self._handle_lint_path_exception(lint_result.fname, e)\n                    else:\n                        # It's a LintedDir.\n                        if self.linter.formatter:\n                            self.linter.formatter.dispatch_file_violations(\n                                lint_result.path,\n                                lint_result,\n                                only_fixable=fix,\n                                warn_unused_ignores=self.linter.config.get(\n                                    \"warn_unused_ignores\"\n                                ),\n                            )\n                        yield lint_result\n            except KeyboardInterrupt:  # pragma: no cover\n                # On keyb"}, {"start_line": 44000, "end_line": 46000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n        # If no paths specified - assume local\n        if not paths:  # pragma: no cover\n            paths = (os.getcwd(),)\n        # Set up the result to hold what we get back\n        result = LintingResult()\n\n        expanded_paths: list[str] = []\n        expanded_path_to_linted_dir = {}\n        sql_exts = self.config.get(\"sql_file_exts\", default=\".sql\").lower().split(\",\")\n\n        for path in paths:\n            linted_dir = LintedDir(path, retain_files=retain_files)\n            result.add(linted_dir)\n            for fname in paths_from_path(\n                path,\n                ignore_non_existent_files=ignore_non_existent_files,\n                ignore_files=ignore_files,\n                target_file_exts=sql_exts,\n            ):\n                expanded_paths.append(fname)\n                expanded_path_to_linted_dir[fname] = linted_dir\n\n        files_count = len(expanded_paths)\n        if processes is None:\n            processes = self.config.get(\"processes\", default=1)\n        assert processes is not None\n        # Hard set processes to 1 if only 1 file is queued.\n        # The overhead will never be worth it with one file.\n        if files_count == 1:\n            processes = 1\n\n        # to avoid circular import\n        from sqlfluff.core.linter.runner import get_runner\n\n        runner, effective_processes = get_runner(\n            self,\n            self.config,\n            processes=processes,\n            allow_process_parallelism=self.allow_process_parallelism,\n        )\n\n        if self.formatter and effective_processes != 1:\n            self.formatter.dispatch_processing_header(effective_processes)\n\n        # Show files progress bar only when there is more than one.\n        first_path = expanded_paths[0] if expanded_paths else \"\"\n        progress_bar_files = tqdm(\n            total=files_count,\n            desc=f\"file {first_path}\",\n            leave=False,\n            disable=files_count <= 1 or progress_bar_configuration.disable_progress_bar,\n        )\n"}, {"start_line": 45000, "end_line": 47000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "sert processes is not None\n        # Hard set processes to 1 if only 1 file is queued.\n        # The overhead will never be worth it with one file.\n        if files_count == 1:\n            processes = 1\n\n        # to avoid circular import\n        from sqlfluff.core.linter.runner import get_runner\n\n        runner, effective_processes = get_runner(\n            self,\n            self.config,\n            processes=processes,\n            allow_process_parallelism=self.allow_process_parallelism,\n        )\n\n        if self.formatter and effective_processes != 1:\n            self.formatter.dispatch_processing_header(effective_processes)\n\n        # Show files progress bar only when there is more than one.\n        first_path = expanded_paths[0] if expanded_paths else \"\"\n        progress_bar_files = tqdm(\n            total=files_count,\n            desc=f\"file {first_path}\",\n            leave=False,\n            disable=files_count <= 1 or progress_bar_configuration.disable_progress_bar,\n        )\n\n        for i, linted_file in enumerate(runner.run(expanded_paths, fix), start=1):\n            linted_dir = expanded_path_to_linted_dir[linted_file.path]\n            linted_dir.add(linted_file)\n            # If any fatal errors, then stop iteration.\n            if any(v.fatal for v in linted_file.violations):  # pragma: no cover\n                linter_logger.error(\"Fatal linting error. Halting further linting.\")\n                break\n\n            # If we're applying fixes, then do that here.\n            if apply_fixes:\n                num_tmp_prs_errors = linted_file.num_violations(\n                    types=TMP_PRS_ERROR_TYPES,\n                    filter_ignore=False,\n                    filter_warning=False,\n                )\n                if fix_even_unparsable or num_tmp_prs_errors == 0:\n                    linted_file.persist_tree(\n                        suffix=fixed_file_suffix, formatter=self.formatter\n                    )\n\n            # Progress bar for files is rendered o"}], "retrieved_count": 10, "cost_time": 1.1182584762573242}
{"question": "What is the relationship between SQLFluff's FormatterInterface and output generation?", "answer": null, "relative_code_list": null, "ground_truth": "FormatterInterface serves as the abstract interface for SQLFluff's output generation system, providing a callback mechanism for different stages of the linting process. Key relationships include: 1) Abstract interface - FormatterInterface defines abstract methods that must be implemented by concrete formatter classes to handle various output events; 2) Callback system - Provides dispatch methods for different linting stages including dispatch_lint_header(), dispatch_file_violations(), dispatch_parse_header(), and dispatch_template_header(); 3) CLI integration - OutputStreamFormatter implements FormatterInterface to provide human-readable output with colorization and formatting for command-line usage; 4) Linter integration - Linter class accepts an optional FormatterInterface parameter and calls its methods at appropriate points during linting to generate real-time output; 5) Output formatting - Handles formatting of violations, file status, configuration information, and timing statistics for display; 6) Color support - Provides colorize() method for ANSI color codes to enhance readability of output; 7) Multi-format support - Enables different output formats (human, JSON, YAML, GitHub annotations) through different formatter implementations; 8) Progress tracking - Supports progress bars and real-time feedback during long-running linting operations; 9) Error reporting - Formats and displays various types of errors (lexing, parsing, templating, linting) in a consistent manner; 10) Extensibility - Allows custom formatters to be implemented for different output requirements (logging, GUI integration, CI/CD systems).", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "formatter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the formatter interface which can be used by the CLI.\n\nThe linter module provides an optional formatter input which effectively\nallows callbacks at various points of the linting process. This is primarily\nto allow printed output at various points by the CLI, but could also be used\nfor logging our other processes looking to report back as the linting process\ncontinues.\n\nIn this module we only define the interface. Any modules wishing to use the\ninterface should override with their own implementation.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlfluff.core.types import Color\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.linter import LintedFile\n\n\nclass FormatterInterface(ABC):\n    \"\"\"Generic formatter interface.\"\"\"\n\n    @abstractmethod\n    def dispatch_persist_filename(self, filename: str, result: str) -> None:\n        \"\"\"Called after a formatted file as been persisted to disk.\"\"\"\n        ...\n\n    @abstractmethod\n    def dispatch_lint_header(self, fname: str, rules: list[str]) -> None:\n        \"\"\"Dispatch the header displayed before linting.\"\"\"\n        ...\n\n    @abstractmethod\n    def dispatch_file_violations(\n        self,\n        fname: str,\n        linted_file: \"LintedFile\",\n        only_fixable: bool,\n        warn_unused_ignores: bool,\n    ) -> None:\n        \"\"\"Dispatch any violations found in a file.\"\"\"\n        ...\n\n    @abstractmethod\n    def dispatch_dialect_warning(self, dialect: str) -> None:\n        \"\"\"Dispatch a warning for dialects.\"\"\"\n        ...\n\n    @abstractmethod\n    def dispatch_template_header(\n        self,\n        fname: str,\n        linter_config: \"FluffConfig\",\n        file_config: Optional[\"FluffConfig\"],\n    ) -> None:\n        \"\"\"Dispatch the header displayed before templating.\"\"\"\n        ...\n\n    @abstractmethod\n    def dispatch_parse_header(self, fname: str) -> None:\n        \"\"\"Dispatch the header displayed before parsi"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "formatters.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/cli", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the formatters for the CLI.\"\"\"\n\nimport os\nimport sys\nfrom io import StringIO\nfrom typing import Optional, Union\n\nimport click\nfrom colorama import Style\n\nfrom sqlfluff.cli import EXIT_FAIL, EXIT_SUCCESS\nfrom sqlfluff.cli.helpers import (\n    get_package_version,\n    get_python_implementation,\n    get_python_version,\n    pad_line,\n    wrap_field,\n)\nfrom sqlfluff.cli.outputstream import OutputStream\nfrom sqlfluff.core import FluffConfig, Linter, SQLBaseError, TimingSummary\nfrom sqlfluff.core.linter import FormatterInterface, LintedFile, ParsedString\nfrom sqlfluff.core.types import Color\n\n\ndef split_string_on_spaces(s: str, line_length: int = 100) -> list[str]:\n    \"\"\"Split a string into lines based on whitespace.\n\n    For short strings the functionality is trivial.\n    >>> split_string_on_spaces(\"abc\")\n    ['abc']\n\n    For longer sections it will split at an appropriate point.\n    >>> split_string_on_spaces(\"abc def ghi\", line_length=7)\n    ['abc def', 'ghi']\n\n    After splitting, multi-space sections should be intact.\n    >>> split_string_on_spaces(\"a '   ' b c d e f\", line_length=11)\n    [\"a '   ' b c\", 'd e f']\n    \"\"\"\n    line_buff = []\n    str_buff = \"\"\n    # NOTE: We *specify* the single space split, so that on reconstruction\n    # we can accurately represent multi space strings.\n    for token in s.split(\" \"):\n        # Can we put this token on this line without going over?\n        if str_buff:\n            if len(str_buff) + len(token) > line_length:\n                line_buff.append(str_buff)\n                str_buff = token\n            else:\n                str_buff += \" \" + token\n        else:\n            # In the case that the buffer is already empty, add it without checking,\n            # otherwise there might be things that we might never.\n            str_buff = token\n    # If we have left over buff, add it in\n    if str_buff:\n        line_buff.append(str_buff)\n    return line_buff\n\n\ndef format_linting_result_header() -> str:\n    \"\"\"Format the he"}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "formatters.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/cli", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " if rule.aliases:\n            aliases = self.colorize(\", \".join(rule.aliases), Color.light)\n            description += f\" aliases: {aliases}\"\n        return description\n\n    def format_rules(self, linter: Linter, verbose: int = 0) -> str:\n        \"\"\"Format the a set of rules given a `Linter`.\"\"\"\n        text_buffer = StringIO()\n        text_buffer.write(\"==== sqlfluff - rules ====\\n\")\n        text_buffer.write(\n            self.cli_table(\n                [\n                    (\n                        t.code,\n                        self._format_rule_description(t),\n                    )\n                    for t in linter.rule_tuples()\n                ],\n                col_width=80,\n                cols=1,\n                label_color=Color.blue,\n                val_align=\"left\",\n            )\n        )\n        return text_buffer.getvalue()\n\n    def format_dialects(self, dialect_readout, verbose=0) -> str:\n        \"\"\"Format the dialects yielded by `dialect_readout`.\"\"\"\n        text_buffer = StringIO()\n        text_buffer.write(\"==== sqlfluff - dialects ====\\n\")\n        readouts = [\n            (\n                dialect.label,\n                f\"{dialect.name} dialect [inherits from '{dialect.inherits_from}']\",\n            )\n            for dialect in dialect_readout()\n        ]\n        text_buffer.write(\n            self.cli_table(\n                readouts,\n                col_width=60,\n                cols=1,\n                label_color=Color.blue,\n                val_align=\"right\",\n            )\n        )\n        return text_buffer.getvalue()\n\n    def format_dialect_warning(self, dialect: str) -> str:\n        \"\"\"Output a warning for parsing errors.\"\"\"\n        return self.colorize(\n            (\n                \"WARNING: Parsing errors found and dialect is set to \"\n                f\"'{dialect}'. Have you configured your dialect correctly?\"\n            ),\n            Color.light,\n        )\n\n    def print_out_residual_error_counts(\n        self, total_errors: int, nu"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "formatters.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/cli", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ader of a linting result output.\"\"\"\n    text_buffer = StringIO()\n    text_buffer.write(\"==== readout ====\\n\")\n    return text_buffer.getvalue()\n\n\nclass OutputStreamFormatter(FormatterInterface):\n    \"\"\"Formatter which writes to an OutputStream.\n\n    On instantiation, this formatter accepts a function to\n    dispatch messages. Each public method accepts an object\n    or data in a common format, with this class handling the\n    formatting and output.\n\n    This class is designed to be subclassed if we eventually\n    want to provide other methods of surfacing output.\n\n\n    Args:\n        output_stream: Output is sent here\n        verbosity: Specifies how verbose output should be\n        filter_empty: If True, empty messages will not be dispatched\n        output_line_length: Maximum line length\n    \"\"\"\n\n    def __init__(\n        self,\n        output_stream: OutputStream,\n        nocolor: bool,\n        verbosity: int = 0,\n        filter_empty: bool = True,\n        output_line_length: int = 80,\n        show_lint_violations: bool = False,\n    ):\n        self._output_stream = output_stream\n        self.plain_output = self.should_produce_plain_output(nocolor)\n        self.verbosity = verbosity\n        self._filter_empty = filter_empty\n        self.output_line_length = output_line_length\n        self.show_lint_violations = show_lint_violations\n\n    @staticmethod\n    def should_produce_plain_output(nocolor: bool) -> bool:\n        \"\"\"Returns True if text output should be plain (not colored).\"\"\"\n        # If `--color` is specified (nocolor is False), we ignore `NO_COLOR`\n        env_nocolor = bool(os.getenv(\"NO_COLOR\")) and nocolor is not False\n        return nocolor or not sys.stdout.isatty() or env_nocolor\n\n    def _dispatch(self, s: str) -> None:\n        \"\"\"Dispatch a string to the callback.\n\n        This method is designed as a point for subclassing.\n        \"\"\"\n        # The strip here is to filter out any empty messages\n        if (not self._filter_empty) or s.strip(\" \\n\\t\")"}, {"start_line": 0, "end_line": 489, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Linter class and helper classes.\"\"\"\n\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.linter.common import ParsedString, RenderedFile, RuleTuple\nfrom sqlfluff.core.linter.linted_file import LintedFile\nfrom sqlfluff.core.linter.linter import Linter\nfrom sqlfluff.core.linter.linting_result import LintingResult\n\n__all__ = (\n    \"FormatterInterface\",\n    \"RuleTuple\",\n    \"ParsedString\",\n    \"LintedFile\",\n    \"LintingResult\",\n    \"Linter\",\n    \"RenderedFile\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "outputstream.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/cli", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Classes for managing linter output, used with OutputStreamFormatter.\"\"\"\n\nimport abc\nimport os\nfrom typing import Any, Optional\n\nimport click\nfrom tqdm import tqdm\n\nfrom sqlfluff.core import FluffConfig\nfrom sqlfluff.core.types import FormatType\n\n\nclass OutputStream(abc.ABC):\n    \"\"\"Base class for linter output stream.\"\"\"\n\n    def __init__(self, config: FluffConfig, context: Any = None) -> None:\n        self.config = config\n\n    def write(self, message: str) -> None:\n        \"\"\"Write message to output.\"\"\"\n        raise NotImplementedError  # pragma: no cover\n\n    def close(self) -> None:\n        \"\"\"Close output stream.\"\"\"\n        pass\n\n\nclass TqdmOutput(OutputStream):\n    \"\"\"Outputs to stdout, coordinates to avoid conflict with tqdm.\n\n    It may happen that progressbar conflicts with extra printing. Nothing very\n    serious happens then, except that there is printed (not removed) progressbar\n    line. The `external_write_mode` allows to disable tqdm for writing time.\n    \"\"\"\n\n    def __init__(self, config: FluffConfig) -> None:\n        super().__init__(config)\n\n    def write(self, message: str) -> None:\n        \"\"\"Write message to stdout.\"\"\"\n        with tqdm.external_write_mode():\n            click.echo(message=message, color=self.config.get(\"color\"))\n\n\nclass FileOutput(OutputStream):\n    \"\"\"Outputs to a specified file.\"\"\"\n\n    def __init__(self, config: FluffConfig, output_path: str) -> None:\n        super().__init__(config)\n        self.file = open(output_path, \"w\")\n\n    def write(self, message: str) -> None:\n        \"\"\"Write message to output_path.\"\"\"\n        print(message, file=self.file)\n\n    def close(self) -> None:\n        \"\"\"Close output file.\"\"\"\n        self.file.close()\n\n\ndef make_output_stream(\n    config: FluffConfig,\n    format: Optional[str] = None,\n    output_path: Optional[str] = None,\n) -> OutputStream:\n    \"\"\"Create and return appropriate OutputStream instance.\"\"\"\n    if format is None or format == FormatType.human.value:\n        if not output_p"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "formatters.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/cli", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ":\n            self._output_stream.write(s)\n\n    def _format_config(self, linter: Linter) -> str:\n        \"\"\"Format the config of a `Linter`.\"\"\"\n        text_buffer = StringIO()\n        # Only show version information if verbosity is high enough\n        if self.verbosity > 0:\n            text_buffer.write(\"==== sqlfluff ====\\n\")\n            config_content = [\n                (\"sqlfluff\", get_package_version()),\n                (\"python\", get_python_version()),\n                (\"implementation\", get_python_implementation()),\n                (\"verbosity\", self.verbosity),\n            ]\n            if linter.dialect:\n                config_content.append((\"dialect\", linter.dialect.name))\n            config_content += linter.templater.config_pairs()\n            text_buffer.write(\n                self.cli_table(config_content, col_width=30, max_label_width=15)\n            )\n            text_buffer.write(\"\\n\")\n            if linter.config.get(\"rule_allowlist\"):\n                text_buffer.write(\n                    self.cli_table(\n                        [(\"rules\", \", \".join(linter.config.get(\"rule_allowlist\")))],\n                        col_width=41,\n                    )\n                )\n            if self.verbosity > 1:\n                text_buffer.write(\"\\n== Raw Config:\\n\")\n                text_buffer.write(self.format_config_vals(linter.config.iter_vals()))\n        return text_buffer.getvalue()\n\n    def dispatch_config(self, linter: Linter) -> None:\n        \"\"\"Dispatch configuration output appropriately.\"\"\"\n        self._dispatch(self._format_config(linter))\n\n    def dispatch_persist_filename(self, filename: str, result: str) -> None:\n        \"\"\"Dispatch filenames during a persist operation.\"\"\"\n        # Only show the skip records at higher levels of verbosity\n        if self.verbosity >= 2 or result != \"SKIP\":\n            self._dispatch(self.format_filename(filename=filename, success=result))\n\n    def _format_path(self, path: str) -> str:\n        \"\"\"Format paths.\"\""}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "formatters.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/cli", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ",\n        show_lint_violations: bool = False,\n    ):\n        self._output_stream = output_stream\n        self.plain_output = self.should_produce_plain_output(nocolor)\n        self.verbosity = verbosity\n        self._filter_empty = filter_empty\n        self.output_line_length = output_line_length\n        self.show_lint_violations = show_lint_violations\n\n    @staticmethod\n    def should_produce_plain_output(nocolor: bool) -> bool:\n        \"\"\"Returns True if text output should be plain (not colored).\"\"\"\n        # If `--color` is specified (nocolor is False), we ignore `NO_COLOR`\n        env_nocolor = bool(os.getenv(\"NO_COLOR\")) and nocolor is not False\n        return nocolor or not sys.stdout.isatty() or env_nocolor\n\n    def _dispatch(self, s: str) -> None:\n        \"\"\"Dispatch a string to the callback.\n\n        This method is designed as a point for subclassing.\n        \"\"\"\n        # The strip here is to filter out any empty messages\n        if (not self._filter_empty) or s.strip(\" \\n\\t\"):\n            self._output_stream.write(s)\n\n    def _format_config(self, linter: Linter) -> str:\n        \"\"\"Format the config of a `Linter`.\"\"\"\n        text_buffer = StringIO()\n        # Only show version information if verbosity is high enough\n        if self.verbosity > 0:\n            text_buffer.write(\"==== sqlfluff ====\\n\")\n            config_content = [\n                (\"sqlfluff\", get_package_version()),\n                (\"python\", get_python_version()),\n                (\"implementation\", get_python_implementation()),\n                (\"verbosity\", self.verbosity),\n            ]\n            if linter.dialect:\n                config_content.append((\"dialect\", linter.dialect.name))\n            config_content += linter.templater.config_pairs()\n            text_buffer.write(\n                self.cli_table(config_content, col_width=30, max_label_width=15)\n            )\n            text_buffer.write(\"\\n\")\n            if linter.config.get(\"rule_allowlist\"):\n                text_buffer.wri"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "python.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the templaters.\"\"\"\n\nimport ast\nimport re\nfrom collections.abc import Iterable, Iterator\nfrom string import Formatter\nfrom typing import Any, Callable, NamedTuple, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLTemplaterError\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.helpers.slice import offset_slice, zero_slice\nfrom sqlfluff.core.helpers.string import findall\nfrom sqlfluff.core.templaters.base import (\n    RawFileSlice,\n    RawTemplater,\n    TemplatedFile,\n    TemplatedFileSlice,\n    large_file_check,\n    templater_logger,\n)\n\n\nclass IntermediateFileSlice(NamedTuple):\n    \"\"\"An intermediate representation of a partially sliced File.\"\"\"\n\n    intermediate_type: str\n    source_slice: slice\n    templated_slice: slice\n    slice_buffer: list[RawFileSlice]\n\n    def _trim_end(\n        self, templated_str: str, target_end: str = \"head\"\n    ) -> tuple[\"IntermediateFileSlice\", list[TemplatedFileSlice]]:\n        \"\"\"Trim the ends of a intermediate segment.\"\"\"\n        target_idx = 0 if target_end == \"head\" else -1\n        terminator_types = (\"block_start\") if target_end == \"head\" else (\"block_end\")\n        main_source_slice = self.source_slice\n        main_templated_slice = self.templated_slice\n        slice_buffer = self.slice_buffer\n\n        end_buffer = []\n\n        # Yield any leading literals, comments or blocks.\n        while len(slice_buffer) > 0 and slice_buffer[target_idx].slice_type in (\n            \"literal\",\n            \"block_start\",\n            \"block_end\",\n            \"comment\",\n        ):\n            focus = slice_buffer[target_idx]\n            templater_logger.debug(\"            %s Focus: %s\", target_end, focus)\n            # Is it a zero length item?\n            if focus.slice_type in (\"block_start\", \"block_end\", \"comment\"):\n                # Only add the length in the source space.\n                templated_len = 0\n            else:\n                # Assume it's a literal, chec"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "formatters_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/cli", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The Test file for CLI Formatters.\"\"\"\n\nimport pathlib\nimport re\nimport textwrap\n\nimport pytest\n\nfrom sqlfluff.cli.commands import fix\nfrom sqlfluff.cli.formatters import OutputStreamFormatter\nfrom sqlfluff.cli.outputstream import FileOutput\nfrom sqlfluff.core import FluffConfig\nfrom sqlfluff.core.errors import SQLLintError\nfrom sqlfluff.core.parser import RawSegment\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.rules import RuleGhost\nfrom sqlfluff.core.templaters.base import TemplatedFile\nfrom sqlfluff.core.types import Color\n\n\ndef escape_ansi(line):\n    \"\"\"Remove ANSI color codes for testing.\"\"\"\n    ansi_escape = re.compile(\"\\u001b\\\\[[0-9]+(;[0-9]+)?m\")\n    return ansi_escape.sub(\"\", line)\n\n\ndef test__cli__formatters__filename_nocol(tmpdir):\n    \"\"\"Test formatting filenames.\"\"\"\n    formatter = OutputStreamFormatter(\n        FileOutput(FluffConfig(require_dialect=False), str(tmpdir / \"out.txt\")), False\n    )\n    res = formatter.format_filename(\"blahblah\", success=True)\n    assert escape_ansi(res) == \"== [blahblah] PASS\"\n\n\ndef test__cli__formatters__violation(tmpdir):\n    \"\"\"Test formatting violations.\n\n    NB Position is 1 + start_pos.\n    \"\"\"\n    s = RawSegment(\n        \"foobarbar\",\n        PositionMarker(\n            slice(10, 19),\n            slice(10, 19),\n            TemplatedFile.from_string(\"      \\n\\n  foobarbar\"),\n        ),\n    )\n    r = RuleGhost(\"A\", \"some-name\", \"DESC\")\n    v = SQLLintError(description=r.description, segment=s, rule=r)\n    formatter = OutputStreamFormatter(\n        FileOutput(FluffConfig(require_dialect=False), str(tmpdir / \"out.txt\")), False\n    )\n    f = formatter.format_violation(v)\n    # Position is 3, 3 because foobarbar is on the third\n    # line (i.e. it has two newlines preceding it) and\n    # it's at the third position in that line (i.e. there\n    # are two characters between it and the preceding\n    # newline).\n    assert escape_ansi(f) == \"L:   3 | P:   3 |    A | DESC [some-name]\"\n\n\ndef test__c"}], "retrieved_count": 10, "cost_time": 1.1591236591339111}
{"question": "Why does SQLFluff implement a multi-dialect SQL parser for different SQL variants?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff implements a multi-dialect SQL parser to support the diverse ecosystem of SQL variants used across different database systems and applications. Key reasons include: 1) Database diversity - Different database systems (PostgreSQL, MySQL, Snowflake, BigQuery, etc.) have their own SQL dialects with unique syntax and features; 2) Vendor-specific extensions - Each database vendor adds proprietary SQL extensions that aren't part of standard SQL; 3) Syntax variations - Common operations have different syntax across dialects (e.g., string concatenation, date functions, window functions); 4) Keyword differences - Reserved keywords vary between dialects, requiring different lexing and parsing rules; 5) Function variations - Built-in functions have different names, parameters, and behavior across dialects; 6) Data type differences - Each dialect supports different data types and type casting syntax; 7) User requirements - Users need to lint SQL written for their specific database system; 8) Migration support - Organizations often need to support multiple dialects during database migrations; 9) Tool integration - SQLFluff needs to integrate with various database tools and frameworks that use different dialects; 10) Community needs - The SQL community uses multiple dialects, and SQLFluff aims to serve the entire community.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Contains SQL Dialects.\n\nNote that individual dialects are only imported as needed at runtime.\nThis avoids circular references.\n\nTo enable this, any modules outside of .dialects cannot import dialects\ndirectly. They should import `dialect_selector` and use that to fetch\ndialects.\n\nWithin .dialects, each dialect is free to depend on other dialects as\nrequired. Any dependent dialects will be loaded as needed.\n\"\"\"\n\nfrom collections.abc import Iterator\nfrom importlib import import_module\nfrom typing import NamedTuple\n\n# Eventually it would be a good to dynamically discover dialects\n# from any module beginning with \"dialect_\" within this folder.\nfrom sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.errors import SQLFluffUserError\n\n_dialect_lookup = {\n    \"ansi\": (\"dialect_ansi\", \"ansi_dialect\"),\n    \"athena\": (\"dialect_athena\", \"athena_dialect\"),\n    \"bigquery\": (\"dialect_bigquery\", \"bigquery_dialect\"),\n    \"clickhouse\": (\"dialect_clickhouse\", \"clickhouse_dialect\"),\n    \"databricks\": (\"dialect_databricks\", \"databricks_dialect\"),\n    \"db2\": (\"dialect_db2\", \"db2_dialect\"),\n    \"doris\": (\"dialect_doris\", \"doris_dialect\"),\n    \"duckdb\": (\"dialect_duckdb\", \"duckdb_dialect\"),\n    \"exasol\": (\"dialect_exasol\", \"exasol_dialect\"),\n    \"flink\": (\"dialect_flink\", \"flink_dialect\"),\n    \"greenplum\": (\"dialect_greenplum\", \"greenplum_dialect\"),\n    \"hive\": (\"dialect_hive\", \"hive_dialect\"),\n    \"impala\": (\"dialect_impala\", \"impala_dialect\"),\n    \"materialize\": (\"dialect_materialize\", \"materialize_dialect\"),\n    \"mariadb\": (\"dialect_mariadb\", \"mariadb_dialect\"),\n    \"mysql\": (\"dialect_mysql\", \"mysql_dialect\"),\n    \"oracle\": (\"dialect_oracle\", \"oracle_dialect\"),\n    \"postgres\": (\"dialect_postgres\", \"postgres_dialect\"),\n    \"redshift\": (\"dialect_redshift\", \"redshift_dialect\"),\n    \"snowflake\": (\"dialect_snowflake\", \"snowflake_dialect\"),\n    \"soql\": (\"dialect_soql\", \"soql_dialect\"),\n    \"sparksql\": (\"dialect_sparksql\", \"sparksql_dialect\"),\n    \"sqlite\": (\"dialect_sqlite\", \"sql"}, {"start_line": 0, "end_line": 35, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.dialects.\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parser.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the Parser class.\"\"\"\n\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import check_still_complete\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.parser.segments import BaseFileSegment, BaseSegment\n\n\nclass Parser:\n    \"\"\"Instantiates parsed queries from a sequence of lexed raw segments.\"\"\"\n\n    def __init__(\n        self, config: Optional[FluffConfig] = None, dialect: Optional[str] = None\n    ):\n        if config and dialect:\n            raise ValueError(  # pragma: no cover\n                \"Parser does not support setting both `config` and `dialect`.\"\n            )\n        # Use the provided config or create one from the dialect.\n        self.config = config or FluffConfig.from_kwargs(dialect=dialect)\n        self.RootSegment: type[BaseFileSegment] = self.config.get(\n            \"dialect_obj\"\n        ).get_root_segment()\n\n    def parse(\n        self,\n        segments: Sequence[\"BaseSegment\"],\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> Optional[\"BaseSegment\"]:\n        \"\"\"Parse a series of lexed tokens using the current dialect.\"\"\"\n        if not segments:  # pragma: no cover\n            # This should normally never happen because there will usually\n            # be an end_of_file segment. It would probably only happen in\n            # api use cases.\n            return None\n\n        # NOTE: This is the only time we use the parse context not in the\n        # context of a context manager. That's because it's the initial\n        # instantiation.\n        ctx = ParseContext.from_config(config=self.config)\n        # Kick off parsing with the root segment. The BaseFileSegment has\n        # a unique entry point to facilitate exactly this. All other segments\n        # will use the standard .match() route.\n        root = self.RootSegment.root_pars"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "conftest.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Sharing fixtures to test the dialects.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.parser import BaseSegment, Lexer\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\n\n\ndef lex(raw, config):\n    \"\"\"Basic parsing for the tests below.\"\"\"\n    # Set up the lexer\n    lex = Lexer(config=config)\n    # Lex the string for matching. For a good test, this would\n    # arguably happen as a fixture, but it's easier to pass strings\n    # as parameters than pre-lexed segment strings.\n    segments, vs = lex.lex(raw)\n    assert not vs\n    print(segments)\n    return segments\n\n\ndef validate_segment(segmentref, config):\n    \"\"\"Get and validate segment for tests below.\"\"\"\n    Seg = config.get(\"dialect_obj\").ref(segmentref)\n    if isinstance(Seg, Matchable):\n        return Seg\n    try:\n        if issubclass(Seg, BaseSegment):\n            return Seg\n    except TypeError:\n        pass\n    raise TypeError(\n        \"{} is not of type Segment or Matchable. Test is invalid.\".format(segmentref)\n    )\n\n\ndef _dialect_specific_segment_parses(dialect, segmentref, raw, caplog):\n    \"\"\"Test that specific segments parse as expected.\n\n    NB: We're testing the PARSE function not the MATCH function\n    although this will be a recursive parse and so the match\n    function of SUBSECTIONS will be tested if present. The match\n    function of the parent will not be tested.\n    \"\"\"\n    config = FluffConfig(overrides=dict(dialect=dialect))\n    segments = lex(raw, config=config)\n    Seg = validate_segment(segmentref, config=config)\n\n    # Most segments won't handle the end of file marker. We should strip it.\n    if segments[-1].is_type(\"end_of_file\"):\n        segments = segments[:-1]\n\n    ctx = ParseContext.from_config(config)\n    with caplog.at_level(logging.DEBUG):\n        result = Seg.match(segments, 0, parse_context=ctx)\n    assert isinst"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the base dialect class.\"\"\"\n\nimport sys\nfrom typing import Any, Optional, Union, cast\n\nfrom sqlfluff.core.parser import (\n    BaseSegment,\n    KeywordSegment,\n    SegmentGenerator,\n    StringParser,\n)\nfrom sqlfluff.core.parser.grammar.base import BaseGrammar, Nothing\nfrom sqlfluff.core.parser.lexer import LexerType\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.types import BracketPairTuple, DialectElementType\n\n\nclass Dialect:\n    \"\"\"Serves as the basis for runtime resolution of Grammar.\n\n    Args:\n        name (:obj:`str`): The name of the dialect, used for lookup.\n        lexer_matchers (iterable of :obj:`StringLexer`): A structure defining\n            the lexing config for this dialect.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        root_segment_name: str,\n        lexer_matchers: Optional[list[LexerType]] = None,\n        library: Optional[dict[str, DialectElementType]] = None,\n        sets: Optional[dict[str, set[Union[str, BracketPairTuple]]]] = None,\n        inherits_from: Optional[str] = None,\n        formatted_name: Optional[str] = None,\n        docstring: Optional[str] = None,\n    ) -> None:\n        self._library = library or {}\n        self.name = name\n        self.lexer_matchers = lexer_matchers\n        self.expanded = False\n        self._sets = sets or {}\n        self.inherits_from = inherits_from\n        self.root_segment_name = root_segment_name\n        # Attributes for documentation\n        self.formatted_name: str = formatted_name or name\n        self.docstring = docstring or f\"The dialect for {self.formatted_name}.\"\n\n    def __repr__(self) -> str:  # pragma: no cover\n        return f\"<Dialect: {self.name}>\"\n\n    def expand(self) -> \"Dialect\":\n        \"\"\"Expand any callable references to concrete ones.\n\n        This must be called before using the dialect. But\n        allows more flexible definitions to happen at runtime.\n\n        NOTE: This method returns a copy of the current dialect\n  "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "dialect_ansi.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The core ANSI dialect.\n\nThis is the core SQL grammar. We'll probably extend this or make it pluggable\nfor other dialects. Here we encode the structure of the language.\n\nThere shouldn't be any underlying \"machinery\" here, that should all\nbe defined elsewhere.\n\nA lot of the inspiration for this sql grammar is taken from the cockroach\nlabs full sql grammar. In particular their way for dividing up the expression\ngrammar. Check out their docs, they're awesome.\nhttps://www.cockroachlabs.com/docs/stable/sql-grammar.html#select_stmt\n\"\"\"\n\nfrom collections.abc import Generator\nfrom enum import Enum\nfrom typing import NamedTuple, Optional, Union, cast\n\nfrom sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.dialects.common import AliasInfo, ColumnAliasInfo\nfrom sqlfluff.core.parser import (\n    AnyNumberOf,\n    AnySetOf,\n    Anything,\n    BaseFileSegment,\n    BaseSegment,\n    BinaryOperatorSegment,\n    Bracketed,\n    BracketedSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    Conditional,\n    Dedent,\n    Delimited,\n    IdentifierSegment,\n    ImplicitIndent,\n    Indent,\n    KeywordSegment,\n    LiteralKeywordSegment,\n    LiteralSegment,\n    Matchable,\n    MultiStringParser,\n    NewlineSegment,\n    Nothing,\n    OneOf,\n    OptionallyBracketed,\n    ParseMode,\n    RawSegment,\n    Ref,\n    RegexLexer,\n    RegexParser,\n    SegmentGenerator,\n    Sequence,\n    StringLexer,\n    StringParser,\n    SymbolSegment,\n    TypedParser,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.dialects.dialect_ansi_keywords import (\n    ansi_reserved_keywords,\n    ansi_unreserved_keywords,\n)\n\nansi_dialect = Dialect(\n    \"ansi\",\n    root_segment_name=\"FileSegment\",\n    formatted_name=\"ANSI\",\n    docstring=\"\"\"**Default Casing**: ``UPPERCASE``\n\n**Quotes**: String Literals: ``''``, Identifiers: ``\"\"``\n\nThis is the base dialect which holds most of the definitions of common\nSQL commands "}, {"start_line": 0, "end_line": 209, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Dialects, segregated to make imports manageable.\n\nNOTE: dialects should not be imported directly from this\nmodule, but should be accessed instead using the selector\nmethods in `sqlfluff.core.dialects`.\n\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "bigquery_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests specific to the snowflake dialect.\"\"\"\n\nimport hypothesis.strategies as st\nimport pytest\nfrom hypothesis import example, given, note, settings\n\nfrom sqlfluff.core import FluffConfig\nfrom sqlfluff.core.parser import Lexer, Parser\n\n\n@settings(max_examples=100, deadline=None)\n@given(\n    st.lists(\n        st.tuples(st.sampled_from([\"<\", \"=\", \">\"]), st.sampled_from([\"AND\", \"OR\"])),\n        min_size=1,\n        max_size=30,\n    )\n)\n@example(data=[(\"<\", \"AND\")])\n@example(data=[(\">\", \"AND\")])\n@example(data=[(\"<\", \"AND\"), (\">\", \"AND\")])\n@example(data=[(\"<\", \"AND\"), (\"=\", \"AND\"), (\">\", \"AND\")])\n@example(data=[(\">\", \"AND\"), (\"<\", \"AND\")])\n@example(data=[(\"<\", \"AND\"), (\"<\", \"AND\"), (\">\", \"AND\")])\n@example(data=[(\">\", \"AND\"), (\">\", \"AND\"), (\"<\", \"AND\")])\ndef test_bigquery_relational_operator_parsing(data):\n    \"\"\"Tests queries with a diverse mixture of relational operators.\"\"\"\n    # Generate a simple SELECT query with relational operators and conjunctions\n    # as specified in 'data'. Note the conjunctions are used as separators\n    # between comparisons, sn the conjunction in the first item is not used.\n    filter = []\n    for i, (relation, conjunction) in enumerate(data):\n        if i:\n            filter.append(f\" {conjunction} \")\n        filter.append(f\"a {relation} b\")\n    raw = f'SELECT * FROM t WHERE {\"\".join(filter)}'\n    note(f\"query: {raw}\")\n    # Load the right dialect\n    config = FluffConfig(overrides=dict(dialect=\"bigquery\"))\n    tokens, lex_vs = Lexer(config=config).lex(raw)\n    # From just the initial parse, check we're all there\n    assert \"\".join(token.raw for token in tokens) == raw\n    # Check we don't have lexing issues\n    assert not lex_vs\n\n    # Do the parse WITHOUT lots of logging\n    # The logs get too long here to be useful. We should use\n    # specific segment tests if we want to debug logs.\n    parsed = Parser(config=config).parse(tokens)\n    print(f\"Post-parse structure: {parsed.to_tuple(show_raw=True)}\")\n    print(f\"Post-parse structure: {pa"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "dialect_ansi.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "sonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    Conditional,\n    Dedent,\n    Delimited,\n    IdentifierSegment,\n    ImplicitIndent,\n    Indent,\n    KeywordSegment,\n    LiteralKeywordSegment,\n    LiteralSegment,\n    Matchable,\n    MultiStringParser,\n    NewlineSegment,\n    Nothing,\n    OneOf,\n    OptionallyBracketed,\n    ParseMode,\n    RawSegment,\n    Ref,\n    RegexLexer,\n    RegexParser,\n    SegmentGenerator,\n    Sequence,\n    StringLexer,\n    StringParser,\n    SymbolSegment,\n    TypedParser,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.dialects.dialect_ansi_keywords import (\n    ansi_reserved_keywords,\n    ansi_unreserved_keywords,\n)\n\nansi_dialect = Dialect(\n    \"ansi\",\n    root_segment_name=\"FileSegment\",\n    formatted_name=\"ANSI\",\n    docstring=\"\"\"**Default Casing**: ``UPPERCASE``\n\n**Quotes**: String Literals: ``''``, Identifiers: ``\"\"``\n\nThis is the base dialect which holds most of the definitions of common\nSQL commands and structures. If the dialect which you're actually using\nisn't specifically implemented by SQLFluff, using this dialect is a good\nplace to start.\n\nThis dialect doesn't intend to be brutal in adhering to (and only to) the\nANSI SQL spec *(mostly because ANSI charges for access to that spec)*. It aims\nto be a representation of vanilla SQL before any other project adds their\nspin to it, and so may contain a slightly wider set of functions than actually\navailable in true ANSI SQL.\"\"\",\n)\n\nansi_dialect.set_lexer_matchers(\n    [\n        # Match all forms of whitespace except newlines and carriage returns:\n        # https://stackoverflow.com/questions/3469080/match-whitespace-but-not-newlines\n        # This pattern allows us to also match non-breaking spaces (#2189).\n        RegexLexer(\"whitespace\", r\"[^\\S\\r\\n]+\", WhitespaceSegment),\n        RegexLexer(\n            \"inline_comment\",\n            r\"(--|#)[^\\n]*\",\n            CommentSegment,\n            segment_kwargs={\"trim_start\": (\"--\", \"#\")}"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "bricks\": (\"dialect_databricks\", \"databricks_dialect\"),\n    \"db2\": (\"dialect_db2\", \"db2_dialect\"),\n    \"doris\": (\"dialect_doris\", \"doris_dialect\"),\n    \"duckdb\": (\"dialect_duckdb\", \"duckdb_dialect\"),\n    \"exasol\": (\"dialect_exasol\", \"exasol_dialect\"),\n    \"flink\": (\"dialect_flink\", \"flink_dialect\"),\n    \"greenplum\": (\"dialect_greenplum\", \"greenplum_dialect\"),\n    \"hive\": (\"dialect_hive\", \"hive_dialect\"),\n    \"impala\": (\"dialect_impala\", \"impala_dialect\"),\n    \"materialize\": (\"dialect_materialize\", \"materialize_dialect\"),\n    \"mariadb\": (\"dialect_mariadb\", \"mariadb_dialect\"),\n    \"mysql\": (\"dialect_mysql\", \"mysql_dialect\"),\n    \"oracle\": (\"dialect_oracle\", \"oracle_dialect\"),\n    \"postgres\": (\"dialect_postgres\", \"postgres_dialect\"),\n    \"redshift\": (\"dialect_redshift\", \"redshift_dialect\"),\n    \"snowflake\": (\"dialect_snowflake\", \"snowflake_dialect\"),\n    \"soql\": (\"dialect_soql\", \"soql_dialect\"),\n    \"sparksql\": (\"dialect_sparksql\", \"sparksql_dialect\"),\n    \"sqlite\": (\"dialect_sqlite\", \"sqlite_dialect\"),\n    \"starrocks\": (\"dialect_starrocks\", \"starrocks_dialect\"),\n    \"teradata\": (\"dialect_teradata\", \"teradata_dialect\"),\n    \"trino\": (\"dialect_trino\", \"trino_dialect\"),\n    \"tsql\": (\"dialect_tsql\", \"tsql_dialect\"),\n    \"vertica\": (\"dialect_vertica\", \"vertica_dialect\"),\n}\n\n_legacy_dialects = {\n    \"exasol_fs\": (\n        \"As of 0.7.0 the 'exasol_fs' dialect has been combined with \"\n        \"the 'exasol' dialect, and is no longer a standalone dialect. \"\n        \"Please use the 'exasol' dialect instead.\"\n    ),\n    \"spark3\": (\n        \"The 'spark3' dialect has been renamed to sparksql. \"\n        \"Please use the 'sparksql' dialect instead.\"\n    ),\n}\n\n\ndef load_raw_dialect(label: str, base_module: str = \"sqlfluff.dialects\") -> Dialect:\n    \"\"\"Dynamically load a dialect.\"\"\"\n    if label in _legacy_dialects:\n        raise SQLFluffUserError(_legacy_dialects[label])\n    elif label not in _dialect_lookup:\n        raise KeyError(\"Unknown dialect\")\n    module_name, name = _dialect_loo"}], "retrieved_count": 10, "cost_time": 1.1469202041625977}
{"question": "Why does SQLFluff use a RuleSet-based approach for rule management?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff uses a RuleSet-based approach for rule management to provide centralized control, organization, and flexibility in handling linting rules. Key reasons include: 1) Centralized registration - RuleSet provides a single point for registering and managing all rules through the @ruleset.register decorator; 2) Configuration management - RuleSet validates rule configuration options and ensures they match predefined validation rules; 3) Rule filtering - Enables dynamic filtering of rules based on configuration settings (allowlisting/denylisting) through get_rulelist(); 4) Runtime instantiation - Rules are registered as classes at module load time but instantiated at runtime, allowing configuration values to be passed dynamically; 5) Metadata management - RuleSet maintains comprehensive metadata for each rule including code, name, description, groups, and aliases; 6) Naming convention enforcement - Enforces the Rule_XXXX naming convention and validates rule codes to prevent conflicts; 7) Group validation - Ensures all rules belong to required groups (like 'all') for proper categorization; 8) Plugin integration - Supports plugin-based rule registration, allowing custom rules to be added without modifying the core codebase; 9) Code collision prevention - Prevents duplicate rule codes from being registered, maintaining rule uniqueness; 10) Extensibility - Provides a framework for adding new rules and rule types while maintaining consistency and validation.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 1349, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Configuration and examples for individual rules.\"\"\"\n\nfrom sqlfluff.core.plugin.host import get_plugin_manager\nfrom sqlfluff.core.rules.base import (\n    BaseRule,\n    EvalResultType,\n    LintResult,\n    RuleGhost,\n    RulePack,\n    RuleSet,\n)\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.fix import LintFix\n\n\ndef _load_standard_rules() -> RuleSet:\n    \"\"\"Initialise the standard ruleset.\n\n    We do this on each call so that dynamic rules changes\n    are possible.\n    \"\"\"\n    std_rule_set = RuleSet(name=\"standard\", config_info=get_config_info())\n\n    # Iterate through the rules list and register each rule with the standard set.\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            std_rule_set.register(rule)\n\n    return std_rule_set\n\n\ndef get_ruleset(name: str = \"standard\") -> RuleSet:\n    \"\"\"Get a ruleset by name.\"\"\"\n    std_rules = _load_standard_rules()\n    lookup = {std_rules.name: std_rules}\n    # Return a copy in case someone modifies the register.\n    return lookup[name].copy()\n\n\n__all__ = (\n    \"get_ruleset\",\n    \"RuleSet\",\n    \"RulePack\",\n    \"BaseRule\",\n    \"LintResult\",\n    \"LintFix\",\n    \"RuleContext\",\n    \"RuleGhost\",\n    \"EvalResultType\",\n    \"ConfigInfo\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "rules_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the standard set of rules.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import Linter\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.linter import RuleTuple\nfrom sqlfluff.core.parser import WhitespaceSegment\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.rules import BaseRule, LintFix, LintResult, get_ruleset\nfrom sqlfluff.core.rules.crawlers import RootOnlyCrawler, SegmentSeekerCrawler\nfrom sqlfluff.core.rules.doc_decorators import (\n    document_configuration,\n    document_fix_compatible,\n    document_groups,\n)\nfrom sqlfluff.core.rules.loader import get_rules_from_path\nfrom sqlfluff.core.templaters.base import TemplatedFile\nfrom sqlfluff.utils.testing.logging import fluff_log_catcher\nfrom sqlfluff.utils.testing.rules import get_rule_from_set\nfrom test.fixtures.rules.custom.L000 import Rule_L000\nfrom test.fixtures.rules.custom.S000 import Rule_S000\n\n\nclass Rule_T042(BaseRule):\n    \"\"\"A dummy rule.\"\"\"\n\n    groups = (\"all\",)\n\n    def _eval(self, context):\n        pass\n\n\nclass Rule_T001(BaseRule):\n    \"\"\"A deliberately malicious rule.\n\n    **Anti-pattern**\n\n    Blah blah\n    \"\"\"\n\n    groups = (\"all\",)\n    crawl_behaviour = SegmentSeekerCrawler({\"whitespace\"})\n    is_fix_compatible = True\n\n    def _eval(self, context):\n        \"\"\"Stars make newlines.\"\"\"\n        if context.segment.is_type(\"whitespace\"):\n            return LintResult(\n                anchor=context.segment,\n                fixes=[\n                    LintFix.replace(\n                        context.segment, [WhitespaceSegment(context.segment.raw + \" \")]\n                    )\n                ],\n            )\n\n\nclass Rule_T002(BaseRule):\n    \"\"\"A rule which says all raw code segments are bad.\n\n    This is used for testing unparsable code.\n    \"\"\"\n\n    groups = (\"all\",)\n    # Root only crawler so that the in-rule filters don't kick in.\n    crawl_behaviour = RootOnlyCrawler()\n\n    def _eval(s"}, {"start_line": 35000, "end_line": 37000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "the keys) may be codes, groups, aliases or names. The values\n            of the mapping are sets of rule codes *only*. This object acts as\n            a lookup to be able to translate selectors (which may contain\n            diverse references) into a consolidated list of rule codes. This\n            mapping contains the full set of rules, rather than just the filtered\n            set present in the `rules` attribute.\n    \"\"\"\n\n    rules: list[BaseRule]\n    reference_map: dict[str, set[str]]\n\n    def codes(self) -> Iterator[str]:\n        \"\"\"Returns an iterator through the codes contained in the pack.\"\"\"\n        return (r.code for r in self.rules)\n\n\nclass RuleSet:\n    \"\"\"Class to define a ruleset.\n\n    A rule set is instantiated on module load, but the references\n    to each of its classes are instantiated at runtime. This means\n    that configuration values can be passed to those rules live\n    and be responsive to any changes in configuration from the\n    path that the file is in.\n\n    Rules should be fetched using the :meth:`get_rulelist` command which\n    also handles any filtering (i.e. allowlisting and denylisting).\n\n    New rules should be added to the instance of this class using the\n    :meth:`register` decorator. That decorator registers the class, but also\n    performs basic type and name-convention checks.\n\n    The code for the rule will be parsed from the name, the description\n    from the docstring. The eval function is assumed that it will be\n    overridden by the subclass, and the parent class raises an error on\n    this function if not overridden.\n\n    \"\"\"\n\n    def __init__(self, name: str, config_info: dict[str, ConfigInfo]) -> None:\n        self.name = name\n        self.config_info = config_info\n        self._register: dict[str, RuleManifest] = {}\n\n    def _validate_config_options(\n        self, config: \"FluffConfig\", rule_ref: Optional[str] = None\n    ) -> None:\n        \"\"\"Ensure that all config options are valid.\n\n        Config options can also b"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "rules_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "errides=dict(rules=\"T000\", dialect=\"ansi\")),\n        user_rules=[Rule_T000],\n    )\n\n    assert linter.lint_string(\"select 1\").check_tuples() == [(\"T000\", 1, 1)]\n\n\ndef test_rule_must_belong_to_all_group():\n    \"\"\"Assert correct 'groups' config for rule.\"\"\"\n    std_rule_set = get_ruleset()\n\n    with pytest.raises(AssertionError):\n\n        @std_rule_set.register\n        class Rule_T000(BaseRule):\n            \"\"\"Badly configured rule, no groups attribute.\"\"\"\n\n            def _eval(self, **kwargs):\n                pass\n\n    with pytest.raises(AssertionError):\n\n        @std_rule_set.register\n        class Rule_T001(BaseRule):\n            \"\"\"Badly configured rule, no 'all' group.\"\"\"\n\n            groups = ()\n\n            def _eval(self, **kwargs):\n                pass\n\n\ndef test_std_rule_import_fail_bad_naming():\n    \"\"\"Check that rule import from file works.\"\"\"\n    assert get_rules_from_path(\n        rules_path=\"test/fixtures/rules/custom/*.py\",\n        base_module=\"test.fixtures.rules.custom\",\n    ) == [Rule_L000, Rule_S000]\n\n    with pytest.raises(AttributeError) as e:\n        get_rules_from_path(\n            rules_path=\"test/fixtures/rules/custom/bad_rule_name/*.py\",\n            base_module=\"test.fixtures.rules.custom.bad_rule_name\",\n        )\n\n    e.match(\"Rule classes must be named in the format of\")\n\n\ndef test_rule_set_return_informative_error_when_rule_not_registered():\n    \"\"\"Assert that a rule that throws an exception returns it as a validation.\"\"\"\n    cfg = FluffConfig(overrides={\"dialect\": \"ansi\"})\n    with pytest.raises(ValueError) as e:\n        get_rule_from_set(\"L000\", config=cfg)\n\n    e.match(\"'L000' not in\")\n\n\nseg = WhitespaceSegment(\n    pos_marker=PositionMarker(\n        slice(0, 1), slice(0, 1), TemplatedFile(\" \", fname=\"<str>\")\n    )\n)\n\n\n@pytest.mark.parametrize(\n    \"lint_result, expected\",\n    [\n        (LintResult(), \"LintResult(<empty>)\"),\n        (LintResult(seg), \"LintResult(<WhitespaceSegment: ([L:  1, P:  1]) ' '>)\"),\n        (\n            LintR"}, {"start_line": 1000, "end_line": 2820, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e that currently, the gap _before_ and _after_ the semicolon \"\n                \"is considered 'between' statements.\"\n            ),\n        },\n        \"maximum_empty_lines_inside_statements\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The maximum number of empty lines allowed inside statements.\"\n            ),\n        },\n        \"wildcard_policy\": {\n            \"validation\": [\"single\", \"multiple\"],\n            \"definition\": \"Treatment of wildcards. Defaults to ``single``.\",\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.layout.LT01 import Rule_LT01\n    from sqlfluff.rules.layout.LT02 import Rule_LT02\n    from sqlfluff.rules.layout.LT03 import Rule_LT03\n    from sqlfluff.rules.layout.LT04 import Rule_LT04\n    from sqlfluff.rules.layout.LT05 import Rule_LT05\n    from sqlfluff.rules.layout.LT06 import Rule_LT06\n    from sqlfluff.rules.layout.LT07 import Rule_LT07\n    from sqlfluff.rules.layout.LT08 import Rule_LT08\n    from sqlfluff.rules.layout.LT09 import Rule_LT09\n    from sqlfluff.rules.layout.LT10 import Rule_LT10\n    from sqlfluff.rules.layout.LT11 import Rule_LT11\n    from sqlfluff.rules.layout.LT12 import Rule_LT12\n    from sqlfluff.rules.layout.LT13 import Rule_LT13\n    from sqlfluff.rules.layout.LT14 import Rule_LT14\n    from sqlfluff.rules.layout.LT15 import Rule_LT15\n\n    return [\n        Rule_LT01,\n        Rule_LT02,\n        Rule_LT03,\n        Rule_LT04,\n        Rule_LT05,\n        Rule_LT06,\n        Rule_LT07,\n        Rule_LT08,\n        Rule_LT09,\n        Rule_LT10,\n        Rule_LT11,\n        Rule_LT12,\n        Rule_LT13,\n        Rule_LT14,\n        Rule_LT15,\n    ]\n"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "rules.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/utils/testing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "in y:\n                if \"configs\" not in y[i].keys():\n                    y[i].update({\"configs\": global_config})\n        # Replace any commas with underscores so we can reference specific tests.\n        # e.g. `pytest -k AL05_CV12`. Commas break as test keys.\n        ids.extend([rule.replace(\",\", \"_\") + \"_\" + t for t in y])\n        test_cases.extend([RuleTestCase(rule=rule, **v) for k, v in y.items()])\n\n    return ids, test_cases\n\n\ndef get_rule_from_set(code: str, config: FluffConfig) -> BaseRule:\n    \"\"\"Fetch a rule from the rule set.\"\"\"\n    for r in get_ruleset().get_rulepack(config=config).rules:\n        if r.code == code:  # pragma: no cover\n            return r\n    raise ValueError(f\"{code!r} not in {get_ruleset()!r}\")\n\n\ndef _setup_config(\n    code: str, configs: Optional[ConfigMappingType] = None\n) -> FluffConfig:\n    \"\"\"Helper function to set up config consistently for pass & fail functions.\"\"\"\n    overrides: ConfigMappingType = {\"rules\": code}\n    _core_section = configs.get(\"core\", {}) if configs else {}\n    if not isinstance(_core_section, dict) or \"dialect\" not in _core_section:\n        overrides[\"dialect\"] = \"ansi\"\n    return FluffConfig(configs=configs, overrides=overrides)\n\n\ndef assert_rule_fail_in_sql(\n    code: str,\n    sql: str,\n    configs: Optional[ConfigMappingType] = None,\n    line_numbers: Optional[list[int]] = None,\n) -> tuple[str, list[SQLBaseError]]:\n    \"\"\"Assert that a given rule does fail on the given sql.\n\n    Args:\n        code (str): The code of the rule to test.\n        sql (str): The SQL text to check against.\n        configs (:obj:`ConfigMappingType`, optional): A config dict\n            object containing any overrides.\n        line_numbers (list of int, optional): The line numbers which\n            we want to test that errors occurred on.\n\n    Returns:\n        Tuple: values(fixed_sql (str), violations (list))\n            fixed_sql (str): The fixed string after linting. Note that for\n                testing purposes, `.lint_string"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# names, aliases and description are less appropriate to inherit.\n        # NOTE: This applies in particular to CP02, which inherits all groups\n        # from CP01. If we don't do this, those groups don't show in the docs.\n        for base in reversed(bases):\n            if \"groups\" in class_dict:\n                break\n            elif base.groups:\n                class_dict[\"groups\"] = base.groups\n                break\n\n        # If the rule doesn't itself define `config_keywords`, check the parent\n        # classes for them. If we don't do this then they'll still be available to\n        # the rule, but they won't appear in the docs.\n        for base in reversed(bases):\n            if \"config_keywords\" in class_dict:\n                break\n            elif base.config_keywords:\n                class_dict[\"config_keywords\"] = base.config_keywords\n                break\n\n        class_dict = RuleMetaclass._populate_docstring(name, class_dict)\n        # Don't try and infer code and description for the base classes\n        if name not in (\"BaseRule\",):\n            class_dict = RuleMetaclass._populate_code_and_description(name, class_dict)\n        # Validate rule names\n        rule_name = class_dict.get(\"name\", \"\")\n        if rule_name:\n            if not RuleMetaclass._valid_rule_name_regex.match(rule_name):\n                raise SQLFluffUserError(\n                    f\"Tried to define rule with unexpected \"\n                    f\"name format: {rule_name}. Rule names should be lowercase \"\n                    \"and snake_case with optional `.` characters to indicate \"\n                    \"a namespace or grouping. e.g. `layout.spacing`.\"\n                )\n\n        # Use the stock __new__ method now we've adjusted the docstring.\n        # There are no overload variants of type.__new__ that are compatible, so\n        # we ignore type checking in this case.\n        return super().__new__(mcs, name, bases, class_dict)  # type: ignore\n\n\nclass BaseRule(metaclass=RuleMetaclass):\n  "}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "rules_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ion\" not in RuleWithoutConfig_ZZ99.__doc__\n\n\ndef test_rules_name_validation():\n    \"\"\"Ensure that rule names are validated.\"\"\"\n    with pytest.raises(SQLFluffUserError) as exc_info:\n\n        class RuleWithoutBadName_ZZ99(BaseRule):\n            \"\"\"A new rule without configuration.\"\"\"\n\n            name = \"MY-KEBAB-CASE-NAME\"\n\n    assert \"Tried to define rule with unexpected name\" in exc_info.value.args[0]\n    assert \"MY-KEBAB-CASE-NAME\" in exc_info.value.args[0]\n\n\ndef test_rule_exception_is_caught_to_validation():\n    \"\"\"Assert that a rule that throws an exception returns it as a validation.\"\"\"\n    std_rule_set = get_ruleset()\n\n    @std_rule_set.register\n    class Rule_T000(BaseRule):\n        \"\"\"Rule that throws an exception.\"\"\"\n\n        groups = (\"all\",)\n        crawl_behaviour = RootOnlyCrawler()\n\n        def _eval(self, segment, parent_stack, **kwargs):\n            raise Exception(\"Catch me or I'll deny any linting results from you\")\n\n    linter = Linter(\n        config=FluffConfig(overrides=dict(rules=\"T000\", dialect=\"ansi\")),\n        user_rules=[Rule_T000],\n    )\n\n    assert linter.lint_string(\"select 1\").check_tuples() == [(\"T000\", 1, 1)]\n\n\ndef test_rule_must_belong_to_all_group():\n    \"\"\"Assert correct 'groups' config for rule.\"\"\"\n    std_rule_set = get_ruleset()\n\n    with pytest.raises(AssertionError):\n\n        @std_rule_set.register\n        class Rule_T000(BaseRule):\n            \"\"\"Badly configured rule, no groups attribute.\"\"\"\n\n            def _eval(self, **kwargs):\n                pass\n\n    with pytest.raises(AssertionError):\n\n        @std_rule_set.register\n        class Rule_T001(BaseRule):\n            \"\"\"Badly configured rule, no 'all' group.\"\"\"\n\n            groups = ()\n\n            def _eval(self, **kwargs):\n                pass\n\n\ndef test_std_rule_import_fail_bad_naming():\n    \"\"\"Check that rule import from file works.\"\"\"\n    assert get_rules_from_path(\n        rules_path=\"test/fixtures/rules/custom/*.py\",\n        base_module=\"test.fixtures.rules.custom"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Implements the base rule class.\n\nRules crawl through the trees returned by the parser and evaluate particular\nrules.\n\nThe intent is that it should be possible for the rules to be expressed\nas simply as possible, with as much of the complexity abstracted away.\n\nThe evaluation function should take enough arguments that it can evaluate\nthe position of the given segment in relation to its neighbors, and that\nthe segment which finally \"triggers\" the error, should be the one that would\nbe corrected OR if the rule relates to something that is missing, then it\nshould flag on the segment FOLLOWING, the place that the desired element is\nmissing.\n\"\"\"\n\nimport bdb\nimport copy\nimport fnmatch\nimport logging\nimport pathlib\nimport re\nfrom collections import defaultdict, namedtuple\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    ClassVar,\n    DefaultDict,\n    Optional,\n    Union,\n)\n\nimport regex\n\nfrom sqlfluff.core.errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`LintFix`, optional): An array of any\n            fixes which would correct this issue. If not present then it's\n            assumed that this issue will have to manually fixed.\n        memory (:obj:`dict`, optional): An object which stores any working\n            memory for the rule. The `memory` returned in any `LintResult`\n            will be passed as an input to the next segment to be crawled.\n        description (:obj:`str`, optional): A description of the problem\n            identified as part of this result. This will override the\n            description of the rule as what gets reported to the user\n            with the problem if provided.\n        source (:obj:`str`, optional): A string identifier for what\n            generated the result. Within larger libraries like reflow this\n            can be useful for tracking where a result came from.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        anchor: Optional[BaseSegment] = None,\n        fixes: Optional[list[\"LintFix\"]] = None,\n  "}], "retrieved_count": 10, "cost_time": 1.1505680084228516}
{"question": "Why does SQLFluff separate BaseSegment from RawSegment in its parser architecture?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff separates BaseSegment from RawSegment in its parser architecture to create a clear hierarchy and separation of concerns between composite and atomic elements. Key reasons include: 1) Structural clarity - BaseSegment represents composite elements that can contain other segments, while RawSegment represents atomic tokens with no children; 2) Tree representation - This separation enables the creation of a proper parse tree where internal nodes (BaseSegment) contain leaf nodes (RawSegment); 3) Lexer output - RawSegment serves as the primary output of the lexer, representing individual tokens like keywords, identifiers, and literals; 4) Parser input - BaseSegment provides the foundation for building complex SQL structures by combining multiple RawSegments; 5) Type safety - The separation allows for different type systems and validation rules for composite vs atomic elements; 6) Memory efficiency - RawSegments can be optimized for atomic token storage while BaseSegments handle tree traversal and relationship management; 7) Extensibility - New segment types can inherit from either BaseSegment or RawSegment depending on their nature; 8) Processing optimization - Different processing strategies can be applied to composite vs atomic elements; 9) Error handling - Different error recovery strategies can be implemented for composite vs atomic parsing failures; 10) Rule application - Linting rules can target specific segment types more precisely based on this separation.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 966, "belongs_to": {"file_name": "segments_raw_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test the RawSegment class.\"\"\"\n\nfrom sqlfluff.core.parser.segments.base import PathStep\n\n\ndef test__parser__raw_get_raw_segments(raw_segments):\n    \"\"\"Test niche case of calling get_raw_segments on a raw segment.\"\"\"\n    for s in raw_segments:\n        assert s.get_raw_segments() == [s]\n\n\ndef test__parser__raw_segments_with_ancestors(\n    raw_segments, DummySegment, DummyAuxSegment\n):\n    \"\"\"Test raw_segments_with_ancestors.\n\n    This is used in the reflow module to assess parse depth.\n    \"\"\"\n    test_seg = DummySegment([DummyAuxSegment(raw_segments[:1]), raw_segments[1]])\n    # Result should be the same raw segment, but with appropriate parents\n    assert test_seg.raw_segments_with_ancestors == [\n        (\n            raw_segments[0],\n            [\n                PathStep(test_seg, 0, 2, (0, 1)),\n                PathStep(test_seg.segments[0], 0, 1, (0,)),\n            ],\n        ),\n        (raw_segments[1], [PathStep(test_seg, 1, 2, (0, 1))]),\n    ]\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "raw.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Raw segment definitions.\n\nThis is designed to be the root segment, without\nany children, and the output of the lexer.\n\"\"\"\n\nfrom typing import Any, Callable, Optional, Union, cast\nfrom uuid import uuid4\n\nimport regex as re\n\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\n\n\nclass RawSegment(BaseSegment):\n    \"\"\"This is a segment without any subsegments.\"\"\"\n\n    type = \"raw\"\n    _is_code = True\n    _is_comment = False\n    _is_whitespace = False\n    # Classes inheriting from RawSegment may provide a _default_raw\n    # to enable simple initialisation.\n    _default_raw = \"\"\n\n    def __init__(\n        self,\n        raw: Optional[str] = None,\n        pos_marker: Optional[PositionMarker] = None,\n        # For legacy and syntactic sugar we allow the simple\n        # `type` argument here, but for more precise inheritance\n        # we suggest using the `instance_types` option.\n        type: Optional[str] = None,\n        instance_types: tuple[str, ...] = (),\n        trim_start: Optional[tuple[str, ...]] = None,\n        trim_chars: Optional[tuple[str, ...]] = None,\n        source_fixes: Optional[list[SourceFix]] = None,\n        uuid: Optional[int] = None,\n        quoted_value: Optional[tuple[str, Union[int, str]]] = None,\n        escape_replacements: Optional[list[tuple[str, str]]] = None,\n        casefold: Optional[Callable[[str], str]] = None,\n    ):\n        \"\"\"Initialise raw segment.\n\n        If raw is not provided, we default to _default_raw if present.\n        If pos_marker is not provided, it is assume that this will be\n        inserted later as part of a reposition phase.\n        \"\"\"\n        if raw is not None:  # NB, raw *can* be an empty string and be valid\n            self._raw = raw\n        else:\n            self._raw = self._default_raw\n        self._raw_upper = self._raw.upper()\n        # pos marker is required here. We ignore the typing initially\n        # because it might *initially* b"}, {"start_line": 0, "end_line": 433, "belongs_to": {"file_name": "segments_file_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test the BaseFileSegment class.\"\"\"\n\nfrom sqlfluff.core.parser import BaseFileSegment\n\n\ndef test__parser__base_segments_file(raw_segments):\n    \"\"\"Test BaseFileSegment to behave as expected.\"\"\"\n    base_seg = BaseFileSegment(raw_segments, fname=\"/some/dir/file.sql\")\n    assert base_seg.type == \"file\"\n    assert base_seg.file_path == \"/some/dir/file.sql\"\n    assert base_seg.can_start_end_non_code\n    assert base_seg.allow_empty\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "common.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Common segment types used as building blocks of dialects.\n\nThe expectation for these segments is that they have no additional\nlogic (or very minimal logic).\n\"\"\"\n\nfrom sqlfluff.core.parser.segments.base import BaseSegment\nfrom sqlfluff.core.parser.segments.raw import RawSegment\n\n\nclass CodeSegment(RawSegment):\n    \"\"\"An alias for RawSegment.\n\n    This has a more explicit name for segment creation.\n    \"\"\"\n\n    pass\n\n\nclass UnlexableSegment(CodeSegment):\n    \"\"\"A placeholder to unlexable sections.\n\n    This otherwise behaves exactly like a code section.\n    \"\"\"\n\n    type = \"unlexable\"\n\n\nclass CommentSegment(RawSegment):\n    \"\"\"Segment containing a comment.\"\"\"\n\n    type = \"comment\"\n    _is_code = False\n    _is_comment = True\n\n\nclass WhitespaceSegment(RawSegment):\n    \"\"\"Segment containing whitespace.\"\"\"\n\n    type = \"whitespace\"\n    _is_whitespace = True\n    _is_code = False\n    _is_comment = False\n    _default_raw = \" \"\n\n\nclass NewlineSegment(RawSegment):\n    \"\"\"Segment containing a newline.\n\n    NOTE: NewlineSegment does not inherit from WhitespaceSegment.\n    Therefore NewlineSegment.is_type('whitespace') returns False.\n\n    This is intentional and convenient for rules. If users want\n    to match on both, call .is_type('whitespace', 'newline')\n    \"\"\"\n\n    type = \"newline\"\n    _is_whitespace = True\n    _is_code = False\n    _is_comment = False\n    _default_raw = \"\\n\"\n\n\nclass SymbolSegment(CodeSegment):\n    \"\"\"A segment used for matching single entities which aren't keywords.\n\n    We rename the segment class here so that descendants of\n    _ProtoKeywordSegment can use the same functionality\n    but don't end up being labelled as a `keyword` later.\n    \"\"\"\n\n    type = \"symbol\"\n\n\nclass IdentifierSegment(CodeSegment):\n    \"\"\"An identifier segment.\n\n    Defined here for type inheritance.\n    \"\"\"\n\n    type = \"identifier\"\n\n\nclass LiteralSegment(CodeSegment):\n    \"\"\"A literal segment.\n\n    Defined here for type inheritance.\n    \"\"\"\n\n    type = \"literal\"\n\n\nclass BinaryOpera"}, {"start_line": 1000, "end_line": 2225, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ",\n    LiteralSegment,\n    NewlineSegment,\n    RawSegment,\n    SegmentGenerator,\n    SourceFix,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.types import ParseMode\n\n__all__ = (\n    \"BaseSegment\",\n    \"SourceFix\",\n    \"BaseFileSegment\",\n    \"BracketedSegment\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"SymbolSegment\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"LiteralKeywordSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"SegmentGenerator\",\n    \"Sequence\",\n    \"OneOf\",\n    \"Delimited\",\n    \"Bracketed\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\",\n    \"Conditional\",\n    \"StringParser\",\n    \"MultiStringParser\",\n    \"TypedParser\",\n    \"RegexParser\",\n    \"PositionMarker\",\n    \"Lexer\",\n    \"StringLexer\",\n    \"RegexLexer\",\n    \"Parser\",\n    \"Matchable\",\n    \"ParseMode\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Base segment definitions.\n\nHere we define:\n- BaseSegment. This is the root class for all segments, and is\n  designed to hold other subsegments.\n- UnparsableSegment. A special wrapper to indicate that the parse\n  function failed on this block of segments and to prevent further\n  analysis.\n\"\"\"\n\n# Import annotations for py 3.7 to allow `weakref.Referencetype[\"BaseSegment\"]`\nfrom __future__ import annotations\n\nimport logging\nimport weakref\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom functools import cached_property\nfrom io import StringIO\nfrom itertools import chain\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    ClassVar,\n    Optional,\n    Union,\n    cast,\n)\nfrom uuid import uuid4\n\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import trim_non_code_segments\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.types import SimpleHintType\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.raw import RawSegment\n\n# Instantiate the linter logger (only for use in methods involved with fixing.)\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\nTupleSerialisedSegment = tuple[str, Union[str, tuple[\"TupleSerialisedSegment\", ...]]]\nRecordSerialisedSegment = dict[\n    str, Union[None, str, \"RecordSerialisedSegment\", list[\"RecordSerialisedSegment\"]]\n]\n\n\n@dataclass(frozen=True)\nclass SourceFix:\n    \"\"\"A stored reference to a fix in the non-templated file.\"\"\"\n\n    edit: str\n    source_slice: slice\n    # TODO: It might be possible to refactor this to not require\n    # a templated_slice (because in theory it's unnecessary).\n    # However much of the fix handling code assumes we need\n    # a position in the templated file to interpret it.\n    # More work required to ach"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "file.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Definition of the BaseFileSegment.\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Optional\n\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments.base import BaseSegment, UnparsableSegment\n\n\nclass BaseFileSegment(BaseSegment):\n    \"\"\"A segment representing a whole file or script.\n\n    This is also the default \"root\" segment of the dialect,\n    and so is usually instantiated directly. It therefore\n    has no match_grammar.\n    \"\"\"\n\n    type = \"file\"\n    # The file segment is the only one which can start or end with non-code\n    can_start_end_non_code = True\n    # A file can be empty!\n    allow_empty = True\n\n    def __init__(\n        self,\n        segments: tuple[BaseSegment, ...],\n        pos_marker: Optional[PositionMarker] = None,\n        fname: Optional[str] = None,\n    ):\n        self._file_path = fname\n        super().__init__(segments, pos_marker=pos_marker)\n\n    @property\n    def file_path(self) -> Optional[str]:\n        \"\"\"File path of a parsed SQL file.\"\"\"\n        return self._file_path\n\n    @abstractmethod\n    def get_table_references(self) -> set[str]:\n        \"\"\"Use parsed tree to extract table references.\"\"\"\n\n    @classmethod\n    def root_parse(\n        cls,\n        segments: tuple[BaseSegment, ...],\n        parse_context: ParseContext,\n        fname: Optional[str] = None,\n    ) -> \"BaseFileSegment\":\n        \"\"\"This is the entry method into parsing a file lexed segments.\n\n        For single pass matching, this trims any non code off\n        the start, matches the middle and then trims the end.\n\n        Anything unexpected at the end is regarded as unparsable.\n        \"\"\"\n        # Trim the start\n        _start_idx = 0\n        for _start_idx in range(len(segments)):\n            if segments[_start_idx].is_code:\n                break\n\n        # Trim the end\n        _end_idx = len(segments)\n        for _end_idx in range(len(segments), _start_idx - 1, -1):\n   "}, {"start_line": 0, "end_line": 1715, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Definitions of the segment classes.\"\"\"\n\nfrom sqlfluff.core.parser.segments.base import (\n    BaseSegment,\n    SourceFix,\n    UnparsableSegment,\n)\nfrom sqlfluff.core.parser.segments.bracketed import BracketedSegment\nfrom sqlfluff.core.parser.segments.common import (\n    BinaryOperatorSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    IdentifierSegment,\n    LiteralSegment,\n    NewlineSegment,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.segments.file import BaseFileSegment\nfrom sqlfluff.core.parser.segments.generator import SegmentGenerator\nfrom sqlfluff.core.parser.segments.keyword import KeywordSegment, LiteralKeywordSegment\nfrom sqlfluff.core.parser.segments.meta import (\n    Dedent,\n    EndOfFile,\n    ImplicitIndent,\n    Indent,\n    MetaSegment,\n    TemplateLoop,\n    TemplateSegment,\n)\nfrom sqlfluff.core.parser.segments.raw import RawSegment\n\n__all__ = (\n    \"BaseSegment\",\n    \"BaseFileSegment\",\n    \"UnparsableSegment\",\n    \"BracketedSegment\",\n    \"SegmentGenerator\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"LiteralKeywordSegment\",\n    \"SymbolSegment\",\n    \"MetaSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"TemplateSegment\",\n    \"EndOfFile\",\n    \"TemplateLoop\",\n    \"SourceFix\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "segments_base_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test the BaseSegment class.\"\"\"\n\nimport pickle\n\nimport pytest\n\nfrom sqlfluff.core.parser import BaseSegment, PositionMarker, RawSegment\nfrom sqlfluff.core.parser.segments.base import PathStep\nfrom sqlfluff.core.rules.base import LintFix\nfrom sqlfluff.core.templaters import TemplatedFile\n\n\ndef test__parser__base_segments_type(DummySegment):\n    \"\"\"Test the .is_type() method.\"\"\"\n    assert BaseSegment.class_is_type(\"base\")\n    assert not BaseSegment.class_is_type(\"foo\")\n    assert not BaseSegment.class_is_type(\"foo\", \"bar\")\n    assert DummySegment.class_is_type(\"dummy\")\n    assert DummySegment.class_is_type(\"base\")\n    assert DummySegment.class_is_type(\"base\", \"foo\", \"bar\")\n\n\ndef test__parser__base_segments_class_types(DummySegment):\n    \"\"\"Test the metaclass ._class_types attribute.\"\"\"\n    assert DummySegment._class_types == {\"dummy\", \"base\"}\n\n\ndef test__parser__base_segments_descendant_type_set(\n    raw_segments, DummySegment, DummyAuxSegment\n):\n    \"\"\"Test the .descendant_type_set() method.\"\"\"\n    test_seg = DummySegment([DummyAuxSegment(raw_segments)])\n    assert test_seg.descendant_type_set == {\"raw\", \"base\", \"dummy_aux\"}\n\n\ndef test__parser__base_segments_direct_descendant_type_set(\n    raw_segments, DummySegment, DummyAuxSegment\n):\n    \"\"\"Test the .direct_descendant_type_set() method.\"\"\"\n    test_seg = DummySegment([DummyAuxSegment(raw_segments)])\n    assert test_seg.direct_descendant_type_set == {\"base\", \"dummy_aux\"}\n\n\ndef test__parser__base_segments_to_tuple_a(raw_segments, DummySegment, DummyAuxSegment):\n    \"\"\"Test the .to_tuple() method.\"\"\"\n    test_seg = DummySegment([DummyAuxSegment(raw_segments)])\n    assert test_seg.to_tuple() == (\n        \"dummy\",\n        ((\"dummy_aux\", ((\"raw\", ()), (\"raw\", ()))),),\n    )\n\n\ndef test__parser__base_segments_to_tuple_b(raw_segments, DummySegment, DummyAuxSegment):\n    \"\"\"Test the .to_tuple() method.\"\"\"\n    test_seg = DummySegment(\n        [DummyAuxSegment(raw_segments + (DummyAuxSegment(raw_segments[:1]),))]\n    )\n    "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parsers.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Individual segment parsers.\n\nMatchable objects which return individual segments.\n\"\"\"\n\nfrom abc import abstractmethod\nfrom collections.abc import Collection, Sequence\nfrom typing import Any, Callable, Optional\nfrom uuid import uuid4\n\nimport regex\n\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.segments import BaseSegment, RawSegment\nfrom sqlfluff.core.parser.types import SimpleHintType\n\n\nclass BaseParser(Matchable):\n    \"\"\"An abstract class from which other Parsers should inherit.\"\"\"\n\n    # Meta segments are handled separately. All Parser elements\n    # are assumed to be not meta.\n    is_meta: bool = False\n\n    @abstractmethod\n    def __init__(\n        self,\n        raw_class: type[RawSegment],\n        type: Optional[str] = None,\n        optional: bool = False,\n        # The following kwargs are passed on to the segment:\n        trim_chars: Optional[tuple[str, ...]] = None,\n        casefold: Optional[Callable[[str], str]] = None,\n    ) -> None:\n        self.raw_class = raw_class\n        # Store instance_types rather than just type to allow\n        # for multiple possible types to be supported in derivative\n        # classes.\n        self._instance_types: tuple[str, ...] = (type or raw_class.type,)\n        self.optional = optional\n        self._trim_chars = trim_chars\n        self.casefold = casefold\n        # Generate a cache key\n        self._cache_key = uuid4().hex\n\n    def cache_key(self) -> str:\n        \"\"\"Get the cache key for this parser.\n\n        For parsers, they're unique per-instance.\n        \"\"\"\n        return self._cache_key\n\n    def is_optional(self) -> bool:\n        \"\"\"Return whether this element is optional.\"\"\"\n        return self.optional\n\n    def segment_kwargs(self) -> dict[str, Any]:\n        \"\"\"Generates the segment_kwargs package for generating a matched segment.\"\"\"\n        segment_kwargs: dict[str, Any"}], "retrieved_count": 10, "cost_time": 1.1991536617279053}
{"question": "Where in the SQLFluff codebase is the core SQL parser implemented?", "answer": null, "relative_code_list": null, "ground_truth": "The core SQL parser in SQLFluff is implemented across several key modules in the codebase. The main components are located in: 1) src/sqlfluff/core/parser/parser.py - Contains the main Parser class that orchestrates the parsing process; 2) src/sqlfluff/core/parser/lexer.py - Implements the Lexer class for tokenizing SQL input; 3) src/sqlfluff/core/parser/segments/ - Contains segment classes including BaseSegment, RawSegment, and specialized segment types; 4) src/sqlfluff/core/parser/grammar.py - Implements grammar classes (Sequence, OneOf, Delimited, etc.) for defining SQL structure; 5) src/sqlfluff/core/parser/parsers.py - Contains parser classes for individual segment types; 6) src/sqlfluff/dialects/ - Contains dialect-specific parser implementations (dialect_ansi.py, dialect_postgres.py, etc.); 7) src/sqlfluff/core/parser/context.py - Manages parsing context and state; 8) src/sqlfluff/core/parser/match_result.py - Handles parsing match results and tree construction; 9) src/sqlfluff/core/parser/markers.py - Implements position tracking for segments; 10) src/sqlfluff/core/parser/helpers.py - Contains utility functions for parsing operations.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 38, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.core.parser.\"\"\"\n"}, {"start_line": 1000, "end_line": 2225, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ",\n    LiteralSegment,\n    NewlineSegment,\n    RawSegment,\n    SegmentGenerator,\n    SourceFix,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.types import ParseMode\n\n__all__ = (\n    \"BaseSegment\",\n    \"SourceFix\",\n    \"BaseFileSegment\",\n    \"BracketedSegment\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"SymbolSegment\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"LiteralKeywordSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"SegmentGenerator\",\n    \"Sequence\",\n    \"OneOf\",\n    \"Delimited\",\n    \"Bracketed\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\",\n    \"Conditional\",\n    \"StringParser\",\n    \"MultiStringParser\",\n    \"TypedParser\",\n    \"RegexParser\",\n    \"PositionMarker\",\n    \"Lexer\",\n    \"StringLexer\",\n    \"RegexLexer\",\n    \"Parser\",\n    \"Matchable\",\n    \"ParseMode\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"init file for the parser.\"\"\"\n\nfrom sqlfluff.core.parser.grammar import (\n    AnyNumberOf,\n    AnySetOf,\n    Anything,\n    Bracketed,\n    Conditional,\n    Delimited,\n    Nothing,\n    OneOf,\n    OptionallyBracketed,\n    OptionallyDelimited,\n    Ref,\n    Sequence,\n)\nfrom sqlfluff.core.parser.lexer import Lexer, RegexLexer, StringLexer\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.parser import Parser\nfrom sqlfluff.core.parser.parsers import (\n    MultiStringParser,\n    RegexParser,\n    StringParser,\n    TypedParser,\n)\nfrom sqlfluff.core.parser.segments import (\n    BaseFileSegment,\n    BaseSegment,\n    BinaryOperatorSegment,\n    BracketedSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    Dedent,\n    IdentifierSegment,\n    ImplicitIndent,\n    Indent,\n    KeywordSegment,\n    LiteralKeywordSegment,\n    LiteralSegment,\n    NewlineSegment,\n    RawSegment,\n    SegmentGenerator,\n    SourceFix,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.types import ParseMode\n\n__all__ = (\n    \"BaseSegment\",\n    \"SourceFix\",\n    \"BaseFileSegment\",\n    \"BracketedSegment\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"SymbolSegment\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"LiteralKeywordSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"SegmentGenerator\",\n    \"Sequence\",\n    \"OneOf\",\n    \"Delimited\",\n    \"Bracketed\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\","}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parser.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the Parser class.\"\"\"\n\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import check_still_complete\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.parser.segments import BaseFileSegment, BaseSegment\n\n\nclass Parser:\n    \"\"\"Instantiates parsed queries from a sequence of lexed raw segments.\"\"\"\n\n    def __init__(\n        self, config: Optional[FluffConfig] = None, dialect: Optional[str] = None\n    ):\n        if config and dialect:\n            raise ValueError(  # pragma: no cover\n                \"Parser does not support setting both `config` and `dialect`.\"\n            )\n        # Use the provided config or create one from the dialect.\n        self.config = config or FluffConfig.from_kwargs(dialect=dialect)\n        self.RootSegment: type[BaseFileSegment] = self.config.get(\n            \"dialect_obj\"\n        ).get_root_segment()\n\n    def parse(\n        self,\n        segments: Sequence[\"BaseSegment\"],\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> Optional[\"BaseSegment\"]:\n        \"\"\"Parse a series of lexed tokens using the current dialect.\"\"\"\n        if not segments:  # pragma: no cover\n            # This should normally never happen because there will usually\n            # be an end_of_file segment. It would probably only happen in\n            # api use cases.\n            return None\n\n        # NOTE: This is the only time we use the parse context not in the\n        # context of a context manager. That's because it's the initial\n        # instantiation.\n        ctx = ParseContext.from_config(config=self.config)\n        # Kick off parsing with the root segment. The BaseFileSegment has\n        # a unique entry point to facilitate exactly this. All other segments\n        # will use the standard .match() route.\n        root = self.RootSegment.root_pars"}, {"start_line": 0, "end_line": 1589, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The core elements of sqlfluff.\"\"\"\n\nimport tblib.pickling_support\n\n# Config objects\nfrom sqlfluff.core.config import FluffConfig\n\n# Dialect introspection\nfrom sqlfluff.core.dialects import dialect_readout, dialect_selector\n\n# All of the errors.\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLFluffUserError,\n    SQLLexError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\n\n# Public classes\nfrom sqlfluff.core.linter import Linter\nfrom sqlfluff.core.parser import Lexer, Parser\n\n# Timing objects\nfrom sqlfluff.core.timing import TimingSummary\n\n__all__ = (\n    \"FluffConfig\",\n    \"Linter\",\n    \"Lexer\",\n    \"Parser\",\n    \"dialect_selector\",\n    \"dialect_readout\",\n    \"SQLBaseError\",\n    \"SQLTemplaterError\",\n    \"SQLLexError\",\n    \"SQLParseError\",\n    \"SQLLintError\",\n    \"SQLFluffUserError\",\n    \"TimingSummary\",\n)\n\n# This is for \"sqlfluff lint\" and \"sqlfluff fix\" multiprocessing (--processes)\n# support. If an exception (i.e. runtime error) occurs in a worker process, we\n# want to return the tracebook to the main process and report it there, as part\n# of the normal output. However, anything returned from a multiprocessing.Pool\n# worker must be serializable using \"pickle\". By default, Python traceback\n# objects cannot be pickled. The tblib package addresses this limitation; we\n# simply need to install it before creating the worker pool. See these links for\n# additional context:\n# https://pypi.org/project/tblib/\n# https://stackoverflow.com/questions/6126007/python-getting-a-traceback-from-a-multiprocessing-process\ntblib.pickling_support.install()\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "dialect_ansi.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The core ANSI dialect.\n\nThis is the core SQL grammar. We'll probably extend this or make it pluggable\nfor other dialects. Here we encode the structure of the language.\n\nThere shouldn't be any underlying \"machinery\" here, that should all\nbe defined elsewhere.\n\nA lot of the inspiration for this sql grammar is taken from the cockroach\nlabs full sql grammar. In particular their way for dividing up the expression\ngrammar. Check out their docs, they're awesome.\nhttps://www.cockroachlabs.com/docs/stable/sql-grammar.html#select_stmt\n\"\"\"\n\nfrom collections.abc import Generator\nfrom enum import Enum\nfrom typing import NamedTuple, Optional, Union, cast\n\nfrom sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.dialects.common import AliasInfo, ColumnAliasInfo\nfrom sqlfluff.core.parser import (\n    AnyNumberOf,\n    AnySetOf,\n    Anything,\n    BaseFileSegment,\n    BaseSegment,\n    BinaryOperatorSegment,\n    Bracketed,\n    BracketedSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    Conditional,\n    Dedent,\n    Delimited,\n    IdentifierSegment,\n    ImplicitIndent,\n    Indent,\n    KeywordSegment,\n    LiteralKeywordSegment,\n    LiteralSegment,\n    Matchable,\n    MultiStringParser,\n    NewlineSegment,\n    Nothing,\n    OneOf,\n    OptionallyBracketed,\n    ParseMode,\n    RawSegment,\n    Ref,\n    RegexLexer,\n    RegexParser,\n    SegmentGenerator,\n    Sequence,\n    StringLexer,\n    StringParser,\n    SymbolSegment,\n    TypedParser,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.dialects.dialect_ansi_keywords import (\n    ansi_reserved_keywords,\n    ansi_unreserved_keywords,\n)\n\nansi_dialect = Dialect(\n    \"ansi\",\n    root_segment_name=\"FileSegment\",\n    formatted_name=\"ANSI\",\n    docstring=\"\"\"**Default Casing**: ``UPPERCASE``\n\n**Quotes**: String Literals: ``''``, Identifiers: ``\"\"``\n\nThis is the base dialect which holds most of the definitions of common\nSQL commands "}, {"start_line": 0, "end_line": 1715, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Definitions of the segment classes.\"\"\"\n\nfrom sqlfluff.core.parser.segments.base import (\n    BaseSegment,\n    SourceFix,\n    UnparsableSegment,\n)\nfrom sqlfluff.core.parser.segments.bracketed import BracketedSegment\nfrom sqlfluff.core.parser.segments.common import (\n    BinaryOperatorSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    IdentifierSegment,\n    LiteralSegment,\n    NewlineSegment,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.segments.file import BaseFileSegment\nfrom sqlfluff.core.parser.segments.generator import SegmentGenerator\nfrom sqlfluff.core.parser.segments.keyword import KeywordSegment, LiteralKeywordSegment\nfrom sqlfluff.core.parser.segments.meta import (\n    Dedent,\n    EndOfFile,\n    ImplicitIndent,\n    Indent,\n    MetaSegment,\n    TemplateLoop,\n    TemplateSegment,\n)\nfrom sqlfluff.core.parser.segments.raw import RawSegment\n\n__all__ = (\n    \"BaseSegment\",\n    \"BaseFileSegment\",\n    \"UnparsableSegment\",\n    \"BracketedSegment\",\n    \"SegmentGenerator\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"LiteralKeywordSegment\",\n    \"SymbolSegment\",\n    \"MetaSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"TemplateSegment\",\n    \"EndOfFile\",\n    \"TemplateLoop\",\n    \"SourceFix\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n)\n"}, {"start_line": 0, "end_line": 1849, "belongs_to": {"file_name": "helpers.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Helpers for the parser module.\"\"\"\n\nfrom typing import TYPE_CHECKING\n\nfrom sqlfluff.core.errors import SQLParseError\n\nif TYPE_CHECKING:\n    from sqlfluff.core.parser.segments import BaseSegment  # pragma: no cover\n\n\ndef join_segments_raw(segments: tuple[\"BaseSegment\", ...]) -> str:\n    \"\"\"Make a string from the joined `raw` attributes of an iterable of segments.\"\"\"\n    return \"\".join(s.raw for s in segments)\n\n\ndef check_still_complete(\n    segments_in: tuple[\"BaseSegment\", ...],\n    matched_segments: tuple[\"BaseSegment\", ...],\n    unmatched_segments: tuple[\"BaseSegment\", ...],\n) -> bool:\n    \"\"\"Check that the segments in are the same as the segments out.\"\"\"\n    initial_str = join_segments_raw(segments_in)\n    current_str = join_segments_raw(matched_segments + unmatched_segments)\n\n    if initial_str != current_str:  # pragma: no cover\n        segment = unmatched_segments[0] if unmatched_segments else None\n        raise SQLParseError(\n            f\"Parse completeness check fail: {current_str!r} != {initial_str!r}\",\n            segment=segment,\n        )\n    return True\n\n\ndef trim_non_code_segments(\n    segments: tuple[\"BaseSegment\", ...],\n) -> tuple[\n    tuple[\"BaseSegment\", ...], tuple[\"BaseSegment\", ...], tuple[\"BaseSegment\", ...]\n]:\n    \"\"\"Take segments and split off surrounding non-code segments as appropriate.\n\n    We use slices to avoid creating too many unnecessary tuples.\n    \"\"\"\n    pre_idx = 0\n    seg_len = len(segments)\n    post_idx = seg_len\n\n    if segments:\n        seg_len = len(segments)\n\n        # Trim the start\n        while pre_idx < seg_len and not segments[pre_idx].is_code:\n            pre_idx += 1\n\n        # Trim the end\n        while post_idx > pre_idx and not segments[post_idx - 1].is_code:\n            post_idx -= 1\n\n    return segments[:pre_idx], segments[pre_idx:post_idx], segments[post_idx:]\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "file.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Definition of the BaseFileSegment.\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Optional\n\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments.base import BaseSegment, UnparsableSegment\n\n\nclass BaseFileSegment(BaseSegment):\n    \"\"\"A segment representing a whole file or script.\n\n    This is also the default \"root\" segment of the dialect,\n    and so is usually instantiated directly. It therefore\n    has no match_grammar.\n    \"\"\"\n\n    type = \"file\"\n    # The file segment is the only one which can start or end with non-code\n    can_start_end_non_code = True\n    # A file can be empty!\n    allow_empty = True\n\n    def __init__(\n        self,\n        segments: tuple[BaseSegment, ...],\n        pos_marker: Optional[PositionMarker] = None,\n        fname: Optional[str] = None,\n    ):\n        self._file_path = fname\n        super().__init__(segments, pos_marker=pos_marker)\n\n    @property\n    def file_path(self) -> Optional[str]:\n        \"\"\"File path of a parsed SQL file.\"\"\"\n        return self._file_path\n\n    @abstractmethod\n    def get_table_references(self) -> set[str]:\n        \"\"\"Use parsed tree to extract table references.\"\"\"\n\n    @classmethod\n    def root_parse(\n        cls,\n        segments: tuple[BaseSegment, ...],\n        parse_context: ParseContext,\n        fname: Optional[str] = None,\n    ) -> \"BaseFileSegment\":\n        \"\"\"This is the entry method into parsing a file lexed segments.\n\n        For single pass matching, this trims any non code off\n        the start, matches the middle and then trims the end.\n\n        Anything unexpected at the end is regarded as unparsable.\n        \"\"\"\n        # Trim the start\n        _start_idx = 0\n        for _start_idx in range(len(segments)):\n            if segments[_start_idx].is_code:\n                break\n\n        # Trim the end\n        _end_idx = len(segments)\n        for _end_idx in range(len(segments), _start_idx - 1, -1):\n   "}, {"start_line": 0, "end_line": 664, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/grammar", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Definitions of grammars.\"\"\"\n\nfrom sqlfluff.core.parser.grammar.anyof import (\n    AnyNumberOf,\n    AnySetOf,\n    OneOf,\n    OptionallyBracketed,\n)\nfrom sqlfluff.core.parser.grammar.base import Anything, Nothing, Ref\nfrom sqlfluff.core.parser.grammar.conditional import Conditional\nfrom sqlfluff.core.parser.grammar.delimited import Delimited, OptionallyDelimited\nfrom sqlfluff.core.parser.grammar.sequence import Bracketed, Sequence\n\n__all__ = (\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"OneOf\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\",\n    \"Delimited\",\n    \"Sequence\",\n    \"Bracketed\",\n    \"Conditional\",\n)\n"}], "retrieved_count": 10, "cost_time": 1.1898164749145508}
{"question": "Why does SQLFluff include a template processing system for dynamic SQL files?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff includes a template processing system for dynamic SQL files to support modern SQL development practices where SQL code is often generated or templated. Key reasons include: 1) Modern SQL workflows - Many organizations use templating engines like Jinja, dbt, or Python format strings to generate dynamic SQL; 2) Code reusability - Templates allow SQL code to be parameterized and reused across different contexts; 3) Environment-specific SQL - Templates enable SQL to be adapted for different environments (dev, staging, production) with different parameters; 4) Dynamic queries - Many applications generate SQL queries dynamically based on user input or application state; 5) Framework integration - Tools like dbt, Apache Airflow, and other data engineering frameworks heavily use templated SQL; 6) Linting accuracy - To properly lint templated SQL, SQLFluff must first render the templates to see the actual SQL that will be executed; 7) Source mapping - The template system maintains mapping between the original templated code and the rendered SQL for accurate error reporting; 8) Conditional logic - Templates often include conditional statements that generate different SQL based on parameters; 9) Loop constructs - Templates support loops that generate repetitive SQL structures; 10) Variable substitution - Templates allow for dynamic variable substitution while maintaining SQL syntax validation.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the templaters.\"\"\"\n\nimport logging\nfrom bisect import bisect_left\nfrom collections.abc import Iterable, Iterator\nfrom typing import (\n    Any,\n    Callable,\n    NamedTuple,\n    Optional,\n    TypeVar,\n)\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLFluffSkipFile, SQLTemplaterError\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.helpers.slice import zero_slice\n\n# Instantiate the templater logger\ntemplater_logger = logging.getLogger(\"sqlfluff.templater\")\n\n\ndef iter_indices_of_newlines(raw_str: str) -> Iterator[int]:\n    \"\"\"Find the indices of all newlines in a string.\"\"\"\n    init_idx = -1\n    while True:\n        nl_pos = raw_str.find(\"\\n\", init_idx + 1)\n        if nl_pos >= 0:\n            yield nl_pos\n            init_idx = nl_pos\n        else:\n            break  # pragma: no cover TODO?\n\n\nT = TypeVar(\"T\")\n\n\ndef large_file_check(func: Callable[..., T]) -> Callable[..., T]:\n    \"\"\"Raise an exception if the file is over a defined size.\n\n    Designed to be implemented as a decorator on `.process()` methods.\n\n    If no config is provided or the relevant config value is set\n    to zero then the check is skipped.\n    \"\"\"\n\n    def _wrapped(\n        self: Any,\n        *,\n        in_str: str,\n        fname: str,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n    ) -> T:\n        if config:\n            limit = config.get(\"large_file_skip_char_limit\")\n            if limit:\n                templater_logger.warning(\n                    \"The config value large_file_skip_char_limit was found set. \"\n                    \"This feature will be removed in a future release, please \"\n                    \"use the more efficient 'large_file_skip_byte_limit' instead.\"\n                )\n            if limit and len(in_str) > limit:\n                raise SQLFluffSkipFile(\n                    f\"Length of file {fname!r} is over {limit} characters. \"\n                    \"Sk"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "python.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the templaters.\"\"\"\n\nimport ast\nimport re\nfrom collections.abc import Iterable, Iterator\nfrom string import Formatter\nfrom typing import Any, Callable, NamedTuple, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLTemplaterError\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.helpers.slice import offset_slice, zero_slice\nfrom sqlfluff.core.helpers.string import findall\nfrom sqlfluff.core.templaters.base import (\n    RawFileSlice,\n    RawTemplater,\n    TemplatedFile,\n    TemplatedFileSlice,\n    large_file_check,\n    templater_logger,\n)\n\n\nclass IntermediateFileSlice(NamedTuple):\n    \"\"\"An intermediate representation of a partially sliced File.\"\"\"\n\n    intermediate_type: str\n    source_slice: slice\n    templated_slice: slice\n    slice_buffer: list[RawFileSlice]\n\n    def _trim_end(\n        self, templated_str: str, target_end: str = \"head\"\n    ) -> tuple[\"IntermediateFileSlice\", list[TemplatedFileSlice]]:\n        \"\"\"Trim the ends of a intermediate segment.\"\"\"\n        target_idx = 0 if target_end == \"head\" else -1\n        terminator_types = (\"block_start\") if target_end == \"head\" else (\"block_end\")\n        main_source_slice = self.source_slice\n        main_templated_slice = self.templated_slice\n        slice_buffer = self.slice_buffer\n\n        end_buffer = []\n\n        # Yield any leading literals, comments or blocks.\n        while len(slice_buffer) > 0 and slice_buffer[target_idx].slice_type in (\n            \"literal\",\n            \"block_start\",\n            \"block_end\",\n            \"comment\",\n        ):\n            focus = slice_buffer[target_idx]\n            templater_logger.debug(\"            %s Focus: %s\", target_end, focus)\n            # Is it a zero length item?\n            if focus.slice_type in (\"block_start\", \"block_end\", \"comment\"):\n                # Only add the length in the source space.\n                templated_len = 0\n            else:\n                # Assume it's a literal, chec"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "placeholder.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the placeholder template.\"\"\"\n\nimport logging\nfrom typing import Any, Optional\n\nimport regex\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLTemplaterError\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.helpers.slice import offset_slice\nfrom sqlfluff.core.templaters.base import (\n    RawFileSlice,\n    RawTemplater,\n    TemplatedFile,\n    TemplatedFileSlice,\n    large_file_check,\n)\n\n# Instantiate the templater logger\ntemplater_logger = logging.getLogger(\"sqlfluff.templater\")\n\nKNOWN_STYLES = {\n    # e.g. WHERE bla = :name\n    \"colon\": regex.compile(r\"(?<![:\\w\\x5c]):(?P<param_name>\\w+)(?!:)\", regex.UNICODE),\n    # e.g. SELECT :\"column\" FROM :table WHERE bla = :'name'\n    \"colon_optional_quotes\": regex.compile(\n        r\"(?<!:):(?P<quotation>['\\\"]?)(?P<param_name>[\\w_]+)\\1\", regex.UNICODE\n    ),\n    # e.g. WHERE bla = table:name - use with caution as more prone to false positives\n    \"colon_nospaces\": regex.compile(r\"(?<!:):(?P<param_name>\\w+)\", regex.UNICODE),\n    # e.g. WHERE bla = :2\n    \"numeric_colon\": regex.compile(\n        r\"(?<![:\\w\\x5c]):(?P<param_name>\\d+)\", regex.UNICODE\n    ),\n    # e.g. WHERE bla = %(name)s\n    \"pyformat\": regex.compile(\n        r\"(?<![:\\w\\x5c])%\\((?P<param_name>[\\w_]+)\\)s\", regex.UNICODE\n    ),\n    # e.g. WHERE bla = $name or WHERE bla = ${name}\n    \"dollar\": regex.compile(\n        r\"(?<![:\\w\\x5c])\\${?(?P<param_name>[\\w_]+)}?\", regex.UNICODE\n    ),\n    # e.g. WHERE bla = $name$ (DbUp compatible)\n    \"dollar_surround\": regex.compile(\n        r\"(?<![:\\w\\x5c])\\$(?P<param_name>[-\\w]+)\\$\", regex.UNICODE\n    ),\n    # e.g. USE ${flyway:database}.schema_name;\n    \"flyway_var\": regex.compile(r\"\\${(?P<param_name>\\w+[:\\w_]+)}\", regex.UNICODE),\n    # e.g. WHERE bla = ?\n    \"question_mark\": regex.compile(r\"(?<![:\\w\\x5c])\\?\", regex.UNICODE),\n    # e.g. WHERE bla = $3 or WHERE bla = ${3}\n    \"numeric_dollar\": regex.compile(\n        r\"(?<![:\\w\\x5c])\\${?(?P<param_name>[\\d]+)}?\", regex.U"}, {"start_line": 0, "end_line": 863, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Templater Code.\"\"\"\n\nfrom collections.abc import Iterator\n\n# Although these shouldn't usually be instantiated from here\n# we import them to make sure they get registered.\nfrom sqlfluff.core.templaters.base import RawFileSlice, RawTemplater, TemplatedFile\nfrom sqlfluff.core.templaters.jinja import JinjaTemplater\nfrom sqlfluff.core.templaters.placeholder import PlaceholderTemplater\nfrom sqlfluff.core.templaters.python import PythonTemplater\n\n\ndef core_templaters() -> Iterator[type[RawTemplater]]:\n    \"\"\"Returns the templater tuples for the core templaters.\"\"\"\n    yield from [\n        RawTemplater,\n        JinjaTemplater,\n        PythonTemplater,\n        PlaceholderTemplater,\n    ]\n\n\n__all__ = (\n    \"RawFileSlice\",\n    \"TemplatedFile\",\n    \"RawTemplater\",\n    \"JinjaTemplater\",\n    \"PythonTemplater\",\n    \"PlaceholderTemplater\",\n    \"core_templaters\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "jinja.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the templaters.\"\"\"\n\nimport copy\nimport importlib\nimport importlib.util\nimport logging\nimport os.path\nimport pkgutil\nimport sys\nfrom collections.abc import Iterable, Iterator\nfrom functools import reduce\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Optional,\n    Union,\n    cast,\n)\n\nimport jinja2.nodes\nimport jinja2.parser\nfrom jinja2 import (\n    Environment,\n    FileSystemLoader,\n    TemplateError,\n    TemplateSyntaxError,\n    meta,\n)\nfrom jinja2.exceptions import TemplateNotFound, UndefinedError\nfrom jinja2.ext import Extension\nfrom jinja2.sandbox import SandboxedEnvironment\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLFluffUserError, SQLTemplaterError\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.helpers.slice import is_zero_slice, slice_length\nfrom sqlfluff.core.templaters.base import (\n    RawFileSlice,\n    TemplatedFile,\n    TemplatedFileSlice,\n    large_file_check,\n)\nfrom sqlfluff.core.templaters.builtins.dbt import DBT_BUILTINS\nfrom sqlfluff.core.templaters.python import PythonTemplater\nfrom sqlfluff.core.templaters.slicers.tracer import JinjaAnalyzer, JinjaTrace\n\nif TYPE_CHECKING:  # pragma: no cover\n    from jinja2.runtime import Macro\n\n# Instantiate the templater logger\ntemplater_logger = logging.getLogger(\"sqlfluff.templater\")\n\n\nclass UndefinedRecorder:\n    \"\"\"Similar to jinja2.StrictUndefined, but remembers, not fails.\"\"\"\n\n    # Tell Jinja this object is safe to call and does not alter data.\n    # https://jinja.palletsprojects.com/en/3.0.x/sandbox/#jinja2.sandbox.SandboxedEnvironment.is_safe_callable\n    unsafe_callable = False\n    alters_data = False\n\n    def __init__(self, name: str, undefined_set: set[str]) -> None:\n        self.name = name\n        # Reference to undefined set to modify, it is assumed that the\n        # calling code keeps a reference to this variable to they can\n        # continue to access it after modification by this class.\n        sel"}, {"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "python_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     # templated file from the test case.\n        (lambda x: templated_file),\n        config=FluffConfig(\n            configs={\"templater\": {\"unwrap_wrapped_queries\": unwrap_wrapped}},\n            overrides={\"dialect\": \"ansi\"},\n        ),\n    )\n    # Check contiguous\n    prev_slice = None\n    for templated_slice in resp:\n        if prev_slice:\n            assert templated_slice.source_slice.start == prev_slice[0].stop\n            assert templated_slice.templated_slice.start == prev_slice[1].stop\n        prev_slice = (templated_slice.source_slice, templated_slice.templated_slice)\n    # check result\n    assert resp == result\n\n\ndef test__templater_python_large_file_check():\n    \"\"\"Test large file skipping.\n\n    The check is separately called on each .process() method\n    so it makes sense to test a few templaters.\n    \"\"\"\n    # First check we can process the file normally without config.\n    PythonTemplater().process(in_str=\"SELECT 1\", fname=\"<string>\")\n    # Then check we raise a skip exception when config is set low.\n    with pytest.raises(SQLFluffSkipFile) as excinfo:\n        PythonTemplater().process(\n            in_str=\"SELECT 1\",\n            fname=\"<string>\",\n            config=FluffConfig(\n                overrides={\"dialect\": \"ansi\", \"large_file_skip_char_limit\": 2},\n            ),\n        )\n\n    assert \"Length of file\" in str(excinfo.value)\n\n\n@pytest.mark.parametrize(\n    \"raw_str,result\",\n    [\n        (\"\", \"\"),\n        (\n            \"SELECT * FROM {foo.bar}\",\n            \"SELECT * FROM foobar\",\n        ),\n        (\n            \"SELECT {foo} FROM {foo.bar}\",\n            \"SELECT bar FROM foobar\",\n        ),\n        (\n            \"SELECT {num:.2f} FROM blah\",\n            \"SELECT 123.00 FROM blah\",\n        ),\n        (\n            \"SELECT {self.number:.1f} FROM blah\",\n            \"SELECT 42.0 FROM blah\",\n        ),\n        (\n            \"SELECT * FROM {obj.schema}.{obj.table}\",\n            \"SELECT * FROM my_schema.my_table\",\n        ),\n    ],\n)\ndef test__templat"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "python_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for templaters.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import FluffConfig, SQLTemplaterError\nfrom sqlfluff.core.errors import SQLFluffSkipFile\nfrom sqlfluff.core.templaters import PythonTemplater\nfrom sqlfluff.core.templaters.base import RawFileSlice, TemplatedFileSlice\nfrom sqlfluff.core.templaters.python import IntermediateFileSlice\n\nPYTHON_STRING = \"SELECT * FROM {blah}\"\n\n\ndef test__templater_python():\n    \"\"\"Test the python templater.\"\"\"\n    t = PythonTemplater(override_context=dict(blah=\"foo\"))\n    instr = PYTHON_STRING\n    outstr, _ = t.process(in_str=instr, fname=\"test\")\n    assert str(outstr) == \"SELECT * FROM foo\"\n\n\ndef test__templater_python_error():\n    \"\"\"Test error handling in the python templater.\"\"\"\n    t = PythonTemplater(override_context=dict(noblah=\"foo\"))\n    instr = PYTHON_STRING\n    with pytest.raises(SQLTemplaterError):\n        t.process(in_str=instr, fname=\"test\")\n\n\n@pytest.mark.parametrize(\n    \"int_slice,templated_str,head_test,tail_test,int_test\",\n    [\n        # Test Invariante\n        (\n            IntermediateFileSlice(\n                \"compound\",\n                slice(0, 5),\n                slice(0, 5),\n                [RawFileSlice(\"{{i}}\", \"templated\", 0)],\n            ),\n            \"foo\",\n            [],\n            [],\n            IntermediateFileSlice(\n                \"compound\",\n                slice(0, 5),\n                slice(0, 5),\n                [RawFileSlice(\"{{i}}\", \"templated\", 0)],\n            ),\n        ),\n        # Test Complete Trimming\n        (\n            IntermediateFileSlice(\n                \"compound\",\n                slice(0, 3),\n                slice(0, 3),\n                [RawFileSlice(\"foo\", \"literal\", 0)],\n            ),\n            \"foo\",\n            [TemplatedFileSlice(\"literal\", slice(0, 3), slice(0, 3))],\n            [],\n            IntermediateFileSlice(\n                \"compound\",\n                slice(3, 3),\n                slice(3, 3),\n                [],\n            ),\n    "}, {"start_line": 22000, "end_line": 24000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "f SQLTemplaterError\n            if templating was successful enough that we may move to attempt parsing.\n\n        Raises:\n            SQLTemplaterError: If templating fails fatally, then this method\n                should raise a :obj:`SQLTemplaterError` instead which will be\n                caught and displayed appropriately.\n\n        \"\"\"\n        return TemplatedFile(in_str, fname=fname), []\n\n    @large_file_check\n    def process_with_variants(\n        self,\n        *,\n        in_str: str,\n        fname: str,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n    ) -> Iterator[tuple[TemplatedFile, list[SQLTemplaterError]]]:\n        \"\"\"Extended version of `process` which returns multiple variants.\n\n        Unless explicitly defined, this simply yields the result of .process().\n        \"\"\"\n        yield self.process(\n            in_str=in_str, fname=fname, config=config, formatter=formatter\n        )\n\n    def __eq__(self, other: Any) -> bool:\n        \"\"\"Return true if `other` is of the same class as this one.\n\n        NB: This is useful in comparing configs.\n        \"\"\"\n        return isinstance(other, self.__class__)\n\n    def config_pairs(self) -> list[tuple[str, str]]:\n        \"\"\"Returns info about the given templater for output by the cli.\n\n        Returns:\n            list[tuple[str, str]]: A list of tuples containing information\n                about the given templater. Each tuple contains two strings:\n                the string 'templater' and the name of the templater.\n        \"\"\"\n        return [(\"templater\", self.name)]\n\n    def get_context(\n        self,\n        fname: Optional[str],\n        config: Optional[FluffConfig],\n    ) -> dict[str, Any]:\n        \"\"\"Get the templating context from the config.\n\n        This function retrieves the templating context from the config by\n        loading the config and updating the live_context dictionary with the\n        loaded_context and other predefined context dicti"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "placeholder_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for templaters.\"\"\"\n\nimport pytest\n\nfrom sqlfluff.core import FluffConfig\nfrom sqlfluff.core.templaters import PlaceholderTemplater\n\n\ndef test__templater_raw():\n    \"\"\"Test the templaters when nothing has to be replaced.\"\"\"\n    t = PlaceholderTemplater(override_context=dict(param_style=\"colon\"))\n    instr = \"SELECT * FROM {{blah}} WHERE %(gnepr)s OR e~':'\"\n    outstr, _ = t.process(in_str=instr, fname=\"test\")\n    assert str(outstr) == instr\n\n\n@pytest.mark.parametrize(\n    \"instr, param_style, expected_outstr, values\",\n    [\n        (\n            \"SELECT * FROM f, o, o WHERE a < 10\\n\\n\",\n            \"colon\",\n            \"SELECT * FROM f, o, o WHERE a < 10\\n\\n\",\n            dict(\n                unused=7777,\n            ),\n        ),\n        (\n            \"\"\"\n            SELECT user_mail, city_id\n            FROM users_data\n            WHERE userid = :user_id AND date > :start_date\n            \"\"\",\n            \"colon\",\n            \"\"\"\n            SELECT user_mail, city_id\n            FROM users_data\n            WHERE userid = 42 AND date > '2021-10-01'\n            \"\"\",\n            dict(\n                user_id=\"42\",\n                start_date=\"'2021-10-01'\",\n                city_ids=\"(1, 2, 3, 45)\",\n            ),\n        ),\n        (\n            \"\"\"\n            SELECT user_mail, city_id\n            FROM users_data\n            WHERE userid = :user_id AND date > :start_date\"\"\",\n            \"colon\",\n            \"\"\"\n            SELECT user_mail, city_id\n            FROM users_data\n            WHERE userid = 42 AND date > '2021-10-01'\"\"\",\n            dict(\n                user_id=\"42\",\n                start_date=\"'2021-10-01'\",\n                city_ids=\"(1, 2, 3, 45)\",\n            ),\n        ),\n        (\n            \"\"\"\n            SELECT user_mail, city_id\n            FROM users_data\n            WHERE (city_id) IN :city_ids\n            AND date > '2020-10-01'\n            \"\"\",\n            \"colon\",\n            \"\"\"\n            SELECT user_mail, city_id\n            F"}, {"start_line": 17000, "end_line": 19000, "belongs_to": {"file_name": "python_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ception when config is set low.\n    with pytest.raises(SQLFluffSkipFile) as excinfo:\n        PythonTemplater().process(\n            in_str=\"SELECT 1\",\n            fname=\"<string>\",\n            config=FluffConfig(\n                overrides={\"dialect\": \"ansi\", \"large_file_skip_char_limit\": 2},\n            ),\n        )\n\n    assert \"Length of file\" in str(excinfo.value)\n\n\n@pytest.mark.parametrize(\n    \"raw_str,result\",\n    [\n        (\"\", \"\"),\n        (\n            \"SELECT * FROM {foo.bar}\",\n            \"SELECT * FROM foobar\",\n        ),\n        (\n            \"SELECT {foo} FROM {foo.bar}\",\n            \"SELECT bar FROM foobar\",\n        ),\n        (\n            \"SELECT {num:.2f} FROM blah\",\n            \"SELECT 123.00 FROM blah\",\n        ),\n        (\n            \"SELECT {self.number:.1f} FROM blah\",\n            \"SELECT 42.0 FROM blah\",\n        ),\n        (\n            \"SELECT * FROM {obj.schema}.{obj.table}\",\n            \"SELECT * FROM my_schema.my_table\",\n        ),\n    ],\n)\ndef test__templater_python_dot_notation_variables(raw_str, result):\n    \"\"\"Test template variables that contain a dot character (`.`).\"\"\"\n    context = {\n        \"foo\": \"bar\",\n        \"num\": 123,\n        \"sqlfluff\": {\n            \"foo.bar\": \"foobar\",\n            \"self.number\": 42,\n            \"obj.schema\": \"my_schema\",\n            \"obj.table\": \"my_table\",\n        },\n    }\n    t = PythonTemplater(override_context=context)\n    instr = raw_str\n    outstr, _ = t.process(in_str=instr, fname=\"test\")\n    assert str(outstr) == result\n\n\n@pytest.mark.parametrize(\n    \"context,error_string\",\n    [\n        # No additional context (i.e. no sqlfluff key)\n        (\n            {},\n            \"magic key 'sqlfluff' missing from context.  This key is required \"\n            \"for template variables containing '.'.\",\n        ),\n        # No key missing within sqlfluff dict.\n        (\n            {\"sqlfluff\": {\"a\": \"b\"}},\n            \"'foo.bar' key missing from 'sqlfluff' dict in context. Template \"\n            \"variables "}], "retrieved_count": 10, "cost_time": 1.2230989933013916}
{"question": "Where does SQLFluff store its rule implementations?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff stores its rule implementations across several organized locations in the codebase. The main rule storage locations include: 1) src/sqlfluff/rules/ - The main directory containing all built-in rule implementations organized by category; 2) src/sqlfluff/rules/capitalisation/ - Rules for SQL capitalization and formatting standards; 3) src/sqlfluff/rules/layout/ - Rules for SQL layout, spacing, and indentation; 4) src/sqlfluff/rules/aliasing/ - Rules for table and column alias usage; 5) src/sqlfluff/rules/references/ - Rules for proper table and column reference handling; 6) src/sqlfluff/rules/ambiguous/ - Rules for detecting ambiguous SQL constructs; 7) src/sqlfluff/rules/structure/ - Rules for SQL structure and organization; 8) src/sqlfluff/rules/convention/ - Rules for SQL naming conventions and best practices; 9) src/sqlfluff/rules/jinja/ - Rules specific to Jinja templating; 10) src/sqlfluff/rules/tsql/ - Rules specific to T-SQL dialect; 11) src/sqlfluff/core/rules/base.py - Contains the BaseRule class that all rules inherit from; 12) plugins/ - Custom rules can be implemented as plugins in separate packages.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 1576, "belongs_to": {"file_name": "loader.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Methods to load rules.\"\"\"\n\nimport os\nfrom glob import glob\nfrom importlib import import_module\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.rules.base import BaseRule\n\n\ndef get_rules_from_path(\n    # All rule files are expected in the format of L*.py\n    rules_path: str = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), \"../../rules\", \"L*.py\")\n    ),\n    base_module: str = \"sqlfluff.rules\",\n) -> list[type[\"BaseRule\"]]:\n    \"\"\"Reads all of the Rule classes from a path into a list.\"\"\"\n    # Create a rules dictionary for importing in\n    # sqlfluff/src/sqlfluff/core/rules/__init__.py\n    rules = []\n\n    for module in sorted(glob(rules_path)):\n        # Manipulate the module path to extract the filename without the .py\n        rule_id = os.path.splitext(os.path.basename(module))[0]\n        # All rule classes are expected in the format of Rule_L*\n        rule_class_name = f\"Rule_{rule_id}\"\n        # NOTE: We import the module outside of the try clause to\n        # properly catch any import errors.\n        rule_module = import_module(f\"{base_module}.{rule_id}\")\n        try:\n            rule_class = getattr(rule_module, rule_class_name)\n        except AttributeError as e:\n            raise AttributeError(\n                \"Rule classes must be named in the format of Rule_*. \"\n                f\"[{rule_class_name}]\"\n            ) from e\n        # Add the rules to the rules dictionary for\n        # sqlfluff/src/sqlfluff/core/rules/__init__.py\n        rules.append(rule_class)\n\n    return rules\n"}, {"start_line": 0, "end_line": 1349, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Configuration and examples for individual rules.\"\"\"\n\nfrom sqlfluff.core.plugin.host import get_plugin_manager\nfrom sqlfluff.core.rules.base import (\n    BaseRule,\n    EvalResultType,\n    LintResult,\n    RuleGhost,\n    RulePack,\n    RuleSet,\n)\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.fix import LintFix\n\n\ndef _load_standard_rules() -> RuleSet:\n    \"\"\"Initialise the standard ruleset.\n\n    We do this on each call so that dynamic rules changes\n    are possible.\n    \"\"\"\n    std_rule_set = RuleSet(name=\"standard\", config_info=get_config_info())\n\n    # Iterate through the rules list and register each rule with the standard set.\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            std_rule_set.register(rule)\n\n    return std_rule_set\n\n\ndef get_ruleset(name: str = \"standard\") -> RuleSet:\n    \"\"\"Get a ruleset by name.\"\"\"\n    std_rules = _load_standard_rules()\n    lookup = {std_rules.name: std_rules}\n    # Return a copy in case someone modifies the register.\n    return lookup[name].copy()\n\n\n__all__ = (\n    \"get_ruleset\",\n    \"RuleSet\",\n    \"RulePack\",\n    \"BaseRule\",\n    \"LintResult\",\n    \"LintFix\",\n    \"RuleContext\",\n    \"RuleGhost\",\n    \"EvalResultType\",\n    \"ConfigInfo\",\n)\n"}, {"start_line": 0, "end_line": 45, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Standard Rules packaged with sqlfluff.\"\"\"\n"}, {"start_line": 0, "end_line": 1269, "belongs_to": {"file_name": "lib.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/plugin", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Base implementation for the plugin.\"\"\"\n\nfrom typing import Any\n\nfrom sqlfluff.core.config import load_config_resource\nfrom sqlfluff.core.plugin import hookimpl\nfrom sqlfluff.core.rules import BaseRule, ConfigInfo\nfrom sqlfluff.core.rules.config_info import STANDARD_CONFIG_INFO_DICT\nfrom sqlfluff.core.rules.loader import get_rules_from_path\nfrom sqlfluff.core.templaters import RawTemplater, core_templaters\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: All standard rules will eventually be loaded as\n    plugins and so before 2.0.0, once all legacy plugin definitions\n    are migrated, this function will be amended to return no rules.\n    \"\"\"\n    return get_rules_from_path()\n\n\n@hookimpl\ndef get_templaters() -> list[type[RawTemplater]]:\n    \"\"\"Get templaters.\"\"\"\n    templaters = list(t for t in core_templaters())\n    return templaters\n\n\n@hookimpl\ndef load_default_config() -> dict[str, Any]:\n    \"\"\"Loads the default configuration for the plugin.\"\"\"\n    return load_config_resource(\n        package=\"sqlfluff.core\",\n        file_name=\"default_config.cfg\",\n    )\n\n\n@hookimpl\ndef get_configs_info() -> dict[str, ConfigInfo]:\n    \"\"\"Get rule config validations and descriptions.\"\"\"\n    return STANDARD_CONFIG_INFO_DICT\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "rules_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the standard set of rules.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import Linter\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.linter import RuleTuple\nfrom sqlfluff.core.parser import WhitespaceSegment\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.rules import BaseRule, LintFix, LintResult, get_ruleset\nfrom sqlfluff.core.rules.crawlers import RootOnlyCrawler, SegmentSeekerCrawler\nfrom sqlfluff.core.rules.doc_decorators import (\n    document_configuration,\n    document_fix_compatible,\n    document_groups,\n)\nfrom sqlfluff.core.rules.loader import get_rules_from_path\nfrom sqlfluff.core.templaters.base import TemplatedFile\nfrom sqlfluff.utils.testing.logging import fluff_log_catcher\nfrom sqlfluff.utils.testing.rules import get_rule_from_set\nfrom test.fixtures.rules.custom.L000 import Rule_L000\nfrom test.fixtures.rules.custom.S000 import Rule_S000\n\n\nclass Rule_T042(BaseRule):\n    \"\"\"A dummy rule.\"\"\"\n\n    groups = (\"all\",)\n\n    def _eval(self, context):\n        pass\n\n\nclass Rule_T001(BaseRule):\n    \"\"\"A deliberately malicious rule.\n\n    **Anti-pattern**\n\n    Blah blah\n    \"\"\"\n\n    groups = (\"all\",)\n    crawl_behaviour = SegmentSeekerCrawler({\"whitespace\"})\n    is_fix_compatible = True\n\n    def _eval(self, context):\n        \"\"\"Stars make newlines.\"\"\"\n        if context.segment.is_type(\"whitespace\"):\n            return LintResult(\n                anchor=context.segment,\n                fixes=[\n                    LintFix.replace(\n                        context.segment, [WhitespaceSegment(context.segment.raw + \" \")]\n                    )\n                ],\n            )\n\n\nclass Rule_T002(BaseRule):\n    \"\"\"A rule which says all raw code segments are bad.\n\n    This is used for testing unparsable code.\n    \"\"\"\n\n    groups = (\"all\",)\n    # Root only crawler so that the in-rule filters don't kick in.\n    crawl_behaviour = RootOnlyCrawler()\n\n    def _eval(s"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Implements the base rule class.\n\nRules crawl through the trees returned by the parser and evaluate particular\nrules.\n\nThe intent is that it should be possible for the rules to be expressed\nas simply as possible, with as much of the complexity abstracted away.\n\nThe evaluation function should take enough arguments that it can evaluate\nthe position of the given segment in relation to its neighbors, and that\nthe segment which finally \"triggers\" the error, should be the one that would\nbe corrected OR if the rule relates to something that is missing, then it\nshould flag on the segment FOLLOWING, the place that the desired element is\nmissing.\n\"\"\"\n\nimport bdb\nimport copy\nimport fnmatch\nimport logging\nimport pathlib\nimport re\nfrom collections import defaultdict, namedtuple\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    ClassVar,\n    DefaultDict,\n    Optional,\n    Union,\n)\n\nimport regex\n\nfrom sqlfluff.core.errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a "}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "layed in the sqlfluff\n    docs.\n    \"\"\"\n\n    # Precompile the regular expressions\n    _doc_search_regex: ClassVar = re.compile(\n        \"(\\\\s{4}\\\\*\\\\*Anti-pattern\\\\*\\\\*|\\\\s{4}\\\\.\\\\. note::|\"\n        \"\\\\s\\\\s{4}\\\\*\\\\*Configuration\\\\*\\\\*)\",\n        flags=re.MULTILINE,\n    )\n    _valid_classname_regex: ClassVar = regex.compile(\n        r\"Rule_?([A-Z]{1}[a-zA-Z]+)?_([A-Z0-9]{4})\"\n    )\n    _valid_rule_name_regex: ClassVar = regex.compile(r\"[a-z][a-z\\.\\_]+\")\n\n    @staticmethod\n    def _populate_code_and_description(\n        name: str, class_dict: dict[str, Any]\n    ) -> dict[str, Any]:\n        \"\"\"Extract and validate the rule code & description.\n\n        We expect that rules are defined as classes with the name `Rule_XXXX`\n        where `XXXX` is of the form `LLNN`, where L is a letter and N is a\n        two digit number. For backward compatibility we also still support\n        the legacy format of LNNN i.e. a single letter and three digit number.\n\n        The two letters should be indicative of the grouping and focus of\n        the rule. e.g. capitalisation rules have the code CP for CaPitalisation.\n\n        If this receives classes by any other name, then it will raise a\n        :exc:`ValueError`.\n        \"\"\"\n        rule_name_match = RuleMetaclass._valid_classname_regex.match(name)\n        # Validate the name\n        if not rule_name_match:  # pragma: no cover\n            raise SQLFluffUserError(\n                f\"Tried to define rule class with \"\n                f\"unexpected format: {name}. Format should be: \"\n                \"'Rule_PluginName_LL23' (for plugins) or \"\n                \"`Rule_LL23` (for core rules).\"\n            )\n\n        plugin_name, code = rule_name_match.groups()\n        # If the docstring is multiline, then we extract just summary.\n        description = class_dict[\"__doc__\"].replace(\"``\", \"'\").split(\"\\n\")[0]\n        if plugin_name:\n            code = f\"{plugin_name}_{code}\"\n\n        class_dict[\"code\"] = code\n        class_dict[\"description\"] = desc"}, {"start_line": 1000, "end_line": 2820, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e that currently, the gap _before_ and _after_ the semicolon \"\n                \"is considered 'between' statements.\"\n            ),\n        },\n        \"maximum_empty_lines_inside_statements\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The maximum number of empty lines allowed inside statements.\"\n            ),\n        },\n        \"wildcard_policy\": {\n            \"validation\": [\"single\", \"multiple\"],\n            \"definition\": \"Treatment of wildcards. Defaults to ``single``.\",\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.layout.LT01 import Rule_LT01\n    from sqlfluff.rules.layout.LT02 import Rule_LT02\n    from sqlfluff.rules.layout.LT03 import Rule_LT03\n    from sqlfluff.rules.layout.LT04 import Rule_LT04\n    from sqlfluff.rules.layout.LT05 import Rule_LT05\n    from sqlfluff.rules.layout.LT06 import Rule_LT06\n    from sqlfluff.rules.layout.LT07 import Rule_LT07\n    from sqlfluff.rules.layout.LT08 import Rule_LT08\n    from sqlfluff.rules.layout.LT09 import Rule_LT09\n    from sqlfluff.rules.layout.LT10 import Rule_LT10\n    from sqlfluff.rules.layout.LT11 import Rule_LT11\n    from sqlfluff.rules.layout.LT12 import Rule_LT12\n    from sqlfluff.rules.layout.LT13 import Rule_LT13\n    from sqlfluff.rules.layout.LT14 import Rule_LT14\n    from sqlfluff.rules.layout.LT15 import Rule_LT15\n\n    return [\n        Rule_LT01,\n        Rule_LT02,\n        Rule_LT03,\n        Rule_LT04,\n        Rule_LT05,\n        Rule_LT06,\n        Rule_LT07,\n        Rule_LT08,\n        Rule_LT09,\n        Rule_LT10,\n        Rule_LT11,\n        Rule_LT12,\n        Rule_LT13,\n        Rule_LT14,\n        Rule_LT15,\n    ]\n"}, {"start_line": 0, "end_line": 1183, "belongs_to": {"file_name": "hookspecs.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/plugin", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the specification to implement a plugin.\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import TYPE_CHECKING, Any\n\nimport pluggy\n\nfrom sqlfluff.core.plugin import plugin_base_name\n\nif TYPE_CHECKING:  # pragma: no cover\n    # NOTE: This import is against the normal import rules, but is here for strict\n    # type checking. We have an exception for this in the import linter.\n    from sqlfluff.core.rules.base import BaseRule\n\nhookspec = pluggy.HookspecMarker(plugin_base_name)\n\n\nclass PluginSpec:\n    \"\"\"Defines the method signatures for plugin implementations.\"\"\"\n\n    @hookspec\n    @abstractmethod\n    def get_rules(self) -> list[type[\"BaseRule\"]]:\n        \"\"\"Get plugin rules.\"\"\"\n\n    @hookspec\n    @abstractmethod\n    def load_default_config(self) -> dict[str, Any]:\n        \"\"\"Loads the default configuration for the plugin.\"\"\"\n\n    @hookspec\n    @abstractmethod\n    # TODO: This type annotation could probably be more specific but that would\n    # require making the config info object something more like a namedTuple rather\n    # than a dict.\n    def get_configs_info(self) -> dict[str, dict[str, Any]]:\n        \"\"\"Get rule config validations and descriptions.\"\"\"\n"}, {"start_line": 1000, "end_line": 2695, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      \"validation\": [\"all\", \"aliases\", \"column_aliases\", \"table_aliases\", \"none\"],\n            \"definition\": \"Types of quoted identifiers to flag violations for.\",\n        },\n        \"allow_space_in_identifier\": {\n            \"validation\": [True, False],\n            \"definition\": (\"Should spaces in identifiers be allowed?\"),\n        },\n        \"additional_allowed_characters\": {\n            \"definition\": (\n                \"Optional list of extra allowed characters, \"\n                \"in addition to alphanumerics (A-Z, a-z, 0-9) and underscores.\"\n            ),\n        },\n        \"prefer_quoted_identifiers\": {\n            \"validation\": [True, False],\n            \"definition\": (\n                \"If ``True``, requires every identifier to be quoted. \"\n                \"Defaults to ``False``.\"\n            ),\n        },\n        \"prefer_quoted_keywords\": {\n            \"validation\": [True, False],\n            \"definition\": (\n                \"If ``True``, requires every keyword used as an identifier to be \"\n                \"quoted. Defaults to ``False``.\"\n            ),\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.references.RF01 import Rule_RF01\n    from sqlfluff.rules.references.RF02 import Rule_RF02\n    from sqlfluff.rules.references.RF03 import Rule_RF03\n    from sqlfluff.rules.references.RF04 import Rule_RF04\n    from sqlfluff.rules.references.RF05 import Rule_RF05\n    from sqlfluff.rules.references.RF06 import Rule_RF06\n\n    return [Rule_RF01, Rule_RF02, Rule_RF03, Rule_RF04, Rule_RF05, Rule_RF06]\n"}], "retrieved_count": 10, "cost_time": 1.2054173946380615}
{"question": "Where does SQLFluff's rule evaluation flow from rule discovery through fix generation?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's rule evaluation flow follows a systematic process from rule discovery to fix generation. The flow includes: 1) Rule discovery - Rules are discovered through the RuleSet class which maintains a registry of all available rules; 2) Rule filtering - Rules are filtered based on configuration settings (rules, exclude_rules, dialect compatibility); 3) Rule instantiation - Filtered rules are instantiated with configuration parameters and context; 4) Parse tree traversal - The linter traverses the parse tree using rule-specific crawlers (RootOnlyCrawler, SegmentSeekerCrawler); 5) Rule evaluation - Each rule's _eval() method is called with RuleContext containing segment, parent stack, and configuration; 6) Violation detection - Rules return LintResult objects when violations are found, including anchor segments and descriptions; 7) Fix generation - Rules that support fixing generate LintFix objects specifying the type of fix (create, edit, delete); 8) Fix validation - Generated fixes are validated for safety and compatibility with templated code; 9) Fix application - Valid fixes are applied to the parse tree to generate corrected SQL; 10) Result aggregation - All violations and fixes are collected and returned as part of the LintedFile object.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Implements the base rule class.\n\nRules crawl through the trees returned by the parser and evaluate particular\nrules.\n\nThe intent is that it should be possible for the rules to be expressed\nas simply as possible, with as much of the complexity abstracted away.\n\nThe evaluation function should take enough arguments that it can evaluate\nthe position of the given segment in relation to its neighbors, and that\nthe segment which finally \"triggers\" the error, should be the one that would\nbe corrected OR if the rule relates to something that is missing, then it\nshould flag on the segment FOLLOWING, the place that the desired element is\nmissing.\n\"\"\"\n\nimport bdb\nimport copy\nimport fnmatch\nimport logging\nimport pathlib\nimport re\nfrom collections import defaultdict, namedtuple\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    ClassVar,\n    DefaultDict,\n    Optional,\n    Union,\n)\n\nimport regex\n\nfrom sqlfluff.core.errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a "}, {"start_line": 25000, "end_line": 27000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "                tree = new_tree\n                                previous_versions.add(loop_check_tuple)\n                                changed = True\n                                continue\n                            else:\n                                # Applying these fixes took us back to a state\n                                # which we've seen before. We're in a loop, so\n                                # we want to stop.\n                                cls._warn_unfixable(crawler.code)\n\n                    # Record rule timing\n                    rule_timings.append(\n                        (crawler.code, crawler.name, time.monotonic() - t0)\n                    )\n\n                if fix and not changed:\n                    # We did not change the file. Either the file is clean (no\n                    # fixes), or any fixes which are present will take us back\n                    # to a previous state.\n                    linter_logger.info(\n                        f\"Fix loop complete for {phase} phase. Stability \"\n                        f\"achieved after {loop}/{loop_limit} loops.\"\n                    )\n                    break\n            else:\n                if fix:\n                    # The linter loop hit the limit before reaching a stable point\n                    # (i.e. free of lint errors). If this happens, it's usually\n                    # because one or more rules produced fixes which did not address\n                    # the original issue **or** created new issues.\n                    linter_logger.warning(\n                        f\"Loop limit on fixes reached [{loop_limit}].\"\n                    )\n\n                    # Discard any fixes for the linting errors, since they caused a\n                    # loop. IMPORTANT: By doing this, we are telling SQLFluff that\n                    # these linting errors are \"unfixable\". This is important,\n                    # because when \"sqlfluff fix\" encounters unfixable lint errors,\n                   "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fix.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the LintFix class, returned by rules when recommending a fix.\"\"\"\n\nimport logging\nfrom collections.abc import Iterable, Sized\nfrom itertools import chain\nfrom typing import Any, Optional, cast\n\nfrom sqlfluff.core.parser import BaseSegment, PositionMarker, RawSegment, SourceFix\nfrom sqlfluff.core.templaters import RawFileSlice, TemplatedFile\n\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\n\nclass LintFix:\n    \"\"\"A class to hold a potential fix to a linting violation.\n\n    Args:\n        edit_type (:obj:`str`): One of `create_before`, `create_after`,\n            `replace`, `delete` to indicate the kind of fix this represents.\n        anchor (:obj:`BaseSegment`): A segment which represents\n            the *position* that this fix should be applied at. For deletions\n            it represents the segment to delete, for creations it implies the\n            position to create at (with the existing element at this position\n            to be moved *after* the edit), for a `replace` it implies the\n            segment to be replaced.\n        edit (iterable of :obj:`BaseSegment`, optional): For `replace` and\n            `create` fixes, this holds the iterable of segments to create\n            or replace at the given `anchor` point.\n        source (iterable of :obj:`BaseSegment`, optional): For `replace` and\n            `create` fixes, this holds iterable of segments that provided\n            code. IMPORTANT: The linter uses this to prevent copying material\n            from templated areas.\n    \"\"\"\n\n    def __init__(\n        self,\n        edit_type: str,\n        anchor: BaseSegment,\n        edit: Optional[Iterable[BaseSegment]] = None,\n        source: Optional[Iterable[BaseSegment]] = None,\n    ) -> None:\n        if edit_type not in (\n            \"create_before\",\n            \"create_after\",\n            \"replace\",\n            \"delete\",\n        ):  # pragma: no cover\n            raise ValueError(f\"Unexpected edit_type: {edit_type}\")\n        self.edit_type = edit_type\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "patch.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Helpers for generating patches to fix files.\"\"\"\n\nimport logging\nfrom collections.abc import Iterator\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nfrom sqlfluff.core.parser import BaseSegment\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.templaters import TemplatedFile\n\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\n\n@dataclass\nclass FixPatch:\n    \"\"\"An edit patch for a source file.\"\"\"\n\n    templated_slice: slice\n    fixed_raw: str\n    # The patch category, functions mostly for debugging and explanation\n    # than for function. It allows traceability of *why* this patch was\n    # generated. It has no significance for processing.\n    patch_category: str\n    source_slice: slice\n    templated_str: str\n    source_str: str\n\n    def dedupe_tuple(self) -> tuple[slice, str]:\n        \"\"\"Generate a tuple of this fix for deduping.\"\"\"\n        return (self.source_slice, self.fixed_raw)\n\n\ndef _iter_source_fix_patches(\n    segment: BaseSegment, templated_file: TemplatedFile\n) -> Iterator[FixPatch]:\n    \"\"\"Yield any source patches as fixes now.\n\n    NOTE: This yields source fixes for the segment and any of its\n    children, so it's important to call it at the right point in\n    the recursion to avoid yielding duplicates.\n    \"\"\"\n    for source_fix in segment.source_fixes:\n        yield FixPatch(\n            source_fix.templated_slice,\n            source_fix.edit,\n            patch_category=\"source\",\n            source_slice=source_fix.source_slice,\n            templated_str=templated_file.templated_str[source_fix.templated_slice],\n            source_str=templated_file.source_str[source_fix.source_slice],\n        )\n\n\ndef _iter_templated_patches(\n    segment: BaseSegment, templated_file: TemplatedFile\n) -> Iterator[FixPatch]:\n    \"\"\"Iterate through the segments generating fix patches.\n\n    The patches are generated in TEMPLATED space. This is important\n    so that we defer dealing with any loops until later. At this stage\n    ev"}, {"start_line": 19000, "end_line": 21000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "'t\n                    # do fixes. Any results returned won't be seen by the user\n                    # anyway (linting errors ADDED by rules changing SQL, are\n                    # not reported back to the user - only initial linting errors),\n                    # so there's absolutely no reason to run them.\n                    if (\n                        fix\n                        and not is_first_linter_pass()\n                        and not crawler.is_fix_compatible\n                    ):\n                        continue\n\n                    progress_bar_crawler.set_description(f\"rule {crawler.code}\")\n                    t0 = time.monotonic()\n\n                    # fixes should be a dict {} with keys edit, delete, create\n                    # delete is just a list of segments to delete\n                    # edit and create are list of tuples. The first element is\n                    # the \"anchor\", the segment to look for either to edit or to\n                    # insert BEFORE. The second is the element to insert or create.\n                    linting_errors, _, fixes, _ = crawler.crawl(\n                        tree,\n                        dialect=config.get(\"dialect_obj\"),\n                        fix=fix,\n                        templated_file=templated_file,\n                        ignore_mask=ignore_mask,\n                        fname=fname,\n                        config=config,\n                    )\n                    if is_first_linter_pass():\n                        initial_linting_errors += linting_errors\n\n                    if fix and fixes:\n                        linter_logger.info(f\"Applying Fixes [{crawler.code}]: {fixes}\")\n                        # Do some sanity checks on the fixes before applying.\n                        anchor_info = compute_anchor_edit_info(fixes)\n                        if any(\n                            not info.is_valid for info in anchor_info.values()\n                        ):  # pragma: no cover\n                   "}, {"start_line": 19000, "end_line": 21000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "m this evaluation.\n\n        Note that an evaluate function should always accept `**kwargs`, but\n        if it relies on any available kwargs, it should explicitly call\n        them out at definition.\n\n        Returns:\n            :obj:`LintResult`, list of :obj:`LintResult` or :obj:`None`.\n\n        The reason that this method is called :meth:`_eval` and not `eval` is\n        a bit of a hack with sphinx autodoc, to make it so that the rule\n        documentation auto-generates nicely.\n\n        \"\"\"\n        raise NotImplementedError(\n            (\n                \"{} has not had its `eval` function defined. This is a problem \"\n                \"with the rule setup.\"\n            ).format(self.__class__.__name__)\n        )  # pragma: no cover\n\n    def crawl(\n        self,\n        tree: BaseSegment,\n        dialect: \"Dialect\",\n        fix: bool,\n        templated_file: Optional[\"TemplatedFile\"],\n        ignore_mask: Optional[\"IgnoreMask\"],\n        fname: Optional[str],\n        config: \"FluffConfig\",\n    ) -> tuple[\n        list[SQLLintError],\n        tuple[RawSegment, ...],\n        list[LintFix],\n        Optional[dict[str, Any]],\n    ]:\n        \"\"\"Run the rule on a given tree.\n\n        Returns:\n            A tuple of (vs, raw_stack, fixes, memory)\n\n        \"\"\"\n        root_context = RuleContext(\n            dialect=dialect,\n            fix=fix,\n            templated_file=templated_file,\n            path=pathlib.Path(fname) if fname else None,\n            segment=tree,\n            config=config,\n        )\n        vs: list[SQLLintError] = []\n        fixes: list[LintFix] = []\n\n        # Propagates memory from one rule _eval() to the next.\n        memory = root_context.memory\n        context = root_context\n        for context in self.crawl_behaviour.crawl(root_context):\n            try:\n                context.memory = memory\n                res = self._eval(context=context)\n            except (bdb.BdbQuit, KeyboardInterrupt):  # pragma: no cover\n                raise\n         "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`LintFix`, optional): An array of any\n            fixes which would correct this issue. If not present then it's\n            assumed that this issue will have to manually fixed.\n        memory (:obj:`dict`, optional): An object which stores any working\n            memory for the rule. The `memory` returned in any `LintResult`\n            will be passed as an input to the next segment to be crawled.\n        description (:obj:`str`, optional): A description of the problem\n            identified as part of this result. This will override the\n            description of the rule as what gets reported to the user\n            with the problem if provided.\n        source (:obj:`str`, optional): A string identifier for what\n            generated the result. Within larger libraries like reflow this\n            can be useful for tracking where a result came from.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        anchor: Optional[BaseSegment] = None,\n        fixes: Optional[list[\"LintFix\"]] = None,\n  "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`Lin"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the linter class.\"\"\"\n\nimport fnmatch\nimport logging\nimport os\nimport time\nfrom collections.abc import Iterator, Sequence\nfrom typing import TYPE_CHECKING, Optional, cast\n\nimport regex\nfrom tqdm import tqdm\n\nfrom sqlfluff.core.config import FluffConfig, progress_bar_configuration\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLFluffSkipFile,\n    SQLLexError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.helpers.file import get_encoding\nfrom sqlfluff.core.linter.common import (\n    ParsedString,\n    ParsedVariant,\n    RenderedFile,\n    RuleTuple,\n)\nfrom sqlfluff.core.linter.discovery import paths_from_path\nfrom sqlfluff.core.linter.fix import apply_fixes, compute_anchor_edit_info\nfrom sqlfluff.core.linter.linted_dir import LintedDir\nfrom sqlfluff.core.linter.linted_file import (\n    TMP_PRS_ERROR_TYPES,\n    FileTimings,\n    LintedFile,\n)\nfrom sqlfluff.core.linter.linting_result import LintingResult\nfrom sqlfluff.core.parser import Lexer, Parser\nfrom sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\nfrom sqlfluff.core.rules import BaseRule, RulePack, get_ruleset\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.rules.noqa import IgnoreMask\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.meta import MetaSegment\n    from sqlfluff.core.templaters import RawTemplater, TemplatedFile\n\n\nRuleTimingsType = list[tuple[str, str, float]]\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass Linter:\n    \"\"\"The interface class to interact with the linter.\"\"\"\n\n    # Default to allowing process parallelism\n    allow_process_parallelism = True\n\n    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n        dialect: Optional[str] = None,\n  "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fix_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test routines for fixing errors.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core.linter.fix import compute_anchor_edit_info\nfrom sqlfluff.core.linter.patch import FixPatch, generate_source_patches\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments import (\n    BaseSegment,\n    RawSegment,\n    TemplateSegment,\n)\nfrom sqlfluff.core.parser.segments.raw import SourceFix\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters import RawFileSlice, TemplatedFile\nfrom sqlfluff.core.templaters.base import TemplatedFileSlice\n\n\n@pytest.fixture(scope=\"module\")\ndef raw_segments(generate_test_segments):\n    \"\"\"Construct a list of raw segments as a fixture.\"\"\"\n    return generate_test_segments([\"foobar\", \".barfoo\"])\n\n\ndef test__rules_base_segments_compute_anchor_edit_info(raw_segments):\n    \"\"\"Test BaseSegment.compute_anchor_edit_info().\"\"\"\n    # Construct a fix buffer, intentionally with:\n    # - one duplicate.\n    # - two different incompatible fixes on the same segment.\n    fixes = [\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"a\")]),\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"a\")]),\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"b\")]),\n    ]\n    anchor_info_dict = compute_anchor_edit_info(fixes)\n    # Check the target segment is the only key we have.\n    assert list(anchor_info_dict.keys()) == [raw_segments[0].uuid]\n    anchor_info = anchor_info_dict[raw_segments[0].uuid]\n    # Check that the duplicate as been deduplicated.\n    # i.e. this isn't 3.\n    assert anchor_info.replace == 2\n    # Check the fixes themselves.\n    # NOTE: There's no duplicated first fix.\n    assert anchor_info.fixes == [\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"a\")]),\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"b\")]),\n    ]\n    # Check the first replace\n    assert anchor_info._first_replace == LintFix.replace(\n        r"}], "retrieved_count": 10, "cost_time": 1.2227087020874023}
{"question": "What is SQLFluff's rule categorization system?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's rule categorization system organizes rules into groups and categories for easier management and selection. The system includes: 1) Core rules - a special 'core' group containing rules that are stable, apply to most dialects, can detect syntax issues, and aren't too opinionated toward one style. Core rules make it easier to roll out SQLFluff to teams by providing a 'common sense' subset initially; 2) Rule groups - rules can belong to multiple groups like 'all', 'layout', 'capitalisation', 'aliasing', 'references', 'ambiguous', 'structure', 'convention', etc. Each rule must belong to the 'all' group; 3) Rule references - rules can be selected by code (e.g., 'LT01'), name (e.g., 'layout.spacing'), alias (often deprecated codes like 'L003'), or group (e.g., 'layout' or 'capitalisation'); 4) Rule metadata - each rule has a code, name, description, groups tuple, and aliases tuple; 5) RuleSet class - manages rule registration, validation, and filtering with methods like register() and get_rulelist(); 6) Configuration integration - rules can be enabled/disabled via config files using rules and exclude_rules parameters, and can be downgraded to warnings using the warnings parameter.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "rules_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the standard set of rules.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import Linter\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.linter import RuleTuple\nfrom sqlfluff.core.parser import WhitespaceSegment\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.rules import BaseRule, LintFix, LintResult, get_ruleset\nfrom sqlfluff.core.rules.crawlers import RootOnlyCrawler, SegmentSeekerCrawler\nfrom sqlfluff.core.rules.doc_decorators import (\n    document_configuration,\n    document_fix_compatible,\n    document_groups,\n)\nfrom sqlfluff.core.rules.loader import get_rules_from_path\nfrom sqlfluff.core.templaters.base import TemplatedFile\nfrom sqlfluff.utils.testing.logging import fluff_log_catcher\nfrom sqlfluff.utils.testing.rules import get_rule_from_set\nfrom test.fixtures.rules.custom.L000 import Rule_L000\nfrom test.fixtures.rules.custom.S000 import Rule_S000\n\n\nclass Rule_T042(BaseRule):\n    \"\"\"A dummy rule.\"\"\"\n\n    groups = (\"all\",)\n\n    def _eval(self, context):\n        pass\n\n\nclass Rule_T001(BaseRule):\n    \"\"\"A deliberately malicious rule.\n\n    **Anti-pattern**\n\n    Blah blah\n    \"\"\"\n\n    groups = (\"all\",)\n    crawl_behaviour = SegmentSeekerCrawler({\"whitespace\"})\n    is_fix_compatible = True\n\n    def _eval(self, context):\n        \"\"\"Stars make newlines.\"\"\"\n        if context.segment.is_type(\"whitespace\"):\n            return LintResult(\n                anchor=context.segment,\n                fixes=[\n                    LintFix.replace(\n                        context.segment, [WhitespaceSegment(context.segment.raw + \" \")]\n                    )\n                ],\n            )\n\n\nclass Rule_T002(BaseRule):\n    \"\"\"A rule which says all raw code segments are bad.\n\n    This is used for testing unparsable code.\n    \"\"\"\n\n    groups = (\"all\",)\n    # Root only crawler so that the in-rule filters don't kick in.\n    crawl_behaviour = RootOnlyCrawler()\n\n    def _eval(s"}, {"start_line": 0, "end_line": 45, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Standard Rules packaged with sqlfluff.\"\"\"\n"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "layed in the sqlfluff\n    docs.\n    \"\"\"\n\n    # Precompile the regular expressions\n    _doc_search_regex: ClassVar = re.compile(\n        \"(\\\\s{4}\\\\*\\\\*Anti-pattern\\\\*\\\\*|\\\\s{4}\\\\.\\\\. note::|\"\n        \"\\\\s\\\\s{4}\\\\*\\\\*Configuration\\\\*\\\\*)\",\n        flags=re.MULTILINE,\n    )\n    _valid_classname_regex: ClassVar = regex.compile(\n        r\"Rule_?([A-Z]{1}[a-zA-Z]+)?_([A-Z0-9]{4})\"\n    )\n    _valid_rule_name_regex: ClassVar = regex.compile(r\"[a-z][a-z\\.\\_]+\")\n\n    @staticmethod\n    def _populate_code_and_description(\n        name: str, class_dict: dict[str, Any]\n    ) -> dict[str, Any]:\n        \"\"\"Extract and validate the rule code & description.\n\n        We expect that rules are defined as classes with the name `Rule_XXXX`\n        where `XXXX` is of the form `LLNN`, where L is a letter and N is a\n        two digit number. For backward compatibility we also still support\n        the legacy format of LNNN i.e. a single letter and three digit number.\n\n        The two letters should be indicative of the grouping and focus of\n        the rule. e.g. capitalisation rules have the code CP for CaPitalisation.\n\n        If this receives classes by any other name, then it will raise a\n        :exc:`ValueError`.\n        \"\"\"\n        rule_name_match = RuleMetaclass._valid_classname_regex.match(name)\n        # Validate the name\n        if not rule_name_match:  # pragma: no cover\n            raise SQLFluffUserError(\n                f\"Tried to define rule class with \"\n                f\"unexpected format: {name}. Format should be: \"\n                \"'Rule_PluginName_LL23' (for plugins) or \"\n                \"`Rule_LL23` (for core rules).\"\n            )\n\n        plugin_name, code = rule_name_match.groups()\n        # If the docstring is multiline, then we extract just summary.\n        description = class_dict[\"__doc__\"].replace(\"``\", \"'\").split(\"\\n\")[0]\n        if plugin_name:\n            code = f\"{plugin_name}_{code}\"\n\n        class_dict[\"code\"] = code\n        class_dict[\"description\"] = desc"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Implements the base rule class.\n\nRules crawl through the trees returned by the parser and evaluate particular\nrules.\n\nThe intent is that it should be possible for the rules to be expressed\nas simply as possible, with as much of the complexity abstracted away.\n\nThe evaluation function should take enough arguments that it can evaluate\nthe position of the given segment in relation to its neighbors, and that\nthe segment which finally \"triggers\" the error, should be the one that would\nbe corrected OR if the rule relates to something that is missing, then it\nshould flag on the segment FOLLOWING, the place that the desired element is\nmissing.\n\"\"\"\n\nimport bdb\nimport copy\nimport fnmatch\nimport logging\nimport pathlib\nimport re\nfrom collections import defaultdict, namedtuple\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    ClassVar,\n    DefaultDict,\n    Optional,\n    Union,\n)\n\nimport regex\n\nfrom sqlfluff.core.errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a "}, {"start_line": 0, "end_line": 1349, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Configuration and examples for individual rules.\"\"\"\n\nfrom sqlfluff.core.plugin.host import get_plugin_manager\nfrom sqlfluff.core.rules.base import (\n    BaseRule,\n    EvalResultType,\n    LintResult,\n    RuleGhost,\n    RulePack,\n    RuleSet,\n)\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.fix import LintFix\n\n\ndef _load_standard_rules() -> RuleSet:\n    \"\"\"Initialise the standard ruleset.\n\n    We do this on each call so that dynamic rules changes\n    are possible.\n    \"\"\"\n    std_rule_set = RuleSet(name=\"standard\", config_info=get_config_info())\n\n    # Iterate through the rules list and register each rule with the standard set.\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            std_rule_set.register(rule)\n\n    return std_rule_set\n\n\ndef get_ruleset(name: str = \"standard\") -> RuleSet:\n    \"\"\"Get a ruleset by name.\"\"\"\n    std_rules = _load_standard_rules()\n    lookup = {std_rules.name: std_rules}\n    # Return a copy in case someone modifies the register.\n    return lookup[name].copy()\n\n\n__all__ = (\n    \"get_ruleset\",\n    \"RuleSet\",\n    \"RulePack\",\n    \"BaseRule\",\n    \"LintResult\",\n    \"LintFix\",\n    \"RuleContext\",\n    \"RuleGhost\",\n    \"EvalResultType\",\n    \"ConfigInfo\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "sqlfluff_domain.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/docs/source/_ext", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The sqlfluff domain for documenting rules.\"\"\"\n\nfrom sphinx import addnodes\nfrom sphinx.directives import ObjectDescription\nfrom sphinx.domains import Domain, ObjType\nfrom sphinx.roles import XRefRole\nfrom sphinx.util.nodes import make_refnode\n\n\nclass SQLFluffRule(ObjectDescription):\n    \"\"\"SQLFluff rule directive for sphinx.\n\n    Rule directives can be used as shown below.\n\n    .. code-block:: rst\n\n        .. sqlfluff:rule:: AM01\n                           ambiguous.distinct\n\n            Write the documentation for the rule here.\n\n    To cross reference (i.e. refer to) objects defined like this\n    both the code and name reference is available:\n\n    .. code-block:: rst\n\n        :sqlfluff:ref:`CP02`\n        :sqlfluff:ref:`capitalisation.identifiers`\n\n    \"\"\"\n\n    def handle_signature(self, sig, signode):\n        \"\"\"Handle the initial signature of the node.\n\n        This formats the header of the section.\n        \"\"\"\n        raw_obj_type = \"code\" if len(sig) == 4 else \"rule\"\n        obj_type = raw_obj_type.capitalize() + \" \"\n        signode += addnodes.desc_type(obj_type, obj_type)\n        signode += addnodes.desc_name(sig, sig)\n\n        fullname = obj_type + sig\n        signode[\"type\"] = raw_obj_type\n        signode[\"sig\"] = sig\n        signode[\"fullname\"] = fullname\n        return (fullname, raw_obj_type, sig)\n\n    def add_target_and_index(self, name_cls, sig, signode):\n        \"\"\"Hook to add the permalink and index entries.\"\"\"\n        # Add an ID for permalinks\n        node_id = \"rule\" + \"-\" + sig\n        signode[\"ids\"].append(node_id)\n        if len(sig) == 4:\n            # If it's a code, add support for legacy links too.\n            # Both of these formats have been used in the past.\n            signode[\"ids\"].append(f\"sqlfluff.rules.Rule_{sig}\")\n            signode[\"ids\"].append(f\"sqlfluff.rules.sphinx.Rule_{sig}\")\n        # Add to domain for xref resolution\n        fluff = self.env.get_domain(\"sqlfluff\")\n        fluff.add_rule(sig)\n        # Add to index\n  "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`Lin"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "std_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the standard set of rules.\"\"\"\n\nimport pytest\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.rules import get_ruleset\nfrom sqlfluff.utils.testing.rules import assert_rule_raises_violations_in_file\n\n\n@pytest.mark.parametrize(\n    \"rule,path,violations\",\n    [\n        (\"LT01\", \"indentation_errors.sql\", [(4, 24)]),\n        (\n            \"LT02\",\n            \"indentation_errors.sql\",\n            [(2, 1), (3, 1), (4, 1), (5, 1)],\n        ),\n        # Check we get comma whitespace errors\n        (\"LT01\", \"whitespace_errors.sql\", [(2, 9), (3, 12)]),\n        # Check we get operator whitespace errors and it works with brackets\n        (\n            \"LT01\",\n            \"operator_errors.sql\",\n            [(3, 8), (4, 10), (7, 6), (7, 7), (7, 9), (7, 10), (7, 12), (7, 13)],\n        ),\n        (\"LT03\", \"operator_errors.sql\", [(5, 9)]),\n        (\n            \"LT01\",\n            \"operator_errors_negative.sql\",\n            [(2, 6), (2, 9), (5, 6), (5, 7)],\n        ),\n        # Hard indentation errors\n        (\n            \"LT02\",\n            \"indentation_error_hard.sql\",\n            [\n                (2, 1),\n                (6, 1),\n                (9, 1),\n                (11, 15),\n                (12, 1),\n                (12, 33),\n                (13, 15),\n                (14, 1),\n                (14, 36),\n                (18, 1),\n                (19, 1),\n                (20, 1),\n            ],\n        ),\n        # Check bracket handling with closing brackets and contained indents works.\n        (\"LT02\", \"indentation_error_contained.sql\", []),\n        # Check we handle block comments as expect. Github #236\n        (\n            \"LT05\",\n            \"block_comment_errors.sql\",\n            # Errors should flag on the first element of the line.\n            [(1, 1), (2, 5), (4, 5)],\n        ),\n        (\"LT05\", \"block_comment_errors_2.sql\", [(1, 1), (2, 1)]),\n        # Column references\n        (\"RF02\", \"column_references.sql\", [(1, 8)]),\n        (\"RF02\", \"co"}, {"start_line": 1000, "end_line": 2820, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e that currently, the gap _before_ and _after_ the semicolon \"\n                \"is considered 'between' statements.\"\n            ),\n        },\n        \"maximum_empty_lines_inside_statements\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The maximum number of empty lines allowed inside statements.\"\n            ),\n        },\n        \"wildcard_policy\": {\n            \"validation\": [\"single\", \"multiple\"],\n            \"definition\": \"Treatment of wildcards. Defaults to ``single``.\",\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.layout.LT01 import Rule_LT01\n    from sqlfluff.rules.layout.LT02 import Rule_LT02\n    from sqlfluff.rules.layout.LT03 import Rule_LT03\n    from sqlfluff.rules.layout.LT04 import Rule_LT04\n    from sqlfluff.rules.layout.LT05 import Rule_LT05\n    from sqlfluff.rules.layout.LT06 import Rule_LT06\n    from sqlfluff.rules.layout.LT07 import Rule_LT07\n    from sqlfluff.rules.layout.LT08 import Rule_LT08\n    from sqlfluff.rules.layout.LT09 import Rule_LT09\n    from sqlfluff.rules.layout.LT10 import Rule_LT10\n    from sqlfluff.rules.layout.LT11 import Rule_LT11\n    from sqlfluff.rules.layout.LT12 import Rule_LT12\n    from sqlfluff.rules.layout.LT13 import Rule_LT13\n    from sqlfluff.rules.layout.LT14 import Rule_LT14\n    from sqlfluff.rules.layout.LT15 import Rule_LT15\n\n    return [\n        Rule_LT01,\n        Rule_LT02,\n        Rule_LT03,\n        Rule_LT04,\n        Rule_LT05,\n        Rule_LT06,\n        Rule_LT07,\n        Rule_LT08,\n        Rule_LT09,\n        Rule_LT10,\n        Rule_LT11,\n        Rule_LT12,\n        Rule_LT13,\n        Rule_LT14,\n        Rule_LT15,\n    ]\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "common.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines small container classes to hold intermediate results during linting.\"\"\"\n\nfrom typing import Any, NamedTuple, Optional, Union\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLLexError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.parser.segments.base import BaseSegment\nfrom sqlfluff.core.templaters import TemplatedFile\n\n\nclass RuleTuple(NamedTuple):\n    \"\"\"Rule Tuple object for describing rules.\"\"\"\n\n    code: str\n    name: str\n    description: str\n    groups: tuple[str, ...]\n    aliases: tuple[str, ...]\n\n\nclass RenderedFile(NamedTuple):\n    \"\"\"An object to store the result of a templated file/string.\n\n    This is notable as it's the intermediate state between what happens\n    in the main process and the child processes when running in parallel mode.\n    \"\"\"\n\n    templated_variants: list[TemplatedFile]\n    templater_violations: list[SQLTemplaterError]\n    config: FluffConfig\n    time_dict: dict[str, float]\n    fname: str\n    encoding: str\n    source_str: str\n\n\nclass ParsedVariant(NamedTuple):\n    \"\"\"An object to store the result of parsing a single TemplatedFile.\n\n    Args:\n        templated_file (:obj:`TemplatedFile`): Containing the details\n            of the templated file. If templating fails, this will be `None`.\n        tree (:obj:`BaseSegment`): The segment structure representing the\n            parsed file. If parsing fails due to an unrecoverable\n            violation then we will be None.\n        lexing_violations (:obj:`list` of :obj:`SQLLexError`): Any violations\n            raised during the lexing phase.\n        parsing_violations (:obj:`list` of :obj:`SQLParseError`): Any violations\n            raised during the lexing phase.\n    \"\"\"\n\n    templated_file: TemplatedFile\n    tree: Optional[BaseSegment]\n    lexing_violations: list[SQLLexError]\n    parsing_violations: list[SQLParseError]\n\n    def violations(self) -> list[Union[SQLLexError, SQLParseError]]:\n        \"\"\"Return"}], "retrieved_count": 10, "cost_time": 1.2879292964935303}
{"question": "Where in SQLFluff is the configuration system implemented?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's configuration system is implemented across several key modules in the codebase. The main configuration components are located in: 1) src/sqlfluff/core/config/fluffconfig.py - Contains the main FluffConfig class that manages all configuration; 2) src/sqlfluff/core/default_config.cfg - Contains the built-in default configuration values; 3) src/sqlfluff/core/config/__init__.py - Provides configuration loading and validation utilities; 4) src/sqlfluff/core/config/helpers.py - Contains helper functions for configuration processing; 5) src/sqlfluff/core/config/loader.py - Handles loading configuration from various file formats; 6) src/sqlfluff/core/config/validation.py - Implements configuration validation logic; 7) src/sqlfluff/core/rules/config_info.py - Manages rule-specific configuration information; 8) src/sqlfluff/core/rules/base.py - Contains configuration handling for individual rules; 9) src/sqlfluff/cli/commands.py - Handles command-line configuration overrides; 10) src/sqlfluff/core/config/nested_combine.py - Implements configuration merging and inheritance logic.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Init file for the config module.\n\nThis holds all the methods and classes for configuration.\n\"\"\"\n\nfrom typing import Optional\n\nfrom sqlfluff.core.config.file import (\n    load_config_file_as_dict,\n    load_config_string_as_dict,\n)\nfrom sqlfluff.core.config.fluffconfig import FluffConfig\nfrom sqlfluff.core.config.loader import (\n    ConfigLoader,\n    load_config_at_path,\n    load_config_file,\n    load_config_resource,\n    load_config_string,\n    load_config_up_to_path,\n)\n\n__all__ = (\n    \"FluffConfig\",\n    \"ConfigLoader\",\n    \"load_config_file\",\n    \"load_config_resource\",\n    \"load_config_string\",\n    \"load_config_at_path\",\n    \"load_config_up_to_path\",\n    \"progress_bar_configuration\",\n    \"clear_config_caches\",\n)\n\n\ndef clear_config_caches() -> None:\n    \"\"\"Clear any of the cached config methods.\n\n    This is primarily used during testing where the cache may be be rendered unreliable\n    by using moving around files while setting up tests. Some of the cached methods\n    rely on *filename* caching, and so we may break one of the assumptions of the\n    caching routines (that files aren't modified while SQLFluff is running) during\n    the test suite. That means we need to clear the cache during those times to\n    get reliable results.\n\n    NOTE: You may not notice those results when running tests individually locally\n    as they may only be visible when running the whole test suite.\n    \"\"\"\n    load_config_file_as_dict.cache_clear()\n    load_config_at_path.cache_clear()\n    load_config_string_as_dict.cache_clear()\n    pass\n\n\nclass ProgressBarConfiguration:\n    \"\"\"Singleton-esque progress bar configuration.\n\n    It's expected to be set during starting with parameters coming from commands\n    parameters, then to be just utilized as just\n    ```\n    from sqlfluff.core.config import progress_bar_configuration\n    is_progressbar_disabled = progress_bar_configuration.disable_progress_bar\n    ```\n    \"\"\"\n\n    _disable_progress_bar: Optional[bool] = True\n\n    @property\n    "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fluffconfig.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Module for loading config.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom collections.abc import Iterable\nfrom copy import copy, deepcopy\nfrom itertools import chain\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nimport pluggy\n\nfrom sqlfluff.core.config.ini import coerce_value\nfrom sqlfluff.core.config.loader import load_config_string, load_config_up_to_path\nfrom sqlfluff.core.config.validate import validate_config_dict\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.helpers.dict import (\n    dict_diff,\n    iter_records_from_nested_dict,\n    nested_combine,\n    records_to_nested_dict,\n)\nfrom sqlfluff.core.helpers.string import (\n    split_colon_separated_string,\n    split_comma_separated_string,\n)\nfrom sqlfluff.core.plugin.host import get_plugin_manager\nfrom sqlfluff.core.types import ConfigMappingType, ConfigValueOrListType\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.templaters.base import RawTemplater\n\n# Instantiate the config logger\nconfig_logger = logging.getLogger(\"sqlfluff.config\")\n\n\nclass FluffConfig:\n    \"\"\"The persistent object for internal methods to access configuration.\n\n    This class is designed to be instantiated once for each file and then be\n    reused by each part of the process. For multiple files in the same path, a\n    parent object will be created for the each path and then variants of it\n    are created *for each file*. The object itself contains the references\n    to any long lived objects which might be used by multiple parts of the\n    codebase such as the dialect and the templater (both of which can be\n    resource intensive to load & instantiate), which allows (for example),\n    multiple files to reuse the same instance of the relevant dialect.\n\n    It is also designed to pickle well for use in parallel operations.\n\n    Args:\n        configs (ConfigMappingType, optional): A nested dict of config\n            values from which to construct the config.\n        extra_config_path"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fluffconfig_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the configuration routines.\"\"\"\n\nimport logging\nimport os\n\nimport pytest\n\nimport sqlfluff\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.templaters import (\n    JinjaTemplater,\n    PlaceholderTemplater,\n    PythonTemplater,\n    RawTemplater,\n)\nfrom sqlfluff.utils.testing.logging import fluff_log_catcher\n\nconfig_b = {\n    \"core\": {\"rules\": \"LT03\", \"dialect\": \"ansi\"},\n    \"layout\": {\n        \"type\": {\"comma\": {\"line_position\": \"trailing\", \"spacing_before\": \"touch\"}}\n    },\n}\n\nconfig_c = {\n    \"core\": {\"rules\": \"LT03\", \"dialect\": \"ansi\"},\n    # NOTE:\n    # - NOT_A_RULE doesn't match anything.\n    # - L001 is an alias, but no longer a rule.\n    # - layout is a group and but doesn't match any individual rule.\n    \"rules\": {\n        \"NOT_A_RULE\": {\"foo\": \"bar\"},\n        \"L001\": {\"foo\": \"bar\"},\n        \"layout\": {\"foo\": \"bar\"},\n    },\n}\n\n\ndef test__config__from_strings():\n    \"\"\"Test loading config from multiple strings.\"\"\"\n    strings = [\n        \"[sqlfluff]\\ndialect=mysql\\ntesting_val=foobar\",\n        \"[sqlfluff]\\ndialect=postgres\\ntesting_val2=bar\",\n        \"[sqlfluff]\\ndialect=mysql\\ntesting_val=foo\",\n    ]\n    cfg = FluffConfig.from_strings(*strings)\n    assert cfg.get(\"dialect\") == \"mysql\"\n    assert cfg.get(\"testing_val2\") == \"bar\"\n    assert cfg.get(\"testing_val\") == \"foo\"\n\n\ndef test__config__nested_config_tests():\n    \"\"\"Test linting with overridden config in nested paths.\n\n    This looks like a linter test but it's actually a config\n    test.\n    \"\"\"\n    lntr = Linter(\n        # Exclude CP02 in overrides (similar to cli --exclude-rules)\n        config=FluffConfig(overrides=dict(exclude_rules=\"CP02\", dialect=\"ansi\"))\n    )\n    lnt = lntr.lint_path(\"test/fixtures/config/inheritance_b\")\n    violations = lnt.check_tuples_by_path()\n    for k in violations:\n        if k.endswith(\"nested\\\\example.sql\"):\n            # CP01 is enabled in the .sqlfluff file and not excluded.\n            assert"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "fluffconfig.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " config logger\nconfig_logger = logging.getLogger(\"sqlfluff.config\")\n\n\nclass FluffConfig:\n    \"\"\"The persistent object for internal methods to access configuration.\n\n    This class is designed to be instantiated once for each file and then be\n    reused by each part of the process. For multiple files in the same path, a\n    parent object will be created for the each path and then variants of it\n    are created *for each file*. The object itself contains the references\n    to any long lived objects which might be used by multiple parts of the\n    codebase such as the dialect and the templater (both of which can be\n    resource intensive to load & instantiate), which allows (for example),\n    multiple files to reuse the same instance of the relevant dialect.\n\n    It is also designed to pickle well for use in parallel operations.\n\n    Args:\n        configs (ConfigMappingType, optional): A nested dict of config\n            values from which to construct the config.\n        extra_config_path (str, optional): An optional additional path\n            to load config files from. These are loaded last if found\n            and take precedence over any pre-existing config values.\n            Note that when provided directly to the class, this path\n            is not loaded for the class in question (it's assumed that\n            has already been done, and the results are incorporated in\n            the `configs` argument), but it *is* passed onward to child\n            config instances, which will use it.\n        ignore_local_config (bool, optional, defaults to False): If set to\n            True, this skips loading configuration from the user home\n            directory (``~``) or ``appdir`` path.\n        overrides (ConfigMappingType, optional): A additional set of\n            configs to merge into the ``core`` section of the config\n            object at the end. These values take precedence over all\n            other provided values and are inherited by child configs.\n           "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "loader.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Config loading methods and helpers.\n\nThis is designed to house the main functions which are exposed by the\noverall config module. There is some caching in this module, which\nis designed around caching the configuration loaded at *specific paths*\nrather than the individual file caching in the `file` module.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport os\nimport os.path\nimport sys\nfrom functools import cache\nfrom importlib.resources import files\nfrom pathlib import Path\nfrom typing import (\n    Optional,\n)\n\nimport platformdirs\nimport platformdirs.macos\nimport platformdirs.unix\n\nfrom sqlfluff.core.config.file import (\n    load_config_file_as_dict,\n    load_config_string_as_dict,\n)\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.helpers.dict import nested_combine\nfrom sqlfluff.core.helpers.file import iter_intermediate_paths\nfrom sqlfluff.core.types import ConfigMappingType\n\n# Instantiate the config logger\nconfig_logger = logging.getLogger(\"sqlfluff.config\")\n\nglobal_loader = None\n\"\"\":obj:`ConfigLoader`: A variable to hold the single module loader when loaded.\n\nWe define a global loader, so that between calls to load config, we\ncan still cache appropriately\n\"\"\"\n\n\ndef _get_user_config_dir_path(sys_platform: str) -> str:\n    \"\"\"Get the user config dir for this system.\n\n    Args:\n        sys_platform (str): The result of ``sys.platform()``. Provided\n            as an argument here for ease of testing. In normal usage\n            it should only be  called with ``sys.platform()``. This\n            argument only applies to switching between linux and macos.\n            Win32 detection still uses the underlying ``sys.platform()``\n            methods.\n    \"\"\"\n    appname = \"sqlfluff\"\n    appauthor = \"sqlfluff\"\n\n    # First try the default SQLFluff specific cross-platform config path.\n    cross_platform_path = os.path.expanduser(\"~/.config/sqlfluff\")\n    if os.path.exists(cross_platform_path):\n        return cross_platform_path\n\n    # Th"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "loader_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the configuration routines.\"\"\"\n\nimport os\nimport sys\nfrom contextlib import contextmanager\nfrom unittest.mock import call, patch\n\nimport pytest\n\nfrom sqlfluff.core import FluffConfig\nfrom sqlfluff.core.config import (\n    load_config_at_path,\n    load_config_file,\n    load_config_string,\n    load_config_up_to_path,\n)\nfrom sqlfluff.core.config.loader import (\n    _get_user_config_dir_path,\n    _load_user_appdir_config,\n)\nfrom sqlfluff.core.errors import SQLFluffUserError\n\nconfig_a = {\n    \"core\": {\"testing_val\": \"foobar\", \"testing_int\": 4, \"dialect\": \"mysql\"},\n    \"bar\": {\"foo\": \"barbar\"},\n}\n\n\n@pytest.fixture\ndef mock_xdg_home(monkeypatch):\n    \"\"\"Sets the XDG_CONFIG_HOME variable.\"\"\"\n    monkeypatch.setenv(\"XDG_CONFIG_HOME\", \"~/.config/my/special/path\")\n\n\ndef test__config__load_file_dir():\n    \"\"\"Test loading config from a directory path.\"\"\"\n    cfg = load_config_at_path(\n        os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\")\n    )\n    assert cfg == config_a\n\n\ndef test__config__load_from_string():\n    \"\"\"Test loading config from a string.\"\"\"\n    # Load a string\n    with open(\n        os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\", \".sqlfluff\")\n    ) as f:\n        config_string = f.read()\n    cfg = load_config_string(config_string)\n    assert cfg == config_a\n\n\ndef test__config__load_file_f():\n    \"\"\"Test loading config from a file path.\"\"\"\n    cfg = load_config_at_path(\n        os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\", \"testing.sql\")\n    )\n    assert cfg == config_a\n\n\ndef test__config__load_file_missing_extra():\n    \"\"\"Test loading config from a file path if extra path is not found.\"\"\"\n    with pytest.raises(SQLFluffUserError):\n        load_config_up_to_path(\n            os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\", \"testing.sql\"),\n            extra_config_path=\"non/existent/path\",\n        )\n\n\ndef test__config__load_nested():\n    \"\"\"Test nested overwrite and order of precedence of config files.\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "file.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Lower level routines for config file loading and caching.\n\nFunctions in this module load config from *individual* files and\nresources. While some are cached, they are cached on the basis of\nnot processing individual files more than once.\n\nFor the cached functions it is VERY recommended to make sure they\nare copied before any edits happen to them, as those edits may\npropagate back up into the cache. Typically the results are passed\nto `nested_combine` either immediately, or eventually after returning\nwhich should negate this effect.\n\"\"\"\n\nimport os.path\nfrom functools import cache\nfrom typing import Optional\n\nfrom sqlfluff.core.config.ini import load_ini_string\nfrom sqlfluff.core.config.toml import load_toml_file_config\nfrom sqlfluff.core.config.validate import validate_config_dict\nfrom sqlfluff.core.helpers.string import (\n    split_comma_separated_string,\n)\nfrom sqlfluff.core.types import ConfigMappingType\n\nCOMMA_SEPARATED_PATH_KEYS = (\n    \"load_macros_from_path\",\n    \"loader_search_path\",\n    \"exclude_macros_from_path\",\n)\nRESOLVE_PATH_SUFFIXES = (\"_path\", \"_dir\")\n\n\ndef _load_raw_file_as_dict(filepath: str) -> ConfigMappingType:\n    \"\"\"Loads the raw dict object from file without interpolation.\"\"\"\n    filename = os.path.basename(filepath)\n    if filename == \"pyproject.toml\":\n        return load_toml_file_config(filepath)\n    # If it's not a pyproject file, assume that it's an ini file.\n    with open(filepath, mode=\"r\") as file:\n        return load_ini_string(file.read())\n\n\ndef _resolve_path(filepath: str, val: str) -> str:\n    \"\"\"Try to resolve a path found in a config value.\"\"\"\n    # Make the referenced path.\n    ref_path = os.path.join(os.path.dirname(filepath), val)\n    # Check if it exists, and if it does, replace the value with the path.\n    return ref_path if os.path.exists(ref_path) else val\n\n\ndef _resolve_paths_in_config(\n    config: ConfigMappingType, filepath: str, logging_reference: Optional[str] = None\n) -> None:\n    \"\"\"Attempt to resolve any paths fo"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "ini.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Methods for loading config files with an ini-style format.\n\nThis includes `.sqlfluff` and `tox.ini` files.\n\"\"\"\n\nimport configparser\n\nfrom sqlfluff.core.helpers.dict import NestedDictRecord, records_to_nested_dict\nfrom sqlfluff.core.types import ConfigMappingType, ConfigValueType\n\n\ndef coerce_value(val: str) -> ConfigValueType:\n    \"\"\"Try to coerce to a more specific type.\"\"\"\n    # Try to coerce it to a more specific type,\n    # otherwise just make it a string.\n    v: ConfigValueType\n    try:\n        v = int(val)\n    except ValueError:\n        try:\n            v = float(val)\n        except ValueError:\n            cleaned_val = val.strip().lower()\n            if cleaned_val == \"true\":\n                v = True\n            elif cleaned_val == \"false\":\n                v = False\n            elif cleaned_val == \"none\":\n                v = None\n            else:\n                v = val\n    return v\n\n\ndef load_ini_string(cfg_content: str) -> ConfigMappingType:\n    \"\"\"Read an ini-style config string.\n\n    This would include loading a `.sqlfluff` file.\n\n    Notes:\n    - We rename the root `sqlfluff` section, to `core` so that it's in\n      line with other config files.\n    - The `configparser` reads everything as strings, but this method will\n      attempt to find better types for values based on their content.\n    - Path resolution isn't done here, that all happens later.\n    - Unlike most cfg file readers, SQLFluff is case-sensitive in how\n      it reads config files. This is to ensure we support the case\n      sensitivity of jinja.\n    \"\"\"\n    # If the string is empty, no need to parse it.\n    if not cfg_content:\n        return {}\n\n    # Disable interpolation so we can load macros\n    config = configparser.ConfigParser(delimiters=\"=\", interpolation=None)\n    # NB: We want to be case sensitive in how we read from files,\n    # because jinja is also case sensitive. To do this we override\n    # the optionxform attribute.\n    config.optionxform = lambda option: option  # type"}, {"start_line": 0, "end_line": 1349, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Configuration and examples for individual rules.\"\"\"\n\nfrom sqlfluff.core.plugin.host import get_plugin_manager\nfrom sqlfluff.core.rules.base import (\n    BaseRule,\n    EvalResultType,\n    LintResult,\n    RuleGhost,\n    RulePack,\n    RuleSet,\n)\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.fix import LintFix\n\n\ndef _load_standard_rules() -> RuleSet:\n    \"\"\"Initialise the standard ruleset.\n\n    We do this on each call so that dynamic rules changes\n    are possible.\n    \"\"\"\n    std_rule_set = RuleSet(name=\"standard\", config_info=get_config_info())\n\n    # Iterate through the rules list and register each rule with the standard set.\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            std_rule_set.register(rule)\n\n    return std_rule_set\n\n\ndef get_ruleset(name: str = \"standard\") -> RuleSet:\n    \"\"\"Get a ruleset by name.\"\"\"\n    std_rules = _load_standard_rules()\n    lookup = {std_rules.name: std_rules}\n    # Return a copy in case someone modifies the register.\n    return lookup[name].copy()\n\n\n__all__ = (\n    \"get_ruleset\",\n    \"RuleSet\",\n    \"RulePack\",\n    \"BaseRule\",\n    \"LintResult\",\n    \"LintFix\",\n    \"RuleContext\",\n    \"RuleGhost\",\n    \"EvalResultType\",\n    \"ConfigInfo\",\n)\n"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "fluffconfig_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "strings.\"\"\"\n    strings = [\n        \"[sqlfluff]\\ndialect=mysql\\ntesting_val=foobar\",\n        \"[sqlfluff]\\ndialect=postgres\\ntesting_val2=bar\",\n        \"[sqlfluff]\\ndialect=mysql\\ntesting_val=foo\",\n    ]\n    cfg = FluffConfig.from_strings(*strings)\n    assert cfg.get(\"dialect\") == \"mysql\"\n    assert cfg.get(\"testing_val2\") == \"bar\"\n    assert cfg.get(\"testing_val\") == \"foo\"\n\n\ndef test__config__nested_config_tests():\n    \"\"\"Test linting with overridden config in nested paths.\n\n    This looks like a linter test but it's actually a config\n    test.\n    \"\"\"\n    lntr = Linter(\n        # Exclude CP02 in overrides (similar to cli --exclude-rules)\n        config=FluffConfig(overrides=dict(exclude_rules=\"CP02\", dialect=\"ansi\"))\n    )\n    lnt = lntr.lint_path(\"test/fixtures/config/inheritance_b\")\n    violations = lnt.check_tuples_by_path()\n    for k in violations:\n        if k.endswith(\"nested\\\\example.sql\"):\n            # CP01 is enabled in the .sqlfluff file and not excluded.\n            assert (\"CP01\", 1, 4) in violations[k]\n            # LT02 is enabled in the .sqlfluff file and not excluded.\n            assert (\"LT02\", 1, 1) in violations[k]\n            # CP02 is enabled in the .sqlfluff file but excluded by the\n            # override above.\n            assert \"CP02\" not in [c[0] for c in violations[k]]\n        elif k.endswith(\"inheritance_b\\\\example.sql\"):\n            # CP01 is enabled because while disabled in the tox.ini file,\n            # the exclude-rules option is overridden by the override above\n            # which effectively sets the exclude to CP02 and in effect\n            # re-enables CP01.\n            # This may seem counter-intuitive but is in line with current\n            # documentation on how to use `rules` and `exclude-rules`.\n            # https://docs.sqlfluff.com/en/latest/perma/rule_disabling.html\n            assert (\"CP01\", 1, 4) in violations[k]\n            # CP02 is disabled because of the override above.\n            assert \"CP02\" not in [c[0] fo"}], "retrieved_count": 10, "cost_time": 1.222407341003418}
{"question": "Why does SQLFluff's incremental parsing improve performance for large SQL files?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's incremental parsing improves performance for large SQL files by avoiding redundant parsing work and optimizing memory usage. Key benefits include: 1) Caching mechanisms - Parsed segments and grammar matches are cached to avoid re-parsing identical structures; 2) Selective re-parsing - Only modified sections of SQL files are re-parsed when changes are detected; 3) Memory efficiency - Incremental parsing reduces memory usage by reusing previously parsed structures; 4) Grammar optimization - Frequently used grammar patterns are optimized and cached for faster matching; 5) Segment reuse - Previously parsed segments can be reused when they haven't changed; 6) Parse tree optimization - The parse tree structure is optimized to minimize traversal overhead; 7) Context preservation - Parsing context is preserved between incremental updates to avoid redundant work; 8) Batch processing - Multiple small changes can be batched together for more efficient processing; 9) Dependency tracking - Only dependent sections are re-parsed when upstream changes occur; 10) Performance scaling - Incremental parsing scales better with file size compared to full re-parsing approaches.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parser.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the Parser class.\"\"\"\n\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import check_still_complete\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.parser.segments import BaseFileSegment, BaseSegment\n\n\nclass Parser:\n    \"\"\"Instantiates parsed queries from a sequence of lexed raw segments.\"\"\"\n\n    def __init__(\n        self, config: Optional[FluffConfig] = None, dialect: Optional[str] = None\n    ):\n        if config and dialect:\n            raise ValueError(  # pragma: no cover\n                \"Parser does not support setting both `config` and `dialect`.\"\n            )\n        # Use the provided config or create one from the dialect.\n        self.config = config or FluffConfig.from_kwargs(dialect=dialect)\n        self.RootSegment: type[BaseFileSegment] = self.config.get(\n            \"dialect_obj\"\n        ).get_root_segment()\n\n    def parse(\n        self,\n        segments: Sequence[\"BaseSegment\"],\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> Optional[\"BaseSegment\"]:\n        \"\"\"Parse a series of lexed tokens using the current dialect.\"\"\"\n        if not segments:  # pragma: no cover\n            # This should normally never happen because there will usually\n            # be an end_of_file segment. It would probably only happen in\n            # api use cases.\n            return None\n\n        # NOTE: This is the only time we use the parse context not in the\n        # context of a context manager. That's because it's the initial\n        # instantiation.\n        ctx = ParseContext.from_config(config=self.config)\n        # Kick off parsing with the root segment. The BaseFileSegment has\n        # a unique entry point to facilitate exactly this. All other segments\n        # will use the standard .match() route.\n        root = self.RootSegment.root_pars"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "file.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Definition of the BaseFileSegment.\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Optional\n\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments.base import BaseSegment, UnparsableSegment\n\n\nclass BaseFileSegment(BaseSegment):\n    \"\"\"A segment representing a whole file or script.\n\n    This is also the default \"root\" segment of the dialect,\n    and so is usually instantiated directly. It therefore\n    has no match_grammar.\n    \"\"\"\n\n    type = \"file\"\n    # The file segment is the only one which can start or end with non-code\n    can_start_end_non_code = True\n    # A file can be empty!\n    allow_empty = True\n\n    def __init__(\n        self,\n        segments: tuple[BaseSegment, ...],\n        pos_marker: Optional[PositionMarker] = None,\n        fname: Optional[str] = None,\n    ):\n        self._file_path = fname\n        super().__init__(segments, pos_marker=pos_marker)\n\n    @property\n    def file_path(self) -> Optional[str]:\n        \"\"\"File path of a parsed SQL file.\"\"\"\n        return self._file_path\n\n    @abstractmethod\n    def get_table_references(self) -> set[str]:\n        \"\"\"Use parsed tree to extract table references.\"\"\"\n\n    @classmethod\n    def root_parse(\n        cls,\n        segments: tuple[BaseSegment, ...],\n        parse_context: ParseContext,\n        fname: Optional[str] = None,\n    ) -> \"BaseFileSegment\":\n        \"\"\"This is the entry method into parsing a file lexed segments.\n\n        For single pass matching, this trims any non code off\n        the start, matches the middle and then trims the end.\n\n        Anything unexpected at the end is regarded as unparsable.\n        \"\"\"\n        # Trim the start\n        _start_idx = 0\n        for _start_idx in range(len(segments)):\n            if segments[_start_idx].is_code:\n                break\n\n        # Trim the end\n        _end_idx = len(segments)\n        for _end_idx in range(len(segments), _start_idx - 1, -1):\n   "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "context.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The parser context.\n\nThis mirrors some of the same design of the flask\ncontext manager. https://flask.palletsprojects.com/en/1.1.x/\n\nThe context acts as a way of keeping track of state, references\nto common configuration and dialects, logging and also the parse\nand match depth of the current operation.\n\"\"\"\n\nimport logging\nimport uuid\nfrom collections import defaultdict\nfrom collections.abc import Iterator, Sequence\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, NoReturn, Optional\n\nfrom tqdm import tqdm\n\nfrom sqlfluff.core.config import progress_bar_configuration\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects.base import Dialect\n    from sqlfluff.core.parser.match_result import MatchResult\n    from sqlfluff.core.parser.matchable import Matchable\n\n# Get the parser logger\nparser_logger = logging.getLogger(\"sqlfluff.parser\")\n\n\nclass ParseContext:\n    \"\"\"Object to handle the context at hand during parsing.\n\n    Holds two tiers of references.\n    1. Persistent config, like references to the dialect or\n       the current verbosity and logger.\n    2. Stack config, like the parse and match depth.\n\n    The manipulation of the stack config is done using a context\n    manager and layered config objects inside the context.\n\n    NOTE: We use context managers here to avoid _copying_\n    the context, just to mutate it safely. This is significantly\n    more performant than the copy operation, but does require some\n    care to use properly.\n\n    When fetching elements from the context, we first look\n    at the top level stack config object and the persistent\n    config values (stored as attributes of the ParseContext\n    itself).\n    \"\"\"\n\n    def __init__(\n        self,\n        dialect: \"Dialect\",\n        indentation_config: Optional[dict[str, Any]] = None,\n    ) -> None:\n        \"\"\"Initialize a new instance of the class.\n\n        Args:\n            dialect (Dialect): The dialect "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the linter class.\"\"\"\n\nimport fnmatch\nimport logging\nimport os\nimport time\nfrom collections.abc import Iterator, Sequence\nfrom typing import TYPE_CHECKING, Optional, cast\n\nimport regex\nfrom tqdm import tqdm\n\nfrom sqlfluff.core.config import FluffConfig, progress_bar_configuration\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLFluffSkipFile,\n    SQLLexError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.helpers.file import get_encoding\nfrom sqlfluff.core.linter.common import (\n    ParsedString,\n    ParsedVariant,\n    RenderedFile,\n    RuleTuple,\n)\nfrom sqlfluff.core.linter.discovery import paths_from_path\nfrom sqlfluff.core.linter.fix import apply_fixes, compute_anchor_edit_info\nfrom sqlfluff.core.linter.linted_dir import LintedDir\nfrom sqlfluff.core.linter.linted_file import (\n    TMP_PRS_ERROR_TYPES,\n    FileTimings,\n    LintedFile,\n)\nfrom sqlfluff.core.linter.linting_result import LintingResult\nfrom sqlfluff.core.parser import Lexer, Parser\nfrom sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\nfrom sqlfluff.core.rules import BaseRule, RulePack, get_ruleset\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.rules.noqa import IgnoreMask\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.meta import MetaSegment\n    from sqlfluff.core.templaters import RawTemplater, TemplatedFile\n\n\nRuleTimingsType = list[tuple[str, str, float]]\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass Linter:\n    \"\"\"The interface class to interact with the linter.\"\"\"\n\n    # Default to allowing process parallelism\n    allow_process_parallelism = True\n\n    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n        dialect: Optional[str] = None,\n  "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "discovery.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Discovery methods for sql files.\n\nThe main public method here is `paths_from_path` which takes\npotentially ambiguous paths and file input and resolves them\ninto specific file references. The method also processes the\n`.sqlfluffignore` functionality in the process.\n\"\"\"\n\nimport logging\nimport os\nfrom collections.abc import Iterable, Iterator, Sequence\nfrom pathlib import Path\nfrom typing import Callable, Optional\n\nimport pathspec\n\nfrom sqlfluff.core.config.file import load_config_file_as_dict\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.helpers.file import iter_intermediate_paths\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\nWalkableType = Iterable[tuple[str, Optional[list[str]], list[str]]]\nIgnoreSpecRecord = tuple[str, str, pathspec.PathSpec]\nIgnoreSpecRecords = list[IgnoreSpecRecord]\n\n\ndef _check_ignore_specs(\n    absolute_filepath: str, ignore_specs: IgnoreSpecRecords\n) -> Optional[str]:\n    \"\"\"Check a filepath against the loaded ignore files.\n\n    Returns:\n        The path of an ignorefile if found, None otherwise.\n    \"\"\"\n    for dirname, filename, spec in ignore_specs:\n        if spec.match_file(os.path.relpath(absolute_filepath, dirname)):\n            return os.path.join(dirname, filename)\n    return None\n\n\ndef _load_specs_from_lines(\n    lines: Iterable[str], logging_reference: str\n) -> pathspec.PathSpec:\n    \"\"\"Load the ignore spec from an iterable of lines.\n\n    Raises SQLFluffUserError if unparsable for any reason.\n    \"\"\"\n    try:\n        return pathspec.PathSpec.from_lines(\"gitwildmatch\", lines)\n    except Exception:\n        _error_msg = f\"Error parsing ignore patterns in {logging_reference}\"\n        # If the iterable is a Sequence type, then include the patterns.\n        if isinstance(lines, Sequence):\n            _error_msg += f\": {lines}\"\n        raise SQLFluffUserError(_error_msg)\n\n\ndef _load_ignorefile(dirpath: str, filename: str) -> IgnoreSpecRecord:\n    \"\"\"Load a "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Base segment definitions.\n\nHere we define:\n- BaseSegment. This is the root class for all segments, and is\n  designed to hold other subsegments.\n- UnparsableSegment. A special wrapper to indicate that the parse\n  function failed on this block of segments and to prevent further\n  analysis.\n\"\"\"\n\n# Import annotations for py 3.7 to allow `weakref.Referencetype[\"BaseSegment\"]`\nfrom __future__ import annotations\n\nimport logging\nimport weakref\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom functools import cached_property\nfrom io import StringIO\nfrom itertools import chain\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    ClassVar,\n    Optional,\n    Union,\n    cast,\n)\nfrom uuid import uuid4\n\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import trim_non_code_segments\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.types import SimpleHintType\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.raw import RawSegment\n\n# Instantiate the linter logger (only for use in methods involved with fixing.)\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\nTupleSerialisedSegment = tuple[str, Union[str, tuple[\"TupleSerialisedSegment\", ...]]]\nRecordSerialisedSegment = dict[\n    str, Union[None, str, \"RecordSerialisedSegment\", list[\"RecordSerialisedSegment\"]]\n]\n\n\n@dataclass(frozen=True)\nclass SourceFix:\n    \"\"\"A stored reference to a fix in the non-templated file.\"\"\"\n\n    edit: str\n    source_slice: slice\n    # TODO: It might be possible to refactor this to not require\n    # a templated_slice (because in theory it's unnecessary).\n    # However much of the fix handling code assumes we need\n    # a position in the templated file to interpret it.\n    # More work required to ach"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "common.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines small container classes to hold intermediate results during linting.\"\"\"\n\nfrom typing import Any, NamedTuple, Optional, Union\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLLexError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.parser.segments.base import BaseSegment\nfrom sqlfluff.core.templaters import TemplatedFile\n\n\nclass RuleTuple(NamedTuple):\n    \"\"\"Rule Tuple object for describing rules.\"\"\"\n\n    code: str\n    name: str\n    description: str\n    groups: tuple[str, ...]\n    aliases: tuple[str, ...]\n\n\nclass RenderedFile(NamedTuple):\n    \"\"\"An object to store the result of a templated file/string.\n\n    This is notable as it's the intermediate state between what happens\n    in the main process and the child processes when running in parallel mode.\n    \"\"\"\n\n    templated_variants: list[TemplatedFile]\n    templater_violations: list[SQLTemplaterError]\n    config: FluffConfig\n    time_dict: dict[str, float]\n    fname: str\n    encoding: str\n    source_str: str\n\n\nclass ParsedVariant(NamedTuple):\n    \"\"\"An object to store the result of parsing a single TemplatedFile.\n\n    Args:\n        templated_file (:obj:`TemplatedFile`): Containing the details\n            of the templated file. If templating fails, this will be `None`.\n        tree (:obj:`BaseSegment`): The segment structure representing the\n            parsed file. If parsing fails due to an unrecoverable\n            violation then we will be None.\n        lexing_violations (:obj:`list` of :obj:`SQLLexError`): Any violations\n            raised during the lexing phase.\n        parsing_violations (:obj:`list` of :obj:`SQLParseError`): Any violations\n            raised during the lexing phase.\n    \"\"\"\n\n    templated_file: TemplatedFile\n    tree: Optional[BaseSegment]\n    lexing_violations: list[SQLLexError]\n    parsing_violations: list[SQLParseError]\n\n    def violations(self) -> list[Union[SQLLexError, SQLParseError]]:\n        \"\"\"Return"}, {"start_line": 47000, "end_line": 48594, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "nly when there is more than one file.\n            # Additionally, as it's updated after each loop, we need to get file name\n            # from the next loop. This is why `enumerate` starts with `1` and there\n            # is `i < len` to not exceed files list length.\n            progress_bar_files.update(n=1)\n            if i < len(expanded_paths):\n                progress_bar_files.set_description(f\"file {expanded_paths[i]}\")\n\n        result.stop_timer()\n        return result\n\n    def parse_path(\n        self,\n        path: str,\n        parse_statistics: bool = False,\n    ) -> Iterator[ParsedString]:\n        \"\"\"Parse a path of sql files.\n\n        NB: This a generator which will yield the result of each file\n        within the path iteratively.\n        \"\"\"\n        sql_exts = self.config.get(\"sql_file_exts\", default=\".sql\").lower().split(\",\")\n        for fname in paths_from_path(\n            path,\n            target_file_exts=sql_exts,\n        ):\n            if self.formatter:\n                self.formatter.dispatch_path(path)\n            # Load the file with the config and yield the result.\n            try:\n                raw_file, config, encoding = self.load_raw_file_and_config(\n                    fname, self.config\n                )\n            except SQLFluffSkipFile as s:\n                linter_logger.warning(str(s))\n                continue\n            yield self.parse_string(\n                raw_file,\n                fname=fname,\n                config=config,\n                encoding=encoding,\n                parse_statistics=parse_statistics,\n            )\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "lexer.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The code for the Lexer.\"\"\"\n\nimport logging\nfrom collections.abc import Iterator\nfrom typing import Any, NamedTuple, Optional, Union\nfrom uuid import UUID, uuid4\n\nimport regex\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLLexError\nfrom sqlfluff.core.helpers.slice import is_zero_slice, offset_slice, to_tuple\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments import (\n    BaseSegment,\n    Dedent,\n    EndOfFile,\n    Indent,\n    MetaSegment,\n    RawSegment,\n    TemplateLoop,\n    TemplateSegment,\n    UnlexableSegment,\n)\nfrom sqlfluff.core.templaters import TemplatedFile\nfrom sqlfluff.core.templaters.base import TemplatedFileSlice\n\n# Instantiate the lexer logger\nlexer_logger = logging.getLogger(\"sqlfluff.lexer\")\n\n\nclass BlockTracker:\n    \"\"\"This is an object for keeping track of templating blocks.\n\n    Using the .enter() and .exit() methods on opening and closing\n    blocks, we can match up tags of the same level so that later\n    it's easier to treat them the same way in the linting engine.\n\n    In case looping means that we encounter the same block more\n    than once, we use cache uuids against their source location\n    so that if we try to re-enter the block again, it will get\n    the same uuid on the second pass.\n    \"\"\"\n\n    _stack: list[UUID] = []\n    _map: dict[tuple[int, int], UUID] = {}\n\n    def enter(self, src_slice: slice) -> None:\n        \"\"\"Add a block to the stack.\"\"\"\n        key = to_tuple(src_slice)\n        uuid = self._map.get(key, None)\n\n        if not uuid:\n            uuid = uuid4()\n            self._map[key] = uuid\n            lexer_logger.debug(\n                \"        Entering block stack @ %s: %s (fresh)\",\n                src_slice,\n                uuid,\n            )\n        else:\n            lexer_logger.debug(\n                \"        Entering block stack @ %s: %s (cached)\",\n                src_slice,\n                uuid,\n            )\n\n        self._stack.append(uui"}, {"start_line": 0, "end_line": 1849, "belongs_to": {"file_name": "helpers.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Helpers for the parser module.\"\"\"\n\nfrom typing import TYPE_CHECKING\n\nfrom sqlfluff.core.errors import SQLParseError\n\nif TYPE_CHECKING:\n    from sqlfluff.core.parser.segments import BaseSegment  # pragma: no cover\n\n\ndef join_segments_raw(segments: tuple[\"BaseSegment\", ...]) -> str:\n    \"\"\"Make a string from the joined `raw` attributes of an iterable of segments.\"\"\"\n    return \"\".join(s.raw for s in segments)\n\n\ndef check_still_complete(\n    segments_in: tuple[\"BaseSegment\", ...],\n    matched_segments: tuple[\"BaseSegment\", ...],\n    unmatched_segments: tuple[\"BaseSegment\", ...],\n) -> bool:\n    \"\"\"Check that the segments in are the same as the segments out.\"\"\"\n    initial_str = join_segments_raw(segments_in)\n    current_str = join_segments_raw(matched_segments + unmatched_segments)\n\n    if initial_str != current_str:  # pragma: no cover\n        segment = unmatched_segments[0] if unmatched_segments else None\n        raise SQLParseError(\n            f\"Parse completeness check fail: {current_str!r} != {initial_str!r}\",\n            segment=segment,\n        )\n    return True\n\n\ndef trim_non_code_segments(\n    segments: tuple[\"BaseSegment\", ...],\n) -> tuple[\n    tuple[\"BaseSegment\", ...], tuple[\"BaseSegment\", ...], tuple[\"BaseSegment\", ...]\n]:\n    \"\"\"Take segments and split off surrounding non-code segments as appropriate.\n\n    We use slices to avoid creating too many unnecessary tuples.\n    \"\"\"\n    pre_idx = 0\n    seg_len = len(segments)\n    post_idx = seg_len\n\n    if segments:\n        seg_len = len(segments)\n\n        # Trim the start\n        while pre_idx < seg_len and not segments[pre_idx].is_code:\n            pre_idx += 1\n\n        # Trim the end\n        while post_idx > pre_idx and not segments[post_idx - 1].is_code:\n            post_idx -= 1\n\n    return segments[:pre_idx], segments[pre_idx:post_idx], segments[post_idx:]\n"}], "retrieved_count": 10, "cost_time": 1.2676188945770264}
{"question": "Where does SQLFluff's configuration loading flow from file discovery through inheritance resolution?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's configuration loading flow follows a hierarchical path from file discovery through inheritance resolution. The flow includes: 1) Default configuration - The process starts with built-in default configuration loaded from src/sqlfluff/core/default_config.cfg; 2) User home directory - Configuration is loaded from user's home directory (~) for global user settings; 3) App config directory - System-specific app config directory (~/.config/sqlfluff on Unix, AppData on Windows) is checked; 4) Working directory hierarchy - Configuration files are searched in directories from working directory up to home directory; 5) Project-specific files - Configuration files (.sqlfluff, setup.cfg, tox.ini, pyproject.toml) are loaded from current project directory; 6) Subdirectory inheritance - Configuration is inherited from parent directories down to the file being processed; 7) File-specific configuration - In-file configuration directives (-- noqa: comments) are processed; 8) Command-line overrides - Command-line arguments override file-based configuration; 9) Configuration merging - All configuration sources are merged using nested_combine() with later sources overriding earlier ones; 10) Validation and resolution - Final configuration is validated and resolved into the FluffConfig object used throughout the system.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "loader_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the configuration routines.\"\"\"\n\nimport os\nimport sys\nfrom contextlib import contextmanager\nfrom unittest.mock import call, patch\n\nimport pytest\n\nfrom sqlfluff.core import FluffConfig\nfrom sqlfluff.core.config import (\n    load_config_at_path,\n    load_config_file,\n    load_config_string,\n    load_config_up_to_path,\n)\nfrom sqlfluff.core.config.loader import (\n    _get_user_config_dir_path,\n    _load_user_appdir_config,\n)\nfrom sqlfluff.core.errors import SQLFluffUserError\n\nconfig_a = {\n    \"core\": {\"testing_val\": \"foobar\", \"testing_int\": 4, \"dialect\": \"mysql\"},\n    \"bar\": {\"foo\": \"barbar\"},\n}\n\n\n@pytest.fixture\ndef mock_xdg_home(monkeypatch):\n    \"\"\"Sets the XDG_CONFIG_HOME variable.\"\"\"\n    monkeypatch.setenv(\"XDG_CONFIG_HOME\", \"~/.config/my/special/path\")\n\n\ndef test__config__load_file_dir():\n    \"\"\"Test loading config from a directory path.\"\"\"\n    cfg = load_config_at_path(\n        os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\")\n    )\n    assert cfg == config_a\n\n\ndef test__config__load_from_string():\n    \"\"\"Test loading config from a string.\"\"\"\n    # Load a string\n    with open(\n        os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\", \".sqlfluff\")\n    ) as f:\n        config_string = f.read()\n    cfg = load_config_string(config_string)\n    assert cfg == config_a\n\n\ndef test__config__load_file_f():\n    \"\"\"Test loading config from a file path.\"\"\"\n    cfg = load_config_at_path(\n        os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\", \"testing.sql\")\n    )\n    assert cfg == config_a\n\n\ndef test__config__load_file_missing_extra():\n    \"\"\"Test loading config from a file path if extra path is not found.\"\"\"\n    with pytest.raises(SQLFluffUserError):\n        load_config_up_to_path(\n            os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\", \"testing.sql\"),\n            extra_config_path=\"non/existent/path\",\n        )\n\n\ndef test__config__load_nested():\n    \"\"\"Test nested overwrite and order of precedence of config files.\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "file.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Lower level routines for config file loading and caching.\n\nFunctions in this module load config from *individual* files and\nresources. While some are cached, they are cached on the basis of\nnot processing individual files more than once.\n\nFor the cached functions it is VERY recommended to make sure they\nare copied before any edits happen to them, as those edits may\npropagate back up into the cache. Typically the results are passed\nto `nested_combine` either immediately, or eventually after returning\nwhich should negate this effect.\n\"\"\"\n\nimport os.path\nfrom functools import cache\nfrom typing import Optional\n\nfrom sqlfluff.core.config.ini import load_ini_string\nfrom sqlfluff.core.config.toml import load_toml_file_config\nfrom sqlfluff.core.config.validate import validate_config_dict\nfrom sqlfluff.core.helpers.string import (\n    split_comma_separated_string,\n)\nfrom sqlfluff.core.types import ConfigMappingType\n\nCOMMA_SEPARATED_PATH_KEYS = (\n    \"load_macros_from_path\",\n    \"loader_search_path\",\n    \"exclude_macros_from_path\",\n)\nRESOLVE_PATH_SUFFIXES = (\"_path\", \"_dir\")\n\n\ndef _load_raw_file_as_dict(filepath: str) -> ConfigMappingType:\n    \"\"\"Loads the raw dict object from file without interpolation.\"\"\"\n    filename = os.path.basename(filepath)\n    if filename == \"pyproject.toml\":\n        return load_toml_file_config(filepath)\n    # If it's not a pyproject file, assume that it's an ini file.\n    with open(filepath, mode=\"r\") as file:\n        return load_ini_string(file.read())\n\n\ndef _resolve_path(filepath: str, val: str) -> str:\n    \"\"\"Try to resolve a path found in a config value.\"\"\"\n    # Make the referenced path.\n    ref_path = os.path.join(os.path.dirname(filepath), val)\n    # Check if it exists, and if it does, replace the value with the path.\n    return ref_path if os.path.exists(ref_path) else val\n\n\ndef _resolve_paths_in_config(\n    config: ConfigMappingType, filepath: str, logging_reference: Optional[str] = None\n) -> None:\n    \"\"\"Attempt to resolve any paths fo"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fluffconfig.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Module for loading config.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom collections.abc import Iterable\nfrom copy import copy, deepcopy\nfrom itertools import chain\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nimport pluggy\n\nfrom sqlfluff.core.config.ini import coerce_value\nfrom sqlfluff.core.config.loader import load_config_string, load_config_up_to_path\nfrom sqlfluff.core.config.validate import validate_config_dict\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.helpers.dict import (\n    dict_diff,\n    iter_records_from_nested_dict,\n    nested_combine,\n    records_to_nested_dict,\n)\nfrom sqlfluff.core.helpers.string import (\n    split_colon_separated_string,\n    split_comma_separated_string,\n)\nfrom sqlfluff.core.plugin.host import get_plugin_manager\nfrom sqlfluff.core.types import ConfigMappingType, ConfigValueOrListType\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.templaters.base import RawTemplater\n\n# Instantiate the config logger\nconfig_logger = logging.getLogger(\"sqlfluff.config\")\n\n\nclass FluffConfig:\n    \"\"\"The persistent object for internal methods to access configuration.\n\n    This class is designed to be instantiated once for each file and then be\n    reused by each part of the process. For multiple files in the same path, a\n    parent object will be created for the each path and then variants of it\n    are created *for each file*. The object itself contains the references\n    to any long lived objects which might be used by multiple parts of the\n    codebase such as the dialect and the templater (both of which can be\n    resource intensive to load & instantiate), which allows (for example),\n    multiple files to reuse the same instance of the relevant dialect.\n\n    It is also designed to pickle well for use in parallel operations.\n\n    Args:\n        configs (ConfigMappingType, optional): A nested dict of config\n            values from which to construct the config.\n        extra_config_path"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fluffconfig_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the configuration routines.\"\"\"\n\nimport logging\nimport os\n\nimport pytest\n\nimport sqlfluff\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.templaters import (\n    JinjaTemplater,\n    PlaceholderTemplater,\n    PythonTemplater,\n    RawTemplater,\n)\nfrom sqlfluff.utils.testing.logging import fluff_log_catcher\n\nconfig_b = {\n    \"core\": {\"rules\": \"LT03\", \"dialect\": \"ansi\"},\n    \"layout\": {\n        \"type\": {\"comma\": {\"line_position\": \"trailing\", \"spacing_before\": \"touch\"}}\n    },\n}\n\nconfig_c = {\n    \"core\": {\"rules\": \"LT03\", \"dialect\": \"ansi\"},\n    # NOTE:\n    # - NOT_A_RULE doesn't match anything.\n    # - L001 is an alias, but no longer a rule.\n    # - layout is a group and but doesn't match any individual rule.\n    \"rules\": {\n        \"NOT_A_RULE\": {\"foo\": \"bar\"},\n        \"L001\": {\"foo\": \"bar\"},\n        \"layout\": {\"foo\": \"bar\"},\n    },\n}\n\n\ndef test__config__from_strings():\n    \"\"\"Test loading config from multiple strings.\"\"\"\n    strings = [\n        \"[sqlfluff]\\ndialect=mysql\\ntesting_val=foobar\",\n        \"[sqlfluff]\\ndialect=postgres\\ntesting_val2=bar\",\n        \"[sqlfluff]\\ndialect=mysql\\ntesting_val=foo\",\n    ]\n    cfg = FluffConfig.from_strings(*strings)\n    assert cfg.get(\"dialect\") == \"mysql\"\n    assert cfg.get(\"testing_val2\") == \"bar\"\n    assert cfg.get(\"testing_val\") == \"foo\"\n\n\ndef test__config__nested_config_tests():\n    \"\"\"Test linting with overridden config in nested paths.\n\n    This looks like a linter test but it's actually a config\n    test.\n    \"\"\"\n    lntr = Linter(\n        # Exclude CP02 in overrides (similar to cli --exclude-rules)\n        config=FluffConfig(overrides=dict(exclude_rules=\"CP02\", dialect=\"ansi\"))\n    )\n    lnt = lntr.lint_path(\"test/fixtures/config/inheritance_b\")\n    violations = lnt.check_tuples_by_path()\n    for k in violations:\n        if k.endswith(\"nested\\\\example.sql\"):\n            # CP01 is enabled in the .sqlfluff file and not excluded.\n            assert"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "loader.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " filesystem,\n        # but depending on the user's setup, this might result in\n        # permissions errors.\n        parent_config_paths = list(\n            iter_intermediate_paths(\n                Path(path).absolute(), Path(os.path.expanduser(\"~\"))\n            )\n        )\n        # Stripping off the home directory and the current working\n        # directory, since they are both covered by other code\n        # here\n        parent_config_paths = parent_config_paths[1:-1]\n        parent_config_stack = [\n            load_config_at_path(str(p.resolve())) for p in list(parent_config_paths)\n        ]\n        # Resolve paths to ensure caching is accurate.\n        config_paths = iter_intermediate_paths(Path(path).absolute(), Path.cwd())\n        config_stack = [load_config_at_path(str(p.resolve())) for p in config_paths]\n\n    # 4) Extra config paths.\n    # When calling `load_config_file_as_dict` we resolve the path first so that caching\n    # is more efficient.\n    extra_config = {}\n    if extra_config_path:\n        try:\n            extra_config = load_config_file_as_dict(\n                str(Path(extra_config_path).resolve())\n            )\n        except FileNotFoundError:\n            raise SQLFluffUserError(\n                f\"Extra config path '{extra_config_path}' does not exist.\"\n            )\n\n    return nested_combine(\n        user_appdir_config,\n        user_config,\n        *parent_config_stack,\n        *config_stack,\n        extra_config,\n    )\n\n\nclass ConfigLoader:\n    \"\"\"The class for loading config files.\n\n    NOTE: Deprecated class maintained because it was in our example\n    plugin for a long while. Remove once this warning has been live for\n    an appropriate amount of time.\n    \"\"\"\n\n    def __init__(self) -> None:  # pragma: no cover\n        config_logger.warning(\n            \"ConfigLoader is deprecated, and no longer necessary. \"\n            \"Please update your plugin to use the config loading functions directly \"\n            \"to remove this message.\"\n      "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Init file for the config module.\n\nThis holds all the methods and classes for configuration.\n\"\"\"\n\nfrom typing import Optional\n\nfrom sqlfluff.core.config.file import (\n    load_config_file_as_dict,\n    load_config_string_as_dict,\n)\nfrom sqlfluff.core.config.fluffconfig import FluffConfig\nfrom sqlfluff.core.config.loader import (\n    ConfigLoader,\n    load_config_at_path,\n    load_config_file,\n    load_config_resource,\n    load_config_string,\n    load_config_up_to_path,\n)\n\n__all__ = (\n    \"FluffConfig\",\n    \"ConfigLoader\",\n    \"load_config_file\",\n    \"load_config_resource\",\n    \"load_config_string\",\n    \"load_config_at_path\",\n    \"load_config_up_to_path\",\n    \"progress_bar_configuration\",\n    \"clear_config_caches\",\n)\n\n\ndef clear_config_caches() -> None:\n    \"\"\"Clear any of the cached config methods.\n\n    This is primarily used during testing where the cache may be be rendered unreliable\n    by using moving around files while setting up tests. Some of the cached methods\n    rely on *filename* caching, and so we may break one of the assumptions of the\n    caching routines (that files aren't modified while SQLFluff is running) during\n    the test suite. That means we need to clear the cache during those times to\n    get reliable results.\n\n    NOTE: You may not notice those results when running tests individually locally\n    as they may only be visible when running the whole test suite.\n    \"\"\"\n    load_config_file_as_dict.cache_clear()\n    load_config_at_path.cache_clear()\n    load_config_string_as_dict.cache_clear()\n    pass\n\n\nclass ProgressBarConfiguration:\n    \"\"\"Singleton-esque progress bar configuration.\n\n    It's expected to be set during starting with parameters coming from commands\n    parameters, then to be just utilized as just\n    ```\n    from sqlfluff.core.config import progress_bar_configuration\n    is_progressbar_disabled = progress_bar_configuration.disable_progress_bar\n    ```\n    \"\"\"\n\n    _disable_progress_bar: Optional[bool] = True\n\n    @property\n    "}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "loader.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "d finally ``setup.cfg``.\n\n    By accepting only a path string, we enable efficient caching of\n    results, such that configuration can be reused between files without\n    reloading the information from disk.\n    \"\"\"\n    # The potential filenames we would look for at this path.\n    # NB: later in this list overwrites earlier\n    filename_options = [\n        \"setup.cfg\",\n        \"tox.ini\",\n        \"pep8.ini\",\n        \".sqlfluff\",\n        \"pyproject.toml\",\n    ]\n\n    configs: ConfigMappingType = {}\n\n    if os.path.isdir(path):\n        p = path\n    else:\n        p = os.path.dirname(path)\n\n    d = os.listdir(os.path.expanduser(p))\n    # iterate this way round to make sure things overwrite is the right direction.\n    # NOTE: The `configs` variable is passed back in at each stage.\n    for fname in filename_options:\n        if fname in d:\n            configs = load_config_file(p, fname, configs=configs)\n\n    return configs\n\n\ndef _load_user_appdir_config() -> ConfigMappingType:\n    \"\"\"Load the config from the user's OS specific appdir config directory.\"\"\"\n    user_config_dir_path = _get_user_config_dir_path(sys.platform)\n    if os.path.exists(user_config_dir_path):\n        return load_config_at_path(user_config_dir_path)\n    else:\n        return {}\n\n\ndef load_config_up_to_path(\n    path: str,\n    extra_config_path: Optional[str] = None,\n    ignore_local_config: bool = False,\n) -> ConfigMappingType:\n    \"\"\"Loads a selection of config files from both the path and its parent paths.\n\n    Args:\n        path (str): The directory which is the target of the search. Config\n            files in subdirectories will not be loaded by this method, but\n            valid config files between this path and the current working\n            path will.\n        extra_config_path (str, optional): An additional path to load config\n            from. This path is not used in iterating through intermediate\n            paths, and is loaded last (taking the highest precedence in\n            combining th"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "loader_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\ndef test__config__load_from_string():\n    \"\"\"Test loading config from a string.\"\"\"\n    # Load a string\n    with open(\n        os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\", \".sqlfluff\")\n    ) as f:\n        config_string = f.read()\n    cfg = load_config_string(config_string)\n    assert cfg == config_a\n\n\ndef test__config__load_file_f():\n    \"\"\"Test loading config from a file path.\"\"\"\n    cfg = load_config_at_path(\n        os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\", \"testing.sql\")\n    )\n    assert cfg == config_a\n\n\ndef test__config__load_file_missing_extra():\n    \"\"\"Test loading config from a file path if extra path is not found.\"\"\"\n    with pytest.raises(SQLFluffUserError):\n        load_config_up_to_path(\n            os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\", \"testing.sql\"),\n            extra_config_path=\"non/existent/path\",\n        )\n\n\ndef test__config__load_nested():\n    \"\"\"Test nested overwrite and order of precedence of config files.\"\"\"\n    cfg = load_config_up_to_path(\n        os.path.join(\n            \"test\", \"fixtures\", \"config\", \"inheritance_a\", \"nested\", \"blah.sql\"\n        ),\n        extra_config_path=os.path.join(\n            \"test\",\n            \"fixtures\",\n            \"config\",\n            \"inheritance_a\",\n            \"extra\",\n            \"this_can_have_any_name.cfg\",\n        ),\n    )\n    assert cfg == {\n        \"core\": {\n            # Outer .sqlfluff defines dialect & testing_val and not overridden.\n            \"dialect\": \"mysql\",\n            \"testing_val\": \"foobar\",\n            # tesing_int is defined in many. Inner pyproject.toml takes precedence.\n            \"testing_int\": 1,\n            # testing_bar is defined only in setup.cfg\n            \"testing_bar\": 7.698,\n        },\n        # bar is defined in a few, but the extra_config takes precedence.\n        \"bar\": {\"foo\": \"foobarextra\"},\n        # fnarr is defined in a few. Inner tox.ini takes precedence.\n        \"fnarr\": {\"fnarr\": {\"foo\": \"foobar\"}},\n    }\n\n\n"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "loader_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    cfg = load_config_up_to_path(\n        os.path.join(\n            \"test\", \"fixtures\", \"config\", \"inheritance_a\", \"nested\", \"blah.sql\"\n        ),\n        extra_config_path=os.path.join(\n            \"test\",\n            \"fixtures\",\n            \"config\",\n            \"inheritance_a\",\n            \"extra\",\n            \"this_can_have_any_name.cfg\",\n        ),\n    )\n    assert cfg == {\n        \"core\": {\n            # Outer .sqlfluff defines dialect & testing_val and not overridden.\n            \"dialect\": \"mysql\",\n            \"testing_val\": \"foobar\",\n            # tesing_int is defined in many. Inner pyproject.toml takes precedence.\n            \"testing_int\": 1,\n            # testing_bar is defined only in setup.cfg\n            \"testing_bar\": 7.698,\n        },\n        # bar is defined in a few, but the extra_config takes precedence.\n        \"bar\": {\"foo\": \"foobarextra\"},\n        # fnarr is defined in a few. Inner tox.ini takes precedence.\n        \"fnarr\": {\"fnarr\": {\"foo\": \"foobar\"}},\n    }\n\n\n@contextmanager\ndef change_dir(path):\n    \"\"\"Set the current working directory to `path` for the duration of the context.\"\"\"\n    original_dir = os.getcwd()\n    try:\n        os.chdir(path)\n        yield\n    finally:\n        os.chdir(original_dir)\n\n\n@pytest.mark.skipif(\n    sys.platform == \"win32\",\n    reason=\"Seems test is not executed under home directory on Windows\",\n)\ndef test__config__load_parent():\n    \"\"\"Test that config is loaded from parent directory of current working directory.\"\"\"\n    with change_dir(\n        os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\", \"nested\")\n    ):\n        cfg = load_config_up_to_path(\"blah.sql\")\n    assert cfg == {\n        \"core\": {\n            \"dialect\": \"mysql\",\n            \"testing_val\": \"foobar\",\n            \"testing_int\": 1,\n            \"testing_bar\": 7.698,\n        },\n        \"bar\": {\"foo\": \"foobar\"},\n        \"fnarr\": {\"fnarr\": {\"foo\": \"foobar\"}},\n    }\n\n\ndef test__config__load_toml():\n    \"\"\"Test loading config from a pyproject.toml"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "loader.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "appauthor=appauthor\n        ).user_config_dir\n    # Defer to the self-detecting paths.\n    # NOTE: On Windows this means that the `sys_platform` argument is not\n    # applied.\n    return platformdirs.user_config_dir(appname, appauthor)\n\n\ndef load_config_file(\n    file_dir: str, file_name: str, configs: Optional[ConfigMappingType] = None\n) -> ConfigMappingType:\n    \"\"\"Load a config file from the filesystem.\n\n    Args:\n        file_dir (str): The path to the location of file to be loaded.\n            This should be a reference to the directory *only* and not\n            include the filename itself. Any paths in the loaded file\n            are resolved relative to this location.\n        file_name (str): The filename of the file to be loaded. If the\n            filename is ``pyproject.toml`` then the file is loaded in\n            ``toml`` format, but otherwise is assumed to be in ``ini``\n            format (as per ``.sqlfluff``).\n        configs (ConfigMappingType, optional): A base set of configs to\n            merge the loaded configs onto. If not provided, the result\n            will contain only the values loaded from the string.\n\n    Returns:\n        :obj:`ConfigMappingType`: A nested dictionary of config values.\n    \"\"\"\n    file_path = os.path.join(file_dir, file_name)\n    raw_config = load_config_file_as_dict(file_path)\n    # We always run `nested_combine()` because it has the side effect\n    # of making a copy of the objects provided. This prevents us\n    # from editing items which also sit within the cache.\n    return nested_combine(configs or {}, raw_config)\n\n\ndef load_config_resource(package: str, file_name: str) -> ConfigMappingType:\n    \"\"\"Load a config resource from a python package.\n\n    Args:\n        package (str): The name of the python package to load the resource\n            from.\n        file_name (str): The filename of the file to be loaded. If the\n            filename is ``pyproject.toml`` then the file is loaded in\n            ``toml`` format, but"}], "retrieved_count": 10, "cost_time": 1.2497236728668213}
{"question": "Where does SQLFluff's parsing flow from source files through lexing to AST generation?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's parsing flow follows a multi-stage pipeline from source files to AST generation. The flow includes: 1) Source file input - SQL files are read from disk or provided as strings to the Linter class; 2) Template processing - If templated SQL is detected, the Templater (Jinja, Python, etc.) renders the template to valid SQL; 3) Lexing stage - The Lexer class breaks down SQL into individual tokens (RawSegment objects) using dialect-specific lexer matchers; 4) Token processing - Lexed tokens are mapped to template slices and converted to RawSegment objects with position markers; 5) Parser initialization - The Parser class is instantiated with the appropriate dialect and configuration; 6) Grammar application - Dialect-specific grammars are applied to lexed segments to identify SQL structures; 7) Tree construction - Segments are recursively matched and combined into a hierarchical parse tree structure; 8) AST generation - The final parse tree (AST) is created with FileSegment as root containing StatementSegments and their sub-components; 9) Validation - The parser validates that all segments were properly processed and no content was lost; 10) Error handling - Any parsing failures are captured as SQLParseError objects for reporting.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parser.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the Parser class.\"\"\"\n\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import check_still_complete\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.parser.segments import BaseFileSegment, BaseSegment\n\n\nclass Parser:\n    \"\"\"Instantiates parsed queries from a sequence of lexed raw segments.\"\"\"\n\n    def __init__(\n        self, config: Optional[FluffConfig] = None, dialect: Optional[str] = None\n    ):\n        if config and dialect:\n            raise ValueError(  # pragma: no cover\n                \"Parser does not support setting both `config` and `dialect`.\"\n            )\n        # Use the provided config or create one from the dialect.\n        self.config = config or FluffConfig.from_kwargs(dialect=dialect)\n        self.RootSegment: type[BaseFileSegment] = self.config.get(\n            \"dialect_obj\"\n        ).get_root_segment()\n\n    def parse(\n        self,\n        segments: Sequence[\"BaseSegment\"],\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> Optional[\"BaseSegment\"]:\n        \"\"\"Parse a series of lexed tokens using the current dialect.\"\"\"\n        if not segments:  # pragma: no cover\n            # This should normally never happen because there will usually\n            # be an end_of_file segment. It would probably only happen in\n            # api use cases.\n            return None\n\n        # NOTE: This is the only time we use the parse context not in the\n        # context of a context manager. That's because it's the initial\n        # instantiation.\n        ctx = ParseContext.from_config(config=self.config)\n        # Kick off parsing with the root segment. The BaseFileSegment has\n        # a unique entry point to facilitate exactly this. All other segments\n        # will use the standard .match() route.\n        root = self.RootSegment.root_pars"}, {"start_line": 0, "end_line": 38, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.core.parser.\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "dialects_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Automated tests for all dialects.\n\nAny files in the test/fixtures/dialects/ directory will be picked up\nand automatically tested against the appropriate dialect.\n\"\"\"\n\nfrom typing import Any, Optional\n\nimport pytest\n\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.linter import ParsedString, RenderedFile\nfrom sqlfluff.core.parser.segments.base import BaseSegment\nfrom sqlfluff.core.templaters import TemplatedFile\n\nfrom ..conftest import (\n    compute_parse_tree_hash,\n    get_parse_fixtures,\n    load_file,\n    make_dialect_path,\n    parse_example_file,\n)\n\nparse_success_examples, parse_structure_examples = get_parse_fixtures(\n    fail_on_missing_yml=True\n)\n\n\ndef lex_and_parse(config_overrides: dict[str, Any], raw: str) -> Optional[ParsedString]:\n    \"\"\"Performs a Lex and Parse, with cacheable inputs within fixture.\"\"\"\n    # Load the right dialect\n    config = FluffConfig(overrides=config_overrides)\n    # Construct rendered file (to skip the templater)\n    templated_file = TemplatedFile.from_string(raw)\n    rendered_file = RenderedFile(\n        [templated_file],\n        [],\n        config,\n        {},\n        templated_file.fname,\n        \"utf8\",\n        raw,\n    )\n    # Parse (which includes lexing)\n    linter = Linter(config=config)\n    parsed_file = linter.parse_rendered(rendered_file)\n    if not raw:  # Empty file case\n        # We're just checking there aren't exceptions in this case.\n        return None\n    # Check we managed to parse\n    assert parsed_file.tree\n    # From just the initial parse, check we're all there\n    assert \"\".join(token.raw for token in parsed_file.tree.raw_segments) == raw\n    # Check we don't have lexing or parsing issues\n    assert not parsed_file.violations\n    return parsed_file\n\n\n@pytest.mark.integration\n@pytest.mark.parse_suite\n@pytest.mark.parametrize(\"dialect,file\", parse_success_examples)\ndef test__dialect__base_file_parse(dialect, file):\n    \"\"\"For given test examples, check successful parsing.\"\"\"\n    raw = load"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"init file for the parser.\"\"\"\n\nfrom sqlfluff.core.parser.grammar import (\n    AnyNumberOf,\n    AnySetOf,\n    Anything,\n    Bracketed,\n    Conditional,\n    Delimited,\n    Nothing,\n    OneOf,\n    OptionallyBracketed,\n    OptionallyDelimited,\n    Ref,\n    Sequence,\n)\nfrom sqlfluff.core.parser.lexer import Lexer, RegexLexer, StringLexer\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.parser import Parser\nfrom sqlfluff.core.parser.parsers import (\n    MultiStringParser,\n    RegexParser,\n    StringParser,\n    TypedParser,\n)\nfrom sqlfluff.core.parser.segments import (\n    BaseFileSegment,\n    BaseSegment,\n    BinaryOperatorSegment,\n    BracketedSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    Dedent,\n    IdentifierSegment,\n    ImplicitIndent,\n    Indent,\n    KeywordSegment,\n    LiteralKeywordSegment,\n    LiteralSegment,\n    NewlineSegment,\n    RawSegment,\n    SegmentGenerator,\n    SourceFix,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.types import ParseMode\n\n__all__ = (\n    \"BaseSegment\",\n    \"SourceFix\",\n    \"BaseFileSegment\",\n    \"BracketedSegment\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"SymbolSegment\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"LiteralKeywordSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"SegmentGenerator\",\n    \"Sequence\",\n    \"OneOf\",\n    \"Delimited\",\n    \"Bracketed\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\","}, {"start_line": 1000, "end_line": 2225, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ",\n    LiteralSegment,\n    NewlineSegment,\n    RawSegment,\n    SegmentGenerator,\n    SourceFix,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.types import ParseMode\n\n__all__ = (\n    \"BaseSegment\",\n    \"SourceFix\",\n    \"BaseFileSegment\",\n    \"BracketedSegment\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"SymbolSegment\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"LiteralKeywordSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"SegmentGenerator\",\n    \"Sequence\",\n    \"OneOf\",\n    \"Delimited\",\n    \"Bracketed\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\",\n    \"Conditional\",\n    \"StringParser\",\n    \"MultiStringParser\",\n    \"TypedParser\",\n    \"RegexParser\",\n    \"PositionMarker\",\n    \"Lexer\",\n    \"StringLexer\",\n    \"RegexLexer\",\n    \"Parser\",\n    \"Matchable\",\n    \"ParseMode\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "lexer.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The code for the Lexer.\"\"\"\n\nimport logging\nfrom collections.abc import Iterator\nfrom typing import Any, NamedTuple, Optional, Union\nfrom uuid import UUID, uuid4\n\nimport regex\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLLexError\nfrom sqlfluff.core.helpers.slice import is_zero_slice, offset_slice, to_tuple\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments import (\n    BaseSegment,\n    Dedent,\n    EndOfFile,\n    Indent,\n    MetaSegment,\n    RawSegment,\n    TemplateLoop,\n    TemplateSegment,\n    UnlexableSegment,\n)\nfrom sqlfluff.core.templaters import TemplatedFile\nfrom sqlfluff.core.templaters.base import TemplatedFileSlice\n\n# Instantiate the lexer logger\nlexer_logger = logging.getLogger(\"sqlfluff.lexer\")\n\n\nclass BlockTracker:\n    \"\"\"This is an object for keeping track of templating blocks.\n\n    Using the .enter() and .exit() methods on opening and closing\n    blocks, we can match up tags of the same level so that later\n    it's easier to treat them the same way in the linting engine.\n\n    In case looping means that we encounter the same block more\n    than once, we use cache uuids against their source location\n    so that if we try to re-enter the block again, it will get\n    the same uuid on the second pass.\n    \"\"\"\n\n    _stack: list[UUID] = []\n    _map: dict[tuple[int, int], UUID] = {}\n\n    def enter(self, src_slice: slice) -> None:\n        \"\"\"Add a block to the stack.\"\"\"\n        key = to_tuple(src_slice)\n        uuid = self._map.get(key, None)\n\n        if not uuid:\n            uuid = uuid4()\n            self._map[key] = uuid\n            lexer_logger.debug(\n                \"        Entering block stack @ %s: %s (fresh)\",\n                src_slice,\n                uuid,\n            )\n        else:\n            lexer_logger.debug(\n                \"        Entering block stack @ %s: %s (cached)\",\n                src_slice,\n                uuid,\n            )\n\n        self._stack.append(uui"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "file.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Definition of the BaseFileSegment.\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Optional\n\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments.base import BaseSegment, UnparsableSegment\n\n\nclass BaseFileSegment(BaseSegment):\n    \"\"\"A segment representing a whole file or script.\n\n    This is also the default \"root\" segment of the dialect,\n    and so is usually instantiated directly. It therefore\n    has no match_grammar.\n    \"\"\"\n\n    type = \"file\"\n    # The file segment is the only one which can start or end with non-code\n    can_start_end_non_code = True\n    # A file can be empty!\n    allow_empty = True\n\n    def __init__(\n        self,\n        segments: tuple[BaseSegment, ...],\n        pos_marker: Optional[PositionMarker] = None,\n        fname: Optional[str] = None,\n    ):\n        self._file_path = fname\n        super().__init__(segments, pos_marker=pos_marker)\n\n    @property\n    def file_path(self) -> Optional[str]:\n        \"\"\"File path of a parsed SQL file.\"\"\"\n        return self._file_path\n\n    @abstractmethod\n    def get_table_references(self) -> set[str]:\n        \"\"\"Use parsed tree to extract table references.\"\"\"\n\n    @classmethod\n    def root_parse(\n        cls,\n        segments: tuple[BaseSegment, ...],\n        parse_context: ParseContext,\n        fname: Optional[str] = None,\n    ) -> \"BaseFileSegment\":\n        \"\"\"This is the entry method into parsing a file lexed segments.\n\n        For single pass matching, this trims any non code off\n        the start, matches the middle and then trims the end.\n\n        Anything unexpected at the end is regarded as unparsable.\n        \"\"\"\n        # Trim the start\n        _start_idx = 0\n        for _start_idx in range(len(segments)):\n            if segments[_start_idx].is_code:\n                break\n\n        # Trim the end\n        _end_idx = len(segments)\n        for _end_idx in range(len(segments), _start_idx - 1, -1):\n   "}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "emplating block indents.\n                    if not templating_blocks_indent:\n                        continue  # pragma: no cover\n            new_segments.append(segment)\n\n        # Return new buffer\n        return new_segments, violations\n\n    @staticmethod\n    def _parse_tokens(\n        tokens: Sequence[BaseSegment],\n        config: FluffConfig,\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> tuple[Optional[BaseSegment], list[SQLParseError]]:\n        parser = Parser(config=config)\n        violations = []\n        # Parse the file and log any problems\n        try:\n            parsed: Optional[BaseSegment] = parser.parse(\n                # Regardless of how the sequence was passed in, we should\n                # coerce it to a tuple here, before we head deeper into\n                # the parsing process.\n                tuple(tokens),\n                fname=fname,\n                parse_statistics=parse_statistics,\n            )\n        except SQLParseError as err:\n            linter_logger.info(\"PARSING FAILED! : %s\", err)\n            violations.append(err)\n            return None, violations\n\n        if parsed is None:  # pragma: no cover\n            return None, violations\n\n        linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n        linter_logger.info(\"\\n\" + parsed.stringify())\n        # We may succeed parsing, but still have unparsable segments. Extract them\n        # here.\n        for unparsable in parsed.iter_unparsables():\n            # No exception has been raised explicitly, but we still create one here\n            # so that we can use the common interface\n            assert unparsable.pos_marker\n            violations.append(\n                SQLParseError(\n                    \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n                    \"{1!r}\".format(\n                        unparsable.pos_marker.working_loc,\n                        (\n                            unparsable.raw\n   "}, {"start_line": 0, "end_line": 38, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.core.linter.\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "conftest.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Sharing fixtures to test the dialects.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.parser import BaseSegment, Lexer\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\n\n\ndef lex(raw, config):\n    \"\"\"Basic parsing for the tests below.\"\"\"\n    # Set up the lexer\n    lex = Lexer(config=config)\n    # Lex the string for matching. For a good test, this would\n    # arguably happen as a fixture, but it's easier to pass strings\n    # as parameters than pre-lexed segment strings.\n    segments, vs = lex.lex(raw)\n    assert not vs\n    print(segments)\n    return segments\n\n\ndef validate_segment(segmentref, config):\n    \"\"\"Get and validate segment for tests below.\"\"\"\n    Seg = config.get(\"dialect_obj\").ref(segmentref)\n    if isinstance(Seg, Matchable):\n        return Seg\n    try:\n        if issubclass(Seg, BaseSegment):\n            return Seg\n    except TypeError:\n        pass\n    raise TypeError(\n        \"{} is not of type Segment or Matchable. Test is invalid.\".format(segmentref)\n    )\n\n\ndef _dialect_specific_segment_parses(dialect, segmentref, raw, caplog):\n    \"\"\"Test that specific segments parse as expected.\n\n    NB: We're testing the PARSE function not the MATCH function\n    although this will be a recursive parse and so the match\n    function of SUBSECTIONS will be tested if present. The match\n    function of the parent will not be tested.\n    \"\"\"\n    config = FluffConfig(overrides=dict(dialect=dialect))\n    segments = lex(raw, config=config)\n    Seg = validate_segment(segmentref, config=config)\n\n    # Most segments won't handle the end of file marker. We should strip it.\n    if segments[-1].is_type(\"end_of_file\"):\n        segments = segments[:-1]\n\n    ctx = ParseContext.from_config(config)\n    with caplog.at_level(logging.DEBUG):\n        result = Seg.match(segments, 0, parse_context=ctx)\n    assert isinst"}], "retrieved_count": 10, "cost_time": 1.2721600532531738}
{"question": "Why does SQLFluff implement dialect-specific parsing rather than using a unified SQL parser?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff implements dialect-specific parsing rather than a unified SQL parser to address the significant variations and incompatibilities between different SQL dialects. Key reasons include: 1) Syntax differences - Different SQL dialects have varying syntax for common operations (e.g., string concatenation, date functions, window functions); 2) Keyword variations - Reserved keywords differ between dialects (e.g., TOP vs LIMIT, ISNULL vs IFNULL); 3) Data type differences - Each dialect supports different data types and type casting syntax; 4) Function variations - Built-in functions have different names and parameter patterns across dialects; 5) Extensions and proprietary features - Each database vendor adds proprietary SQL extensions that aren't part of standard SQL; 6) Parsing accuracy - Dialect-specific parsers can provide more accurate parsing and better error detection for each specific dialect; 7) Rule applicability - Different linting rules may only be relevant or applicable to certain dialects; 8) User expectations - Users expect SQLFluff to understand their specific dialect's syntax and conventions; 9) Error reporting - Dialect-specific parsing enables more precise error messages and suggestions; 10) Future extensibility - The dialect system allows SQLFluff to support new dialects without affecting existing ones.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 35, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.dialects.\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parser.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the Parser class.\"\"\"\n\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import check_still_complete\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.parser.segments import BaseFileSegment, BaseSegment\n\n\nclass Parser:\n    \"\"\"Instantiates parsed queries from a sequence of lexed raw segments.\"\"\"\n\n    def __init__(\n        self, config: Optional[FluffConfig] = None, dialect: Optional[str] = None\n    ):\n        if config and dialect:\n            raise ValueError(  # pragma: no cover\n                \"Parser does not support setting both `config` and `dialect`.\"\n            )\n        # Use the provided config or create one from the dialect.\n        self.config = config or FluffConfig.from_kwargs(dialect=dialect)\n        self.RootSegment: type[BaseFileSegment] = self.config.get(\n            \"dialect_obj\"\n        ).get_root_segment()\n\n    def parse(\n        self,\n        segments: Sequence[\"BaseSegment\"],\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> Optional[\"BaseSegment\"]:\n        \"\"\"Parse a series of lexed tokens using the current dialect.\"\"\"\n        if not segments:  # pragma: no cover\n            # This should normally never happen because there will usually\n            # be an end_of_file segment. It would probably only happen in\n            # api use cases.\n            return None\n\n        # NOTE: This is the only time we use the parse context not in the\n        # context of a context manager. That's because it's the initial\n        # instantiation.\n        ctx = ParseContext.from_config(config=self.config)\n        # Kick off parsing with the root segment. The BaseFileSegment has\n        # a unique entry point to facilitate exactly this. All other segments\n        # will use the standard .match() route.\n        root = self.RootSegment.root_pars"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "conftest.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Sharing fixtures to test the dialects.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.parser import BaseSegment, Lexer\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\n\n\ndef lex(raw, config):\n    \"\"\"Basic parsing for the tests below.\"\"\"\n    # Set up the lexer\n    lex = Lexer(config=config)\n    # Lex the string for matching. For a good test, this would\n    # arguably happen as a fixture, but it's easier to pass strings\n    # as parameters than pre-lexed segment strings.\n    segments, vs = lex.lex(raw)\n    assert not vs\n    print(segments)\n    return segments\n\n\ndef validate_segment(segmentref, config):\n    \"\"\"Get and validate segment for tests below.\"\"\"\n    Seg = config.get(\"dialect_obj\").ref(segmentref)\n    if isinstance(Seg, Matchable):\n        return Seg\n    try:\n        if issubclass(Seg, BaseSegment):\n            return Seg\n    except TypeError:\n        pass\n    raise TypeError(\n        \"{} is not of type Segment or Matchable. Test is invalid.\".format(segmentref)\n    )\n\n\ndef _dialect_specific_segment_parses(dialect, segmentref, raw, caplog):\n    \"\"\"Test that specific segments parse as expected.\n\n    NB: We're testing the PARSE function not the MATCH function\n    although this will be a recursive parse and so the match\n    function of SUBSECTIONS will be tested if present. The match\n    function of the parent will not be tested.\n    \"\"\"\n    config = FluffConfig(overrides=dict(dialect=dialect))\n    segments = lex(raw, config=config)\n    Seg = validate_segment(segmentref, config=config)\n\n    # Most segments won't handle the end of file marker. We should strip it.\n    if segments[-1].is_type(\"end_of_file\"):\n        segments = segments[:-1]\n\n    ctx = ParseContext.from_config(config)\n    with caplog.at_level(logging.DEBUG):\n        result = Seg.match(segments, 0, parse_context=ctx)\n    assert isinst"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Contains SQL Dialects.\n\nNote that individual dialects are only imported as needed at runtime.\nThis avoids circular references.\n\nTo enable this, any modules outside of .dialects cannot import dialects\ndirectly. They should import `dialect_selector` and use that to fetch\ndialects.\n\nWithin .dialects, each dialect is free to depend on other dialects as\nrequired. Any dependent dialects will be loaded as needed.\n\"\"\"\n\nfrom collections.abc import Iterator\nfrom importlib import import_module\nfrom typing import NamedTuple\n\n# Eventually it would be a good to dynamically discover dialects\n# from any module beginning with \"dialect_\" within this folder.\nfrom sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.errors import SQLFluffUserError\n\n_dialect_lookup = {\n    \"ansi\": (\"dialect_ansi\", \"ansi_dialect\"),\n    \"athena\": (\"dialect_athena\", \"athena_dialect\"),\n    \"bigquery\": (\"dialect_bigquery\", \"bigquery_dialect\"),\n    \"clickhouse\": (\"dialect_clickhouse\", \"clickhouse_dialect\"),\n    \"databricks\": (\"dialect_databricks\", \"databricks_dialect\"),\n    \"db2\": (\"dialect_db2\", \"db2_dialect\"),\n    \"doris\": (\"dialect_doris\", \"doris_dialect\"),\n    \"duckdb\": (\"dialect_duckdb\", \"duckdb_dialect\"),\n    \"exasol\": (\"dialect_exasol\", \"exasol_dialect\"),\n    \"flink\": (\"dialect_flink\", \"flink_dialect\"),\n    \"greenplum\": (\"dialect_greenplum\", \"greenplum_dialect\"),\n    \"hive\": (\"dialect_hive\", \"hive_dialect\"),\n    \"impala\": (\"dialect_impala\", \"impala_dialect\"),\n    \"materialize\": (\"dialect_materialize\", \"materialize_dialect\"),\n    \"mariadb\": (\"dialect_mariadb\", \"mariadb_dialect\"),\n    \"mysql\": (\"dialect_mysql\", \"mysql_dialect\"),\n    \"oracle\": (\"dialect_oracle\", \"oracle_dialect\"),\n    \"postgres\": (\"dialect_postgres\", \"postgres_dialect\"),\n    \"redshift\": (\"dialect_redshift\", \"redshift_dialect\"),\n    \"snowflake\": (\"dialect_snowflake\", \"snowflake_dialect\"),\n    \"soql\": (\"dialect_soql\", \"soql_dialect\"),\n    \"sparksql\": (\"dialect_sparksql\", \"sparksql_dialect\"),\n    \"sqlite\": (\"dialect_sqlite\", \"sql"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the base dialect class.\"\"\"\n\nimport sys\nfrom typing import Any, Optional, Union, cast\n\nfrom sqlfluff.core.parser import (\n    BaseSegment,\n    KeywordSegment,\n    SegmentGenerator,\n    StringParser,\n)\nfrom sqlfluff.core.parser.grammar.base import BaseGrammar, Nothing\nfrom sqlfluff.core.parser.lexer import LexerType\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.types import BracketPairTuple, DialectElementType\n\n\nclass Dialect:\n    \"\"\"Serves as the basis for runtime resolution of Grammar.\n\n    Args:\n        name (:obj:`str`): The name of the dialect, used for lookup.\n        lexer_matchers (iterable of :obj:`StringLexer`): A structure defining\n            the lexing config for this dialect.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        root_segment_name: str,\n        lexer_matchers: Optional[list[LexerType]] = None,\n        library: Optional[dict[str, DialectElementType]] = None,\n        sets: Optional[dict[str, set[Union[str, BracketPairTuple]]]] = None,\n        inherits_from: Optional[str] = None,\n        formatted_name: Optional[str] = None,\n        docstring: Optional[str] = None,\n    ) -> None:\n        self._library = library or {}\n        self.name = name\n        self.lexer_matchers = lexer_matchers\n        self.expanded = False\n        self._sets = sets or {}\n        self.inherits_from = inherits_from\n        self.root_segment_name = root_segment_name\n        # Attributes for documentation\n        self.formatted_name: str = formatted_name or name\n        self.docstring = docstring or f\"The dialect for {self.formatted_name}.\"\n\n    def __repr__(self) -> str:  # pragma: no cover\n        return f\"<Dialect: {self.name}>\"\n\n    def expand(self) -> \"Dialect\":\n        \"\"\"Expand any callable references to concrete ones.\n\n        This must be called before using the dialect. But\n        allows more flexible definitions to happen at runtime.\n\n        NOTE: This method returns a copy of the current dialect\n  "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "bigquery_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests specific to the snowflake dialect.\"\"\"\n\nimport hypothesis.strategies as st\nimport pytest\nfrom hypothesis import example, given, note, settings\n\nfrom sqlfluff.core import FluffConfig\nfrom sqlfluff.core.parser import Lexer, Parser\n\n\n@settings(max_examples=100, deadline=None)\n@given(\n    st.lists(\n        st.tuples(st.sampled_from([\"<\", \"=\", \">\"]), st.sampled_from([\"AND\", \"OR\"])),\n        min_size=1,\n        max_size=30,\n    )\n)\n@example(data=[(\"<\", \"AND\")])\n@example(data=[(\">\", \"AND\")])\n@example(data=[(\"<\", \"AND\"), (\">\", \"AND\")])\n@example(data=[(\"<\", \"AND\"), (\"=\", \"AND\"), (\">\", \"AND\")])\n@example(data=[(\">\", \"AND\"), (\"<\", \"AND\")])\n@example(data=[(\"<\", \"AND\"), (\"<\", \"AND\"), (\">\", \"AND\")])\n@example(data=[(\">\", \"AND\"), (\">\", \"AND\"), (\"<\", \"AND\")])\ndef test_bigquery_relational_operator_parsing(data):\n    \"\"\"Tests queries with a diverse mixture of relational operators.\"\"\"\n    # Generate a simple SELECT query with relational operators and conjunctions\n    # as specified in 'data'. Note the conjunctions are used as separators\n    # between comparisons, sn the conjunction in the first item is not used.\n    filter = []\n    for i, (relation, conjunction) in enumerate(data):\n        if i:\n            filter.append(f\" {conjunction} \")\n        filter.append(f\"a {relation} b\")\n    raw = f'SELECT * FROM t WHERE {\"\".join(filter)}'\n    note(f\"query: {raw}\")\n    # Load the right dialect\n    config = FluffConfig(overrides=dict(dialect=\"bigquery\"))\n    tokens, lex_vs = Lexer(config=config).lex(raw)\n    # From just the initial parse, check we're all there\n    assert \"\".join(token.raw for token in tokens) == raw\n    # Check we don't have lexing issues\n    assert not lex_vs\n\n    # Do the parse WITHOUT lots of logging\n    # The logs get too long here to be useful. We should use\n    # specific segment tests if we want to debug logs.\n    parsed = Parser(config=config).parse(tokens)\n    print(f\"Post-parse structure: {parsed.to_tuple(show_raw=True)}\")\n    print(f\"Post-parse structure: {pa"}, {"start_line": 0, "end_line": 38, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.core.parser.\"\"\"\n"}, {"start_line": 0, "end_line": 209, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Dialects, segregated to make imports manageable.\n\nNOTE: dialects should not be imported directly from this\nmodule, but should be accessed instead using the selector\nmethods in `sqlfluff.core.dialects`.\n\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "dialect_ansi.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The core ANSI dialect.\n\nThis is the core SQL grammar. We'll probably extend this or make it pluggable\nfor other dialects. Here we encode the structure of the language.\n\nThere shouldn't be any underlying \"machinery\" here, that should all\nbe defined elsewhere.\n\nA lot of the inspiration for this sql grammar is taken from the cockroach\nlabs full sql grammar. In particular their way for dividing up the expression\ngrammar. Check out their docs, they're awesome.\nhttps://www.cockroachlabs.com/docs/stable/sql-grammar.html#select_stmt\n\"\"\"\n\nfrom collections.abc import Generator\nfrom enum import Enum\nfrom typing import NamedTuple, Optional, Union, cast\n\nfrom sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.dialects.common import AliasInfo, ColumnAliasInfo\nfrom sqlfluff.core.parser import (\n    AnyNumberOf,\n    AnySetOf,\n    Anything,\n    BaseFileSegment,\n    BaseSegment,\n    BinaryOperatorSegment,\n    Bracketed,\n    BracketedSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    Conditional,\n    Dedent,\n    Delimited,\n    IdentifierSegment,\n    ImplicitIndent,\n    Indent,\n    KeywordSegment,\n    LiteralKeywordSegment,\n    LiteralSegment,\n    Matchable,\n    MultiStringParser,\n    NewlineSegment,\n    Nothing,\n    OneOf,\n    OptionallyBracketed,\n    ParseMode,\n    RawSegment,\n    Ref,\n    RegexLexer,\n    RegexParser,\n    SegmentGenerator,\n    Sequence,\n    StringLexer,\n    StringParser,\n    SymbolSegment,\n    TypedParser,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.dialects.dialect_ansi_keywords import (\n    ansi_reserved_keywords,\n    ansi_unreserved_keywords,\n)\n\nansi_dialect = Dialect(\n    \"ansi\",\n    root_segment_name=\"FileSegment\",\n    formatted_name=\"ANSI\",\n    docstring=\"\"\"**Default Casing**: ``UPPERCASE``\n\n**Quotes**: String Literals: ``''``, Identifiers: ``\"\"``\n\nThis is the base dialect which holds most of the definitions of common\nSQL commands "}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "dialect_snowflake.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "flake_dialect.sets(\"initialize_types\"),\n            KeywordSegment,\n            type=\"initialize_type\",\n        )\n    ),\n    CompressionType=OneOf(\n        MultiStringParser(\n            snowflake_dialect.sets(\"compression_types\"),\n            KeywordSegment,\n            type=\"compression_type\",\n        ),\n        MultiStringParser(\n            [\n                f\"'{compression}'\"\n                for compression in snowflake_dialect.sets(\"compression_types\")\n            ],\n            KeywordSegment,\n            type=\"compression_type\",\n        ),\n    ),\n    ScalingPolicy=OneOf(\n        MultiStringParser(\n            snowflake_dialect.sets(\"warehouse_scaling_policies\"),\n            KeywordSegment,\n            type=\"scaling_policy\",\n        ),\n        MultiStringParser(\n            [\n                f\"'{scaling_policy}'\"\n                for scaling_policy in snowflake_dialect.sets(\n                    \"warehouse_scaling_policies\"\n                )\n            ],\n            KeywordSegment,\n            type=\"scaling_policy\",\n        ),\n    ),\n    ValidationModeOptionSegment=RegexParser(\n        r\"'?RETURN_(?:\\d+_ROWS|ERRORS|ALL_ERRORS)'?\",\n        CodeSegment,\n        type=\"validation_mode_option\",\n    ),\n    CopyOptionOnErrorSegment=RegexParser(\n        r\"'?CONTINUE'?|'?SKIP_FILE(?:_[0-9]+%?)?'?|'?ABORT_STATEMENT'?\",\n        LiteralSegment,\n        type=\"copy_on_error_option\",\n    ),\n    DynamicTableLagIntervalSegment=RegexParser(\n        r\"DYNAMIC|'.*'\",\n        LiteralSegment,\n        type=\"dynamic_table_lag_interval_segment\",\n    ),\n    DoubleQuotedUDFBody=TypedParser(\n        \"double_quote\",\n        CodeSegment,\n        type=\"udf_body\",\n        trim_chars=('\"',),\n    ),\n    SingleQuotedUDFBody=TypedParser(\n        \"single_quote\",\n        CodeSegment,\n        type=\"udf_body\",\n        trim_chars=(\"'\",),\n    ),\n    DollarQuotedUDFBody=TypedParser(\n        \"dollar_quote\",\n        CodeSegment,\n        type=\"udf_body\",\n        trim_chars=(\"$\",),\n    ),\n    StagePath=Re"}], "retrieved_count": 10, "cost_time": 1.2996866703033447}
{"question": "Where does the data flow when SQLFluff processes templated SQL from template parsing through rule application to fix generation?", "answer": null, "relative_code_list": null, "ground_truth": "The data flow when SQLFluff processes templated SQL follows a structured sequence from template parsing through rule application to fix generation: 1) Template parsing phase begins where raw SQL with template syntax is processed through the templating engine (Jinja, Python format strings, or dbt templates) to resolve dynamic content and placeholders, creating a TemplatedFile object that maintains both original template and rendered SQL, 2) Template compilation occurs where template variables and expressions are resolved according to the templating engine's rules, with the BlockTracker maintaining information about template blocks and position mapping between template source and rendered output, 3) Lexing phase begins where the templated SQL is tokenized by the dialect-specific lexer, which processes the rendered SQL and creates a sequence of tokens while maintaining awareness of the original template structure, 4) Parsing phase occurs where tokens are processed by the dialect-specific parser to create a parse tree of BaseSegment objects, with the parser handling dialect-specific syntax constructs and maintaining structural information, 5) Rule application phase begins where the Linter applies configured rules to the parse tree, with each rule's _eval() method receiving segments and analyzing them for violations, generating LintResult objects when issues are found, 6) Violation collection happens where all rule violations are collected and organized, with each violation containing information about the location, type, and severity of the issue, 7) Fix generation phase occurs where rules that support automatic fixes create LintFix objects describing how to correct the violations, including the type of fix (create, edit, delete), target segment, and new content, 8) Fix application happens where the LintedFile.apply_fixes() method applies all generated fixes to the original SQL, creating FixPatch objects that represent the actual text changes needed, 9) Position mapping occurs throughout the process where the system maintains accurate position information between template source, rendered SQL, and final output for error reporting and fix application, 10) Output generation happens where the corrected SQL is generated with proper formatting and structure maintained, while preserving the original template structure for template-aware fixes, 11) Error handling occurs throughout the process where template processing errors, parsing errors, and fix application errors are caught and reported with appropriate context, 12) The entire data flow is coordinated through SQLFluff's core processing pipeline, ensuring that template processing, parsing, linting, and fixing work together seamlessly while maintaining accuracy and performance.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "patch.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Helpers for generating patches to fix files.\"\"\"\n\nimport logging\nfrom collections.abc import Iterator\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nfrom sqlfluff.core.parser import BaseSegment\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.templaters import TemplatedFile\n\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\n\n@dataclass\nclass FixPatch:\n    \"\"\"An edit patch for a source file.\"\"\"\n\n    templated_slice: slice\n    fixed_raw: str\n    # The patch category, functions mostly for debugging and explanation\n    # than for function. It allows traceability of *why* this patch was\n    # generated. It has no significance for processing.\n    patch_category: str\n    source_slice: slice\n    templated_str: str\n    source_str: str\n\n    def dedupe_tuple(self) -> tuple[slice, str]:\n        \"\"\"Generate a tuple of this fix for deduping.\"\"\"\n        return (self.source_slice, self.fixed_raw)\n\n\ndef _iter_source_fix_patches(\n    segment: BaseSegment, templated_file: TemplatedFile\n) -> Iterator[FixPatch]:\n    \"\"\"Yield any source patches as fixes now.\n\n    NOTE: This yields source fixes for the segment and any of its\n    children, so it's important to call it at the right point in\n    the recursion to avoid yielding duplicates.\n    \"\"\"\n    for source_fix in segment.source_fixes:\n        yield FixPatch(\n            source_fix.templated_slice,\n            source_fix.edit,\n            patch_category=\"source\",\n            source_slice=source_fix.source_slice,\n            templated_str=templated_file.templated_str[source_fix.templated_slice],\n            source_str=templated_file.source_str[source_fix.source_slice],\n        )\n\n\ndef _iter_templated_patches(\n    segment: BaseSegment, templated_file: TemplatedFile\n) -> Iterator[FixPatch]:\n    \"\"\"Iterate through the segments generating fix patches.\n\n    The patches are generated in TEMPLATED space. This is important\n    so that we defer dealing with any loops until later. At this stage\n    ev"}, {"start_line": 35000, "end_line": 37000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "the excluded rules\n        return {k: v.intersection(noqa_set) for k, v in output_map.items()}\n\n    @classmethod\n    def lint_rendered(\n        cls,\n        rendered: RenderedFile,\n        rule_pack: RulePack,\n        fix: bool = False,\n        formatter: Optional[FormatterInterface] = None,\n    ) -> LintedFile:\n        \"\"\"Take a RenderedFile and return a LintedFile.\"\"\"\n        parsed = cls.parse_rendered(rendered)\n        return cls.lint_parsed(\n            parsed,\n            rule_pack=rule_pack,\n            fix=fix,\n            formatter=formatter,\n            encoding=rendered.encoding,\n        )\n\n    # ### Instance Methods\n    # These are tied to a specific instance and so are not necessarily\n    # safe to use in parallel operations.\n\n    def render_string(\n        self, in_str: str, fname: str, config: FluffConfig, encoding: str\n    ) -> RenderedFile:\n        \"\"\"Template the file.\"\"\"\n        linter_logger.info(\"Rendering String [%s] (%s)\", self.templater.name, fname)\n\n        # Start the templating timer\n        t0 = time.monotonic()\n\n        # Newlines are normalised to unix-style line endings (\\n).\n        # The motivation is that Jinja normalises newlines during templating and\n        # we want consistent mapping between the raw and templated slices.\n        in_str = self._normalise_newlines(in_str)\n\n        # Since Linter.__init__() does not require a dialect to be specified,\n        # check for one now. (We're processing a string, not a file, so we're\n        # not going to pick up a .sqlfluff or other config file to provide a\n        # missing dialect at this point.)\n        config.verify_dialect_specified()\n        if not config.get(\"templater_obj\") == self.templater:\n            linter_logger.warning(\n                f\"Attempt to set templater to {config.get('templater_obj').name} \"\n                f\"failed. Using {self.templater.name} templater. Templater cannot \"\n                \"be set in a .sqlfluff file in a subdirectory of the current \"\n         "}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "python.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "se the 'sqlfluff' magic fixed context key. \"\n                        \"https://docs.sqlfluff.com/en/stable/\"\n                        \"perma/python_templating.html\".format(err)\n                    )\n                else:\n                    raise SQLTemplaterError(\n                        \"Failure in Python templating: {}. Have you configured your \"\n                        \"variables? https://docs.sqlfluff.com/en/stable/\"\n                        \"perma/variables.html\".format(err)\n                    )\n            return rendered_str\n\n        raw_sliced, sliced_file, new_str = self.slice_file(\n            in_str,\n            render_func=render_func,\n            config=config,\n        )\n        return (\n            TemplatedFile(\n                source_str=in_str,\n                templated_str=new_str,\n                fname=fname,\n                sliced_file=sliced_file,\n                raw_sliced=raw_sliced,\n            ),\n            [],\n        )\n\n    def slice_file(\n        self,\n        raw_str: str,\n        render_func: Callable[[str], str],\n        config: Optional[FluffConfig] = None,\n        append_to_templated: str = \"\",\n    ) -> tuple[list[RawFileSlice], list[TemplatedFileSlice], str]:\n        \"\"\"Slice the file to determine regions where we can fix.\"\"\"\n        templater_logger.info(\"Slicing File Template\")\n        templater_logger.debug(\"    Raw String: %r\", raw_str)\n        # Render the templated string.\n        # NOTE: This seems excessive in this simple example, but for other templating\n        # engines we need more control over the rendering so may need to call this\n        # method more than once.\n        templated_str = render_func(raw_str)\n        templater_logger.debug(\"    Templated String: %r\", templated_str)\n        # Slice the raw file\n        raw_sliced = list(self._slice_template(raw_str))\n        templater_logger.debug(\"    Raw Sliced:\")\n        for idx, raw_slice in enumerate(raw_sliced):\n            templater_logger.debug(\"        %s: %r\", i"}, {"start_line": 29000, "end_line": 31000, "belongs_to": {"file_name": "jinja.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "=(\"templating\" in config.get(\"ignore\")),\n        )\n\n        try:\n            # Slice the file once rendered.\n            raw_sliced, sliced_file, out_str = self.slice_file(\n                in_str,\n                render_func=render_func,\n                config=config,\n            )\n            return (\n                TemplatedFile(\n                    source_str=in_str,\n                    templated_str=out_str,\n                    fname=fname,\n                    sliced_file=sliced_file,\n                    raw_sliced=raw_sliced,\n                ),\n                self._generate_violations_for_undefined_variables(\n                    in_str, syntax_tree, undefined_variables\n                ),\n            )\n        except (TemplateError, TypeError) as err:\n            templater_logger.info(\"Unrecoverable Jinja Error: %s\", err, exc_info=True)\n            raise SQLTemplaterError(\n                (\n                    \"Unrecoverable failure in Jinja templating: {}. Have you \"\n                    \"correctly configured your variables? \"\n                    \"https://docs.sqlfluff.com/en/latest/perma/variables.html\"\n                ).format(err),\n                # We don't have actual line number information, but specify\n                # line 1 so users can ignore with \"noqa\" if they want. (The\n                # default is line 0, which can't be ignored because it's not\n                # a valid line number.)\n                line_no=1,\n                line_pos=1,\n            )\n\n    def slice_file(\n        self,\n        raw_str: str,\n        render_func: Callable[[str], str],\n        config: Optional[FluffConfig] = None,\n        append_to_templated: str = \"\",\n    ) -> tuple[list[RawFileSlice], list[TemplatedFileSlice], str]:\n        \"\"\"Slice the file to determine regions where we can fix.\n\n        Args:\n            raw_str (str): The raw string to be sliced.\n            render_func (Callable[[str], str]): The rendering function to be used.\n            config (optional): O"}, {"start_line": 26000, "end_line": 28000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "omplete for {phase} phase. Stability \"\n                        f\"achieved after {loop}/{loop_limit} loops.\"\n                    )\n                    break\n            else:\n                if fix:\n                    # The linter loop hit the limit before reaching a stable point\n                    # (i.e. free of lint errors). If this happens, it's usually\n                    # because one or more rules produced fixes which did not address\n                    # the original issue **or** created new issues.\n                    linter_logger.warning(\n                        f\"Loop limit on fixes reached [{loop_limit}].\"\n                    )\n\n                    # Discard any fixes for the linting errors, since they caused a\n                    # loop. IMPORTANT: By doing this, we are telling SQLFluff that\n                    # these linting errors are \"unfixable\". This is important,\n                    # because when \"sqlfluff fix\" encounters unfixable lint errors,\n                    # it exits with a \"failure\" exit code, which is exactly what we\n                    # want in this situation. (Reason: Although this is more of an\n                    # internal SQLFluff issue, users deserve to know about it,\n                    # because it means their file(s) weren't fixed.\n                    for violation in initial_linting_errors:\n                        if isinstance(violation, SQLLintError):\n                            violation.fixes = []\n\n                    # Return the original parse tree, before any fixes were applied.\n                    # Reason: When the linter hits the loop limit, the file is often\n                    # messy, e.g. some of the fixes were applied repeatedly, possibly\n                    # other weird things. We don't want the user to see this junk!\n                    return save_tree, initial_linting_errors, ignore_mask, rule_timings\n\n        if config.get(\"ignore_templated_areas\", default=True):\n            initial_linting_errors = cl"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "linted_file.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n        # It's also not the FIXED file either.\n        linter_logger.debug(\"### Templated File.\")\n        for idx, file_slice in enumerate(self.templated_file.sliced_file):\n            t_str = self.templated_file.templated_str[file_slice.templated_slice]\n            s_str = self.templated_file.source_str[file_slice.source_slice]\n            if t_str == s_str:\n                linter_logger.debug(\n                    \"    File slice: %s %r [invariant]\", idx, file_slice\n                )\n            else:\n                linter_logger.debug(\"    File slice: %s %r\", idx, file_slice)\n                linter_logger.debug(\"    \\t\\t\\ttemplated: %r\\tsource: %r\", t_str, s_str)\n\n        original_source = self.templated_file.source_str\n\n        # Generate patches from the fixed tree. In the process we sort\n        # and deduplicate them so that the resultant list is in the\n        # the right order for the source file without any duplicates.\n        filtered_source_patches = generate_source_patches(\n            self.tree, self.templated_file\n        )\n        linter_logger.debug(\"Filtered source patches:\")\n        for idx, patch in enumerate(filtered_source_patches):\n            linter_logger.debug(\"    %s: %s\", idx, patch)\n\n        # Any Template tags in the source file are off limits, unless\n        # we're explicitly fixing the source file.\n        source_only_slices = self.templated_file.source_only_slices()\n        linter_logger.debug(\"Source-only slices: %s\", source_only_slices)\n\n        # We now slice up the file using the patches and any source only slices.\n        # This gives us regions to apply changes to.\n        slice_buff = self._slice_source_file_using_patches(\n            filtered_source_patches, source_only_slices, self.templated_file.source_str\n        )\n\n        linter_logger.debug(\"Final slice buffer: %s\", slice_buff)\n\n        # Iterate through the patches, building up the new string.\n        fixed_source_string = self._build_up_fixed_source_string(\n        "}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "linted_file.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " file and refer back to the locations\n        in the original file.\n\n        NB: This is MUCH FASTER than the original approach\n        using difflib in pre 0.4.0.\n\n        There is an important distinction here between Slices and\n        Segments. A Slice is a portion of a file which is determined\n        by the templater based on which portions of the source file\n        are templated or not, and therefore before Lexing and so is\n        completely dialect agnostic. A Segment is determined by the\n        Lexer from portions of strings after templating.\n        \"\"\"\n        assert self.templated_file, \"Fixing a string requires successful templating.\"\n        linter_logger.debug(\"Original Tree: %r\", self.templated_file.templated_str)\n        assert self.tree, \"Fixing a string requires successful parsing.\"\n        linter_logger.debug(\"Fixed Tree: %r\", self.tree.raw)\n\n        # The sliced file is contiguous in the TEMPLATED space.\n        # NB: It has gaps and repeats in the source space.\n        # It's also not the FIXED file either.\n        linter_logger.debug(\"### Templated File.\")\n        for idx, file_slice in enumerate(self.templated_file.sliced_file):\n            t_str = self.templated_file.templated_str[file_slice.templated_slice]\n            s_str = self.templated_file.source_str[file_slice.source_slice]\n            if t_str == s_str:\n                linter_logger.debug(\n                    \"    File slice: %s %r [invariant]\", idx, file_slice\n                )\n            else:\n                linter_logger.debug(\"    File slice: %s %r\", idx, file_slice)\n                linter_logger.debug(\"    \\t\\t\\ttemplated: %r\\tsource: %r\", t_str, s_str)\n\n        original_source = self.templated_file.source_str\n\n        # Generate patches from the fixed tree. In the process we sort\n        # and deduplicate them so that the resultant list is in the\n        # the right order for the source file without any duplicates.\n        filtered_source_patches = generate_source_patche"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "std_roundtrip_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Round trip tests for rules with a fix method.\"\"\"\n\nimport os\nimport re\nimport shutil\nimport tempfile\nfrom io import StringIO\n\nimport pytest\nfrom click.testing import CliRunner\n\nfrom sqlfluff.cli.commands import fix, lint\n\n\ndef generic_roundtrip_test(source_file, rulestring):\n    \"\"\"Run a roundtrip test given a sql file and a rule.\n\n    We take a file buffer, lint, fix and lint, finally checking that\n    the file fails initially but not after fixing.\n    \"\"\"\n    if isinstance(source_file, str):\n        # If it's a string, treat it as a path so lets load it.\n        with open(source_file) as f:\n            source_file = StringIO(f.read())\n\n    filename = \"testing.sql\"\n    # Lets get the path of a file to use\n    tempdir_path = tempfile.mkdtemp()\n    filepath = os.path.join(tempdir_path, filename)\n    # Open the example file and write the content to it\n    with open(filepath, mode=\"w\") as dest_file:\n        for line in source_file:\n            dest_file.write(line)\n    runner = CliRunner()\n    # Check that we first detect the issue\n    result = runner.invoke(lint, [\"--rules\", rulestring, \"--dialect=ansi\", filepath])\n    assert result.exit_code == 1\n    # Fix the file (in force mode)\n    result = runner.invoke(\n        fix, [\"--rules\", rulestring, \"--dialect=ansi\", \"-f\", filepath]\n    )\n    assert result.exit_code == 0\n    # Now lint the file and check for exceptions\n    result = runner.invoke(lint, [\"--rules\", rulestring, \"--dialect=ansi\", filepath])\n    assert result.exit_code == 0\n    shutil.rmtree(tempdir_path)\n\n\ndef jinja_roundtrip_test(\n    source_path, rulestring, sqlfile=\"test.sql\", cfgfile=\".sqlfluff\"\n):\n    \"\"\"Run a roundtrip test path and rule.\n\n    We take a file buffer, lint, fix and lint, finally checking that\n    the file fails initially but not after fixing. Additionally\n    we also check that we haven't messed up the templating tags\n    in the process.\n    \"\"\"\n    tempdir_path = tempfile.mkdtemp()\n    sql_filepath = os.path.join(tempdir_path, sqlfile)"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linted_file.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the LintedFile class.\n\nThis holds linting results for a single file, and also\ncontains all of the routines to apply fixes to that file\npost linting.\n\"\"\"\n\nimport logging\nimport os\nimport shutil\nimport stat\nimport tempfile\nfrom collections import defaultdict\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass\nfrom typing import NamedTuple, Optional, Union\n\nfrom sqlfluff.core.errors import (\n    CheckTuple,\n    SQLBaseError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.linter.patch import FixPatch, generate_source_patches\n\n# Classes needed only for type checking\nfrom sqlfluff.core.parser.segments import BaseSegment\nfrom sqlfluff.core.rules.noqa import IgnoreMask\nfrom sqlfluff.core.templaters import RawFileSlice, TemplatedFile\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\nTMP_PRS_ERROR_TYPES = (SQLTemplaterError, SQLParseError)\n\n\n@dataclass\nclass FileTimings:\n    \"\"\"A dataclass for holding the timings information for a file.\"\"\"\n\n    step_timings: dict[str, float]\n    # NOTE: Because rules may run more than once for any\n    # given file we record each run and then we can post\n    # process this as we wish later.\n    rule_timings: list[tuple[str, str, float]]\n\n    def __repr__(self) -> str:  # pragma: no cover\n        return \"<FileTimings>\"\n\n    def get_rule_timing_dict(self) -> dict[str, float]:\n        \"\"\"Generate a summary to total time in each rule.\n\n        This is primarily for csv export.\n        \"\"\"\n        total_times: dict[str, float] = defaultdict(float)\n\n        for code, _, time in self.rule_timings:\n            total_times[code] += time\n\n        # Return as plain dict\n        return dict(total_times.items())\n\n\nclass LintedFile(NamedTuple):\n    \"\"\"A class to store the idea of a linted file.\"\"\"\n\n    path: str\n    violations: list[SQLBaseError]\n    timings: Optional[FileTimings]\n    tre"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "templater_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/test", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e.sql\",\n        \"ST06_test.sql\",\n    ],\n)\ndef test__dbt_templated_models_do_not_raise_lint_error(\n    project_dir,\n    fname,\n    caplog,\n    dbt_fluff_config,\n):\n    \"\"\"Test that templated dbt models do not raise a linting error.\"\"\"\n    linter = Linter(config=FluffConfig(configs=dbt_fluff_config))\n    # Log rules output.\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.rules\"):\n        lnt = linter.lint_path(\n            path=os.path.join(project_dir, \"models/my_new_project/\", fname)\n        )\n    for linted_file in lnt.files:\n        # Log the rendered file to facilitate better debugging of the files.\n        print(f\"## FILE: {linted_file.path}\")\n        print(\"\\n\\n## RENDERED FILE:\\n\\n\")\n        print(linted_file.templated_file.templated_str)\n        print(\"\\n\\n## PARSED TREE:\\n\\n\")\n        print(linted_file.tree.stringify())\n        print(\"\\n\\n## VIOLATIONS:\")\n        for idx, v in enumerate(linted_file.violations):\n            print(f\"   {idx}:{v.to_dict()}\")\n\n    violations = lnt.check_tuples()\n    assert len(violations) == 0\n\n\ndef _clean_path(glob_expression):\n    \"\"\"Clear out files matching the provided glob expression.\"\"\"\n    for fsp in glob.glob(glob_expression):\n        os.remove(fsp)\n\n\n@pytest.mark.parametrize(\n    \"path\", [\"models/my_new_project/issue_1608.sql\", \"snapshots/issue_1771.sql\"]\n)\ndef test__dbt_templated_models_fix_does_not_corrupt_file(\n    project_dir,\n    path,\n    caplog,\n    dbt_fluff_config,\n):\n    \"\"\"Test issues where previously \"sqlfluff fix\" corrupted the file.\"\"\"\n    test_glob = os.path.join(project_dir, os.path.dirname(path), \"*FIXED.sql\")\n    _clean_path(test_glob)\n    lntr = Linter(config=FluffConfig(configs=dbt_fluff_config))\n    with caplog.at_level(logging.INFO, logger=\"sqlfluff.linter\"):\n        lnt = lntr.lint_path(os.path.join(project_dir, path), fix=True)\n    try:\n        lnt.persist_changes(fixed_file_suffix=\"FIXED\")\n        with open(os.path.join(project_dir, path + \".after\")) as f:\n            comp_buff = f.read"}], "retrieved_count": 10, "cost_time": 1.285163164138794}
{"question": "Why does SQLFluff's selective rule application reduce processing overhead?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's selective rule application reduces processing overhead by only running rules that are relevant to the current context and configuration. Key benefits include: 1) Rule filtering - Only enabled rules are executed, skipping disabled or excluded rules entirely; 2) Context-aware execution - Rules are only applied to segments they're designed to handle; 3) Configuration-based selection - Rules are filtered based on user configuration (rules, exclude_rules, warnings); 4) Dialect-specific filtering - Rules that don't apply to the current dialect are skipped; 5) Performance optimization - Avoiding unnecessary rule evaluations reduces CPU usage and execution time; 6) Memory efficiency - Selective rule application reduces memory usage by not loading unused rule logic; 7) Scalability - Performance improves as the number of available rules increases; 8) Customization - Users can focus on specific rule categories without running all rules; 9) Incremental processing - Only relevant rules are re-evaluated when configuration changes; 10) Resource management - Better resource utilization by avoiding redundant rule processing.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Implements the base rule class.\n\nRules crawl through the trees returned by the parser and evaluate particular\nrules.\n\nThe intent is that it should be possible for the rules to be expressed\nas simply as possible, with as much of the complexity abstracted away.\n\nThe evaluation function should take enough arguments that it can evaluate\nthe position of the given segment in relation to its neighbors, and that\nthe segment which finally \"triggers\" the error, should be the one that would\nbe corrected OR if the rule relates to something that is missing, then it\nshould flag on the segment FOLLOWING, the place that the desired element is\nmissing.\n\"\"\"\n\nimport bdb\nimport copy\nimport fnmatch\nimport logging\nimport pathlib\nimport re\nfrom collections import defaultdict, namedtuple\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    ClassVar,\n    DefaultDict,\n    Optional,\n    Union,\n)\n\nimport regex\n\nfrom sqlfluff.core.errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "t import LintingResult\nfrom sqlfluff.core.parser import Lexer, Parser\nfrom sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\nfrom sqlfluff.core.rules import BaseRule, RulePack, get_ruleset\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.rules.noqa import IgnoreMask\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.meta import MetaSegment\n    from sqlfluff.core.templaters import RawTemplater, TemplatedFile\n\n\nRuleTimingsType = list[tuple[str, str, float]]\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass Linter:\n    \"\"\"The interface class to interact with the linter.\"\"\"\n\n    # Default to allowing process parallelism\n    allow_process_parallelism = True\n\n    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n        dialect: Optional[str] = None,\n        rules: Optional[list[str]] = None,\n        user_rules: Optional[list[type[BaseRule]]] = None,\n        exclude_rules: Optional[list[str]] = None,\n    ) -> None:\n        if config and (dialect or rules or exclude_rules):\n            raise ValueError(  # pragma: no cover\n                \"Linter does not support setting both `config` and any of \"\n                \"`dialect`, `rules` or `exclude_rules`. The latter are \"\n                \"provided as convenience methods to avoid needing to \"\n                \"set the `config` object. If using `config`, please \"\n                \"provide all the other values within that object.\"\n            )\n        # Use the provided config or create one from the kwargs.\n        self.config = config or FluffConfig.from_kwargs(\n            dialect=dialect,\n            rules=rules,\n            exclude_rules=exclude_rules,\n            # Don't require a dialect to be provided yet. Defer this until we\n            # are actually linting something, since the dir"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`Lin"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "raw_file_slices.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/utils/functional", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Surrogate class for working with RawFileSlice collections.\"\"\"\n\nfrom typing import Callable, Optional\n\nfrom sqlfluff.core.templaters.base import RawFileSlice, TemplatedFile\n\n\nclass RawFileSlices(tuple[RawFileSlice, ...]):\n    \"\"\"Encapsulates a sequence of one or more RawFileSlice.\n\n    The slices may or may not be contiguous in a file.\n    Provides useful operations on a sequence of slices to simplify rule creation.\n    \"\"\"\n\n    def __new__(\n        cls, *raw_slices: RawFileSlice, templated_file: Optional[TemplatedFile] = None\n    ) -> \"RawFileSlices\":\n        \"\"\"Override new operator.\"\"\"\n        return super().__new__(cls, raw_slices)\n\n    def __init__(self, *_: RawFileSlice, templated_file: TemplatedFile):\n        self.templated_file = templated_file\n\n    def all(self, predicate: Optional[Callable[[RawFileSlice], bool]] = None) -> bool:\n        \"\"\"Do all the raw slices match?\"\"\"\n        for s in self:\n            if predicate is not None and not predicate(s):\n                return False\n        return True\n\n    def any(self, predicate: Optional[Callable[[RawFileSlice], bool]] = None) -> bool:\n        \"\"\"Do any of the raw slices match?\"\"\"\n        for s in self:\n            if predicate is None or predicate(s):\n                return True\n        return False\n\n    def select(\n        self,\n        select_if: Optional[Callable[[RawFileSlice], bool]] = None,\n        loop_while: Optional[Callable[[RawFileSlice], bool]] = None,\n        start_slice: Optional[RawFileSlice] = None,\n        stop_slice: Optional[RawFileSlice] = None,\n    ) -> \"RawFileSlices\":\n        \"\"\"Retrieve range/subset.\n\n        NOTE: Iterates the slices BETWEEN start_slice and stop_slice, i.e. those\n        slices are not included in the loop.\n        \"\"\"\n        start_index = self.index(start_slice) if start_slice else -1\n        stop_index = self.index(stop_slice) if stop_slice else len(self)\n        buff = []\n        for slice_ in self[start_index + 1 : stop_index]:\n            if loop_while is "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "LT09.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ldcard_policy\"]\n    crawl_behaviour = SegmentSeekerCrawler({\"select_clause\"})\n    is_fix_compatible = True\n\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        self.wildcard_policy: str\n        assert context.segment.is_type(\"select_clause\")\n        select_targets_info = self._get_indexes(context)\n        select_clause = FunctionalContext(context).segment\n        wildcards = select_clause.children(\n            sp.is_type(\"select_clause_element\")\n        ).children(sp.is_type(\"wildcard_expression\"))\n        has_wildcard = bool(wildcards)\n        if len(select_targets_info.select_targets) == 1 and (\n            not has_wildcard or self.wildcard_policy == \"single\"\n        ):\n            return self._eval_single_select_target_element(\n                select_targets_info,\n                context,\n            )\n        elif len(select_targets_info.select_targets):\n            return self._eval_multiple_select_target_elements(\n                select_targets_info, context.segment\n            )\n        return None\n\n    @staticmethod\n    def _get_indexes(context: RuleContext) -> SelectTargetsInfo:\n        children = FunctionalContext(context).segment.children()\n        select_targets = children.select(sp.is_type(\"select_clause_element\"))\n        first_select_target_idx = children.find(select_targets.get())\n        selects = children.select(sp.is_keyword(\"select\"))\n        select_idx = children.find(selects.get()) if selects else -1\n        newlines = children.select(sp.is_type(\"newline\"))\n        first_new_line_idx = children.find(newlines.get()) if newlines else -1\n        comment_after_select_idx = -1\n        if newlines:\n            comment_after_select = children.select(\n                sp.is_type(\"comment\"),\n                start_seg=selects.get(),\n                stop_seg=newlines.get(),\n                loop_while=sp.or_(\n                    sp.is_type(\"comment\"), sp.is_type(\"whitespace\"), sp.is_meta()\n                ),\n            )\n            "}, {"start_line": 1000, "end_line": 2820, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e that currently, the gap _before_ and _after_ the semicolon \"\n                \"is considered 'between' statements.\"\n            ),\n        },\n        \"maximum_empty_lines_inside_statements\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The maximum number of empty lines allowed inside statements.\"\n            ),\n        },\n        \"wildcard_policy\": {\n            \"validation\": [\"single\", \"multiple\"],\n            \"definition\": \"Treatment of wildcards. Defaults to ``single``.\",\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.layout.LT01 import Rule_LT01\n    from sqlfluff.rules.layout.LT02 import Rule_LT02\n    from sqlfluff.rules.layout.LT03 import Rule_LT03\n    from sqlfluff.rules.layout.LT04 import Rule_LT04\n    from sqlfluff.rules.layout.LT05 import Rule_LT05\n    from sqlfluff.rules.layout.LT06 import Rule_LT06\n    from sqlfluff.rules.layout.LT07 import Rule_LT07\n    from sqlfluff.rules.layout.LT08 import Rule_LT08\n    from sqlfluff.rules.layout.LT09 import Rule_LT09\n    from sqlfluff.rules.layout.LT10 import Rule_LT10\n    from sqlfluff.rules.layout.LT11 import Rule_LT11\n    from sqlfluff.rules.layout.LT12 import Rule_LT12\n    from sqlfluff.rules.layout.LT13 import Rule_LT13\n    from sqlfluff.rules.layout.LT14 import Rule_LT14\n    from sqlfluff.rules.layout.LT15 import Rule_LT15\n\n    return [\n        Rule_LT01,\n        Rule_LT02,\n        Rule_LT03,\n        Rule_LT04,\n        Rule_LT05,\n        Rule_LT06,\n        Rule_LT07,\n        Rule_LT08,\n        Rule_LT09,\n        Rule_LT10,\n        Rule_LT11,\n        Rule_LT12,\n        Rule_LT13,\n        Rule_LT14,\n        Rule_LT15,\n    ]\n"}, {"start_line": 0, "end_line": 1349, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Configuration and examples for individual rules.\"\"\"\n\nfrom sqlfluff.core.plugin.host import get_plugin_manager\nfrom sqlfluff.core.rules.base import (\n    BaseRule,\n    EvalResultType,\n    LintResult,\n    RuleGhost,\n    RulePack,\n    RuleSet,\n)\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.fix import LintFix\n\n\ndef _load_standard_rules() -> RuleSet:\n    \"\"\"Initialise the standard ruleset.\n\n    We do this on each call so that dynamic rules changes\n    are possible.\n    \"\"\"\n    std_rule_set = RuleSet(name=\"standard\", config_info=get_config_info())\n\n    # Iterate through the rules list and register each rule with the standard set.\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            std_rule_set.register(rule)\n\n    return std_rule_set\n\n\ndef get_ruleset(name: str = \"standard\") -> RuleSet:\n    \"\"\"Get a ruleset by name.\"\"\"\n    std_rules = _load_standard_rules()\n    lookup = {std_rules.name: std_rules}\n    # Return a copy in case someone modifies the register.\n    return lookup[name].copy()\n\n\n__all__ = (\n    \"get_ruleset\",\n    \"RuleSet\",\n    \"RulePack\",\n    \"BaseRule\",\n    \"LintResult\",\n    \"LintFix\",\n    \"RuleContext\",\n    \"RuleGhost\",\n    \"EvalResultType\",\n    \"ConfigInfo\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "rules_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the standard set of rules.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import Linter\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.linter import RuleTuple\nfrom sqlfluff.core.parser import WhitespaceSegment\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.rules import BaseRule, LintFix, LintResult, get_ruleset\nfrom sqlfluff.core.rules.crawlers import RootOnlyCrawler, SegmentSeekerCrawler\nfrom sqlfluff.core.rules.doc_decorators import (\n    document_configuration,\n    document_fix_compatible,\n    document_groups,\n)\nfrom sqlfluff.core.rules.loader import get_rules_from_path\nfrom sqlfluff.core.templaters.base import TemplatedFile\nfrom sqlfluff.utils.testing.logging import fluff_log_catcher\nfrom sqlfluff.utils.testing.rules import get_rule_from_set\nfrom test.fixtures.rules.custom.L000 import Rule_L000\nfrom test.fixtures.rules.custom.S000 import Rule_S000\n\n\nclass Rule_T042(BaseRule):\n    \"\"\"A dummy rule.\"\"\"\n\n    groups = (\"all\",)\n\n    def _eval(self, context):\n        pass\n\n\nclass Rule_T001(BaseRule):\n    \"\"\"A deliberately malicious rule.\n\n    **Anti-pattern**\n\n    Blah blah\n    \"\"\"\n\n    groups = (\"all\",)\n    crawl_behaviour = SegmentSeekerCrawler({\"whitespace\"})\n    is_fix_compatible = True\n\n    def _eval(self, context):\n        \"\"\"Stars make newlines.\"\"\"\n        if context.segment.is_type(\"whitespace\"):\n            return LintResult(\n                anchor=context.segment,\n                fixes=[\n                    LintFix.replace(\n                        context.segment, [WhitespaceSegment(context.segment.raw + \" \")]\n                    )\n                ],\n            )\n\n\nclass Rule_T002(BaseRule):\n    \"\"\"A rule which says all raw code segments are bad.\n\n    This is used for testing unparsable code.\n    \"\"\"\n\n    groups = (\"all\",)\n    # Root only crawler so that the in-rule filters don't kick in.\n    crawl_behaviour = RootOnlyCrawler()\n\n    def _eval(s"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "templated_file_slices.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/utils/functional", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Surrogate class for working with TemplatedFileSlice collections.\"\"\"\n\nfrom typing import Callable, Optional\n\nfrom sqlfluff.core.templaters.base import TemplatedFile, TemplatedFileSlice\n\n\nclass TemplatedFileSlices(tuple[TemplatedFileSlice, ...]):\n    \"\"\"Encapsulates a sequence of one or more TemplatedFileSlice.\n\n    The slices may or may not be contiguous in a file.\n    Provides useful operations on a sequence of slices to simplify rule creation.\n    \"\"\"\n\n    def __new__(\n        cls,\n        *templated_slices: TemplatedFileSlice,\n        templated_file: Optional[TemplatedFile] = None,\n    ) -> \"TemplatedFileSlices\":\n        \"\"\"Override new operator.\"\"\"\n        return super().__new__(cls, templated_slices)\n\n    def __init__(self, *_: TemplatedFileSlice, templated_file: TemplatedFile) -> None:\n        self.templated_file = templated_file\n\n    def all(\n        self, predicate: Optional[Callable[[TemplatedFileSlice], bool]] = None\n    ) -> bool:\n        \"\"\"Do all the templated slices match?\"\"\"\n        for s in self:\n            if predicate is not None and not predicate(s):\n                return False\n        return True\n\n    def any(\n        self, predicate: Optional[Callable[[TemplatedFileSlice], bool]] = None\n    ) -> bool:  # pragma: no cover\n        \"\"\"Do any of the templated slices match?\"\"\"\n        for s in self:\n            if predicate is None or predicate(s):\n                return True\n        return False\n\n    def select(\n        self,\n        select_if: Optional[Callable[[TemplatedFileSlice], bool]] = None,\n        loop_while: Optional[Callable[[TemplatedFileSlice], bool]] = None,\n        start_slice: Optional[TemplatedFileSlice] = None,\n        stop_slice: Optional[TemplatedFileSlice] = None,\n    ) -> \"TemplatedFileSlices\":  # pragma: no cover\n        \"\"\"Retrieve range/subset.\n\n        NOTE: Iterates the slices BETWEEN start_slice and stop_slice, i.e. those\n        slices are not included in the loop.\n        \"\"\"\n        start_index = self.index(start_s"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "crawlers.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Definitions of crawlers.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Iterator\nfrom typing import Any, cast\n\nfrom sqlfluff.core.parser.segments.base import BaseSegment\nfrom sqlfluff.core.parser.segments.raw import RawSegment\nfrom sqlfluff.core.rules.context import RuleContext\n\n\nclass BaseCrawler(ABC):\n    \"\"\"The base interface for crawler classes.\"\"\"\n\n    def __init__(self, works_on_unparsable: bool = False, **kwargs: Any) -> None:\n        self.works_on_unparsable = works_on_unparsable\n\n    def passes_filter(self, segment: BaseSegment) -> bool:\n        \"\"\"Returns true if this segment considered at all.\n\n        This method is called during crawling but also\n        in evaluating the anchors for linting violations\n        and their fixes to make sure we don't get issues\n        with linting sections of queries that we can't\n        parse.\n\n        See `BaseRule._process_lint_result()`.\n        \"\"\"\n        return self.works_on_unparsable or not segment.is_type(\"unparsable\")\n\n    @abstractmethod\n    def crawl(self, context: RuleContext) -> Iterator[RuleContext]:\n        \"\"\"Yields a RuleContext for each segment the rule should process.\"\"\"\n\n\nclass RootOnlyCrawler(BaseCrawler):\n    \"\"\"A crawler that doesn't crawl.\n\n    This just yields one context on the root-level (topmost) segment of the file.\n    \"\"\"\n\n    def crawl(self, context: RuleContext) -> Iterator[RuleContext]:\n        \"\"\"Yields a RuleContext for each segment the rule should process.\"\"\"\n        if self.passes_filter(context.segment):\n            yield context\n\n\nclass SegmentSeekerCrawler(BaseCrawler):\n    \"\"\"A crawler that efficiently searches for specific segment types.\n\n    The segment type(s) are specified on creation.\n    \"\"\"\n\n    def __init__(\n        self,\n        types: set[str],\n        provide_raw_stack: bool = False,\n        allow_recurse: bool = True,\n        **kwargs: Any,\n    ) -> None:\n        self.types = types\n        # Tracking a raw stack involves a lot of tuple manipu"}], "retrieved_count": 10, "cost_time": 1.2877492904663086}
{"question": "Where does SQLFluff implement its fix generation logic?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff implements its fix generation logic across several key modules in the codebase. The main fix generation components are located in: 1) src/sqlfluff/core/rules/fix.py - Contains the LintFix class and fix generation utilities; 2) src/sqlfluff/core/linter/fix.py - Implements fix application logic and patch generation; 3) src/sqlfluff/core/linter/patch.py - Contains FixPatch class for managing code patches; 4) src/sqlfluff/core/rules/base.py - Contains fix-related methods in BaseRule class; 5) src/sqlfluff/core/linter/linted_file.py - Implements fix application to files; 6) src/sqlfluff/core/parser/segments/base.py - Contains segment manipulation methods for fixes; 7) src/sqlfluff/core/parser/segments/raw.py - Implements raw segment fix handling; 8) src/sqlfluff/core/linter/linter.py - Orchestrates fix generation and application; 9) src/sqlfluff/core/rules/context.py - Provides context for fix generation; 10) src/sqlfluff/core/parser/helpers.py - Contains utilities for segment manipulation during fixes.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "patch.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Helpers for generating patches to fix files.\"\"\"\n\nimport logging\nfrom collections.abc import Iterator\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nfrom sqlfluff.core.parser import BaseSegment\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.templaters import TemplatedFile\n\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\n\n@dataclass\nclass FixPatch:\n    \"\"\"An edit patch for a source file.\"\"\"\n\n    templated_slice: slice\n    fixed_raw: str\n    # The patch category, functions mostly for debugging and explanation\n    # than for function. It allows traceability of *why* this patch was\n    # generated. It has no significance for processing.\n    patch_category: str\n    source_slice: slice\n    templated_str: str\n    source_str: str\n\n    def dedupe_tuple(self) -> tuple[slice, str]:\n        \"\"\"Generate a tuple of this fix for deduping.\"\"\"\n        return (self.source_slice, self.fixed_raw)\n\n\ndef _iter_source_fix_patches(\n    segment: BaseSegment, templated_file: TemplatedFile\n) -> Iterator[FixPatch]:\n    \"\"\"Yield any source patches as fixes now.\n\n    NOTE: This yields source fixes for the segment and any of its\n    children, so it's important to call it at the right point in\n    the recursion to avoid yielding duplicates.\n    \"\"\"\n    for source_fix in segment.source_fixes:\n        yield FixPatch(\n            source_fix.templated_slice,\n            source_fix.edit,\n            patch_category=\"source\",\n            source_slice=source_fix.source_slice,\n            templated_str=templated_file.templated_str[source_fix.templated_slice],\n            source_str=templated_file.source_str[source_fix.source_slice],\n        )\n\n\ndef _iter_templated_patches(\n    segment: BaseSegment, templated_file: TemplatedFile\n) -> Iterator[FixPatch]:\n    \"\"\"Iterate through the segments generating fix patches.\n\n    The patches are generated in TEMPLATED space. This is important\n    so that we defer dealing with any loops until later. At this stage\n    ev"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fix.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Helper classes & methods for applying fixes to segments.\"\"\"\n\nimport logging\nfrom collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlfluff.core.parser import BaseSegment, SourceFix\nfrom sqlfluff.core.rules.fix import LintFix\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n\n\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\n\n@dataclass\nclass AnchorEditInfo:\n    \"\"\"For a given fix anchor, count of the fix edit types and fixes for it.\"\"\"\n\n    delete: int = field(default=0)\n    replace: int = field(default=0)\n    create_before: int = field(default=0)\n    create_after: int = field(default=0)\n    fixes: list[\"LintFix\"] = field(default_factory=list)\n    source_fixes: list[SourceFix] = field(default_factory=list)\n    # First fix of edit_type \"replace\" in \"fixes\"\n    _first_replace: Optional[\"LintFix\"] = field(default=None)\n\n    def add(self, fix: \"LintFix\") -> None:\n        \"\"\"Adds the fix and updates stats.\n\n        We also allow potentially multiple source fixes on the same\n        anchor by condensing them together here.\n        \"\"\"\n        if fix in self.fixes:\n            # Deduplicate fixes in case it's already in there.\n            return\n\n        if fix.is_just_source_edit():\n            assert fix.edit\n            # is_just_source_edit confirms there will be a list\n            # so we can hint that to mypy.\n            self.source_fixes += fix.edit[0].source_fixes\n            # is there already a replace?\n            if self._first_replace:\n                assert self._first_replace.edit\n                # is_just_source_edit confirms there will be a list\n                # and that's the only way to get into _first_replace\n                # if it's populated so we can hint that to mypy.\n                linter_logger.info(\n                    \"Multiple edits detected, condensing %s onto %s\",\n                    fix,\n                    self._first_repl"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fix.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the LintFix class, returned by rules when recommending a fix.\"\"\"\n\nimport logging\nfrom collections.abc import Iterable, Sized\nfrom itertools import chain\nfrom typing import Any, Optional, cast\n\nfrom sqlfluff.core.parser import BaseSegment, PositionMarker, RawSegment, SourceFix\nfrom sqlfluff.core.templaters import RawFileSlice, TemplatedFile\n\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\n\nclass LintFix:\n    \"\"\"A class to hold a potential fix to a linting violation.\n\n    Args:\n        edit_type (:obj:`str`): One of `create_before`, `create_after`,\n            `replace`, `delete` to indicate the kind of fix this represents.\n        anchor (:obj:`BaseSegment`): A segment which represents\n            the *position* that this fix should be applied at. For deletions\n            it represents the segment to delete, for creations it implies the\n            position to create at (with the existing element at this position\n            to be moved *after* the edit), for a `replace` it implies the\n            segment to be replaced.\n        edit (iterable of :obj:`BaseSegment`, optional): For `replace` and\n            `create` fixes, this holds the iterable of segments to create\n            or replace at the given `anchor` point.\n        source (iterable of :obj:`BaseSegment`, optional): For `replace` and\n            `create` fixes, this holds iterable of segments that provided\n            code. IMPORTANT: The linter uses this to prevent copying material\n            from templated areas.\n    \"\"\"\n\n    def __init__(\n        self,\n        edit_type: str,\n        anchor: BaseSegment,\n        edit: Optional[Iterable[BaseSegment]] = None,\n        source: Optional[Iterable[BaseSegment]] = None,\n    ) -> None:\n        if edit_type not in (\n            \"create_before\",\n            \"create_after\",\n            \"replace\",\n            \"delete\",\n        ):  # pragma: no cover\n            raise ValueError(f\"Unexpected edit_type: {edit_type}\")\n        self.edit_type = edit_type\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fix_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test routines for fixing errors.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core.linter.fix import compute_anchor_edit_info\nfrom sqlfluff.core.linter.patch import FixPatch, generate_source_patches\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments import (\n    BaseSegment,\n    RawSegment,\n    TemplateSegment,\n)\nfrom sqlfluff.core.parser.segments.raw import SourceFix\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters import RawFileSlice, TemplatedFile\nfrom sqlfluff.core.templaters.base import TemplatedFileSlice\n\n\n@pytest.fixture(scope=\"module\")\ndef raw_segments(generate_test_segments):\n    \"\"\"Construct a list of raw segments as a fixture.\"\"\"\n    return generate_test_segments([\"foobar\", \".barfoo\"])\n\n\ndef test__rules_base_segments_compute_anchor_edit_info(raw_segments):\n    \"\"\"Test BaseSegment.compute_anchor_edit_info().\"\"\"\n    # Construct a fix buffer, intentionally with:\n    # - one duplicate.\n    # - two different incompatible fixes on the same segment.\n    fixes = [\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"a\")]),\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"a\")]),\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"b\")]),\n    ]\n    anchor_info_dict = compute_anchor_edit_info(fixes)\n    # Check the target segment is the only key we have.\n    assert list(anchor_info_dict.keys()) == [raw_segments[0].uuid]\n    anchor_info = anchor_info_dict[raw_segments[0].uuid]\n    # Check that the duplicate as been deduplicated.\n    # i.e. this isn't 3.\n    assert anchor_info.replace == 2\n    # Check the fixes themselves.\n    # NOTE: There's no duplicated first fix.\n    assert anchor_info.fixes == [\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"a\")]),\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"b\")]),\n    ]\n    # Check the first replace\n    assert anchor_info._first_replace == LintFix.replace(\n        r"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "re.parser.matchable import Matchable\nfrom sqlfluff.core.parser.types import SimpleHintType\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.raw import RawSegment\n\n# Instantiate the linter logger (only for use in methods involved with fixing.)\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\nTupleSerialisedSegment = tuple[str, Union[str, tuple[\"TupleSerialisedSegment\", ...]]]\nRecordSerialisedSegment = dict[\n    str, Union[None, str, \"RecordSerialisedSegment\", list[\"RecordSerialisedSegment\"]]\n]\n\n\n@dataclass(frozen=True)\nclass SourceFix:\n    \"\"\"A stored reference to a fix in the non-templated file.\"\"\"\n\n    edit: str\n    source_slice: slice\n    # TODO: It might be possible to refactor this to not require\n    # a templated_slice (because in theory it's unnecessary).\n    # However much of the fix handling code assumes we need\n    # a position in the templated file to interpret it.\n    # More work required to achieve that if desired.\n    templated_slice: slice\n\n    def __hash__(self) -> int:\n        # Only hash based on the source slice, not the\n        # templated slice (which might change)\n        return hash((self.edit, self.source_slice.start, self.source_slice.stop))\n\n\n@dataclass(frozen=True)\nclass PathStep:\n    \"\"\"An element of the response to BaseSegment.path_to().\n\n    Attributes:\n        segment (:obj:`BaseSegment`): The segment in the chain.\n        idx (int): The index of the target within its `segment`.\n        len (int): The number of children `segment` has.\n        code_idxs (:obj:`tuple` of int): The indices which contain code.\n    \"\"\"\n\n    segment: BaseSegment\n    idx: int\n    len: int\n    code_idxs: tuple[int, ...]\n\n\ndef _iter_base_types(\n    new_type: Optional[str], bases: tuple[type[BaseSegment]]\n) -> Iterator[str]:\n    \"\"\"Iterate types for a new segment class.\n\n    This is a helper method used within in the construction of\n    SegmentMetaclass so that we can construct a fro"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linted_file_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests covering the LintedFile class and it's methods.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core.linter import LintedFile\nfrom sqlfluff.core.linter.patch import FixPatch\nfrom sqlfluff.core.templaters import RawFileSlice\n\n\n@pytest.mark.parametrize(\n    \"source_slices,source_patches,raw_source_string,expected_result\",\n    # NOTE: For all of these examples we're not setting the patch_category\n    # of the fix patches. They're not used at this step so irrelevant for\n    # testing.\n    [\n        # Trivial example\n        ([slice(0, 1)], [], \"a\", \"a\"),\n        # Simple replacement\n        (\n            [slice(0, 1), slice(1, 2), slice(2, 3)],\n            [FixPatch(slice(1, 2), \"d\", \"\", slice(1, 2), \"b\", \"b\")],\n            \"abc\",\n            \"adc\",\n        ),\n        # Simple insertion\n        (\n            [slice(0, 1), slice(1, 1), slice(1, 2)],\n            [FixPatch(slice(1, 1), \"b\", \"\", slice(1, 1), \"\", \"\")],\n            \"ac\",\n            \"abc\",\n        ),\n        # Simple deletion\n        (\n            [slice(0, 1), slice(1, 2), slice(2, 3)],\n            [FixPatch(slice(1, 2), \"\", \"\", slice(1, 2), \"b\", \"b\")],\n            \"abc\",\n            \"ac\",\n        ),\n        # Illustrative templated example (although practically at\n        # this step, the routine shouldn't care if it's templated).\n        (\n            [slice(0, 2), slice(2, 7), slice(7, 9)],\n            [FixPatch(slice(2, 3), \"{{ b }}\", \"\", slice(2, 7), \"b\", \"{{b}}\")],\n            \"a {{b}} c\",\n            \"a {{ b }} c\",\n        ),\n    ],\n)\ndef test__linted_file__build_up_fixed_source_string(\n    source_slices, source_patches, raw_source_string, expected_result, caplog\n):\n    \"\"\"Test _build_up_fixed_source_string.\n\n    This is part of fix_string().\n    \"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.linter\"):\n        result = LintedFile._build_up_fixed_source_string(\n            source_slices, source_patches, raw_source_string\n        )\n    assert result == expected_result\n\n\n@pytest.m"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linted_file.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the LintedFile class.\n\nThis holds linting results for a single file, and also\ncontains all of the routines to apply fixes to that file\npost linting.\n\"\"\"\n\nimport logging\nimport os\nimport shutil\nimport stat\nimport tempfile\nfrom collections import defaultdict\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass\nfrom typing import NamedTuple, Optional, Union\n\nfrom sqlfluff.core.errors import (\n    CheckTuple,\n    SQLBaseError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.linter.patch import FixPatch, generate_source_patches\n\n# Classes needed only for type checking\nfrom sqlfluff.core.parser.segments import BaseSegment\nfrom sqlfluff.core.rules.noqa import IgnoreMask\nfrom sqlfluff.core.templaters import RawFileSlice, TemplatedFile\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\nTMP_PRS_ERROR_TYPES = (SQLTemplaterError, SQLParseError)\n\n\n@dataclass\nclass FileTimings:\n    \"\"\"A dataclass for holding the timings information for a file.\"\"\"\n\n    step_timings: dict[str, float]\n    # NOTE: Because rules may run more than once for any\n    # given file we record each run and then we can post\n    # process this as we wish later.\n    rule_timings: list[tuple[str, str, float]]\n\n    def __repr__(self) -> str:  # pragma: no cover\n        return \"<FileTimings>\"\n\n    def get_rule_timing_dict(self) -> dict[str, float]:\n        \"\"\"Generate a summary to total time in each rule.\n\n        This is primarily for csv export.\n        \"\"\"\n        total_times: dict[str, float] = defaultdict(float)\n\n        for code, _, time in self.rule_timings:\n            total_times[code] += time\n\n        # Return as plain dict\n        return dict(total_times.items())\n\n\nclass LintedFile(NamedTuple):\n    \"\"\"A class to store the idea of a linted file.\"\"\"\n\n    path: str\n    violations: list[SQLBaseError]\n    timings: Optional[FileTimings]\n    tre"}, {"start_line": 36000, "end_line": 38000, "belongs_to": {"file_name": "commands.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/cli", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "=True,\n    help=(\n        \"[DEPRECATED - From 3.0 onward this is the default behaviour] \"\n        \"Apply fixes will also be applied file by file, during the \"\n        \"linting process, rather than waiting until all files are \"\n        \"linted before fixing.\"\n    ),\n)\n@click.option(\n    \"--check\",\n    is_flag=True,\n    help=(\n        \"Analyse all files and ask for confirmation before applying \"\n        \"any fixes. Fixes will be applied all together at the end of \"\n        \"the operation.\"\n    ),\n)\n@click.option(\n    \"-q\",\n    \"--quiet\",\n    is_flag=True,\n    help=(\n        \"Reduces the amount of output to stdout to a minimal level. \"\n        \"This is effectively the opposite of -v. NOTE: It will only \"\n        \"take effect if -f/--force is also set.\"\n    ),\n)\n@click.option(\n    \"-x\",\n    \"--fixed-suffix\",\n    default=None,\n    help=\"An optional suffix to add to fixed files.\",\n)\n@click.option(\n    \"--FIX-EVEN-UNPARSABLE\",\n    is_flag=True,\n    default=None,\n    help=(\n        \"Enables fixing of files that have templating or parse errors. \"\n        \"Note that the similar-sounding '--ignore' or 'noqa' features merely \"\n        \"prevent errors from being *displayed*. For safety reasons, the 'fix'\"\n        \"command will not make any fixes in files that have templating or parse \"\n        \"errors unless '--FIX-EVEN-UNPARSABLE' is enabled on the command line\"\n        \"or in the .sqlfluff config file.\"\n    ),\n)\n@click.option(\n    \"--show-lint-violations\",\n    is_flag=True,\n    help=\"Show lint violations\",\n)\n@click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\ndef fix(\n    force: bool,\n    paths: tuple[str],\n    disregard_sqlfluffignores: bool,\n    check: bool = False,\n    bench: bool = False,\n    quiet: bool = False,\n    fixed_suffix: str = \"\",\n    logger: Optional[logging.Logger] = None,\n    processes: Optional[int] = None,\n    disable_progress_bar: Optional[bool] = False,\n    persist_timing: Optional[str] = None,\n    extra_config_path: Optional[str] = None,\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the linter class.\"\"\"\n\nimport fnmatch\nimport logging\nimport os\nimport time\nfrom collections.abc import Iterator, Sequence\nfrom typing import TYPE_CHECKING, Optional, cast\n\nimport regex\nfrom tqdm import tqdm\n\nfrom sqlfluff.core.config import FluffConfig, progress_bar_configuration\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLFluffSkipFile,\n    SQLLexError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.helpers.file import get_encoding\nfrom sqlfluff.core.linter.common import (\n    ParsedString,\n    ParsedVariant,\n    RenderedFile,\n    RuleTuple,\n)\nfrom sqlfluff.core.linter.discovery import paths_from_path\nfrom sqlfluff.core.linter.fix import apply_fixes, compute_anchor_edit_info\nfrom sqlfluff.core.linter.linted_dir import LintedDir\nfrom sqlfluff.core.linter.linted_file import (\n    TMP_PRS_ERROR_TYPES,\n    FileTimings,\n    LintedFile,\n)\nfrom sqlfluff.core.linter.linting_result import LintingResult\nfrom sqlfluff.core.parser import Lexer, Parser\nfrom sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\nfrom sqlfluff.core.rules import BaseRule, RulePack, get_ruleset\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.rules.noqa import IgnoreMask\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.meta import MetaSegment\n    from sqlfluff.core.templaters import RawTemplater, TemplatedFile\n\n\nRuleTimingsType = list[tuple[str, str, float]]\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass Linter:\n    \"\"\"The interface class to interact with the linter.\"\"\"\n\n    # Default to allowing process parallelism\n    allow_process_parallelism = True\n\n    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n        dialect: Optional[str] = None,\n  "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Base segment definitions.\n\nHere we define:\n- BaseSegment. This is the root class for all segments, and is\n  designed to hold other subsegments.\n- UnparsableSegment. A special wrapper to indicate that the parse\n  function failed on this block of segments and to prevent further\n  analysis.\n\"\"\"\n\n# Import annotations for py 3.7 to allow `weakref.Referencetype[\"BaseSegment\"]`\nfrom __future__ import annotations\n\nimport logging\nimport weakref\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom functools import cached_property\nfrom io import StringIO\nfrom itertools import chain\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    ClassVar,\n    Optional,\n    Union,\n    cast,\n)\nfrom uuid import uuid4\n\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import trim_non_code_segments\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.types import SimpleHintType\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.raw import RawSegment\n\n# Instantiate the linter logger (only for use in methods involved with fixing.)\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\nTupleSerialisedSegment = tuple[str, Union[str, tuple[\"TupleSerialisedSegment\", ...]]]\nRecordSerialisedSegment = dict[\n    str, Union[None, str, \"RecordSerialisedSegment\", list[\"RecordSerialisedSegment\"]]\n]\n\n\n@dataclass(frozen=True)\nclass SourceFix:\n    \"\"\"A stored reference to a fix in the non-templated file.\"\"\"\n\n    edit: str\n    source_slice: slice\n    # TODO: It might be possible to refactor this to not require\n    # a templated_slice (because in theory it's unnecessary).\n    # However much of the fix handling code assumes we need\n    # a position in the templated file to interpret it.\n    # More work required to ach"}], "retrieved_count": 10, "cost_time": 0.3427770137786865}
{"question": "Where in SQLFluff's codebase is the \"parse\" method defined?", "answer": null, "relative_code_list": null, "ground_truth": "The \"parse\" method in SQLFluff is defined in several key locations in the codebase. The main parse method implementations are located in: 1) src/sqlfluff/core/parser/parser.py - Contains the main Parser.parse() method that orchestrates the parsing process; 2) src/sqlfluff/core/linter/linter.py - Contains Linter.parse_string() and Linter.parse_path() methods for parsing SQL strings and files; 3) src/sqlfluff/core/parser/segments/base.py - Contains BaseSegment.parse() method for parsing individual segments; 4) src/sqlfluff/core/parser/segments/file.py - Contains BaseFileSegment.root_parse() method for parsing complete files; 5) src/sqlfluff/core/parser/grammar.py - Contains grammar parsing methods for different grammar types; 6) src/sqlfluff/core/parser/parsers.py - Contains parser classes with parse() methods for individual segment types; 7) src/sqlfluff/core/linter/linter.py - Contains _parse_tokens() static method for parsing lexed tokens; 8) src/sqlfluff/core/parser/context.py - Contains parsing context management for the parse process; 9) src/sqlfluff/core/parser/match_result.py - Contains methods for handling parse results; 10) src/sqlfluff/api/simple.py - Contains parse() function for the simple API interface.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parser.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the Parser class.\"\"\"\n\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import check_still_complete\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.parser.segments import BaseFileSegment, BaseSegment\n\n\nclass Parser:\n    \"\"\"Instantiates parsed queries from a sequence of lexed raw segments.\"\"\"\n\n    def __init__(\n        self, config: Optional[FluffConfig] = None, dialect: Optional[str] = None\n    ):\n        if config and dialect:\n            raise ValueError(  # pragma: no cover\n                \"Parser does not support setting both `config` and `dialect`.\"\n            )\n        # Use the provided config or create one from the dialect.\n        self.config = config or FluffConfig.from_kwargs(dialect=dialect)\n        self.RootSegment: type[BaseFileSegment] = self.config.get(\n            \"dialect_obj\"\n        ).get_root_segment()\n\n    def parse(\n        self,\n        segments: Sequence[\"BaseSegment\"],\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> Optional[\"BaseSegment\"]:\n        \"\"\"Parse a series of lexed tokens using the current dialect.\"\"\"\n        if not segments:  # pragma: no cover\n            # This should normally never happen because there will usually\n            # be an end_of_file segment. It would probably only happen in\n            # api use cases.\n            return None\n\n        # NOTE: This is the only time we use the parse context not in the\n        # context of a context manager. That's because it's the initial\n        # instantiation.\n        ctx = ParseContext.from_config(config=self.config)\n        # Kick off parsing with the root segment. The BaseFileSegment has\n        # a unique entry point to facilitate exactly this. All other segments\n        # will use the standard .match() route.\n        root = self.RootSegment.root_pars"}, {"start_line": 0, "end_line": 38, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.core.parser.\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "file.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Definition of the BaseFileSegment.\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Optional\n\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments.base import BaseSegment, UnparsableSegment\n\n\nclass BaseFileSegment(BaseSegment):\n    \"\"\"A segment representing a whole file or script.\n\n    This is also the default \"root\" segment of the dialect,\n    and so is usually instantiated directly. It therefore\n    has no match_grammar.\n    \"\"\"\n\n    type = \"file\"\n    # The file segment is the only one which can start or end with non-code\n    can_start_end_non_code = True\n    # A file can be empty!\n    allow_empty = True\n\n    def __init__(\n        self,\n        segments: tuple[BaseSegment, ...],\n        pos_marker: Optional[PositionMarker] = None,\n        fname: Optional[str] = None,\n    ):\n        self._file_path = fname\n        super().__init__(segments, pos_marker=pos_marker)\n\n    @property\n    def file_path(self) -> Optional[str]:\n        \"\"\"File path of a parsed SQL file.\"\"\"\n        return self._file_path\n\n    @abstractmethod\n    def get_table_references(self) -> set[str]:\n        \"\"\"Use parsed tree to extract table references.\"\"\"\n\n    @classmethod\n    def root_parse(\n        cls,\n        segments: tuple[BaseSegment, ...],\n        parse_context: ParseContext,\n        fname: Optional[str] = None,\n    ) -> \"BaseFileSegment\":\n        \"\"\"This is the entry method into parsing a file lexed segments.\n\n        For single pass matching, this trims any non code off\n        the start, matches the middle and then trims the end.\n\n        Anything unexpected at the end is regarded as unparsable.\n        \"\"\"\n        # Trim the start\n        _start_idx = 0\n        for _start_idx in range(len(segments)):\n            if segments[_start_idx].is_code:\n                break\n\n        # Trim the end\n        _end_idx = len(segments)\n        for _end_idx in range(len(segments), _start_idx - 1, -1):\n   "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parsers.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Individual segment parsers.\n\nMatchable objects which return individual segments.\n\"\"\"\n\nfrom abc import abstractmethod\nfrom collections.abc import Collection, Sequence\nfrom typing import Any, Callable, Optional\nfrom uuid import uuid4\n\nimport regex\n\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.segments import BaseSegment, RawSegment\nfrom sqlfluff.core.parser.types import SimpleHintType\n\n\nclass BaseParser(Matchable):\n    \"\"\"An abstract class from which other Parsers should inherit.\"\"\"\n\n    # Meta segments are handled separately. All Parser elements\n    # are assumed to be not meta.\n    is_meta: bool = False\n\n    @abstractmethod\n    def __init__(\n        self,\n        raw_class: type[RawSegment],\n        type: Optional[str] = None,\n        optional: bool = False,\n        # The following kwargs are passed on to the segment:\n        trim_chars: Optional[tuple[str, ...]] = None,\n        casefold: Optional[Callable[[str], str]] = None,\n    ) -> None:\n        self.raw_class = raw_class\n        # Store instance_types rather than just type to allow\n        # for multiple possible types to be supported in derivative\n        # classes.\n        self._instance_types: tuple[str, ...] = (type or raw_class.type,)\n        self.optional = optional\n        self._trim_chars = trim_chars\n        self.casefold = casefold\n        # Generate a cache key\n        self._cache_key = uuid4().hex\n\n    def cache_key(self) -> str:\n        \"\"\"Get the cache key for this parser.\n\n        For parsers, they're unique per-instance.\n        \"\"\"\n        return self._cache_key\n\n    def is_optional(self) -> bool:\n        \"\"\"Return whether this element is optional.\"\"\"\n        return self.optional\n\n    def segment_kwargs(self) -> dict[str, Any]:\n        \"\"\"Generates the segment_kwargs package for generating a matched segment.\"\"\"\n        segment_kwargs: dict[str, Any"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"init file for the parser.\"\"\"\n\nfrom sqlfluff.core.parser.grammar import (\n    AnyNumberOf,\n    AnySetOf,\n    Anything,\n    Bracketed,\n    Conditional,\n    Delimited,\n    Nothing,\n    OneOf,\n    OptionallyBracketed,\n    OptionallyDelimited,\n    Ref,\n    Sequence,\n)\nfrom sqlfluff.core.parser.lexer import Lexer, RegexLexer, StringLexer\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.parser import Parser\nfrom sqlfluff.core.parser.parsers import (\n    MultiStringParser,\n    RegexParser,\n    StringParser,\n    TypedParser,\n)\nfrom sqlfluff.core.parser.segments import (\n    BaseFileSegment,\n    BaseSegment,\n    BinaryOperatorSegment,\n    BracketedSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    Dedent,\n    IdentifierSegment,\n    ImplicitIndent,\n    Indent,\n    KeywordSegment,\n    LiteralKeywordSegment,\n    LiteralSegment,\n    NewlineSegment,\n    RawSegment,\n    SegmentGenerator,\n    SourceFix,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.types import ParseMode\n\n__all__ = (\n    \"BaseSegment\",\n    \"SourceFix\",\n    \"BaseFileSegment\",\n    \"BracketedSegment\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"SymbolSegment\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"LiteralKeywordSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"SegmentGenerator\",\n    \"Sequence\",\n    \"OneOf\",\n    \"Delimited\",\n    \"Bracketed\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\","}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parse_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The Test file for The New Parser (Grammar Classes).\"\"\"\n\nfrom sqlfluff.core import FluffConfig\nfrom sqlfluff.core.errors import SQLParseError\nfrom sqlfluff.core.linter.linter import Linter\nfrom sqlfluff.core.parser import Anything, BaseSegment, KeywordSegment, StringParser\nfrom sqlfluff.core.parser.context import ParseContext\n\nBarKeyword = StringParser(\"bar\", KeywordSegment)\n\n\nclass BasicSegment(BaseSegment):\n    \"\"\"A basic segment for testing parse and expand.\"\"\"\n\n    type = \"basic\"\n    match_grammar = Anything()\n\n\ndef test__parser__parse_match(test_segments):\n    \"\"\"Test match method on a real segment.\"\"\"\n    ctx = ParseContext(dialect=None)\n    # This should match and have consumed everything, which should\n    # now be part of a BasicSegment.\n    match = BasicSegment.match(test_segments, 0, parse_context=ctx)\n    assert match\n    matched = match.apply(test_segments)\n    assert len(matched) == 1\n    assert isinstance(matched[0], BasicSegment)\n    assert matched[0].segments[0].type == \"raw\"\n\n\ndef test__parser__parse_error():\n    \"\"\"Test that SQLParseError is raised for unparsable section.\"\"\"\n    in_str = \"SELECT ;\"\n    lnt = Linter(dialect=\"ansi\")\n    parsed = lnt.parse_string(in_str)\n\n    assert len(parsed.violations) == 1\n    violation = parsed.violations[0]\n    assert isinstance(violation, SQLParseError)\n    assert violation.desc() == \"Line 1, Position 1: Found unparsable section: 'SELECT'\"\n\n    # Check that the expected labels work for logging.\n    # TODO: This is more specific that in previous iterations, but we could\n    # definitely make this easier to read.\n    assert (\n        'Expected: \"<Delimited: '\n        \"[<Ref: 'SelectClauseElementSegment'>]> \"\n        \"after <WordSegment: ([L:  1, P:  1]) 'SELECT'>. \"\n        \"Found nothing.\"\n    ) in parsed.tree.stringify()\n\n\ndef test_parse_jinja_macro_exclude():\n    \"\"\"Test parsing when excluding macros with unknown tags.\n\n    This test case has a file which defines the unknown tag `materialization` which\n    w"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/grammar", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Base grammar, Ref, Anything and Nothing.\"\"\"\n\nimport copy\nfrom collections.abc import Sequence\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Optional,\n    TypeVar,\n    Union,\n)\nfrom uuid import UUID, uuid4\n\nfrom sqlfluff.core.helpers.string import curtail_string\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.match_algorithms import greedy_match\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.segments import BaseSegment\nfrom sqlfluff.core.parser.types import ParseMode, SimpleHintType\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects.base import Dialect\n\n\ndef cached_method_for_parse_context(\n    func: Callable[[Any, ParseContext, Optional[tuple[str]]], SimpleHintType],\n) -> Callable[..., SimpleHintType]:\n    \"\"\"A decorator to cache the output of this method for a given parse context.\n\n    This cache automatically invalidates if the uuid\n    of the parse context changes. The value is store\n    in the __dict__ attribute of the class against a\n    key unique to that function.\n    \"\"\"\n    cache_key = \"__cache_\" + func.__name__\n\n    def wrapped_method(\n        self: Any, parse_context: ParseContext, crumbs: Optional[tuple[str]] = None\n    ) -> SimpleHintType:\n        \"\"\"Cache the output of the method against a given parse context.\n\n        Note: kwargs are not taken into account in the caching, but\n        for the current use case of dependency loop debugging that's\n        ok.\n        \"\"\"\n        try:\n            cache_tuple: tuple[UUID, SimpleHintType] = self.__dict__[cache_key]\n            # Is the value for the current context?\n            if cache_tuple[0] == parse_context.uuid:\n                # If so return it.\n                return cache_tuple[1]\n        except KeyError:\n            # Failed to find an item in the cache.\n            pass\n\n        # If we're here, we either didn't find a match in the c"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "context.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The parser context.\n\nThis mirrors some of the same design of the flask\ncontext manager. https://flask.palletsprojects.com/en/1.1.x/\n\nThe context acts as a way of keeping track of state, references\nto common configuration and dialects, logging and also the parse\nand match depth of the current operation.\n\"\"\"\n\nimport logging\nimport uuid\nfrom collections import defaultdict\nfrom collections.abc import Iterator, Sequence\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, NoReturn, Optional\n\nfrom tqdm import tqdm\n\nfrom sqlfluff.core.config import progress_bar_configuration\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects.base import Dialect\n    from sqlfluff.core.parser.match_result import MatchResult\n    from sqlfluff.core.parser.matchable import Matchable\n\n# Get the parser logger\nparser_logger = logging.getLogger(\"sqlfluff.parser\")\n\n\nclass ParseContext:\n    \"\"\"Object to handle the context at hand during parsing.\n\n    Holds two tiers of references.\n    1. Persistent config, like references to the dialect or\n       the current verbosity and logger.\n    2. Stack config, like the parse and match depth.\n\n    The manipulation of the stack config is done using a context\n    manager and layered config objects inside the context.\n\n    NOTE: We use context managers here to avoid _copying_\n    the context, just to mutate it safely. This is significantly\n    more performant than the copy operation, but does require some\n    care to use properly.\n\n    When fetching elements from the context, we first look\n    at the top level stack config object and the persistent\n    config values (stored as attributes of the ParseContext\n    itself).\n    \"\"\"\n\n    def __init__(\n        self,\n        dialect: \"Dialect\",\n        indentation_config: Optional[dict[str, Any]] = None,\n    ) -> None:\n        \"\"\"Initialize a new instance of the class.\n\n        Args:\n            dialect (Dialect): The dialect "}, {"start_line": 0, "end_line": 57, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser/grammar", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the sqlfluff.core.parser.grammar module.\"\"\"\n"}, {"start_line": 1000, "end_line": 2225, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ",\n    LiteralSegment,\n    NewlineSegment,\n    RawSegment,\n    SegmentGenerator,\n    SourceFix,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.types import ParseMode\n\n__all__ = (\n    \"BaseSegment\",\n    \"SourceFix\",\n    \"BaseFileSegment\",\n    \"BracketedSegment\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"SymbolSegment\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"LiteralKeywordSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"SegmentGenerator\",\n    \"Sequence\",\n    \"OneOf\",\n    \"Delimited\",\n    \"Bracketed\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\",\n    \"Conditional\",\n    \"StringParser\",\n    \"MultiStringParser\",\n    \"TypedParser\",\n    \"RegexParser\",\n    \"PositionMarker\",\n    \"Lexer\",\n    \"StringLexer\",\n    \"RegexLexer\",\n    \"Parser\",\n    \"Matchable\",\n    \"ParseMode\",\n)\n"}], "retrieved_count": 10, "cost_time": 0.3354682922363281}
{"question": "Where is the \"lint\" method defined in SQLFluff's class hierarchy?", "answer": null, "relative_code_list": null, "ground_truth": "The \"lint\" method in SQLFluff's class hierarchy is defined in several key locations. The main lint method implementations are located in: 1) src/sqlfluff/core/linter/linter.py - Contains the main Linter.lint() method that orchestrates the entire linting process; 2) src/sqlfluff/core/rules/base.py - Contains BaseRule._eval() method which is the core linting logic for individual rules; 3) src/sqlfluff/core/linter/linted_file.py - Contains LintedFile.lint() method for linting individual files; 4) src/sqlfluff/core/linter/linted_dir.py - Contains LintedDir.lint() method for linting directories; 5) src/sqlfluff/core/rules/ruleset.py - Contains RuleSet.lint() method for applying rule sets; 6) src/sqlfluff/cli/commands.py - Contains lint command implementation; 7) src/sqlfluff/api/simple.py - Contains lint() function for the simple API; 8) src/sqlfluff/core/rules/context.py - Contains linting context management; 9) src/sqlfluff/core/rules/flow.py - Contains linting flow control logic; 10) src/sqlfluff/core/rules/analysis.py - Contains linting analysis utilities.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 38, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.core.linter.\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linting_result.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the linter class.\"\"\"\n\nimport csv\nimport time\nfrom collections.abc import Iterable, Mapping\nfrom typing import TYPE_CHECKING, Any, Optional, TypeVar, Union\n\nfrom sqlfluff.core.errors import CheckTuple, SQLBaseError\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.linter.linted_dir import LintedDir, LintingRecord\nfrom sqlfluff.core.timing import RuleTimingSummary, TimingSummary\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.parser.segments.base import BaseSegment\n\n\ndef sum_dicts(d1: Mapping[str, int], d2: Mapping[str, int]) -> dict[str, int]:\n    \"\"\"Take the keys of two dictionaries and add their values.\"\"\"\n    keys = set(d1.keys()) | set(d2.keys())\n    return {key: d1.get(key, 0) + d2.get(key, 0) for key in keys}\n\n\nT = TypeVar(\"T\")\n\n\ndef combine_dicts(*d: dict[str, T]) -> dict[str, T]:\n    \"\"\"Take any set of dictionaries and combine them.\"\"\"\n    dict_buffer: dict[str, T] = {}\n    for dct in d:\n        dict_buffer.update(dct)\n    return dict_buffer\n\n\nclass LintingResult:\n    \"\"\"A class to represent the result of a linting operation.\n\n    Notably this might be a collection of paths, all with multiple\n    potential files within them.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.paths: list[LintedDir] = []\n        self._start_time: float = time.monotonic()\n        self.total_time: float = 0.0\n\n    def add(self, path: LintedDir) -> None:\n        \"\"\"Add a new `LintedDir` to this result.\"\"\"\n        self.paths.append(path)\n\n    def stop_timer(self) -> None:\n        \"\"\"Stop the linting timer.\"\"\"\n        self.total_time = time.monotonic() - self._start_time\n\n    def check_tuples(\n        self, raise_on_non_linting_violations: bool = True\n    ) -> list[CheckTuple]:\n        \"\"\"Fetch all check_tuples from all contained `LintedDir` objects.\n\n        Returns:\n            A list of check tuples.\n        \"\"\"\n        return [\n            t\n            for path in self.paths\n            for t in path.check_tuples(\n            "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the linter class.\"\"\"\n\nimport fnmatch\nimport logging\nimport os\nimport time\nfrom collections.abc import Iterator, Sequence\nfrom typing import TYPE_CHECKING, Optional, cast\n\nimport regex\nfrom tqdm import tqdm\n\nfrom sqlfluff.core.config import FluffConfig, progress_bar_configuration\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLFluffSkipFile,\n    SQLLexError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.helpers.file import get_encoding\nfrom sqlfluff.core.linter.common import (\n    ParsedString,\n    ParsedVariant,\n    RenderedFile,\n    RuleTuple,\n)\nfrom sqlfluff.core.linter.discovery import paths_from_path\nfrom sqlfluff.core.linter.fix import apply_fixes, compute_anchor_edit_info\nfrom sqlfluff.core.linter.linted_dir import LintedDir\nfrom sqlfluff.core.linter.linted_file import (\n    TMP_PRS_ERROR_TYPES,\n    FileTimings,\n    LintedFile,\n)\nfrom sqlfluff.core.linter.linting_result import LintingResult\nfrom sqlfluff.core.parser import Lexer, Parser\nfrom sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\nfrom sqlfluff.core.rules import BaseRule, RulePack, get_ruleset\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.rules.noqa import IgnoreMask\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.meta import MetaSegment\n    from sqlfluff.core.templaters import RawTemplater, TemplatedFile\n\n\nRuleTimingsType = list[tuple[str, str, float]]\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass Linter:\n    \"\"\"The interface class to interact with the linter.\"\"\"\n\n    # Default to allowing process parallelism\n    allow_process_parallelism = True\n\n    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n        dialect: Optional[str] = None,\n  "}, {"start_line": 0, "end_line": 489, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Linter class and helper classes.\"\"\"\n\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.linter.common import ParsedString, RenderedFile, RuleTuple\nfrom sqlfluff.core.linter.linted_file import LintedFile\nfrom sqlfluff.core.linter.linter import Linter\nfrom sqlfluff.core.linter.linting_result import LintingResult\n\n__all__ = (\n    \"FormatterInterface\",\n    \"RuleTuple\",\n    \"ParsedString\",\n    \"LintedFile\",\n    \"LintingResult\",\n    \"Linter\",\n    \"RenderedFile\",\n)\n"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "t import LintingResult\nfrom sqlfluff.core.parser import Lexer, Parser\nfrom sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\nfrom sqlfluff.core.rules import BaseRule, RulePack, get_ruleset\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.rules.noqa import IgnoreMask\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.meta import MetaSegment\n    from sqlfluff.core.templaters import RawTemplater, TemplatedFile\n\n\nRuleTimingsType = list[tuple[str, str, float]]\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass Linter:\n    \"\"\"The interface class to interact with the linter.\"\"\"\n\n    # Default to allowing process parallelism\n    allow_process_parallelism = True\n\n    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n        dialect: Optional[str] = None,\n        rules: Optional[list[str]] = None,\n        user_rules: Optional[list[type[BaseRule]]] = None,\n        exclude_rules: Optional[list[str]] = None,\n    ) -> None:\n        if config and (dialect or rules or exclude_rules):\n            raise ValueError(  # pragma: no cover\n                \"Linter does not support setting both `config` and any of \"\n                \"`dialect`, `rules` or `exclude_rules`. The latter are \"\n                \"provided as convenience methods to avoid needing to \"\n                \"set the `config` object. If using `config`, please \"\n                \"provide all the other values within that object.\"\n            )\n        # Use the provided config or create one from the kwargs.\n        self.config = config or FluffConfig.from_kwargs(\n            dialect=dialect,\n            rules=rules,\n            exclude_rules=exclude_rules,\n            # Don't require a dialect to be provided yet. Defer this until we\n            # are actually linting something, since the dir"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`Lin"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linted_dir.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the LintedDir class.\n\nThis stores the idea of a collection of linted files at a single start path\n\n\"\"\"\n\nfrom collections.abc import Iterable\nfrom typing import Optional, TypedDict, Union\n\nfrom sqlfluff.core.errors import (\n    CheckTuple,\n    SerializedObject,\n    SQLBaseError,\n    SQLLintError,\n)\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.linter.linted_file import TMP_PRS_ERROR_TYPES, LintedFile\nfrom sqlfluff.core.parser.segments.base import BaseSegment\n\n\nclass LintingRecord(TypedDict):\n    \"\"\"A class to store the linted file statistics.\"\"\"\n\n    filepath: str\n    violations: list[SerializedObject]\n    # Things like file length\n    statistics: dict[str, int]\n    # Raw timings, in seconds, for both rules and steps\n    timings: dict[str, float]\n\n\nclass LintedDir:\n    \"\"\"A class to store the idea of a collection of linted files at a single start path.\n\n    A LintedDir may contain files in subdirectories, but they all share\n    a common root.\n\n    Importantly, this class also abstracts away from the given LintedFile\n    object and allows us to either _keep_ those objects for later use, or\n    extract the results from them and allow the original object to be discarded\n    and save memory overhead if not required.\n    \"\"\"\n\n    def __init__(self, path: str, retain_files: bool = True) -> None:\n        self.files: list[LintedFile] = []\n        self.path: str = path\n        self.retain_files: bool = retain_files\n        # Records\n        self._records: list[LintingRecord] = []\n        # Stats\n        self._num_files: int = 0\n        self._num_clean: int = 0\n        self._num_unclean: int = 0\n        self._num_violations: int = 0\n        self.num_unfiltered_tmp_prs_errors: int = 0\n        self._unfiltered_tmp_prs_errors_map: dict[str, int] = {}\n        self.num_tmp_prs_errors: int = 0\n        self.num_unfixable_lint_errors: int = 0\n        # Timing\n        self.step_timings: list[dict[str, float]] = []\n        self.rule_timings: list[t"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "common.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines small container classes to hold intermediate results during linting.\"\"\"\n\nfrom typing import Any, NamedTuple, Optional, Union\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLLexError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.parser.segments.base import BaseSegment\nfrom sqlfluff.core.templaters import TemplatedFile\n\n\nclass RuleTuple(NamedTuple):\n    \"\"\"Rule Tuple object for describing rules.\"\"\"\n\n    code: str\n    name: str\n    description: str\n    groups: tuple[str, ...]\n    aliases: tuple[str, ...]\n\n\nclass RenderedFile(NamedTuple):\n    \"\"\"An object to store the result of a templated file/string.\n\n    This is notable as it's the intermediate state between what happens\n    in the main process and the child processes when running in parallel mode.\n    \"\"\"\n\n    templated_variants: list[TemplatedFile]\n    templater_violations: list[SQLTemplaterError]\n    config: FluffConfig\n    time_dict: dict[str, float]\n    fname: str\n    encoding: str\n    source_str: str\n\n\nclass ParsedVariant(NamedTuple):\n    \"\"\"An object to store the result of parsing a single TemplatedFile.\n\n    Args:\n        templated_file (:obj:`TemplatedFile`): Containing the details\n            of the templated file. If templating fails, this will be `None`.\n        tree (:obj:`BaseSegment`): The segment structure representing the\n            parsed file. If parsing fails due to an unrecoverable\n            violation then we will be None.\n        lexing_violations (:obj:`list` of :obj:`SQLLexError`): Any violations\n            raised during the lexing phase.\n        parsing_violations (:obj:`list` of :obj:`SQLParseError`): Any violations\n            raised during the lexing phase.\n    \"\"\"\n\n    templated_file: TemplatedFile\n    tree: Optional[BaseSegment]\n    lexing_violations: list[SQLLexError]\n    parsing_violations: list[SQLParseError]\n\n    def violations(self) -> list[Union[SQLLexError, SQLParseError]]:\n        \"\"\"Return"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linter_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the Linter class and LintingResult class.\"\"\"\n\nimport logging\nimport os\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom sqlfluff.cli.formatters import OutputStreamFormatter\nfrom sqlfluff.cli.outputstream import make_output_stream\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLFluffSkipFile,\n    SQLLexError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.linter import runner\nfrom sqlfluff.core.linter.linting_result import combine_dicts, sum_dicts\nfrom sqlfluff.core.linter.runner import get_runner\nfrom sqlfluff.utils.testing.logging import fluff_log_catcher\n\n\nclass DummyLintError(SQLBaseError):\n    \"\"\"Fake lint error used by tests, similar to SQLLintError.\"\"\"\n\n    def __init__(self, line_no: int, code: str = \"LT01\"):\n        self._code = code\n        super().__init__(line_no=line_no)\n\n\ndef normalise_paths(paths):\n    \"\"\"Test normalising paths.\n\n    NB Paths on difference platforms might look different, so this\n    makes them comparable.\n    \"\"\"\n    return {pth.replace(\"/\", \".\").replace(\"\\\\\", \".\") for pth in paths}\n\n\n@pytest.mark.parametrize(\"filesize,raises_skip\", [(0, False), (5, True), (2000, False)])\ndef test__linter__skip_large_bytes(filesize, raises_skip):\n    \"\"\"Test extracting paths from a file path.\"\"\"\n    config = FluffConfig(\n        overrides={\"large_file_skip_byte_limit\": filesize, \"dialect\": \"ansi\"}\n    )\n    # First check the function directly\n    if raises_skip:\n        with pytest.raises(SQLFluffSkipFile) as excinfo:\n            Linter.load_raw_file_and_config(\n                \"test/fixtures/linter/indentation_errors.sql\", config\n            )\n        assert \"Skipping\" in str(excinfo.value)\n        assert f\"over the limit of {filesize}\" in str(excinfo.value)\n    # If NOT raises, then we'll catch the raise an error and the test will fail.\n\n    # Then check that it either is or isn't linted appropriately via lint_paths.\n    lntr = Linter(config)\n"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`LintFix`, optional): An array of any\n            fixes which would correct this issue. If not present then it's\n            assumed that this issue will have to manually fixed.\n        memory (:obj:`dict`, optional): An object which stores any working\n            memory for the rule. The `memory` returned in any `LintResult`\n            will be passed as an input to the next segment to be crawled.\n        description (:obj:`str`, optional): A description of the problem\n            identified as part of this result. This will override the\n            description of the rule as what gets reported to the user\n            with the problem if provided.\n        source (:obj:`str`, optional): A string identifier for what\n            generated the result. Within larger libraries like reflow this\n            can be useful for tracking where a result came from.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        anchor: Optional[BaseSegment] = None,\n        fixes: Optional[list[\"LintFix\"]] = None,\n  "}], "retrieved_count": 10, "cost_time": 0.33973050117492676}
{"question": "Where are SQLFluff's BaseRule class definitions located?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's BaseRule class definitions are located in several key locations in the codebase. The main BaseRule implementations are located in: 1) src/sqlfluff/core/rules/base.py - Contains the main BaseRule class definition and RuleMetaclass; 2) src/sqlfluff/core/rules/ruleset.py - Contains RuleSet class for managing collections of rules; 3) src/sqlfluff/core/rules/context.py - Contains rule context and evaluation utilities; 4) src/sqlfluff/core/rules/flow.py - Contains rule flow control and execution logic; 5) src/sqlfluff/core/rules/analysis.py - Contains rule analysis and validation utilities; 6) src/sqlfluff/core/rules/config_info.py - Contains rule configuration management; 7) src/sqlfluff/core/rules/__init__.py - Contains rule module initialization and imports; 8) src/sqlfluff/core/rules/fix.py - Contains fix-related rule utilities; 9) src/sqlfluff/core/rules/code_0100.py through code_9999.py - Contains specific rule implementations that inherit from BaseRule; 10) src/sqlfluff/core/rules/rule_helpers.py - Contains helper functions for rule development.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 1576, "belongs_to": {"file_name": "loader.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Methods to load rules.\"\"\"\n\nimport os\nfrom glob import glob\nfrom importlib import import_module\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.rules.base import BaseRule\n\n\ndef get_rules_from_path(\n    # All rule files are expected in the format of L*.py\n    rules_path: str = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), \"../../rules\", \"L*.py\")\n    ),\n    base_module: str = \"sqlfluff.rules\",\n) -> list[type[\"BaseRule\"]]:\n    \"\"\"Reads all of the Rule classes from a path into a list.\"\"\"\n    # Create a rules dictionary for importing in\n    # sqlfluff/src/sqlfluff/core/rules/__init__.py\n    rules = []\n\n    for module in sorted(glob(rules_path)):\n        # Manipulate the module path to extract the filename without the .py\n        rule_id = os.path.splitext(os.path.basename(module))[0]\n        # All rule classes are expected in the format of Rule_L*\n        rule_class_name = f\"Rule_{rule_id}\"\n        # NOTE: We import the module outside of the try clause to\n        # properly catch any import errors.\n        rule_module = import_module(f\"{base_module}.{rule_id}\")\n        try:\n            rule_class = getattr(rule_module, rule_class_name)\n        except AttributeError as e:\n            raise AttributeError(\n                \"Rule classes must be named in the format of Rule_*. \"\n                f\"[{rule_class_name}]\"\n            ) from e\n        # Add the rules to the rules dictionary for\n        # sqlfluff/src/sqlfluff/core/rules/__init__.py\n        rules.append(rule_class)\n\n    return rules\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Implements the base rule class.\n\nRules crawl through the trees returned by the parser and evaluate particular\nrules.\n\nThe intent is that it should be possible for the rules to be expressed\nas simply as possible, with as much of the complexity abstracted away.\n\nThe evaluation function should take enough arguments that it can evaluate\nthe position of the given segment in relation to its neighbors, and that\nthe segment which finally \"triggers\" the error, should be the one that would\nbe corrected OR if the rule relates to something that is missing, then it\nshould flag on the segment FOLLOWING, the place that the desired element is\nmissing.\n\"\"\"\n\nimport bdb\nimport copy\nimport fnmatch\nimport logging\nimport pathlib\nimport re\nfrom collections import defaultdict, namedtuple\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    ClassVar,\n    DefaultDict,\n    Optional,\n    Union,\n)\n\nimport regex\n\nfrom sqlfluff.core.errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a "}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tion for the base classes\n        if name not in (\"BaseRule\",):\n            class_dict = RuleMetaclass._populate_code_and_description(name, class_dict)\n        # Validate rule names\n        rule_name = class_dict.get(\"name\", \"\")\n        if rule_name:\n            if not RuleMetaclass._valid_rule_name_regex.match(rule_name):\n                raise SQLFluffUserError(\n                    f\"Tried to define rule with unexpected \"\n                    f\"name format: {rule_name}. Rule names should be lowercase \"\n                    \"and snake_case with optional `.` characters to indicate \"\n                    \"a namespace or grouping. e.g. `layout.spacing`.\"\n                )\n\n        # Use the stock __new__ method now we've adjusted the docstring.\n        # There are no overload variants of type.__new__ that are compatible, so\n        # we ignore type checking in this case.\n        return super().__new__(mcs, name, bases, class_dict)  # type: ignore\n\n\nclass BaseRule(metaclass=RuleMetaclass):\n    \"\"\"The base class for a rule.\n\n    Args:\n        code (:obj:`str`): The identifier for this rule, used in inclusion\n            or exclusion.\n        description (:obj:`str`): A human readable description of what this\n            rule does. It will be displayed when any violations are found.\n\n    \"\"\"\n\n    _check_docstring = True\n    _works_on_unparsable = True\n    _adjust_anchors = False\n    targets_templated = False\n    # Some fix routines do their own checking for whether their fixes\n    # are safe around templated elements. For those - the default\n    # safety checks might be inappropriate. In those cases, set\n    # template_safe_fixes to True.\n    template_safe_fixes = False\n\n    # Config settings supported for this rule.\n    # See config_info.py for supported values.\n    config_keywords: list[str] = []\n    # Lint loop / crawl behavior. When appropriate, rules can (and should)\n    # override these values to make linting faster.\n    crawl_behaviour: BaseCrawler\n    # Rules can ove"}, {"start_line": 0, "end_line": 45, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Standard Rules packaged with sqlfluff.\"\"\"\n"}, {"start_line": 0, "end_line": 1349, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Configuration and examples for individual rules.\"\"\"\n\nfrom sqlfluff.core.plugin.host import get_plugin_manager\nfrom sqlfluff.core.rules.base import (\n    BaseRule,\n    EvalResultType,\n    LintResult,\n    RuleGhost,\n    RulePack,\n    RuleSet,\n)\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.fix import LintFix\n\n\ndef _load_standard_rules() -> RuleSet:\n    \"\"\"Initialise the standard ruleset.\n\n    We do this on each call so that dynamic rules changes\n    are possible.\n    \"\"\"\n    std_rule_set = RuleSet(name=\"standard\", config_info=get_config_info())\n\n    # Iterate through the rules list and register each rule with the standard set.\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            std_rule_set.register(rule)\n\n    return std_rule_set\n\n\ndef get_ruleset(name: str = \"standard\") -> RuleSet:\n    \"\"\"Get a ruleset by name.\"\"\"\n    std_rules = _load_standard_rules()\n    lookup = {std_rules.name: std_rules}\n    # Return a copy in case someone modifies the register.\n    return lookup[name].copy()\n\n\n__all__ = (\n    \"get_ruleset\",\n    \"RuleSet\",\n    \"RulePack\",\n    \"BaseRule\",\n    \"LintResult\",\n    \"LintFix\",\n    \"RuleContext\",\n    \"RuleGhost\",\n    \"EvalResultType\",\n    \"ConfigInfo\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "rules_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the standard set of rules.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import Linter\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.linter import RuleTuple\nfrom sqlfluff.core.parser import WhitespaceSegment\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.rules import BaseRule, LintFix, LintResult, get_ruleset\nfrom sqlfluff.core.rules.crawlers import RootOnlyCrawler, SegmentSeekerCrawler\nfrom sqlfluff.core.rules.doc_decorators import (\n    document_configuration,\n    document_fix_compatible,\n    document_groups,\n)\nfrom sqlfluff.core.rules.loader import get_rules_from_path\nfrom sqlfluff.core.templaters.base import TemplatedFile\nfrom sqlfluff.utils.testing.logging import fluff_log_catcher\nfrom sqlfluff.utils.testing.rules import get_rule_from_set\nfrom test.fixtures.rules.custom.L000 import Rule_L000\nfrom test.fixtures.rules.custom.S000 import Rule_S000\n\n\nclass Rule_T042(BaseRule):\n    \"\"\"A dummy rule.\"\"\"\n\n    groups = (\"all\",)\n\n    def _eval(self, context):\n        pass\n\n\nclass Rule_T001(BaseRule):\n    \"\"\"A deliberately malicious rule.\n\n    **Anti-pattern**\n\n    Blah blah\n    \"\"\"\n\n    groups = (\"all\",)\n    crawl_behaviour = SegmentSeekerCrawler({\"whitespace\"})\n    is_fix_compatible = True\n\n    def _eval(self, context):\n        \"\"\"Stars make newlines.\"\"\"\n        if context.segment.is_type(\"whitespace\"):\n            return LintResult(\n                anchor=context.segment,\n                fixes=[\n                    LintFix.replace(\n                        context.segment, [WhitespaceSegment(context.segment.raw + \" \")]\n                    )\n                ],\n            )\n\n\nclass Rule_T002(BaseRule):\n    \"\"\"A rule which says all raw code segments are bad.\n\n    This is used for testing unparsable code.\n    \"\"\"\n\n    groups = (\"all\",)\n    # Root only crawler so that the in-rule filters don't kick in.\n    crawl_behaviour = RootOnlyCrawler()\n\n    def _eval(s"}, {"start_line": 0, "end_line": 1740, "belongs_to": {"file_name": "context.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Define RuleContext class.\"\"\"\n\nimport pathlib\nfrom dataclasses import dataclass, field\nfrom typing import Any, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.dialects import Dialect\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n\n@dataclass\nclass RuleContext:\n    \"\"\"Class for holding the context passed to rule eval functions.\"\"\"\n\n    # These don't change within a file.\n    dialect: Dialect\n    fix: bool\n    templated_file: Optional[TemplatedFile]\n    path: Optional[pathlib.Path]\n    config: FluffConfig\n\n    # These change within a file.\n    # segment: The segment in question\n    segment: BaseSegment\n    # parent_stack: A tuple of the path from the root to this segment.\n    parent_stack: tuple[BaseSegment, ...] = field(default=tuple())\n    # raw_stack: All of the raw segments so far in the file\n    raw_stack: tuple[RawSegment, ...] = field(default=tuple())\n    # memory: Arbitrary storage for the rule\n    memory: Any = field(default_factory=dict)\n    # segment_idx: The index of this segment in the parent\n    segment_idx: int = field(default=0)\n\n    @property\n    def siblings_pre(self) -> tuple[BaseSegment, ...]:  # pragma: no cover\n        \"\"\"Return sibling segments prior to self.segment.\"\"\"\n        if self.parent_stack:\n            return self.parent_stack[-1].segments[: self.segment_idx]\n        else:\n            return tuple()\n\n    @property\n    def siblings_post(self) -> tuple[BaseSegment, ...]:\n        \"\"\"Return sibling segments after self.segment.\"\"\"\n        if self.parent_stack:\n            return self.parent_stack[-1].segments[self.segment_idx + 1 :]\n        else:\n            return tuple()  # pragma: no cover\n"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "layed in the sqlfluff\n    docs.\n    \"\"\"\n\n    # Precompile the regular expressions\n    _doc_search_regex: ClassVar = re.compile(\n        \"(\\\\s{4}\\\\*\\\\*Anti-pattern\\\\*\\\\*|\\\\s{4}\\\\.\\\\. note::|\"\n        \"\\\\s\\\\s{4}\\\\*\\\\*Configuration\\\\*\\\\*)\",\n        flags=re.MULTILINE,\n    )\n    _valid_classname_regex: ClassVar = regex.compile(\n        r\"Rule_?([A-Z]{1}[a-zA-Z]+)?_([A-Z0-9]{4})\"\n    )\n    _valid_rule_name_regex: ClassVar = regex.compile(r\"[a-z][a-z\\.\\_]+\")\n\n    @staticmethod\n    def _populate_code_and_description(\n        name: str, class_dict: dict[str, Any]\n    ) -> dict[str, Any]:\n        \"\"\"Extract and validate the rule code & description.\n\n        We expect that rules are defined as classes with the name `Rule_XXXX`\n        where `XXXX` is of the form `LLNN`, where L is a letter and N is a\n        two digit number. For backward compatibility we also still support\n        the legacy format of LNNN i.e. a single letter and three digit number.\n\n        The two letters should be indicative of the grouping and focus of\n        the rule. e.g. capitalisation rules have the code CP for CaPitalisation.\n\n        If this receives classes by any other name, then it will raise a\n        :exc:`ValueError`.\n        \"\"\"\n        rule_name_match = RuleMetaclass._valid_classname_regex.match(name)\n        # Validate the name\n        if not rule_name_match:  # pragma: no cover\n            raise SQLFluffUserError(\n                f\"Tried to define rule class with \"\n                f\"unexpected format: {name}. Format should be: \"\n                \"'Rule_PluginName_LL23' (for plugins) or \"\n                \"`Rule_LL23` (for core rules).\"\n            )\n\n        plugin_name, code = rule_name_match.groups()\n        # If the docstring is multiline, then we extract just summary.\n        description = class_dict[\"__doc__\"].replace(\"``\", \"'\").split(\"\\n\")[0]\n        if plugin_name:\n            code = f\"{plugin_name}_{code}\"\n\n        class_dict[\"code\"] = code\n        class_dict[\"description\"] = desc"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# names, aliases and description are less appropriate to inherit.\n        # NOTE: This applies in particular to CP02, which inherits all groups\n        # from CP01. If we don't do this, those groups don't show in the docs.\n        for base in reversed(bases):\n            if \"groups\" in class_dict:\n                break\n            elif base.groups:\n                class_dict[\"groups\"] = base.groups\n                break\n\n        # If the rule doesn't itself define `config_keywords`, check the parent\n        # classes for them. If we don't do this then they'll still be available to\n        # the rule, but they won't appear in the docs.\n        for base in reversed(bases):\n            if \"config_keywords\" in class_dict:\n                break\n            elif base.config_keywords:\n                class_dict[\"config_keywords\"] = base.config_keywords\n                break\n\n        class_dict = RuleMetaclass._populate_docstring(name, class_dict)\n        # Don't try and infer code and description for the base classes\n        if name not in (\"BaseRule\",):\n            class_dict = RuleMetaclass._populate_code_and_description(name, class_dict)\n        # Validate rule names\n        rule_name = class_dict.get(\"name\", \"\")\n        if rule_name:\n            if not RuleMetaclass._valid_rule_name_regex.match(rule_name):\n                raise SQLFluffUserError(\n                    f\"Tried to define rule with unexpected \"\n                    f\"name format: {rule_name}. Rule names should be lowercase \"\n                    \"and snake_case with optional `.` characters to indicate \"\n                    \"a namespace or grouping. e.g. `layout.spacing`.\"\n                )\n\n        # Use the stock __new__ method now we've adjusted the docstring.\n        # There are no overload variants of type.__new__ that are compatible, so\n        # we ignore type checking in this case.\n        return super().__new__(mcs, name, bases, class_dict)  # type: ignore\n\n\nclass BaseRule(metaclass=RuleMetaclass):\n  "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`LintFix`, optional): An array of any\n            fixes which would correct this issue. If not present then it's\n            assumed that this issue will have to manually fixed.\n        memory (:obj:`dict`, optional): An object which stores any working\n            memory for the rule. The `memory` returned in any `LintResult`\n            will be passed as an input to the next segment to be crawled.\n        description (:obj:`str`, optional): A description of the problem\n            identified as part of this result. This will override the\n            description of the rule as what gets reported to the user\n            with the problem if provided.\n        source (:obj:`str`, optional): A string identifier for what\n            generated the result. Within larger libraries like reflow this\n            can be useful for tracking where a result came from.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        anchor: Optional[BaseSegment] = None,\n        fixes: Optional[list[\"LintFix\"]] = None,\n  "}], "retrieved_count": 10, "cost_time": 0.3325207233428955}
{"question": "Where are SQLFluff's dialect-specific parser implementations located?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's dialect-specific parser implementations are located in several key locations in the codebase. The main dialect-specific parser components are located in: 1) src/sqlfluff/core/dialects/ - Contains all dialect-specific implementations including ansi.py, postgres.py, mysql.py, snowflake.py, etc.; 2) src/sqlfluff/core/dialects/base.py - Contains the base Dialect class definition; 3) src/sqlfluff/core/dialects/common.py - Contains common dialect utilities and shared components; 4) src/sqlfluff/core/parser/grammar.py - Contains grammar definitions that are dialect-specific; 5) src/sqlfluff/core/parser/parsers.py - Contains parser classes that handle dialect-specific syntax; 6) src/sqlfluff/core/parser/segments/ - Contains segment definitions that vary by dialect; 7) src/sqlfluff/core/lexer.py - Contains dialect-specific lexer matchers and tokenization logic; 8) src/sqlfluff/core/dialects/__init__.py - Contains dialect registration and selection logic; 9) src/sqlfluff/core/parser/context.py - Contains dialect-aware parsing context; 10) src/sqlfluff/core/config/fluffconfig.py - Contains dialect configuration management.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 35, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.dialects.\"\"\"\n"}, {"start_line": 0, "end_line": 209, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Dialects, segregated to make imports manageable.\n\nNOTE: dialects should not be imported directly from this\nmodule, but should be accessed instead using the selector\nmethods in `sqlfluff.core.dialects`.\n\"\"\"\n"}, {"start_line": 0, "end_line": 38, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.core.parser.\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parser.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the Parser class.\"\"\"\n\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import check_still_complete\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.parser.segments import BaseFileSegment, BaseSegment\n\n\nclass Parser:\n    \"\"\"Instantiates parsed queries from a sequence of lexed raw segments.\"\"\"\n\n    def __init__(\n        self, config: Optional[FluffConfig] = None, dialect: Optional[str] = None\n    ):\n        if config and dialect:\n            raise ValueError(  # pragma: no cover\n                \"Parser does not support setting both `config` and `dialect`.\"\n            )\n        # Use the provided config or create one from the dialect.\n        self.config = config or FluffConfig.from_kwargs(dialect=dialect)\n        self.RootSegment: type[BaseFileSegment] = self.config.get(\n            \"dialect_obj\"\n        ).get_root_segment()\n\n    def parse(\n        self,\n        segments: Sequence[\"BaseSegment\"],\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> Optional[\"BaseSegment\"]:\n        \"\"\"Parse a series of lexed tokens using the current dialect.\"\"\"\n        if not segments:  # pragma: no cover\n            # This should normally never happen because there will usually\n            # be an end_of_file segment. It would probably only happen in\n            # api use cases.\n            return None\n\n        # NOTE: This is the only time we use the parse context not in the\n        # context of a context manager. That's because it's the initial\n        # instantiation.\n        ctx = ParseContext.from_config(config=self.config)\n        # Kick off parsing with the root segment. The BaseFileSegment has\n        # a unique entry point to facilitate exactly this. All other segments\n        # will use the standard .match() route.\n        root = self.RootSegment.root_pars"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"init file for the parser.\"\"\"\n\nfrom sqlfluff.core.parser.grammar import (\n    AnyNumberOf,\n    AnySetOf,\n    Anything,\n    Bracketed,\n    Conditional,\n    Delimited,\n    Nothing,\n    OneOf,\n    OptionallyBracketed,\n    OptionallyDelimited,\n    Ref,\n    Sequence,\n)\nfrom sqlfluff.core.parser.lexer import Lexer, RegexLexer, StringLexer\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.parser import Parser\nfrom sqlfluff.core.parser.parsers import (\n    MultiStringParser,\n    RegexParser,\n    StringParser,\n    TypedParser,\n)\nfrom sqlfluff.core.parser.segments import (\n    BaseFileSegment,\n    BaseSegment,\n    BinaryOperatorSegment,\n    BracketedSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    Dedent,\n    IdentifierSegment,\n    ImplicitIndent,\n    Indent,\n    KeywordSegment,\n    LiteralKeywordSegment,\n    LiteralSegment,\n    NewlineSegment,\n    RawSegment,\n    SegmentGenerator,\n    SourceFix,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.types import ParseMode\n\n__all__ = (\n    \"BaseSegment\",\n    \"SourceFix\",\n    \"BaseFileSegment\",\n    \"BracketedSegment\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"SymbolSegment\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"LiteralKeywordSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"SegmentGenerator\",\n    \"Sequence\",\n    \"OneOf\",\n    \"Delimited\",\n    \"Bracketed\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\","}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Contains SQL Dialects.\n\nNote that individual dialects are only imported as needed at runtime.\nThis avoids circular references.\n\nTo enable this, any modules outside of .dialects cannot import dialects\ndirectly. They should import `dialect_selector` and use that to fetch\ndialects.\n\nWithin .dialects, each dialect is free to depend on other dialects as\nrequired. Any dependent dialects will be loaded as needed.\n\"\"\"\n\nfrom collections.abc import Iterator\nfrom importlib import import_module\nfrom typing import NamedTuple\n\n# Eventually it would be a good to dynamically discover dialects\n# from any module beginning with \"dialect_\" within this folder.\nfrom sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.errors import SQLFluffUserError\n\n_dialect_lookup = {\n    \"ansi\": (\"dialect_ansi\", \"ansi_dialect\"),\n    \"athena\": (\"dialect_athena\", \"athena_dialect\"),\n    \"bigquery\": (\"dialect_bigquery\", \"bigquery_dialect\"),\n    \"clickhouse\": (\"dialect_clickhouse\", \"clickhouse_dialect\"),\n    \"databricks\": (\"dialect_databricks\", \"databricks_dialect\"),\n    \"db2\": (\"dialect_db2\", \"db2_dialect\"),\n    \"doris\": (\"dialect_doris\", \"doris_dialect\"),\n    \"duckdb\": (\"dialect_duckdb\", \"duckdb_dialect\"),\n    \"exasol\": (\"dialect_exasol\", \"exasol_dialect\"),\n    \"flink\": (\"dialect_flink\", \"flink_dialect\"),\n    \"greenplum\": (\"dialect_greenplum\", \"greenplum_dialect\"),\n    \"hive\": (\"dialect_hive\", \"hive_dialect\"),\n    \"impala\": (\"dialect_impala\", \"impala_dialect\"),\n    \"materialize\": (\"dialect_materialize\", \"materialize_dialect\"),\n    \"mariadb\": (\"dialect_mariadb\", \"mariadb_dialect\"),\n    \"mysql\": (\"dialect_mysql\", \"mysql_dialect\"),\n    \"oracle\": (\"dialect_oracle\", \"oracle_dialect\"),\n    \"postgres\": (\"dialect_postgres\", \"postgres_dialect\"),\n    \"redshift\": (\"dialect_redshift\", \"redshift_dialect\"),\n    \"snowflake\": (\"dialect_snowflake\", \"snowflake_dialect\"),\n    \"soql\": (\"dialect_soql\", \"soql_dialect\"),\n    \"sparksql\": (\"dialect_sparksql\", \"sparksql_dialect\"),\n    \"sqlite\": (\"dialect_sqlite\", \"sql"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the base dialect class.\"\"\"\n\nimport sys\nfrom typing import Any, Optional, Union, cast\n\nfrom sqlfluff.core.parser import (\n    BaseSegment,\n    KeywordSegment,\n    SegmentGenerator,\n    StringParser,\n)\nfrom sqlfluff.core.parser.grammar.base import BaseGrammar, Nothing\nfrom sqlfluff.core.parser.lexer import LexerType\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.types import BracketPairTuple, DialectElementType\n\n\nclass Dialect:\n    \"\"\"Serves as the basis for runtime resolution of Grammar.\n\n    Args:\n        name (:obj:`str`): The name of the dialect, used for lookup.\n        lexer_matchers (iterable of :obj:`StringLexer`): A structure defining\n            the lexing config for this dialect.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        root_segment_name: str,\n        lexer_matchers: Optional[list[LexerType]] = None,\n        library: Optional[dict[str, DialectElementType]] = None,\n        sets: Optional[dict[str, set[Union[str, BracketPairTuple]]]] = None,\n        inherits_from: Optional[str] = None,\n        formatted_name: Optional[str] = None,\n        docstring: Optional[str] = None,\n    ) -> None:\n        self._library = library or {}\n        self.name = name\n        self.lexer_matchers = lexer_matchers\n        self.expanded = False\n        self._sets = sets or {}\n        self.inherits_from = inherits_from\n        self.root_segment_name = root_segment_name\n        # Attributes for documentation\n        self.formatted_name: str = formatted_name or name\n        self.docstring = docstring or f\"The dialect for {self.formatted_name}.\"\n\n    def __repr__(self) -> str:  # pragma: no cover\n        return f\"<Dialect: {self.name}>\"\n\n    def expand(self) -> \"Dialect\":\n        \"\"\"Expand any callable references to concrete ones.\n\n        This must be called before using the dialect. But\n        allows more flexible definitions to happen at runtime.\n\n        NOTE: This method returns a copy of the current dialect\n  "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "conftest.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Sharing fixtures to test the dialects.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.parser import BaseSegment, Lexer\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\n\n\ndef lex(raw, config):\n    \"\"\"Basic parsing for the tests below.\"\"\"\n    # Set up the lexer\n    lex = Lexer(config=config)\n    # Lex the string for matching. For a good test, this would\n    # arguably happen as a fixture, but it's easier to pass strings\n    # as parameters than pre-lexed segment strings.\n    segments, vs = lex.lex(raw)\n    assert not vs\n    print(segments)\n    return segments\n\n\ndef validate_segment(segmentref, config):\n    \"\"\"Get and validate segment for tests below.\"\"\"\n    Seg = config.get(\"dialect_obj\").ref(segmentref)\n    if isinstance(Seg, Matchable):\n        return Seg\n    try:\n        if issubclass(Seg, BaseSegment):\n            return Seg\n    except TypeError:\n        pass\n    raise TypeError(\n        \"{} is not of type Segment or Matchable. Test is invalid.\".format(segmentref)\n    )\n\n\ndef _dialect_specific_segment_parses(dialect, segmentref, raw, caplog):\n    \"\"\"Test that specific segments parse as expected.\n\n    NB: We're testing the PARSE function not the MATCH function\n    although this will be a recursive parse and so the match\n    function of SUBSECTIONS will be tested if present. The match\n    function of the parent will not be tested.\n    \"\"\"\n    config = FluffConfig(overrides=dict(dialect=dialect))\n    segments = lex(raw, config=config)\n    Seg = validate_segment(segmentref, config=config)\n\n    # Most segments won't handle the end of file marker. We should strip it.\n    if segments[-1].is_type(\"end_of_file\"):\n        segments = segments[:-1]\n\n    ctx = ParseContext.from_config(config)\n    with caplog.at_level(logging.DEBUG):\n        result = Seg.match(segments, 0, parse_context=ctx)\n    assert isinst"}, {"start_line": 1000, "end_line": 2225, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ",\n    LiteralSegment,\n    NewlineSegment,\n    RawSegment,\n    SegmentGenerator,\n    SourceFix,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.types import ParseMode\n\n__all__ = (\n    \"BaseSegment\",\n    \"SourceFix\",\n    \"BaseFileSegment\",\n    \"BracketedSegment\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"SymbolSegment\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"LiteralKeywordSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"SegmentGenerator\",\n    \"Sequence\",\n    \"OneOf\",\n    \"Delimited\",\n    \"Bracketed\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\",\n    \"Conditional\",\n    \"StringParser\",\n    \"MultiStringParser\",\n    \"TypedParser\",\n    \"RegexParser\",\n    \"PositionMarker\",\n    \"Lexer\",\n    \"StringLexer\",\n    \"RegexLexer\",\n    \"Parser\",\n    \"Matchable\",\n    \"ParseMode\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "dialect_ansi.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The core ANSI dialect.\n\nThis is the core SQL grammar. We'll probably extend this or make it pluggable\nfor other dialects. Here we encode the structure of the language.\n\nThere shouldn't be any underlying \"machinery\" here, that should all\nbe defined elsewhere.\n\nA lot of the inspiration for this sql grammar is taken from the cockroach\nlabs full sql grammar. In particular their way for dividing up the expression\ngrammar. Check out their docs, they're awesome.\nhttps://www.cockroachlabs.com/docs/stable/sql-grammar.html#select_stmt\n\"\"\"\n\nfrom collections.abc import Generator\nfrom enum import Enum\nfrom typing import NamedTuple, Optional, Union, cast\n\nfrom sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.dialects.common import AliasInfo, ColumnAliasInfo\nfrom sqlfluff.core.parser import (\n    AnyNumberOf,\n    AnySetOf,\n    Anything,\n    BaseFileSegment,\n    BaseSegment,\n    BinaryOperatorSegment,\n    Bracketed,\n    BracketedSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    Conditional,\n    Dedent,\n    Delimited,\n    IdentifierSegment,\n    ImplicitIndent,\n    Indent,\n    KeywordSegment,\n    LiteralKeywordSegment,\n    LiteralSegment,\n    Matchable,\n    MultiStringParser,\n    NewlineSegment,\n    Nothing,\n    OneOf,\n    OptionallyBracketed,\n    ParseMode,\n    RawSegment,\n    Ref,\n    RegexLexer,\n    RegexParser,\n    SegmentGenerator,\n    Sequence,\n    StringLexer,\n    StringParser,\n    SymbolSegment,\n    TypedParser,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.dialects.dialect_ansi_keywords import (\n    ansi_reserved_keywords,\n    ansi_unreserved_keywords,\n)\n\nansi_dialect = Dialect(\n    \"ansi\",\n    root_segment_name=\"FileSegment\",\n    formatted_name=\"ANSI\",\n    docstring=\"\"\"**Default Casing**: ``UPPERCASE``\n\n**Quotes**: String Literals: ``''``, Identifiers: ``\"\"``\n\nThis is the base dialect which holds most of the definitions of common\nSQL commands "}], "retrieved_count": 10, "cost_time": 0.3370954990386963}
{"question": "How does SQLFluff implement its SQL parsing system?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff implements its SQL parsing system through a multi-stage pipeline that transforms raw SQL text into a structured parse tree. The parsing system works as follows: 1) Templating Stage - Raw SQL is first processed through templating engines (Jinja, Python format strings, dbt) to handle dynamic content and placeholders; 2) Lexing Stage - The templated SQL is tokenized by the Lexer class into RawSegment objects using dialect-specific matchers and patterns; 3) Parsing Stage - The Parser class transforms lexed tokens into a hierarchical parse tree using grammar rules defined in grammar.py, with different grammar types (Sequence, OneOf, Delimited, Bracketed, etc.); 4) Segment System - The parse tree is built using BaseSegment and RawSegment classes, where BaseSegment represents composite nodes and RawSegment represents atomic tokens; 5) Grammar Matching - The parser uses match() methods to recursively apply grammar rules and build the tree structure; 6) Context Management - ParseContext tracks parsing state, position, and dialect-specific information throughout the process; 7) Error Handling - Unparsable segments are wrapped in UnparsableSegment with error information; 8) Dialect Support - Different SQL dialects have specialized grammar rules and segment definitions; 9) Position Tracking - PositionMarker objects maintain source position information for error reporting; 10) Tree Validation - The resulting parse tree is validated for structural correctness and completeness.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 38, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.core.parser.\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parser.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the Parser class.\"\"\"\n\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import check_still_complete\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.parser.segments import BaseFileSegment, BaseSegment\n\n\nclass Parser:\n    \"\"\"Instantiates parsed queries from a sequence of lexed raw segments.\"\"\"\n\n    def __init__(\n        self, config: Optional[FluffConfig] = None, dialect: Optional[str] = None\n    ):\n        if config and dialect:\n            raise ValueError(  # pragma: no cover\n                \"Parser does not support setting both `config` and `dialect`.\"\n            )\n        # Use the provided config or create one from the dialect.\n        self.config = config or FluffConfig.from_kwargs(dialect=dialect)\n        self.RootSegment: type[BaseFileSegment] = self.config.get(\n            \"dialect_obj\"\n        ).get_root_segment()\n\n    def parse(\n        self,\n        segments: Sequence[\"BaseSegment\"],\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> Optional[\"BaseSegment\"]:\n        \"\"\"Parse a series of lexed tokens using the current dialect.\"\"\"\n        if not segments:  # pragma: no cover\n            # This should normally never happen because there will usually\n            # be an end_of_file segment. It would probably only happen in\n            # api use cases.\n            return None\n\n        # NOTE: This is the only time we use the parse context not in the\n        # context of a context manager. That's because it's the initial\n        # instantiation.\n        ctx = ParseContext.from_config(config=self.config)\n        # Kick off parsing with the root segment. The BaseFileSegment has\n        # a unique entry point to facilitate exactly this. All other segments\n        # will use the standard .match() route.\n        root = self.RootSegment.root_pars"}, {"start_line": 1000, "end_line": 2225, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ",\n    LiteralSegment,\n    NewlineSegment,\n    RawSegment,\n    SegmentGenerator,\n    SourceFix,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.types import ParseMode\n\n__all__ = (\n    \"BaseSegment\",\n    \"SourceFix\",\n    \"BaseFileSegment\",\n    \"BracketedSegment\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"SymbolSegment\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"LiteralKeywordSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"SegmentGenerator\",\n    \"Sequence\",\n    \"OneOf\",\n    \"Delimited\",\n    \"Bracketed\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\",\n    \"Conditional\",\n    \"StringParser\",\n    \"MultiStringParser\",\n    \"TypedParser\",\n    \"RegexParser\",\n    \"PositionMarker\",\n    \"Lexer\",\n    \"StringLexer\",\n    \"RegexLexer\",\n    \"Parser\",\n    \"Matchable\",\n    \"ParseMode\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "context.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The parser context.\n\nThis mirrors some of the same design of the flask\ncontext manager. https://flask.palletsprojects.com/en/1.1.x/\n\nThe context acts as a way of keeping track of state, references\nto common configuration and dialects, logging and also the parse\nand match depth of the current operation.\n\"\"\"\n\nimport logging\nimport uuid\nfrom collections import defaultdict\nfrom collections.abc import Iterator, Sequence\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, NoReturn, Optional\n\nfrom tqdm import tqdm\n\nfrom sqlfluff.core.config import progress_bar_configuration\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects.base import Dialect\n    from sqlfluff.core.parser.match_result import MatchResult\n    from sqlfluff.core.parser.matchable import Matchable\n\n# Get the parser logger\nparser_logger = logging.getLogger(\"sqlfluff.parser\")\n\n\nclass ParseContext:\n    \"\"\"Object to handle the context at hand during parsing.\n\n    Holds two tiers of references.\n    1. Persistent config, like references to the dialect or\n       the current verbosity and logger.\n    2. Stack config, like the parse and match depth.\n\n    The manipulation of the stack config is done using a context\n    manager and layered config objects inside the context.\n\n    NOTE: We use context managers here to avoid _copying_\n    the context, just to mutate it safely. This is significantly\n    more performant than the copy operation, but does require some\n    care to use properly.\n\n    When fetching elements from the context, we first look\n    at the top level stack config object and the persistent\n    config values (stored as attributes of the ParseContext\n    itself).\n    \"\"\"\n\n    def __init__(\n        self,\n        dialect: \"Dialect\",\n        indentation_config: Optional[dict[str, Any]] = None,\n    ) -> None:\n        \"\"\"Initialize a new instance of the class.\n\n        Args:\n            dialect (Dialect): The dialect "}, {"start_line": 0, "end_line": 1589, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The core elements of sqlfluff.\"\"\"\n\nimport tblib.pickling_support\n\n# Config objects\nfrom sqlfluff.core.config import FluffConfig\n\n# Dialect introspection\nfrom sqlfluff.core.dialects import dialect_readout, dialect_selector\n\n# All of the errors.\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLFluffUserError,\n    SQLLexError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\n\n# Public classes\nfrom sqlfluff.core.linter import Linter\nfrom sqlfluff.core.parser import Lexer, Parser\n\n# Timing objects\nfrom sqlfluff.core.timing import TimingSummary\n\n__all__ = (\n    \"FluffConfig\",\n    \"Linter\",\n    \"Lexer\",\n    \"Parser\",\n    \"dialect_selector\",\n    \"dialect_readout\",\n    \"SQLBaseError\",\n    \"SQLTemplaterError\",\n    \"SQLLexError\",\n    \"SQLParseError\",\n    \"SQLLintError\",\n    \"SQLFluffUserError\",\n    \"TimingSummary\",\n)\n\n# This is for \"sqlfluff lint\" and \"sqlfluff fix\" multiprocessing (--processes)\n# support. If an exception (i.e. runtime error) occurs in a worker process, we\n# want to return the tracebook to the main process and report it there, as part\n# of the normal output. However, anything returned from a multiprocessing.Pool\n# worker must be serializable using \"pickle\". By default, Python traceback\n# objects cannot be pickled. The tblib package addresses this limitation; we\n# simply need to install it before creating the worker pool. See these links for\n# additional context:\n# https://pypi.org/project/tblib/\n# https://stackoverflow.com/questions/6126007/python-getting-a-traceback-from-a-multiprocessing-process\ntblib.pickling_support.install()\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"init file for the parser.\"\"\"\n\nfrom sqlfluff.core.parser.grammar import (\n    AnyNumberOf,\n    AnySetOf,\n    Anything,\n    Bracketed,\n    Conditional,\n    Delimited,\n    Nothing,\n    OneOf,\n    OptionallyBracketed,\n    OptionallyDelimited,\n    Ref,\n    Sequence,\n)\nfrom sqlfluff.core.parser.lexer import Lexer, RegexLexer, StringLexer\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.parser import Parser\nfrom sqlfluff.core.parser.parsers import (\n    MultiStringParser,\n    RegexParser,\n    StringParser,\n    TypedParser,\n)\nfrom sqlfluff.core.parser.segments import (\n    BaseFileSegment,\n    BaseSegment,\n    BinaryOperatorSegment,\n    BracketedSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    Dedent,\n    IdentifierSegment,\n    ImplicitIndent,\n    Indent,\n    KeywordSegment,\n    LiteralKeywordSegment,\n    LiteralSegment,\n    NewlineSegment,\n    RawSegment,\n    SegmentGenerator,\n    SourceFix,\n    SymbolSegment,\n    UnlexableSegment,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.core.parser.types import ParseMode\n\n__all__ = (\n    \"BaseSegment\",\n    \"SourceFix\",\n    \"BaseFileSegment\",\n    \"BracketedSegment\",\n    \"RawSegment\",\n    \"CodeSegment\",\n    \"UnlexableSegment\",\n    \"CommentSegment\",\n    \"WhitespaceSegment\",\n    \"NewlineSegment\",\n    \"KeywordSegment\",\n    \"SymbolSegment\",\n    \"IdentifierSegment\",\n    \"LiteralSegment\",\n    \"LiteralKeywordSegment\",\n    \"BinaryOperatorSegment\",\n    \"CompositeBinaryOperatorSegment\",\n    \"ComparisonOperatorSegment\",\n    \"CompositeComparisonOperatorSegment\",\n    \"WordSegment\",\n    \"Indent\",\n    \"Dedent\",\n    \"ImplicitIndent\",\n    \"SegmentGenerator\",\n    \"Sequence\",\n    \"OneOf\",\n    \"Delimited\",\n    \"Bracketed\",\n    \"AnyNumberOf\",\n    \"AnySetOf\",\n    \"Ref\",\n    \"Anything\",\n    \"Nothing\",\n    \"OptionallyBracketed\",\n    \"OptionallyDelimited\","}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "lexer.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The code for the Lexer.\"\"\"\n\nimport logging\nfrom collections.abc import Iterator\nfrom typing import Any, NamedTuple, Optional, Union\nfrom uuid import UUID, uuid4\n\nimport regex\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLLexError\nfrom sqlfluff.core.helpers.slice import is_zero_slice, offset_slice, to_tuple\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments import (\n    BaseSegment,\n    Dedent,\n    EndOfFile,\n    Indent,\n    MetaSegment,\n    RawSegment,\n    TemplateLoop,\n    TemplateSegment,\n    UnlexableSegment,\n)\nfrom sqlfluff.core.templaters import TemplatedFile\nfrom sqlfluff.core.templaters.base import TemplatedFileSlice\n\n# Instantiate the lexer logger\nlexer_logger = logging.getLogger(\"sqlfluff.lexer\")\n\n\nclass BlockTracker:\n    \"\"\"This is an object for keeping track of templating blocks.\n\n    Using the .enter() and .exit() methods on opening and closing\n    blocks, we can match up tags of the same level so that later\n    it's easier to treat them the same way in the linting engine.\n\n    In case looping means that we encounter the same block more\n    than once, we use cache uuids against their source location\n    so that if we try to re-enter the block again, it will get\n    the same uuid on the second pass.\n    \"\"\"\n\n    _stack: list[UUID] = []\n    _map: dict[tuple[int, int], UUID] = {}\n\n    def enter(self, src_slice: slice) -> None:\n        \"\"\"Add a block to the stack.\"\"\"\n        key = to_tuple(src_slice)\n        uuid = self._map.get(key, None)\n\n        if not uuid:\n            uuid = uuid4()\n            self._map[key] = uuid\n            lexer_logger.debug(\n                \"        Entering block stack @ %s: %s (fresh)\",\n                src_slice,\n                uuid,\n            )\n        else:\n            lexer_logger.debug(\n                \"        Entering block stack @ %s: %s (cached)\",\n                src_slice,\n                uuid,\n            )\n\n        self._stack.append(uui"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parsers.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Individual segment parsers.\n\nMatchable objects which return individual segments.\n\"\"\"\n\nfrom abc import abstractmethod\nfrom collections.abc import Collection, Sequence\nfrom typing import Any, Callable, Optional\nfrom uuid import uuid4\n\nimport regex\n\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.segments import BaseSegment, RawSegment\nfrom sqlfluff.core.parser.types import SimpleHintType\n\n\nclass BaseParser(Matchable):\n    \"\"\"An abstract class from which other Parsers should inherit.\"\"\"\n\n    # Meta segments are handled separately. All Parser elements\n    # are assumed to be not meta.\n    is_meta: bool = False\n\n    @abstractmethod\n    def __init__(\n        self,\n        raw_class: type[RawSegment],\n        type: Optional[str] = None,\n        optional: bool = False,\n        # The following kwargs are passed on to the segment:\n        trim_chars: Optional[tuple[str, ...]] = None,\n        casefold: Optional[Callable[[str], str]] = None,\n    ) -> None:\n        self.raw_class = raw_class\n        # Store instance_types rather than just type to allow\n        # for multiple possible types to be supported in derivative\n        # classes.\n        self._instance_types: tuple[str, ...] = (type or raw_class.type,)\n        self.optional = optional\n        self._trim_chars = trim_chars\n        self.casefold = casefold\n        # Generate a cache key\n        self._cache_key = uuid4().hex\n\n    def cache_key(self) -> str:\n        \"\"\"Get the cache key for this parser.\n\n        For parsers, they're unique per-instance.\n        \"\"\"\n        return self._cache_key\n\n    def is_optional(self) -> bool:\n        \"\"\"Return whether this element is optional.\"\"\"\n        return self.optional\n\n    def segment_kwargs(self) -> dict[str, Any]:\n        \"\"\"Generates the segment_kwargs package for generating a matched segment.\"\"\"\n        segment_kwargs: dict[str, Any"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "common.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines small container classes to hold intermediate results during linting.\"\"\"\n\nfrom typing import Any, NamedTuple, Optional, Union\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLLexError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.parser.segments.base import BaseSegment\nfrom sqlfluff.core.templaters import TemplatedFile\n\n\nclass RuleTuple(NamedTuple):\n    \"\"\"Rule Tuple object for describing rules.\"\"\"\n\n    code: str\n    name: str\n    description: str\n    groups: tuple[str, ...]\n    aliases: tuple[str, ...]\n\n\nclass RenderedFile(NamedTuple):\n    \"\"\"An object to store the result of a templated file/string.\n\n    This is notable as it's the intermediate state between what happens\n    in the main process and the child processes when running in parallel mode.\n    \"\"\"\n\n    templated_variants: list[TemplatedFile]\n    templater_violations: list[SQLTemplaterError]\n    config: FluffConfig\n    time_dict: dict[str, float]\n    fname: str\n    encoding: str\n    source_str: str\n\n\nclass ParsedVariant(NamedTuple):\n    \"\"\"An object to store the result of parsing a single TemplatedFile.\n\n    Args:\n        templated_file (:obj:`TemplatedFile`): Containing the details\n            of the templated file. If templating fails, this will be `None`.\n        tree (:obj:`BaseSegment`): The segment structure representing the\n            parsed file. If parsing fails due to an unrecoverable\n            violation then we will be None.\n        lexing_violations (:obj:`list` of :obj:`SQLLexError`): Any violations\n            raised during the lexing phase.\n        parsing_violations (:obj:`list` of :obj:`SQLParseError`): Any violations\n            raised during the lexing phase.\n    \"\"\"\n\n    templated_file: TemplatedFile\n    tree: Optional[BaseSegment]\n    lexing_violations: list[SQLLexError]\n    parsing_violations: list[SQLParseError]\n\n    def violations(self) -> list[Union[SQLLexError, SQLParseError]]:\n        \"\"\"Return"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the base dialect class.\"\"\"\n\nimport sys\nfrom typing import Any, Optional, Union, cast\n\nfrom sqlfluff.core.parser import (\n    BaseSegment,\n    KeywordSegment,\n    SegmentGenerator,\n    StringParser,\n)\nfrom sqlfluff.core.parser.grammar.base import BaseGrammar, Nothing\nfrom sqlfluff.core.parser.lexer import LexerType\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.types import BracketPairTuple, DialectElementType\n\n\nclass Dialect:\n    \"\"\"Serves as the basis for runtime resolution of Grammar.\n\n    Args:\n        name (:obj:`str`): The name of the dialect, used for lookup.\n        lexer_matchers (iterable of :obj:`StringLexer`): A structure defining\n            the lexing config for this dialect.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        root_segment_name: str,\n        lexer_matchers: Optional[list[LexerType]] = None,\n        library: Optional[dict[str, DialectElementType]] = None,\n        sets: Optional[dict[str, set[Union[str, BracketPairTuple]]]] = None,\n        inherits_from: Optional[str] = None,\n        formatted_name: Optional[str] = None,\n        docstring: Optional[str] = None,\n    ) -> None:\n        self._library = library or {}\n        self.name = name\n        self.lexer_matchers = lexer_matchers\n        self.expanded = False\n        self._sets = sets or {}\n        self.inherits_from = inherits_from\n        self.root_segment_name = root_segment_name\n        # Attributes for documentation\n        self.formatted_name: str = formatted_name or name\n        self.docstring = docstring or f\"The dialect for {self.formatted_name}.\"\n\n    def __repr__(self) -> str:  # pragma: no cover\n        return f\"<Dialect: {self.name}>\"\n\n    def expand(self) -> \"Dialect\":\n        \"\"\"Expand any callable references to concrete ones.\n\n        This must be called before using the dialect. But\n        allows more flexible definitions to happen at runtime.\n\n        NOTE: This method returns a copy of the current dialect\n  "}], "retrieved_count": 10, "cost_time": 0.34505319595336914}
{"question": "How does SQLFluff implement its multi-dialect support system?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff implements its multi-dialect support system through a modular architecture that allows different SQL dialects to coexist and be selected at runtime. The multi-dialect system works as follows: 1) Dialect Base Class - All dialects inherit from a base Dialect class defined in dialects/base.py, which provides common functionality and interface; 2) Dialect Registration - Each dialect registers itself with the dialect system through entry points or direct registration in dialects/__init__.py; 3) Dialect Selection - Users can select dialects through configuration (dialect setting) or command-line arguments, with the system loading the appropriate dialect implementation; 4) Grammar Specialization - Each dialect defines its own grammar rules in grammar.py, extending or overriding base grammar patterns for dialect-specific syntax; 5) Lexer Customization - Dialects can customize the lexer through get_lexer_matchers() method, adding dialect-specific tokens and patterns; 6) Segment Definitions - Dialects can define custom segment types that handle dialect-specific syntax constructs; 7) Parser Extensions - Dialect-specific parsers can handle unique syntax patterns and constructs not supported by the base parser; 8) Configuration Integration - Dialect settings are integrated into the configuration system, allowing dialect-specific configuration options; 9) Error Handling - Dialect-specific error messages and handling for syntax that's valid in one dialect but not another; 10) Testing Support - Each dialect has its own test suite to ensure dialect-specific features work correctly.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 35, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.dialects.\"\"\"\n"}, {"start_line": 3000, "end_line": 4188, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "kup[label]\n    module = import_module(f\"{base_module}.{module_name}\")\n    result: Dialect = getattr(module, name)\n    result.add_update_segments({k: getattr(module, k) for k in dir(module)})\n    return result\n\n\nclass DialectTuple(NamedTuple):\n    \"\"\"Dialect Tuple object for describing dialects.\"\"\"\n\n    label: str\n    name: str\n    inherits_from: str\n    docstring: str\n\n\ndef dialect_readout() -> Iterator[DialectTuple]:\n    \"\"\"Generate a readout of available dialects.\"\"\"\n    for dialect_label in sorted(_dialect_lookup):\n        dialect = load_raw_dialect(dialect_label)\n        yield DialectTuple(\n            label=dialect_label,\n            name=dialect.formatted_name,\n            inherits_from=dialect.inherits_from or \"nothing\",\n            docstring=dialect.docstring,\n        )\n\n\ndef dialect_selector(s: str) -> Dialect:\n    \"\"\"Return a dialect given its name.\"\"\"\n    dialect = load_raw_dialect(s)\n    # Expand any callable references at this point.\n    # NOTE: The result of .expand() is a new class.\n    return dialect.expand()\n\n\n__all__ = [\n    \"Dialect\",\n    \"DialectTuple\",\n    \"SQLFluffUserError\",\n    \"load_raw_dialect\",\n    \"dialect_readout\",\n    \"dialect_selector\",\n]\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Contains SQL Dialects.\n\nNote that individual dialects are only imported as needed at runtime.\nThis avoids circular references.\n\nTo enable this, any modules outside of .dialects cannot import dialects\ndirectly. They should import `dialect_selector` and use that to fetch\ndialects.\n\nWithin .dialects, each dialect is free to depend on other dialects as\nrequired. Any dependent dialects will be loaded as needed.\n\"\"\"\n\nfrom collections.abc import Iterator\nfrom importlib import import_module\nfrom typing import NamedTuple\n\n# Eventually it would be a good to dynamically discover dialects\n# from any module beginning with \"dialect_\" within this folder.\nfrom sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.errors import SQLFluffUserError\n\n_dialect_lookup = {\n    \"ansi\": (\"dialect_ansi\", \"ansi_dialect\"),\n    \"athena\": (\"dialect_athena\", \"athena_dialect\"),\n    \"bigquery\": (\"dialect_bigquery\", \"bigquery_dialect\"),\n    \"clickhouse\": (\"dialect_clickhouse\", \"clickhouse_dialect\"),\n    \"databricks\": (\"dialect_databricks\", \"databricks_dialect\"),\n    \"db2\": (\"dialect_db2\", \"db2_dialect\"),\n    \"doris\": (\"dialect_doris\", \"doris_dialect\"),\n    \"duckdb\": (\"dialect_duckdb\", \"duckdb_dialect\"),\n    \"exasol\": (\"dialect_exasol\", \"exasol_dialect\"),\n    \"flink\": (\"dialect_flink\", \"flink_dialect\"),\n    \"greenplum\": (\"dialect_greenplum\", \"greenplum_dialect\"),\n    \"hive\": (\"dialect_hive\", \"hive_dialect\"),\n    \"impala\": (\"dialect_impala\", \"impala_dialect\"),\n    \"materialize\": (\"dialect_materialize\", \"materialize_dialect\"),\n    \"mariadb\": (\"dialect_mariadb\", \"mariadb_dialect\"),\n    \"mysql\": (\"dialect_mysql\", \"mysql_dialect\"),\n    \"oracle\": (\"dialect_oracle\", \"oracle_dialect\"),\n    \"postgres\": (\"dialect_postgres\", \"postgres_dialect\"),\n    \"redshift\": (\"dialect_redshift\", \"redshift_dialect\"),\n    \"snowflake\": (\"dialect_snowflake\", \"snowflake_dialect\"),\n    \"soql\": (\"dialect_soql\", \"soql_dialect\"),\n    \"sparksql\": (\"dialect_sparksql\", \"sparksql_dialect\"),\n    \"sqlite\": (\"dialect_sqlite\", \"sql"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ite_dialect\"),\n    \"starrocks\": (\"dialect_starrocks\", \"starrocks_dialect\"),\n    \"teradata\": (\"dialect_teradata\", \"teradata_dialect\"),\n    \"trino\": (\"dialect_trino\", \"trino_dialect\"),\n    \"tsql\": (\"dialect_tsql\", \"tsql_dialect\"),\n    \"vertica\": (\"dialect_vertica\", \"vertica_dialect\"),\n}\n\n_legacy_dialects = {\n    \"exasol_fs\": (\n        \"As of 0.7.0 the 'exasol_fs' dialect has been combined with \"\n        \"the 'exasol' dialect, and is no longer a standalone dialect. \"\n        \"Please use the 'exasol' dialect instead.\"\n    ),\n    \"spark3\": (\n        \"The 'spark3' dialect has been renamed to sparksql. \"\n        \"Please use the 'sparksql' dialect instead.\"\n    ),\n}\n\n\ndef load_raw_dialect(label: str, base_module: str = \"sqlfluff.dialects\") -> Dialect:\n    \"\"\"Dynamically load a dialect.\"\"\"\n    if label in _legacy_dialects:\n        raise SQLFluffUserError(_legacy_dialects[label])\n    elif label not in _dialect_lookup:\n        raise KeyError(\"Unknown dialect\")\n    module_name, name = _dialect_lookup[label]\n    module = import_module(f\"{base_module}.{module_name}\")\n    result: Dialect = getattr(module, name)\n    result.add_update_segments({k: getattr(module, k) for k in dir(module)})\n    return result\n\n\nclass DialectTuple(NamedTuple):\n    \"\"\"Dialect Tuple object for describing dialects.\"\"\"\n\n    label: str\n    name: str\n    inherits_from: str\n    docstring: str\n\n\ndef dialect_readout() -> Iterator[DialectTuple]:\n    \"\"\"Generate a readout of available dialects.\"\"\"\n    for dialect_label in sorted(_dialect_lookup):\n        dialect = load_raw_dialect(dialect_label)\n        yield DialectTuple(\n            label=dialect_label,\n            name=dialect.formatted_name,\n            inherits_from=dialect.inherits_from or \"nothing\",\n            docstring=dialect.docstring,\n        )\n\n\ndef dialect_selector(s: str) -> Dialect:\n    \"\"\"Return a dialect given its name.\"\"\"\n    dialect = load_raw_dialect(s)\n    # Expand any callable references at this point.\n    # NOTE: The result of .expand() is "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the base dialect class.\"\"\"\n\nimport sys\nfrom typing import Any, Optional, Union, cast\n\nfrom sqlfluff.core.parser import (\n    BaseSegment,\n    KeywordSegment,\n    SegmentGenerator,\n    StringParser,\n)\nfrom sqlfluff.core.parser.grammar.base import BaseGrammar, Nothing\nfrom sqlfluff.core.parser.lexer import LexerType\nfrom sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.types import BracketPairTuple, DialectElementType\n\n\nclass Dialect:\n    \"\"\"Serves as the basis for runtime resolution of Grammar.\n\n    Args:\n        name (:obj:`str`): The name of the dialect, used for lookup.\n        lexer_matchers (iterable of :obj:`StringLexer`): A structure defining\n            the lexing config for this dialect.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        root_segment_name: str,\n        lexer_matchers: Optional[list[LexerType]] = None,\n        library: Optional[dict[str, DialectElementType]] = None,\n        sets: Optional[dict[str, set[Union[str, BracketPairTuple]]]] = None,\n        inherits_from: Optional[str] = None,\n        formatted_name: Optional[str] = None,\n        docstring: Optional[str] = None,\n    ) -> None:\n        self._library = library or {}\n        self.name = name\n        self.lexer_matchers = lexer_matchers\n        self.expanded = False\n        self._sets = sets or {}\n        self.inherits_from = inherits_from\n        self.root_segment_name = root_segment_name\n        # Attributes for documentation\n        self.formatted_name: str = formatted_name or name\n        self.docstring = docstring or f\"The dialect for {self.formatted_name}.\"\n\n    def __repr__(self) -> str:  # pragma: no cover\n        return f\"<Dialect: {self.name}>\"\n\n    def expand(self) -> \"Dialect\":\n        \"\"\"Expand any callable references to concrete ones.\n\n        This must be called before using the dialect. But\n        allows more flexible definitions to happen at runtime.\n\n        NOTE: This method returns a copy of the current dialect\n  "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "bricks\": (\"dialect_databricks\", \"databricks_dialect\"),\n    \"db2\": (\"dialect_db2\", \"db2_dialect\"),\n    \"doris\": (\"dialect_doris\", \"doris_dialect\"),\n    \"duckdb\": (\"dialect_duckdb\", \"duckdb_dialect\"),\n    \"exasol\": (\"dialect_exasol\", \"exasol_dialect\"),\n    \"flink\": (\"dialect_flink\", \"flink_dialect\"),\n    \"greenplum\": (\"dialect_greenplum\", \"greenplum_dialect\"),\n    \"hive\": (\"dialect_hive\", \"hive_dialect\"),\n    \"impala\": (\"dialect_impala\", \"impala_dialect\"),\n    \"materialize\": (\"dialect_materialize\", \"materialize_dialect\"),\n    \"mariadb\": (\"dialect_mariadb\", \"mariadb_dialect\"),\n    \"mysql\": (\"dialect_mysql\", \"mysql_dialect\"),\n    \"oracle\": (\"dialect_oracle\", \"oracle_dialect\"),\n    \"postgres\": (\"dialect_postgres\", \"postgres_dialect\"),\n    \"redshift\": (\"dialect_redshift\", \"redshift_dialect\"),\n    \"snowflake\": (\"dialect_snowflake\", \"snowflake_dialect\"),\n    \"soql\": (\"dialect_soql\", \"soql_dialect\"),\n    \"sparksql\": (\"dialect_sparksql\", \"sparksql_dialect\"),\n    \"sqlite\": (\"dialect_sqlite\", \"sqlite_dialect\"),\n    \"starrocks\": (\"dialect_starrocks\", \"starrocks_dialect\"),\n    \"teradata\": (\"dialect_teradata\", \"teradata_dialect\"),\n    \"trino\": (\"dialect_trino\", \"trino_dialect\"),\n    \"tsql\": (\"dialect_tsql\", \"tsql_dialect\"),\n    \"vertica\": (\"dialect_vertica\", \"vertica_dialect\"),\n}\n\n_legacy_dialects = {\n    \"exasol_fs\": (\n        \"As of 0.7.0 the 'exasol_fs' dialect has been combined with \"\n        \"the 'exasol' dialect, and is no longer a standalone dialect. \"\n        \"Please use the 'exasol' dialect instead.\"\n    ),\n    \"spark3\": (\n        \"The 'spark3' dialect has been renamed to sparksql. \"\n        \"Please use the 'sparksql' dialect instead.\"\n    ),\n}\n\n\ndef load_raw_dialect(label: str, base_module: str = \"sqlfluff.dialects\") -> Dialect:\n    \"\"\"Dynamically load a dialect.\"\"\"\n    if label in _legacy_dialects:\n        raise SQLFluffUserError(_legacy_dialects[label])\n    elif label not in _dialect_lookup:\n        raise KeyError(\"Unknown dialect\")\n    module_name, name = _dialect_loo"}, {"start_line": 0, "end_line": 209, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Dialects, segregated to make imports manageable.\n\nNOTE: dialects should not be imported directly from this\nmodule, but should be accessed instead using the selector\nmethods in `sqlfluff.core.dialects`.\n\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "conftest.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Sharing fixtures to test the dialects.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.parser import BaseSegment, Lexer\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\n\n\ndef lex(raw, config):\n    \"\"\"Basic parsing for the tests below.\"\"\"\n    # Set up the lexer\n    lex = Lexer(config=config)\n    # Lex the string for matching. For a good test, this would\n    # arguably happen as a fixture, but it's easier to pass strings\n    # as parameters than pre-lexed segment strings.\n    segments, vs = lex.lex(raw)\n    assert not vs\n    print(segments)\n    return segments\n\n\ndef validate_segment(segmentref, config):\n    \"\"\"Get and validate segment for tests below.\"\"\"\n    Seg = config.get(\"dialect_obj\").ref(segmentref)\n    if isinstance(Seg, Matchable):\n        return Seg\n    try:\n        if issubclass(Seg, BaseSegment):\n            return Seg\n    except TypeError:\n        pass\n    raise TypeError(\n        \"{} is not of type Segment or Matchable. Test is invalid.\".format(segmentref)\n    )\n\n\ndef _dialect_specific_segment_parses(dialect, segmentref, raw, caplog):\n    \"\"\"Test that specific segments parse as expected.\n\n    NB: We're testing the PARSE function not the MATCH function\n    although this will be a recursive parse and so the match\n    function of SUBSECTIONS will be tested if present. The match\n    function of the parent will not be tested.\n    \"\"\"\n    config = FluffConfig(overrides=dict(dialect=dialect))\n    segments = lex(raw, config=config)\n    Seg = validate_segment(segmentref, config=config)\n\n    # Most segments won't handle the end of file marker. We should strip it.\n    if segments[-1].is_type(\"end_of_file\"):\n        segments = segments[:-1]\n\n    ctx = ParseContext.from_config(config)\n    with caplog.at_level(logging.DEBUG):\n        result = Seg.match(segments, 0, parse_context=ctx)\n    assert isinst"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parser.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the Parser class.\"\"\"\n\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import check_still_complete\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.parser.segments import BaseFileSegment, BaseSegment\n\n\nclass Parser:\n    \"\"\"Instantiates parsed queries from a sequence of lexed raw segments.\"\"\"\n\n    def __init__(\n        self, config: Optional[FluffConfig] = None, dialect: Optional[str] = None\n    ):\n        if config and dialect:\n            raise ValueError(  # pragma: no cover\n                \"Parser does not support setting both `config` and `dialect`.\"\n            )\n        # Use the provided config or create one from the dialect.\n        self.config = config or FluffConfig.from_kwargs(dialect=dialect)\n        self.RootSegment: type[BaseFileSegment] = self.config.get(\n            \"dialect_obj\"\n        ).get_root_segment()\n\n    def parse(\n        self,\n        segments: Sequence[\"BaseSegment\"],\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> Optional[\"BaseSegment\"]:\n        \"\"\"Parse a series of lexed tokens using the current dialect.\"\"\"\n        if not segments:  # pragma: no cover\n            # This should normally never happen because there will usually\n            # be an end_of_file segment. It would probably only happen in\n            # api use cases.\n            return None\n\n        # NOTE: This is the only time we use the parse context not in the\n        # context of a context manager. That's because it's the initial\n        # instantiation.\n        ctx = ParseContext.from_config(config=self.config)\n        # Kick off parsing with the root segment. The BaseFileSegment has\n        # a unique entry point to facilitate exactly this. All other segments\n        # will use the standard .match() route.\n        root = self.RootSegment.root_pars"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "dialect_ansi.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The core ANSI dialect.\n\nThis is the core SQL grammar. We'll probably extend this or make it pluggable\nfor other dialects. Here we encode the structure of the language.\n\nThere shouldn't be any underlying \"machinery\" here, that should all\nbe defined elsewhere.\n\nA lot of the inspiration for this sql grammar is taken from the cockroach\nlabs full sql grammar. In particular their way for dividing up the expression\ngrammar. Check out their docs, they're awesome.\nhttps://www.cockroachlabs.com/docs/stable/sql-grammar.html#select_stmt\n\"\"\"\n\nfrom collections.abc import Generator\nfrom enum import Enum\nfrom typing import NamedTuple, Optional, Union, cast\n\nfrom sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.dialects.common import AliasInfo, ColumnAliasInfo\nfrom sqlfluff.core.parser import (\n    AnyNumberOf,\n    AnySetOf,\n    Anything,\n    BaseFileSegment,\n    BaseSegment,\n    BinaryOperatorSegment,\n    Bracketed,\n    BracketedSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    Conditional,\n    Dedent,\n    Delimited,\n    IdentifierSegment,\n    ImplicitIndent,\n    Indent,\n    KeywordSegment,\n    LiteralKeywordSegment,\n    LiteralSegment,\n    Matchable,\n    MultiStringParser,\n    NewlineSegment,\n    Nothing,\n    OneOf,\n    OptionallyBracketed,\n    ParseMode,\n    RawSegment,\n    Ref,\n    RegexLexer,\n    RegexParser,\n    SegmentGenerator,\n    Sequence,\n    StringLexer,\n    StringParser,\n    SymbolSegment,\n    TypedParser,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.dialects.dialect_ansi_keywords import (\n    ansi_reserved_keywords,\n    ansi_unreserved_keywords,\n)\n\nansi_dialect = Dialect(\n    \"ansi\",\n    root_segment_name=\"FileSegment\",\n    formatted_name=\"ANSI\",\n    docstring=\"\"\"**Default Casing**: ``UPPERCASE``\n\n**Quotes**: String Literals: ``''``, Identifiers: ``\"\"``\n\nThis is the base dialect which holds most of the definitions of common\nSQL commands "}], "retrieved_count": 10, "cost_time": 0.33860325813293457}
{"question": "How does SQLFluff handle different SQL dialects and syntax variations?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff handles different SQL dialects and syntax variations through a comprehensive system that adapts parsing, lexing, and linting to dialect-specific requirements. The dialect handling works as follows: 1) Dialect Detection - SQLFluff can automatically detect dialects based on SQL syntax patterns or users can explicitly specify the dialect through configuration; 2) Grammar Adaptation - Each dialect defines its own grammar rules that extend or override base grammar patterns to handle dialect-specific syntax constructs; 3) Lexer Customization - Dialects customize the lexer through get_lexer_matchers() method, adding dialect-specific keywords, operators, and token patterns; 4) Segment Specialization - Dialect-specific segment types handle unique syntax constructs like PostgreSQL's JSON operators or Snowflake's dollar-quoted strings; 5) Parser Extensions - Dialect parsers can handle syntax variations like different function call syntax, window function implementations, or CTE syntax differences; 6) Rule Adaptation - Linting rules can be dialect-aware, applying different standards or skipping certain checks based on dialect capabilities; 7) Configuration Integration - Dialect settings integrate with the configuration system, allowing dialect-specific rule configurations and options; 8) Error Context - Error messages and suggestions are tailored to the specific dialect being used; 9) Template Support - Templating engines can be dialect-aware, handling dialect-specific placeholder syntax; 10) Testing Framework - Each dialect has comprehensive test suites covering dialect-specific syntax variations and edge cases.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 35, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.dialects.\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "bigquery_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests specific to the snowflake dialect.\"\"\"\n\nimport hypothesis.strategies as st\nimport pytest\nfrom hypothesis import example, given, note, settings\n\nfrom sqlfluff.core import FluffConfig\nfrom sqlfluff.core.parser import Lexer, Parser\n\n\n@settings(max_examples=100, deadline=None)\n@given(\n    st.lists(\n        st.tuples(st.sampled_from([\"<\", \"=\", \">\"]), st.sampled_from([\"AND\", \"OR\"])),\n        min_size=1,\n        max_size=30,\n    )\n)\n@example(data=[(\"<\", \"AND\")])\n@example(data=[(\">\", \"AND\")])\n@example(data=[(\"<\", \"AND\"), (\">\", \"AND\")])\n@example(data=[(\"<\", \"AND\"), (\"=\", \"AND\"), (\">\", \"AND\")])\n@example(data=[(\">\", \"AND\"), (\"<\", \"AND\")])\n@example(data=[(\"<\", \"AND\"), (\"<\", \"AND\"), (\">\", \"AND\")])\n@example(data=[(\">\", \"AND\"), (\">\", \"AND\"), (\"<\", \"AND\")])\ndef test_bigquery_relational_operator_parsing(data):\n    \"\"\"Tests queries with a diverse mixture of relational operators.\"\"\"\n    # Generate a simple SELECT query with relational operators and conjunctions\n    # as specified in 'data'. Note the conjunctions are used as separators\n    # between comparisons, sn the conjunction in the first item is not used.\n    filter = []\n    for i, (relation, conjunction) in enumerate(data):\n        if i:\n            filter.append(f\" {conjunction} \")\n        filter.append(f\"a {relation} b\")\n    raw = f'SELECT * FROM t WHERE {\"\".join(filter)}'\n    note(f\"query: {raw}\")\n    # Load the right dialect\n    config = FluffConfig(overrides=dict(dialect=\"bigquery\"))\n    tokens, lex_vs = Lexer(config=config).lex(raw)\n    # From just the initial parse, check we're all there\n    assert \"\".join(token.raw for token in tokens) == raw\n    # Check we don't have lexing issues\n    assert not lex_vs\n\n    # Do the parse WITHOUT lots of logging\n    # The logs get too long here to be useful. We should use\n    # specific segment tests if we want to debug logs.\n    parsed = Parser(config=config).parse(tokens)\n    print(f\"Post-parse structure: {parsed.to_tuple(show_raw=True)}\")\n    print(f\"Post-parse structure: {pa"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "conftest.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Sharing fixtures to test the dialects.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.parser import BaseSegment, Lexer\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.matchable import Matchable\n\n\ndef lex(raw, config):\n    \"\"\"Basic parsing for the tests below.\"\"\"\n    # Set up the lexer\n    lex = Lexer(config=config)\n    # Lex the string for matching. For a good test, this would\n    # arguably happen as a fixture, but it's easier to pass strings\n    # as parameters than pre-lexed segment strings.\n    segments, vs = lex.lex(raw)\n    assert not vs\n    print(segments)\n    return segments\n\n\ndef validate_segment(segmentref, config):\n    \"\"\"Get and validate segment for tests below.\"\"\"\n    Seg = config.get(\"dialect_obj\").ref(segmentref)\n    if isinstance(Seg, Matchable):\n        return Seg\n    try:\n        if issubclass(Seg, BaseSegment):\n            return Seg\n    except TypeError:\n        pass\n    raise TypeError(\n        \"{} is not of type Segment or Matchable. Test is invalid.\".format(segmentref)\n    )\n\n\ndef _dialect_specific_segment_parses(dialect, segmentref, raw, caplog):\n    \"\"\"Test that specific segments parse as expected.\n\n    NB: We're testing the PARSE function not the MATCH function\n    although this will be a recursive parse and so the match\n    function of SUBSECTIONS will be tested if present. The match\n    function of the parent will not be tested.\n    \"\"\"\n    config = FluffConfig(overrides=dict(dialect=dialect))\n    segments = lex(raw, config=config)\n    Seg = validate_segment(segmentref, config=config)\n\n    # Most segments won't handle the end of file marker. We should strip it.\n    if segments[-1].is_type(\"end_of_file\"):\n        segments = segments[:-1]\n\n    ctx = ParseContext.from_config(config)\n    with caplog.at_level(logging.DEBUG):\n        result = Seg.match(segments, 0, parse_context=ctx)\n    assert isinst"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Contains SQL Dialects.\n\nNote that individual dialects are only imported as needed at runtime.\nThis avoids circular references.\n\nTo enable this, any modules outside of .dialects cannot import dialects\ndirectly. They should import `dialect_selector` and use that to fetch\ndialects.\n\nWithin .dialects, each dialect is free to depend on other dialects as\nrequired. Any dependent dialects will be loaded as needed.\n\"\"\"\n\nfrom collections.abc import Iterator\nfrom importlib import import_module\nfrom typing import NamedTuple\n\n# Eventually it would be a good to dynamically discover dialects\n# from any module beginning with \"dialect_\" within this folder.\nfrom sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.errors import SQLFluffUserError\n\n_dialect_lookup = {\n    \"ansi\": (\"dialect_ansi\", \"ansi_dialect\"),\n    \"athena\": (\"dialect_athena\", \"athena_dialect\"),\n    \"bigquery\": (\"dialect_bigquery\", \"bigquery_dialect\"),\n    \"clickhouse\": (\"dialect_clickhouse\", \"clickhouse_dialect\"),\n    \"databricks\": (\"dialect_databricks\", \"databricks_dialect\"),\n    \"db2\": (\"dialect_db2\", \"db2_dialect\"),\n    \"doris\": (\"dialect_doris\", \"doris_dialect\"),\n    \"duckdb\": (\"dialect_duckdb\", \"duckdb_dialect\"),\n    \"exasol\": (\"dialect_exasol\", \"exasol_dialect\"),\n    \"flink\": (\"dialect_flink\", \"flink_dialect\"),\n    \"greenplum\": (\"dialect_greenplum\", \"greenplum_dialect\"),\n    \"hive\": (\"dialect_hive\", \"hive_dialect\"),\n    \"impala\": (\"dialect_impala\", \"impala_dialect\"),\n    \"materialize\": (\"dialect_materialize\", \"materialize_dialect\"),\n    \"mariadb\": (\"dialect_mariadb\", \"mariadb_dialect\"),\n    \"mysql\": (\"dialect_mysql\", \"mysql_dialect\"),\n    \"oracle\": (\"dialect_oracle\", \"oracle_dialect\"),\n    \"postgres\": (\"dialect_postgres\", \"postgres_dialect\"),\n    \"redshift\": (\"dialect_redshift\", \"redshift_dialect\"),\n    \"snowflake\": (\"dialect_snowflake\", \"snowflake_dialect\"),\n    \"soql\": (\"dialect_soql\", \"soql_dialect\"),\n    \"sparksql\": (\"dialect_sparksql\", \"sparksql_dialect\"),\n    \"sqlite\": (\"dialect_sqlite\", \"sql"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "dialect_ansi.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The core ANSI dialect.\n\nThis is the core SQL grammar. We'll probably extend this or make it pluggable\nfor other dialects. Here we encode the structure of the language.\n\nThere shouldn't be any underlying \"machinery\" here, that should all\nbe defined elsewhere.\n\nA lot of the inspiration for this sql grammar is taken from the cockroach\nlabs full sql grammar. In particular their way for dividing up the expression\ngrammar. Check out their docs, they're awesome.\nhttps://www.cockroachlabs.com/docs/stable/sql-grammar.html#select_stmt\n\"\"\"\n\nfrom collections.abc import Generator\nfrom enum import Enum\nfrom typing import NamedTuple, Optional, Union, cast\n\nfrom sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.dialects.common import AliasInfo, ColumnAliasInfo\nfrom sqlfluff.core.parser import (\n    AnyNumberOf,\n    AnySetOf,\n    Anything,\n    BaseFileSegment,\n    BaseSegment,\n    BinaryOperatorSegment,\n    Bracketed,\n    BracketedSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    CompositeBinaryOperatorSegment,\n    CompositeComparisonOperatorSegment,\n    Conditional,\n    Dedent,\n    Delimited,\n    IdentifierSegment,\n    ImplicitIndent,\n    Indent,\n    KeywordSegment,\n    LiteralKeywordSegment,\n    LiteralSegment,\n    Matchable,\n    MultiStringParser,\n    NewlineSegment,\n    Nothing,\n    OneOf,\n    OptionallyBracketed,\n    ParseMode,\n    RawSegment,\n    Ref,\n    RegexLexer,\n    RegexParser,\n    SegmentGenerator,\n    Sequence,\n    StringLexer,\n    StringParser,\n    SymbolSegment,\n    TypedParser,\n    WhitespaceSegment,\n    WordSegment,\n)\nfrom sqlfluff.dialects.dialect_ansi_keywords import (\n    ansi_reserved_keywords,\n    ansi_unreserved_keywords,\n)\n\nansi_dialect = Dialect(\n    \"ansi\",\n    root_segment_name=\"FileSegment\",\n    formatted_name=\"ANSI\",\n    docstring=\"\"\"**Default Casing**: ``UPPERCASE``\n\n**Quotes**: String Literals: ``''``, Identifiers: ``\"\"``\n\nThis is the base dialect which holds most of the definitions of common\nSQL commands "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "flink_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the FlinkSQL dialect.\"\"\"\n\nfrom sqlfluff.core import FluffConfig, Linter\n\n\nclass TestFlinkSQLDialect:\n    \"\"\"Test FlinkSQL dialect parsing.\"\"\"\n\n    def test_flink_dialect_basic(self):\n        \"\"\"Test basic FlinkSQL dialect functionality.\"\"\"\n        config = FluffConfig(overrides={\"dialect\": \"flink\"})\n        linter = Linter(config=config)\n\n        # Test simple SELECT statement\n        sql = \"SELECT * FROM my_table;\\n\"\n        result = linter.lint_string(sql)\n        assert result is not None\n        # Check for parsing errors only, ignore style warnings\n        parsing_errors = [v for v in result.violations if v.rule.code.startswith(\"PRS\")]\n        assert len(parsing_errors) == 0\n\n    def test_flink_create_table_basic(self):\n        \"\"\"Test basic CREATE TABLE statement.\"\"\"\n        config = FluffConfig(overrides={\"dialect\": \"flink\"})\n        linter = Linter(config=config)\n\n        sql = \"\"\"\n        CREATE TABLE my_table (\n            id INT,\n            name STRING,\n            age INT\n        ) WITH (\n            'connector' = 'kafka',\n            'topic' = 'my-topic'\n        )\n        \"\"\"\n        result = linter.lint_string(sql)\n        assert result is not None\n        # Allow for some parsing issues initially\n\n    def test_flink_row_data_type(self):\n        \"\"\"Test FlinkSQL ROW data type.\"\"\"\n        config = FluffConfig(overrides={\"dialect\": \"flink\"})\n        linter = Linter(config=config)\n\n        sql = \"\"\"\n        CREATE TABLE my_table (\n            id INT,\n            nested_data ROW<name STRING, age INT>\n        ) WITH (\n            'connector' = 'kafka'\n        )\n        \"\"\"\n        result = linter.lint_string(sql)\n        assert result is not None\n\n    def test_flink_timestamp_with_precision(self):\n        \"\"\"Test FlinkSQL TIMESTAMP with precision.\"\"\"\n        config = FluffConfig(overrides={\"dialect\": \"flink\"})\n        linter = Linter(config=config)\n\n        sql = \"\"\"\n        CREATE TABLE my_table (\n            id INT,\n            event_time TI"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "dialect_tsql.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "mpression\",\n        )\n    ),\n    FileEncodingSegment=SegmentGenerator(\n        lambda dialect: MultiStringParser(\n            dialect.sets(\"file_encoding\"),\n            CodeSegment,\n            type=\"file_encoding\",\n        )\n    ),\n    SerdeMethodSegment=SegmentGenerator(\n        lambda dialect: MultiStringParser(\n            dialect.sets(\"serde_method\"),\n            CodeSegment,\n            type=\"serde_method\",\n        )\n    ),\n    ProcedureParameterGrammar=Sequence(\n        Ref(\"ParameterNameSegment\", optional=True),\n        Sequence(\"AS\", optional=True),\n        Ref(\"DatatypeSegment\"),\n        AnySetOf(\"VARYING\", Sequence(\"NOT\", optional=True), \"NULL\"),\n        Sequence(Ref(\"EqualsSegment\"), Ref(\"ExpressionSegment\"), optional=True),\n    ),\n    DateFormatSegment=SegmentGenerator(\n        lambda dialect: MultiStringParser(\n            dialect.sets(\"date_format\"),\n            CodeSegment,\n            type=\"date_format\",\n        )\n    ),\n    # Here we add a special case for a DotSegment where we don't want to apply\n    # LT01's respace rule.\n    LeadingDotSegment=StringParser(\".\", SymbolSegment, type=\"leading_dot\"),\n    HexadecimalLiteralSegment=RegexParser(\n        r\"([xX]'([\\da-fA-F][\\da-fA-F])+'|0x[\\da-fA-F]+)\",\n        LiteralSegment,\n        type=\"numeric_literal\",\n    ),\n    PlusComparisonSegment=StringParser(\n        \"+\", SymbolSegment, type=\"raw_comparison_operator\"\n    ),\n    MinusComparisonSegment=StringParser(\n        \"-\", SymbolSegment, type=\"raw_comparison_operator\"\n    ),\n    MultiplyComparisonSegment=StringParser(\n        \"*\", SymbolSegment, type=\"raw_comparison_operator\"\n    ),\n    DivideComparisonSegment=StringParser(\n        \"/\", SymbolSegment, type=\"raw_comparison_operator\"\n    ),\n    ModuloComparisonSegment=StringParser(\n        \"%\", SymbolSegment, type=\"raw_comparison_operator\"\n    ),\n)\n\ntsql_dialect.replace(\n    # Overriding to cover TSQL allowed identifier name characters\n    # https://docs.microsoft.com/en-us/sql/relational-databases/database"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "dialect_sparksql.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The ANSI Compliant SparkSQL dialect.\n\nInherits from ANSI.\nSpark SQL ANSI Mode is more restrictive regarding\nkeywords than the Default Mode, and still shares\nsome syntax with hive.\n\nBased on:\nhttps://spark.apache.org/docs/latest/sql-ref.html\nhttps://spark.apache.org/docs/latest/sql-ref-ansi-compliance.html\nhttps://github.com/apache/spark/blob/master/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4\n\"\"\"\n\nfrom sqlfluff.core.dialects import load_raw_dialect\nfrom sqlfluff.core.parser import (\n    AnyNumberOf,\n    AnySetOf,\n    Anything,\n    BaseSegment,\n    Bracketed,\n    BracketedSegment,\n    CodeSegment,\n    CommentSegment,\n    ComparisonOperatorSegment,\n    Conditional,\n    Dedent,\n    Delimited,\n    IdentifierSegment,\n    ImplicitIndent,\n    Indent,\n    KeywordSegment,\n    LiteralSegment,\n    Matchable,\n    MultiStringParser,\n    OneOf,\n    OptionallyBracketed,\n    ParseMode,\n    Ref,\n    RegexLexer,\n    RegexParser,\n    Sequence,\n    StringLexer,\n    StringParser,\n    SymbolSegment,\n    TypedParser,\n    WordSegment,\n)\nfrom sqlfluff.dialects import dialect_ansi as ansi\nfrom sqlfluff.dialects import dialect_hive as hive\nfrom sqlfluff.dialects.dialect_sparksql_keywords import (\n    RESERVED_KEYWORDS,\n    UNRESERVED_KEYWORDS,\n)\n\nansi_dialect = load_raw_dialect(\"ansi\")\nhive_dialect = load_raw_dialect(\"hive\")\nsparksql_dialect = ansi_dialect.copy_as(\n    \"sparksql\",\n    formatted_name=\"Apache Spark SQL\",\n    docstring=\"\"\"**Default Casing**: SparkSQL is case insensitive with\nboth quoted and unquoted identifiers (_\"delimited\"_ identifiers in\nSpark terminology). See the `Spark Identifiers`_ docs.\n\n**Quotes**: String Literals: ``''`` or ``\"\"``, Identifiers: |back_quotes|.\n\nThe dialect for Apache `Spark SQL`_. This includes relevant\nsyntax from :ref:`hive_dialect_ref` for commands that permit Hive Format.\nSpark SQL extensions provided by the `Delta Lake`_ project are also implemented\nin this dialect.\n\nThis implementation focuses on the `Ansi Compli"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "dialect_snowflake.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The Snowflake dialect.\n\nInherits from ANSI.\n\nBased on https://docs.snowflake.com/en/sql-reference-commands.html\n\"\"\"\n\nfrom sqlfluff.core.dialects import load_raw_dialect\nfrom sqlfluff.core.parser import (\n    AnyNumberOf,\n    AnySetOf,\n    Anything,\n    BaseSegment,\n    Bracketed,\n    CodeSegment,\n    CommentSegment,\n    Dedent,\n    Delimited,\n    IdentifierSegment,\n    ImplicitIndent,\n    Indent,\n    KeywordSegment,\n    LiteralSegment,\n    Matchable,\n    MultiStringParser,\n    Nothing,\n    OneOf,\n    OptionallyBracketed,\n    OptionallyDelimited,\n    ParseMode,\n    Ref,\n    RegexLexer,\n    RegexParser,\n    SegmentGenerator,\n    Sequence,\n    StringLexer,\n    StringParser,\n    SymbolSegment,\n    TypedParser,\n)\nfrom sqlfluff.dialects import dialect_ansi as ansi\nfrom sqlfluff.dialects.dialect_snowflake_keywords import (\n    snowflake_reserved_keywords,\n    snowflake_unreserved_keywords,\n)\n\nansi_dialect = load_raw_dialect(\"ansi\")\nsnowflake_dialect = ansi_dialect.copy_as(\n    \"snowflake\",\n    formatted_name=\"Snowflake\",\n    docstring=\"\"\"**Default Casing**: ``UPPERCASE``\n\n**Quotes**: String Literals: ``''``, Identifiers: ``\"\"``\n\nThe dialect for\n`Snowflake <https://docs.snowflake.com/en/sql-reference.html>`_,\nwhich has much of its syntax inherited from :ref:`postgres_dialect_ref`.\"\"\",\n)\n\nsnowflake_dialect.patch_lexer_matchers(\n    [\n        # In snowflake, a double single quote resolves as a single quote in the string.\n        # https://docs.snowflake.com/en/sql-reference/data-types-text.html#single-quoted-string-constants\n        RegexLexer(\n            \"single_quote\",\n            r\"'([^'\\\\]|\\\\.|'')*'\",\n            CodeSegment,\n        ),\n        RegexLexer(\n            \"inline_comment\",\n            r\"(--|#|//)[^\\n]*\",\n            CommentSegment,\n            segment_kwargs={\"trim_start\": (\"--\", \"#\", \"//\")},\n        ),\n    ]\n)\n\nsnowflake_dialect.insert_lexer_matchers(\n    [\n        # Keyword assigner needed for keyword functions.\n        StringLexer(\"parameter_assigner\","}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "dialect_sparksql.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "gParser,\n    SymbolSegment,\n    TypedParser,\n    WordSegment,\n)\nfrom sqlfluff.dialects import dialect_ansi as ansi\nfrom sqlfluff.dialects import dialect_hive as hive\nfrom sqlfluff.dialects.dialect_sparksql_keywords import (\n    RESERVED_KEYWORDS,\n    UNRESERVED_KEYWORDS,\n)\n\nansi_dialect = load_raw_dialect(\"ansi\")\nhive_dialect = load_raw_dialect(\"hive\")\nsparksql_dialect = ansi_dialect.copy_as(\n    \"sparksql\",\n    formatted_name=\"Apache Spark SQL\",\n    docstring=\"\"\"**Default Casing**: SparkSQL is case insensitive with\nboth quoted and unquoted identifiers (_\"delimited\"_ identifiers in\nSpark terminology). See the `Spark Identifiers`_ docs.\n\n**Quotes**: String Literals: ``''`` or ``\"\"``, Identifiers: |back_quotes|.\n\nThe dialect for Apache `Spark SQL`_. This includes relevant\nsyntax from :ref:`hive_dialect_ref` for commands that permit Hive Format.\nSpark SQL extensions provided by the `Delta Lake`_ project are also implemented\nin this dialect.\n\nThis implementation focuses on the `Ansi Compliant Mode`_ introduced in\nSpark3, instead of being Hive Compliant. The introduction of ANSI Compliance\nprovides better data quality and easier migration from traditional DBMS.\n\nVersions of Spark prior to 3.x will only support the Hive dialect.\n\n.. _`Spark SQL`: https://spark.apache.org/docs/latest/sql-ref.html\n.. _`Delta Lake`: https://docs.delta.io/latest/quick-start.html#set-up-apache-spark-with-delta-lake\n.. _`Ansi Compliant Mode`: https://spark.apache.org/docs/latest/sql-ref-ansi-compliance.html\n.. _`Spark Identifiers`: https://spark.apache.org/docs/latest/sql-ref-identifier.html\"\"\",  # noqa: E501\n)\n\nsparksql_dialect.patch_lexer_matchers(\n    [\n        # Spark SQL, only -- is used for single-line comment\n        RegexLexer(\n            \"inline_comment\",\n            r\"(--)[^\\n]*\",\n            CommentSegment,\n            segment_kwargs={\"trim_start\": \"--\"},\n        ),\n        # == and <=> are valid equal operations\n        # <=> is a non-null equals in Spark SQL\n        # https://spar"}], "retrieved_count": 10, "cost_time": 0.34119272232055664}
{"question": "How does SQLFluff's rule system work?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's rule system works through a hierarchical architecture that enables modular, configurable linting rules. The rule system operates as follows: 1) BaseRule Class - All rules inherit from BaseRule class defined in base.py, which provides common functionality including configuration handling, metadata management, and the core _eval() method; 2) RuleMetaclass - Automatically registers rules and manages rule metadata (code, name, description, groups, aliases) through metaclass processing; 3) RuleSet Management - The RuleSet class manages collections of rules, handles rule filtering, validation, and provides methods for rule discovery and application; 4) Rule Evaluation - Each rule implements an _eval() method that receives a segment and context, analyzes the parse tree, and returns violations or fixes; 5) Configuration Integration - Rules integrate with the configuration system through config_info.py, allowing per-rule configuration and rule enablement/disablement; 6) Rule Categorization - Rules are organized into groups (Core, Layout, References, etc.) and can be referenced by code, name, alias, or group; 7) Fix Generation - Rules can generate LintFix objects that describe how to automatically correct violations; 8) Rule Context - The rule context provides access to parse tree, configuration, and other contextual information during evaluation; 9) Rule Flow Control - The rule system supports different linting phases and crawl behaviors for efficient rule application; 10) Plugin Support - The rule system supports custom rules through plugin mechanisms, allowing third-party rule development.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 1349, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Configuration and examples for individual rules.\"\"\"\n\nfrom sqlfluff.core.plugin.host import get_plugin_manager\nfrom sqlfluff.core.rules.base import (\n    BaseRule,\n    EvalResultType,\n    LintResult,\n    RuleGhost,\n    RulePack,\n    RuleSet,\n)\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.fix import LintFix\n\n\ndef _load_standard_rules() -> RuleSet:\n    \"\"\"Initialise the standard ruleset.\n\n    We do this on each call so that dynamic rules changes\n    are possible.\n    \"\"\"\n    std_rule_set = RuleSet(name=\"standard\", config_info=get_config_info())\n\n    # Iterate through the rules list and register each rule with the standard set.\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            std_rule_set.register(rule)\n\n    return std_rule_set\n\n\ndef get_ruleset(name: str = \"standard\") -> RuleSet:\n    \"\"\"Get a ruleset by name.\"\"\"\n    std_rules = _load_standard_rules()\n    lookup = {std_rules.name: std_rules}\n    # Return a copy in case someone modifies the register.\n    return lookup[name].copy()\n\n\n__all__ = (\n    \"get_ruleset\",\n    \"RuleSet\",\n    \"RulePack\",\n    \"BaseRule\",\n    \"LintResult\",\n    \"LintFix\",\n    \"RuleContext\",\n    \"RuleGhost\",\n    \"EvalResultType\",\n    \"ConfigInfo\",\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "rules_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the standard set of rules.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import Linter\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.linter import RuleTuple\nfrom sqlfluff.core.parser import WhitespaceSegment\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.rules import BaseRule, LintFix, LintResult, get_ruleset\nfrom sqlfluff.core.rules.crawlers import RootOnlyCrawler, SegmentSeekerCrawler\nfrom sqlfluff.core.rules.doc_decorators import (\n    document_configuration,\n    document_fix_compatible,\n    document_groups,\n)\nfrom sqlfluff.core.rules.loader import get_rules_from_path\nfrom sqlfluff.core.templaters.base import TemplatedFile\nfrom sqlfluff.utils.testing.logging import fluff_log_catcher\nfrom sqlfluff.utils.testing.rules import get_rule_from_set\nfrom test.fixtures.rules.custom.L000 import Rule_L000\nfrom test.fixtures.rules.custom.S000 import Rule_S000\n\n\nclass Rule_T042(BaseRule):\n    \"\"\"A dummy rule.\"\"\"\n\n    groups = (\"all\",)\n\n    def _eval(self, context):\n        pass\n\n\nclass Rule_T001(BaseRule):\n    \"\"\"A deliberately malicious rule.\n\n    **Anti-pattern**\n\n    Blah blah\n    \"\"\"\n\n    groups = (\"all\",)\n    crawl_behaviour = SegmentSeekerCrawler({\"whitespace\"})\n    is_fix_compatible = True\n\n    def _eval(self, context):\n        \"\"\"Stars make newlines.\"\"\"\n        if context.segment.is_type(\"whitespace\"):\n            return LintResult(\n                anchor=context.segment,\n                fixes=[\n                    LintFix.replace(\n                        context.segment, [WhitespaceSegment(context.segment.raw + \" \")]\n                    )\n                ],\n            )\n\n\nclass Rule_T002(BaseRule):\n    \"\"\"A rule which says all raw code segments are bad.\n\n    This is used for testing unparsable code.\n    \"\"\"\n\n    groups = (\"all\",)\n    # Root only crawler so that the in-rule filters don't kick in.\n    crawl_behaviour = RootOnlyCrawler()\n\n    def _eval(s"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Implements the base rule class.\n\nRules crawl through the trees returned by the parser and evaluate particular\nrules.\n\nThe intent is that it should be possible for the rules to be expressed\nas simply as possible, with as much of the complexity abstracted away.\n\nThe evaluation function should take enough arguments that it can evaluate\nthe position of the given segment in relation to its neighbors, and that\nthe segment which finally \"triggers\" the error, should be the one that would\nbe corrected OR if the rule relates to something that is missing, then it\nshould flag on the segment FOLLOWING, the place that the desired element is\nmissing.\n\"\"\"\n\nimport bdb\nimport copy\nimport fnmatch\nimport logging\nimport pathlib\nimport re\nfrom collections import defaultdict, namedtuple\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    ClassVar,\n    DefaultDict,\n    Optional,\n    Union,\n)\n\nimport regex\n\nfrom sqlfluff.core.errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a "}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "layed in the sqlfluff\n    docs.\n    \"\"\"\n\n    # Precompile the regular expressions\n    _doc_search_regex: ClassVar = re.compile(\n        \"(\\\\s{4}\\\\*\\\\*Anti-pattern\\\\*\\\\*|\\\\s{4}\\\\.\\\\. note::|\"\n        \"\\\\s\\\\s{4}\\\\*\\\\*Configuration\\\\*\\\\*)\",\n        flags=re.MULTILINE,\n    )\n    _valid_classname_regex: ClassVar = regex.compile(\n        r\"Rule_?([A-Z]{1}[a-zA-Z]+)?_([A-Z0-9]{4})\"\n    )\n    _valid_rule_name_regex: ClassVar = regex.compile(r\"[a-z][a-z\\.\\_]+\")\n\n    @staticmethod\n    def _populate_code_and_description(\n        name: str, class_dict: dict[str, Any]\n    ) -> dict[str, Any]:\n        \"\"\"Extract and validate the rule code & description.\n\n        We expect that rules are defined as classes with the name `Rule_XXXX`\n        where `XXXX` is of the form `LLNN`, where L is a letter and N is a\n        two digit number. For backward compatibility we also still support\n        the legacy format of LNNN i.e. a single letter and three digit number.\n\n        The two letters should be indicative of the grouping and focus of\n        the rule. e.g. capitalisation rules have the code CP for CaPitalisation.\n\n        If this receives classes by any other name, then it will raise a\n        :exc:`ValueError`.\n        \"\"\"\n        rule_name_match = RuleMetaclass._valid_classname_regex.match(name)\n        # Validate the name\n        if not rule_name_match:  # pragma: no cover\n            raise SQLFluffUserError(\n                f\"Tried to define rule class with \"\n                f\"unexpected format: {name}. Format should be: \"\n                \"'Rule_PluginName_LL23' (for plugins) or \"\n                \"`Rule_LL23` (for core rules).\"\n            )\n\n        plugin_name, code = rule_name_match.groups()\n        # If the docstring is multiline, then we extract just summary.\n        description = class_dict[\"__doc__\"].replace(\"``\", \"'\").split(\"\\n\")[0]\n        if plugin_name:\n            code = f\"{plugin_name}_{code}\"\n\n        class_dict[\"code\"] = code\n        class_dict[\"description\"] = desc"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "sqlfluff_domain.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/docs/source/_ext", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"The sqlfluff domain for documenting rules.\"\"\"\n\nfrom sphinx import addnodes\nfrom sphinx.directives import ObjectDescription\nfrom sphinx.domains import Domain, ObjType\nfrom sphinx.roles import XRefRole\nfrom sphinx.util.nodes import make_refnode\n\n\nclass SQLFluffRule(ObjectDescription):\n    \"\"\"SQLFluff rule directive for sphinx.\n\n    Rule directives can be used as shown below.\n\n    .. code-block:: rst\n\n        .. sqlfluff:rule:: AM01\n                           ambiguous.distinct\n\n            Write the documentation for the rule here.\n\n    To cross reference (i.e. refer to) objects defined like this\n    both the code and name reference is available:\n\n    .. code-block:: rst\n\n        :sqlfluff:ref:`CP02`\n        :sqlfluff:ref:`capitalisation.identifiers`\n\n    \"\"\"\n\n    def handle_signature(self, sig, signode):\n        \"\"\"Handle the initial signature of the node.\n\n        This formats the header of the section.\n        \"\"\"\n        raw_obj_type = \"code\" if len(sig) == 4 else \"rule\"\n        obj_type = raw_obj_type.capitalize() + \" \"\n        signode += addnodes.desc_type(obj_type, obj_type)\n        signode += addnodes.desc_name(sig, sig)\n\n        fullname = obj_type + sig\n        signode[\"type\"] = raw_obj_type\n        signode[\"sig\"] = sig\n        signode[\"fullname\"] = fullname\n        return (fullname, raw_obj_type, sig)\n\n    def add_target_and_index(self, name_cls, sig, signode):\n        \"\"\"Hook to add the permalink and index entries.\"\"\"\n        # Add an ID for permalinks\n        node_id = \"rule\" + \"-\" + sig\n        signode[\"ids\"].append(node_id)\n        if len(sig) == 4:\n            # If it's a code, add support for legacy links too.\n            # Both of these formats have been used in the past.\n            signode[\"ids\"].append(f\"sqlfluff.rules.Rule_{sig}\")\n            signode[\"ids\"].append(f\"sqlfluff.rules.sphinx.Rule_{sig}\")\n        # Add to domain for xref resolution\n        fluff = self.env.get_domain(\"sqlfluff\")\n        fluff.add_rule(sig)\n        # Add to index\n  "}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tion for the base classes\n        if name not in (\"BaseRule\",):\n            class_dict = RuleMetaclass._populate_code_and_description(name, class_dict)\n        # Validate rule names\n        rule_name = class_dict.get(\"name\", \"\")\n        if rule_name:\n            if not RuleMetaclass._valid_rule_name_regex.match(rule_name):\n                raise SQLFluffUserError(\n                    f\"Tried to define rule with unexpected \"\n                    f\"name format: {rule_name}. Rule names should be lowercase \"\n                    \"and snake_case with optional `.` characters to indicate \"\n                    \"a namespace or grouping. e.g. `layout.spacing`.\"\n                )\n\n        # Use the stock __new__ method now we've adjusted the docstring.\n        # There are no overload variants of type.__new__ that are compatible, so\n        # we ignore type checking in this case.\n        return super().__new__(mcs, name, bases, class_dict)  # type: ignore\n\n\nclass BaseRule(metaclass=RuleMetaclass):\n    \"\"\"The base class for a rule.\n\n    Args:\n        code (:obj:`str`): The identifier for this rule, used in inclusion\n            or exclusion.\n        description (:obj:`str`): A human readable description of what this\n            rule does. It will be displayed when any violations are found.\n\n    \"\"\"\n\n    _check_docstring = True\n    _works_on_unparsable = True\n    _adjust_anchors = False\n    targets_templated = False\n    # Some fix routines do their own checking for whether their fixes\n    # are safe around templated elements. For those - the default\n    # safety checks might be inappropriate. In those cases, set\n    # template_safe_fixes to True.\n    template_safe_fixes = False\n\n    # Config settings supported for this rule.\n    # See config_info.py for supported values.\n    config_keywords: list[str] = []\n    # Lint loop / crawl behavior. When appropriate, rules can (and should)\n    # override these values to make linting faster.\n    crawl_behaviour: BaseCrawler\n    # Rules can ove"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`Lin"}, {"start_line": 0, "end_line": 1576, "belongs_to": {"file_name": "loader.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Methods to load rules.\"\"\"\n\nimport os\nfrom glob import glob\nfrom importlib import import_module\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.rules.base import BaseRule\n\n\ndef get_rules_from_path(\n    # All rule files are expected in the format of L*.py\n    rules_path: str = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), \"../../rules\", \"L*.py\")\n    ),\n    base_module: str = \"sqlfluff.rules\",\n) -> list[type[\"BaseRule\"]]:\n    \"\"\"Reads all of the Rule classes from a path into a list.\"\"\"\n    # Create a rules dictionary for importing in\n    # sqlfluff/src/sqlfluff/core/rules/__init__.py\n    rules = []\n\n    for module in sorted(glob(rules_path)):\n        # Manipulate the module path to extract the filename without the .py\n        rule_id = os.path.splitext(os.path.basename(module))[0]\n        # All rule classes are expected in the format of Rule_L*\n        rule_class_name = f\"Rule_{rule_id}\"\n        # NOTE: We import the module outside of the try clause to\n        # properly catch any import errors.\n        rule_module = import_module(f\"{base_module}.{rule_id}\")\n        try:\n            rule_class = getattr(rule_module, rule_class_name)\n        except AttributeError as e:\n            raise AttributeError(\n                \"Rule classes must be named in the format of Rule_*. \"\n                f\"[{rule_class_name}]\"\n            ) from e\n        # Add the rules to the rules dictionary for\n        # sqlfluff/src/sqlfluff/core/rules/__init__.py\n        rules.append(rule_class)\n\n    return rules\n"}, {"start_line": 1000, "end_line": 2820, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e that currently, the gap _before_ and _after_ the semicolon \"\n                \"is considered 'between' statements.\"\n            ),\n        },\n        \"maximum_empty_lines_inside_statements\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The maximum number of empty lines allowed inside statements.\"\n            ),\n        },\n        \"wildcard_policy\": {\n            \"validation\": [\"single\", \"multiple\"],\n            \"definition\": \"Treatment of wildcards. Defaults to ``single``.\",\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.layout.LT01 import Rule_LT01\n    from sqlfluff.rules.layout.LT02 import Rule_LT02\n    from sqlfluff.rules.layout.LT03 import Rule_LT03\n    from sqlfluff.rules.layout.LT04 import Rule_LT04\n    from sqlfluff.rules.layout.LT05 import Rule_LT05\n    from sqlfluff.rules.layout.LT06 import Rule_LT06\n    from sqlfluff.rules.layout.LT07 import Rule_LT07\n    from sqlfluff.rules.layout.LT08 import Rule_LT08\n    from sqlfluff.rules.layout.LT09 import Rule_LT09\n    from sqlfluff.rules.layout.LT10 import Rule_LT10\n    from sqlfluff.rules.layout.LT11 import Rule_LT11\n    from sqlfluff.rules.layout.LT12 import Rule_LT12\n    from sqlfluff.rules.layout.LT13 import Rule_LT13\n    from sqlfluff.rules.layout.LT14 import Rule_LT14\n    from sqlfluff.rules.layout.LT15 import Rule_LT15\n\n    return [\n        Rule_LT01,\n        Rule_LT02,\n        Rule_LT03,\n        Rule_LT04,\n        Rule_LT05,\n        Rule_LT06,\n        Rule_LT07,\n        Rule_LT08,\n        Rule_LT09,\n        Rule_LT10,\n        Rule_LT11,\n        Rule_LT12,\n        Rule_LT13,\n        Rule_LT14,\n        Rule_LT15,\n    ]\n"}, {"start_line": 0, "end_line": 45, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Standard Rules packaged with sqlfluff.\"\"\"\n"}], "retrieved_count": 10, "cost_time": 0.36256933212280273}
{"question": "How does SQLFluff implement its fix system?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff implements its fix system through a comprehensive mechanism that allows rules to generate and apply automatic corrections to SQL code. The fix system works as follows: 1) LintFix Objects - Rules generate LintFix objects that describe specific changes to be made, including the type of fix (create, edit, delete), target segment, and new content; 2) Fix Generation - During rule evaluation, rules can create LintFix objects when violations are detected, specifying exactly how to correct the issue; 3) Fix Application - The LintedFile.apply_fixes() method applies all generated fixes to the original SQL, creating a corrected version; 4) Patch Generation - The fix system generates FixPatch objects that represent the actual text changes needed, including line numbers and character positions; 5) Edit Computation - The compute_anchor_edit_info() function calculates the precise text edits needed to implement each fix; 6) Segment Manipulation - Fixes can create, edit, or delete segments in the parse tree, with the system handling the structural changes; 7) Position Tracking - The fix system maintains accurate position information throughout the fix application process; 8) Fix Validation - Applied fixes are validated to ensure they don't introduce new syntax errors or violate other rules; 9) Output Generation - The corrected SQL is generated with proper formatting and structure maintained; 10) Error Handling - The fix system handles edge cases and provides fallback behavior when fixes cannot be applied cleanly.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "patch.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Helpers for generating patches to fix files.\"\"\"\n\nimport logging\nfrom collections.abc import Iterator\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nfrom sqlfluff.core.parser import BaseSegment\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.templaters import TemplatedFile\n\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\n\n@dataclass\nclass FixPatch:\n    \"\"\"An edit patch for a source file.\"\"\"\n\n    templated_slice: slice\n    fixed_raw: str\n    # The patch category, functions mostly for debugging and explanation\n    # than for function. It allows traceability of *why* this patch was\n    # generated. It has no significance for processing.\n    patch_category: str\n    source_slice: slice\n    templated_str: str\n    source_str: str\n\n    def dedupe_tuple(self) -> tuple[slice, str]:\n        \"\"\"Generate a tuple of this fix for deduping.\"\"\"\n        return (self.source_slice, self.fixed_raw)\n\n\ndef _iter_source_fix_patches(\n    segment: BaseSegment, templated_file: TemplatedFile\n) -> Iterator[FixPatch]:\n    \"\"\"Yield any source patches as fixes now.\n\n    NOTE: This yields source fixes for the segment and any of its\n    children, so it's important to call it at the right point in\n    the recursion to avoid yielding duplicates.\n    \"\"\"\n    for source_fix in segment.source_fixes:\n        yield FixPatch(\n            source_fix.templated_slice,\n            source_fix.edit,\n            patch_category=\"source\",\n            source_slice=source_fix.source_slice,\n            templated_str=templated_file.templated_str[source_fix.templated_slice],\n            source_str=templated_file.source_str[source_fix.source_slice],\n        )\n\n\ndef _iter_templated_patches(\n    segment: BaseSegment, templated_file: TemplatedFile\n) -> Iterator[FixPatch]:\n    \"\"\"Iterate through the segments generating fix patches.\n\n    The patches are generated in TEMPLATED space. This is important\n    so that we defer dealing with any loops until later. At this stage\n    ev"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fix.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Helper classes & methods for applying fixes to segments.\"\"\"\n\nimport logging\nfrom collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlfluff.core.parser import BaseSegment, SourceFix\nfrom sqlfluff.core.rules.fix import LintFix\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n\n\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\n\n@dataclass\nclass AnchorEditInfo:\n    \"\"\"For a given fix anchor, count of the fix edit types and fixes for it.\"\"\"\n\n    delete: int = field(default=0)\n    replace: int = field(default=0)\n    create_before: int = field(default=0)\n    create_after: int = field(default=0)\n    fixes: list[\"LintFix\"] = field(default_factory=list)\n    source_fixes: list[SourceFix] = field(default_factory=list)\n    # First fix of edit_type \"replace\" in \"fixes\"\n    _first_replace: Optional[\"LintFix\"] = field(default=None)\n\n    def add(self, fix: \"LintFix\") -> None:\n        \"\"\"Adds the fix and updates stats.\n\n        We also allow potentially multiple source fixes on the same\n        anchor by condensing them together here.\n        \"\"\"\n        if fix in self.fixes:\n            # Deduplicate fixes in case it's already in there.\n            return\n\n        if fix.is_just_source_edit():\n            assert fix.edit\n            # is_just_source_edit confirms there will be a list\n            # so we can hint that to mypy.\n            self.source_fixes += fix.edit[0].source_fixes\n            # is there already a replace?\n            if self._first_replace:\n                assert self._first_replace.edit\n                # is_just_source_edit confirms there will be a list\n                # and that's the only way to get into _first_replace\n                # if it's populated so we can hint that to mypy.\n                linter_logger.info(\n                    \"Multiple edits detected, condensing %s onto %s\",\n                    fix,\n                    self._first_repl"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fix_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test routines for fixing errors.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core.linter.fix import compute_anchor_edit_info\nfrom sqlfluff.core.linter.patch import FixPatch, generate_source_patches\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments import (\n    BaseSegment,\n    RawSegment,\n    TemplateSegment,\n)\nfrom sqlfluff.core.parser.segments.raw import SourceFix\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters import RawFileSlice, TemplatedFile\nfrom sqlfluff.core.templaters.base import TemplatedFileSlice\n\n\n@pytest.fixture(scope=\"module\")\ndef raw_segments(generate_test_segments):\n    \"\"\"Construct a list of raw segments as a fixture.\"\"\"\n    return generate_test_segments([\"foobar\", \".barfoo\"])\n\n\ndef test__rules_base_segments_compute_anchor_edit_info(raw_segments):\n    \"\"\"Test BaseSegment.compute_anchor_edit_info().\"\"\"\n    # Construct a fix buffer, intentionally with:\n    # - one duplicate.\n    # - two different incompatible fixes on the same segment.\n    fixes = [\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"a\")]),\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"a\")]),\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"b\")]),\n    ]\n    anchor_info_dict = compute_anchor_edit_info(fixes)\n    # Check the target segment is the only key we have.\n    assert list(anchor_info_dict.keys()) == [raw_segments[0].uuid]\n    anchor_info = anchor_info_dict[raw_segments[0].uuid]\n    # Check that the duplicate as been deduplicated.\n    # i.e. this isn't 3.\n    assert anchor_info.replace == 2\n    # Check the fixes themselves.\n    # NOTE: There's no duplicated first fix.\n    assert anchor_info.fixes == [\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"a\")]),\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"b\")]),\n    ]\n    # Check the first replace\n    assert anchor_info._first_replace == LintFix.replace(\n        r"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fix.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the LintFix class, returned by rules when recommending a fix.\"\"\"\n\nimport logging\nfrom collections.abc import Iterable, Sized\nfrom itertools import chain\nfrom typing import Any, Optional, cast\n\nfrom sqlfluff.core.parser import BaseSegment, PositionMarker, RawSegment, SourceFix\nfrom sqlfluff.core.templaters import RawFileSlice, TemplatedFile\n\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\n\nclass LintFix:\n    \"\"\"A class to hold a potential fix to a linting violation.\n\n    Args:\n        edit_type (:obj:`str`): One of `create_before`, `create_after`,\n            `replace`, `delete` to indicate the kind of fix this represents.\n        anchor (:obj:`BaseSegment`): A segment which represents\n            the *position* that this fix should be applied at. For deletions\n            it represents the segment to delete, for creations it implies the\n            position to create at (with the existing element at this position\n            to be moved *after* the edit), for a `replace` it implies the\n            segment to be replaced.\n        edit (iterable of :obj:`BaseSegment`, optional): For `replace` and\n            `create` fixes, this holds the iterable of segments to create\n            or replace at the given `anchor` point.\n        source (iterable of :obj:`BaseSegment`, optional): For `replace` and\n            `create` fixes, this holds iterable of segments that provided\n            code. IMPORTANT: The linter uses this to prevent copying material\n            from templated areas.\n    \"\"\"\n\n    def __init__(\n        self,\n        edit_type: str,\n        anchor: BaseSegment,\n        edit: Optional[Iterable[BaseSegment]] = None,\n        source: Optional[Iterable[BaseSegment]] = None,\n    ) -> None:\n        if edit_type not in (\n            \"create_before\",\n            \"create_after\",\n            \"replace\",\n            \"delete\",\n        ):  # pragma: no cover\n            raise ValueError(f\"Unexpected edit_type: {edit_type}\")\n        self.edit_type = edit_type\n"}, {"start_line": 36000, "end_line": 38000, "belongs_to": {"file_name": "commands.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/cli", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "=True,\n    help=(\n        \"[DEPRECATED - From 3.0 onward this is the default behaviour] \"\n        \"Apply fixes will also be applied file by file, during the \"\n        \"linting process, rather than waiting until all files are \"\n        \"linted before fixing.\"\n    ),\n)\n@click.option(\n    \"--check\",\n    is_flag=True,\n    help=(\n        \"Analyse all files and ask for confirmation before applying \"\n        \"any fixes. Fixes will be applied all together at the end of \"\n        \"the operation.\"\n    ),\n)\n@click.option(\n    \"-q\",\n    \"--quiet\",\n    is_flag=True,\n    help=(\n        \"Reduces the amount of output to stdout to a minimal level. \"\n        \"This is effectively the opposite of -v. NOTE: It will only \"\n        \"take effect if -f/--force is also set.\"\n    ),\n)\n@click.option(\n    \"-x\",\n    \"--fixed-suffix\",\n    default=None,\n    help=\"An optional suffix to add to fixed files.\",\n)\n@click.option(\n    \"--FIX-EVEN-UNPARSABLE\",\n    is_flag=True,\n    default=None,\n    help=(\n        \"Enables fixing of files that have templating or parse errors. \"\n        \"Note that the similar-sounding '--ignore' or 'noqa' features merely \"\n        \"prevent errors from being *displayed*. For safety reasons, the 'fix'\"\n        \"command will not make any fixes in files that have templating or parse \"\n        \"errors unless '--FIX-EVEN-UNPARSABLE' is enabled on the command line\"\n        \"or in the .sqlfluff config file.\"\n    ),\n)\n@click.option(\n    \"--show-lint-violations\",\n    is_flag=True,\n    help=\"Show lint violations\",\n)\n@click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\ndef fix(\n    force: bool,\n    paths: tuple[str],\n    disregard_sqlfluffignores: bool,\n    check: bool = False,\n    bench: bool = False,\n    quiet: bool = False,\n    fixed_suffix: str = \"\",\n    logger: Optional[logging.Logger] = None,\n    processes: Optional[int] = None,\n    disable_progress_bar: Optional[bool] = False,\n    persist_timing: Optional[str] = None,\n    extra_config_path: Optional[str] = None,\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linted_file_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests covering the LintedFile class and it's methods.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core.linter import LintedFile\nfrom sqlfluff.core.linter.patch import FixPatch\nfrom sqlfluff.core.templaters import RawFileSlice\n\n\n@pytest.mark.parametrize(\n    \"source_slices,source_patches,raw_source_string,expected_result\",\n    # NOTE: For all of these examples we're not setting the patch_category\n    # of the fix patches. They're not used at this step so irrelevant for\n    # testing.\n    [\n        # Trivial example\n        ([slice(0, 1)], [], \"a\", \"a\"),\n        # Simple replacement\n        (\n            [slice(0, 1), slice(1, 2), slice(2, 3)],\n            [FixPatch(slice(1, 2), \"d\", \"\", slice(1, 2), \"b\", \"b\")],\n            \"abc\",\n            \"adc\",\n        ),\n        # Simple insertion\n        (\n            [slice(0, 1), slice(1, 1), slice(1, 2)],\n            [FixPatch(slice(1, 1), \"b\", \"\", slice(1, 1), \"\", \"\")],\n            \"ac\",\n            \"abc\",\n        ),\n        # Simple deletion\n        (\n            [slice(0, 1), slice(1, 2), slice(2, 3)],\n            [FixPatch(slice(1, 2), \"\", \"\", slice(1, 2), \"b\", \"b\")],\n            \"abc\",\n            \"ac\",\n        ),\n        # Illustrative templated example (although practically at\n        # this step, the routine shouldn't care if it's templated).\n        (\n            [slice(0, 2), slice(2, 7), slice(7, 9)],\n            [FixPatch(slice(2, 3), \"{{ b }}\", \"\", slice(2, 7), \"b\", \"{{b}}\")],\n            \"a {{b}} c\",\n            \"a {{ b }} c\",\n        ),\n    ],\n)\ndef test__linted_file__build_up_fixed_source_string(\n    source_slices, source_patches, raw_source_string, expected_result, caplog\n):\n    \"\"\"Test _build_up_fixed_source_string.\n\n    This is part of fix_string().\n    \"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.linter\"):\n        result = LintedFile._build_up_fixed_source_string(\n            source_slices, source_patches, raw_source_string\n        )\n    assert result == expected_result\n\n\n@pytest.m"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "re.parser.matchable import Matchable\nfrom sqlfluff.core.parser.types import SimpleHintType\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.parser.segments.raw import RawSegment\n\n# Instantiate the linter logger (only for use in methods involved with fixing.)\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\nTupleSerialisedSegment = tuple[str, Union[str, tuple[\"TupleSerialisedSegment\", ...]]]\nRecordSerialisedSegment = dict[\n    str, Union[None, str, \"RecordSerialisedSegment\", list[\"RecordSerialisedSegment\"]]\n]\n\n\n@dataclass(frozen=True)\nclass SourceFix:\n    \"\"\"A stored reference to a fix in the non-templated file.\"\"\"\n\n    edit: str\n    source_slice: slice\n    # TODO: It might be possible to refactor this to not require\n    # a templated_slice (because in theory it's unnecessary).\n    # However much of the fix handling code assumes we need\n    # a position in the templated file to interpret it.\n    # More work required to achieve that if desired.\n    templated_slice: slice\n\n    def __hash__(self) -> int:\n        # Only hash based on the source slice, not the\n        # templated slice (which might change)\n        return hash((self.edit, self.source_slice.start, self.source_slice.stop))\n\n\n@dataclass(frozen=True)\nclass PathStep:\n    \"\"\"An element of the response to BaseSegment.path_to().\n\n    Attributes:\n        segment (:obj:`BaseSegment`): The segment in the chain.\n        idx (int): The index of the target within its `segment`.\n        len (int): The number of children `segment` has.\n        code_idxs (:obj:`tuple` of int): The indices which contain code.\n    \"\"\"\n\n    segment: BaseSegment\n    idx: int\n    len: int\n    code_idxs: tuple[int, ...]\n\n\ndef _iter_base_types(\n    new_type: Optional[str], bases: tuple[type[BaseSegment]]\n) -> Iterator[str]:\n    \"\"\"Iterate types for a new segment class.\n\n    This is a helper method used within in the construction of\n    SegmentMetaclass so that we can construct a fro"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linted_file.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the LintedFile class.\n\nThis holds linting results for a single file, and also\ncontains all of the routines to apply fixes to that file\npost linting.\n\"\"\"\n\nimport logging\nimport os\nimport shutil\nimport stat\nimport tempfile\nfrom collections import defaultdict\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass\nfrom typing import NamedTuple, Optional, Union\n\nfrom sqlfluff.core.errors import (\n    CheckTuple,\n    SQLBaseError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.linter.patch import FixPatch, generate_source_patches\n\n# Classes needed only for type checking\nfrom sqlfluff.core.parser.segments import BaseSegment\nfrom sqlfluff.core.rules.noqa import IgnoreMask\nfrom sqlfluff.core.templaters import RawFileSlice, TemplatedFile\n\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\nTMP_PRS_ERROR_TYPES = (SQLTemplaterError, SQLParseError)\n\n\n@dataclass\nclass FileTimings:\n    \"\"\"A dataclass for holding the timings information for a file.\"\"\"\n\n    step_timings: dict[str, float]\n    # NOTE: Because rules may run more than once for any\n    # given file we record each run and then we can post\n    # process this as we wish later.\n    rule_timings: list[tuple[str, str, float]]\n\n    def __repr__(self) -> str:  # pragma: no cover\n        return \"<FileTimings>\"\n\n    def get_rule_timing_dict(self) -> dict[str, float]:\n        \"\"\"Generate a summary to total time in each rule.\n\n        This is primarily for csv export.\n        \"\"\"\n        total_times: dict[str, float] = defaultdict(float)\n\n        for code, _, time in self.rule_timings:\n            total_times[code] += time\n\n        # Return as plain dict\n        return dict(total_times.items())\n\n\nclass LintedFile(NamedTuple):\n    \"\"\"A class to store the idea of a linted file.\"\"\"\n\n    path: str\n    violations: list[SQLBaseError]\n    timings: Optional[FileTimings]\n    tre"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "std_fix_auto_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Automated tests for fixing violations.\n\nAny files in the test/fixtures/linter/autofix directory will be picked up\nand automatically tested against the appropriate dialect.\n\"\"\"\n\nimport json\nimport logging\nimport os\nimport shutil\nimport tempfile\nfrom typing import Optional\n\nimport pytest\nimport yaml\n\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.config import clear_config_caches\n\n# Construct the tests from the filepath\ntest_cases = []\nbase_auto_fix_path = (\"test\", \"fixtures\", \"linter\", \"autofix\")\n\n# Generate the filenames for each dialect from the parser test directory\nfor dialect in os.listdir(os.path.join(*base_auto_fix_path)):\n    # Ignore documentation\n    if dialect.endswith(\".md\"):\n        continue\n    # assume that d is now the name of a dialect\n    dirlist = os.listdir(os.path.join(*base_auto_fix_path, dialect))\n    for test_case in dirlist:\n        test_cases.append(\n            (\n                # The dialect\n                dialect,\n                # The directory name\n                test_case,\n            )\n        )\n\n\ndef make_dialect_path(dialect, fname):\n    \"\"\"Work out how to find paths given a dialect and a file name.\"\"\"\n    return os.path.join(\"test\", \"fixtures\", \"dialects\", dialect, fname)\n\n\ndef auto_fix_test(dialect, folder, caplog):\n    \"\"\"A test for roundtrip testing, take a file buffer, lint, fix and lint.\n\n    This is explicitly different from the linter version of this, in that\n    it uses the command line rather than the direct api.\n    \"\"\"\n    # Log just the rules logger for this test.\n    # NOTE: In debugging it may be instructive to enable some of\n    # the other loggers listed here to debug particular issues.\n    # Enabling all of them results in very long logs so use\n    # wisely.\n    # caplog.set_level(logging.DEBUG, logger=\"sqlfluff.templater\")\n    # caplog.set_level(logging.DEBUG, logger=\"sqlfluff.lexer\")\n    caplog.set_level(logging.DEBUG, logger=\"sqlfluff.linter\")\n    caplog.set_level(logging.DEBUG, logger=\"sq"}, {"start_line": 25000, "end_line": 27000, "belongs_to": {"file_name": "linter.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "                tree = new_tree\n                                previous_versions.add(loop_check_tuple)\n                                changed = True\n                                continue\n                            else:\n                                # Applying these fixes took us back to a state\n                                # which we've seen before. We're in a loop, so\n                                # we want to stop.\n                                cls._warn_unfixable(crawler.code)\n\n                    # Record rule timing\n                    rule_timings.append(\n                        (crawler.code, crawler.name, time.monotonic() - t0)\n                    )\n\n                if fix and not changed:\n                    # We did not change the file. Either the file is clean (no\n                    # fixes), or any fixes which are present will take us back\n                    # to a previous state.\n                    linter_logger.info(\n                        f\"Fix loop complete for {phase} phase. Stability \"\n                        f\"achieved after {loop}/{loop_limit} loops.\"\n                    )\n                    break\n            else:\n                if fix:\n                    # The linter loop hit the limit before reaching a stable point\n                    # (i.e. free of lint errors). If this happens, it's usually\n                    # because one or more rules produced fixes which did not address\n                    # the original issue **or** created new issues.\n                    linter_logger.warning(\n                        f\"Loop limit on fixes reached [{loop_limit}].\"\n                    )\n\n                    # Discard any fixes for the linting errors, since they caused a\n                    # loop. IMPORTANT: By doing this, we are telling SQLFluff that\n                    # these linting errors are \"unfixable\". This is important,\n                    # because when \"sqlfluff fix\" encounters unfixable lint errors,\n                   "}], "retrieved_count": 10, "cost_time": 0.34092092514038086}
{"question": "How do different SQLFluff linting rules interact with the parsed SQL structure?", "answer": null, "relative_code_list": null, "ground_truth": "Different SQLFluff linting rules interact with the parsed SQL structure through a systematic approach that traverses the parse tree and analyzes specific patterns. The rule interaction works as follows: 1) Parse Tree Traversal - Rules receive the parsed SQL structure as a tree of BaseSegment objects and traverse it using crawl behaviors (root, segment, or statement-level crawling); 2) Segment Analysis - Each rule's _eval() method receives individual segments and analyzes their properties, children, and relationships within the parse tree; 3) Context Access - Rules access contextual information through the rule context, including parent segments, sibling relationships, and broader tree structure; 4) Pattern Matching - Rules use pattern matching to identify specific SQL constructs, such as finding all SELECT statements, JOIN clauses, or function calls; 5) Structural Validation - Rules validate the structural correctness of SQL constructs, checking for proper nesting, required elements, and syntax compliance; 6) Cross-Reference Analysis - Rules can analyze relationships between different parts of the SQL, such as checking if referenced tables exist or if aliases are properly defined; 7) Configuration Integration - Rules apply configuration settings to determine what constitutes a violation, allowing for flexible rule behavior; 8) Fix Generation - When violations are found, rules can generate LintFix objects that describe how to correct the issues in the parse tree; 9) Error Reporting - Rules report violations with specific locations, messages, and severity levels based on their analysis of the parse structure; 10) Performance Optimization - Rules use efficient traversal patterns and caching to minimize the performance impact of analyzing large parse trees.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 38, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for sqlfluff.core.linter.\"\"\"\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "rules_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for the standard set of rules.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import Linter\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLFluffUserError\nfrom sqlfluff.core.linter import RuleTuple\nfrom sqlfluff.core.parser import WhitespaceSegment\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.rules import BaseRule, LintFix, LintResult, get_ruleset\nfrom sqlfluff.core.rules.crawlers import RootOnlyCrawler, SegmentSeekerCrawler\nfrom sqlfluff.core.rules.doc_decorators import (\n    document_configuration,\n    document_fix_compatible,\n    document_groups,\n)\nfrom sqlfluff.core.rules.loader import get_rules_from_path\nfrom sqlfluff.core.templaters.base import TemplatedFile\nfrom sqlfluff.utils.testing.logging import fluff_log_catcher\nfrom sqlfluff.utils.testing.rules import get_rule_from_set\nfrom test.fixtures.rules.custom.L000 import Rule_L000\nfrom test.fixtures.rules.custom.S000 import Rule_S000\n\n\nclass Rule_T042(BaseRule):\n    \"\"\"A dummy rule.\"\"\"\n\n    groups = (\"all\",)\n\n    def _eval(self, context):\n        pass\n\n\nclass Rule_T001(BaseRule):\n    \"\"\"A deliberately malicious rule.\n\n    **Anti-pattern**\n\n    Blah blah\n    \"\"\"\n\n    groups = (\"all\",)\n    crawl_behaviour = SegmentSeekerCrawler({\"whitespace\"})\n    is_fix_compatible = True\n\n    def _eval(self, context):\n        \"\"\"Stars make newlines.\"\"\"\n        if context.segment.is_type(\"whitespace\"):\n            return LintResult(\n                anchor=context.segment,\n                fixes=[\n                    LintFix.replace(\n                        context.segment, [WhitespaceSegment(context.segment.raw + \" \")]\n                    )\n                ],\n            )\n\n\nclass Rule_T002(BaseRule):\n    \"\"\"A rule which says all raw code segments are bad.\n\n    This is used for testing unparsable code.\n    \"\"\"\n\n    groups = (\"all\",)\n    # Root only crawler so that the in-rule filters don't kick in.\n    crawl_behaviour = RootOnlyCrawler()\n\n    def _eval(s"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "rules_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "t of selected codes:\n    selected_codes = set(tpl[0] for tpl in linter.rule_tuples())\n    # Check selected rules\n    assert selected_codes == resulting_codes\n\n\ndef test__rules__filter_unparsable():\n    \"\"\"Test that rules that handle their own crawling respect unparsable.\"\"\"\n    # Set up a linter with the user rule\n    linter = Linter(user_rules=[Rule_T002], dialect=\"ansi\", rules=[\"T002\"])\n    # Lint a simple parsable file and check we do get issues\n    # It's parsable, so we should get issues.\n    res = linter.lint_string(\"SELECT 1\")\n    assert any(v.rule_code() == \"T002\" for v in res.violations)\n    # Lint an unparsable file. Check we don't get any violations.\n    # It's not parsable so we shouldn't get issues.\n    res = linter.lint_string(\"asd asdf sdfg\")\n    assert not any(v.rule_code() == \"T002\" for v in res.violations)\n\n\ndef test__rules__result_unparsable():\n    \"\"\"Test that the linter won't allow rules which make the file unparsable.\"\"\"\n    # Set up a linter with the user rule\n    linter = Linter(user_rules=[Rule_T003], dialect=\"ansi\", rules=[\"T003\"])\n    # Lint a simple parsable file and check we do get issues\n    # It's parsable, so we should get issues.\n    raw_sql = \"SELECT 1 FROM a\"\n    with fluff_log_catcher(logging.WARNING, \"sqlfluff\") as caplog:\n        res = linter.lint_string(raw_sql, fix=True)\n    # Check we got the warning.\n    assert \"would result in an unparsable file\" in caplog.text\n    # Check we get the violation.\n    assert any(v.rule_code() == \"T003\" for v in res.violations)\n    # The resulting file should be _the same_ because it would have resulted\n    # in an unparsable file if applied.\n    assert res.tree.raw == raw_sql\n\n\n@pytest.mark.parametrize(\n    \"sql_query, check_tuples\",\n    [\n        (\n            \"SELECT * FROM foo\",\n            # Even though there's a runaway fix, we should still\n            # find each issue once and not duplicates of them.\n            [\n                (\"T001\", 1, 7),\n                (\"T001\", 1, 9),\n         "}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "rules_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\")\n        name = \"fake_basic\"\n        aliases = (\"fb1\", \"foo\")  # NB: Foo is a group on another rule.\n        crawl_behaviour = RootOnlyCrawler()\n\n        def _eval(self, **kwargs):\n            pass\n\n    class Rule_T011(Rule_T010):\n        \"\"\"Fake Basic Rule.\n\n        NOTE: We inherit crawl behaviour and _eval from above.\n        \"\"\"\n\n        groups = (\"all\", \"test\", \"foo\")\n        name = \"fake_other\"\n        aliases = (\"fb2\",)\n\n    class Rule_T012(Rule_T010):\n        \"\"\"Fake Basic Rule.\n\n        NOTE: We inherit crawl behaviour and _eval from above.\n        \"\"\"\n\n        # NB: \"fake_other\" is the name of another rule.\n        groups = (\"all\", \"foo\", \"fake_other\")\n        # No aliases, Name collides with the alias of another rule.\n        name = \"fake_again\"\n        aliases = ()\n\n    cfg = FluffConfig(\n        overrides={\"rules\": rules, \"exclude_rules\": exclude_rules, \"dialect\": \"ansi\"}\n    )\n    linter = Linter(config=cfg, user_rules=[Rule_T010, Rule_T011, Rule_T012])\n    # Get the set of selected codes:\n    selected_codes = set(tpl[0] for tpl in linter.rule_tuples())\n    # Check selected rules\n    assert selected_codes == resulting_codes\n\n\ndef test__rules__filter_unparsable():\n    \"\"\"Test that rules that handle their own crawling respect unparsable.\"\"\"\n    # Set up a linter with the user rule\n    linter = Linter(user_rules=[Rule_T002], dialect=\"ansi\", rules=[\"T002\"])\n    # Lint a simple parsable file and check we do get issues\n    # It's parsable, so we should get issues.\n    res = linter.lint_string(\"SELECT 1\")\n    assert any(v.rule_code() == \"T002\" for v in res.violations)\n    # Lint an unparsable file. Check we don't get any violations.\n    # It's not parsable so we shouldn't get issues.\n    res = linter.lint_string(\"asd asdf sdfg\")\n    assert not any(v.rule_code() == \"T002\" for v in res.violations)\n\n\ndef test__rules__result_unparsable():\n    \"\"\"Test that the linter won't allow rules which make the file unparsable.\"\"\"\n    # Set up a linter with the user rule\n  "}, {"start_line": 19000, "end_line": 21000, "belongs_to": {"file_name": "linter_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "or advanced API usage and within rules.\n    sql = \"\"\"\n    WITH cte AS (\n        SELECT * FROM tab_a\n    )\n    SELECT\n        cte.col_a,\n        tab_b.col_b\n    FROM cte\n    INNER JOIN tab_b;\n    \"\"\"\n    linter = Linter(dialect=\"ansi\")\n    parsed = linter.parse_string(sql)\n\n    # CTEDefinitionSegment.get_identifier\n    cte_segment = next(parsed.tree.recursive_crawl(\"common_table_expression\"))\n    assert cte_segment.get_identifier().raw == \"cte\"\n\n    # BaseFileSegment.get_table_references & StatementSegment.get_table_references\n    assert parsed.tree.get_table_references() == {\"tab_a\", \"tab_b\"}\n\n\ndef test_normalise_newlines():\n    \"\"\"Test normalising newlines to unix-style line endings.\"\"\"\n    in_str = \"SELECT\\r\\n foo\\n FROM \\r \\n\\r bar;\"\n    out_str = \"SELECT\\n foo\\n FROM \\n \\n\\n bar;\"\n    assert out_str == Linter._normalise_newlines(in_str)\n\n\n@pytest.mark.parametrize(\n    \"fix_even_unparsable\",\n    [False, True],\n)\ndef test_unparsable_fix_output(fix_even_unparsable):\n    \"\"\"Tests functionality and logging output with unparsable sections.\n\n    NOTE: While we cover different paths, the result for this test is the\n    same for both values of `fix_even_unparsable`. We probably need a better\n    test case at some point so that we can actually see the difference.\n    \"\"\"\n    config = FluffConfig(\n        overrides={\"fix_even_unparsable\": fix_even_unparsable, \"dialect\": \"ansi\"}\n    )\n    linter = Linter(config=config)\n    # Attempt to fix it, capturing the logging output.\n    with fluff_log_catcher(logging.WARNING, \"sqlfluff.linter\") as caplog:\n        result = linter.lint_paths(\n            (\"test/fixtures/linter/parse_error_2.sql\",),\n            fix=True,\n            apply_fixes=True,\n            fixed_file_suffix=f\"_{fix_even_unparsable}_fix\",\n            fix_even_unparsable=fix_even_unparsable,\n        )\n    # Assert that it parsed (i.e. we found a select_statement), but with an\n    # unparsable section in there too.\n    assert result.tree\n    assert \"select_statement\""}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`Lin"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "simple_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/api", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e_pos\": 41,\n                \"start_file_pos\": 40,\n                \"end_line_no\": 1,\n                \"end_line_pos\": 41,\n                \"end_file_pos\": 40,\n            }\n        ],\n        \"warning\": False,\n    },\n]\n\n\ndef test__api__lint_string_without_violations():\n    \"\"\"Check lint functionality when there is no violation.\"\"\"\n    result = sqlfluff.lint(\"select column from table\\n\")\n    assert result == []\n\n\ndef test__api__lint_string():\n    \"\"\"Basic checking of lint functionality.\"\"\"\n    result = sqlfluff.lint(my_bad_query)\n    # Check return types.\n    assert isinstance(result, list)\n    assert all(isinstance(elem, dict) for elem in result)\n    # Check actual result\n    assert result == lint_result\n\n\ndef test__api__lint_string_specific():\n    \"\"\"Basic checking of lint functionality.\"\"\"\n    rules = [\"CP02\", \"LT12\"]\n    result = sqlfluff.lint(my_bad_query, rules=rules)\n    # Check which rules are found\n    assert all(elem[\"code\"] in rules for elem in result)\n\n\ndef test__api__lint_string_specific_single():\n    \"\"\"Basic checking of lint functionality.\"\"\"\n    rules = [\"CP02\"]\n    result = sqlfluff.lint(my_bad_query, rules=rules)\n    # Check which rules are found\n    assert all(elem[\"code\"] in rules for elem in result)\n\n\ndef test__api__lint_string_specific_exclude():\n    \"\"\"Basic checking of lint functionality.\"\"\"\n    exclude_rules = [\"LT12\", \"CP01\", \"AL03\", \"CP02\", \"LT09\", \"LT01\"]\n    result = sqlfluff.lint(my_bad_query, exclude_rules=exclude_rules)\n    # Check only AM04 is found\n    assert len(result) == 1\n    assert \"AM04\" == result[0][\"code\"]\n\n\ndef test__api__lint_string_specific_exclude_single():\n    \"\"\"Basic checking of lint functionality.\"\"\"\n    exclude_rules = [\"LT01\"]\n    result = sqlfluff.lint(my_bad_query, exclude_rules=exclude_rules)\n    # Check only AM04 is found\n    assert len(result) == 9\n    assert set([\"LT12\", \"CP01\", \"AL03\", \"CP02\", \"LT09\", \"AM04\"]) == set(\n        [r[\"code\"] for r in result]\n    )\n\n\ndef test__api__lint_string_specific_exclude_all_fai"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Implements the base rule class.\n\nRules crawl through the trees returned by the parser and evaluate particular\nrules.\n\nThe intent is that it should be possible for the rules to be expressed\nas simply as possible, with as much of the complexity abstracted away.\n\nThe evaluation function should take enough arguments that it can evaluate\nthe position of the given segment in relation to its neighbors, and that\nthe segment which finally \"triggers\" the error, should be the one that would\nbe corrected OR if the rule relates to something that is missing, then it\nshould flag on the segment FOLLOWING, the place that the desired element is\nmissing.\n\"\"\"\n\nimport bdb\nimport copy\nimport fnmatch\nimport logging\nimport pathlib\nimport re\nfrom collections import defaultdict, namedtuple\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    ClassVar,\n    DefaultDict,\n    Optional,\n    Union,\n)\n\nimport regex\n\nfrom sqlfluff.core.errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a "}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "flink_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/dialects", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "   name STRING,\n            full_name AS CONCAT(name, '_suffix')\n        ) WITH (\n            'connector' = 'kafka'\n        )\n        \"\"\"\n        result = linter.lint_string(sql)\n        assert result is not None\n\n    def test_flink_metadata_column(self):\n        \"\"\"Test FlinkSQL metadata column.\"\"\"\n        config = FluffConfig(overrides={\"dialect\": \"flink\"})\n        linter = Linter(config=config)\n\n        sql = \"\"\"\n        CREATE TABLE my_table (\n            id INT,\n            name STRING,\n            kafka_offset BIGINT METADATA FROM 'offset'\n        ) WITH (\n            'connector' = 'kafka'\n        )\n        \"\"\"\n        result = linter.lint_string(sql)\n        assert result is not None\n\n    def test_flink_show_statements(self):\n        \"\"\"Test FlinkSQL SHOW statements.\"\"\"\n        config = FluffConfig(overrides={\"dialect\": \"flink\"})\n        linter = Linter(config=config)\n\n        statements = [\n            \"SHOW CATALOGS\",\n            \"SHOW DATABASES\",\n            \"SHOW TABLES\",\n            \"SHOW VIEWS\",\n            \"SHOW FUNCTIONS\",\n            \"SHOW MODULES\",\n            \"SHOW JARS\",\n            \"SHOW JOBS\",\n        ]\n\n        for sql in statements:\n            result = linter.lint_string(sql)\n            assert result is not None\n\n    def test_flink_use_statements(self):\n        \"\"\"Test FlinkSQL USE statements.\"\"\"\n        config = FluffConfig(overrides={\"dialect\": \"flink\"})\n        linter = Linter(config=config)\n\n        statements = [\n            \"USE CATALOG my_catalog\",\n            \"USE my_database\",\n            \"USE my_catalog.my_database\",\n        ]\n\n        for sql in statements:\n            result = linter.lint_string(sql)\n            assert result is not None\n\n    def test_flink_describe_statement(self):\n        \"\"\"Test FlinkSQL DESCRIBE statement.\"\"\"\n        config = FluffConfig(overrides={\"dialect\": \"flink\"})\n        linter = Linter(config=config)\n\n        sql = \"DESCRIBE my_table\"\n        result = linter.lint_string(sql)\n        assert result is no"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "common.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines small container classes to hold intermediate results during linting.\"\"\"\n\nfrom typing import Any, NamedTuple, Optional, Union\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLLexError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.parser.segments.base import BaseSegment\nfrom sqlfluff.core.templaters import TemplatedFile\n\n\nclass RuleTuple(NamedTuple):\n    \"\"\"Rule Tuple object for describing rules.\"\"\"\n\n    code: str\n    name: str\n    description: str\n    groups: tuple[str, ...]\n    aliases: tuple[str, ...]\n\n\nclass RenderedFile(NamedTuple):\n    \"\"\"An object to store the result of a templated file/string.\n\n    This is notable as it's the intermediate state between what happens\n    in the main process and the child processes when running in parallel mode.\n    \"\"\"\n\n    templated_variants: list[TemplatedFile]\n    templater_violations: list[SQLTemplaterError]\n    config: FluffConfig\n    time_dict: dict[str, float]\n    fname: str\n    encoding: str\n    source_str: str\n\n\nclass ParsedVariant(NamedTuple):\n    \"\"\"An object to store the result of parsing a single TemplatedFile.\n\n    Args:\n        templated_file (:obj:`TemplatedFile`): Containing the details\n            of the templated file. If templating fails, this will be `None`.\n        tree (:obj:`BaseSegment`): The segment structure representing the\n            parsed file. If parsing fails due to an unrecoverable\n            violation then we will be None.\n        lexing_violations (:obj:`list` of :obj:`SQLLexError`): Any violations\n            raised during the lexing phase.\n        parsing_violations (:obj:`list` of :obj:`SQLParseError`): Any violations\n            raised during the lexing phase.\n    \"\"\"\n\n    templated_file: TemplatedFile\n    tree: Optional[BaseSegment]\n    lexing_violations: list[SQLLexError]\n    parsing_violations: list[SQLParseError]\n\n    def violations(self) -> list[Union[SQLLexError, SQLParseError]]:\n        \"\"\"Return"}], "retrieved_count": 10, "cost_time": 0.3447906970977783}
{"question": "How does SQLFluff handle template processing in SQL files?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff handles template processing in SQL files through a sophisticated templating system that supports multiple templating engines and handles dynamic SQL content. The template processing works as follows: 1) Template Detection - SQLFluff automatically detects templating engines based on file content, supporting Jinja, Python format strings, and dbt templates; 2) Template Compilation - Raw SQL with template syntax is processed through the appropriate templating engine to resolve dynamic content and placeholders; 3) TemplatedFile Objects - The templating system creates TemplatedFile objects that maintain both the original template and the rendered SQL, along with mapping information; 4) Block Tracking - The BlockTracker maintains information about template blocks, allowing the system to map between template positions and rendered SQL positions; 5) Template Elements - TemplateElement objects represent different parts of the template, including static SQL, template blocks, and dynamic content; 6) Position Mapping - The system maintains accurate position mapping between template source and rendered output for error reporting and fix application; 7) Variable Resolution - Template variables and expressions are resolved according to the templating engine's rules and any provided context; 8) Conditional Logic - Template conditionals and loops are processed to generate the appropriate SQL variants; 9) Error Handling - Template processing errors are caught and reported with context about the template syntax and rendering issues; 10) Integration with Parsing - The templated SQL is then passed to the lexer and parser, with the system maintaining awareness of the original template structure.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "python.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the templaters.\"\"\"\n\nimport ast\nimport re\nfrom collections.abc import Iterable, Iterator\nfrom string import Formatter\nfrom typing import Any, Callable, NamedTuple, Optional\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLTemplaterError\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.helpers.slice import offset_slice, zero_slice\nfrom sqlfluff.core.helpers.string import findall\nfrom sqlfluff.core.templaters.base import (\n    RawFileSlice,\n    RawTemplater,\n    TemplatedFile,\n    TemplatedFileSlice,\n    large_file_check,\n    templater_logger,\n)\n\n\nclass IntermediateFileSlice(NamedTuple):\n    \"\"\"An intermediate representation of a partially sliced File.\"\"\"\n\n    intermediate_type: str\n    source_slice: slice\n    templated_slice: slice\n    slice_buffer: list[RawFileSlice]\n\n    def _trim_end(\n        self, templated_str: str, target_end: str = \"head\"\n    ) -> tuple[\"IntermediateFileSlice\", list[TemplatedFileSlice]]:\n        \"\"\"Trim the ends of a intermediate segment.\"\"\"\n        target_idx = 0 if target_end == \"head\" else -1\n        terminator_types = (\"block_start\") if target_end == \"head\" else (\"block_end\")\n        main_source_slice = self.source_slice\n        main_templated_slice = self.templated_slice\n        slice_buffer = self.slice_buffer\n\n        end_buffer = []\n\n        # Yield any leading literals, comments or blocks.\n        while len(slice_buffer) > 0 and slice_buffer[target_idx].slice_type in (\n            \"literal\",\n            \"block_start\",\n            \"block_end\",\n            \"comment\",\n        ):\n            focus = slice_buffer[target_idx]\n            templater_logger.debug(\"            %s Focus: %s\", target_end, focus)\n            # Is it a zero length item?\n            if focus.slice_type in (\"block_start\", \"block_end\", \"comment\"):\n                # Only add the length in the source space.\n                templated_len = 0\n            else:\n                # Assume it's a literal, chec"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the templaters.\"\"\"\n\nimport logging\nfrom bisect import bisect_left\nfrom collections.abc import Iterable, Iterator\nfrom typing import (\n    Any,\n    Callable,\n    NamedTuple,\n    Optional,\n    TypeVar,\n)\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLFluffSkipFile, SQLTemplaterError\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.helpers.slice import zero_slice\n\n# Instantiate the templater logger\ntemplater_logger = logging.getLogger(\"sqlfluff.templater\")\n\n\ndef iter_indices_of_newlines(raw_str: str) -> Iterator[int]:\n    \"\"\"Find the indices of all newlines in a string.\"\"\"\n    init_idx = -1\n    while True:\n        nl_pos = raw_str.find(\"\\n\", init_idx + 1)\n        if nl_pos >= 0:\n            yield nl_pos\n            init_idx = nl_pos\n        else:\n            break  # pragma: no cover TODO?\n\n\nT = TypeVar(\"T\")\n\n\ndef large_file_check(func: Callable[..., T]) -> Callable[..., T]:\n    \"\"\"Raise an exception if the file is over a defined size.\n\n    Designed to be implemented as a decorator on `.process()` methods.\n\n    If no config is provided or the relevant config value is set\n    to zero then the check is skipped.\n    \"\"\"\n\n    def _wrapped(\n        self: Any,\n        *,\n        in_str: str,\n        fname: str,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n    ) -> T:\n        if config:\n            limit = config.get(\"large_file_skip_char_limit\")\n            if limit:\n                templater_logger.warning(\n                    \"The config value large_file_skip_char_limit was found set. \"\n                    \"This feature will be removed in a future release, please \"\n                    \"use the more efficient 'large_file_skip_byte_limit' instead.\"\n                )\n            if limit and len(in_str) > limit:\n                raise SQLFluffSkipFile(\n                    f\"Length of file {fname!r} is over {limit} characters. \"\n                    \"Sk"}, {"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "python_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     # templated file from the test case.\n        (lambda x: templated_file),\n        config=FluffConfig(\n            configs={\"templater\": {\"unwrap_wrapped_queries\": unwrap_wrapped}},\n            overrides={\"dialect\": \"ansi\"},\n        ),\n    )\n    # Check contiguous\n    prev_slice = None\n    for templated_slice in resp:\n        if prev_slice:\n            assert templated_slice.source_slice.start == prev_slice[0].stop\n            assert templated_slice.templated_slice.start == prev_slice[1].stop\n        prev_slice = (templated_slice.source_slice, templated_slice.templated_slice)\n    # check result\n    assert resp == result\n\n\ndef test__templater_python_large_file_check():\n    \"\"\"Test large file skipping.\n\n    The check is separately called on each .process() method\n    so it makes sense to test a few templaters.\n    \"\"\"\n    # First check we can process the file normally without config.\n    PythonTemplater().process(in_str=\"SELECT 1\", fname=\"<string>\")\n    # Then check we raise a skip exception when config is set low.\n    with pytest.raises(SQLFluffSkipFile) as excinfo:\n        PythonTemplater().process(\n            in_str=\"SELECT 1\",\n            fname=\"<string>\",\n            config=FluffConfig(\n                overrides={\"dialect\": \"ansi\", \"large_file_skip_char_limit\": 2},\n            ),\n        )\n\n    assert \"Length of file\" in str(excinfo.value)\n\n\n@pytest.mark.parametrize(\n    \"raw_str,result\",\n    [\n        (\"\", \"\"),\n        (\n            \"SELECT * FROM {foo.bar}\",\n            \"SELECT * FROM foobar\",\n        ),\n        (\n            \"SELECT {foo} FROM {foo.bar}\",\n            \"SELECT bar FROM foobar\",\n        ),\n        (\n            \"SELECT {num:.2f} FROM blah\",\n            \"SELECT 123.00 FROM blah\",\n        ),\n        (\n            \"SELECT {self.number:.1f} FROM blah\",\n            \"SELECT 42.0 FROM blah\",\n        ),\n        (\n            \"SELECT * FROM {obj.schema}.{obj.table}\",\n            \"SELECT * FROM my_schema.my_table\",\n        ),\n    ],\n)\ndef test__templat"}, {"start_line": 22000, "end_line": 24000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "f SQLTemplaterError\n            if templating was successful enough that we may move to attempt parsing.\n\n        Raises:\n            SQLTemplaterError: If templating fails fatally, then this method\n                should raise a :obj:`SQLTemplaterError` instead which will be\n                caught and displayed appropriately.\n\n        \"\"\"\n        return TemplatedFile(in_str, fname=fname), []\n\n    @large_file_check\n    def process_with_variants(\n        self,\n        *,\n        in_str: str,\n        fname: str,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n    ) -> Iterator[tuple[TemplatedFile, list[SQLTemplaterError]]]:\n        \"\"\"Extended version of `process` which returns multiple variants.\n\n        Unless explicitly defined, this simply yields the result of .process().\n        \"\"\"\n        yield self.process(\n            in_str=in_str, fname=fname, config=config, formatter=formatter\n        )\n\n    def __eq__(self, other: Any) -> bool:\n        \"\"\"Return true if `other` is of the same class as this one.\n\n        NB: This is useful in comparing configs.\n        \"\"\"\n        return isinstance(other, self.__class__)\n\n    def config_pairs(self) -> list[tuple[str, str]]:\n        \"\"\"Returns info about the given templater for output by the cli.\n\n        Returns:\n            list[tuple[str, str]]: A list of tuples containing information\n                about the given templater. Each tuple contains two strings:\n                the string 'templater' and the name of the templater.\n        \"\"\"\n        return [(\"templater\", self.name)]\n\n    def get_context(\n        self,\n        fname: Optional[str],\n        config: Optional[FluffConfig],\n    ) -> dict[str, Any]:\n        \"\"\"Get the templating context from the config.\n\n        This function retrieves the templating context from the config by\n        loading the config and updating the live_context dictionary with the\n        loaded_context and other predefined context dicti"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "python_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for templaters.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core import FluffConfig, SQLTemplaterError\nfrom sqlfluff.core.errors import SQLFluffSkipFile\nfrom sqlfluff.core.templaters import PythonTemplater\nfrom sqlfluff.core.templaters.base import RawFileSlice, TemplatedFileSlice\nfrom sqlfluff.core.templaters.python import IntermediateFileSlice\n\nPYTHON_STRING = \"SELECT * FROM {blah}\"\n\n\ndef test__templater_python():\n    \"\"\"Test the python templater.\"\"\"\n    t = PythonTemplater(override_context=dict(blah=\"foo\"))\n    instr = PYTHON_STRING\n    outstr, _ = t.process(in_str=instr, fname=\"test\")\n    assert str(outstr) == \"SELECT * FROM foo\"\n\n\ndef test__templater_python_error():\n    \"\"\"Test error handling in the python templater.\"\"\"\n    t = PythonTemplater(override_context=dict(noblah=\"foo\"))\n    instr = PYTHON_STRING\n    with pytest.raises(SQLTemplaterError):\n        t.process(in_str=instr, fname=\"test\")\n\n\n@pytest.mark.parametrize(\n    \"int_slice,templated_str,head_test,tail_test,int_test\",\n    [\n        # Test Invariante\n        (\n            IntermediateFileSlice(\n                \"compound\",\n                slice(0, 5),\n                slice(0, 5),\n                [RawFileSlice(\"{{i}}\", \"templated\", 0)],\n            ),\n            \"foo\",\n            [],\n            [],\n            IntermediateFileSlice(\n                \"compound\",\n                slice(0, 5),\n                slice(0, 5),\n                [RawFileSlice(\"{{i}}\", \"templated\", 0)],\n            ),\n        ),\n        # Test Complete Trimming\n        (\n            IntermediateFileSlice(\n                \"compound\",\n                slice(0, 3),\n                slice(0, 3),\n                [RawFileSlice(\"foo\", \"literal\", 0)],\n            ),\n            \"foo\",\n            [TemplatedFileSlice(\"literal\", slice(0, 3), slice(0, 3))],\n            [],\n            IntermediateFileSlice(\n                \"compound\",\n                slice(3, 3),\n                slice(3, 3),\n                [],\n            ),\n    "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "placeholder.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the placeholder template.\"\"\"\n\nimport logging\nfrom typing import Any, Optional\n\nimport regex\n\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import SQLTemplaterError\nfrom sqlfluff.core.formatter import FormatterInterface\nfrom sqlfluff.core.helpers.slice import offset_slice\nfrom sqlfluff.core.templaters.base import (\n    RawFileSlice,\n    RawTemplater,\n    TemplatedFile,\n    TemplatedFileSlice,\n    large_file_check,\n)\n\n# Instantiate the templater logger\ntemplater_logger = logging.getLogger(\"sqlfluff.templater\")\n\nKNOWN_STYLES = {\n    # e.g. WHERE bla = :name\n    \"colon\": regex.compile(r\"(?<![:\\w\\x5c]):(?P<param_name>\\w+)(?!:)\", regex.UNICODE),\n    # e.g. SELECT :\"column\" FROM :table WHERE bla = :'name'\n    \"colon_optional_quotes\": regex.compile(\n        r\"(?<!:):(?P<quotation>['\\\"]?)(?P<param_name>[\\w_]+)\\1\", regex.UNICODE\n    ),\n    # e.g. WHERE bla = table:name - use with caution as more prone to false positives\n    \"colon_nospaces\": regex.compile(r\"(?<!:):(?P<param_name>\\w+)\", regex.UNICODE),\n    # e.g. WHERE bla = :2\n    \"numeric_colon\": regex.compile(\n        r\"(?<![:\\w\\x5c]):(?P<param_name>\\d+)\", regex.UNICODE\n    ),\n    # e.g. WHERE bla = %(name)s\n    \"pyformat\": regex.compile(\n        r\"(?<![:\\w\\x5c])%\\((?P<param_name>[\\w_]+)\\)s\", regex.UNICODE\n    ),\n    # e.g. WHERE bla = $name or WHERE bla = ${name}\n    \"dollar\": regex.compile(\n        r\"(?<![:\\w\\x5c])\\${?(?P<param_name>[\\w_]+)}?\", regex.UNICODE\n    ),\n    # e.g. WHERE bla = $name$ (DbUp compatible)\n    \"dollar_surround\": regex.compile(\n        r\"(?<![:\\w\\x5c])\\$(?P<param_name>[-\\w]+)\\$\", regex.UNICODE\n    ),\n    # e.g. USE ${flyway:database}.schema_name;\n    \"flyway_var\": regex.compile(r\"\\${(?P<param_name>\\w+[:\\w_]+)}\", regex.UNICODE),\n    # e.g. WHERE bla = ?\n    \"question_mark\": regex.compile(r\"(?<![:\\w\\x5c])\\?\", regex.UNICODE),\n    # e.g. WHERE bla = $3 or WHERE bla = ${3}\n    \"numeric_dollar\": regex.compile(\n        r\"(?<![:\\w\\x5c])\\${?(?P<param_name>[\\d]+)}?\", regex.U"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "placeholder_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for templaters.\"\"\"\n\nimport pytest\n\nfrom sqlfluff.core import FluffConfig\nfrom sqlfluff.core.templaters import PlaceholderTemplater\n\n\ndef test__templater_raw():\n    \"\"\"Test the templaters when nothing has to be replaced.\"\"\"\n    t = PlaceholderTemplater(override_context=dict(param_style=\"colon\"))\n    instr = \"SELECT * FROM {{blah}} WHERE %(gnepr)s OR e~':'\"\n    outstr, _ = t.process(in_str=instr, fname=\"test\")\n    assert str(outstr) == instr\n\n\n@pytest.mark.parametrize(\n    \"instr, param_style, expected_outstr, values\",\n    [\n        (\n            \"SELECT * FROM f, o, o WHERE a < 10\\n\\n\",\n            \"colon\",\n            \"SELECT * FROM f, o, o WHERE a < 10\\n\\n\",\n            dict(\n                unused=7777,\n            ),\n        ),\n        (\n            \"\"\"\n            SELECT user_mail, city_id\n            FROM users_data\n            WHERE userid = :user_id AND date > :start_date\n            \"\"\",\n            \"colon\",\n            \"\"\"\n            SELECT user_mail, city_id\n            FROM users_data\n            WHERE userid = 42 AND date > '2021-10-01'\n            \"\"\",\n            dict(\n                user_id=\"42\",\n                start_date=\"'2021-10-01'\",\n                city_ids=\"(1, 2, 3, 45)\",\n            ),\n        ),\n        (\n            \"\"\"\n            SELECT user_mail, city_id\n            FROM users_data\n            WHERE userid = :user_id AND date > :start_date\"\"\",\n            \"colon\",\n            \"\"\"\n            SELECT user_mail, city_id\n            FROM users_data\n            WHERE userid = 42 AND date > '2021-10-01'\"\"\",\n            dict(\n                user_id=\"42\",\n                start_date=\"'2021-10-01'\",\n                city_ids=\"(1, 2, 3, 45)\",\n            ),\n        ),\n        (\n            \"\"\"\n            SELECT user_mail, city_id\n            FROM users_data\n            WHERE (city_id) IN :city_ids\n            AND date > '2020-10-01'\n            \"\"\",\n            \"colon\",\n            \"\"\"\n            SELECT user_mail, city_id\n            F"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "templater_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/test", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "uffConfig(configs=dbt_fluff_config)\n    templated_file, _ = dbt_templater.process(\n        in_str=path.read_text(),\n        fname=str(path),\n        config=config,\n    )\n    template_output_folder_path = dbt_project_folder / \"templated_output/\"\n    fixture_path = _get_fixture_path(template_output_folder_path, fname)\n    assert str(templated_file) == fixture_path.read_text()\n    # Check we can lex the output too.\n    # https://github.com/sqlfluff/sqlfluff/issues/4013\n    lexer = Lexer(config=config)\n    _, lexing_violations = lexer.lex(templated_file)\n    assert not lexing_violations\n\n\ndef _get_fixture_path(template_output_folder_path, fname):\n    fixture_path: Path = template_output_folder_path / fname  # Default fixture location\n    dbt_version_specific_fixture_folder = \"dbt_utils_0.8.0\"\n    # Determine where it would exist.\n    version_specific_path = (\n        Path(template_output_folder_path) / dbt_version_specific_fixture_folder / fname\n    )\n    if version_specific_path.is_file():\n        # Ok, it exists. Use this path instead.\n        fixture_path = version_specific_path\n    return fixture_path\n\n\n@pytest.mark.parametrize(\n    \"fnames_input, fnames_expected_sequence\",\n    [\n        [\n            (\n                Path(\"models\") / \"depends_on_ephemeral\" / \"a.sql\",\n                Path(\"models\") / \"depends_on_ephemeral\" / \"b.sql\",\n                Path(\"models\") / \"depends_on_ephemeral\" / \"d.sql\",\n            ),\n            # c.sql is not present in the original list and should not appear here,\n            # even though b.sql depends on it. This test ensures that \"out of scope\"\n            # files, e.g. those ignored using \".sqlfluffignore\" or in directories\n            # outside what was specified, are not inadvertently processed.\n            (\n                Path(\"models\") / \"depends_on_ephemeral\" / \"a.sql\",\n                Path(\"models\") / \"depends_on_ephemeral\" / \"b.sql\",\n                Path(\"models\") / \"depends_on_ephemeral\" / \"d.sql\",\n            ),\n     "}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "python.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "se the 'sqlfluff' magic fixed context key. \"\n                        \"https://docs.sqlfluff.com/en/stable/\"\n                        \"perma/python_templating.html\".format(err)\n                    )\n                else:\n                    raise SQLTemplaterError(\n                        \"Failure in Python templating: {}. Have you configured your \"\n                        \"variables? https://docs.sqlfluff.com/en/stable/\"\n                        \"perma/variables.html\".format(err)\n                    )\n            return rendered_str\n\n        raw_sliced, sliced_file, new_str = self.slice_file(\n            in_str,\n            render_func=render_func,\n            config=config,\n        )\n        return (\n            TemplatedFile(\n                source_str=in_str,\n                templated_str=new_str,\n                fname=fname,\n                sliced_file=sliced_file,\n                raw_sliced=raw_sliced,\n            ),\n            [],\n        )\n\n    def slice_file(\n        self,\n        raw_str: str,\n        render_func: Callable[[str], str],\n        config: Optional[FluffConfig] = None,\n        append_to_templated: str = \"\",\n    ) -> tuple[list[RawFileSlice], list[TemplatedFileSlice], str]:\n        \"\"\"Slice the file to determine regions where we can fix.\"\"\"\n        templater_logger.info(\"Slicing File Template\")\n        templater_logger.debug(\"    Raw String: %r\", raw_str)\n        # Render the templated string.\n        # NOTE: This seems excessive in this simple example, but for other templating\n        # engines we need more control over the rendering so may need to call this\n        # method more than once.\n        templated_str = render_func(raw_str)\n        templater_logger.debug(\"    Templated String: %r\", templated_str)\n        # Slice the raw file\n        raw_sliced = list(self._slice_template(raw_str))\n        templater_logger.debug(\"    Raw Sliced:\")\n        for idx, raw_slice in enumerate(raw_sliced):\n            templater_logger.debug(\"        %s: %r\", i"}, {"start_line": 0, "end_line": 863, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Templater Code.\"\"\"\n\nfrom collections.abc import Iterator\n\n# Although these shouldn't usually be instantiated from here\n# we import them to make sure they get registered.\nfrom sqlfluff.core.templaters.base import RawFileSlice, RawTemplater, TemplatedFile\nfrom sqlfluff.core.templaters.jinja import JinjaTemplater\nfrom sqlfluff.core.templaters.placeholder import PlaceholderTemplater\nfrom sqlfluff.core.templaters.python import PythonTemplater\n\n\ndef core_templaters() -> Iterator[type[RawTemplater]]:\n    \"\"\"Returns the templater tuples for the core templaters.\"\"\"\n    yield from [\n        RawTemplater,\n        JinjaTemplater,\n        PythonTemplater,\n        PlaceholderTemplater,\n    ]\n\n\n__all__ = (\n    \"RawFileSlice\",\n    \"TemplatedFile\",\n    \"RawTemplater\",\n    \"JinjaTemplater\",\n    \"PythonTemplater\",\n    \"PlaceholderTemplater\",\n    \"core_templaters\",\n)\n"}], "retrieved_count": 10, "cost_time": 0.32434558868408203}
{"question": "How can SQLFluff's plugin API be used to create custom linting rules?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's plugin API can be used to create custom linting rules through a well-defined extension mechanism that integrates seamlessly with the core system. The plugin API usage works as follows: 1) Rule Definition - Custom rules inherit from BaseRule class and implement the required _eval() method that contains the core linting logic; 2) Metadata Declaration - Rules declare their metadata including code, name, description, groups, and aliases through class attributes or the RuleMetaclass; 3) Plugin Registration - Custom rules are registered through Python's entry point system in setup.py or pyproject.toml, making them discoverable by SQLFluff; 4) Configuration Integration - Plugin rules can define their own configuration options that integrate with SQLFluff's configuration system; 5) Fix Generation - Custom rules can generate LintFix objects to provide automatic corrections for violations they detect; 6) Context Access - Plugin rules have access to the same rule context as built-in rules, including parse tree, configuration, and dialect information; 7) Error Handling - Plugin rules should implement proper error handling to prevent crashes and provide meaningful error messages; 8) Documentation - Custom rules can provide documentation that gets integrated into SQLFluff's help system; 9) Testing - Plugin rules should include comprehensive tests to ensure they work correctly across different SQL dialects and scenarios; 10) Distribution - Custom rules can be distributed as separate Python packages that users can install and configure independently.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 1573, "belongs_to": {"file_name": "rules.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/plugins/sqlfluff-plugin-example/src/sqlfluff_plugin_example", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"An example of a custom rule implemented through the plugin system.\n\nThis uses the rules API supported from 0.4.0 onwards.\n\"\"\"\n\nfrom sqlfluff.core.rules import (\n    BaseRule,\n    LintResult,\n    RuleContext,\n)\nfrom sqlfluff.core.rules.crawlers import SegmentSeekerCrawler\n\n\n# These two decorators allow plugins\n# to be displayed in the sqlfluff docs\nclass Rule_Example_L001(BaseRule):\n    \"\"\"ORDER BY on these columns is forbidden!\n\n    **Anti-pattern**\n\n    Using ``ORDER BY`` one some forbidden columns.\n\n    .. code-block:: sql\n\n        SELECT *\n        FROM foo\n        ORDER BY\n            bar,\n            baz\n\n    **Best practice**\n\n    Do not order by these columns.\n\n    .. code-block:: sql\n\n        SELECT *\n        FROM foo\n        ORDER BY bar\n    \"\"\"\n\n    groups = (\"all\",)\n    config_keywords = [\"forbidden_columns\"]\n    crawl_behaviour = SegmentSeekerCrawler({\"orderby_clause\"})\n    is_fix_compatible = True\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Overwrite __init__ to set config.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.forbidden_columns = [\n            col.strip() for col in self.forbidden_columns.split(\",\")\n        ]\n\n    def _eval(self, context: RuleContext):\n        \"\"\"We should not ORDER BY forbidden_columns.\"\"\"\n        for seg in context.segment.segments:\n            col_name = seg.raw.lower()\n            if col_name in self.forbidden_columns:\n                return LintResult(\n                    anchor=seg,\n                    description=f\"Column `{col_name}` not allowed in ORDER BY.\",\n                )\n"}, {"start_line": 0, "end_line": 1987, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/plugins/sqlfluff-plugin-example/src/sqlfluff_plugin_example", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"An example of a custom rule implemented through the plugin system.\n\nThis uses the rules API supported from 0.4.0 onwards.\n\"\"\"\n\nfrom typing import Any\n\nfrom sqlfluff.core.config import load_config_resource\nfrom sqlfluff.core.plugin import hookimpl\nfrom sqlfluff.core.rules import BaseRule, ConfigInfo\n\n# For backward compatibility we still support importing\n# rules within the body of the root plugin module. This is included\n# here for illustration, but also such that support for this import\n# order can be tested in the test suite (and that the associated\n# warning is triggered).\n# See note below in `get_rules()` for more details.\n# i.e. we DO NOT recommend importing here:\nfrom sqlfluff_plugin_example.rules import Rule_Example_L001  # noqa: F401\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: It is much better that we only import the rule on demand.\n    The root module of the plugin (i.e. this file which contains\n    all of the hook implementations) should have fully loaded before\n    we try and import the rules. This is partly for performance\n    reasons - but more because the definition of a BaseRule requires\n    that all of the get_configs_info() methods have both been\n    defined _and have run_ before so all the validation information\n    is available for the validation steps in the meta class.\n    \"\"\"\n    # i.e. we DO recommend importing here:\n    from sqlfluff_plugin_example.rules import Rule_Example_L001  # noqa: F811\n\n    return [Rule_Example_L001]\n\n\n@hookimpl\ndef load_default_config() -> dict[str, Any]:\n    \"\"\"Loads the default configuration for the plugin.\"\"\"\n    return load_config_resource(\n        package=\"sqlfluff_plugin_example\",\n        file_name=\"plugin_default_config.cfg\",\n    )\n\n\n@hookimpl\ndef get_configs_info() -> dict[str, dict[str, ConfigInfo]]:\n    \"\"\"Get rule config validations and descriptions.\"\"\"\n    return {\n        \"forbidden_columns\": {\"definition\": \"A list of column to forbid\"},\n    }\n"}, {"start_line": 0, "end_line": 1183, "belongs_to": {"file_name": "hookspecs.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/plugin", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the specification to implement a plugin.\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import TYPE_CHECKING, Any\n\nimport pluggy\n\nfrom sqlfluff.core.plugin import plugin_base_name\n\nif TYPE_CHECKING:  # pragma: no cover\n    # NOTE: This import is against the normal import rules, but is here for strict\n    # type checking. We have an exception for this in the import linter.\n    from sqlfluff.core.rules.base import BaseRule\n\nhookspec = pluggy.HookspecMarker(plugin_base_name)\n\n\nclass PluginSpec:\n    \"\"\"Defines the method signatures for plugin implementations.\"\"\"\n\n    @hookspec\n    @abstractmethod\n    def get_rules(self) -> list[type[\"BaseRule\"]]:\n        \"\"\"Get plugin rules.\"\"\"\n\n    @hookspec\n    @abstractmethod\n    def load_default_config(self) -> dict[str, Any]:\n        \"\"\"Loads the default configuration for the plugin.\"\"\"\n\n    @hookspec\n    @abstractmethod\n    # TODO: This type annotation could probably be more specific but that would\n    # require making the config info object something more like a namedTuple rather\n    # than a dict.\n    def get_configs_info(self) -> dict[str, dict[str, Any]]:\n        \"\"\"Get rule config validations and descriptions.\"\"\"\n"}, {"start_line": 0, "end_line": 1269, "belongs_to": {"file_name": "lib.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/plugin", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Base implementation for the plugin.\"\"\"\n\nfrom typing import Any\n\nfrom sqlfluff.core.config import load_config_resource\nfrom sqlfluff.core.plugin import hookimpl\nfrom sqlfluff.core.rules import BaseRule, ConfigInfo\nfrom sqlfluff.core.rules.config_info import STANDARD_CONFIG_INFO_DICT\nfrom sqlfluff.core.rules.loader import get_rules_from_path\nfrom sqlfluff.core.templaters import RawTemplater, core_templaters\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: All standard rules will eventually be loaded as\n    plugins and so before 2.0.0, once all legacy plugin definitions\n    are migrated, this function will be amended to return no rules.\n    \"\"\"\n    return get_rules_from_path()\n\n\n@hookimpl\ndef get_templaters() -> list[type[RawTemplater]]:\n    \"\"\"Get templaters.\"\"\"\n    templaters = list(t for t in core_templaters())\n    return templaters\n\n\n@hookimpl\ndef load_default_config() -> dict[str, Any]:\n    \"\"\"Loads the default configuration for the plugin.\"\"\"\n    return load_config_resource(\n        package=\"sqlfluff.core\",\n        file_name=\"default_config.cfg\",\n    )\n\n\n@hookimpl\ndef get_configs_info() -> dict[str, ConfigInfo]:\n    \"\"\"Get rule config validations and descriptions.\"\"\"\n    return STANDARD_CONFIG_INFO_DICT\n"}, {"start_line": 1000, "end_line": 2820, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e that currently, the gap _before_ and _after_ the semicolon \"\n                \"is considered 'between' statements.\"\n            ),\n        },\n        \"maximum_empty_lines_inside_statements\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The maximum number of empty lines allowed inside statements.\"\n            ),\n        },\n        \"wildcard_policy\": {\n            \"validation\": [\"single\", \"multiple\"],\n            \"definition\": \"Treatment of wildcards. Defaults to ``single``.\",\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.layout.LT01 import Rule_LT01\n    from sqlfluff.rules.layout.LT02 import Rule_LT02\n    from sqlfluff.rules.layout.LT03 import Rule_LT03\n    from sqlfluff.rules.layout.LT04 import Rule_LT04\n    from sqlfluff.rules.layout.LT05 import Rule_LT05\n    from sqlfluff.rules.layout.LT06 import Rule_LT06\n    from sqlfluff.rules.layout.LT07 import Rule_LT07\n    from sqlfluff.rules.layout.LT08 import Rule_LT08\n    from sqlfluff.rules.layout.LT09 import Rule_LT09\n    from sqlfluff.rules.layout.LT10 import Rule_LT10\n    from sqlfluff.rules.layout.LT11 import Rule_LT11\n    from sqlfluff.rules.layout.LT12 import Rule_LT12\n    from sqlfluff.rules.layout.LT13 import Rule_LT13\n    from sqlfluff.rules.layout.LT14 import Rule_LT14\n    from sqlfluff.rules.layout.LT15 import Rule_LT15\n\n    return [\n        Rule_LT01,\n        Rule_LT02,\n        Rule_LT03,\n        Rule_LT04,\n        Rule_LT05,\n        Rule_LT06,\n        Rule_LT07,\n        Rule_LT08,\n        Rule_LT09,\n        Rule_LT10,\n        Rule_LT11,\n        Rule_LT12,\n        Rule_LT13,\n        Rule_LT14,\n        Rule_LT15,\n    ]\n"}, {"start_line": 1000, "end_line": 2364, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/aliasing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tion\": \"How to handle comparison casefolding in an alias.\",\n        },\n        \"min_alias_length\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The minimum length of an alias to allow without raising a violation.\"\n            ),\n        },\n        \"max_alias_length\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The maximum length of an alias to allow without raising a violation.\"\n            ),\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.aliasing.AL01 import Rule_AL01\n    from sqlfluff.rules.aliasing.AL02 import Rule_AL02\n    from sqlfluff.rules.aliasing.AL03 import Rule_AL03\n    from sqlfluff.rules.aliasing.AL04 import Rule_AL04\n    from sqlfluff.rules.aliasing.AL05 import Rule_AL05\n    from sqlfluff.rules.aliasing.AL06 import Rule_AL06\n    from sqlfluff.rules.aliasing.AL07 import Rule_AL07\n    from sqlfluff.rules.aliasing.AL08 import Rule_AL08\n    from sqlfluff.rules.aliasing.AL09 import Rule_AL09\n\n    return [\n        Rule_AL01,\n        Rule_AL02,\n        Rule_AL03,\n        Rule_AL04,\n        Rule_AL05,\n        Rule_AL06,\n        Rule_AL07,\n        Rule_AL08,\n        Rule_AL09,\n    ]\n"}, {"start_line": 0, "end_line": 393, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/plugin", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Marker to be imported and used in plugins (and for own implementations).\"\"\"\n\nfrom typing import Any, Callable, TypeVar, cast\n\nimport pluggy\n\n# Improvement suggested by @oremanj on python/typing gitter\nF = TypeVar(\"F\", bound=Callable[..., Any])\n\nproject_name = \"sqlfluff\"\nplugin_base_name = f\"{project_name}-plugin\"\nhookimpl = cast(Callable[[F], F], pluggy.HookimplMarker(plugin_base_name))\n"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "definition\": (\"Should count(0) be preferred over count(*) and count(1)?\"),\n        },\n        \"multiline_newline\": {\n            \"validation\": [True, False],\n            \"definition\": (\n                \"Should semi-colons be placed on a new line after multi-line \"\n                \"statements?\"\n            ),\n        },\n        \"require_final_semicolon\": {\n            \"validation\": [True, False],\n            \"definition\": (\n                \"Should final semi-colons be required? \"\n                \"(N.B. forcing trailing semi-colons is not recommended for dbt users \"\n                \"as it can cause issues when wrapping the query within other SQL \"\n                \"queries).\"\n            ),\n        },\n        \"preferred_quoted_literal_style\": {\n            \"validation\": [\"consistent\", \"single_quotes\", \"double_quotes\"],\n            \"definition\": (\n                \"Preferred quoting style to use for the quoted literals. If set to \"\n                \"``consistent`` quoting style is derived from the first quoted literal \"\n                \"in the file.\"\n            ),\n        },\n        \"preferred_type_casting_style\": {\n            \"validation\": [\"consistent\", \"shorthand\", \"convert\", \"cast\"],\n            \"definition\": (\"The expectation for using sql type casting\"),\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.convention.CV01 import Rule_CV01\n    from sqlfluff.rules.convention.CV02 import Rule_CV02\n    from sqlfluff.rules.convention.CV03 import Rule_CV03\n    from sqlfluff.rules.convention.CV04 import Rule_CV04\n    from sqlfluff.rules.convention.CV05 import Rule_CV05\n    from sqlfluff.rules.convention.CV06 import Rule_CV06\n    from sqlfluff.rules.convention.CV07 import Rule_CV07\n    from sqlfluff.rules.convention.CV08 import Rule_CV08\n    from sqlfluff.rules.convention.CV09 import Rule_CV09\n    from sql"}, {"start_line": 2000, "end_line": 3407, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "om the first quoted literal \"\n                \"in the file.\"\n            ),\n        },\n        \"preferred_type_casting_style\": {\n            \"validation\": [\"consistent\", \"shorthand\", \"convert\", \"cast\"],\n            \"definition\": (\"The expectation for using sql type casting\"),\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.convention.CV01 import Rule_CV01\n    from sqlfluff.rules.convention.CV02 import Rule_CV02\n    from sqlfluff.rules.convention.CV03 import Rule_CV03\n    from sqlfluff.rules.convention.CV04 import Rule_CV04\n    from sqlfluff.rules.convention.CV05 import Rule_CV05\n    from sqlfluff.rules.convention.CV06 import Rule_CV06\n    from sqlfluff.rules.convention.CV07 import Rule_CV07\n    from sqlfluff.rules.convention.CV08 import Rule_CV08\n    from sqlfluff.rules.convention.CV09 import Rule_CV09\n    from sqlfluff.rules.convention.CV10 import Rule_CV10\n    from sqlfluff.rules.convention.CV11 import Rule_CV11\n    from sqlfluff.rules.convention.CV12 import Rule_CV12\n\n    return [\n        Rule_CV01,\n        Rule_CV02,\n        Rule_CV03,\n        Rule_CV04,\n        Rule_CV05,\n        Rule_CV06,\n        Rule_CV07,\n        Rule_CV08,\n        Rule_CV09,\n        Rule_CV10,\n        Rule_CV11,\n        Rule_CV12,\n    ]\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "rules.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/utils/testing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Testing utils for rule plugins.\"\"\"\n\nfrom collections.abc import Collection\nfrom glob import glob\nfrom typing import NamedTuple, Optional, Union\n\nimport pytest\nimport yaml\n\nfrom sqlfluff.core import Linter\nfrom sqlfluff.core.config import FluffConfig\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n)\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.rules import BaseRule, get_ruleset\nfrom sqlfluff.core.types import ConfigMappingType\n\nFixDictType = dict[str, Union[str, int]]\nViolationDictType = dict[str, Union[str, int, bool, list[FixDictType]]]\n\n\nclass RuleTestCase(NamedTuple):\n    \"\"\"Used like a dataclass by rule tests.\"\"\"\n\n    rule: str\n    desc: Optional[str] = None\n    pass_str: Optional[str] = None\n    fail_str: Optional[str] = None\n    violations: Optional[set[ViolationDictType]] = None\n    fix_str: Optional[str] = None\n    violations_after_fix: Optional[set[ViolationDictType]] = None\n    configs: Optional[ConfigMappingType] = None\n    skip: Optional[str] = None\n    line_numbers: list[int] = []\n\n    def evaluate(self) -> None:\n        \"\"\"Evaluate the test case.\n\n        NOTE: This method is designed to be run in a pytest context and\n        will call methods such as `pytest.skip()` as part of it's execution.\n        It may not be suitable for other testing contexts.\n        \"\"\"\n        rules__test_helper(self)\n\n\ndef load_test_cases(\n    test_cases_path: str,\n) -> tuple[list[str], list[RuleTestCase]]:\n    \"\"\"Load rule test cases from YAML files.\n\n    Args:\n        test_cases_path (str): A glob string specifying the files containing\n            test cases to load.\n    \"\"\"\n    ids = []\n    test_cases = []\n\n    for path in sorted(glob(test_cases_path)):\n        with open(path) as f:\n            raw = f.read()\n\n        y = yaml.safe_load(raw)\n\n        rule = y.pop(\"rule\")\n        global_config = y.pop(\"configs\", None)\n        if global_config:\n            for i "}], "retrieved_count": 10, "cost_time": 0.3409147262573242}
{"question": "How can SQLFluff's BaseRule API be extended to implement new rule types?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's BaseRule API can be extended to implement new rule types through inheritance and customization of the base rule infrastructure. The BaseRule API extension works as follows: 1) Class Inheritance - New rule types inherit from BaseRule class and can override specific methods to customize behavior while maintaining compatibility; 2) _eval() Method Implementation - The core rule logic is implemented in the _eval() method, which receives segments and context and returns violations or fixes; 3) Metadata Customization - Rules can customize their metadata including code, name, description, groups, aliases, and configuration options; 4) Crawl Behavior Configuration - Rules can specify their crawl behavior (root, segment, or statement-level) to control how they traverse the parse tree; 5) Configuration Integration - New rule types can define their own configuration parameters that integrate with SQLFluff's configuration system; 6) Fix Generation - Extended rules can generate LintFix objects to provide automatic corrections, leveraging the existing fix infrastructure; 7) Context Access - Extended rules have access to the full rule context including parse tree, configuration, dialect information, and helper methods; 8) Error Handling - Extended rules can implement custom error handling and validation logic specific to their requirements; 9) Performance Optimization - Extended rules can implement caching and optimization strategies to improve performance; 10) Testing Framework - Extended rules can leverage SQLFluff's testing utilities to ensure they work correctly across different scenarios and dialects.", "score": null, "retrieved_content": [{"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tion for the base classes\n        if name not in (\"BaseRule\",):\n            class_dict = RuleMetaclass._populate_code_and_description(name, class_dict)\n        # Validate rule names\n        rule_name = class_dict.get(\"name\", \"\")\n        if rule_name:\n            if not RuleMetaclass._valid_rule_name_regex.match(rule_name):\n                raise SQLFluffUserError(\n                    f\"Tried to define rule with unexpected \"\n                    f\"name format: {rule_name}. Rule names should be lowercase \"\n                    \"and snake_case with optional `.` characters to indicate \"\n                    \"a namespace or grouping. e.g. `layout.spacing`.\"\n                )\n\n        # Use the stock __new__ method now we've adjusted the docstring.\n        # There are no overload variants of type.__new__ that are compatible, so\n        # we ignore type checking in this case.\n        return super().__new__(mcs, name, bases, class_dict)  # type: ignore\n\n\nclass BaseRule(metaclass=RuleMetaclass):\n    \"\"\"The base class for a rule.\n\n    Args:\n        code (:obj:`str`): The identifier for this rule, used in inclusion\n            or exclusion.\n        description (:obj:`str`): A human readable description of what this\n            rule does. It will be displayed when any violations are found.\n\n    \"\"\"\n\n    _check_docstring = True\n    _works_on_unparsable = True\n    _adjust_anchors = False\n    targets_templated = False\n    # Some fix routines do their own checking for whether their fixes\n    # are safe around templated elements. For those - the default\n    # safety checks might be inappropriate. In those cases, set\n    # template_safe_fixes to True.\n    template_safe_fixes = False\n\n    # Config settings supported for this rule.\n    # See config_info.py for supported values.\n    config_keywords: list[str] = []\n    # Lint loop / crawl behavior. When appropriate, rules can (and should)\n    # override these values to make linting faster.\n    crawl_behaviour: BaseCrawler\n    # Rules can ove"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# names, aliases and description are less appropriate to inherit.\n        # NOTE: This applies in particular to CP02, which inherits all groups\n        # from CP01. If we don't do this, those groups don't show in the docs.\n        for base in reversed(bases):\n            if \"groups\" in class_dict:\n                break\n            elif base.groups:\n                class_dict[\"groups\"] = base.groups\n                break\n\n        # If the rule doesn't itself define `config_keywords`, check the parent\n        # classes for them. If we don't do this then they'll still be available to\n        # the rule, but they won't appear in the docs.\n        for base in reversed(bases):\n            if \"config_keywords\" in class_dict:\n                break\n            elif base.config_keywords:\n                class_dict[\"config_keywords\"] = base.config_keywords\n                break\n\n        class_dict = RuleMetaclass._populate_docstring(name, class_dict)\n        # Don't try and infer code and description for the base classes\n        if name not in (\"BaseRule\",):\n            class_dict = RuleMetaclass._populate_code_and_description(name, class_dict)\n        # Validate rule names\n        rule_name = class_dict.get(\"name\", \"\")\n        if rule_name:\n            if not RuleMetaclass._valid_rule_name_regex.match(rule_name):\n                raise SQLFluffUserError(\n                    f\"Tried to define rule with unexpected \"\n                    f\"name format: {rule_name}. Rule names should be lowercase \"\n                    \"and snake_case with optional `.` characters to indicate \"\n                    \"a namespace or grouping. e.g. `layout.spacing`.\"\n                )\n\n        # Use the stock __new__ method now we've adjusted the docstring.\n        # There are no overload variants of type.__new__ that are compatible, so\n        # we ignore type checking in this case.\n        return super().__new__(mcs, name, bases, class_dict)  # type: ignore\n\n\nclass BaseRule(metaclass=RuleMetaclass):\n  "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Implements the base rule class.\n\nRules crawl through the trees returned by the parser and evaluate particular\nrules.\n\nThe intent is that it should be possible for the rules to be expressed\nas simply as possible, with as much of the complexity abstracted away.\n\nThe evaluation function should take enough arguments that it can evaluate\nthe position of the given segment in relation to its neighbors, and that\nthe segment which finally \"triggers\" the error, should be the one that would\nbe corrected OR if the rule relates to something that is missing, then it\nshould flag on the segment FOLLOWING, the place that the desired element is\nmissing.\n\"\"\"\n\nimport bdb\nimport copy\nimport fnmatch\nimport logging\nimport pathlib\nimport re\nfrom collections import defaultdict, namedtuple\nfrom collections.abc import Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    ClassVar,\n    DefaultDict,\n    Optional,\n    Union,\n)\n\nimport regex\n\nfrom sqlfluff.core.errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a "}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      return f\"LintResult({self.description}: {self.anchor}{fix_coda})\"\n        return f\"LintResult({self.anchor}{fix_coda})\"\n\n    def to_linting_error(self, rule: \"BaseRule\") -> Optional[SQLLintError]:\n        \"\"\"Convert a linting result to a :exc:`SQLLintError` if appropriate.\"\"\"\n        if self.anchor:\n            # Allow description override from the LintResult\n            description = self.description or rule.description\n            return SQLLintError(\n                rule=rule,\n                segment=self.anchor,\n                fixes=self.fixes,\n                description=description,\n            )\n\n        return None\n\n\nEvalResultType = Union[LintResult, list[LintResult], None]\n\n\nclass RuleMetaclass(type):\n    \"\"\"The metaclass for rules.\n\n    This metaclass provides provides auto-enrichment of the\n    rule docstring so that examples, groups, aliases and\n    names are added.\n\n    The reason we enrich the docstring is so that it can be\n    picked up by autodoc and all be displayed in the sqlfluff\n    docs.\n    \"\"\"\n\n    # Precompile the regular expressions\n    _doc_search_regex: ClassVar = re.compile(\n        \"(\\\\s{4}\\\\*\\\\*Anti-pattern\\\\*\\\\*|\\\\s{4}\\\\.\\\\. note::|\"\n        \"\\\\s\\\\s{4}\\\\*\\\\*Configuration\\\\*\\\\*)\",\n        flags=re.MULTILINE,\n    )\n    _valid_classname_regex: ClassVar = regex.compile(\n        r\"Rule_?([A-Z]{1}[a-zA-Z]+)?_([A-Z0-9]{4})\"\n    )\n    _valid_rule_name_regex: ClassVar = regex.compile(r\"[a-z][a-z\\.\\_]+\")\n\n    @staticmethod\n    def _populate_code_and_description(\n        name: str, class_dict: dict[str, Any]\n    ) -> dict[str, Any]:\n        \"\"\"Extract and validate the rule code & description.\n\n        We expect that rules are defined as classes with the name `Rule_XXXX`\n        where `XXXX` is of the form `LLNN`, where L is a letter and N is a\n        two digit number. For backward compatibility we also still support\n        the legacy format of LNNN i.e. a single letter and three digit number.\n\n        The two letters should be indicativ"}, {"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rride this to specify \"post\". \"Post\" rules are those that are\n    # not expected to trigger any downstream rules, e.g. capitalization fixes.\n    # They run on two occasions:\n    # - On the first pass of the main phase\n    # - In a second linter pass after the main phase\n    lint_phase = \"main\"\n    # Groups attribute to be overwritten.\n    groups: tuple[str, ...] = ()\n    # Name attribute to be overwritten.\n    # NOTE: for backward compatibility we should handle the case\n    # where no name is set gracefully.\n    name: str = \"\"\n    # Optional set of aliases for the rule. Most often used for old codes which\n    # referred to this rule.\n    aliases: tuple[str, ...] = ()\n\n    # NOTE: code and description are provided here as hints, but should not\n    # be set directly. They are set automatically by the metaclass based on\n    # the class _name_ when defined.\n    code: str\n    description: str\n\n    # Should we document this rule as fixable? Used by the metaclass to add\n    # a line to the docstring.\n    is_fix_compatible = False\n\n    # Add comma separated string to Base Rule to ensure that it uses the same\n    # Configuration that is defined in the Config.py file\n    split_comma_separated_string = staticmethod(split_comma_separated_string)\n\n    def __init__(self, code: str, description: str, **kwargs: Any) -> None:\n        self.description = description\n        self.code = code\n        # kwargs represents the config passed to the rule. Add all kwargs as class\n        # attributes so they can be accessed in rules which inherit from this class\n        for key, value in kwargs.items():\n            self.__dict__[key] = value\n\n        # We also define a custom logger here, which also includes the code\n        # of the rule in the logging.\n        self.logger = RuleLoggingAdapter(rules_logger, {\"code\": code})\n        # Validate that declared configuration options exist\n        for keyword in self.config_keywords:\n            if keyword not in kwargs.keys():\n                rais"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`LintFix`, optional): An array of any\n            fixes which would correct this issue. If not present then it's\n            assumed that this issue will have to manually fixed.\n        memory (:obj:`dict`, optional): An object which stores any working\n            memory for the rule. The `memory` returned in any `LintResult`\n            will be passed as an input to the next segment to be crawled.\n        description (:obj:`str`, optional): A description of the problem\n            identified as part of this result. This will override the\n            description of the rule as what gets reported to the user\n            with the problem if provided.\n        source (:obj:`str`, optional): A string identifier for what\n            generated the result. Within larger libraries like reflow this\n            can be useful for tracking where a result came from.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        anchor: Optional[BaseSegment] = None,\n        fixes: Optional[list[\"LintFix\"]] = None,\n  "}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "layed in the sqlfluff\n    docs.\n    \"\"\"\n\n    # Precompile the regular expressions\n    _doc_search_regex: ClassVar = re.compile(\n        \"(\\\\s{4}\\\\*\\\\*Anti-pattern\\\\*\\\\*|\\\\s{4}\\\\.\\\\. note::|\"\n        \"\\\\s\\\\s{4}\\\\*\\\\*Configuration\\\\*\\\\*)\",\n        flags=re.MULTILINE,\n    )\n    _valid_classname_regex: ClassVar = regex.compile(\n        r\"Rule_?([A-Z]{1}[a-zA-Z]+)?_([A-Z0-9]{4})\"\n    )\n    _valid_rule_name_regex: ClassVar = regex.compile(r\"[a-z][a-z\\.\\_]+\")\n\n    @staticmethod\n    def _populate_code_and_description(\n        name: str, class_dict: dict[str, Any]\n    ) -> dict[str, Any]:\n        \"\"\"Extract and validate the rule code & description.\n\n        We expect that rules are defined as classes with the name `Rule_XXXX`\n        where `XXXX` is of the form `LLNN`, where L is a letter and N is a\n        two digit number. For backward compatibility we also still support\n        the legacy format of LNNN i.e. a single letter and three digit number.\n\n        The two letters should be indicative of the grouping and focus of\n        the rule. e.g. capitalisation rules have the code CP for CaPitalisation.\n\n        If this receives classes by any other name, then it will raise a\n        :exc:`ValueError`.\n        \"\"\"\n        rule_name_match = RuleMetaclass._valid_classname_regex.match(name)\n        # Validate the name\n        if not rule_name_match:  # pragma: no cover\n            raise SQLFluffUserError(\n                f\"Tried to define rule class with \"\n                f\"unexpected format: {name}. Format should be: \"\n                \"'Rule_PluginName_LL23' (for plugins) or \"\n                \"`Rule_LL23` (for core rules).\"\n            )\n\n        plugin_name, code = rule_name_match.groups()\n        # If the docstring is multiline, then we extract just summary.\n        description = class_dict[\"__doc__\"].replace(\"``\", \"'\").split(\"\\n\")[0]\n        if plugin_name:\n            code = f\"{plugin_name}_{code}\"\n\n        class_dict[\"code\"] = code\n        class_dict[\"description\"] = desc"}, {"start_line": 15000, "end_line": 17000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  \"\"\"The base class for a rule.\n\n    Args:\n        code (:obj:`str`): The identifier for this rule, used in inclusion\n            or exclusion.\n        description (:obj:`str`): A human readable description of what this\n            rule does. It will be displayed when any violations are found.\n\n    \"\"\"\n\n    _check_docstring = True\n    _works_on_unparsable = True\n    _adjust_anchors = False\n    targets_templated = False\n    # Some fix routines do their own checking for whether their fixes\n    # are safe around templated elements. For those - the default\n    # safety checks might be inappropriate. In those cases, set\n    # template_safe_fixes to True.\n    template_safe_fixes = False\n\n    # Config settings supported for this rule.\n    # See config_info.py for supported values.\n    config_keywords: list[str] = []\n    # Lint loop / crawl behavior. When appropriate, rules can (and should)\n    # override these values to make linting faster.\n    crawl_behaviour: BaseCrawler\n    # Rules can override this to specify \"post\". \"Post\" rules are those that are\n    # not expected to trigger any downstream rules, e.g. capitalization fixes.\n    # They run on two occasions:\n    # - On the first pass of the main phase\n    # - In a second linter pass after the main phase\n    lint_phase = \"main\"\n    # Groups attribute to be overwritten.\n    groups: tuple[str, ...] = ()\n    # Name attribute to be overwritten.\n    # NOTE: for backward compatibility we should handle the case\n    # where no name is set gracefully.\n    name: str = \"\"\n    # Optional set of aliases for the rule. Most often used for old codes which\n    # referred to this rule.\n    aliases: tuple[str, ...] = ()\n\n    # NOTE: code and description are provided here as hints, but should not\n    # be set directly. They are set automatically by the metaclass based on\n    # the class _name_ when defined.\n    code: str\n    description: str\n\n    # Should we document this rule as fixable? Used by the metaclass to add\n    # a line to the do"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "errors import SQLFluffUserError, SQLLintError\nfrom sqlfluff.core.helpers.string import split_comma_separated_string\nfrom sqlfluff.core.parser import BaseSegment, RawSegment\nfrom sqlfluff.core.plugin.host import is_main_process, plugins_loaded\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.crawlers import BaseCrawler\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters.base import TemplatedFile\n\n# Best solution for generic types on older python versions\n# https://github.com/python/typeshed/issues/7855\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.config import FluffConfig\n    from sqlfluff.core.dialects import Dialect\n    from sqlfluff.core.plugin.hookspecs import PluginSpec\n    from sqlfluff.core.rules.noqa import IgnoreMask\n\n    _LoggerAdapter = logging.LoggerAdapter[logging.Logger]\nelse:\n    _LoggerAdapter = logging.LoggerAdapter\n\n# The ghost of a rule (mostly used for testing)\nRuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"name\", \"description\"])\n\n# Instantiate the rules logger\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n\n\nclass RuleLoggingAdapter(_LoggerAdapter):\n    \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n\n    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"] if self.extra else \"\", msg), kwargs\n\n\nclass LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n\n    Args:\n        anchor (:obj:`BaseSegment`, optional): A segment which represents\n            the *position* of the problem. NB: Each fix will also hold\n            its own reference to position, so this position is mostly for\n            alerting the user to where the *problem* is.\n        fixes (:obj:`list` of :obj:`Lin"}, {"start_line": 0, "end_line": 1183, "belongs_to": {"file_name": "hookspecs.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/plugin", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the specification to implement a plugin.\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import TYPE_CHECKING, Any\n\nimport pluggy\n\nfrom sqlfluff.core.plugin import plugin_base_name\n\nif TYPE_CHECKING:  # pragma: no cover\n    # NOTE: This import is against the normal import rules, but is here for strict\n    # type checking. We have an exception for this in the import linter.\n    from sqlfluff.core.rules.base import BaseRule\n\nhookspec = pluggy.HookspecMarker(plugin_base_name)\n\n\nclass PluginSpec:\n    \"\"\"Defines the method signatures for plugin implementations.\"\"\"\n\n    @hookspec\n    @abstractmethod\n    def get_rules(self) -> list[type[\"BaseRule\"]]:\n        \"\"\"Get plugin rules.\"\"\"\n\n    @hookspec\n    @abstractmethod\n    def load_default_config(self) -> dict[str, Any]:\n        \"\"\"Loads the default configuration for the plugin.\"\"\"\n\n    @hookspec\n    @abstractmethod\n    # TODO: This type annotation could probably be more specific but that would\n    # require making the config info object something more like a namedTuple rather\n    # than a dict.\n    def get_configs_info(self) -> dict[str, dict[str, Any]]:\n        \"\"\"Get rule config validations and descriptions.\"\"\"\n"}], "retrieved_count": 10, "cost_time": 0.3521735668182373}
{"question": "How can SQLFluff's fix API be leveraged for custom code transformations?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's fix API can be leveraged for custom code transformations through its comprehensive fix generation and application system that provides precise control over SQL code modifications. The fix API for custom transformations works as follows: 1) LintFix Creation - Custom transformations can create LintFix objects that describe specific changes including create, edit, or delete operations on segments; 2) Segment Manipulation - The fix API provides methods to manipulate segments in the parse tree, allowing for complex transformations like restructuring SQL statements; 3) Position-Aware Changes - Fixes maintain accurate position information, enabling precise transformations that preserve code structure and formatting; 4) Batch Operations - Multiple fixes can be generated and applied together, allowing for complex multi-step transformations; 5) Context Preservation - The fix system maintains context information during transformations, ensuring that changes are applied correctly within the broader SQL structure; 6) Validation Integration - Custom transformations can leverage SQLFluff's validation system to ensure that generated code is syntactically correct; 7) Template Awareness - The fix API is template-aware, allowing transformations to work correctly with templated SQL while preserving template structure; 8) Error Handling - Custom transformations can implement error handling to gracefully handle edge cases and provide meaningful error messages; 9) Performance Optimization - The fix API supports efficient transformations through caching and optimized segment manipulation; 10) Integration with Rules - Custom transformations can be integrated with linting rules to provide automatic code improvements based on detected issues.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "01_basic_api_usage.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/examples", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"This is an example of how to use the simple sqlfluff api.\"\"\"\n\nfrom typing import Any, Iterator, Union\n\nimport sqlfluff\n\n#  -------- LINTING ----------\n\nmy_bad_query = \"SeLEct  *, 1, blah as  fOO  from mySchema.myTable\"\n\n# Lint the given string and return an array of violations in JSON representation.\nlint_result = sqlfluff.lint(my_bad_query, dialect=\"bigquery\")\n# lint_result =\n# [\n#     {\n#         \"code\": \"CP01\",\n#         \"line_no\": 1,\n#         \"line_pos\": 1,\n#         \"description\": \"Keywords must be consistently upper case.\",\n#     }\n#     ...\n# ]\n\n#  -------- FIXING ----------\n\n# Fix the given string and get a string back which has been fixed.\nfix_result_1 = sqlfluff.fix(my_bad_query, dialect=\"bigquery\")\n# fix_result_1 = 'SELECT  *, 1, blah AS  foo  FROM myschema.mytable\\n'\n\n# We can also fix just specific rules.\nfix_result_2 = sqlfluff.fix(my_bad_query, rules=[\"CP01\"])\n# fix_result_2 = 'SELECT  *, 1, blah AS  fOO  FROM mySchema.myTable'\n\n# Or a subset of rules...\nfix_result_3 = sqlfluff.fix(my_bad_query, rules=[\"CP01\", \"CP02\"])\n# fix_result_3 = 'SELECT  *, 1, blah AS  fOO  FROM myschema.mytable'\n\n#  -------- PARSING ----------\n\n# Parse the given string and return a JSON representation of the parsed tree.\nparse_result = sqlfluff.parse(my_bad_query)\n# parse_result = {'file': {'statement': {...}, 'newline': '\\n'}}\n\n# This JSON structure can then be parsed as required.\n# An example usage is shown below:\n\n\ndef get_json_segment(\n    parse_result: dict[str, Any], segment_type: str\n) -> Iterator[Union[str, dict[str, Any], list[dict[str, Any]]]]:\n    \"\"\"Recursively search JSON parse result for specified segment type.\n\n    Args:\n        parse_result (Dict[str, Any]): JSON parse result from `sqlfluff.fix`.\n        segment_type (str): The segment type to search for.\n\n    Yields:\n        Iterator[Union[str, dict[str, Any], list[dict[str, Any]]]]:\n        Retrieves children of specified segment type as either a string for a raw\n        segment or as JSON or an array of "}, {"start_line": 36000, "end_line": 38000, "belongs_to": {"file_name": "commands.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/cli", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "=True,\n    help=(\n        \"[DEPRECATED - From 3.0 onward this is the default behaviour] \"\n        \"Apply fixes will also be applied file by file, during the \"\n        \"linting process, rather than waiting until all files are \"\n        \"linted before fixing.\"\n    ),\n)\n@click.option(\n    \"--check\",\n    is_flag=True,\n    help=(\n        \"Analyse all files and ask for confirmation before applying \"\n        \"any fixes. Fixes will be applied all together at the end of \"\n        \"the operation.\"\n    ),\n)\n@click.option(\n    \"-q\",\n    \"--quiet\",\n    is_flag=True,\n    help=(\n        \"Reduces the amount of output to stdout to a minimal level. \"\n        \"This is effectively the opposite of -v. NOTE: It will only \"\n        \"take effect if -f/--force is also set.\"\n    ),\n)\n@click.option(\n    \"-x\",\n    \"--fixed-suffix\",\n    default=None,\n    help=\"An optional suffix to add to fixed files.\",\n)\n@click.option(\n    \"--FIX-EVEN-UNPARSABLE\",\n    is_flag=True,\n    default=None,\n    help=(\n        \"Enables fixing of files that have templating or parse errors. \"\n        \"Note that the similar-sounding '--ignore' or 'noqa' features merely \"\n        \"prevent errors from being *displayed*. For safety reasons, the 'fix'\"\n        \"command will not make any fixes in files that have templating or parse \"\n        \"errors unless '--FIX-EVEN-UNPARSABLE' is enabled on the command line\"\n        \"or in the .sqlfluff config file.\"\n    ),\n)\n@click.option(\n    \"--show-lint-violations\",\n    is_flag=True,\n    help=\"Show lint violations\",\n)\n@click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\ndef fix(\n    force: bool,\n    paths: tuple[str],\n    disregard_sqlfluffignores: bool,\n    check: bool = False,\n    bench: bool = False,\n    quiet: bool = False,\n    fixed_suffix: str = \"\",\n    logger: Optional[logging.Logger] = None,\n    processes: Optional[int] = None,\n    disable_progress_bar: Optional[bool] = False,\n    persist_timing: Optional[str] = None,\n    extra_config_path: Optional[str] = None,\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "patch.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Helpers for generating patches to fix files.\"\"\"\n\nimport logging\nfrom collections.abc import Iterator\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nfrom sqlfluff.core.parser import BaseSegment\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.templaters import TemplatedFile\n\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\n\n@dataclass\nclass FixPatch:\n    \"\"\"An edit patch for a source file.\"\"\"\n\n    templated_slice: slice\n    fixed_raw: str\n    # The patch category, functions mostly for debugging and explanation\n    # than for function. It allows traceability of *why* this patch was\n    # generated. It has no significance for processing.\n    patch_category: str\n    source_slice: slice\n    templated_str: str\n    source_str: str\n\n    def dedupe_tuple(self) -> tuple[slice, str]:\n        \"\"\"Generate a tuple of this fix for deduping.\"\"\"\n        return (self.source_slice, self.fixed_raw)\n\n\ndef _iter_source_fix_patches(\n    segment: BaseSegment, templated_file: TemplatedFile\n) -> Iterator[FixPatch]:\n    \"\"\"Yield any source patches as fixes now.\n\n    NOTE: This yields source fixes for the segment and any of its\n    children, so it's important to call it at the right point in\n    the recursion to avoid yielding duplicates.\n    \"\"\"\n    for source_fix in segment.source_fixes:\n        yield FixPatch(\n            source_fix.templated_slice,\n            source_fix.edit,\n            patch_category=\"source\",\n            source_slice=source_fix.source_slice,\n            templated_str=templated_file.templated_str[source_fix.templated_slice],\n            source_str=templated_file.source_str[source_fix.source_slice],\n        )\n\n\ndef _iter_templated_patches(\n    segment: BaseSegment, templated_file: TemplatedFile\n) -> Iterator[FixPatch]:\n    \"\"\"Iterate through the segments generating fix patches.\n\n    The patches are generated in TEMPLATED space. This is important\n    so that we defer dealing with any loops until later. At this stage\n    ev"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "linted_file_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests covering the LintedFile class and it's methods.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core.linter import LintedFile\nfrom sqlfluff.core.linter.patch import FixPatch\nfrom sqlfluff.core.templaters import RawFileSlice\n\n\n@pytest.mark.parametrize(\n    \"source_slices,source_patches,raw_source_string,expected_result\",\n    # NOTE: For all of these examples we're not setting the patch_category\n    # of the fix patches. They're not used at this step so irrelevant for\n    # testing.\n    [\n        # Trivial example\n        ([slice(0, 1)], [], \"a\", \"a\"),\n        # Simple replacement\n        (\n            [slice(0, 1), slice(1, 2), slice(2, 3)],\n            [FixPatch(slice(1, 2), \"d\", \"\", slice(1, 2), \"b\", \"b\")],\n            \"abc\",\n            \"adc\",\n        ),\n        # Simple insertion\n        (\n            [slice(0, 1), slice(1, 1), slice(1, 2)],\n            [FixPatch(slice(1, 1), \"b\", \"\", slice(1, 1), \"\", \"\")],\n            \"ac\",\n            \"abc\",\n        ),\n        # Simple deletion\n        (\n            [slice(0, 1), slice(1, 2), slice(2, 3)],\n            [FixPatch(slice(1, 2), \"\", \"\", slice(1, 2), \"b\", \"b\")],\n            \"abc\",\n            \"ac\",\n        ),\n        # Illustrative templated example (although practically at\n        # this step, the routine shouldn't care if it's templated).\n        (\n            [slice(0, 2), slice(2, 7), slice(7, 9)],\n            [FixPatch(slice(2, 3), \"{{ b }}\", \"\", slice(2, 7), \"b\", \"{{b}}\")],\n            \"a {{b}} c\",\n            \"a {{ b }} c\",\n        ),\n    ],\n)\ndef test__linted_file__build_up_fixed_source_string(\n    source_slices, source_patches, raw_source_string, expected_result, caplog\n):\n    \"\"\"Test _build_up_fixed_source_string.\n\n    This is part of fix_string().\n    \"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.linter\"):\n        result = LintedFile._build_up_fixed_source_string(\n            source_slices, source_patches, raw_source_string\n        )\n    assert result == expected_result\n\n\n@pytest.m"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fix_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test routines for fixing errors.\"\"\"\n\nimport logging\n\nimport pytest\n\nfrom sqlfluff.core.linter.fix import compute_anchor_edit_info\nfrom sqlfluff.core.linter.patch import FixPatch, generate_source_patches\nfrom sqlfluff.core.parser.markers import PositionMarker\nfrom sqlfluff.core.parser.segments import (\n    BaseSegment,\n    RawSegment,\n    TemplateSegment,\n)\nfrom sqlfluff.core.parser.segments.raw import SourceFix\nfrom sqlfluff.core.rules.fix import LintFix\nfrom sqlfluff.core.templaters import RawFileSlice, TemplatedFile\nfrom sqlfluff.core.templaters.base import TemplatedFileSlice\n\n\n@pytest.fixture(scope=\"module\")\ndef raw_segments(generate_test_segments):\n    \"\"\"Construct a list of raw segments as a fixture.\"\"\"\n    return generate_test_segments([\"foobar\", \".barfoo\"])\n\n\ndef test__rules_base_segments_compute_anchor_edit_info(raw_segments):\n    \"\"\"Test BaseSegment.compute_anchor_edit_info().\"\"\"\n    # Construct a fix buffer, intentionally with:\n    # - one duplicate.\n    # - two different incompatible fixes on the same segment.\n    fixes = [\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"a\")]),\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"a\")]),\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"b\")]),\n    ]\n    anchor_info_dict = compute_anchor_edit_info(fixes)\n    # Check the target segment is the only key we have.\n    assert list(anchor_info_dict.keys()) == [raw_segments[0].uuid]\n    anchor_info = anchor_info_dict[raw_segments[0].uuid]\n    # Check that the duplicate as been deduplicated.\n    # i.e. this isn't 3.\n    assert anchor_info.replace == 2\n    # Check the fixes themselves.\n    # NOTE: There's no duplicated first fix.\n    assert anchor_info.fixes == [\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"a\")]),\n        LintFix.replace(raw_segments[0], [raw_segments[0].edit(raw=\"b\")]),\n    ]\n    # Check the first replace\n    assert anchor_info._first_replace == LintFix.replace(\n        r"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "std_fix_auto_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Automated tests for fixing violations.\n\nAny files in the test/fixtures/linter/autofix directory will be picked up\nand automatically tested against the appropriate dialect.\n\"\"\"\n\nimport json\nimport logging\nimport os\nimport shutil\nimport tempfile\nfrom typing import Optional\n\nimport pytest\nimport yaml\n\nfrom sqlfluff.core import FluffConfig, Linter\nfrom sqlfluff.core.config import clear_config_caches\n\n# Construct the tests from the filepath\ntest_cases = []\nbase_auto_fix_path = (\"test\", \"fixtures\", \"linter\", \"autofix\")\n\n# Generate the filenames for each dialect from the parser test directory\nfor dialect in os.listdir(os.path.join(*base_auto_fix_path)):\n    # Ignore documentation\n    if dialect.endswith(\".md\"):\n        continue\n    # assume that d is now the name of a dialect\n    dirlist = os.listdir(os.path.join(*base_auto_fix_path, dialect))\n    for test_case in dirlist:\n        test_cases.append(\n            (\n                # The dialect\n                dialect,\n                # The directory name\n                test_case,\n            )\n        )\n\n\ndef make_dialect_path(dialect, fname):\n    \"\"\"Work out how to find paths given a dialect and a file name.\"\"\"\n    return os.path.join(\"test\", \"fixtures\", \"dialects\", dialect, fname)\n\n\ndef auto_fix_test(dialect, folder, caplog):\n    \"\"\"A test for roundtrip testing, take a file buffer, lint, fix and lint.\n\n    This is explicitly different from the linter version of this, in that\n    it uses the command line rather than the direct api.\n    \"\"\"\n    # Log just the rules logger for this test.\n    # NOTE: In debugging it may be instructive to enable some of\n    # the other loggers listed here to debug particular issues.\n    # Enabling all of them results in very long logs so use\n    # wisely.\n    # caplog.set_level(logging.DEBUG, logger=\"sqlfluff.templater\")\n    # caplog.set_level(logging.DEBUG, logger=\"sqlfluff.lexer\")\n    caplog.set_level(logging.DEBUG, logger=\"sqlfluff.linter\")\n    caplog.set_level(logging.DEBUG, logger=\"sq"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fix.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Defines the LintFix class, returned by rules when recommending a fix.\"\"\"\n\nimport logging\nfrom collections.abc import Iterable, Sized\nfrom itertools import chain\nfrom typing import Any, Optional, cast\n\nfrom sqlfluff.core.parser import BaseSegment, PositionMarker, RawSegment, SourceFix\nfrom sqlfluff.core.templaters import RawFileSlice, TemplatedFile\n\nrules_logger = logging.getLogger(\"sqlfluff.rules\")\n\n\nclass LintFix:\n    \"\"\"A class to hold a potential fix to a linting violation.\n\n    Args:\n        edit_type (:obj:`str`): One of `create_before`, `create_after`,\n            `replace`, `delete` to indicate the kind of fix this represents.\n        anchor (:obj:`BaseSegment`): A segment which represents\n            the *position* that this fix should be applied at. For deletions\n            it represents the segment to delete, for creations it implies the\n            position to create at (with the existing element at this position\n            to be moved *after* the edit), for a `replace` it implies the\n            segment to be replaced.\n        edit (iterable of :obj:`BaseSegment`, optional): For `replace` and\n            `create` fixes, this holds the iterable of segments to create\n            or replace at the given `anchor` point.\n        source (iterable of :obj:`BaseSegment`, optional): For `replace` and\n            `create` fixes, this holds iterable of segments that provided\n            code. IMPORTANT: The linter uses this to prevent copying material\n            from templated areas.\n    \"\"\"\n\n    def __init__(\n        self,\n        edit_type: str,\n        anchor: BaseSegment,\n        edit: Optional[Iterable[BaseSegment]] = None,\n        source: Optional[Iterable[BaseSegment]] = None,\n    ) -> None:\n        if edit_type not in (\n            \"create_before\",\n            \"create_after\",\n            \"replace\",\n            \"delete\",\n        ):  # pragma: no cover\n            raise ValueError(f\"Unexpected edit_type: {edit_type}\")\n        self.edit_type = edit_type\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "std_roundtrip_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Round trip tests for rules with a fix method.\"\"\"\n\nimport os\nimport re\nimport shutil\nimport tempfile\nfrom io import StringIO\n\nimport pytest\nfrom click.testing import CliRunner\n\nfrom sqlfluff.cli.commands import fix, lint\n\n\ndef generic_roundtrip_test(source_file, rulestring):\n    \"\"\"Run a roundtrip test given a sql file and a rule.\n\n    We take a file buffer, lint, fix and lint, finally checking that\n    the file fails initially but not after fixing.\n    \"\"\"\n    if isinstance(source_file, str):\n        # If it's a string, treat it as a path so lets load it.\n        with open(source_file) as f:\n            source_file = StringIO(f.read())\n\n    filename = \"testing.sql\"\n    # Lets get the path of a file to use\n    tempdir_path = tempfile.mkdtemp()\n    filepath = os.path.join(tempdir_path, filename)\n    # Open the example file and write the content to it\n    with open(filepath, mode=\"w\") as dest_file:\n        for line in source_file:\n            dest_file.write(line)\n    runner = CliRunner()\n    # Check that we first detect the issue\n    result = runner.invoke(lint, [\"--rules\", rulestring, \"--dialect=ansi\", filepath])\n    assert result.exit_code == 1\n    # Fix the file (in force mode)\n    result = runner.invoke(\n        fix, [\"--rules\", rulestring, \"--dialect=ansi\", \"-f\", filepath]\n    )\n    assert result.exit_code == 0\n    # Now lint the file and check for exceptions\n    result = runner.invoke(lint, [\"--rules\", rulestring, \"--dialect=ansi\", filepath])\n    assert result.exit_code == 0\n    shutil.rmtree(tempdir_path)\n\n\ndef jinja_roundtrip_test(\n    source_path, rulestring, sqlfile=\"test.sql\", cfgfile=\".sqlfluff\"\n):\n    \"\"\"Run a roundtrip test path and rule.\n\n    We take a file buffer, lint, fix and lint, finally checking that\n    the file fails initially but not after fixing. Additionally\n    we also check that we haven't messed up the templating tags\n    in the process.\n    \"\"\"\n    tempdir_path = tempfile.mkdtemp()\n    sql_filepath = os.path.join(tempdir_path, sqlfile)"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fix.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Helper classes & methods for applying fixes to segments.\"\"\"\n\nimport logging\nfrom collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlfluff.core.parser import BaseSegment, SourceFix\nfrom sqlfluff.core.rules.fix import LintFix\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.dialects import Dialect\n\n\nlinter_logger = logging.getLogger(\"sqlfluff.linter\")\n\n\n@dataclass\nclass AnchorEditInfo:\n    \"\"\"For a given fix anchor, count of the fix edit types and fixes for it.\"\"\"\n\n    delete: int = field(default=0)\n    replace: int = field(default=0)\n    create_before: int = field(default=0)\n    create_after: int = field(default=0)\n    fixes: list[\"LintFix\"] = field(default_factory=list)\n    source_fixes: list[SourceFix] = field(default_factory=list)\n    # First fix of edit_type \"replace\" in \"fixes\"\n    _first_replace: Optional[\"LintFix\"] = field(default=None)\n\n    def add(self, fix: \"LintFix\") -> None:\n        \"\"\"Adds the fix and updates stats.\n\n        We also allow potentially multiple source fixes on the same\n        anchor by condensing them together here.\n        \"\"\"\n        if fix in self.fixes:\n            # Deduplicate fixes in case it's already in there.\n            return\n\n        if fix.is_just_source_edit():\n            assert fix.edit\n            # is_just_source_edit confirms there will be a list\n            # so we can hint that to mypy.\n            self.source_fixes += fix.edit[0].source_fixes\n            # is there already a replace?\n            if self._first_replace:\n                assert self._first_replace.edit\n                # is_just_source_edit confirms there will be a list\n                # and that's the only way to get into _first_replace\n                # if it's populated so we can hint that to mypy.\n                linter_logger.info(\n                    \"Multiple edits detected, condensing %s onto %s\",\n                    fix,\n                    self._first_repl"}, {"start_line": 36000, "end_line": 38000, "belongs_to": {"file_name": "commands_test.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/test/cli", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "D\",\n        sql_path,\n    ]\n    if method == \"command-line\":\n        if fix_even_unparsable:\n            options.append(\"--FIX-EVEN-UNPARSABLE\")\n    else:\n        assert method == \"config-file\"\n        with open(str(tmpdir / \".sqlfluff\"), \"w\") as f:\n            print(\n                f\"[sqlfluff]\\nfix_even_unparsable = {fix_even_unparsable}\",\n                file=f,\n            )\n    # TRICKY: Switch current directory to the one with the SQL file. Otherwise,\n    # the setting doesn't work. That's because SQLFluff reads it in\n    # sqlfluff.cli.commands.fix(), prior to reading any file-specific settings\n    # (down in sqlfluff.core.linter.Linter._load_raw_file_and_config()).\n    monkeypatch.chdir(str(tmpdir))\n    invoke_assert_code(\n        ret_code=0 if fix_even_unparsable else 1,\n        args=[\n            fix,\n            options,\n        ],\n    )\n    fixed_path = str(tmpdir / \"fix_even_unparsableFIXED.sql\")\n    if fix_even_unparsable:\n        with open(fixed_path, \"r\") as f:\n            fixed_sql = f.read()\n            assert (\n                fixed_sql\n                == \"\"\"SELECT my_col\nFROM my_schema.my_table\nWHERE processdate ! 3\n\"\"\"\n            )\n    else:\n        assert not os.path.isfile(fixed_path)\n\n\n@pytest.mark.parametrize(\n    \"stdin,rules,stdout\",\n    [\n        (\"select * from t\", \"LT02\", \"select * from t\"),  # no change\n        (\n            \" select * from t\",\n            \"LT02\",\n            \"select * from t\",\n        ),  # fix preceding whitespace\n    ],\n)\ndef test__cli__command_fix_stdin(stdin, rules, stdout):\n    \"\"\"Check stdin input for fix works.\"\"\"\n    result = invoke_assert_code(\n        args=[\n            fix,\n            (\"-\", \"--rules\", rules, \"--disable-progress-bar\", \"--dialect=ansi\"),\n        ],\n        cli_input=stdin,\n    )\n    assert result.stdout == stdout\n    assert result.stderr == \"\"\n\n\n@pytest.mark.parametrize(\n    \"stdin,stdout\",\n    [\n        (\"select * from t\\n\", \"select * from t\\n\"),  # no change\n        (\n            \"   sel"}], "retrieved_count": 10, "cost_time": 0.3296377658843994}
{"question": "How can SQLFluff's configuration API be used to implement dynamic rule loading?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's configuration API can be used to implement dynamic rule loading through its flexible configuration system that supports runtime rule management and customization. The configuration API for dynamic rule loading works as follows: 1) Rule Discovery - The configuration system can discover and load rules dynamically based on configuration settings, supporting both built-in and plugin rules; 2) Rule Filtering - Configuration allows filtering rules by various criteria including rule codes, names, groups, and aliases through include/exclude patterns; 3) Rule Enablement - Rules can be dynamically enabled or disabled through configuration settings, allowing for flexible rule sets based on project requirements; 4) Rule Configuration - Each rule can have its own configuration parameters that are loaded dynamically and can be customized per project or environment; 5) Plugin Integration - The configuration system integrates with the plugin system to load custom rules from external packages based on configuration settings; 6) Rule Set Management - Configuration supports defining multiple rule sets that can be loaded and applied dynamically based on context or conditions; 7) Hierarchical Configuration - Rules can be configured at different levels (global, project, file-specific) with proper inheritance and override mechanisms; 8) Validation - The configuration system validates rule configurations and provides error messages for invalid settings; 9) Documentation Integration - Dynamic rule loading integrates with SQLFluff's help system to provide documentation for loaded rules; 10) Performance Optimization - The configuration system caches rule configurations and loading results to improve performance for repeated operations.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 1349, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Configuration and examples for individual rules.\"\"\"\n\nfrom sqlfluff.core.plugin.host import get_plugin_manager\nfrom sqlfluff.core.rules.base import (\n    BaseRule,\n    EvalResultType,\n    LintResult,\n    RuleGhost,\n    RulePack,\n    RuleSet,\n)\nfrom sqlfluff.core.rules.config_info import ConfigInfo, get_config_info\nfrom sqlfluff.core.rules.context import RuleContext\nfrom sqlfluff.core.rules.fix import LintFix\n\n\ndef _load_standard_rules() -> RuleSet:\n    \"\"\"Initialise the standard ruleset.\n\n    We do this on each call so that dynamic rules changes\n    are possible.\n    \"\"\"\n    std_rule_set = RuleSet(name=\"standard\", config_info=get_config_info())\n\n    # Iterate through the rules list and register each rule with the standard set.\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            std_rule_set.register(rule)\n\n    return std_rule_set\n\n\ndef get_ruleset(name: str = \"standard\") -> RuleSet:\n    \"\"\"Get a ruleset by name.\"\"\"\n    std_rules = _load_standard_rules()\n    lookup = {std_rules.name: std_rules}\n    # Return a copy in case someone modifies the register.\n    return lookup[name].copy()\n\n\n__all__ = (\n    \"get_ruleset\",\n    \"RuleSet\",\n    \"RulePack\",\n    \"BaseRule\",\n    \"LintResult\",\n    \"LintFix\",\n    \"RuleContext\",\n    \"RuleGhost\",\n    \"EvalResultType\",\n    \"ConfigInfo\",\n)\n"}, {"start_line": 0, "end_line": 1987, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/plugins/sqlfluff-plugin-example/src/sqlfluff_plugin_example", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"An example of a custom rule implemented through the plugin system.\n\nThis uses the rules API supported from 0.4.0 onwards.\n\"\"\"\n\nfrom typing import Any\n\nfrom sqlfluff.core.config import load_config_resource\nfrom sqlfluff.core.plugin import hookimpl\nfrom sqlfluff.core.rules import BaseRule, ConfigInfo\n\n# For backward compatibility we still support importing\n# rules within the body of the root plugin module. This is included\n# here for illustration, but also such that support for this import\n# order can be tested in the test suite (and that the associated\n# warning is triggered).\n# See note below in `get_rules()` for more details.\n# i.e. we DO NOT recommend importing here:\nfrom sqlfluff_plugin_example.rules import Rule_Example_L001  # noqa: F401\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: It is much better that we only import the rule on demand.\n    The root module of the plugin (i.e. this file which contains\n    all of the hook implementations) should have fully loaded before\n    we try and import the rules. This is partly for performance\n    reasons - but more because the definition of a BaseRule requires\n    that all of the get_configs_info() methods have both been\n    defined _and have run_ before so all the validation information\n    is available for the validation steps in the meta class.\n    \"\"\"\n    # i.e. we DO recommend importing here:\n    from sqlfluff_plugin_example.rules import Rule_Example_L001  # noqa: F811\n\n    return [Rule_Example_L001]\n\n\n@hookimpl\ndef load_default_config() -> dict[str, Any]:\n    \"\"\"Loads the default configuration for the plugin.\"\"\"\n    return load_config_resource(\n        package=\"sqlfluff_plugin_example\",\n        file_name=\"plugin_default_config.cfg\",\n    )\n\n\n@hookimpl\ndef get_configs_info() -> dict[str, dict[str, ConfigInfo]]:\n    \"\"\"Get rule config validations and descriptions.\"\"\"\n    return {\n        \"forbidden_columns\": {\"definition\": \"A list of column to forbid\"},\n    }\n"}, {"start_line": 0, "end_line": 1576, "belongs_to": {"file_name": "loader.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Methods to load rules.\"\"\"\n\nimport os\nfrom glob import glob\nfrom importlib import import_module\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.rules.base import BaseRule\n\n\ndef get_rules_from_path(\n    # All rule files are expected in the format of L*.py\n    rules_path: str = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), \"../../rules\", \"L*.py\")\n    ),\n    base_module: str = \"sqlfluff.rules\",\n) -> list[type[\"BaseRule\"]]:\n    \"\"\"Reads all of the Rule classes from a path into a list.\"\"\"\n    # Create a rules dictionary for importing in\n    # sqlfluff/src/sqlfluff/core/rules/__init__.py\n    rules = []\n\n    for module in sorted(glob(rules_path)):\n        # Manipulate the module path to extract the filename without the .py\n        rule_id = os.path.splitext(os.path.basename(module))[0]\n        # All rule classes are expected in the format of Rule_L*\n        rule_class_name = f\"Rule_{rule_id}\"\n        # NOTE: We import the module outside of the try clause to\n        # properly catch any import errors.\n        rule_module = import_module(f\"{base_module}.{rule_id}\")\n        try:\n            rule_class = getattr(rule_module, rule_class_name)\n        except AttributeError as e:\n            raise AttributeError(\n                \"Rule classes must be named in the format of Rule_*. \"\n                f\"[{rule_class_name}]\"\n            ) from e\n        # Add the rules to the rules dictionary for\n        # sqlfluff/src/sqlfluff/core/rules/__init__.py\n        rules.append(rule_class)\n\n    return rules\n"}, {"start_line": 1000, "end_line": 2820, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e that currently, the gap _before_ and _after_ the semicolon \"\n                \"is considered 'between' statements.\"\n            ),\n        },\n        \"maximum_empty_lines_inside_statements\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The maximum number of empty lines allowed inside statements.\"\n            ),\n        },\n        \"wildcard_policy\": {\n            \"validation\": [\"single\", \"multiple\"],\n            \"definition\": \"Treatment of wildcards. Defaults to ``single``.\",\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.layout.LT01 import Rule_LT01\n    from sqlfluff.rules.layout.LT02 import Rule_LT02\n    from sqlfluff.rules.layout.LT03 import Rule_LT03\n    from sqlfluff.rules.layout.LT04 import Rule_LT04\n    from sqlfluff.rules.layout.LT05 import Rule_LT05\n    from sqlfluff.rules.layout.LT06 import Rule_LT06\n    from sqlfluff.rules.layout.LT07 import Rule_LT07\n    from sqlfluff.rules.layout.LT08 import Rule_LT08\n    from sqlfluff.rules.layout.LT09 import Rule_LT09\n    from sqlfluff.rules.layout.LT10 import Rule_LT10\n    from sqlfluff.rules.layout.LT11 import Rule_LT11\n    from sqlfluff.rules.layout.LT12 import Rule_LT12\n    from sqlfluff.rules.layout.LT13 import Rule_LT13\n    from sqlfluff.rules.layout.LT14 import Rule_LT14\n    from sqlfluff.rules.layout.LT15 import Rule_LT15\n\n    return [\n        Rule_LT01,\n        Rule_LT02,\n        Rule_LT03,\n        Rule_LT04,\n        Rule_LT05,\n        Rule_LT06,\n        Rule_LT07,\n        Rule_LT08,\n        Rule_LT09,\n        Rule_LT10,\n        Rule_LT11,\n        Rule_LT12,\n        Rule_LT13,\n        Rule_LT14,\n        Rule_LT15,\n    ]\n"}, {"start_line": 1000, "end_line": 2364, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/aliasing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tion\": \"How to handle comparison casefolding in an alias.\",\n        },\n        \"min_alias_length\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The minimum length of an alias to allow without raising a violation.\"\n            ),\n        },\n        \"max_alias_length\": {\n            \"validation\": range(1000),\n            \"definition\": (\n                \"The maximum length of an alias to allow without raising a violation.\"\n            ),\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.aliasing.AL01 import Rule_AL01\n    from sqlfluff.rules.aliasing.AL02 import Rule_AL02\n    from sqlfluff.rules.aliasing.AL03 import Rule_AL03\n    from sqlfluff.rules.aliasing.AL04 import Rule_AL04\n    from sqlfluff.rules.aliasing.AL05 import Rule_AL05\n    from sqlfluff.rules.aliasing.AL06 import Rule_AL06\n    from sqlfluff.rules.aliasing.AL07 import Rule_AL07\n    from sqlfluff.rules.aliasing.AL08 import Rule_AL08\n    from sqlfluff.rules.aliasing.AL09 import Rule_AL09\n\n    return [\n        Rule_AL01,\n        Rule_AL02,\n        Rule_AL03,\n        Rule_AL04,\n        Rule_AL05,\n        Rule_AL06,\n        Rule_AL07,\n        Rule_AL08,\n        Rule_AL09,\n    ]\n"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "loaded.get():\n            # Show a warning if a plugin has their imports set up in a suboptimal\n            # way. The example plugin imports the rules in both ways, to test the\n            # triggering of this warning.\n            rules_logger.warning(\n                f\"Rule {name!r} has been imported before all plugins \"\n                \"have been fully loaded. For best performance, plugins \"\n                \"should import any rule definitions within their `get_rules()` \"\n                \"method. Please update your plugin to remove this warning. See: \"\n                \"https://docs.sqlfluff.com/en/stable/perma/plugin_dev.html\"\n            )\n        elif class_dict.get(\"config_keywords\", []):\n            config_docs = \"\\n    **Configuration**\\n\"\n            config_info = get_config_info()\n            for keyword in sorted(class_dict[\"config_keywords\"]):\n                try:\n                    info_dict = config_info[keyword]\n                except KeyError:  # pragma: no cover\n                    # NOTE: For rule developers, please define config info values\n                    # within the specific rule bundle rather than in the central\n                    # `config_info` package unless the value is necessary for\n                    # multiple rules.\n                    raise KeyError(\n                        \"Config value {!r} for rule {} is not configured in \"\n                        \"`config_info`.\".format(keyword, name)\n                    )\n                config_docs += \"\\n    * ``{}``: {}\".format(\n                    keyword, info_dict[\"definition\"]\n                )\n                if (\n                    config_docs[-1] != \".\"\n                    and config_docs[-1] != \"?\"\n                    and config_docs[-1] != \"\\n\"\n                ):\n                    config_docs += \".\"\n                if \"validation\" in info_dict:\n                    config_docs += \" Must be one of ``{}``.\".format(\n                        info_dict[\"validation\"]\n                 "}, {"start_line": 0, "end_line": 1269, "belongs_to": {"file_name": "lib.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/core/plugin", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Base implementation for the plugin.\"\"\"\n\nfrom typing import Any\n\nfrom sqlfluff.core.config import load_config_resource\nfrom sqlfluff.core.plugin import hookimpl\nfrom sqlfluff.core.rules import BaseRule, ConfigInfo\nfrom sqlfluff.core.rules.config_info import STANDARD_CONFIG_INFO_DICT\nfrom sqlfluff.core.rules.loader import get_rules_from_path\nfrom sqlfluff.core.templaters import RawTemplater, core_templaters\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: All standard rules will eventually be loaded as\n    plugins and so before 2.0.0, once all legacy plugin definitions\n    are migrated, this function will be amended to return no rules.\n    \"\"\"\n    return get_rules_from_path()\n\n\n@hookimpl\ndef get_templaters() -> list[type[RawTemplater]]:\n    \"\"\"Get templaters.\"\"\"\n    templaters = list(t for t in core_templaters())\n    return templaters\n\n\n@hookimpl\ndef load_default_config() -> dict[str, Any]:\n    \"\"\"Loads the default configuration for the plugin.\"\"\"\n    return load_config_resource(\n        package=\"sqlfluff.core\",\n        file_name=\"default_config.cfg\",\n    )\n\n\n@hookimpl\ndef get_configs_info() -> dict[str, ConfigInfo]:\n    \"\"\"Get rule config validations and descriptions.\"\"\"\n    return STANDARD_CONFIG_INFO_DICT\n"}, {"start_line": 1000, "end_line": 2695, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      \"validation\": [\"all\", \"aliases\", \"column_aliases\", \"table_aliases\", \"none\"],\n            \"definition\": \"Types of quoted identifiers to flag violations for.\",\n        },\n        \"allow_space_in_identifier\": {\n            \"validation\": [True, False],\n            \"definition\": (\"Should spaces in identifiers be allowed?\"),\n        },\n        \"additional_allowed_characters\": {\n            \"definition\": (\n                \"Optional list of extra allowed characters, \"\n                \"in addition to alphanumerics (A-Z, a-z, 0-9) and underscores.\"\n            ),\n        },\n        \"prefer_quoted_identifiers\": {\n            \"validation\": [True, False],\n            \"definition\": (\n                \"If ``True``, requires every identifier to be quoted. \"\n                \"Defaults to ``False``.\"\n            ),\n        },\n        \"prefer_quoted_keywords\": {\n            \"validation\": [True, False],\n            \"definition\": (\n                \"If ``True``, requires every keyword used as an identifier to be \"\n                \"quoted. Defaults to ``False``.\"\n            ),\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.references.RF01 import Rule_RF01\n    from sqlfluff.rules.references.RF02 import Rule_RF02\n    from sqlfluff.rules.references.RF03 import Rule_RF03\n    from sqlfluff.rules.references.RF04 import Rule_RF04\n    from sqlfluff.rules.references.RF05 import Rule_RF05\n    from sqlfluff.rules.references.RF06 import Rule_RF06\n\n    return [Rule_RF01, Rule_RF02, Rule_RF03, Rule_RF04, Rule_RF05, Rule_RF06]\n"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "definition\": (\"Should count(0) be preferred over count(*) and count(1)?\"),\n        },\n        \"multiline_newline\": {\n            \"validation\": [True, False],\n            \"definition\": (\n                \"Should semi-colons be placed on a new line after multi-line \"\n                \"statements?\"\n            ),\n        },\n        \"require_final_semicolon\": {\n            \"validation\": [True, False],\n            \"definition\": (\n                \"Should final semi-colons be required? \"\n                \"(N.B. forcing trailing semi-colons is not recommended for dbt users \"\n                \"as it can cause issues when wrapping the query within other SQL \"\n                \"queries).\"\n            ),\n        },\n        \"preferred_quoted_literal_style\": {\n            \"validation\": [\"consistent\", \"single_quotes\", \"double_quotes\"],\n            \"definition\": (\n                \"Preferred quoting style to use for the quoted literals. If set to \"\n                \"``consistent`` quoting style is derived from the first quoted literal \"\n                \"in the file.\"\n            ),\n        },\n        \"preferred_type_casting_style\": {\n            \"validation\": [\"consistent\", \"shorthand\", \"convert\", \"cast\"],\n            \"definition\": (\"The expectation for using sql type casting\"),\n        },\n    }\n\n\n@hookimpl\ndef get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.convention.CV01 import Rule_CV01\n    from sqlfluff.rules.convention.CV02 import Rule_CV02\n    from sqlfluff.rules.convention.CV03 import Rule_CV03\n    from sqlfluff.rules.convention.CV04 import Rule_CV04\n    from sqlfluff.rules.convention.CV05 import Rule_CV05\n    from sqlfluff.rules.convention.CV06 import Rule_CV06\n    from sqlfluff.rules.convention.CV07 import Rule_CV07\n    from sqlfluff.rules.convention.CV08 import Rule_CV08\n    from sqlfluff.rules.convention.CV09 import Rule_CV09\n    from sql"}, {"start_line": 0, "end_line": 1573, "belongs_to": {"file_name": "rules.py", "upper_path": "/data2/raymone/swebench-repos/sqlfluff/plugins/sqlfluff-plugin-example/src/sqlfluff_plugin_example", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"An example of a custom rule implemented through the plugin system.\n\nThis uses the rules API supported from 0.4.0 onwards.\n\"\"\"\n\nfrom sqlfluff.core.rules import (\n    BaseRule,\n    LintResult,\n    RuleContext,\n)\nfrom sqlfluff.core.rules.crawlers import SegmentSeekerCrawler\n\n\n# These two decorators allow plugins\n# to be displayed in the sqlfluff docs\nclass Rule_Example_L001(BaseRule):\n    \"\"\"ORDER BY on these columns is forbidden!\n\n    **Anti-pattern**\n\n    Using ``ORDER BY`` one some forbidden columns.\n\n    .. code-block:: sql\n\n        SELECT *\n        FROM foo\n        ORDER BY\n            bar,\n            baz\n\n    **Best practice**\n\n    Do not order by these columns.\n\n    .. code-block:: sql\n\n        SELECT *\n        FROM foo\n        ORDER BY bar\n    \"\"\"\n\n    groups = (\"all\",)\n    config_keywords = [\"forbidden_columns\"]\n    crawl_behaviour = SegmentSeekerCrawler({\"orderby_clause\"})\n    is_fix_compatible = True\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Overwrite __init__ to set config.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.forbidden_columns = [\n            col.strip() for col in self.forbidden_columns.split(\",\")\n        ]\n\n    def _eval(self, context: RuleContext):\n        \"\"\"We should not ORDER BY forbidden_columns.\"\"\"\n        for seg in context.segment.segments:\n            col_name = seg.raw.lower()\n            if col_name in self.forbidden_columns:\n                return LintResult(\n                    anchor=seg,\n                    description=f\"Column `{col_name}` not allowed in ORDER BY.\",\n                )\n"}], "retrieved_count": 10, "cost_time": 0.34752631187438965}
