{"question": "What does sklearn.preprocessing.StandardScaler do to target variables?", "answer": null, "relative_code_list": null, "ground_truth": "StandardScaler does NOT directly operate on target variables (y). It is designed specifically for feature scaling and has the following relationship with target variables:\n\n1. **No Direct Target Processing**:\n   - **StandardScaler.transform()**: Only accepts X (features), not y (targets)\n   - **Target Ignorance**: The scaler is completely unaware of target variables\n   - **Feature-Only Focus**: Designed exclusively for feature preprocessing\n   - **No y Parameter**: The transform method signature is transform(X), not transform(X, y)\n\n2. **Target Variable Handling**:\n   - **Separate Processing**: Target variables must be handled separately if scaling is needed\n   - **TransformedTargetRegressor**: For regression, use this wrapper to scale targets\n   - **Manual Scaling**: Can manually apply scaling to target variables using scale() function\n   - **Pipeline Separation**: Targets are not part of the feature preprocessing pipeline\n\n3. **When Target Scaling is Needed**:\n   - **Regression Problems**: When target variables have very different scales\n   - **Multi-output Regression**: When different outputs have different scales\n   - **Neural Networks**: When using neural network regressors\n   - **Gradient-based Methods**: When using algorithms sensitive to target scale\n\n4. **Alternative Approaches for Target Scaling**:\n   - **TransformedTargetRegressor**: Wraps a regressor with target transformation\n   - **FunctionTransformer**: Can be used in pipelines for target transformation\n   - **Manual Preprocessing**: Apply scaling functions directly to target arrays\n   - **Custom Transformers**: Create custom transformers for target-specific processing\n\n5. **Pipeline Considerations**:\n   - **Feature Pipeline**: StandardScaler belongs in feature preprocessing pipeline\n   - **Target Pipeline**: Target scaling should be handled separately\n   - **Cross-validation**: Target scaling should be applied within cross-validation folds\n   - **Data Leakage**: Must prevent target information from leaking into feature preprocessing\n\n6. **Common Use Cases**:\n   - **Feature Standardization**: StandardScaler is used for feature standardization\n   - **Target Transformation**: Other methods are used for target transformation\n   - **Combined Approach**: Use both feature and target scaling when appropriate\n   - **Model-specific**: Some models handle target scaling internally\n\n7. **Best Practices**:\n   - **Feature Scaling**: Use StandardScaler for feature preprocessing\n   - **Target Scaling**: Use appropriate target transformation methods\n   - **Consistency**: Apply same scaling to training and test data\n   - **Validation**: Ensure scaling doesn't introduce data leakage\n   - **Documentation**: Clearly document both feature and target preprocessing steps", "score": null, "retrieved_content": [{"start_line": 37000, "end_line": 39000, "belongs_to": {"file_name": "test_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "se.scale_)\n\n    assert_array_almost_equal(\n        X_scaled.mean(axis=0), [0.0, 1.109, 1.856, 21.0, 1.559], 2\n    )\n    assert_array_almost_equal(X_scaled.std(axis=0), [0.0, 1.0, 1.0, 1.0, 1.0])\n\n    X_sparse_scaled_mean, X_sparse_scaled_std = mean_variance_axis(\n        X_sparse_scaled.astype(float), 0\n    )\n    assert_array_almost_equal(X_sparse_scaled_mean, X_scaled.mean(axis=0))\n    assert_array_almost_equal(X_sparse_scaled_std, X_scaled.std(axis=0))\n\n    # Check that X has not been modified (copy)\n    assert X_scaled is not X\n    assert X_sparse_scaled is not X_sparse\n\n    X_scaled_back = scaler.inverse_transform(X_scaled)\n    assert X_scaled_back is not X\n    assert X_scaled_back is not X_scaled\n    assert_array_almost_equal(X_scaled_back, X)\n\n    X_sparse_scaled_back = scaler_sparse.inverse_transform(X_sparse_scaled)\n    assert X_sparse_scaled_back is not X_sparse\n    assert X_sparse_scaled_back is not X_sparse_scaled\n    assert_array_almost_equal(X_sparse_scaled_back.toarray(), X)\n\n    if sparse_container in CSR_CONTAINERS:\n        null_transform = StandardScaler(with_mean=False, with_std=False, copy=True)\n        with warnings.catch_warnings(record=True):\n            X_null = null_transform.fit_transform(X_sparse)\n        assert_array_equal(X_null.data, X_sparse.data)\n        X_orig = null_transform.inverse_transform(X_null)\n        assert_array_equal(X_orig.data, X_sparse.data)\n\n\n@pytest.mark.parametrize(\"sparse_container\", CSR_CONTAINERS + CSC_CONTAINERS)\ndef test_scaler_without_copy(sparse_container):\n    # Check that StandardScaler.fit does not change input\n    rng = np.random.RandomState(42)\n    X = rng.randn(4, 5)\n    X[:, 0] = 0.0  # first feature is always of zero\n    X_sparse = sparse_container(X)\n\n    X_copy = X.copy()\n    StandardScaler(copy=False).fit(X)\n    assert_array_equal(X, X_copy)\n\n    X_sparse_copy = X_sparse.copy()\n    StandardScaler(with_mean=False, copy=False).fit(X_sparse)\n    assert_array_equal(X_sparse.toarray(), X_sparse_copy.toar"}, {"start_line": 25000, "end_line": 27000, "belongs_to": {"file_name": "_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "zers of linear models) assume that\n    all features are centered around 0 and have variance in the same\n    order. If a feature has a variance that is orders of magnitude larger\n    than others, it might dominate the objective function and make the\n    estimator unable to learn from other features correctly as expected.\n\n    `StandardScaler` is sensitive to outliers, and the features may scale\n    differently from each other in the presence of outliers. For an example\n    visualization, refer to :ref:`Compare StandardScaler with other scalers\n    <plot_all_scaling_standard_scaler_section>`.\n\n    This scaler can also be applied to sparse CSR or CSC matrices by passing\n    `with_mean=False` to avoid breaking the sparsity structure of the data.\n\n    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n\n    Parameters\n    ----------\n    copy : bool, default=True\n        If False, try to avoid a copy and do inplace scaling instead.\n        This is not guaranteed to always work inplace; e.g. if the data is\n        not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n        returned.\n\n    with_mean : bool, default=True\n        If True, center the data before scaling.\n        This does not work (and will raise an exception) when attempted on\n        sparse matrices, because centering them entails building a dense\n        matrix which in common use cases is likely to be too large to fit in\n        memory.\n\n    with_std : bool, default=True\n        If True, scale the data to unit variance (or equivalently,\n        unit standard deviation).\n\n    Attributes\n    ----------\n    scale_ : ndarray of shape (n_features,) or None\n        Per feature relative scaling of the data to achieve zero mean and unit\n        variance. Generally this is calculated using `np.sqrt(var_)`. If a\n        variance is zero, we can't achieve unit variance, and the data is left\n        as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n        when `with_std=False`.\n\n   "}, {"start_line": 15000, "end_line": 17000, "belongs_to": {"file_name": "test_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "es * [0.0])\n    X_scaled = scale(X, axis=1, with_std=True)\n    assert not np.any(np.isnan(X_scaled))\n    assert_array_almost_equal(X_scaled.mean(axis=1), n_samples * [0.0])\n    assert_array_almost_equal(X_scaled.std(axis=1), n_samples * [1.0])\n    # Check that the data hasn't been modified\n    assert X_scaled is not X\n\n    X_scaled = scaler.fit(X).transform(X, copy=False)\n    assert not np.any(np.isnan(X_scaled))\n    assert_array_almost_equal(X_scaled.mean(axis=0), n_features * [0.0])\n    assert_array_almost_equal(X_scaled.std(axis=0), [0.0, 1.0, 1.0, 1.0, 1.0])\n    # Check that X has not been copied\n    assert X_scaled is X\n\n    X = rng.randn(4, 5)\n    X[:, 0] = 1.0  # first feature is a constant, non zero feature\n    scaler = StandardScaler()\n    X_scaled = scaler.fit(X).transform(X, copy=True)\n    assert not np.any(np.isnan(X_scaled))\n    assert_array_almost_equal(X_scaled.mean(axis=0), n_features * [0.0])\n    assert_array_almost_equal(X_scaled.std(axis=0), [0.0, 1.0, 1.0, 1.0, 1.0])\n    # Check that X has not been copied\n    assert X_scaled is not X\n\n\ndef test_scaler_float16_overflow():\n    # Test if the scaler will not overflow on float16 numpy arrays\n    rng = np.random.RandomState(0)\n    # float16 has a maximum of 65500.0. On the worst case 5 * 200000 is 100000\n    # which is enough to overflow the data type\n    X = rng.uniform(5, 10, [200000, 1]).astype(np.float16)\n\n    with np.errstate(over=\"raise\"):\n        scaler = StandardScaler().fit(X)\n        X_scaled = scaler.transform(X)\n\n    # Calculate the float64 equivalent to verify result\n    X_scaled_f64 = StandardScaler().fit_transform(X.astype(np.float64))\n\n    # Overflow calculations may cause -inf, inf, or nan. Since there is no nan\n    # input, all of the outputs should be finite. This may be redundant since a\n    # FloatingPointError exception will be thrown on overflow above.\n    assert np.all(np.isfinite(X_scaled))\n\n    # The normal distribution is very unlikely to go above 4. At 4.0-8.0 the\n    # floa"}, {"start_line": 24000, "end_line": 26000, "belongs_to": {"file_name": "_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " and scaling to unit variance.\n\n    The standard score of a sample `x` is calculated as:\n\n    .. code-block:: text\n\n        z = (x - u) / s\n\n    where `u` is the mean of the training samples or zero if `with_mean=False`,\n    and `s` is the standard deviation of the training samples or one if\n    `with_std=False`.\n\n    Centering and scaling happen independently on each feature by computing\n    the relevant statistics on the samples in the training set. Mean and\n    standard deviation are then stored to be used on later data using\n    :meth:`transform`.\n\n    Standardization of a dataset is a common requirement for many\n    machine learning estimators: they might behave badly if the\n    individual features do not more or less look like standard normally\n    distributed data (e.g. Gaussian with 0 mean and unit variance).\n\n    For instance many elements used in the objective function of\n    a learning algorithm (such as the RBF kernel of Support Vector\n    Machines or the L1 and L2 regularizers of linear models) assume that\n    all features are centered around 0 and have variance in the same\n    order. If a feature has a variance that is orders of magnitude larger\n    than others, it might dominate the objective function and make the\n    estimator unable to learn from other features correctly as expected.\n\n    `StandardScaler` is sensitive to outliers, and the features may scale\n    differently from each other in the presence of outliers. For an example\n    visualization, refer to :ref:`Compare StandardScaler with other scalers\n    <plot_all_scaling_standard_scaler_section>`.\n\n    This scaler can also be applied to sparse CSR or CSC matrices by passing\n    `with_mean=False` to avoid breaking the sparsity structure of the data.\n\n    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n\n    Parameters\n    ----------\n    copy : bool, default=True\n        If False, try to avoid a copy and do inplace scaling instead.\n        This is not guaranteed to always work inplace"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "test_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "if _check_dim_1axis(X) == 1:\n            assert_almost_equal(scaler.mean_, X.ravel())\n            assert_almost_equal(scaler.scale_, np.ones(n_features))\n            assert_array_almost_equal(X_scaled.mean(axis=0), np.zeros_like(n_features))\n            assert_array_almost_equal(X_scaled.std(axis=0), np.zeros_like(n_features))\n        else:\n            assert_almost_equal(scaler.mean_, X.mean())\n            assert_almost_equal(scaler.scale_, X.std())\n            assert_array_almost_equal(X_scaled.mean(axis=0), np.zeros_like(n_features))\n            assert_array_almost_equal(X_scaled.mean(axis=0), 0.0)\n            assert_array_almost_equal(X_scaled.std(axis=0), 1.0)\n        assert scaler.n_samples_seen_ == X.shape[0]\n\n        # check inverse transform\n        X_scaled_back = scaler.inverse_transform(X_scaled)\n        assert_array_almost_equal(X_scaled_back, X)\n\n    # Constant feature\n    X = np.ones((5, 1))\n    scaler = StandardScaler()\n    X_scaled = scaler.fit(X).transform(X, copy=True)\n    assert_almost_equal(scaler.mean_, 1.0)\n    assert_almost_equal(scaler.scale_, 1.0)\n    assert_array_almost_equal(X_scaled.mean(axis=0), 0.0)\n    assert_array_almost_equal(X_scaled.std(axis=0), 0.0)\n    assert scaler.n_samples_seen_ == X.shape[0]\n\n\n@pytest.mark.parametrize(\"sparse_container\", [None] + CSC_CONTAINERS + CSR_CONTAINERS)\n@pytest.mark.parametrize(\"add_sample_weight\", [False, True])\ndef test_standard_scaler_dtype(add_sample_weight, sparse_container):\n    # Ensure scaling does not affect dtype\n    rng = np.random.RandomState(0)\n    n_samples = 10\n    n_features = 3\n    if add_sample_weight:\n        sample_weight = np.ones(n_samples)\n    else:\n        sample_weight = None\n    with_mean = True\n    if sparse_container is not None:\n        # scipy sparse containers do not support float16, see\n        # https://github.com/scipy/scipy/issues/7408 for more details.\n        supported_dtype = [np.float64, np.float32]\n    else:\n        supported_dtype = [np.float64, np.float32, n"}, {"start_line": 27000, "end_line": 29000, "belongs_to": {"file_name": "_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     .. versionadded:: 0.17\n           *scale_*\n\n    mean_ : ndarray of shape (n_features,) or None\n        The mean value for each feature in the training set.\n        Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.\n\n    var_ : ndarray of shape (n_features,) or None\n        The variance for each feature in the training set. Used to compute\n        `scale_`. Equal to ``None`` when ``with_mean=False`` and\n        ``with_std=False``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_seen_ : int or ndarray of shape (n_features,)\n        The number of samples processed by the estimator for each feature.\n        If there are no missing samples, the ``n_samples_seen`` will be an\n        integer, otherwise it will be an array of dtype int. If\n        `sample_weights` are used it will be a float (if no missing data)\n        or an array of dtype float that sums the weights seen so far.\n        Will be reset on new calls to fit, but increments across\n        ``partial_fit`` calls.\n\n    See Also\n    --------\n    scale : Equivalent function without the estimator API.\n\n    :class:`~sklearn.decomposition.PCA` : Further removes the linear\n        correlation across features with 'whiten=True'.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in fit, and maintained in\n    transform.\n\n    We use a biased estimator for the standard deviation, equivalent to\n    `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n    affect model performance.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n    >>> scaler = StandardScaler()\n    >>> print(scaler.fit(data))\n    S"}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "test_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "led)\n\n\ndef test_scaler_2d_arrays():\n    # Test scaling of 2d array along first axis\n    rng = np.random.RandomState(0)\n    n_features = 5\n    n_samples = 4\n    X = rng.randn(n_samples, n_features)\n    X[:, 0] = 0.0  # first feature is always of zero\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit(X).transform(X, copy=True)\n    assert not np.any(np.isnan(X_scaled))\n    assert scaler.n_samples_seen_ == n_samples\n\n    assert_array_almost_equal(X_scaled.mean(axis=0), n_features * [0.0])\n    assert_array_almost_equal(X_scaled.std(axis=0), [0.0, 1.0, 1.0, 1.0, 1.0])\n    # Check that X has been copied\n    assert X_scaled is not X\n\n    # check inverse transform\n    X_scaled_back = scaler.inverse_transform(X_scaled)\n    assert X_scaled_back is not X\n    assert X_scaled_back is not X_scaled\n    assert_array_almost_equal(X_scaled_back, X)\n\n    X_scaled = scale(X, axis=1, with_std=False)\n    assert not np.any(np.isnan(X_scaled))\n    assert_array_almost_equal(X_scaled.mean(axis=1), n_samples * [0.0])\n    X_scaled = scale(X, axis=1, with_std=True)\n    assert not np.any(np.isnan(X_scaled))\n    assert_array_almost_equal(X_scaled.mean(axis=1), n_samples * [0.0])\n    assert_array_almost_equal(X_scaled.std(axis=1), n_samples * [1.0])\n    # Check that the data hasn't been modified\n    assert X_scaled is not X\n\n    X_scaled = scaler.fit(X).transform(X, copy=False)\n    assert not np.any(np.isnan(X_scaled))\n    assert_array_almost_equal(X_scaled.mean(axis=0), n_features * [0.0])\n    assert_array_almost_equal(X_scaled.std(axis=0), [0.0, 1.0, 1.0, 1.0, 1.0])\n    # Check that X has not been copied\n    assert X_scaled is X\n\n    X = rng.randn(4, 5)\n    X[:, 0] = 1.0  # first feature is a constant, non zero feature\n    scaler = StandardScaler()\n    X_scaled = scaler.fit(X).transform(X, copy=True)\n    assert not np.any(np.isnan(X_scaled))\n    assert_array_almost_equal(X_scaled.mean(axis=0), n_features * [0.0])\n    assert_array_almost_equal(X_scaled.std(axis=0), [0.0, 1.0, 1.0, 1.0, 1.0]"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "---\n    X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The transformed data.\n\n    See Also\n    --------\n    StandardScaler : Performs scaling to unit variance using the Transformer\n        API (e.g. as part of a preprocessing\n        :class:`~sklearn.pipeline.Pipeline`).\n\n    Notes\n    -----\n    This implementation will refuse to center scipy.sparse matrices\n    since it would make them non-sparse and would potentially crash the\n    program with memory exhaustion problems.\n\n    Instead the caller is expected to either set explicitly\n    `with_mean=False` (in that case, only variance scaling will be\n    performed on the features of the CSC matrix) or to call `X.toarray()`\n    if he/she expects the materialized dense array to fit in memory.\n\n    To avoid memory copy the caller should pass a CSC matrix.\n\n    NaNs are treated as missing values: disregarded to compute the statistics,\n    and maintained during the data transformation.\n\n    We use a biased estimator for the standard deviation, equivalent to\n    `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n    affect model performance.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see: :ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.\n\n    .. warning:: Risk of data leak\n\n        Do not use :func:`~sklearn.preprocessing.scale` unless you know\n        what you are doing. A common mistake is to apply it to the entire data\n        *before* splitting into training and test sets. This will bias the\n        model evaluation because information would have leaked from the test\n        set to the training set.\n        In general, we recommend using\n        :class:`~sklearn.preprocessing.StandardScaler` within a\n        :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n        leaking: `pipe = make_pipeline(StandardScaler(), LogisticRegression())`.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import s"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "test_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ctor.startswith(\"sparse\")\n    X = _convert_container(X, array_constructor)\n    Xw = _convert_container(Xw, array_constructor)\n\n    # weighted StandardScaler\n    yw = np.ones(Xw.shape[0])\n    scaler_w = StandardScaler(with_mean=with_mean)\n    scaler_w.fit(Xw, yw, sample_weight=sample_weight)\n\n    # unweighted, but with repeated samples\n    y = np.ones(X.shape[0])\n    scaler = StandardScaler(with_mean=with_mean)\n    scaler.fit(X, y)\n\n    X_test = [[1.5, 2.5, 3.5], [3.5, 4.5, 5.5]]\n\n    assert_almost_equal(scaler.mean_, scaler_w.mean_)\n    assert_almost_equal(scaler.var_, scaler_w.var_)\n    assert_almost_equal(scaler.transform(X_test), scaler_w.transform(X_test))\n\n\ndef test_standard_scaler_1d():\n    # Test scaling of dataset along single axis\n    for X in [X_1row, X_1col, X_list_1row, X_list_1row]:\n        scaler = StandardScaler()\n        X_scaled = scaler.fit(X).transform(X, copy=True)\n\n        if isinstance(X, list):\n            X = np.array(X)  # cast only after scaling done\n\n        if _check_dim_1axis(X) == 1:\n            assert_almost_equal(scaler.mean_, X.ravel())\n            assert_almost_equal(scaler.scale_, np.ones(n_features))\n            assert_array_almost_equal(X_scaled.mean(axis=0), np.zeros_like(n_features))\n            assert_array_almost_equal(X_scaled.std(axis=0), np.zeros_like(n_features))\n        else:\n            assert_almost_equal(scaler.mean_, X.mean())\n            assert_almost_equal(scaler.scale_, X.std())\n            assert_array_almost_equal(X_scaled.mean(axis=0), np.zeros_like(n_features))\n            assert_array_almost_equal(X_scaled.mean(axis=0), 0.0)\n            assert_array_almost_equal(X_scaled.std(axis=0), 1.0)\n        assert scaler.n_samples_seen_ == X.shape[0]\n\n        # check inverse transform\n        X_scaled_back = scaler.inverse_transform(X_scaled)\n        assert_array_almost_equal(X_scaled_back, X)\n\n    # Constant feature\n    X = np.ones((5, 1))\n    scaler = StandardScaler()\n    X_scaled = scaler.fit(X).transform(X, copy=Tru"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "test_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e)\n    assert_almost_equal(scaler.mean_, 1.0)\n    assert_almost_equal(scaler.scale_, 1.0)\n    assert_array_almost_equal(X_scaled.mean(axis=0), 0.0)\n    assert_array_almost_equal(X_scaled.std(axis=0), 0.0)\n    assert scaler.n_samples_seen_ == X.shape[0]\n\n\n@pytest.mark.parametrize(\"sparse_container\", [None] + CSC_CONTAINERS + CSR_CONTAINERS)\n@pytest.mark.parametrize(\"add_sample_weight\", [False, True])\ndef test_standard_scaler_dtype(add_sample_weight, sparse_container):\n    # Ensure scaling does not affect dtype\n    rng = np.random.RandomState(0)\n    n_samples = 10\n    n_features = 3\n    if add_sample_weight:\n        sample_weight = np.ones(n_samples)\n    else:\n        sample_weight = None\n    with_mean = True\n    if sparse_container is not None:\n        # scipy sparse containers do not support float16, see\n        # https://github.com/scipy/scipy/issues/7408 for more details.\n        supported_dtype = [np.float64, np.float32]\n    else:\n        supported_dtype = [np.float64, np.float32, np.float16]\n    for dtype in supported_dtype:\n        X = rng.randn(n_samples, n_features).astype(dtype)\n        if sparse_container is not None:\n            X = sparse_container(X)\n            with_mean = False\n\n        scaler = StandardScaler(with_mean=with_mean)\n        X_scaled = scaler.fit(X, sample_weight=sample_weight).transform(X)\n        assert X.dtype == X_scaled.dtype\n        assert scaler.mean_.dtype == np.float64\n        assert scaler.scale_.dtype == np.float64\n\n\n@pytest.mark.parametrize(\n    \"scaler\",\n    [\n        StandardScaler(with_mean=False),\n        RobustScaler(with_centering=False),\n    ],\n)\n@pytest.mark.parametrize(\"sparse_container\", [None] + CSC_CONTAINERS + CSR_CONTAINERS)\n@pytest.mark.parametrize(\"add_sample_weight\", [False, True])\n@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])\n@pytest.mark.parametrize(\"constant\", [0, 1.0, 100.0])\ndef test_standard_scaler_constant_features(\n    scaler, add_sample_weight, sparse_container, dtype, constant\n):\n    if"}], "retrieved_count": 10, "cost_time": 1.0114243030548096}
{"question": "What does sklearn.ensemble.RandomForestClassifier's predict_proba method return?", "answer": null, "relative_code_list": null, "ground_truth": "RandomForestClassifier's predict_proba method returns class probability estimates for input samples:\n\n1. **Return Format**:\n   - **Shape**: ndarray of shape (n_samples, n_classes)\n   - **Type**: float64 array containing probability values\n   - **Range**: Each probability is between 0.0 and 1.0\n   - **Sum**: Each row sums to 1.0 (probability distribution)\n   - **Order**: Classes are ordered as they appear in classes_ attribute\n\n2. **Calculation Method**:\n   - **Ensemble Averaging**: Averages probability predictions from all trees in the forest\n   - **Tree Probabilities**: Each tree computes class probabilities as fraction of samples in leaf\n   - **Mean Computation**: Final probabilities = mean of all tree probabilities\n   - **Formula**: proba = sum(tree_proba) / n_estimators\n\n3. **Probability Computation**:\n   - **Leaf-based**: Each tree predicts based on class distribution in leaf nodes\n   - **Fraction Calculation**: Probability = (samples of class in leaf) / (total samples in leaf)\n   - **Smoothing**: Natural smoothing occurs through ensemble averaging\n   - **Consistency**: Probabilities are consistent across multiple calls\n\n4. **Multi-output Support**:\n   - **Single Output**: Returns single array of shape (n_samples, n_classes)\n   - **Multiple Outputs**: Returns list of arrays, one per output\n   - **Shape**: Each array has shape (n_samples, n_classes_for_that_output)\n   - **Handling**: Automatically detects and handles multi-output scenarios\n\n5. **Parallel Processing**:\n   - **Joblib Integration**: Uses joblib for parallel tree evaluation\n   - **n_jobs Parameter**: Controls number of parallel jobs\n   - **Memory Efficiency**: Avoids storing all tree predictions simultaneously\n   - **Lock Mechanism**: Uses threading.Lock for thread-safe accumulation\n\n6. **Numerical Properties**:\n   - **Precision**: Uses float64 for numerical stability\n   - **Normalization**: Probabilities are automatically normalized to sum to 1\n   - **Edge Cases**: Handles edge cases like empty trees or single-class leaves\n   - **Consistency**: Results are deterministic for fixed random_state\n\n7. **Performance Characteristics**:\n   - **Computational Cost**: Scales linearly with n_estimators\n   - **Memory Usage**: Efficient memory usage through incremental accumulation\n   - **Parallelization**: Benefits from multi-core processing\n   - **Sparse Support**: Works with sparse input matrices\n\n8. **Relationship to Other Methods**:\n   - **predict()**: Returns class predictions (argmax of predict_proba)\n   - **predict_log_proba()**: Returns log of predict_proba results\n   - **score()**: Uses predict() for accuracy calculation\n   - **decision_function()**: Not available for RandomForestClassifier\n\n9. **Use Cases**:\n   - **Probability Thresholding**: For custom decision thresholds\n   - **Calibration**: For probability calibration methods\n   - **Ensemble Methods**: For stacking or voting classifiers\n   - **Uncertainty Quantification**: For understanding prediction confidence\n   - **Cost-sensitive Learning**: For asymmetric misclassification costs", "score": null, "retrieved_content": [{"start_line": 33000, "end_line": 35000, "belongs_to": {"file_name": "_forest.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "bability of a single tree is the fraction of samples of\n        the same class in a leaf.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, its dtype will be converted to\n            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n            converted into a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        # Check data\n        X = self._validate_X_predict(X)\n\n        # Assign chunk of trees to jobs\n        n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)\n\n        # avoid storing the output of every estimator by summing them here\n        all_proba = [\n            np.zeros((X.shape[0], j), dtype=np.float64)\n            for j in np.atleast_1d(self.n_classes_)\n        ]\n        lock = threading.Lock()\n        Parallel(n_jobs=n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n            delayed(_accumulate_prediction)(e.predict_proba, X, all_proba, lock)\n            for e in self.estimators_\n        )\n\n        for proba in all_proba:\n            proba /= len(self.estimators_)\n\n        if len(all_proba) == 1:\n            return all_proba[0]\n        else:\n            return all_proba\n\n    def predict_log_proba(self, X):\n        \"\"\"\n        Predict class log-probabilities for X.\n\n        The predicted class log-probabilities of an input sample is computed as\n        the log of the mean predicted class probabilities of the trees in the\n        forest.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, its dtype will be converted to\n            ``dtype"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "_mocking.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      proba : ndarray of shape (n_samples, n_classes)\n            The probabilities for each sample and class.\n        \"\"\"\n        if self.methods_to_check == \"all\" or \"predict_proba\" in self.methods_to_check:\n            X, y = self._check_X_y(X)\n        rng = check_random_state(self.random_state)\n        proba = rng.randn(_num_samples(X), len(self.classes_))\n        proba = np.abs(proba, out=proba)\n        proba /= np.sum(proba, axis=1)[:, np.newaxis]\n        return proba\n\n    def decision_function(self, X):\n        \"\"\"Confidence score.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input data.\n\n        Returns\n        -------\n        decision : ndarray of shape (n_samples,) if n_classes == 2\\\n                else (n_samples, n_classes)\n            Confidence score.\n        \"\"\"\n        if (\n            self.methods_to_check == \"all\"\n            or \"decision_function\" in self.methods_to_check\n        ):\n            X, y = self._check_X_y(X)\n        rng = check_random_state(self.random_state)\n        if len(self.classes_) == 2:\n            # for binary classifier, the confidence score is related to\n            # classes_[1] and therefore should be null.\n            return rng.randn(_num_samples(X))\n        else:\n            return rng.randn(_num_samples(X), len(self.classes_))\n\n    def score(self, X=None, Y=None):\n        \"\"\"Fake score.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Y : array-like of shape (n_samples, n_output) or (n_samples,)\n            Target relative to X for classification or regression;\n            None for unsupervised learning.\n\n        Returns\n        -------\n        score : float\n            Either 0 or 1 depending of `foo_param` (i.e. `foo_param > 1 =>\n            score=1` otherwise `score=0`).\n    "}, {"start_line": 26000, "end_line": 28000, "belongs_to": {"file_name": "test_forest.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     [1, 2],\n        [2, 1],\n        [-2, 1],\n        [-1, 1],\n        [-1, 2],\n        [2, -1],\n        [1, -1],\n        [1, -2],\n    ]\n    y_train = [\n        [\"red\", \"blue\"],\n        [\"red\", \"blue\"],\n        [\"red\", \"blue\"],\n        [\"green\", \"green\"],\n        [\"green\", \"green\"],\n        [\"green\", \"green\"],\n        [\"red\", \"purple\"],\n        [\"red\", \"purple\"],\n        [\"red\", \"purple\"],\n        [\"green\", \"yellow\"],\n        [\"green\", \"yellow\"],\n        [\"green\", \"yellow\"],\n    ]\n    X_test = [[-1, -1], [1, 1], [-1, 1], [1, -1]]\n    y_test = [\n        [\"red\", \"blue\"],\n        [\"green\", \"green\"],\n        [\"red\", \"purple\"],\n        [\"green\", \"yellow\"],\n    ]\n\n    est = FOREST_ESTIMATORS[name](random_state=0, bootstrap=False)\n    y_pred = est.fit(X_train, y_train).predict(X_test)\n    assert_array_equal(y_pred, y_test)\n\n    with np.errstate(divide=\"ignore\"):\n        proba = est.predict_proba(X_test)\n        assert len(proba) == 2\n        assert proba[0].shape == (4, 2)\n        assert proba[1].shape == (4, 4)\n\n        log_proba = est.predict_log_proba(X_test)\n        assert len(log_proba) == 2\n        assert log_proba[0].shape == (4, 2)\n        assert log_proba[1].shape == (4, 4)\n\n\n@pytest.mark.parametrize(\"name\", FOREST_CLASSIFIERS)\ndef test_classes_shape(name):\n    # Test that n_classes_ and classes_ have proper shape.\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    # Classification, single output\n    clf = ForestClassifier(random_state=0).fit(X, y)\n\n    assert clf.n_classes_ == 2\n    assert_array_equal(clf.classes_, [-1, 1])\n\n    # Classification, multi-output\n    _y = np.vstack((y, np.array(y) * 2)).T\n    clf = ForestClassifier(random_state=0).fit(X, _y)\n\n    assert_array_equal(clf.n_classes_, [2, 2])\n    assert_array_equal(clf.classes_, [[-1, 1], [-2, 2]])\n\n\ndef test_random_trees_dense_type():\n    # Test that the `sparse_output` parameter of RandomTreesEmbedding\n    # works by returning a dense array.\n\n    # Create the RTE with sparse=False\n    hasher = Random"}, {"start_line": 35000, "end_line": 37000, "belongs_to": {"file_name": "_forest.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "=np.float32``. If a sparse matrix is provided, it will be\n            converted into a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        proba = self.predict_proba(X)\n\n        if self.n_outputs_ == 1:\n            return np.log(proba)\n\n        else:\n            for k in range(self.n_outputs_):\n                proba[k] = np.log(proba[k])\n\n            return proba\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.classifier_tags.multi_label = True\n        tags.input_tags.sparse = True\n        return tags\n\n\nclass ForestRegressor(RegressorMixin, BaseForest, metaclass=ABCMeta):\n    \"\"\"\n    Base class for forest of trees-based regressors.\n\n    Warning: This class should not be used directly. Use derived classes\n    instead.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(\n        self,\n        estimator,\n        n_estimators=100,\n        *,\n        estimator_params=tuple(),\n        bootstrap=False,\n        oob_score=False,\n        n_jobs=None,\n        random_state=None,\n        verbose=0,\n        warm_start=False,\n        max_samples=None,\n    ):\n        super().__init__(\n            estimator,\n            n_estimators=n_estimators,\n            estimator_params=estimator_params,\n            bootstrap=bootstrap,\n            oob_score=oob_score,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start,\n            max_samples=max_samples,\n        )\n\n    def predict(self, X):\n        \"\"\"\n        Predict regression target for X.\n\n        The predicted regression target of an input sample is computed as the\n        mean predicted regression targets of the trees in the forest.\n\n        Parameters\n        ----------\n        "}, {"start_line": 27000, "end_line": 29000, "belongs_to": {"file_name": "test_forest.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "a[1].shape == (4, 4)\n\n        log_proba = est.predict_log_proba(X_test)\n        assert len(log_proba) == 2\n        assert log_proba[0].shape == (4, 2)\n        assert log_proba[1].shape == (4, 4)\n\n\n@pytest.mark.parametrize(\"name\", FOREST_CLASSIFIERS)\ndef test_classes_shape(name):\n    # Test that n_classes_ and classes_ have proper shape.\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    # Classification, single output\n    clf = ForestClassifier(random_state=0).fit(X, y)\n\n    assert clf.n_classes_ == 2\n    assert_array_equal(clf.classes_, [-1, 1])\n\n    # Classification, multi-output\n    _y = np.vstack((y, np.array(y) * 2)).T\n    clf = ForestClassifier(random_state=0).fit(X, _y)\n\n    assert_array_equal(clf.n_classes_, [2, 2])\n    assert_array_equal(clf.classes_, [[-1, 1], [-2, 2]])\n\n\ndef test_random_trees_dense_type():\n    # Test that the `sparse_output` parameter of RandomTreesEmbedding\n    # works by returning a dense array.\n\n    # Create the RTE with sparse=False\n    hasher = RandomTreesEmbedding(n_estimators=10, sparse_output=False)\n    X, y = datasets.make_circles(factor=0.5)\n    X_transformed = hasher.fit_transform(X)\n\n    # Assert that type is ndarray, not scipy.sparse.csr_matrix\n    assert isinstance(X_transformed, np.ndarray)\n\n\ndef test_random_trees_dense_equal():\n    # Test that the `sparse_output` parameter of RandomTreesEmbedding\n    # works by returning the same array for both argument values.\n\n    # Create the RTEs\n    hasher_dense = RandomTreesEmbedding(\n        n_estimators=10, sparse_output=False, random_state=0\n    )\n    hasher_sparse = RandomTreesEmbedding(\n        n_estimators=10, sparse_output=True, random_state=0\n    )\n    X, y = datasets.make_circles(factor=0.5)\n    X_transformed_dense = hasher_dense.fit_transform(X)\n    X_transformed_sparse = hasher_sparse.fit_transform(X)\n\n    # Assert that dense and sparse hashers have same array.\n    assert_array_equal(X_transformed_sparse.toarray(), X_transformed_dense)\n\n\ndef test_random_hasher():\n    # t"}, {"start_line": 31000, "end_line": 33000, "belongs_to": {"file_name": "_forest.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "d_subsample\" or not self.bootstrap:\n                if self.class_weight == \"balanced_subsample\":\n                    class_weight = \"balanced\"\n                else:\n                    class_weight = self.class_weight\n                expanded_class_weight = compute_sample_weight(class_weight, y_original)\n\n        return y, expanded_class_weight\n\n    def predict(self, X):\n        \"\"\"\n        Predict class for X.\n\n        The predicted class of an input sample is a vote by the trees in\n        the forest, weighted by their probability estimates. That is,\n        the predicted class is the one with highest mean probability\n        estimate across the trees.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, its dtype will be converted to\n            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n            converted into a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n            The predicted classes.\n        \"\"\"\n        proba = self.predict_proba(X)\n\n        if self.n_outputs_ == 1:\n            return self.classes_.take(np.argmax(proba, axis=1), axis=0)\n\n        else:\n            n_samples = proba[0].shape[0]\n            # all dtypes should be the same, so just take the first\n            class_type = self.classes_[0].dtype\n            predictions = np.empty((n_samples, self.n_outputs_), dtype=class_type)\n\n            for k in range(self.n_outputs_):\n                predictions[:, k] = self.classes_[k].take(\n                    np.argmax(proba[k], axis=1), axis=0\n                )\n\n            return predictions\n\n    def predict_proba(self, X):\n        \"\"\"\n        Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample are computed as\n        the mean predicted class probabilities of the trees in the forest.\n        The class pro"}, {"start_line": 32000, "end_line": 34000, "belongs_to": {"file_name": "_forest.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    -------\n        y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n            The predicted classes.\n        \"\"\"\n        proba = self.predict_proba(X)\n\n        if self.n_outputs_ == 1:\n            return self.classes_.take(np.argmax(proba, axis=1), axis=0)\n\n        else:\n            n_samples = proba[0].shape[0]\n            # all dtypes should be the same, so just take the first\n            class_type = self.classes_[0].dtype\n            predictions = np.empty((n_samples, self.n_outputs_), dtype=class_type)\n\n            for k in range(self.n_outputs_):\n                predictions[:, k] = self.classes_[k].take(\n                    np.argmax(proba[k], axis=1), axis=0\n                )\n\n            return predictions\n\n    def predict_proba(self, X):\n        \"\"\"\n        Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample are computed as\n        the mean predicted class probabilities of the trees in the forest.\n        The class probability of a single tree is the fraction of samples of\n        the same class in a leaf.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, its dtype will be converted to\n            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n            converted into a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        # Check data\n        X = self._validate_X_predict(X)\n\n        # Assign chunk of trees to jobs\n        n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)\n\n        # avoid storing the output of every estimator by summing them here\n        all_proba = [\n            np."}, {"start_line": 39000, "end_line": 41000, "belongs_to": {"file_name": "_classes.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tree", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "eaf.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        check_input : bool, default=True\n            Allow to bypass several input checking.\n            Don't use this parameter unless you know what you're doing.\n\n        Returns\n        -------\n        proba : ndarray of shape (n_samples, n_classes) or list of n_outputs \\\n            such arrays if n_outputs > 1\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_X_predict(X, check_input)\n        proba = self.tree_.predict(X)\n\n        if self.n_outputs_ == 1:\n            return proba[:, : self.n_classes_]\n        else:\n            all_proba = []\n            for k in range(self.n_outputs_):\n                proba_k = proba[:, k, : self.n_classes_[k]]\n                all_proba.append(proba_k)\n            return all_proba\n\n    def predict_log_proba(self, X):\n        \"\"\"Predict class log-probabilities of the input samples X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        proba : ndarray of shape (n_samples, n_classes) or list of n_outputs \\\n            such arrays if n_outputs > 1\n            The class log-probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        proba = self.predict_proba(X)\n\n        if self.n_outputs_ == 1:\n            return np.log("}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "test_multioutput.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "   # train the forest with each column and assert that predictions are equal\n    for i in range(3):\n        multi_class_svc_ = clone(multi_class_svc)  # create a clone\n        multi_class_svc_.fit(X, y[:, i])\n        assert list(multi_class_svc_.predict(X)) == list(predictions[:, i])\n\n\ndef test_multiclass_multioutput_estimator_predict_proba():\n    seed = 542\n\n    # make test deterministic\n    rng = np.random.RandomState(seed)\n\n    # random features\n    X = rng.normal(size=(5, 5))\n\n    # random labels\n    y1 = np.array([\"b\", \"a\", \"a\", \"b\", \"a\"]).reshape(5, 1)  # 2 classes\n    y2 = np.array([\"d\", \"e\", \"f\", \"e\", \"d\"]).reshape(5, 1)  # 3 classes\n\n    Y = np.concatenate([y1, y2], axis=1)\n\n    clf = MultiOutputClassifier(LogisticRegression(random_state=seed))\n\n    clf.fit(X, Y)\n\n    y_result = clf.predict_proba(X)\n    y_actual = [\n        np.array(\n            [\n                [0.31525135, 0.68474865],\n                [0.81004803, 0.18995197],\n                [0.65664086, 0.34335914],\n                [0.38584929, 0.61415071],\n                [0.83234285, 0.16765715],\n            ]\n        ),\n        np.array(\n            [\n                [0.65759215, 0.20976588, 0.13264197],\n                [0.14996984, 0.82591444, 0.02411571],\n                [0.13111876, 0.13294966, 0.73593158],\n                [0.24663053, 0.65860244, 0.09476703],\n                [0.81458885, 0.1728158, 0.01259535],\n            ]\n        ),\n    ]\n\n    for i in range(len(y_actual)):\n        assert_almost_equal(y_result[i], y_actual[i])\n\n\ndef test_multi_output_classification_sample_weights():\n    # weighted classifier\n    Xw = [[1, 2, 3], [4, 5, 6]]\n    yw = [[3, 2], [2, 3]]\n    w = np.asarray([2.0, 1.0])\n    forest = RandomForestClassifier(n_estimators=10, random_state=1)\n    clf_w = MultiOutputClassifier(forest)\n    clf_w.fit(Xw, yw, w)\n\n    # unweighted, but with repeated samples\n    X = [[1, 2, 3], [1, 2, 3], [4, 5, 6]]\n    y = [[3, 2], [3, 2], [2, 3]]\n    forest = RandomForestClassifier(n_estimato"}, {"start_line": 34000, "end_line": 36000, "belongs_to": {"file_name": "_forest.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "zeros((X.shape[0], j), dtype=np.float64)\n            for j in np.atleast_1d(self.n_classes_)\n        ]\n        lock = threading.Lock()\n        Parallel(n_jobs=n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n            delayed(_accumulate_prediction)(e.predict_proba, X, all_proba, lock)\n            for e in self.estimators_\n        )\n\n        for proba in all_proba:\n            proba /= len(self.estimators_)\n\n        if len(all_proba) == 1:\n            return all_proba[0]\n        else:\n            return all_proba\n\n    def predict_log_proba(self, X):\n        \"\"\"\n        Predict class log-probabilities for X.\n\n        The predicted class log-probabilities of an input sample is computed as\n        the log of the mean predicted class probabilities of the trees in the\n        forest.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, its dtype will be converted to\n            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n            converted into a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        proba = self.predict_proba(X)\n\n        if self.n_outputs_ == 1:\n            return np.log(proba)\n\n        else:\n            for k in range(self.n_outputs_):\n                proba[k] = np.log(proba[k])\n\n            return proba\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.classifier_tags.multi_label = True\n        tags.input_tags.sparse = True\n        return tags\n\n\nclass ForestRegressor(RegressorMixin, BaseForest, metaclass=ABCMeta):\n    \"\"\"\n    Base class for forest of trees-based regressors.\n\n    Warning: This class should not be used directly. Use derived cl"}], "retrieved_count": 10, "cost_time": 1.0119426250457764}
{"question": "What is Scikit-learn's approach to handling sparse matrices?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn provides comprehensive support for sparse matrices through the scipy.sparse framework:\n\n1. **Sparse Matrix Support**:\n   - **Format Support**: Supports CSR, CSC, COO, DOK, BSR, LIL, and DIA formats\n   - **Automatic Conversion**: Converts between formats as needed for optimal performance\n   - **Format Validation**: Validates sparse matrix formats through accept_sparse parameter\n   - **Large Sparse Support**: Handles large sparse matrices with configurable acceptance\n\n2. **Core Utilities**:\n   - **safe_sparse_dot()**: Handles dot products between sparse and dense matrices\n   - **_ensure_sparse_format()**: Converts sparse matrices to specified formats\n   - **sparsefuncs module**: Specialized functions for sparse matrix operations\n   - **_raise_typeerror()**: Validates CSR/CSC format requirements\n\n3. **Memory Efficiency**:\n   - **Sparse Storage**: Only stores non-zero elements, significantly reducing memory usage\n   - **Implicit vs Explicit Zeros**: Distinguishes between stored zeros and implicit zeros\n   - **Sparsity Ratio**: Recommends sparse formats when sparsity > 90%\n   - **Memory Management**: Handles large datasets that would exhaust memory in dense format\n\n4. **Performance Optimization**:\n   - **Format-Specific Operations**: Optimizes operations for specific sparse formats\n   - **Parallel Processing**: Supports parallel operations on sparse matrices\n   - **BLAS Integration**: Leverages optimized BLAS operations where possible\n   - **Cache Efficiency**: Minimizes cache misses through optimized data layout\n\n5. **Algorithm Integration**:\n   - **Linear Models**: Native sparse support for efficient training and prediction\n   - **Feature Extraction**: Sparse-friendly text and image feature extraction\n   - **Clustering**: Efficient sparse matrix clustering algorithms\n   - **Dimensionality Reduction**: Sparse-aware PCA and other reduction methods\n\n6. **Data Validation**:\n   - **check_array()**: Validates sparse matrix inputs with format constraints\n   - **accept_sparse parameter**: Controls which sparse formats are accepted\n   - **Format Conversion**: Automatic conversion to preferred formats\n   - **Density Checks**: Warns about inefficient sparse usage\n\n7. **Specialized Operations**:\n   - **Sparse CoefMixin**: Provides sparsify() and densify() methods for linear models\n   - **Sparse Matrix Semantics**: Two interpretation modes (matrix and graph semantics)\n   - **Sparse Kernels**: Efficient kernel computations for sparse data\n   - **Sparse Distance Metrics**: Optimized distance calculations\n\n8. **Integration with Other Libraries**:\n   - **NumPy Compatibility**: Seamless integration with NumPy arrays\n   - **Pandas Support**: Works with sparse pandas DataFrames\n   - **Scipy Integration**: Leverages scipy.sparse functionality\n   - **Memory Mapping**: Supports memory-mapped sparse matrices for very large datasets", "score": null, "retrieved_content": [{"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_sgd.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "f \"tol\" not in kwargs:\n        kwargs[\"tol\"] = None\n    if \"max_iter\" not in kwargs:\n        kwargs[\"max_iter\"] = 5\n\n\nclass _SparseSGDClassifier(linear_model.SGDClassifier):\n    def fit(self, X, y, *args, **kw):\n        X = sp.csr_matrix(X)\n        return super().fit(X, y, *args, **kw)\n\n    def partial_fit(self, X, y, *args, **kw):\n        X = sp.csr_matrix(X)\n        return super().partial_fit(X, y, *args, **kw)\n\n    def decision_function(self, X):\n        X = sp.csr_matrix(X)\n        return super().decision_function(X)\n\n    def predict_proba(self, X):\n        X = sp.csr_matrix(X)\n        return super().predict_proba(X)\n\n\nclass _SparseSGDRegressor(linear_model.SGDRegressor):\n    def fit(self, X, y, *args, **kw):\n        X = sp.csr_matrix(X)\n        return linear_model.SGDRegressor.fit(self, X, y, *args, **kw)\n\n    def partial_fit(self, X, y, *args, **kw):\n        X = sp.csr_matrix(X)\n        return linear_model.SGDRegressor.partial_fit(self, X, y, *args, **kw)\n\n    def decision_function(self, X, *args, **kw):\n        # XXX untested as of v0.22\n        X = sp.csr_matrix(X)\n        return linear_model.SGDRegressor.decision_function(self, X, *args, **kw)\n\n\nclass _SparseSGDOneClassSVM(linear_model.SGDOneClassSVM):\n    def fit(self, X, *args, **kw):\n        X = sp.csr_matrix(X)\n        return linear_model.SGDOneClassSVM.fit(self, X, *args, **kw)\n\n    def partial_fit(self, X, *args, **kw):\n        X = sp.csr_matrix(X)\n        return linear_model.SGDOneClassSVM.partial_fit(self, X, *args, **kw)\n\n    def decision_function(self, X, *args, **kw):\n        X = sp.csr_matrix(X)\n        return linear_model.SGDOneClassSVM.decision_function(self, X, *args, **kw)\n\n\ndef SGDClassifier(**kwargs):\n    _update_kwargs(kwargs)\n    return linear_model.SGDClassifier(**kwargs)\n\n\ndef SGDRegressor(**kwargs):\n    _update_kwargs(kwargs)\n    return linear_model.SGDRegressor(**kwargs)\n\n\ndef SGDOneClassSVM(**kwargs):\n    _update_kwargs(kwargs)\n    return linear_model.SGDOneClassSVM(**kwargs)\n\n\ndef "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_sparse.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/svm/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "import numpy as np\nimport pytest\nfrom scipy import sparse\n\nfrom sklearn import base, datasets, linear_model, svm\nfrom sklearn.datasets import load_digits, make_blobs, make_classification\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.svm.tests import test_svm\nfrom sklearn.utils._testing import (\n    assert_allclose,\n    assert_array_almost_equal,\n    assert_array_equal,\n    ignore_warnings,\n    skip_if_32bit,\n)\nfrom sklearn.utils.extmath import safe_sparse_dot\nfrom sklearn.utils.fixes import (\n    CSR_CONTAINERS,\n    DOK_CONTAINERS,\n    LIL_CONTAINERS,\n)\n\n# test sample 1\nX = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]])\nY = [1, 1, 1, 2, 2, 2]\nT = np.array([[-1, -1], [2, 2], [3, 2]])\ntrue_result = [1, 2, 2]\n\n# test sample 2\nX2 = np.array(\n    [\n        [0, 0, 0],\n        [1, 1, 1],\n        [2, 0, 0],\n        [0, 0, 2],\n        [3, 3, 3],\n    ]\n)\nY2 = [1, 2, 2, 2, 3]\nT2 = np.array([[-1, -1, -1], [1, 1, 1], [2, 2, 2]])\ntrue_result2 = [1, 2, 3]\n\niris = datasets.load_iris()\nrng = np.random.RandomState(0)\nperm = rng.permutation(iris.target.size)\niris.data = iris.data[perm]\niris.target = iris.target[perm]\n\nX_blobs, y_blobs = make_blobs(n_samples=100, centers=10, random_state=0)\n\n\ndef check_svm_model_equal(dense_svm, X_train, y_train, X_test):\n    # Use the original svm model for dense fit and clone an exactly same\n    # svm model for sparse fit\n    sparse_svm = base.clone(dense_svm)\n\n    dense_svm.fit(X_train.toarray(), y_train)\n    if sparse.issparse(X_test):\n        X_test_dense = X_test.toarray()\n    else:\n        X_test_dense = X_test\n    sparse_svm.fit(X_train, y_train)\n    assert sparse.issparse(sparse_svm.support_vectors_)\n    assert sparse.issparse(sparse_svm.dual_coef_)\n    assert_allclose(dense_svm.support_vectors_, sparse_svm.support_vectors_.toarray())\n    assert_allclose(dense_svm.dual_coef_, sparse_svm.dual_coef_.toarray())\n    if dense_svm.kernel == \"linear\":\n        assert sparse.issparse(sparse_svm.coef_)\n        assert_a"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_sparse.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/svm/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tasets.load_iris()\nrng = np.random.RandomState(0)\nperm = rng.permutation(iris.target.size)\niris.data = iris.data[perm]\niris.target = iris.target[perm]\n\nX_blobs, y_blobs = make_blobs(n_samples=100, centers=10, random_state=0)\n\n\ndef check_svm_model_equal(dense_svm, X_train, y_train, X_test):\n    # Use the original svm model for dense fit and clone an exactly same\n    # svm model for sparse fit\n    sparse_svm = base.clone(dense_svm)\n\n    dense_svm.fit(X_train.toarray(), y_train)\n    if sparse.issparse(X_test):\n        X_test_dense = X_test.toarray()\n    else:\n        X_test_dense = X_test\n    sparse_svm.fit(X_train, y_train)\n    assert sparse.issparse(sparse_svm.support_vectors_)\n    assert sparse.issparse(sparse_svm.dual_coef_)\n    assert_allclose(dense_svm.support_vectors_, sparse_svm.support_vectors_.toarray())\n    assert_allclose(dense_svm.dual_coef_, sparse_svm.dual_coef_.toarray())\n    if dense_svm.kernel == \"linear\":\n        assert sparse.issparse(sparse_svm.coef_)\n        assert_array_almost_equal(dense_svm.coef_, sparse_svm.coef_.toarray())\n    assert_allclose(dense_svm.support_, sparse_svm.support_)\n    assert_allclose(dense_svm.predict(X_test_dense), sparse_svm.predict(X_test))\n\n    assert_array_almost_equal(\n        dense_svm.decision_function(X_test_dense), sparse_svm.decision_function(X_test)\n    )\n    assert_array_almost_equal(\n        dense_svm.decision_function(X_test_dense),\n        sparse_svm.decision_function(X_test_dense),\n    )\n    if isinstance(dense_svm, svm.OneClassSVM):\n        msg = \"cannot use sparse input in 'OneClassSVM' trained on dense data\"\n    else:\n        assert_array_almost_equal(\n            dense_svm.predict_proba(X_test_dense),\n            sparse_svm.predict_proba(X_test),\n            decimal=4,\n        )\n        msg = \"cannot use sparse input in 'SVC' trained on dense data\"\n    if sparse.issparse(X_test):\n        with pytest.raises(ValueError, match=msg):\n            dense_svm.predict(X_test)\n\n\n@skip_if_32bit\n@pytest.mark.parame"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "extmath.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "determinant\n        of an array.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils.extmath import fast_logdet\n    >>> a = np.array([[5, 1], [2, 8]])\n    >>> fast_logdet(a)\n    np.float64(3.6375861597263857)\n    \"\"\"\n    xp, _ = get_namespace(A)\n    sign, ld = xp.linalg.slogdet(A)\n    if not sign > 0:\n        return -xp.inf\n    return ld\n\n\ndef density(w):\n    \"\"\"Compute density of a sparse vector.\n\n    Parameters\n    ----------\n    w : {ndarray, sparse matrix}\n        The input data can be numpy ndarray or a sparse matrix.\n\n    Returns\n    -------\n    float\n        The density of w, between 0 and 1.\n\n    Examples\n    --------\n    >>> from scipy import sparse\n    >>> from sklearn.utils.extmath import density\n    >>> X = sparse.random(10, 10, density=0.25, random_state=0)\n    >>> density(X)\n    0.25\n    \"\"\"\n    if hasattr(w, \"toarray\"):\n        d = float(w.nnz) / (w.shape[0] * w.shape[1])\n    else:\n        d = 0 if w is None else float((w != 0).sum()) / w.size\n    return d\n\n\ndef safe_sparse_dot(a, b, *, dense_output=False):\n    \"\"\"Dot product that handle the sparse matrix case correctly.\n\n    Parameters\n    ----------\n    a : {ndarray, sparse matrix}\n    b : {ndarray, sparse matrix}\n    dense_output : bool, default=False\n        When False, ``a`` and ``b`` both being sparse will yield sparse output.\n        When True, output will always be a dense array.\n\n    Returns\n    -------\n    dot_product : {ndarray, sparse matrix}\n        Sparse if ``a`` and ``b`` are sparse and ``dense_output=False``.\n\n    Examples\n    --------\n    >>> from scipy.sparse import csr_matrix\n    >>> from sklearn.utils.extmath import safe_sparse_dot\n    >>> X = csr_matrix([[1, 2], [3, 4], [5, 6]])\n    >>> dot_product = safe_sparse_dot(X, X.T)\n    >>> dot_product.toarray()\n    array([[ 5, 11, 17],\n           [11, 25, 39],\n           [17, 39, 61]])\n    \"\"\"\n    xp, _ = get_namespace(a, b)\n    if a.ndim > 2 or b.ndim > 2:\n        if sparse.issparse(a):\n            # sparse "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_sgd.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "on(self, X, *args, **kw):\n        # XXX untested as of v0.22\n        X = sp.csr_matrix(X)\n        return linear_model.SGDRegressor.decision_function(self, X, *args, **kw)\n\n\nclass _SparseSGDOneClassSVM(linear_model.SGDOneClassSVM):\n    def fit(self, X, *args, **kw):\n        X = sp.csr_matrix(X)\n        return linear_model.SGDOneClassSVM.fit(self, X, *args, **kw)\n\n    def partial_fit(self, X, *args, **kw):\n        X = sp.csr_matrix(X)\n        return linear_model.SGDOneClassSVM.partial_fit(self, X, *args, **kw)\n\n    def decision_function(self, X, *args, **kw):\n        X = sp.csr_matrix(X)\n        return linear_model.SGDOneClassSVM.decision_function(self, X, *args, **kw)\n\n\ndef SGDClassifier(**kwargs):\n    _update_kwargs(kwargs)\n    return linear_model.SGDClassifier(**kwargs)\n\n\ndef SGDRegressor(**kwargs):\n    _update_kwargs(kwargs)\n    return linear_model.SGDRegressor(**kwargs)\n\n\ndef SGDOneClassSVM(**kwargs):\n    _update_kwargs(kwargs)\n    return linear_model.SGDOneClassSVM(**kwargs)\n\n\ndef SparseSGDClassifier(**kwargs):\n    _update_kwargs(kwargs)\n    return _SparseSGDClassifier(**kwargs)\n\n\ndef SparseSGDRegressor(**kwargs):\n    _update_kwargs(kwargs)\n    return _SparseSGDRegressor(**kwargs)\n\n\ndef SparseSGDOneClassSVM(**kwargs):\n    _update_kwargs(kwargs)\n    return _SparseSGDOneClassSVM(**kwargs)\n\n\n# Test Data\n\n# test sample 1\nX = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]])\nY = [1, 1, 1, 2, 2, 2]\nT = np.array([[-1, -1], [2, 2], [3, 2]])\ntrue_result = [1, 2, 2]\n\n# test sample 2; string class labels\nX2 = np.array(\n    [\n        [-1, 1],\n        [-0.75, 0.5],\n        [-1.5, 1.5],\n        [1, 1],\n        [0.75, 0.5],\n        [1.5, 1.5],\n        [-1, -1],\n        [0, -0.5],\n        [1, -1],\n    ]\n)\nY2 = [\"one\"] * 3 + [\"two\"] * 3 + [\"three\"] * 3\nT2 = np.array([[-1.5, 0.5], [1, 2], [0, -2]])\ntrue_result2 = [\"one\", \"two\", \"three\"]\n\n# test sample 3\nX3 = np.array(\n    [\n        [1, 1, 0, 0, 0, 0],\n        [1, 1, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0],\n        "}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "test_sparse.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/svm/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "lf = svm.LinearSVC(random_state=0).fit(iris_data_sp, iris.target)\n    clf = svm.LinearSVC(random_state=0).fit(iris.data, iris.target)\n\n    assert clf.fit_intercept == sp_clf.fit_intercept\n\n    assert_array_almost_equal(clf.coef_, sp_clf.coef_, decimal=1)\n    assert_array_almost_equal(clf.intercept_, sp_clf.intercept_, decimal=1)\n    assert_allclose(clf.predict(iris.data), sp_clf.predict(iris_data_sp))\n\n    # check decision_function\n    pred = np.argmax(sp_clf.decision_function(iris_data_sp), axis=1)\n    assert_allclose(pred, clf.predict(iris.data))\n\n    # sparsify the coefficients on both models and check that they still\n    # produce the same results\n    clf.sparsify()\n    assert_array_equal(pred, clf.predict(iris_data_sp))\n    sp_clf.sparsify()\n    assert_array_equal(pred, sp_clf.predict(iris_data_sp))\n\n\n@pytest.mark.parametrize(\"csr_container\", CSR_CONTAINERS)\ndef test_weight(csr_container):\n    # Test class weights\n    X_, y_ = make_classification(\n        n_samples=200, n_features=100, weights=[0.833, 0.167], random_state=0\n    )\n\n    X_ = csr_container(X_)\n    for clf in (\n        linear_model.LogisticRegression(),\n        svm.LinearSVC(random_state=0),\n        svm.SVC(),\n    ):\n        clf.set_params(class_weight={0: 5})\n        clf.fit(X_[:180], y_[:180])\n        y_pred = clf.predict(X_[180:])\n        assert np.sum(y_pred == y_[180:]) >= 11\n\n\n@pytest.mark.parametrize(\"lil_container\", LIL_CONTAINERS)\ndef test_sample_weights(lil_container):\n    # Test weights on individual samples\n    X_sp = lil_container(X)\n\n    clf = svm.SVC()\n    clf.fit(X_sp, Y)\n    assert_array_equal(clf.predict([X[2]]), [1.0])\n\n    sample_weight = [0.1] * 3 + [10] * 3\n    clf.fit(X_sp, Y, sample_weight=sample_weight)\n    assert_array_equal(clf.predict([X[2]]), [2.0])\n\n\ndef test_sparse_liblinear_intercept_handling():\n    # Test that sparse liblinear honours intercept_scaling param\n    test_svm.test_dense_liblinear_intercept_handling(svm.LinearSVC)\n\n\n@pytest.mark.parametrize(\n    \"X_train,"}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "se matrix, which for\n        L1-regularized models can be much more memory- and storage-efficient\n        than the usual numpy.ndarray representation.\n\n        The ``intercept_`` member is not converted.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.\n        \"\"\"\n        msg = \"Estimator, %(name)s, must be fitted before sparsifying.\"\n        check_is_fitted(self, msg=msg)\n        self.coef_ = sp.csr_matrix(self.coef_)\n        return self\n\n\nclass LinearRegression(MultiOutputMixin, RegressorMixin, LinearModel):\n    \"\"\"\n    Ordinary least squares Linear Regression.\n\n    LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n    to minimize the residual sum of squares between the observed targets in\n    the dataset, and the targets predicted by the linear approximation.\n\n    Parameters\n    ----------\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    tol : float, default=1e-6\n        The precision of the solution (`coef_`) is determined by `tol` which\n        specifies a different convergence criterion for the `lsqr` solver.\n        `tol` is set as `atol` and `btol` of `scipy.sparse.linalg.lsqr` when\n        fitting on sparse training data. This parameter has"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "(prob.shape[0], -1))\n            return prob\n\n\nclass SparseCoefMixin:\n    \"\"\"Mixin for converting coef_ to and from CSR format.\n\n    L1-regularizing estimators should inherit this.\n    \"\"\"\n\n    def densify(self):\n        \"\"\"\n        Convert coefficient matrix to dense array format.\n\n        Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n        default format of ``coef_`` and is required for fitting, so calling\n        this method is only required on models that have previously been\n        sparsified; otherwise, it is a no-op.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        msg = \"Estimator, %(name)s, must be fitted before densifying.\"\n        check_is_fitted(self, msg=msg)\n        if sp.issparse(self.coef_):\n            self.coef_ = self.coef_.toarray()\n        return self\n\n    def sparsify(self):\n        \"\"\"\n        Convert coefficient matrix to sparse format.\n\n        Converts the ``coef_`` member to a scipy.sparse matrix, which for\n        L1-regularized models can be much more memory- and storage-efficient\n        than the usual numpy.ndarray representation.\n\n        The ``intercept_`` member is not converted.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.\n        \"\"\"\n        msg = \"Estimator, %(name)s, must be fitted before sparsifying.\"\n        check_is_fitted(self, msg=msg)\n        self.coef_ = sp.csr_matrix(self.coef_)\n        return self\n\n\nclass Linea"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "test_sparse.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/svm/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "           0.0,\n            2.0,\n            1.0,\n            2.0,\n            0.0,\n            1.0,\n            0.0,\n            2.0,\n            3.0,\n            1.0,\n            3.0,\n            0.0,\n            1.0,\n            0.0,\n            0.0,\n            2.0,\n            0.0,\n            1.0,\n            2.0,\n            2.0,\n            2.0,\n            3.0,\n            2.0,\n            0.0,\n            3.0,\n            2.0,\n            1.0,\n            2.0,\n            3.0,\n            2.0,\n            2.0,\n            0.0,\n            1.0,\n            0.0,\n            1.0,\n            2.0,\n            3.0,\n            0.0,\n            0.0,\n            2.0,\n            2.0,\n            1.0,\n            3.0,\n            1.0,\n            1.0,\n            0.0,\n            1.0,\n            2.0,\n            1.0,\n            1.0,\n            3.0,\n        ]\n    )\n\n    clf = svm.SVC(kernel=\"linear\").fit(X.toarray(), y)\n    sp_clf = svm.SVC(kernel=\"linear\").fit(X.tocoo(), y)\n\n    assert_array_equal(clf.support_vectors_, sp_clf.support_vectors_.toarray())\n    assert_array_equal(clf.dual_coef_, sp_clf.dual_coef_.toarray())\n\n\n@pytest.mark.parametrize(\"lil_container\", LIL_CONTAINERS)\ndef test_sparse_svc_clone_with_callable_kernel(lil_container):\n    # Test that the \"dense_fit\" is called even though we use sparse input\n    # meaning that everything works fine.\n    a = svm.SVC(C=1, kernel=lambda x, y: x @ y.T, probability=True, random_state=0)\n    b = base.clone(a)\n\n    X_sp = lil_container(X)\n    b.fit(X_sp, Y)\n    pred = b.predict(X_sp)\n    b.predict_proba(X_sp)\n\n    dense_svm = svm.SVC(\n        C=1, kernel=lambda x, y: np.dot(x, y.T), probability=True, random_state=0\n    )\n    pred_dense = dense_svm.fit(X, Y).predict(X)\n    assert_array_equal(pred_dense, pred)\n    # b.decision_function(X_sp)  # XXX : should be supported\n\n\n@pytest.mark.parametrize(\"lil_container\", LIL_CONTAINERS)\ndef test_timeout(lil_container):\n    sp = svm.SVC(\n        C=1, kernel=lambda x, y: x @"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "fixes.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "trices will be deprecated.\nCSR_CONTAINERS = [scipy.sparse.csr_matrix, scipy.sparse.csr_array]\nCSC_CONTAINERS = [scipy.sparse.csc_matrix, scipy.sparse.csc_array]\nCOO_CONTAINERS = [scipy.sparse.coo_matrix, scipy.sparse.coo_array]\nLIL_CONTAINERS = [scipy.sparse.lil_matrix, scipy.sparse.lil_array]\nDOK_CONTAINERS = [scipy.sparse.dok_matrix, scipy.sparse.dok_array]\nBSR_CONTAINERS = [scipy.sparse.bsr_matrix, scipy.sparse.bsr_array]\nDIA_CONTAINERS = [scipy.sparse.dia_matrix, scipy.sparse.dia_array]\n\n# Remove when minimum scipy version is 1.11.0\ntry:\n    from scipy.sparse import sparray  # noqa: F401\n\n    SPARRAY_PRESENT = True\nexcept ImportError:\n    SPARRAY_PRESENT = False\n\n\ndef _object_dtype_isnan(X):\n    return X != X\n\n\n# TODO: Remove when SciPy 1.11 is the minimum supported version\ndef _mode(a, axis=0):\n    if sp_version >= parse_version(\"1.9.0\"):\n        mode = scipy.stats.mode(a, axis=axis, keepdims=True)\n        if sp_version >= parse_version(\"1.10.999\"):\n            # scipy.stats.mode has changed returned array shape with axis=None\n            # and keepdims=True, see https://github.com/scipy/scipy/pull/17561\n            if axis is None:\n                mode = np.ravel(mode)\n        return mode\n    return scipy.stats.mode(a, axis=axis)\n\n\n# TODO: Remove when Scipy 1.12 is the minimum supported version\nif sp_base_version >= parse_version(\"1.12.0\"):\n    _sparse_linalg_cg = scipy.sparse.linalg.cg\nelse:\n\n    def _sparse_linalg_cg(A, b, **kwargs):\n        if \"rtol\" in kwargs:\n            kwargs[\"tol\"] = kwargs.pop(\"rtol\")\n        if \"atol\" not in kwargs:\n            kwargs[\"atol\"] = \"legacy\"\n        return scipy.sparse.linalg.cg(A, b, **kwargs)\n\n\n# TODO : remove this when required minimum version of scipy >= 1.9.0\ndef _yeojohnson_lambda(_neg_log_likelihood, x):\n    \"\"\"Estimate the optimal Yeo-Johnson transformation parameter (lambda).\n\n    This function provides a compatibility workaround for versions of SciPy\n    older than 1.9.0, where `scipy.stats.yeojohnson` did not r"}], "retrieved_count": 10, "cost_time": 1.0302956104278564}
{"question": "What is the relationship between Scikit-learn's Pipeline class and the FeatureUnion class in establishing sequential and parallel processing patterns?", "answer": null, "relative_code_list": null, "ground_truth": "Pipeline and FeatureUnion establish complementary processing patterns in scikit-learn's composition system:\n\n1. **Pipeline - Sequential Processing**:\n   - **Linear Chain**: Executes estimators in a sequential order\n   - **Data Flow**: Output of one step becomes input to the next step\n   - **Method Chaining**: fit(), transform(), predict() flow through all steps\n   - **Intermediate Results**: Each step processes the output of the previous step\n   - **End-to-End**: Final estimator determines the overall estimator type\n\n2. **FeatureUnion - Parallel Processing**:\n   - **Parallel Execution**: Applies multiple transformers to the same input data\n   - **Feature Concatenation**: Combines outputs from multiple transformers\n   - **Independent Processing**: Each transformer operates on the original input\n   - **Feature Addition**: Adds new features without modifying existing ones\n   - **Horizontal Stacking**: Concatenates features horizontally (column-wise)\n\n3. **Processing Patterns**:\n   - **Pipeline Pattern**: A  B  C (sequential transformation)\n   - **FeatureUnion Pattern**: A, B, C  [A_output, B_output, C_output] (parallel transformation)\n   - **Combined Pattern**: Pipeline(FeatureUnion(A, B), C) (parallel then sequential)\n   - **Nested Pattern**: FeatureUnion(Pipeline(A, B), C) (sequential then parallel)\n\n4. **Method Behavior**:\n   - **Pipeline.fit()**: Calls fit() on each step, then fit_transform() on intermediate steps\n   - **Pipeline.transform()**: Calls transform() on each step sequentially\n   - **Pipeline.predict()**: Calls transform() on intermediate steps, then predict() on final estimator\n   - **FeatureUnion.fit()**: Calls fit() on all transformers in parallel\n   - **FeatureUnion.transform()**: Calls transform() on all transformers, then concatenates results\n\n5. **Data Flow Differences**:\n   - **Pipeline**: X  Step1  Step2  ...  Final_Step  Output\n   - **FeatureUnion**: X  [Transformer1, Transformer2, ...]  Concatenated_Output\n   - **Memory Efficiency**: Pipeline processes data incrementally, FeatureUnion processes all at once\n   - **Parallelization**: FeatureUnion can parallelize transformer execution\n\n6. **Use Cases**:\n   - **Pipeline**: Preprocessing  Feature Selection  Model Training\n   - **FeatureUnion**: Multiple feature extraction methods on same data\n   - **Combined**: Feature extraction (parallel)  Feature selection  Model (sequential)\n   - **Complex Workflows**: Nested combinations for sophisticated preprocessing\n\n7. **Parameter Routing**:\n   - **Pipeline**: Parameters prefixed with step names (step__param)\n   - **FeatureUnion**: Parameters prefixed with transformer names (transformer__param)\n   - **Metadata Routing**: Both support advanced metadata routing capabilities\n   - **Validation**: Parameters validated against appropriate step/transformer\n\n8. **Performance Characteristics**:\n   - **Pipeline**: Sequential execution, memory efficient for large datasets\n   - **FeatureUnion**: Parallel execution possible, higher memory usage\n   - **Caching**: Both support joblib caching for expensive computations\n   - **Parallelization**: FeatureUnion can use n_jobs for parallel transformer execution\n\n9. **Integration Features**:\n   - **Cross-validation**: Both work seamlessly with cross-validation\n   - **Grid Search**: Both support parameter tuning through GridSearchCV\n   - **Scoring**: Pipeline inherits scoring from final estimator\n   - **Feature Names**: Both preserve and generate appropriate feature names\n   - **Output Formats**: Both support set_output() for controlling output format", "score": null, "retrieved_content": [{"start_line": 33000, "end_line": 35000, "belongs_to": {"file_name": "test_pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n\n    # fit_transform should behave the same\n    X_transformed_parallel2 = fs_parallel2.fit_transform(X)\n    assert_array_equal(X_transformed.toarray(), X_transformed_parallel2.toarray())\n\n    # transformers should stay fit after fit_transform\n    X_transformed_parallel2 = fs_parallel2.transform(X)\n    assert_array_equal(X_transformed.toarray(), X_transformed_parallel2.toarray())\n\n\ndef test_feature_union_feature_names():\n    word_vect = CountVectorizer(analyzer=\"word\")\n    char_vect = CountVectorizer(analyzer=\"char_wb\", ngram_range=(3, 3))\n    ft = FeatureUnion([(\"chars\", char_vect), (\"words\", word_vect)])\n    ft.fit(JUNK_FOOD_DOCS)\n    feature_names = ft.get_feature_names_out()\n    for feat in feature_names:\n        assert \"chars__\" in feat or \"words__\" in feat\n    assert len(feature_names) == 35\n\n    ft = FeatureUnion([(\"tr1\", Transf())]).fit([[1]])\n\n    msg = re.escape(\n        \"Transformer tr1 (type Transf) does not provide get_feature_names_out\"\n    )\n    with pytest.raises(AttributeError, match=msg):\n        ft.get_feature_names_out()\n\n\ndef test_classes_property():\n    X = iris.data\n    y = iris.target\n\n    reg = make_pipeline(SelectKBest(k=1), LinearRegression())\n    reg.fit(X, y)\n    with pytest.raises(AttributeError):\n        getattr(reg, \"classes_\")\n\n    clf = make_pipeline(SelectKBest(k=1), LogisticRegression(random_state=0))\n    with pytest.raises(AttributeError):\n        getattr(clf, \"classes_\")\n    clf.fit(X, y)\n    assert_array_equal(clf.classes_, np.unique(y))\n\n\ndef test_set_feature_union_steps():\n    mult2 = Mult(2)\n    mult3 = Mult(3)\n    mult5 = Mult(5)\n\n    mult3.get_feature_names_out = lambda input_features: [\"x3\"]\n    mult2.get_feature_names_out = lambda input_features: [\"x2\"]\n    mult5.get_feature_names_out = lambda input_features: [\"x5\"]\n\n    ft = FeatureUnion([(\"m2\", mult2), (\"m3\", mult3)])\n    assert_array_equal([[2, 3]], ft.transform(np.asarray([[1]])))\n    assert_array_equal([\"m2__x2\", \"m3__x3\"], ft.get_feature_names_out())\n\n    # Directl"}, {"start_line": 63000, "end_line": 65000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "lel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionchanged:: v0.20\n           `n_jobs` default changed from 1 to None\n\n    transformer_weights : dict, default=None\n        Multiplicative weights for features per transformer.\n        Keys are transformer names, values the weights.\n        Raises ValueError if key not present in ``transformer_list``.\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting each transformer will be\n        printed as it is completed.\n\n    verbose_feature_names_out : bool, default=True\n        If True, :meth:`get_feature_names_out` will prefix all feature names\n        with the name of the transformer that generated that feature.\n        If False, :meth:`get_feature_names_out` will not prefix any feature\n        names and will error if feature names are not unique.\n\n        .. versionadded:: 1.5\n\n    Attributes\n    ----------\n    named_transformers : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n        Read-only attribute to access any transformer parameter by user\n        given name. Keys are transformer names and values are\n        transformer parameters.\n\n        .. versionadded:: 1.2\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying first transformer in `transformer_list` exposes such an\n        attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when\n        `X` has feature names that are all strings.\n\n        .. versionadded:: 1.3\n\n    See Also\n    --------\n    make_union : Convenience function for simplified feature union\n        construction.\n\n    Examples\n    --------\n    >>> from sklearn.pipeline import FeatureUnion\n  "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "thodMapping,\n    _raise_for_params,\n    _routing_enabled,\n    get_routing_for_object,\n    process_routing,\n)\nfrom sklearn.utils.metaestimators import _BaseComposition, available_if\nfrom sklearn.utils.parallel import Parallel, delayed\nfrom sklearn.utils.validation import check_is_fitted, check_memory\n\n__all__ = [\"FeatureUnion\", \"Pipeline\", \"make_pipeline\", \"make_union\"]\n\n\n@contextmanager\ndef _raise_or_warn_if_not_fitted(estimator):\n    \"\"\"A context manager to make sure a NotFittedError is raised, if a sub-estimator\n    raises the error.\n\n    Otherwise, we raise a warning if the pipeline is not fitted, with the deprecation.\n\n    TODO(1.8): remove this context manager and replace with check_is_fitted.\n    \"\"\"\n    try:\n        yield\n    except NotFittedError as exc:\n        raise NotFittedError(\"Pipeline is not fitted yet.\") from exc\n\n    # we only get here if the above didn't raise\n    try:\n        check_is_fitted(estimator)\n    except NotFittedError:\n        warnings.warn(\n            \"This Pipeline instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using other methods such as transform, \"\n            \"predict, etc. This will raise an error in 1.8 instead of the current \"\n            \"warning.\",\n            FutureWarning,\n        )\n\n\ndef _final_estimator_has(attr):\n    \"\"\"Check that final_estimator has `attr`.\n\n    Used together with `available_if` in `Pipeline`.\"\"\"\n\n    def check(self):\n        # raise original `AttributeError` if `attr` does not exist\n        getattr(self._final_estimator, attr)\n        return True\n\n    return check\n\n\ndef _cached_transform(\n    sub_pipeline, *, cache, param_name, param_value, transform_params\n):\n    \"\"\"Transform a parameter value using a sub-pipeline and cache the result.\n\n    Parameters\n    ----------\n    sub_pipeline : Pipeline\n        The sub-pipeline to be used for transformation.\n    cache : dict\n        The cache dictionary to store the transformed values.\n    param_name : str\n        The n"}, {"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "test_pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "sert pipe.named_steps[\"clf\"].successful\n    assert \"should_succeed\" not in pipe.named_steps[\"transf\"].fit_params\n\n\n@pytest.mark.parametrize(\n    \"method_name\", [\"predict\", \"predict_proba\", \"predict_log_proba\"]\n)\ndef test_predict_methods_with_predict_params(method_name):\n    # tests that Pipeline passes predict_* to the final estimator\n    # when predict_* is invoked\n    pipe = Pipeline([(\"transf\", Transf()), (\"clf\", DummyEstimatorParams())])\n    pipe.fit(None, None)\n    method = getattr(pipe, method_name)\n    method(X=None, got_attribute=True)\n\n    assert pipe.named_steps[\"clf\"].got_attribute\n\n\n@pytest.mark.parametrize(\"csr_container\", CSR_CONTAINERS)\ndef test_feature_union(csr_container):\n    # basic sanity check for feature union\n    X = iris.data.copy()\n    X -= X.mean(axis=0)\n    y = iris.target\n    svd = TruncatedSVD(n_components=2, random_state=0)\n    select = SelectKBest(k=1)\n    fs = FeatureUnion([(\"svd\", svd), (\"select\", select)])\n    fs.fit(X, y)\n    X_transformed = fs.transform(X)\n    assert X_transformed.shape == (X.shape[0], 3)\n\n    # check if it does the expected thing\n    assert_array_almost_equal(X_transformed[:, :-1], svd.fit_transform(X))\n    assert_array_equal(X_transformed[:, -1], select.fit_transform(X, y).ravel())\n\n    # test if it also works for sparse input\n    # We use a different svd object to control the random_state stream\n    fs = FeatureUnion([(\"svd\", svd), (\"select\", select)])\n    X_sp = csr_container(X)\n    X_sp_transformed = fs.fit_transform(X_sp, y)\n    assert_array_almost_equal(X_transformed, X_sp_transformed.toarray())\n\n    # Test clone\n    fs2 = clone(fs)\n    assert fs.transformer_list[0][1] is not fs2.transformer_list[0][1]\n\n    # test setting parameters\n    fs.set_params(select__k=2)\n    assert fs.fit_transform(X, y).shape == (X.shape[0], 4)\n\n    # test it works with transformers missing fit_transform\n    fs = FeatureUnion([(\"mock\", Transf()), (\"svd\", svd), (\"select\", select)])\n    X_transformed = fs.fit_transform(X, y)\n    ass"}, {"start_line": 31000, "end_line": 33000, "belongs_to": {"file_name": "test_pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " transformer_weights={\"pca\": 10}\n    )\n    fs.fit(X, y)\n    X_transformed = fs.transform(X)\n    # test using fit_transform\n    fs = FeatureUnion(\n        [(\"pca\", pca), (\"select\", select)], transformer_weights={\"pca\": 10}\n    )\n    X_fit_transformed = fs.fit_transform(X, y)\n    # test it works with transformers missing fit_transform\n    fs = FeatureUnion(\n        [(\"mock\", Transf()), (\"pca\", pca), (\"select\", select)],\n        transformer_weights={\"mock\": 10},\n    )\n    X_fit_transformed_wo_method = fs.fit_transform(X, y)\n    # check against expected result\n\n    # We use a different pca object to control the random_state stream\n    assert_array_almost_equal(X_transformed[:, :-1], 10 * pca.fit_transform(X))\n    assert_array_equal(X_transformed[:, -1], select.fit_transform(X, y).ravel())\n    assert_array_almost_equal(X_fit_transformed[:, :-1], 10 * pca.fit_transform(X))\n    assert_array_equal(X_fit_transformed[:, -1], select.fit_transform(X, y).ravel())\n    assert X_fit_transformed_wo_method.shape == (X.shape[0], 7)\n\n\ndef test_feature_union_parallel():\n    # test that n_jobs work for FeatureUnion\n    X = JUNK_FOOD_DOCS\n\n    fs = FeatureUnion(\n        [\n            (\"words\", CountVectorizer(analyzer=\"word\")),\n            (\"chars\", CountVectorizer(analyzer=\"char\")),\n        ]\n    )\n\n    fs_parallel = FeatureUnion(\n        [\n            (\"words\", CountVectorizer(analyzer=\"word\")),\n            (\"chars\", CountVectorizer(analyzer=\"char\")),\n        ],\n        n_jobs=2,\n    )\n\n    fs_parallel2 = FeatureUnion(\n        [\n            (\"words\", CountVectorizer(analyzer=\"word\")),\n            (\"chars\", CountVectorizer(analyzer=\"char\")),\n        ],\n        n_jobs=2,\n    )\n\n    fs.fit(X)\n    X_transformed = fs.transform(X)\n    assert X_transformed.shape[0] == len(X)\n\n    fs_parallel.fit(X)\n    X_transformed_parallel = fs_parallel.transform(X)\n    assert X_transformed.shape == X_transformed_parallel.shape\n    assert_array_equal(X_transformed.toarray(), X_transformed_parallel.toarray())"}, {"start_line": 34000, "end_line": 36000, "belongs_to": {"file_name": "test_pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "uteError, match=msg):\n        ft.get_feature_names_out()\n\n\ndef test_classes_property():\n    X = iris.data\n    y = iris.target\n\n    reg = make_pipeline(SelectKBest(k=1), LinearRegression())\n    reg.fit(X, y)\n    with pytest.raises(AttributeError):\n        getattr(reg, \"classes_\")\n\n    clf = make_pipeline(SelectKBest(k=1), LogisticRegression(random_state=0))\n    with pytest.raises(AttributeError):\n        getattr(clf, \"classes_\")\n    clf.fit(X, y)\n    assert_array_equal(clf.classes_, np.unique(y))\n\n\ndef test_set_feature_union_steps():\n    mult2 = Mult(2)\n    mult3 = Mult(3)\n    mult5 = Mult(5)\n\n    mult3.get_feature_names_out = lambda input_features: [\"x3\"]\n    mult2.get_feature_names_out = lambda input_features: [\"x2\"]\n    mult5.get_feature_names_out = lambda input_features: [\"x5\"]\n\n    ft = FeatureUnion([(\"m2\", mult2), (\"m3\", mult3)])\n    assert_array_equal([[2, 3]], ft.transform(np.asarray([[1]])))\n    assert_array_equal([\"m2__x2\", \"m3__x3\"], ft.get_feature_names_out())\n\n    # Directly setting attr\n    ft.transformer_list = [(\"m5\", mult5)]\n    assert_array_equal([[5]], ft.transform(np.asarray([[1]])))\n    assert_array_equal([\"m5__x5\"], ft.get_feature_names_out())\n\n    # Using set_params\n    ft.set_params(transformer_list=[(\"mock\", mult3)])\n    assert_array_equal([[3]], ft.transform(np.asarray([[1]])))\n    assert_array_equal([\"mock__x3\"], ft.get_feature_names_out())\n\n    # Using set_params to replace single step\n    ft.set_params(mock=mult5)\n    assert_array_equal([[5]], ft.transform(np.asarray([[1]])))\n    assert_array_equal([\"mock__x5\"], ft.get_feature_names_out())\n\n\ndef test_set_feature_union_step_drop():\n    mult2 = Mult(2)\n    mult3 = Mult(3)\n\n    mult2.get_feature_names_out = lambda input_features: [\"x2\"]\n    mult3.get_feature_names_out = lambda input_features: [\"x3\"]\n\n    X = np.asarray([[1]])\n\n    ft = FeatureUnion([(\"m2\", mult2), (\"m3\", mult3)])\n    assert_array_equal([[2, 3]], ft.fit(X).transform(X))\n    assert_array_equal([[2, 3]], ft.fit_transform(X))\n  "}, {"start_line": 64000, "end_line": 66000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "added:: 1.5\n\n    Attributes\n    ----------\n    named_transformers : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n        Read-only attribute to access any transformer parameter by user\n        given name. Keys are transformer names and values are\n        transformer parameters.\n\n        .. versionadded:: 1.2\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying first transformer in `transformer_list` exposes such an\n        attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when\n        `X` has feature names that are all strings.\n\n        .. versionadded:: 1.3\n\n    See Also\n    --------\n    make_union : Convenience function for simplified feature union\n        construction.\n\n    Examples\n    --------\n    >>> from sklearn.pipeline import FeatureUnion\n    >>> from sklearn.decomposition import PCA, TruncatedSVD\n    >>> union = FeatureUnion([(\"pca\", PCA(n_components=1)),\n    ...                       (\"svd\", TruncatedSVD(n_components=2))])\n    >>> X = [[0., 1., 3], [2., 2., 5]]\n    >>> union.fit_transform(X)\n    array([[-1.5       ,  3.04, -0.872],\n           [ 1.5       ,  5.72,  0.463]])\n    >>> # An estimator's parameter can be set using '__' syntax\n    >>> union.set_params(svd__n_components=1).fit_transform(X)\n    array([[-1.5       ,  3.04],\n           [ 1.5       ,  5.72]])\n\n    For a more detailed example of usage, see\n    :ref:`sphx_glr_auto_examples_compose_plot_feature_union.py`.\n    \"\"\"\n\n    def __init__(\n        self,\n        transformer_list,\n        *,\n        n_jobs=None,\n        transformer_weights=None,\n        verbose=False,\n        verbose_feature_names_out=True,\n    ):\n        self.transformer_list = transformer_list\n        self.n_jobs = n_jobs\n        self.transformer_weights = transformer_weights\n        self.verb"}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "test_pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "se_feature_names_out():\n    # Test that make_union passes verbose_feature_names_out\n    # to the FeatureUnion.\n    X = iris.data\n    y = iris.target\n\n    pca = PCA()\n    mock = create_mock_transformer(\"transf\")\n    union = make_union(pca, mock, verbose_feature_names_out=False)\n\n    assert not union.verbose_feature_names_out\n\n    fu_union = make_union(pca, mock, verbose_feature_names_out=True)\n    fu_union.fit(X, y)\n\n    assert_array_equal(\n        [\n            \"pca__pca0\",\n            \"pca__pca1\",\n            \"pca__pca2\",\n            \"pca__pca3\",\n            \"transf__transf0\",\n            \"transf__transf1\",\n            \"transf__transf2\",\n        ],\n        fu_union.get_feature_names_out(),\n    )\n\n\ndef test_pipeline_transform():\n    # Test whether pipeline works with a transformer at the end.\n    # Also test pipeline.transform and pipeline.inverse_transform\n    X = iris.data\n    pca = PCA(n_components=2, svd_solver=\"full\")\n    pipeline = Pipeline([(\"pca\", pca)])\n\n    # test transform and fit_transform:\n    X_trans = pipeline.fit(X).transform(X)\n    X_trans2 = pipeline.fit_transform(X)\n    X_trans3 = pca.fit_transform(X)\n    assert_array_almost_equal(X_trans, X_trans2)\n    assert_array_almost_equal(X_trans, X_trans3)\n\n    X_back = pipeline.inverse_transform(X_trans)\n    X_back2 = pca.inverse_transform(X_trans)\n    assert_array_almost_equal(X_back, X_back2)\n\n\ndef test_pipeline_fit_transform():\n    # Test whether pipeline works with a transformer missing fit_transform\n    X = iris.data\n    y = iris.target\n    transf = Transf()\n    pipeline = Pipeline([(\"mock\", transf)])\n\n    # test fit_transform:\n    X_trans = pipeline.fit_transform(X, y)\n    X_trans2 = transf.fit(X, y).transform(X)\n    assert_array_almost_equal(X_trans, X_trans2)\n\n\n@pytest.mark.parametrize(\n    \"start, end\", [(0, 1), (0, 2), (1, 2), (1, 3), (None, 1), (1, None), (None, None)]\n)\ndef test_pipeline_slice(start, end):\n    pipe = Pipeline(\n        [(\"transf1\", Transf()), (\"transf2\", Transf()), (\"clf\", FitP"}, {"start_line": 51000, "end_line": 53000, "belongs_to": {"file_name": "test_pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " of 3\\) Processing transf.* total=.*\\n\"\n                r\"\\[Pipeline\\].*\\(step 2 of 3\\) Processing noop.* total=.*\\n\"\n                r\"\\[Pipeline\\].*\\(step 3 of 3\\) Processing clf.* total=.*\\n$\",\n            ),\n            (\n                Pipeline(\n                    [\n                        (\"transf\", Transf()),\n                        (\"noop\", \"passthrough\"),\n                        (\"clf\", FitParamT()),\n                    ]\n                ),\n                r\"\\[Pipeline\\].*\\(step 1 of 3\\) Processing transf.* total=.*\\n\"\n                r\"\\[Pipeline\\].*\\(step 2 of 3\\) Processing noop.* total=.*\\n\"\n                r\"\\[Pipeline\\].*\\(step 3 of 3\\) Processing clf.* total=.*\\n$\",\n            ),\n            (\n                Pipeline([(\"transf\", Transf()), (\"clf\", None)]),\n                r\"\\[Pipeline\\].*\\(step 1 of 2\\) Processing transf.* total=.*\\n\"\n                r\"\\[Pipeline\\].*\\(step 2 of 2\\) Processing clf.* total=.*\\n$\",\n            ),\n            (\n                Pipeline([(\"transf\", None), (\"mult\", Mult())]),\n                r\"\\[Pipeline\\].*\\(step 1 of 2\\) Processing transf.* total=.*\\n\"\n                r\"\\[Pipeline\\].*\\(step 2 of 2\\) Processing mult.* total=.*\\n$\",\n            ),\n            (\n                Pipeline([(\"transf\", \"passthrough\"), (\"mult\", Mult())]),\n                r\"\\[Pipeline\\].*\\(step 1 of 2\\) Processing transf.* total=.*\\n\"\n                r\"\\[Pipeline\\].*\\(step 2 of 2\\) Processing mult.* total=.*\\n$\",\n            ),\n            (\n                FeatureUnion([(\"mult1\", Mult()), (\"mult2\", Mult())]),\n                r\"\\[FeatureUnion\\].*\\(step 1 of 2\\) Processing mult1.* total=.*\\n\"\n                r\"\\[FeatureUnion\\].*\\(step 2 of 2\\) Processing mult2.* total=.*\\n$\",\n            ),\n            (\n                FeatureUnion([(\"mult1\", \"drop\"), (\"mult2\", Mult()), (\"mult3\", \"drop\")]),\n                r\"\\[FeatureUnion\\].*\\(step 1 of 1\\) Processing mult2.* total=.*\\n$\",\n            ),\n        ],\n        [\"fit\", \"fit_transform\", \"fit_predi"}, {"start_line": 29000, "end_line": 31000, "belongs_to": {"file_name": "test_pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ct\")\n    pipeline.transform\n    assert not hasattr(pipeline, \"inverse_transform\")\n\n\ndef test_make_pipeline():\n    t1 = Transf()\n    t2 = Transf()\n    pipe = make_pipeline(t1, t2)\n    assert isinstance(pipe, Pipeline)\n    assert pipe.steps[0][0] == \"transf-1\"\n    assert pipe.steps[1][0] == \"transf-2\"\n\n    pipe = make_pipeline(t1, t2, FitParamT())\n    assert isinstance(pipe, Pipeline)\n    assert pipe.steps[0][0] == \"transf-1\"\n    assert pipe.steps[1][0] == \"transf-2\"\n    assert pipe.steps[2][0] == \"fitparamt\"\n\n\n@pytest.mark.parametrize(\n    \"pipeline, check_estimator_type\",\n    [\n        (make_pipeline(StandardScaler(), LogisticRegression()), is_classifier),\n        (make_pipeline(StandardScaler(), LinearRegression()), is_regressor),\n        (\n            make_pipeline(StandardScaler()),\n            lambda est: get_tags(est).estimator_type is None,\n        ),\n        (Pipeline([]), lambda est: est._estimator_type is None),\n    ],\n)\ndef test_pipeline_estimator_type(pipeline, check_estimator_type):\n    \"\"\"Check that the estimator type returned by the pipeline is correct.\n\n    Non-regression test as part of:\n    https://github.com/scikit-learn/scikit-learn/issues/30197\n    \"\"\"\n    # Smoke test the repr\n    repr(pipeline)\n    assert check_estimator_type(pipeline)\n\n\ndef test_sklearn_tags_with_empty_pipeline():\n    \"\"\"Check that we propagate properly the tags in a Pipeline.\n\n    Non-regression test as part of:\n    https://github.com/scikit-learn/scikit-learn/issues/30197\n    \"\"\"\n    empty_pipeline = Pipeline(steps=[])\n    be = BaseEstimator()\n\n    expected_tags = be.__sklearn_tags__()\n    assert empty_pipeline.__sklearn_tags__() == expected_tags\n\n\ndef test_feature_union_weights():\n    # test feature union with transformer weights\n    X = iris.data\n    y = iris.target\n    pca = PCA(n_components=2, svd_solver=\"randomized\", random_state=0)\n    select = SelectKBest(k=1)\n    # test using fit followed by transform\n    fs = FeatureUnion(\n        [(\"pca\", pca), (\"select\", select)],"}], "retrieved_count": 10, "cost_time": 1.0196754932403564}
{"question": "What is the structure of Scikit-learn's preprocessing module?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn's preprocessing module is organized into several specialized submodules and components:\n\n1. **Core Data Processing (_data.py)**: Contains the main scaling and normalization transformers:\n   - **StandardScaler**: Standardizes features by removing mean and scaling to unit variance\n   - **MinMaxScaler**: Scales features to a given range (default [0,1])\n   - **MaxAbsScaler**: Scales each feature by its maximum absolute value\n   - **RobustScaler**: Scales features using statistics that are robust to outliers\n   - **Normalizer**: Normalizes samples individually to unit norm (l1, l2, or max)\n   - **PowerTransformer**: Applies power transformations (Box-Cox, Yeo-Johnson)\n   - **QuantileTransformer**: Transforms features using quantile information\n   - **Binarizer**: Binarizes data according to a threshold\n   - **KernelCenterer**: Centers a kernel matrix\n\n2. **Categorical Encoding (_encoders.py)**: Handles categorical feature encoding:\n   - **OneHotEncoder**: Encodes categorical features as a one-hot numeric array\n   - **OrdinalEncoder**: Encodes categorical features as an integer array\n\n3. **Label Processing (_label.py)**: Manages target variable encoding:\n   - **LabelEncoder**: Encodes target labels with values between 0 and n_classes-1\n   - **LabelBinarizer**: Binarizes labels in a one-vs-all fashion\n   - **MultiLabelBinarizer**: Transforms between iterable of iterables and a multilabel format\n\n4. **Discretization (_discretization.py)**:\n   - **KBinsDiscretizer**: Discretizes continuous features into k bins\n\n5. **Polynomial Features (_polynomial.py)**:\n   - **PolynomialFeatures**: Generates polynomial and interaction features\n   - **SplineTransformer**: Generates univariate B-spline bases for features\n\n6. **Function Transformers (_function_transformer.py)**:\n   - **FunctionTransformer**: Constructs a transformer from an arbitrary callable\n\n7. **Target Encoding (_target_encoder.py)**:\n   - **TargetEncoder**: Encodes categorical features using target statistics\n\n8. **Utility Functions**: The module also provides standalone functions for common operations:\n   - **scale()**: Standardize a dataset along any axis\n   - **minmax_scale()**: Transform features by scaling each feature to a given range\n   - **robust_scale()**: Standardize a dataset using robust statistics\n   - **normalize()**: Scale input vectors individually to unit norm\n   - **binarize()**: Binarize data according to a threshold\n   - **add_dummy_feature()**: Add a dummy feature to the dataset\n\nAll these components follow the scikit-learn estimator interface, implementing fit(), transform(), and fit_transform() methods where appropriate. They can be used individually or combined in pipelines for complex preprocessing workflows.", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 1671, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Methods for scaling, centering, normalization, binarization, and more.\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nfrom sklearn.preprocessing._data import (\n    Binarizer,\n    KernelCenterer,\n    MaxAbsScaler,\n    MinMaxScaler,\n    Normalizer,\n    PowerTransformer,\n    QuantileTransformer,\n    RobustScaler,\n    StandardScaler,\n    add_dummy_feature,\n    binarize,\n    maxabs_scale,\n    minmax_scale,\n    normalize,\n    power_transform,\n    quantile_transform,\n    robust_scale,\n    scale,\n)\nfrom sklearn.preprocessing._discretization import KBinsDiscretizer\nfrom sklearn.preprocessing._encoders import OneHotEncoder, OrdinalEncoder\nfrom sklearn.preprocessing._function_transformer import FunctionTransformer\nfrom sklearn.preprocessing._label import (\n    LabelBinarizer,\n    LabelEncoder,\n    MultiLabelBinarizer,\n    label_binarize,\n)\nfrom sklearn.preprocessing._polynomial import PolynomialFeatures, SplineTransformer\nfrom sklearn.preprocessing._target_encoder import TargetEncoder\n\n__all__ = [\n    \"Binarizer\",\n    \"FunctionTransformer\",\n    \"KBinsDiscretizer\",\n    \"KernelCenterer\",\n    \"LabelBinarizer\",\n    \"LabelEncoder\",\n    \"MaxAbsScaler\",\n    \"MinMaxScaler\",\n    \"MultiLabelBinarizer\",\n    \"Normalizer\",\n    \"OneHotEncoder\",\n    \"OrdinalEncoder\",\n    \"PolynomialFeatures\",\n    \"PowerTransformer\",\n    \"QuantileTransformer\",\n    \"RobustScaler\",\n    \"SplineTransformer\",\n    \"StandardScaler\",\n    \"TargetEncoder\",\n    \"add_dummy_feature\",\n    \"binarize\",\n    \"label_binarize\",\n    \"maxabs_scale\",\n    \"minmax_scale\",\n    \"normalize\",\n    \"power_transform\",\n    \"quantile_transform\",\n    \"robust_scale\",\n    \"scale\",\n]\n"}, {"start_line": 37000, "end_line": 39000, "belongs_to": {"file_name": "api_reference.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/doc", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ernoulliRBM\", \"MLPClassifier\", \"MLPRegressor\"],\n            },\n        ],\n    },\n    \"sklearn.pipeline\": {\n        \"short_summary\": \"Pipeline.\",\n        \"description\": _get_guide(\"combining_estimators\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"FeatureUnion\",\n                    \"Pipeline\",\n                    \"make_pipeline\",\n                    \"make_union\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.preprocessing\": {\n        \"short_summary\": \"Preprocessing and normalization.\",\n        \"description\": _get_guide(\"preprocessing\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"Binarizer\",\n                    \"FunctionTransformer\",\n                    \"KBinsDiscretizer\",\n                    \"KernelCenterer\",\n                    \"LabelBinarizer\",\n                    \"LabelEncoder\",\n                    \"MaxAbsScaler\",\n                    \"MinMaxScaler\",\n                    \"MultiLabelBinarizer\",\n                    \"Normalizer\",\n                    \"OneHotEncoder\",\n                    \"OrdinalEncoder\",\n                    \"PolynomialFeatures\",\n                    \"PowerTransformer\",\n                    \"QuantileTransformer\",\n                    \"RobustScaler\",\n                    \"SplineTransformer\",\n                    \"StandardScaler\",\n                    \"TargetEncoder\",\n                    \"add_dummy_feature\",\n                    \"binarize\",\n                    \"label_binarize\",\n                    \"maxabs_scale\",\n                    \"minmax_scale\",\n                    \"normalize\",\n                    \"power_transform\",\n                    \"quantile_transform\",\n                    \"robust_scale\",\n                    \"scale\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.random_projection\": {\n        \"short_summary\": \"Random projection.\",\n        \"description\": _get_guide(\"r"}, {"start_line": 38000, "end_line": 40000, "belongs_to": {"file_name": "api_reference.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/doc", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\",\n                    \"MinMaxScaler\",\n                    \"MultiLabelBinarizer\",\n                    \"Normalizer\",\n                    \"OneHotEncoder\",\n                    \"OrdinalEncoder\",\n                    \"PolynomialFeatures\",\n                    \"PowerTransformer\",\n                    \"QuantileTransformer\",\n                    \"RobustScaler\",\n                    \"SplineTransformer\",\n                    \"StandardScaler\",\n                    \"TargetEncoder\",\n                    \"add_dummy_feature\",\n                    \"binarize\",\n                    \"label_binarize\",\n                    \"maxabs_scale\",\n                    \"minmax_scale\",\n                    \"normalize\",\n                    \"power_transform\",\n                    \"quantile_transform\",\n                    \"robust_scale\",\n                    \"scale\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.random_projection\": {\n        \"short_summary\": \"Random projection.\",\n        \"description\": _get_guide(\"random_projection\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"GaussianRandomProjection\",\n                    \"SparseRandomProjection\",\n                    \"johnson_lindenstrauss_min_dim\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.semi_supervised\": {\n        \"short_summary\": \"Semi-supervised learning.\",\n        \"description\": _get_guide(\"semi_supervised\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"LabelPropagation\",\n                    \"LabelSpreading\",\n                    \"SelfTrainingClassifier\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.svm\": {\n        \"short_summary\": \"Support vector machines.\",\n        \"description\": _get_guide(\"svm\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"LinearSVC\",\n               "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "column_scale,\n    mean_variance_axis,\n    min_max_axis,\n)\nfrom sklearn.utils.sparsefuncs_fast import (\n    inplace_csr_row_normalize_l1,\n    inplace_csr_row_normalize_l2,\n)\nfrom sklearn.utils.validation import (\n    FLOAT_DTYPES,\n    _check_sample_weight,\n    check_is_fitted,\n    check_random_state,\n    validate_data,\n)\n\nBOUNDS_THRESHOLD = 1e-7\n\n__all__ = [\n    \"Binarizer\",\n    \"KernelCenterer\",\n    \"MaxAbsScaler\",\n    \"MinMaxScaler\",\n    \"Normalizer\",\n    \"OneHotEncoder\",\n    \"PowerTransformer\",\n    \"QuantileTransformer\",\n    \"RobustScaler\",\n    \"StandardScaler\",\n    \"add_dummy_feature\",\n    \"binarize\",\n    \"maxabs_scale\",\n    \"minmax_scale\",\n    \"normalize\",\n    \"power_transform\",\n    \"quantile_transform\",\n    \"robust_scale\",\n    \"scale\",\n]\n\n\ndef _is_constant_feature(var, mean, n_samples):\n    \"\"\"Detect if a feature is indistinguishable from a constant feature.\n\n    The detection is based on its computed variance and on the theoretical\n    error bounds of the '2 pass algorithm' for variance computation.\n\n    See \"Algorithms for computing the sample variance: analysis and\n    recommendations\", by Chan, Golub, and LeVeque.\n    \"\"\"\n    # In scikit-learn, variance is always computed using float64 accumulators.\n    eps = np.finfo(np.float64).eps\n\n    upper_bound = n_samples * eps * var + (n_samples * mean * eps) ** 2\n    return var <= upper_bound\n\n\ndef _handle_zeros_in_scale(scale, copy=True, constant_mask=None):\n    \"\"\"Set scales of near constant features to 1.\n\n    The goal is to avoid division by very small or zero values.\n\n    Near constant features are detected automatically by identifying\n    scales close to machine precision unless they are precomputed by\n    the caller and passed with the `constant_mask` kwarg.\n\n    Typically for standard scaling, the scales are the standard\n    deviation while near constant features are better detected on the\n    computed variances which are closer to machine precision by\n    construction.\n    \"\"\"\n    # if we are fitting on 1D"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\n\nimport warnings\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy import sparse, stats\nfrom scipy.special import boxcox, inv_boxcox\n\nfrom sklearn.base import (\n    BaseEstimator,\n    ClassNamePrefixFeaturesOutMixin,\n    OneToOneFeatureMixin,\n    TransformerMixin,\n    _fit_context,\n)\nfrom sklearn.preprocessing._encoders import OneHotEncoder\nfrom sklearn.utils import _array_api, check_array, metadata_routing, resample\nfrom sklearn.utils._array_api import (\n    _find_matching_floating_dtype,\n    _modify_in_place_if_numpy,\n    device,\n    get_namespace,\n    get_namespace_and_device,\n)\nfrom sklearn.utils._param_validation import (\n    Interval,\n    Options,\n    StrOptions,\n    validate_params,\n)\nfrom sklearn.utils.extmath import _incremental_mean_and_var, row_norms\nfrom sklearn.utils.fixes import _yeojohnson_lambda\nfrom sklearn.utils.sparsefuncs import (\n    incr_mean_variance_axis,\n    inplace_column_scale,\n    mean_variance_axis,\n    min_max_axis,\n)\nfrom sklearn.utils.sparsefuncs_fast import (\n    inplace_csr_row_normalize_l1,\n    inplace_csr_row_normalize_l2,\n)\nfrom sklearn.utils.validation import (\n    FLOAT_DTYPES,\n    _check_sample_weight,\n    check_is_fitted,\n    check_random_state,\n    validate_data,\n)\n\nBOUNDS_THRESHOLD = 1e-7\n\n__all__ = [\n    \"Binarizer\",\n    \"KernelCenterer\",\n    \"MaxAbsScaler\",\n    \"MinMaxScaler\",\n    \"Normalizer\",\n    \"OneHotEncoder\",\n    \"PowerTransformer\",\n    \"QuantileTransformer\",\n    \"RobustScaler\",\n    \"StandardScaler\",\n    \"add_dummy_feature\",\n    \"binarize\",\n    \"maxabs_scale\",\n    \"minmax_scale\",\n    \"normalize\",\n    \"power_transform\",\n    \"quantile_transform\",\n    \"robust_scale\",\n    \"scale\",\n]\n\n\ndef _is_constant_feature(var, mean, n_samples):\n    \"\"\"Detect if a feature is indistinguishable from a constant feature.\n\n    The detection is based on its computed variance and on the theoretical\n    error bounds of the '2 pass algorithm' for v"}, {"start_line": 36000, "end_line": 38000, "belongs_to": {"file_name": "api_reference.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/doc", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "          \"KDTree\",\n                    \"KNeighborsClassifier\",\n                    \"KNeighborsRegressor\",\n                    \"KNeighborsTransformer\",\n                    \"KernelDensity\",\n                    \"LocalOutlierFactor\",\n                    \"NearestCentroid\",\n                    \"NearestNeighbors\",\n                    \"NeighborhoodComponentsAnalysis\",\n                    \"RadiusNeighborsClassifier\",\n                    \"RadiusNeighborsRegressor\",\n                    \"RadiusNeighborsTransformer\",\n                    \"kneighbors_graph\",\n                    \"radius_neighbors_graph\",\n                    \"sort_graph_by_row_values\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.neural_network\": {\n        \"short_summary\": \"Neural network models.\",\n        \"description\": _get_guide(\n            \"neural_networks_supervised\", \"neural_networks_unsupervised\"\n        ),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\"BernoulliRBM\", \"MLPClassifier\", \"MLPRegressor\"],\n            },\n        ],\n    },\n    \"sklearn.pipeline\": {\n        \"short_summary\": \"Pipeline.\",\n        \"description\": _get_guide(\"combining_estimators\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"FeatureUnion\",\n                    \"Pipeline\",\n                    \"make_pipeline\",\n                    \"make_union\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.preprocessing\": {\n        \"short_summary\": \"Preprocessing and normalization.\",\n        \"description\": _get_guide(\"preprocessing\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"Binarizer\",\n                    \"FunctionTransformer\",\n                    \"KBinsDiscretizer\",\n                    \"KernelCenterer\",\n                    \"LabelBinarizer\",\n                    \"LabelEncoder\",\n                    \"MaxAbsScaler"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "instance_generator.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/_test_common", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "sklearn.kernel_approximation import (\n    Nystroem,\n    PolynomialCountSketch,\n    RBFSampler,\n    SkewedChi2Sampler,\n)\nfrom sklearn.linear_model import (\n    ARDRegression,\n    BayesianRidge,\n    ElasticNet,\n    ElasticNetCV,\n    GammaRegressor,\n    HuberRegressor,\n    LarsCV,\n    Lasso,\n    LassoCV,\n    LassoLars,\n    LassoLarsCV,\n    LassoLarsIC,\n    LinearRegression,\n    LogisticRegression,\n    LogisticRegressionCV,\n    MultiTaskElasticNet,\n    MultiTaskElasticNetCV,\n    MultiTaskLasso,\n    MultiTaskLassoCV,\n    OrthogonalMatchingPursuitCV,\n    PassiveAggressiveClassifier,\n    PassiveAggressiveRegressor,\n    Perceptron,\n    PoissonRegressor,\n    QuantileRegressor,\n    RANSACRegressor,\n    Ridge,\n    RidgeClassifier,\n    SGDClassifier,\n    SGDOneClassSVM,\n    SGDRegressor,\n    TheilSenRegressor,\n    TweedieRegressor,\n)\nfrom sklearn.manifold import (\n    MDS,\n    TSNE,\n    Isomap,\n    LocallyLinearEmbedding,\n    SpectralEmbedding,\n)\nfrom sklearn.mixture import BayesianGaussianMixture, GaussianMixture\nfrom sklearn.model_selection import (\n    FixedThresholdClassifier,\n    GridSearchCV,\n    HalvingGridSearchCV,\n    HalvingRandomSearchCV,\n    RandomizedSearchCV,\n    TunedThresholdClassifierCV,\n)\nfrom sklearn.multiclass import (\n    OneVsOneClassifier,\n    OneVsRestClassifier,\n    OutputCodeClassifier,\n)\nfrom sklearn.multioutput import (\n    ClassifierChain,\n    MultiOutputClassifier,\n    MultiOutputRegressor,\n    RegressorChain,\n)\nfrom sklearn.neighbors import (\n    KernelDensity,\n    KNeighborsClassifier,\n    KNeighborsRegressor,\n    KNeighborsTransformer,\n    NeighborhoodComponentsAnalysis,\n    RadiusNeighborsTransformer,\n)\nfrom sklearn.neural_network import BernoulliRBM, MLPClassifier, MLPRegressor\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.preprocessing import (\n    KBinsDiscretizer,\n    OneHotEncoder,\n    SplineTransformer,\n    StandardScaler,\n    TargetEncoder,\n)\nfrom sklearn.random_projection import GaussianRandomProjection, SparseRandomP"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport re\nimport warnings\n\nimport numpy as np\nimport numpy.linalg as la\nimport pytest\nfrom scipy import sparse, stats\n\nfrom sklearn import config_context, datasets\nfrom sklearn.base import clone\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.externals._packaging.version import parse as parse_version\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import (\n    Binarizer,\n    KernelCenterer,\n    MaxAbsScaler,\n    MinMaxScaler,\n    Normalizer,\n    PowerTransformer,\n    QuantileTransformer,\n    RobustScaler,\n    StandardScaler,\n    add_dummy_feature,\n    maxabs_scale,\n    minmax_scale,\n    normalize,\n    power_transform,\n    quantile_transform,\n    robust_scale,\n    scale,\n)\nfrom sklearn.preprocessing._data import BOUNDS_THRESHOLD, _handle_zeros_in_scale\nfrom sklearn.svm import SVR\nfrom sklearn.utils import gen_batches, shuffle\nfrom sklearn.utils._array_api import (\n    _convert_to_numpy,\n    _get_namespace_device_dtype_ids,\n    yield_namespace_device_dtype_combinations,\n)\nfrom sklearn.utils._test_common.instance_generator import _get_check_estimator_ids\nfrom sklearn.utils._testing import (\n    _array_api_for_tests,\n    _convert_container,\n    assert_allclose,\n    assert_allclose_dense_sparse,\n    assert_almost_equal,\n    assert_array_almost_equal,\n    assert_array_equal,\n    assert_array_less,\n    skip_if_32bit,\n)\nfrom sklearn.utils.estimator_checks import (\n    check_array_api_input_and_values,\n)\nfrom sklearn.utils.fixes import (\n    COO_CONTAINERS,\n    CSC_CONTAINERS,\n    CSR_CONTAINERS,\n    LIL_CONTAINERS,\n    sp_version,\n)\nfrom sklearn.utils.sparsefuncs import mean_variance_axis\n\niris = datasets.load_iris()\n\n# Make some data to be used many times\nrng = np.random.RandomState(0)\nn_features = 30\nn_samples = 1000\noffsets = rng.uniform(-1, 1, size=n_features)\n"}, {"start_line": 1000, "end_line": 2312, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tTags,\n    RegressorTags,\n    Tags,\n    TargetTags,\n    TransformerTags,\n    get_tags,\n)\nfrom sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\nfrom sklearn.utils.deprecation import deprecated\nfrom sklearn.utils.discovery import all_estimators\nfrom sklearn.utils.extmath import safe_sqr\nfrom sklearn.utils.murmurhash import murmurhash3_32\nfrom sklearn.utils.validation import (\n    as_float_array,\n    assert_all_finite,\n    check_array,\n    check_consistent_length,\n    check_random_state,\n    check_scalar,\n    check_symmetric,\n    check_X_y,\n    column_or_1d,\n    indexable,\n)\n\n__all__ = [\n    \"Bunch\",\n    \"ClassifierTags\",\n    \"DataConversionWarning\",\n    \"InputTags\",\n    \"RegressorTags\",\n    \"Tags\",\n    \"TargetTags\",\n    \"TransformerTags\",\n    \"_safe_indexing\",\n    \"all_estimators\",\n    \"as_float_array\",\n    \"assert_all_finite\",\n    \"check_X_y\",\n    \"check_array\",\n    \"check_consistent_length\",\n    \"check_random_state\",\n    \"check_scalar\",\n    \"check_symmetric\",\n    \"column_or_1d\",\n    \"compute_class_weight\",\n    \"compute_sample_weight\",\n    \"deprecated\",\n    \"estimator_html_repr\",\n    \"gen_batches\",\n    \"gen_even_slices\",\n    \"get_tags\",\n    \"indexable\",\n    \"metadata_routing\",\n    \"murmurhash3_32\",\n    \"resample\",\n    \"safe_mask\",\n    \"safe_sqr\",\n    \"shuffle\",\n]\n"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_pprint.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "       analyzer=\"word\",\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.int64,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.ngram_range = ngram_range\n        self.vocabulary = vocabulary\n        self.binary = binary\n        self.dtype = dtype\n\n\nclass Pipeline(BaseEstimator):\n    def __init__(self, steps, memory=None):\n        self.steps = steps\n        self.memory = memory\n\n\nclass SVC(BaseEstimator):\n    def __init__(\n        self,\n        C=1.0,\n        kernel=\"rbf\",\n        degree=3,\n        gamma=\"auto_deprecated\",\n        coef0=0.0,\n        shrinking=True,\n        probability=False,\n        tol=1e-3,\n        cache_size=200,\n        class_weight=None,\n        verbose=False,\n        max_iter=-1,\n        decision_function_shape=\"ovr\",\n        random_state=None,\n    ):\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.tol = tol\n        self.C = C\n        self.shrinking = shrinking\n        self.probability = probability\n        self.cache_size = cache_size\n        self.class_weight = class_weight\n        self.verbose = verbose\n        self.max_iter = max_iter\n        self.decision_function_shape = decision_function_shape\n        self.random_state = random_state\n\n\nclass PCA(BaseEstimator):\n    def __init__(\n        self,\n        n_components=None,\n        copy=True,\n        whiten=False,\n        svd_solver=\"auto\",\n        tol=0.0,\n        iterated_power=\"auto\",\n        random_state"}], "retrieved_count": 10, "cost_time": 1.0389764308929443}
{"question": "What is the exact meaning of Scikit-learn's \"transformer\" concept?", "answer": null, "relative_code_list": null, "ground_truth": "In scikit-learn, a \"transformer\" is a specific type of estimator that modifies data through a standardized interface. The transformer concept is defined by the TransformerMixin class and follows these key principles:\n\n1. **Core Interface**:\n   - **fit(X, y=None)**: Learns parameters from the training data\n   - **transform(X)**: Applies the learned transformation to new data\n   - **fit_transform(X, y=None)**: Combines fit and transform in one step\n   - **get_feature_names_out()**: Returns output feature names (optional)\n\n2. **Transformation Types**:\n   - **Feature Scaling**: StandardScaler, MinMaxScaler, RobustScaler\n   - **Feature Encoding**: OneHotEncoder, OrdinalEncoder, LabelEncoder\n   - **Feature Selection**: SelectKBest, RFE, SelectFromModel\n   - **Dimensionality Reduction**: PCA, TruncatedSVD, FeatureAgglomeration\n   - **Feature Generation**: PolynomialFeatures, SplineTransformer\n   - **Data Cleaning**: SimpleImputer, MissingIndicator\n   - **Custom Transformations**: FunctionTransformer\n\n3. **Key Characteristics**:\n   - **Stateless Transformation**: Once fitted, applies the same transformation consistently\n   - **Reversible Operations**: Some transformers support inverse_transform()\n   - **Feature Preservation**: Maintains or modifies feature dimensionality\n   - **Sample Preservation**: Does not add or remove samples (rows)\n   - **Pipeline Compatibility**: Works seamlessly in Pipeline objects\n\n4. **TransformerMixin Implementation**:\n   - **Default fit_transform()**: Combines fit() and transform() calls\n   - **Metadata Routing**: Supports metadata routing for advanced use cases\n   - **Output Configuration**: Provides set_output() method for controlling output format\n   - **Feature Names**: Automatically wraps transform methods to handle feature names\n\n5. **Transformation Semantics**:\n   - **Inductive Learning**: Learns transformation parameters from training data\n   - **Consistent Application**: Applies the same transformation to all data\n   - **No Data Leakage**: Transformation parameters are learned only from training data\n   - **Cross-Validation Safe**: Can be safely used in cross-validation pipelines\n\n6. **Specialized Transformer Types**:\n   - **One-to-One Transformers**: OneToOneFeatureMixin for simple feature-wise transformations\n   - **Feature Name Preserving**: ClassNamePrefixFeaturesOutMixin for automatic feature naming\n   - **Target Transformers**: TransformedTargetRegressor for transforming target variables\n   - **Composition Transformers**: FeatureUnion, ColumnTransformer for combining transformations\n\n7. **Pipeline Integration**:\n   - **Sequential Processing**: Transformers can be chained in Pipeline objects\n   - **Parallel Processing**: FeatureUnion allows parallel transformation application\n   - **Conditional Application**: ColumnTransformer applies different transformations to different features\n   - **Memory Efficiency**: Optimized for large-scale data processing\n\n8. **Advanced Features**:\n   - **Partial Fitting**: Some transformers support incremental learning via partial_fit()\n   - **Sample Weights**: Many transformers support weighted fitting\n   - **Sparse Matrix Support**: Optimized for sparse matrix operations\n   - **Memory Management**: Efficient memory usage for large datasets\n   - **Parallel Processing**: Support for parallel transformation application\n\n9. **Validation and Error Handling**:\n   - **Input Validation**: Validates input data types and shapes\n   - **Feature Consistency**: Ensures consistent feature counts across fit/transform\n   - **Error Recovery**: Graceful handling of edge cases and errors\n   - **Warning System**: Issues warnings for potential issues or inefficiencies", "score": null, "retrieved_content": [{"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "_function_transformer.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ring :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X` has feature\n        names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    MaxAbsScaler : Scale each feature by its maximum absolute value.\n    StandardScaler : Standardize features by removing the mean and\n        scaling to unit variance.\n    LabelBinarizer : Binarize labels in a one-vs-all fashion.\n    MultiLabelBinarizer : Transform between iterable of iterables\n        and a multilabel format.\n\n    Notes\n    -----\n    If `func` returns an output with a `columns` attribute, then the columns is enforced\n    to be consistent with the output of `get_feature_names_out`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.preprocessing import FunctionTransformer\n    >>> transformer = FunctionTransformer(np.log1p)\n    >>> X = np.array([[0, 1], [2, 3]])\n    >>> transformer.transform(X)\n    array([[0.       , 0.6931],\n           [1.0986, 1.3862]])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"func\": [callable, None],\n        \"inverse_func\": [callable, None],\n        \"validate\": [\"boolean\"],\n        \"accept_sparse\": [\"boolean\"],\n        \"check_inverse\": [\"boolean\"],\n        \"feature_names_out\": [callable, StrOptions({\"one-to-one\"}), None],\n        \"kw_args\": [dict, None],\n        \"inv_kw_args\": [dict, None],\n    }\n\n    def __init__(\n        self,\n        func=None,\n        inverse_func=None,\n        *,\n        validate=False,\n        accept_sparse=False,\n        check_inverse=True,\n        feature_names_out=None,\n        kw_args=None,\n        inv_kw_args=None,\n    ):\n        self.func = func\n        self.inverse_func = inverse_func\n        self.validate = validate\n        self.accept_sparse = accept_sparse\n        self.check_inverse = check_inverse\n        self.feature_names_out = feature_names_out\n        self.kw_args = kw"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "_function_transformer.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport warnings\nfrom functools import partial\n\nimport numpy as np\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, _fit_context\nfrom sklearn.utils._param_validation import StrOptions\nfrom sklearn.utils._repr_html.estimator import _VisualBlock\nfrom sklearn.utils._set_output import _get_adapter_from_container, _get_output_config\nfrom sklearn.utils.metaestimators import available_if\nfrom sklearn.utils.validation import (\n    _allclose_dense_sparse,\n    _check_feature_names_in,\n    _get_feature_names,\n    _is_pandas_df,\n    _is_polars_df,\n    check_array,\n    validate_data,\n)\n\n\ndef _identity(X):\n    \"\"\"The identity function.\"\"\"\n    return X\n\n\nclass FunctionTransformer(TransformerMixin, BaseEstimator):\n    \"\"\"Constructs a transformer from an arbitrary callable.\n\n    A FunctionTransformer forwards its X (and optionally y) arguments to a\n    user-defined function or function object and returns the result of this\n    function. This is useful for stateless transformations such as taking the\n    log of frequencies, doing custom scaling, etc.\n\n    Note: If a lambda is used as the function, then the resulting\n    transformer will not be pickleable.\n\n    .. versionadded:: 0.17\n\n    Read more in the :ref:`User Guide <function_transformer>`.\n\n    Parameters\n    ----------\n    func : callable, default=None\n        The callable to use for the transformation. This will be passed\n        the same arguments as transform, with args and kwargs forwarded.\n        If func is None, then func will be the identity function.\n\n    inverse_func : callable, default=None\n        The callable to use for the inverse transformation. This will be\n        passed the same arguments as inverse transform, with args and\n        kwargs forwarded. If inverse_func is None, then inverse_func\n        will be the identity function.\n\n    validate : bool, default=False\n        Indicate that the input X array should be checked"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "_column_transformer.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/compose", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "        fitted_transformer, column). `fitted_transformer` can be an estimator,\n        or `'drop'`; `'passthrough'` is replaced with an equivalent\n        :class:`~sklearn.preprocessing.FunctionTransformer`. In case there were\n        no columns selected, this will be the unfitted transformer. If there\n        are remaining columns, the final element is a tuple of the form:\n        ('remainder', transformer, remaining_columns) corresponding to the\n        ``remainder`` parameter. If there are remaining columns, then\n        ``len(transformers_)==len(transformers)+1``, otherwise\n        ``len(transformers_)==len(transformers)``.\n\n        .. versionadded:: 1.7\n            The format of the remaining columns now attempts to match that of the other\n            transformers: if all columns were provided as column names (`str`), the\n            remaining columns are stored as column names; if all columns were provided\n            as mask arrays (`bool`), so are the remaining columns; in all other cases\n            the remaining columns are stored as indices (`int`).\n\n    named_transformers_ : :class:`~sklearn.utils.Bunch`\n        Read-only attribute to access any transformer by given name.\n        Keys are transformer names and values are the fitted transformer\n        objects.\n\n    sparse_output_ : bool\n        Boolean flag indicating whether the output of ``transform`` is a\n        sparse matrix or a dense numpy array, which depends on the output\n        of the individual transformers and the `sparse_threshold` keyword.\n\n    output_indices_ : dict\n        A dictionary from each transformer name to a slice, where the slice\n        corresponds to indices in the transformed output. This is useful to\n        inspect which transformer is responsible for which transformed\n        feature(s).\n\n        .. versionadded:: 1.0\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying transformers expose such an attribute w"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "plot_column_transformer.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/compose", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "categories,\n    remove=(\"footers\", \"quotes\"),\n    return_X_y=True,\n)\n\n##############################################################################\n# Each feature comprises meta information about that post, such as the subject,\n# and the body of the news post.\n\nprint(X_train[0])\n\n##############################################################################\n# Creating transformers\n# ---------------------\n#\n# First, we would like a transformer that extracts the subject and\n# body of each post. Since this is a stateless transformation (does not\n# require state information from training data), we can define a function that\n# performs the data transformation then use\n# :class:`~sklearn.preprocessing.FunctionTransformer` to create a scikit-learn\n# transformer.\n\n\ndef subject_body_extractor(posts):\n    # construct object dtype array with two columns\n    # first column = 'subject' and second column = 'body'\n    features = np.empty(shape=(len(posts), 2), dtype=object)\n    for i, text in enumerate(posts):\n        # temporary variable `_` stores '\\n\\n'\n        headers, _, body = text.partition(\"\\n\\n\")\n        # store body text in second column\n        features[i, 1] = body\n\n        prefix = \"Subject:\"\n        sub = \"\"\n        # save text after 'Subject:' in first column\n        for line in headers.split(\"\\n\"):\n            if line.startswith(prefix):\n                sub = line[len(prefix) :]\n                break\n        features[i, 0] = sub\n\n    return features\n\n\nsubject_body_transformer = FunctionTransformer(subject_body_extractor)\n\n##############################################################################\n# We will also create a transformer that extracts the\n# length of the text and the number of sentences.\n\n\ndef text_stats(posts):\n    return [{\"length\": len(text), \"num_sentences\": text.count(\".\")} for text in posts]\n\n\ntext_stats_transformer = FunctionTransformer(text_stats)\n\n##############################################################################\n# Classification"}, {"start_line": 55000, "end_line": 57000, "belongs_to": {"file_name": "_column_transformer.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/compose", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "f:`User Guide <make_column_transformer>`.\n\n    Parameters\n    ----------\n    *transformers : tuples\n        Tuples of the form (transformer, columns) specifying the\n        transformer objects to be applied to subsets of the data.\n\n        transformer : {'drop', 'passthrough'} or estimator\n            Estimator must support :term:`fit` and :term:`transform`.\n            Special-cased strings 'drop' and 'passthrough' are accepted as\n            well, to indicate to drop the columns or to pass them through\n            untransformed, respectively.\n        columns : str,  array-like of str, int, array-like of int, slice, \\\n                array-like of bool or callable\n            Indexes the data on its second axis. Integers are interpreted as\n            positional columns, while strings can reference DataFrame columns\n            by name. A scalar string or int should be used where\n            ``transformer`` expects X to be a 1d array-like (vector),\n            otherwise a 2d array will be passed to the transformer.\n            A callable is passed the input data `X` and can return any of the\n            above. To select multiple columns by name or dtype, you can use\n            :obj:`make_column_selector`.\n\n    remainder : {'drop', 'passthrough'} or estimator, default='drop'\n        By default, only the specified columns in `transformers` are\n        transformed and combined in the output, and the non-specified\n        columns are dropped. (default of ``'drop'``).\n        By specifying ``remainder='passthrough'``, all remaining columns that\n        were not specified in `transformers` will be automatically passed\n        through. This subset of columns is concatenated with the output of\n        the transformers.\n        By setting ``remainder`` to be an estimator, the remaining\n        non-specified columns will use the ``remainder`` estimator. The\n        estimator must support :term:`fit` and :term:`transform`.\n\n    sparse_threshold : float, default=0.3\n        I"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "_target.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/compose", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "TransformerMixin`. Cannot be set at the same time\n        as `func` and `inverse_func`. If `transformer is None` as well as\n        `func` and `inverse_func`, the transformer will be an identity\n        transformer. Note that the transformer will be cloned during fitting.\n        Also, the transformer is restricting `y` to be a numpy array.\n\n    func : function, default=None\n        Function to apply to `y` before passing to :meth:`fit`. Cannot be set\n        at the same time as `transformer`. If `func is None`, the function used will be\n        the identity function. If `func` is set, `inverse_func` also needs to be\n        provided. The function needs to return a 2-dimensional array.\n\n    inverse_func : function, default=None\n        Function to apply to the prediction of the regressor. Cannot be set at\n        the same time as `transformer`. The inverse function is used to return\n        predictions to the same space of the original training labels. If\n        `inverse_func` is set, `func` also needs to be provided. The inverse\n        function needs to return a 2-dimensional array.\n\n    check_inverse : bool, default=True\n        Whether to check that `transform` followed by `inverse_transform`\n        or `func` followed by `inverse_func` leads to the original targets.\n\n    Attributes\n    ----------\n    regressor_ : object\n        Fitted regressor.\n\n    transformer_ : object\n        Transformer used in :meth:`fit` and :meth:`predict`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying regressor exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    sklearn.preprocessing.FunctionTransformer : Construct a transformer from an\n        arbitrary call"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "mposition):\n    \"\"\"\n    A sequence of data transformers with an optional final predictor.\n\n    `Pipeline` allows you to sequentially apply a list of transformers to\n    preprocess the data and, if desired, conclude the sequence with a final\n    :term:`predictor` for predictive modeling.\n\n    Intermediate steps of the pipeline must be transformers, that is, they\n    must implement `fit` and `transform` methods.\n    The final :term:`estimator` only needs to implement `fit`.\n    The transformers in the pipeline can be cached using ``memory`` argument.\n\n    The purpose of the pipeline is to assemble several steps that can be\n    cross-validated together while setting different parameters. For this, it\n    enables setting parameters of the various steps using their names and the\n    parameter name separated by a `'__'`, as in the example below. A step's\n    estimator may be replaced entirely by setting the parameter with its name\n    to another estimator, or a transformer removed by setting it to\n    `'passthrough'` or `None`.\n\n    For an example use case of `Pipeline` combined with\n    :class:`~sklearn.model_selection.GridSearchCV`, refer to\n    :ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`. The\n    example :ref:`sphx_glr_auto_examples_compose_plot_digits_pipe.py` shows how\n    to grid search on a pipeline using `'__'` as a separator in the parameter names.\n\n    Read more in the :ref:`User Guide <pipeline>`.\n\n    .. versionadded:: 0.5\n\n    Parameters\n    ----------\n    steps : list of tuples\n        List of (name of step, estimator) tuples that are to be chained in\n        sequential order. To be compatible with the scikit-learn API, all steps\n        must define `fit`. All non-last steps must also define `transform`. See\n        :ref:`Combining Estimators <combining_estimators>` for more details.\n\n    transform_input : list of str, default=None\n        The names of the :term:`metadata` parameters that should be transformed by the\n        pipeline befo"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "_column_transformer.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/compose", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "added at the right to the output of the transformers.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.compose import ColumnTransformer\n    >>> from sklearn.preprocessing import Normalizer\n    >>> ct = ColumnTransformer(\n    ...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n    ...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n    >>> X = np.array([[0., 1., 2., 2.],\n    ...               [1., 1., 0., 1.]])\n    >>> # Normalizer scales each row of X to unit norm. A separate scaling\n    >>> # is applied for the two first and two last elements of each\n    >>> # row independently.\n    >>> ct.fit_transform(X)\n    array([[0. , 1. , 0.5, 0.5],\n           [0.5, 0.5, 0. , 1. ]])\n\n    :class:`ColumnTransformer` can be configured with a transformer that requires\n    a 1d array by setting the column to a string:\n\n    >>> from sklearn.feature_extraction.text import CountVectorizer\n    >>> from sklearn.preprocessing import MinMaxScaler\n    >>> import pandas as pd   # doctest: +SKIP\n    >>> X = pd.DataFrame({\n    ...     \"documents\": [\"First item\", \"second one here\", \"Is this the last?\"],\n    ...     \"width\": [3, 4, 5],\n    ... })  # doctest: +SKIP\n    >>> # \"documents\" is a string which configures ColumnTransformer to\n    >>> # pass the documents column as a 1d array to the CountVectorizer\n    >>> ct = ColumnTransformer(\n    ...     [(\"text_preprocess\", CountVectorizer(), \"documents\"),\n    ...      (\"num_preprocess\", MinMaxScaler(), [\"width\"])])\n    >>> X_trans = ct.fit_transform(X)  # doctest: +SKIP\n\n    For a more detailed example of usage, see\n    :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py`.\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"transformers\": [list, Hidden(tuple)],\n        \"remainder\": [\n            StrOptions({\"drop\", \"passthrough\"}),\n            HasMethods([\"fit\", \"transform\"]),\n            HasMethods([\"fit_transform\", \"transform\"]),\n        ],\n        \"sparse_threshold\": [Interval(Real, 0, 1"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_column_transformer.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/compose/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rn.utils.fixes import CSR_CONTAINERS, parse_version\n\n\nclass Trans(TransformerMixin, BaseEstimator):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        # 1D Series -> 2D DataFrame\n        if hasattr(X, \"to_frame\"):\n            return X.to_frame()\n        # 1D array -> 2D array\n        if getattr(X, \"ndim\", 2) == 1:\n            return np.atleast_2d(X).T\n        return X\n\n\nclass DoubleTrans(BaseEstimator):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return 2 * X\n\n\nclass SparseMatrixTrans(BaseEstimator):\n    def __init__(self, csr_container):\n        self.csr_container = csr_container\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        n_samples = len(X)\n        return self.csr_container(sparse.eye(n_samples, n_samples))\n\n\nclass TransNo2D(BaseEstimator):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X\n\n\nclass TransRaise(BaseEstimator):\n    def fit(self, X, y=None):\n        raise ValueError(\"specific message\")\n\n    def transform(self, X, y=None):\n        raise ValueError(\"specific message\")\n\n\ndef test_column_transformer():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    X_res_first1D = np.array([0, 1, 2])\n    X_res_second1D = np.array([2, 4, 6])\n    X_res_first = X_res_first1D.reshape(-1, 1)\n    X_res_both = X_array\n\n    cases = [\n        # single column 1D / 2D\n        (0, X_res_first),\n        ([0], X_res_first),\n        # list-like\n        ([0, 1], X_res_both),\n        (np.array([0, 1]), X_res_both),\n        # slice\n        (slice(0, 1), X_res_first),\n        (slice(0, 2), X_res_both),\n        # boolean mask\n        (np.array([True, False]), X_res_first),\n        ([True, False], X_res_first),\n        (np.array([True, True]), X_res_both),\n        ([True, True], X_res_both),\n    ]\n\n    for selection, res in cases:\n        ct = ColumnTransformer([(\"trans\", Trans(), selection)], rem"}, {"start_line": 28000, "end_line": 30000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " : ndarray of shape (n_rows, n_cols)\n            The submatrix corresponding to bicluster `i`.\n\n        Notes\n        -----\n        Works with sparse matrices. Only works if ``rows_`` and\n        ``columns_`` attributes exist.\n        \"\"\"\n\n        data = check_array(data, accept_sparse=\"csr\")\n        row_ind, col_ind = self.get_indices(i)\n        return data[row_ind[:, np.newaxis], col_ind]\n\n\nclass TransformerMixin(_SetOutputMixin):\n    \"\"\"Mixin class for all transformers in scikit-learn.\n\n    This mixin defines the following functionality:\n\n    - a `fit_transform` method that delegates to `fit` and `transform`;\n    - a `set_output` method to output `X` as a specific container type.\n\n    If :term:`get_feature_names_out` is defined, then :class:`BaseEstimator` will\n    automatically wrap `transform` and `fit_transform` to follow the `set_output`\n    API. See the :ref:`developer_api_set_output` for details.\n\n    :class:`OneToOneFeatureMixin` and\n    :class:`ClassNamePrefixFeaturesOutMixin` are helpful mixins for\n    defining :term:`get_feature_names_out`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, TransformerMixin\n    >>> class MyTransformer(TransformerMixin, BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         return self\n    ...     def transform(self, X):\n    ...         return np.full(shape=len(X), fill_value=self.param)\n    >>> transformer = MyTransformer()\n    >>> X = [[1, 2], [2, 3], [3, 4]]\n    >>> transformer.fit_transform(X)\n    array([1, 1, 1])\n    \"\"\"\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.transformer_tags = TransformerTags()\n        return tags\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transf"}], "retrieved_count": 10, "cost_time": 1.0374600887298584}
{"question": "What are the core components of Scikit-learn's estimator API?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn's estimator API consists of several core components:\n\n1. **BaseEstimator**: The fundamental base class that all estimators inherit from. It provides default implementations for:\n   - Parameter management (get_params, set_params)\n   - Textual and HTML representation\n   - Estimator serialization\n   - Parameter validation\n   - Data validation\n   - Feature names validation\n\n2. **Mixin Classes**: Specialized mixins that define specific estimator types:\n   - **ClassifierMixin**: For classification estimators, provides score method using accuracy_score\n   - **RegressorMixin**: For regression estimators, provides score method using r2_score\n   - **TransformerMixin**: For data transformers, provides fit_transform method\n   - **ClusterMixin**: For clustering algorithms, provides fit_predict method\n   - **BiclusterMixin**: For biclustering algorithms\n\n3. **Core Methods**: All estimators must implement:\n   - **fit(X, y=None)**: Learn from training data\n   - **get_params()**: Return estimator parameters\n   - **set_params()**: Set estimator parameters\n\n4. **Specialized Methods**: Depending on estimator type:\n   - **predict(X)**: For predictors (classifiers, regressors)\n   - **transform(X)**: For transformers\n   - **score(X, y)**: For models that can evaluate goodness of fit\n   - **fit_transform(X, y)**: For transformers (combines fit and transform)\n   - **fit_predict(X, y)**: For clustering (combines fit and predict)\n\n5. **Estimator Tags**: System for programmatic inspection of capabilities including:\n   - Supported input types (sparse matrices, etc.)\n   - Supported output types\n   - Supported methods\n   - Performance characteristics\n\n6. **Validation System**: Built-in validation for:\n   - Input data consistency\n   - Parameter validation\n   - Feature names validation\n   - Fitted state checking", "score": null, "retrieved_content": [{"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "aram2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n\n    # _sklearn_output_config is used by `set_output` to configure the output\n    # container of an estimator.\n    if hasattr(estimator, \"_sklearn_output_config\"):\n        new_object._sklearn_output_config = copy.deepcopy(\n            estimator._sklearn_output_config\n        )\n    return new_object\n\n\nclass BaseEstimator(ReprHTMLMixin, _HTMLDocumentationLinkMixin, _MetadataRequester):\n    \"\"\"Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])\n    \"\"\"\n\n    _html_"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])\n    \"\"\"\n\n    _html_repr = estimator_html_repr\n\n    @classmethod\n    def _get_param_names(cls):\n        \"\"\"Get parameter names for the estimator\"\"\"\n        # fetch the constructor or the original constructor before\n        # deprecation wrapping if any\n        init = getattr(cls.__init__, \"deprecated_original\", cls.__init__)\n        if init is object.__init__:\n            # No explicit constructor to introspect\n            return []\n\n        # introspect the constructor arguments to find the model parameters\n        # to represent\n        init_signature = inspect.signature(init)\n        # Consider the constructor parameters excluding 'self'\n        parameters = [\n            p\n            for p in init_signature.parameters.values()\n            if p.name != \"self\" and p.kind != p.VAR_KEYWORD\n        ]\n        for p in parameters:\n            if p.kind == p.VAR_POSITIONAL:\n                raise RuntimeError(\n                    \"scikit-learn estimators should always \"\n                    \"specify their para"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_pprint.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "   self.intercept_scaling = intercept_scaling\n        self.class_weight = class_weight\n        self.random_state = random_state\n        self.solver = solver\n        self.max_iter = max_iter\n        self.multi_class = multi_class\n        self.verbose = verbose\n        self.warm_start = warm_start\n        self.n_jobs = n_jobs\n        self.l1_ratio = l1_ratio\n\n    def fit(self, X, y):\n        return self\n\n\nclass StandardScaler(TransformerMixin, BaseEstimator):\n    def __init__(self, copy=True, with_mean=True, with_std=True):\n        self.with_mean = with_mean\n        self.with_std = with_std\n        self.copy = copy\n\n    def transform(self, X, copy=None):\n        return self\n\n\nclass RFE(BaseEstimator):\n    def __init__(self, estimator, n_features_to_select=None, step=1, verbose=0):\n        self.estimator = estimator\n        self.n_features_to_select = n_features_to_select\n        self.step = step\n        self.verbose = verbose\n\n\nclass GridSearchCV(BaseEstimator):\n    def __init__(\n        self,\n        estimator,\n        param_grid,\n        scoring=None,\n        n_jobs=None,\n        iid=\"warn\",\n        refit=True,\n        cv=\"warn\",\n        verbose=0,\n        pre_dispatch=\"2*n_jobs\",\n        error_score=\"raise-deprecating\",\n        return_train_score=False,\n    ):\n        self.estimator = estimator\n        self.param_grid = param_grid\n        self.scoring = scoring\n        self.n_jobs = n_jobs\n        self.iid = iid\n        self.refit = refit\n        self.cv = cv\n        self.verbose = verbose\n        self.pre_dispatch = pre_dispatch\n        self.error_score = error_score\n        self.return_train_score = return_train_score\n\n\nclass CountVectorizer(BaseEstimator):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        stop_words=None,\n        token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n        ngram_range=(1, 1),\n "}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n estimator instead of a class.\"\n                )\n            else:\n                raise TypeError(\n                    \"Cannot clone object '%s' (type %s): \"\n                    \"it does not seem to be a scikit-learn \"\n                    \"estimator as it does not implement a \"\n                    \"'get_params' method.\" % (repr(estimator), type(estimator))\n                )\n\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        new_object_params[name] = clone(param, safe=False)\n\n    new_object = klass(**new_object_params)\n    try:\n        new_object._metadata_request = copy.deepcopy(estimator._metadata_request)\n    except AttributeError:\n        pass\n\n    params_set = new_object.get_params(deep=False)\n\n    # quick sanity check of the parameters of the clone\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is not param2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n\n    # _sklearn_output_config is used by `set_output` to configure the output\n    # container of an estimator.\n    if hasattr(estimator, \"_sklearn_output_config\"):\n        new_object._sklearn_output_config = copy.deepcopy(\n            estimator._sklearn_output_config\n        )\n    return new_object\n\n\nclass BaseEstimator(ReprHTMLMixin, _HTMLDocumentationLinkMixin, _MetadataRequester):\n    \"\"\"Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "sklearn_is_fitted.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/developing_estimators", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"\n========================================\n`__sklearn_is_fitted__` as Developer API\n========================================\n\nThe `__sklearn_is_fitted__` method is a convention used in scikit-learn for\nchecking whether an estimator object has been fitted or not. This method is\ntypically implemented in custom estimator classes that are built on top of\nscikit-learn's base classes like `BaseEstimator` or its subclasses.\n\nDevelopers should use :func:`~sklearn.utils.validation.check_is_fitted`\nat the beginning of all methods except `fit`. If they need to customize or\nspeed-up the check, they can implement the `__sklearn_is_fitted__` method as\nshown below.\n\nIn this example the custom estimator showcases the usage of the\n`__sklearn_is_fitted__` method and the `check_is_fitted` utility function\nas developer APIs. The `__sklearn_is_fitted__` method checks fitted status\nby verifying the presence of the `_is_fitted` attribute.\n\"\"\"\n\n# %%\n# An example custom estimator implementing a simple classifier\n# ------------------------------------------------------------\n# This code snippet defines a custom estimator class called `CustomEstimator`\n# that extends both the `BaseEstimator` and `ClassifierMixin` classes from\n# scikit-learn and showcases the usage of the `__sklearn_is_fitted__` method\n# and the `check_is_fitted` utility function.\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_is_fitted\n\n\nclass CustomEstimator(BaseEstimator, ClassifierMixin):\n    def __init__(self, parameter=1):\n        self.parameter = parameter\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the estimator to the training data.\n        \"\"\"\n        self.classes_ = sorted(set(y))\n        # Custom attribute to track if the estimator is fitted\n        self._is_fitted = True\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Perform Predictions\n\n        If the est"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "test_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "or)\n    clf2 = clone(clf)\n\n    assert clf.empty is clf2.empty\n\n\ndef test_clone_class_rather_than_instance():\n    # Check that clone raises expected error message when\n    # cloning class rather than instance\n    msg = \"You should provide an instance of scikit-learn estimator\"\n    with pytest.raises(TypeError, match=msg):\n        clone(MyEstimator)\n\n\ndef test_repr():\n    # Smoke test the repr of the base estimator.\n    my_estimator = MyEstimator()\n    repr(my_estimator)\n    test = T(K(), K())\n    assert repr(test) == \"T(a=K(), b=K())\"\n\n    some_est = T(a=[\"long_params\"] * 1000)\n    assert len(repr(some_est)) == 485\n\n\ndef test_str():\n    # Smoke test the str of the base estimator\n    my_estimator = MyEstimator()\n    str(my_estimator)\n\n\ndef test_get_params():\n    test = T(K(), K)\n\n    assert \"a__d\" in test.get_params(deep=True)\n    assert \"a__d\" not in test.get_params(deep=False)\n\n    test.set_params(a__d=2)\n    assert test.a.d == 2\n\n    with pytest.raises(ValueError):\n        test.set_params(a__a=2)\n\n\n# TODO(1.8): Remove this test when the deprecation is removed\ndef test_is_estimator_type_class():\n    with pytest.warns(FutureWarning, match=\"passing a class to.*is deprecated\"):\n        assert is_classifier(SVC)\n\n    with pytest.warns(FutureWarning, match=\"passing a class to.*is deprecated\"):\n        assert is_regressor(SVR)\n\n    with pytest.warns(FutureWarning, match=\"passing a class to.*is deprecated\"):\n        assert is_clusterer(KMeans)\n\n    with pytest.warns(FutureWarning, match=\"passing a class to.*is deprecated\"):\n        assert is_outlier_detector(IsolationForest)\n\n\n@pytest.mark.parametrize(\n    \"estimator, expected_result\",\n    [\n        (SVC(), True),\n        (GridSearchCV(SVC(), {\"C\": [0.1, 1]}), True),\n        (Pipeline([(\"svc\", SVC())]), True),\n        (Pipeline([(\"svc_cv\", GridSearchCV(SVC(), {\"C\": [0.1, 1]}))]), True),\n        (SVR(), False),\n        (GridSearchCV(SVR(), {\"C\": [0.1, 1]}), False),\n        (Pipeline([(\"svr\", SVR())]), False),\n        (Pipel"}, {"start_line": 43000, "end_line": 45000, "belongs_to": {"file_name": "_testing.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "        return {\"param\": self.param}\n\n    def set_params(self, **params):\n        for key, value in params.items():\n            setattr(self, key, value)\n        return self\n\n    def fit(self, X, y):\n        X, y = check_X_y(X, y)\n        self.is_fitted_ = True\n        self._mean = np.mean(y)\n        return self\n\n    def predict(self, X):\n        check_is_fitted(self)\n        X = check_array(X)\n        return np.ones(shape=(X.shape[0],)) * self._mean\n\n    def score(self, X, y):\n        from sklearn.metrics import r2_score\n\n        return r2_score(y, self.predict(X))\n\n    def __sklearn_tags__(self):\n        return Tags(\n            estimator_type=\"regressor\",\n            classifier_tags=None,\n            regressor_tags=RegressorTags(),\n            transformer_tags=None,\n            target_tags=TargetTags(required=True),\n        )\n\n\nclass MinimalTransformer:\n    \"\"\"Minimal transformer implementation without inheriting from\n    BaseEstimator.\n\n    This estimator should be tested with:\n\n    * `check_estimator` in `test_estimator_checks.py`;\n    * within a `Pipeline` in `test_pipeline.py`;\n    * within a `SearchCV` in `test_search.py`.\n    \"\"\"\n\n    def __init__(self, param=None):\n        self.param = param\n\n    def get_params(self, deep=True):\n        return {\"param\": self.param}\n\n    def set_params(self, **params):\n        for key, value in params.items():\n            setattr(self, key, value)\n        return self\n\n    def fit(self, X, y=None):\n        check_array(X)\n        self.is_fitted_ = True\n        return self\n\n    def transform(self, X, y=None):\n        check_is_fitted(self)\n        X = check_array(X)\n        return X\n\n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X, y)\n\n    def __sklearn_tags__(self):\n        return Tags(\n            estimator_type=\"transformer\",\n            classifier_tags=None,\n            regressor_tags=None,\n            transformer_tags=TransformerTags(),\n            target_tags=TargetTags(required=False),\n "}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "or instance\n            Estimator instance.\n        \"\"\"\n        if not params:\n            # Simple optimization to gain speed (inspect is slow)\n            return self\n        valid_params = self.get_params(deep=True)\n\n        nested_params = defaultdict(dict)  # grouped by prefix\n        for key, value in params.items():\n            key, delim, sub_key = key.partition(\"__\")\n            if key not in valid_params:\n                local_valid_params = self._get_param_names()\n                raise ValueError(\n                    f\"Invalid parameter {key!r} for estimator {self}. \"\n                    f\"Valid parameters are: {local_valid_params!r}.\"\n                )\n\n            if delim:\n                nested_params[key][sub_key] = value\n            else:\n                setattr(self, key, value)\n                valid_params[key] = value\n\n        for key, sub_params in nested_params.items():\n            valid_params[key].set_params(**sub_params)\n\n        return self\n\n    def __sklearn_clone__(self):\n        return _clone_parametrized(self)\n\n    def __repr__(self, N_CHAR_MAX=700):\n        # N_CHAR_MAX is the (approximate) maximum number of non-blank\n        # characters to render. We pass it as an optional parameter to ease\n        # the tests.\n\n        from sklearn.utils._pprint import _EstimatorPrettyPrinter\n\n        N_MAX_ELEMENTS_TO_SHOW = 30  # number of elements to show in sequences\n\n        # use ellipsis for sequences with a lot of elements\n        pp = _EstimatorPrettyPrinter(\n            compact=True,\n            indent=1,\n            indent_at_name=True,\n            n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW,\n        )\n\n        repr_ = pp.pformat(self)\n\n        # Use bruteforce ellipsis when there are a lot of non-blank characters\n        n_nonblank = len(\"\".join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2  # apprx number of chars to keep on both ends\n            regex = r\"^(\\s*\\S){%d}\" % lim\n            # The r"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "metadata_routing_common.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "        self, sample_weight=sample_weight, metadata=metadata\n        )\n        _check_partial_fit_first_call(self, classes)\n        return self\n\n    def fit(self, X, y, sample_weight=\"default\", metadata=\"default\"):\n        if self.registry is not None:\n            self.registry.append(self)\n\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n\n        self.classes_ = np.unique(y)\n        self.coef_ = np.ones_like(X)\n        return self\n\n    def predict(self, X, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n        y_score = np.empty(shape=(len(X),), dtype=\"int8\")\n        y_score[len(X) // 2 :] = 0\n        y_score[: len(X) // 2] = 1\n        return y_score\n\n    def predict_proba(self, X, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n        y_proba = np.empty(shape=(len(X), len(self.classes_)), dtype=np.float32)\n        # each row sums up to 1.0:\n        y_proba[:] = np.random.dirichlet(alpha=np.ones(len(self.classes_)), size=len(X))\n        return y_proba\n\n    def predict_log_proba(self, X, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n        return self.predict_proba(X)\n\n    def decision_function(self, X, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n        y_score = np.empty(shape=(len(X),))\n        y_score[len(X) // 2 :] = 0\n        y_score[: len(X) // 2] = 1\n        return y_score\n\n    def score(self, X, y, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ixes import CSR_CONTAINERS, SPARRAY_PRESENT\nfrom sklearn.utils.metaestimators import available_if\nfrom sklearn.utils.multiclass import type_of_target\nfrom sklearn.utils.validation import (\n    check_array,\n    check_is_fitted,\n    check_X_y,\n    validate_data,\n)\n\n\nclass CorrectNotFittedError(ValueError):\n    \"\"\"Exception class to raise if estimator is used before fitting.\n\n    Like NotFittedError, it inherits from ValueError, but not from\n    AttributeError. Used for testing only.\n    \"\"\"\n\n\nclass BaseBadClassifier(ClassifierMixin, BaseEstimator):\n    def fit(self, X, y):\n        return self\n\n    def predict(self, X):\n        return np.ones(X.shape[0])\n\n\nclass ChangesDict(BaseEstimator):\n    def __init__(self, key=0):\n        self.key = key\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n    def predict(self, X):\n        X = check_array(X)\n        self.key = 1000\n        return np.ones(X.shape[0])\n\n\nclass SetsWrongAttribute(BaseEstimator):\n    def __init__(self, acceptable_key=0):\n        self.acceptable_key = acceptable_key\n\n    def fit(self, X, y=None):\n        self.wrong_attribute = 0\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ChangesWrongAttribute(BaseEstimator):\n    def __init__(self, wrong_attribute=0):\n        self.wrong_attribute = wrong_attribute\n\n    def fit(self, X, y=None):\n        self.wrong_attribute = 1\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ChangesUnderscoreAttribute(BaseEstimator):\n    def fit(self, X, y=None):\n        self._good_attribute = 1\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass RaisesErrorInSetParams(BaseEstimator):\n    def __init__(self, p=0):\n        self.p = p\n\n    def set_params(self, **kwargs):\n        if \"p\" in kwargs:\n            p = kwargs.pop(\"p\")\n            if p < 0:\n                raise ValueError(\"p can't be less than 0\")\n            self.p = p\n        return super().set_params(**kwargs)\n\n    def fit(sel"}], "retrieved_count": 10, "cost_time": 1.0543136596679688}
{"question": "What is Scikit-learn's strategy for handling missing values?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn provides a comprehensive strategy for handling missing values through its impute module:\n\n1. **Univariate Imputation (SimpleImputer)**:\n   - **Mean Imputation**: Replaces missing values with the mean of each feature\n   - **Median Imputation**: Replaces missing values with the median of each feature\n   - **Most Frequent Imputation**: Replaces missing values with the most frequent value\n   - **Constant Imputation**: Replaces missing values with a specified constant value\n   - **Custom Function**: Uses a callable function for custom imputation strategies\n   - **Sparse Matrix Support**: Handles sparse matrices with appropriate missing value markers\n   - **Categorical Data Support**: Works with string values and pandas categoricals\n\n2. **Multivariate Imputation (IterativeImputer)**:\n   - **Model-Based Imputation**: Uses other features to predict missing values\n   - **Round-Robin Strategy**: Iteratively imputes each feature using all other features\n   - **Multiple Estimators**: Supports various estimators (default: BayesianRidge)\n   - **Posterior Sampling**: Can sample from predictive posterior for multiple imputations\n   - **Convergence Control**: Uses tolerance and max_iter parameters for convergence\n   - **Feature Selection**: Can use n_nearest_features for computational efficiency\n\n3. **K-Nearest Neighbors Imputation (KNNImputer)**:\n   - **Distance-Based Imputation**: Uses k-nearest neighbors to estimate missing values\n   - **Weighted Averaging**: Supports uniform and distance-weighted averaging\n   - **Custom Distance Metrics**: Supports custom distance functions\n   - **Handles Multiple Missing Features**: Different neighbors for different features\n   - **Fallback Strategies**: Uses training set average when insufficient neighbors\n\n4. **Missing Value Indicators**:\n   - **MissingIndicator**: Creates binary indicators for missing values\n   - **Feature Addition**: Can add missing indicators to preserve missingness information\n   - **Selective Features**: Can indicate missingness for specific features only\n   - **Pipeline Integration**: Works seamlessly with imputers and other transformers\n\n5. **Missing Value Representations**:\n   - **NaN Support**: Primary missing value marker (np.nan)\n   - **Custom Markers**: Supports custom missing value representations (e.g., -1)\n   - **Pandas Integration**: Handles pd.NA and nullable integer dtypes\n   - **Type Preservation**: Maintains data types where possible\n\n6. **Pipeline Integration**:\n   - **FeatureUnion**: Combines imputation with missing indicators\n   - **ColumnTransformer**: Applies different imputation strategies to different features\n   - **Pipeline Compatibility**: Works seamlessly in preprocessing pipelines\n   - **Cross-Validation Safe**: Prevents data leakage in cross-validation\n\n7. **Advanced Features**:\n   - **Sample Weights**: Supports weighted imputation for biased sampling\n   - **Empty Feature Handling**: Can keep or drop features with all missing values\n   - **Incremental Learning**: Supports partial_fit for large datasets\n   - **Memory Efficiency**: Optimized for large-scale data processing\n\n8. **Estimator Compatibility**:\n   - **Native NaN Support**: Some estimators handle NaN values directly\n   - **Preprocessing Required**: Most estimators require complete data\n   - **Validation Integration**: Works with scikit-learn's validation system\n   - **Performance Optimization**: Efficient implementations for various data types", "score": null, "retrieved_content": [{"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "_knn.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/impute", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " If a feature has no\n        missing values at fit/train time, the feature won't appear on the\n        missing indicator even if there are missing values at transform/test\n        time.\n\n    keep_empty_features : bool, default=False\n        If True, features that consist exclusively of missing values when\n        `fit` is called are returned in results when `transform` is called.\n        The imputed value is always `0`.\n\n        .. versionadded:: 1.2\n\n    Attributes\n    ----------\n    indicator_ : :class:`~sklearn.impute.MissingIndicator`\n        Indicator used to add binary indicators for missing values.\n        ``None`` if add_indicator is False.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    SimpleImputer : Univariate imputer for completing missing values\n        with simple strategies.\n    IterativeImputer : Multivariate imputer that estimates values to impute for\n        each feature with missing values from all the others.\n\n    References\n    ----------\n    * `Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor\n      Hastie, Robert Tibshirani, David Botstein and Russ B. Altman, Missing\n      value estimation methods for DNA microarrays, BIOINFORMATICS Vol. 17\n      no. 6, 2001 Pages 520-525.\n      <https://academic.oup.com/bioinformatics/article/17/6/520/272365>`_\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.impute import KNNImputer\n    >>> X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n    >>> imputer = KNNImputer(n_neighbors=2)\n    >>> imputer.fit_transform(X)\n    array([[1. , 2. , 4. ],\n           [3. , 4. , 3. ],\n           [5.5, 6. , 5. ],\n           [8. , 8. , 7. ]])\n\n    For a more detailed exa"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/impute", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "   For a more detailed example see\n    :ref:`sphx_glr_auto_examples_impute_plot_missing_values.py`.\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **_BaseImputer._parameter_constraints,\n        \"strategy\": [\n            StrOptions({\"mean\", \"median\", \"most_frequent\", \"constant\"}),\n            callable,\n        ],\n        \"fill_value\": \"no_validation\",  # any object is valid\n        \"copy\": [\"boolean\"],\n    }\n\n    def __init__(\n        self,\n        *,\n        missing_values=np.nan,\n        strategy=\"mean\",\n        fill_value=None,\n        copy=True,\n        add_indicator=False,\n        keep_empty_features=False,\n    ):\n        super().__init__(\n            missing_values=missing_values,\n            add_indicator=add_indicator,\n            keep_empty_features=keep_empty_features,\n        )\n        self.strategy = strategy\n        self.fill_value = fill_value\n        self.copy = copy\n\n    def _validate_input(self, X, in_fit):\n        if self.strategy in (\"most_frequent\", \"constant\"):\n            # If input is a list of strings, dtype = object.\n            # Otherwise ValueError is raised in SimpleImputer\n            # with strategy='most_frequent' or 'constant'\n            # because the list is converted to Unicode numpy array\n            if isinstance(X, list) and any(\n                isinstance(elem, str) for row in X for elem in row\n            ):\n                dtype = object\n            else:\n                dtype = None\n        else:\n            dtype = FLOAT_DTYPES\n\n        if not in_fit and self._fit_dtype.kind == \"O\":\n            # Use object dtype if fitted on object dtypes\n            dtype = self._fit_dtype\n\n        if is_pandas_na(self.missing_values) or is_scalar_nan(self.missing_values):\n            ensure_all_finite = \"allow-nan\"\n        else:\n            ensure_all_finite = True\n\n        try:\n            X = validate_data(\n                self,\n                X,\n                reset=in_fit,\n                accept_sparse=\"csc\",\n               "}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "test_gradient_boosting.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  # feature to split on during training, the learned decision trees should be\n    # strictly equivalent (learn a sequence of splits that encode the same\n    # decision function).\n    #\n    # The MinMaxImputer transformer is meant to be a toy implementation of the\n    # \"Missing In Attributes\" (MIA) missing value handling for decision trees\n    # https://www.sciencedirect.com/science/article/abs/pii/S0167865508000305\n    # The implementation of MIA as an imputation transformer was suggested by\n    # \"Remark 3\" in :arxiv:'<1902.06931>`\n\n    class MinMaxImputer(TransformerMixin, BaseEstimator):\n        def fit(self, X, y=None):\n            mm = MinMaxScaler().fit(X)\n            self.data_min_ = mm.data_min_\n            self.data_max_ = mm.data_max_\n            return self\n\n        def transform(self, X):\n            X_min, X_max = X.copy(), X.copy()\n\n            for feature_idx in range(X.shape[1]):\n                nan_mask = np.isnan(X[:, feature_idx])\n                X_min[nan_mask, feature_idx] = self.data_min_[feature_idx] - 1\n                X_max[nan_mask, feature_idx] = self.data_max_[feature_idx] + 1\n\n            return np.concatenate([X_min, X_max], axis=1)\n\n    def make_missing_value_data(n_samples=int(1e4), seed=0):\n        rng = np.random.RandomState(seed)\n        X, y = make_regression(n_samples=n_samples, n_features=4, random_state=rng)\n\n        # Pre-bin the data to ensure a deterministic handling by the 2\n        # strategies and also make it easier to insert np.nan in a structured\n        # way:\n        X = KBinsDiscretizer(\n            n_bins=42, encode=\"ordinal\", quantile_method=\"averaged_inverted_cdf\"\n        ).fit_transform(X)\n\n        # First feature has missing values completely at random:\n        rnd_mask = rng.rand(X.shape[0]) > 0.9\n        X[rnd_mask, 0] = np.nan\n\n        # Second and third features have missing values for extreme values\n        # (censoring missingness):\n        low_mask = X[:, 1] == 0\n        X[low_mask, 1] = np.nan\n\n       "}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "plot_release_highlights_0_22_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "l not be\n    # recomputed.\n    estimator.set_params(isomap__n_neighbors=5)\n    estimator.fit(X)\n\n# %%\n# KNN Based Imputation\n# ------------------------------------\n# We now support imputation for completing missing values using k-Nearest\n# Neighbors.\n#\n# Each sample's missing values are imputed using the mean value from\n# ``n_neighbors`` nearest neighbors found in the training set. Two samples are\n# close if the features that neither is missing are close.\n# By default, a euclidean distance metric\n# that supports missing values,\n# :func:`~sklearn.metrics.pairwise.nan_euclidean_distances`, is used to find the nearest\n# neighbors.\n#\n# Read more in the :ref:`User Guide <knnimpute>`.\n\nfrom sklearn.impute import KNNImputer\n\nX = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\nimputer = KNNImputer(n_neighbors=2)\nprint(imputer.fit_transform(X))\n\n# %%\n# Tree pruning\n# ------------\n#\n# It is now possible to prune most tree-based estimators once the trees are\n# built. The pruning is based on minimal cost-complexity. Read more in the\n# :ref:`User Guide <minimal_cost_complexity_pruning>` for details.\n\nX, y = make_classification(random_state=0)\n\nrf = RandomForestClassifier(random_state=0, ccp_alpha=0).fit(X, y)\nprint(\n    \"Average number of nodes without pruning {:.1f}\".format(\n        np.mean([e.tree_.node_count for e in rf.estimators_])\n    )\n)\n\nrf = RandomForestClassifier(random_state=0, ccp_alpha=0.05).fit(X, y)\nprint(\n    \"Average number of nodes with pruning {:.1f}\".format(\n        np.mean([e.tree_.node_count for e in rf.estimators_])\n    )\n)\n\n# %%\n# Retrieve dataframes from OpenML\n# -------------------------------\n# :func:`datasets.fetch_openml` can now return pandas dataframe and thus\n# properly handle datasets with heterogeneous data:\n\nfrom sklearn.datasets import fetch_openml\n\ntitanic = fetch_openml(\"titanic\", version=1, as_frame=True, parser=\"pandas\")\nprint(titanic.data.head()[[\"pclass\", \"embarked\"]])\n\n# %%\n# Checking scikit-learn compatibility of an estimator\n# "}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "_iterative.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/impute", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "mber of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_features_with_missing_ : int\n        Number of features with missing values.\n\n    indicator_ : :class:`~sklearn.impute.MissingIndicator`\n        Indicator used to add binary indicators for missing values.\n        `None` if `add_indicator=False`.\n\n    random_state_ : RandomState instance\n        RandomState instance that is generated either from a seed, the random\n        number generator or by `np.random`.\n\n    See Also\n    --------\n    SimpleImputer : Univariate imputer for completing missing values\n        with simple strategies.\n    KNNImputer : Multivariate imputer that estimates missing features using\n        nearest samples.\n\n    Notes\n    -----\n    To support imputation in inductive mode we store each feature's estimator\n    during the :meth:`fit` phase, and predict without refitting (in order)\n    during the :meth:`transform` phase.\n\n    Features which contain all missing values at :meth:`fit` are discarded upon\n    :meth:`transform`.\n\n    Using defaults, the imputer scales in :math:`\\\\mathcal{O}(knp^3\\\\min(n,p))`\n    where :math:`k` = `max_iter`, :math:`n` the number of samples and\n    :math:`p` the number of features. It thus becomes prohibitively costly when\n    the number of features increases. Setting\n    `n_nearest_features << n_features`, `skip_complete=True` or increasing `tol`\n    can help to reduce its computational cost.\n\n    Depending on the nature of missing values, simple imputers can be\n    preferable in a prediction context.\n\n    References\n    ----------\n    .. [1] `Stef van Buuren, Karin Groothuis-Oudshoorn (2011). \"mice:\n        Multivariate Imputation by Chained Equations in R\". Journal of\n        Statistical Software 45: 1-67.\n        <https://www."}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/impute", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rray containing non-missing values of each column.\n\n        .. versionadded:: 0.20\n           strategy=\"constant\" for fixed value imputation.\n\n        .. versionadded:: 1.5\n           strategy=callable for custom value imputation.\n\n    fill_value : str or numerical value, default=None\n        When strategy == \"constant\", `fill_value` is used to replace all\n        occurrences of missing_values. For string or object data types,\n        `fill_value` must be a string.\n        If `None`, `fill_value` will be 0 when imputing numerical\n        data and \"missing_value\" for strings or object data types.\n\n    copy : bool, default=True\n        If True, a copy of X will be created. If False, imputation will\n        be done in-place whenever possible. Note that, in the following cases,\n        a new copy will always be made, even if `copy=False`:\n\n        - If `X` is not an array of floating values;\n        - If `X` is encoded as a CSR matrix;\n        - If `add_indicator=True`.\n\n    add_indicator : bool, default=False\n        If True, a :class:`MissingIndicator` transform will stack onto output\n        of the imputer's transform. This allows a predictive estimator\n        to account for missingness despite imputation. If a feature has no\n        missing values at fit/train time, the feature won't appear on\n        the missing indicator even if there are missing values at\n        transform/test time.\n\n    keep_empty_features : bool, default=False\n        If True, features that consist exclusively of missing values when\n        `fit` is called are returned in results when `transform` is called.\n        The imputed value is always `0` except when `strategy=\"constant\"`\n        in which case `fill_value` will be used instead.\n\n        .. versionadded:: 1.2\n\n        .. versionchanged:: 1.6\n            Currently, when `keep_empty_feature=False` and `strategy=\"constant\"`,\n            empty features are not dropped. This behaviour will change in version\n            1.8. Set `keep_empty_"}, {"start_line": 30000, "end_line": 32000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/impute", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_get_mask(self.statistics_, np.nan))\n        names = input_features[non_missing_mask]\n        return self._concatenate_indicator_feature_names_out(names, input_features)\n\n\nclass MissingIndicator(TransformerMixin, BaseEstimator):\n    \"\"\"Binary indicators for missing values.\n\n    Note that this component typically should not be used in a vanilla\n    :class:`~sklearn.pipeline.Pipeline` consisting of transformers and a\n    classifier, but rather could be added using a\n    :class:`~sklearn.pipeline.FeatureUnion` or\n    :class:`~sklearn.compose.ColumnTransformer`.\n\n    Read more in the :ref:`User Guide <impute>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    missing_values : int, float, str, np.nan or None, default=np.nan\n        The placeholder for the missing values. All occurrences of\n        `missing_values` will be imputed. For pandas' dataframes with\n        nullable integer dtypes with missing values, `missing_values`\n        should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.\n\n    features : {'missing-only', 'all'}, default='missing-only'\n        Whether the imputer mask should represent all or a subset of\n        features.\n\n        - If `'missing-only'` (default), the imputer mask will only represent\n          features containing missing values during fit time.\n        - If `'all'`, the imputer mask will represent all features.\n\n    sparse : bool or 'auto', default='auto'\n        Whether the imputer mask format should be sparse or dense.\n\n        - If `'auto'` (default), the imputer mask will be of same type as\n          input.\n        - If `True`, the imputer mask will be a sparse matrix.\n        - If `False`, the imputer mask will be a numpy array.\n\n    error_on_new : bool, default=True\n        If `True`, :meth:`transform` will raise an error when there are\n        features with missing values that have no missing values in\n        :meth:`fit`. This is applicable only when `features='missing-only'`.\n\n    Attributes\n    -----"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/impute", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "eturn names\n\n        indicator_names = self.indicator_.get_feature_names_out(input_features)\n        return np.concatenate([names, indicator_names])\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.allow_nan = is_scalar_nan(self.missing_values)\n        return tags\n\n\nclass SimpleImputer(_BaseImputer):\n    \"\"\"Univariate imputer for completing missing values with simple strategies.\n\n    Replace missing values using a descriptive statistic (e.g. mean, median, or\n    most frequent) along each column, or using a constant value.\n\n    Read more in the :ref:`User Guide <impute>`.\n\n    .. versionadded:: 0.20\n       `SimpleImputer` replaces the previous `sklearn.preprocessing.Imputer`\n       estimator which is now removed.\n\n    Parameters\n    ----------\n    missing_values : int, float, str, np.nan, None or pandas.NA, default=np.nan\n        The placeholder for the missing values. All occurrences of\n        `missing_values` will be imputed. For pandas' dataframes with\n        nullable integer dtypes with missing values, `missing_values`\n        can be set to either `np.nan` or `pd.NA`.\n\n    strategy : str or Callable, default='mean'\n        The imputation strategy.\n\n        - If \"mean\", then replace missing values using the mean along\n          each column. Can only be used with numeric data.\n        - If \"median\", then replace missing values using the median along\n          each column. Can only be used with numeric data.\n        - If \"most_frequent\", then replace missing using the most frequent\n          value along each column. Can be used with strings or numeric data.\n          If there is more than one such value, only the smallest is returned.\n        - If \"constant\", then replace missing values with fill_value. Can be\n          used with strings or numeric data.\n        - If an instance of Callable, then replace missing values using the\n          scalar statistic returned by running the callable over a dense 1d\n          a"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/impute", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "feature=True` to preserve this behaviour.\n\n    Attributes\n    ----------\n    statistics_ : array of shape (n_features,)\n        The imputation fill value for each feature.\n        Computing statistics can result in `np.nan` values.\n        During :meth:`transform`, features corresponding to `np.nan`\n        statistics will be discarded.\n\n    indicator_ : :class:`~sklearn.impute.MissingIndicator`\n        Indicator used to add binary indicators for missing values.\n        `None` if `add_indicator=False`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    IterativeImputer : Multivariate imputer that estimates values to impute for\n        each feature with missing values from all the others.\n    KNNImputer : Multivariate imputer that estimates missing features using\n        nearest samples.\n\n    Notes\n    -----\n    Columns which only contained missing values at :meth:`fit` are discarded\n    upon :meth:`transform` if strategy is not `\"constant\"`.\n\n    In a prediction context, simple imputation usually performs poorly when\n    associated with a weak learner. However, with a powerful learner, it can\n    lead to as good or better performance than complex imputation such as\n    :class:`~sklearn.impute.IterativeImputer` or :class:`~sklearn.impute.KNNImputer`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.impute import SimpleImputer\n    >>> imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n    >>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n    SimpleImputer()\n    >>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n    >>> print(imp_mean.transform(X))\n    [[ 7.   2.   3. ]\n     [ 4.   3.5  6. ]\n     [10.   3.5  9. ]]\n\n "}, {"start_line": 32000, "end_line": 34000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/impute", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "-----\n    features_ : ndarray of shape (n_missing_features,) or (n_features,)\n        The features indices which will be returned when calling\n        :meth:`transform`. They are computed during :meth:`fit`. If\n        `features='all'`, `features_` is equal to `range(n_features)`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    SimpleImputer : Univariate imputation of missing values.\n    IterativeImputer : Multivariate imputation of missing values.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.impute import MissingIndicator\n    >>> X1 = np.array([[np.nan, 1, 3],\n    ...                [4, 0, np.nan],\n    ...                [8, 1, 0]])\n    >>> X2 = np.array([[5, 1, np.nan],\n    ...                [np.nan, 2, 3],\n    ...                [2, 4, 0]])\n    >>> indicator = MissingIndicator()\n    >>> indicator.fit(X1)\n    MissingIndicator()\n    >>> X2_tr = indicator.transform(X2)\n    >>> X2_tr\n    array([[False,  True],\n           [ True, False],\n           [False, False]])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"missing_values\": [MissingValues()],\n        \"features\": [StrOptions({\"missing-only\", \"all\"})],\n        \"sparse\": [\"boolean\", StrOptions({\"auto\"})],\n        \"error_on_new\": [\"boolean\"],\n    }\n\n    def __init__(\n        self,\n        *,\n        missing_values=np.nan,\n        features=\"missing-only\",\n        sparse=\"auto\",\n        error_on_new=True,\n    ):\n        self.missing_values = missing_values\n        self.features = features\n        self.sparse = sparse\n        self.error_on_new = error_on_new\n\n    def _get_missing_features_info(self, X):\n        \"\"\"Compute the imputer mask and the indices of the features\n        con"}], "retrieved_count": 10, "cost_time": 1.0606803894042969}
{"question": "What is the relationship between Scikit-learn's BaseEstimator class and the TransformerMixin class in establishing the interface for estimators and transformers?", "answer": null, "relative_code_list": null, "ground_truth": "The relationship between BaseEstimator and TransformerMixin establishes a hierarchical interface system for scikit-learn estimators and transformers:\n\n1. **BaseEstimator - Foundation Class**:\n   - **Core Functionality**: Provides fundamental estimator capabilities\n   - **Parameter Management**: Implements get_params() and set_params() methods\n   - **Serialization**: Supports pickle-based model persistence\n   - **Validation**: Integrates with parameter validation system\n   - **Feature Tracking**: Manages n_features_in_ and feature_names_in_ attributes\n   - **Metadata Routing**: Supports advanced parameter routing capabilities\n   - **Output Configuration**: Provides set_output() for controlling output formats\n\n2. **TransformerMixin - Specialized Interface**:\n   - **Inheritance**: Inherits from BaseEstimator through _SetOutputMixin\n   - **Transformer Methods**: Defines transform() and fit_transform() methods\n   - **Default Implementation**: Provides default fit_transform() implementation\n   - **Metadata Support**: Handles metadata routing for transform operations\n   - **Feature Names**: Automatically wraps transform methods for feature name handling\n   - **Output Control**: Inherits set_output() functionality for output format control\n\n3. **Interface Hierarchy**:\n   - **BaseEstimator**: All estimators inherit from this base class\n   - **Mixin Classes**: Specialized mixins (TransformerMixin, ClassifierMixin, etc.) provide specific functionality\n   - **Multiple Inheritance**: Estimators can inherit from multiple mixins\n   - **Method Resolution**: Python's MRO (Method Resolution Order) determines method inheritance\n   - **Consistency**: Ensures consistent interface across all estimator types\n\n4. **Method Implementation**:\n   - **BaseEstimator Methods**: get_params(), set_params(), get_feature_names_out()\n   - **TransformerMixin Methods**: fit_transform(), transform() (abstract)\n   - **Combined Interface**: Transformers get both base and specialized methods\n   - **Default Behaviors**: Mixins provide sensible defaults for common operations\n   - **Override Capability**: Subclasses can override any inherited method\n\n5. **Design Patterns**:\n   - **Template Method**: BaseEstimator defines the overall structure\n   - **Strategy Pattern**: Mixins provide different behavioral strategies\n   - **Composition**: Multiple mixins can be combined for complex functionality\n   - **Separation of Concerns**: Base functionality separated from specialized behavior\n   - **Extensibility**: Easy to add new mixins for new estimator types\n\n6. **Validation Integration**:\n   - **Parameter Validation**: BaseEstimator integrates with validation system\n   - **Data Validation**: Both classes support data validation through validate_data()\n   - **Feature Validation**: Consistent feature counting and naming across estimators\n   - **Error Handling**: Unified error handling and messaging system\n   - **Type Safety**: Supports type checking and constraint validation\n\n7. **Pipeline Compatibility**:\n   - **Unified Interface**: All transformers work seamlessly in Pipeline objects\n   - **Method Consistency**: fit(), transform(), fit_transform() work consistently\n   - **Parameter Routing**: Supports advanced parameter routing in pipelines\n   - **Feature Names**: Consistent feature name handling across pipeline steps\n   - **Output Formats**: Unified output format control through set_output()\n\n8. **Advanced Features**:\n   - **Metadata Routing**: Both classes support metadata routing for advanced use cases\n   - **Output Configuration**: Unified output format control\n   - **Feature Names**: Consistent feature name preservation and generation\n   - **Serialization**: Full pickle support for all estimator types\n   - **Cloning**: Deep cloning support for estimator copying\n\n9. **Best Practices**:\n   - **Inheritance Order**: TransformerMixin should come after BaseEstimator in inheritance\n   - **Method Override**: Override specific methods rather than duplicating functionality\n   - **Validation**: Always call super() methods for proper validation\n   - **Documentation**: Document any deviations from standard interface\n   - **Testing**: Use scikit-learn's estimator checks for compatibility verification", "score": null, "retrieved_content": [{"start_line": 28000, "end_line": 30000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " : ndarray of shape (n_rows, n_cols)\n            The submatrix corresponding to bicluster `i`.\n\n        Notes\n        -----\n        Works with sparse matrices. Only works if ``rows_`` and\n        ``columns_`` attributes exist.\n        \"\"\"\n\n        data = check_array(data, accept_sparse=\"csr\")\n        row_ind, col_ind = self.get_indices(i)\n        return data[row_ind[:, np.newaxis], col_ind]\n\n\nclass TransformerMixin(_SetOutputMixin):\n    \"\"\"Mixin class for all transformers in scikit-learn.\n\n    This mixin defines the following functionality:\n\n    - a `fit_transform` method that delegates to `fit` and `transform`;\n    - a `set_output` method to output `X` as a specific container type.\n\n    If :term:`get_feature_names_out` is defined, then :class:`BaseEstimator` will\n    automatically wrap `transform` and `fit_transform` to follow the `set_output`\n    API. See the :ref:`developer_api_set_output` for details.\n\n    :class:`OneToOneFeatureMixin` and\n    :class:`ClassNamePrefixFeaturesOutMixin` are helpful mixins for\n    defining :term:`get_feature_names_out`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, TransformerMixin\n    >>> class MyTransformer(TransformerMixin, BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         return self\n    ...     def transform(self, X):\n    ...         return np.full(shape=len(X), fill_value=self.param)\n    >>> transformer = MyTransformer()\n    >>> X = [[1, 2], [2, 3], [3, 4]]\n    >>> transformer.fit_transform(X)\n    array([1, 1, 1])\n    \"\"\"\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.transformer_tags = TransformerTags()\n        return tags\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transf"}, {"start_line": 6000, "end_line": 7347, "belongs_to": {"file_name": "common.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/asv_benchmarks/benchmarks", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n(self, *args):\n                est_path = get_estimator_path(self, Benchmark.base_commit, args, True)\n                with est_path.open(mode=\"rb\") as f:\n                    estimator_base = pickle.load(f)\n\n                y_val_pred_base = estimator_base.predict(self.X_val)\n                y_val_pred = self.estimator.predict(self.X_val)\n\n                return np.allclose(y_val_pred_base, y_val_pred)\n\n    @property\n    @abstractmethod\n    def params(self):\n        pass\n\n\nclass Transformer(ABC):\n    \"\"\"Abstract base class for benchmarks of estimators implementing transform\"\"\"\n\n    if Benchmark.bench_transform:\n\n        def time_transform(self, *args):\n            self.estimator.transform(self.X)\n\n        def peakmem_transform(self, *args):\n            self.estimator.transform(self.X)\n\n        if Benchmark.base_commit is not None:\n\n            def track_same_transform(self, *args):\n                est_path = get_estimator_path(self, Benchmark.base_commit, args, True)\n                with est_path.open(mode=\"rb\") as f:\n                    estimator_base = pickle.load(f)\n\n                X_val_t_base = estimator_base.transform(self.X_val)\n                X_val_t = self.estimator.transform(self.X_val)\n\n                return np.allclose(X_val_t_base, X_val_t)\n\n    @property\n    @abstractmethod\n    def params(self):\n        pass\n"}, {"start_line": 29000, "end_line": 31000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n` are helpful mixins for\n    defining :term:`get_feature_names_out`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, TransformerMixin\n    >>> class MyTransformer(TransformerMixin, BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         return self\n    ...     def transform(self, X):\n    ...         return np.full(shape=len(X), fill_value=self.param)\n    >>> transformer = MyTransformer()\n    >>> X = [[1, 2], [2, 3], [3, 4]]\n    >>> transformer.fit_transform(X)\n    array([1, 1, 1])\n    \"\"\"\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.transformer_tags = TransformerTags()\n        return tags\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n                default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.\n            Pass only if the estimator accepts additional params in its `fit` method.\n\n        Returns\n        -------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.\n        \"\"\"\n        # non-optimized default implementation; override when a better\n        # method is possible for a given clustering algorithm\n\n        # we do not route parameters here, since consumers don't route. But\n        # since it's possible for a `transform` method to also consume\n        # metadata, we check if that's the case, and we raise a warning telling\n        # users that they "}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "aram2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n\n    # _sklearn_output_config is used by `set_output` to configure the output\n    # container of an estimator.\n    if hasattr(estimator, \"_sklearn_output_config\"):\n        new_object._sklearn_output_config = copy.deepcopy(\n            estimator._sklearn_output_config\n        )\n    return new_object\n\n\nclass BaseEstimator(ReprHTMLMixin, _HTMLDocumentationLinkMixin, _MetadataRequester):\n    \"\"\"Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])\n    \"\"\"\n\n    _html_"}, {"start_line": 43000, "end_line": 45000, "belongs_to": {"file_name": "_testing.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "        return {\"param\": self.param}\n\n    def set_params(self, **params):\n        for key, value in params.items():\n            setattr(self, key, value)\n        return self\n\n    def fit(self, X, y):\n        X, y = check_X_y(X, y)\n        self.is_fitted_ = True\n        self._mean = np.mean(y)\n        return self\n\n    def predict(self, X):\n        check_is_fitted(self)\n        X = check_array(X)\n        return np.ones(shape=(X.shape[0],)) * self._mean\n\n    def score(self, X, y):\n        from sklearn.metrics import r2_score\n\n        return r2_score(y, self.predict(X))\n\n    def __sklearn_tags__(self):\n        return Tags(\n            estimator_type=\"regressor\",\n            classifier_tags=None,\n            regressor_tags=RegressorTags(),\n            transformer_tags=None,\n            target_tags=TargetTags(required=True),\n        )\n\n\nclass MinimalTransformer:\n    \"\"\"Minimal transformer implementation without inheriting from\n    BaseEstimator.\n\n    This estimator should be tested with:\n\n    * `check_estimator` in `test_estimator_checks.py`;\n    * within a `Pipeline` in `test_pipeline.py`;\n    * within a `SearchCV` in `test_search.py`.\n    \"\"\"\n\n    def __init__(self, param=None):\n        self.param = param\n\n    def get_params(self, deep=True):\n        return {\"param\": self.param}\n\n    def set_params(self, **params):\n        for key, value in params.items():\n            setattr(self, key, value)\n        return self\n\n    def fit(self, X, y=None):\n        check_array(X)\n        self.is_fitted_ = True\n        return self\n\n    def transform(self, X, y=None):\n        check_is_fitted(self)\n        X = check_array(X)\n        return X\n\n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X, y)\n\n    def __sklearn_tags__(self):\n        return Tags(\n            estimator_type=\"transformer\",\n            classifier_tags=None,\n            regressor_tags=None,\n            transformer_tags=TransformerTags(),\n            target_tags=TargetTags(required=False),\n "}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "TransfFitParams(Transf):\n    def fit(self, X, y=None, **fit_params):\n        self.fit_params = fit_params\n        return self\n\n\nclass Mult(TransformerMixin, BaseEstimator):\n    def __init__(self, mult=1):\n        self.mult = mult\n\n    def __sklearn_is_fitted__(self):\n        return True\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return np.asarray(X) * self.mult\n\n    def inverse_transform(self, X):\n        return np.asarray(X) / self.mult\n\n    def predict(self, X):\n        return (np.asarray(X) * self.mult).sum(axis=1)\n\n    predict_proba = predict_log_proba = decision_function = predict\n\n    def score(self, X, y=None):\n        return np.sum(X)\n\n\nclass FitParamT(BaseEstimator):\n    \"\"\"Mock classifier\"\"\"\n\n    def __init__(self):\n        self.successful = False\n\n    def fit(self, X, y, should_succeed=False):\n        self.successful = should_succeed\n        self.fitted_ = True\n\n    def predict(self, X):\n        return self.successful\n\n    def fit_predict(self, X, y, should_succeed=False):\n        self.fit(X, y, should_succeed=should_succeed)\n        return self.predict(X)\n\n    def score(self, X, y=None, sample_weight=None):\n        if sample_weight is not None:\n            X = X * sample_weight\n        return np.sum(X)\n\n\nclass DummyTransf(Transf):\n    \"\"\"Transformer which store the column means\"\"\"\n\n    def fit(self, X, y):\n        self.means_ = np.mean(X, axis=0)\n        # store timestamp to figure out whether the result of 'fit' has been\n        # cached or not\n        self.timestamp_ = time.time()\n        return self\n\n\nclass DummyEstimatorParams(BaseEstimator):\n    \"\"\"Mock classifier that takes params on predict\"\"\"\n\n    def __sklearn_is_fitted__(self):\n        return True\n\n    def fit(self, X, y):\n        return self\n\n    def predict(self, X, got_attribute=False):\n        self.got_attribute = got_attribute\n        return self\n\n    def predict_proba(self, X, got_attribute=False):\n        self.got_attribute = got_attribute\n   "}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n estimator instead of a class.\"\n                )\n            else:\n                raise TypeError(\n                    \"Cannot clone object '%s' (type %s): \"\n                    \"it does not seem to be a scikit-learn \"\n                    \"estimator as it does not implement a \"\n                    \"'get_params' method.\" % (repr(estimator), type(estimator))\n                )\n\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        new_object_params[name] = clone(param, safe=False)\n\n    new_object = klass(**new_object_params)\n    try:\n        new_object._metadata_request = copy.deepcopy(estimator._metadata_request)\n    except AttributeError:\n        pass\n\n    params_set = new_object.get_params(deep=False)\n\n    # quick sanity check of the parameters of the clone\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is not param2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n\n    # _sklearn_output_config is used by `set_output` to configure the output\n    # container of an estimator.\n    if hasattr(estimator, \"_sklearn_output_config\"):\n        new_object._sklearn_output_config = copy.deepcopy(\n            estimator._sklearn_output_config\n        )\n    return new_object\n\n\nclass BaseEstimator(ReprHTMLMixin, _HTMLDocumentationLinkMixin, _MetadataRequester):\n    \"\"\"Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])\n    \"\"\"\n\n    _html_repr = estimator_html_repr\n\n    @classmethod\n    def _get_param_names(cls):\n        \"\"\"Get parameter names for the estimator\"\"\"\n        # fetch the constructor or the original constructor before\n        # deprecation wrapping if any\n        init = getattr(cls.__init__, \"deprecated_original\", cls.__init__)\n        if init is object.__init__:\n            # No explicit constructor to introspect\n            return []\n\n        # introspect the constructor arguments to find the model parameters\n        # to represent\n        init_signature = inspect.signature(init)\n        # Consider the constructor parameters excluding 'self'\n        parameters = [\n            p\n            for p in init_signature.parameters.values()\n            if p.name != \"self\" and p.kind != p.VAR_KEYWORD\n        ]\n        for p in parameters:\n            if p.kind == p.VAR_POSITIONAL:\n                raise RuntimeError(\n                    \"scikit-learn estimators should always \"\n                    \"specify their para"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "(TransformerMixin, BaseEstimator):\n    def __init__(self, sparse_container=None):\n        self.sparse_container = sparse_container\n\n    def fit(self, X, y=None):\n        validate_data(self, X)\n        return self\n\n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X)\n\n    def transform(self, X):\n        check_is_fitted(self)\n        X = validate_data(self, X, accept_sparse=True, reset=False)\n        return self.sparse_container(X)\n\n\nclass EstimatorInconsistentForPandas(BaseEstimator):\n    def fit(self, X, y):\n        try:\n            from pandas import DataFrame\n\n            if isinstance(X, DataFrame):\n                self.value_ = X.iloc[0, 0]\n            else:\n                X = check_array(X)\n                self.value_ = X[1, 0]\n            return self\n\n        except ImportError:\n            X = check_array(X)\n            self.value_ = X[1, 0]\n            return self\n\n    def predict(self, X):\n        X = check_array(X)\n        return np.array([self.value_] * X.shape[0])\n\n\nclass UntaggedBinaryClassifier(SGDClassifier):\n    # Toy classifier that only supports binary classification, will fail tests.\n    def fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None):\n        super().fit(X, y, coef_init, intercept_init, sample_weight)\n        if len(self.classes_) > 2:\n            raise ValueError(\"Only 2 classes are supported\")\n        return self\n\n    def partial_fit(self, X, y, classes=None, sample_weight=None):\n        super().partial_fit(X=X, y=y, classes=classes, sample_weight=sample_weight)\n        if len(self.classes_) > 2:\n            raise ValueError(\"Only 2 classes are supported\")\n        return self\n\n\nclass TaggedBinaryClassifier(UntaggedBinaryClassifier):\n    def fit(self, X, y):\n        y_type = type_of_target(y, input_name=\"y\", raise_unknown=True)\n        if y_type != \"binary\":\n            raise ValueError(\n                \"Only binary classification is supported. The type of the target \"\n                f"}, {"start_line": 35000, "end_line": 37000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n.base import ClassNamePrefixFeaturesOutMixin, BaseEstimator\n    >>> class MyEstimator(ClassNamePrefixFeaturesOutMixin, BaseEstimator):\n    ...     def fit(self, X, y=None):\n    ...         self._n_features_out = X.shape[1]\n    ...         return self\n    >>> X = np.array([[1, 2], [3, 4]])\n    >>> MyEstimator().fit(X).get_feature_names_out()\n    array(['myestimator0', 'myestimator1'], dtype=object)\n    \"\"\"\n\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.\n        \"\"\"\n        check_is_fitted(self, \"_n_features_out\")\n        return _generate_get_feature_names_out(\n            self, self._n_features_out, input_features=input_features\n        )\n\n\nclass DensityMixin:\n    \"\"\"Mixin class for all density estimators in scikit-learn.\n\n    This mixin defines the following functionality:\n\n    - sets estimator type to `\"density_estimator\"` through the `estimator_type` tag;\n    - `score` method that default that do no-op.\n\n    Examples\n    --------\n    >>> from sklearn.base import DensityMixin\n    >>> class MyEstimator(DensityMixin):\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    >>> estimator = MyEstimator()\n    >>> hasattr(estimator, \"score\")\n    True\n    \"\"\"\n\n    # TODO(1.8): Remove this attribute\n    _estimator_type = \"DensityEstimator\"\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.estimator"}], "retrieved_count": 10, "cost_time": 1.060270071029663}
{"question": "Where in Scikit-learn's codebase does the estimator interface improve performance compared to algorithm-specific APIs?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn's estimator interface improves performance compared to algorithm-specific APIs through several key optimizations implemented throughout the codebase. Here's where and how these performance improvements are achieved:\n\n1. **BaseEstimator Optimizations**:\n   - **Efficient Parameter Management**: Optimized get_params() and set_params() methods\n   - **Fast Cloning**: Efficient clone() function for estimator copying\n   - **Memory-Efficient State Management**: Optimized storage and retrieval of estimator state\n   - **Cached Parameter Access**: Parameter values are cached for faster access\n   - **Optimized Serialization**: Fast pickle-based serialization and deserialization\n\n2. **Unified Interface Benefits**:\n   - **Consistent Method Signatures**: Standardized fit(), predict(), transform() methods\n   - **Optimized Method Dispatching**: Fast method resolution through inheritance\n   - **Reduced Overhead**: Minimal interface overhead compared to custom APIs\n   - **Efficient Type Checking**: Fast isinstance() and duck typing checks\n   - **Optimized Method Chaining**: Efficient method chaining for complex workflows\n\n3. **Memory Management Optimizations**:\n   - **Shared Memory Pools**: Efficient memory allocation and deallocation\n   - **Copy Avoidance**: Minimizes unnecessary data copying between operations\n   - **Memory Layout Optimization**: Optimized memory layout for better cache performance\n   - **Garbage Collection Optimization**: Efficient cleanup of temporary objects\n   - **Memory Pooling**: Shared memory pools for common data structures\n\n4. **Algorithm-Specific Performance Enhancements**:\n   - **Optimized Base Implementations**: Efficient base class implementations for common operations\n   - **Specialized Mixins**: Performance-optimized mixins for specific estimator types\n   - **Fast Validation**: Optimized input validation with minimal overhead\n   - **Efficient State Tracking**: Fast state management for fitted estimators\n   - **Optimized Attribute Access**: Fast access to estimator attributes and parameters\n\n5. **Pipeline and Meta-Estimator Optimizations**:\n   - **Efficient Pipeline Execution**: Optimized sequential processing of pipeline steps\n   - **Fast Parameter Routing**: Efficient parameter routing through meta-estimators\n   - **Optimized Cloning**: Fast cloning of complex estimator hierarchies\n   - **Efficient State Management**: Optimized state management across pipeline steps\n   - **Memory-Efficient Caching**: Efficient caching of intermediate results\n\n6. **Cross-Validation Performance**:\n   - **Parallel Processing**: Efficient parallel processing of CV folds\n   - **Optimized Splitting**: Fast data splitting algorithms\n   - **Efficient Result Aggregation**: Fast aggregation of CV results\n   - **Memory-Efficient Folding**: Efficient memory usage during CV\n   - **Optimized Scoring**: Fast scoring function application\n\n7. **Data Validation Optimizations**:\n   - **Fast Type Checking**: Optimized type checking and conversion\n   - **Efficient Shape Validation**: Fast shape and dimension validation\n   - **Optimized Sparse Matrix Handling**: Efficient sparse matrix operations\n   - **Fast Feature Name Handling**: Efficient feature name validation and tracking\n   - **Memory-Efficient Validation**: Minimal memory overhead during validation\n\n8. **Hyperparameter Tuning Performance**:\n   - **Efficient Parameter Search**: Optimized parameter space exploration\n   - **Fast Model Cloning**: Efficient cloning for parameter evaluation\n   - **Optimized Result Storage**: Efficient storage of tuning results\n   - **Fast Parameter Routing**: Efficient parameter routing to nested estimators\n   - **Memory-Efficient Tuning**: Minimal memory overhead during tuning\n\n9. **Production Deployment Optimizations**:\n   - **Fast Model Loading**: Efficient model loading and deserialization\n   - **Optimized Prediction**: Fast prediction on new data\n   - **Efficient State Persistence**: Fast saving and loading of model state\n   - **Memory-Efficient Deployment**: Minimal memory footprint for deployment\n   - **Fast Model Updates**: Efficient model updating and incremental learning\n\n10. **Advanced Performance Features**:\n    - **Lazy Evaluation**: Lazy evaluation of expensive computations\n    - **Caching Mechanisms**: Intelligent caching of expensive operations\n    - **Optimized Algorithms**: Algorithm-specific optimizations within the interface\n    - **Fast Error Handling**: Efficient error handling and recovery\n    - **Performance Monitoring**: Built-in performance monitoring capabilities", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "plot_release_highlights_1_6_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# ruff: noqa: CPY001, E501\n\"\"\"\n=======================================\nRelease Highlights for scikit-learn 1.6\n=======================================\n\n.. currentmodule:: sklearn\n\nWe are pleased to announce the release of scikit-learn 1.6! Many bug fixes\nand improvements were added, as well as some key new features. Below we\ndetail the highlights of this release. **For an exhaustive list of\nall the changes**, please refer to the :ref:`release notes <release_notes_1_6>`.\n\nTo install the latest version (with pip)::\n\n    pip install --upgrade scikit-learn\n\nor with conda::\n\n    conda install -c conda-forge scikit-learn\n\n\"\"\"\n\n# %%\n# FrozenEstimator: Freezing an estimator\n# --------------------------------------\n#\n# This meta-estimator allows you to take an estimator and freeze its fit method, meaning\n# that calling `fit` does not perform any operations; also, `fit_predict` and\n# `fit_transform` call `predict` and `transform` respectively without calling `fit`. The\n# original estimator's other methods and properties are left unchanged. An interesting\n# use case for this is to use a pre-fitted model as a transformer step in a pipeline\n# or to pass a pre-fitted model to some of the meta-estimators. Here's a short example:\n\nimport time\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.frozen import FrozenEstimator\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import FixedThresholdClassifier\n\nX, y = make_classification(n_samples=1000, random_state=0)\n\nstart = time.time()\nclassifier = SGDClassifier().fit(X, y)\nprint(f\"Fitting the classifier took {(time.time() - start) * 1_000:.2f} milliseconds\")\n\nstart = time.time()\nthreshold_classifier = FixedThresholdClassifier(\n    estimator=FrozenEstimator(classifier), threshold=0.9\n).fit(X, y)\nprint(\n    f\"Fitting the threshold classifier took {(time.time() - start) * 1_000:.2f} \"\n    \"milliseconds\"\n)\n\n# %%\n# Fitting the threshold classifier skipped fitting the inner `SGDClassifier`. For more\n# d"}, {"start_line": 5000, "end_line": 6389, "belongs_to": {"file_name": "plot_release_highlights_1_2_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n :class:`~discriminant_analysis.LinearDiscriminantAnalysis`\n# --------------------------------------------------------------------------------------------\n# Experimental support for the `Array API <https://data-apis.org/array-api/latest/>`_\n# specification was added to :class:`~discriminant_analysis.LinearDiscriminantAnalysis`.\n# The estimator can now run on any Array API compliant libraries such as\n# `CuPy <https://docs.cupy.dev/en/stable/overview.html>`__, a GPU-accelerated array\n# library. For details, see the :ref:`User Guide <array_api>`.\n\n# %%\n# Improved efficiency of many estimators\n# --------------------------------------\n# In version 1.1 the efficiency of many estimators relying on the computation of\n# pairwise distances (essentially estimators related to clustering, manifold\n# learning and neighbors search algorithms) was greatly improved for float64\n# dense input. Efficiency improvement especially were a reduced memory footprint\n# and a much better scalability on multi-core machines.\n# In version 1.2, the efficiency of these estimators was further improved for all\n# combinations of dense and sparse inputs on float32 and float64 datasets, except\n# the sparse-dense and dense-sparse combinations for the Euclidean and Squared\n# Euclidean Distance metrics.\n# A detailed list of the impacted estimators can be found in the\n# :ref:`changelog <release_notes_1_2>`.\n"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n estimator instead of a class.\"\n                )\n            else:\n                raise TypeError(\n                    \"Cannot clone object '%s' (type %s): \"\n                    \"it does not seem to be a scikit-learn \"\n                    \"estimator as it does not implement a \"\n                    \"'get_params' method.\" % (repr(estimator), type(estimator))\n                )\n\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        new_object_params[name] = clone(param, safe=False)\n\n    new_object = klass(**new_object_params)\n    try:\n        new_object._metadata_request = copy.deepcopy(estimator._metadata_request)\n    except AttributeError:\n        pass\n\n    params_set = new_object.get_params(deep=False)\n\n    # quick sanity check of the parameters of the clone\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is not param2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n\n    # _sklearn_output_config is used by `set_output` to configure the output\n    # container of an estimator.\n    if hasattr(estimator, \"_sklearn_output_config\"):\n        new_object._sklearn_output_config = copy.deepcopy(\n            estimator._sklearn_output_config\n        )\n    return new_object\n\n\nclass BaseEstimator(ReprHTMLMixin, _HTMLDocumentationLinkMixin, _MetadataRequester):\n    \"\"\"Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User"}, {"start_line": 8000, "end_line": 9226, "belongs_to": {"file_name": "plot_release_highlights_1_6_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "troduced and replaces the\n#   previously private `BaseEstimator._validate_data` method. This function extends\n#   :func:`~sklearn.utils.validation.check_array` and adds support for remembering\n#   input feature counts and names.\n# - Estimator tags are now revamped and a part of the public API via\n#   :class:`sklearn.utils.Tags`. Estimators should now override the\n#   :meth:`BaseEstimator.__sklearn_tags__` method instead of implementing a `_more_tags`\n#   method. If you'd like to support multiple scikit-learn versions, you can implement\n#   both methods in your class.\n# - As a consequence of developing a public tag API, we've removed the `_xfail_checks`\n#   tag and tests which are expected to fail are directly passed to\n#   :func:`~sklearn.utils.estimator_checks.check_estimator` and\n#   :func:`~sklearn.utils.estimator_checks.parametrize_with_checks`. See their\n#   corresponding API docs for more details.\n# - Many tests in the common test suite are updated and raise more helpful error\n#   messages. We've also added some new tests, which should help you more easily fix\n#   potential issues with your estimators.\n#\n# An updated version of our :ref:`develop` is also available, which we recommend you\n# check out.\n"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "plot_release_highlights_1_1_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "0 and\n# 5 faster than previously. In summary, the following functions and estimators\n# now benefit from improved performance:\n#\n# - :func:`metrics.pairwise_distances_argmin`\n# - :func:`metrics.pairwise_distances_argmin_min`\n# - :class:`cluster.AffinityPropagation`\n# - :class:`cluster.Birch`\n# - :class:`cluster.MeanShift`\n# - :class:`cluster.OPTICS`\n# - :class:`cluster.SpectralClustering`\n# - :func:`feature_selection.mutual_info_regression`\n# - :class:`neighbors.KNeighborsClassifier`\n# - :class:`neighbors.KNeighborsRegressor`\n# - :class:`neighbors.RadiusNeighborsClassifier`\n# - :class:`neighbors.RadiusNeighborsRegressor`\n# - :class:`neighbors.LocalOutlierFactor`\n# - :class:`neighbors.NearestNeighbors`\n# - :class:`manifold.Isomap`\n# - :class:`manifold.LocallyLinearEmbedding`\n# - :class:`manifold.TSNE`\n# - :func:`manifold.trustworthiness`\n# - :class:`semi_supervised.LabelPropagation`\n# - :class:`semi_supervised.LabelSpreading`\n#\n# To know more about the technical details of this work, you can read\n# `this suite of blog posts <https://blog.scikit-learn.org/technical/performances/>`_.\n#\n# Moreover, the computation of loss functions has been refactored using\n# Cython resulting in performance improvements for the following estimators:\n#\n# - :class:`linear_model.LogisticRegression`\n# - :class:`linear_model.GammaRegressor`\n# - :class:`linear_model.PoissonRegressor`\n# - :class:`linear_model.TweedieRegressor`\n\n# %%\n# :class:`~decomposition.MiniBatchNMF`: an online version of NMF\n# --------------------------------------------------------------\n# The new class :class:`~decomposition.MiniBatchNMF` implements a faster but\n# less accurate version of non-negative matrix factorization\n# (:class:`~decomposition.NMF`). :class:`~decomposition.MiniBatchNMF` divides the\n# data into mini-batches and optimizes the NMF model in an online manner by\n# cycling over the mini-batches, making it better suited for large datasets. In\n# particular, it implements `partial_fit`, which can be used for "}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "aram2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n\n    # _sklearn_output_config is used by `set_output` to configure the output\n    # container of an estimator.\n    if hasattr(estimator, \"_sklearn_output_config\"):\n        new_object._sklearn_output_config = copy.deepcopy(\n            estimator._sklearn_output_config\n        )\n    return new_object\n\n\nclass BaseEstimator(ReprHTMLMixin, _HTMLDocumentationLinkMixin, _MetadataRequester):\n    \"\"\"Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])\n    \"\"\"\n\n    _html_"}, {"start_line": 47000, "end_line": 49000, "belongs_to": {"file_name": "estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "stimator.fit(X, y)\n            if hasattr(estimator, \"predict\"):\n                pred = estimator.predict(X)\n                if tags.target_tags.multi_output and not tags.target_tags.single_output:\n                    assert pred.shape == (X.shape[0], 1)\n                else:\n                    assert pred.shape == (X.shape[0],)\n            if hasattr(estimator, \"predict_proba\"):\n                probs = estimator.predict_proba(X)\n                if not tags.classifier_tags.multi_class:\n                    expected_probs_shape = (X.shape[0], 2)\n                else:\n                    expected_probs_shape = (X.shape[0], 4)\n                assert probs.shape == expected_probs_shape\n\n\ndef check_estimator_sparse_matrix(name, estimator_orig):\n    _check_estimator_sparse_container(name, estimator_orig, sparse.csr_matrix)\n\n\ndef check_estimator_sparse_array(name, estimator_orig):\n    _check_estimator_sparse_container(name, estimator_orig, sparse.csr_array)\n\n\ndef check_f_contiguous_array_estimator(name, estimator_orig):\n    # Non-regression test for:\n    # https://github.com/scikit-learn/scikit-learn/issues/23988\n    # https://github.com/scikit-learn/scikit-learn/issues/24013\n    estimator = clone(estimator_orig)\n\n    rng = np.random.RandomState(0)\n    X = 3 * rng.uniform(size=(20, 3))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    X = np.asfortranarray(X)\n    y = X[:, 0].astype(int)\n    y = _enforce_estimator_tags_y(estimator_orig, y)\n\n    estimator.fit(X, y)\n\n    if hasattr(estimator, \"transform\"):\n        estimator.transform(X)\n\n    if hasattr(estimator, \"predict\"):\n        estimator.predict(X)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_sample_weights_pandas_series(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type pandas.Series in the 'fit' function.\n    estimator = clone(estimator_orig)\n    try:\n        import pandas as pd\n\n        X = np.array(\n            [\n                [1, 1],\n            "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC, NuSVC\nfrom sklearn.utils import _array_api, all_estimators, deprecated\nfrom sklearn.utils._param_validation import Interval, StrOptions\nfrom sklearn.utils._test_common.instance_generator import (\n    _construct_instances,\n    _get_expected_failed_checks,\n)\nfrom sklearn.utils._testing import (\n    MinimalClassifier,\n    MinimalRegressor,\n    MinimalTransformer,\n    SkipTest,\n    ignore_warnings,\n    raises,\n)\nfrom sklearn.utils.estimator_checks import (\n    _check_name,\n    _NotAnArray,\n    _yield_all_checks,\n    check_array_api_input,\n    check_class_weight_balanced_linear_classifier,\n    check_classifier_data_not_an_array,\n    check_classifier_not_supporting_multiclass,\n    check_classifiers_multilabel_output_format_decision_function,\n    check_classifiers_multilabel_output_format_predict,\n    check_classifiers_multilabel_output_format_predict_proba,\n    check_classifiers_one_label_sample_weights,\n    check_dataframe_column_names_consistency,\n    check_decision_proba_consistency,\n    check_dict_unchanged,\n    check_dont_overwrite_parameters,\n    check_estimator,\n    check_estimator_cloneable,\n    check_estimator_repr,\n    check_estimator_sparse_array,\n    check_estimator_sparse_matrix,\n    check_estimator_sparse_tag,\n    check_estimator_tags_renamed,\n    check_estimators_nan_inf,\n    check_estimators_overwrite_params,\n    check_estimators_unfitted,\n    check_fit_check_is_fitted,\n    check_fit_score_takes_y,\n    check_methods_sample_order_invariance,\n    check_methods_subset_invariance,\n    check_mixin_order,\n    check_no_attributes_set_in_init,\n    check_outlier_contamination,\n    check_outlier_corruption,\n    check_parameters_default_constructible,\n    check_positive_only_tag_during_fit,\n    check_regressor_data_not_an_array,\n    check_requires_y_none,\n    check_sample_weights_pandas_series,\n    check_set_params,\n    estimator_checks_generator,\n    set_random_state,\n)\nfrom sklearn.utils.f"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Various utilities to check the compatibility of estimators with scikit-learn API.\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\nfrom __future__ import annotations\n\nimport pickle\nimport re\nimport textwrap\nimport warnings\nfrom contextlib import nullcontext\nfrom copy import deepcopy\nfrom functools import partial, wraps\nfrom inspect import signature\nfrom numbers import Integral, Real\nfrom typing import Callable, Literal\n\nimport joblib\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.stats import rankdata\n\nfrom sklearn import config_context\nfrom sklearn.base import (\n    BaseEstimator,\n    BiclusterMixin,\n    ClassifierMixin,\n    ClassNamePrefixFeaturesOutMixin,\n    ClusterMixin,\n    DensityMixin,\n    MetaEstimatorMixin,\n    MultiOutputMixin,\n    OneToOneFeatureMixin,\n    OutlierMixin,\n    RegressorMixin,\n    TransformerMixin,\n    clone,\n    is_classifier,\n    is_outlier_detector,\n    is_regressor,\n)\nfrom sklearn.datasets import (\n    load_iris,\n    make_blobs,\n    make_classification,\n    make_multilabel_classification,\n    make_regression,\n)\nfrom sklearn.exceptions import (\n    DataConversionWarning,\n    EstimatorCheckFailedWarning,\n    NotFittedError,\n    SkipTestWarning,\n)\nfrom sklearn.linear_model._base import LinearClassifierMixin\nfrom sklearn.metrics import accuracy_score, adjusted_rand_score, f1_score\nfrom sklearn.metrics.pairwise import linear_kernel, pairwise_distances, rbf_kernel\nfrom sklearn.model_selection import LeaveOneGroupOut, ShuffleSplit, train_test_split\nfrom sklearn.model_selection._validation import _safe_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, scale\nfrom sklearn.utils import _safe_indexing, shuffle\nfrom sklearn.utils._array_api import (\n    _atol_for_type,\n    _convert_to_numpy,\n    get_namespace,\n    yield_namespace_device_dtype_combinations,\n)\nfrom sklearn.utils._array_api import device as array_device\nfrom sklearn.utils._missing import is_sca"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "common.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/asv_benchmarks/benchmarks", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "operty\n    @abstractmethod\n    def params(self):\n        pass\n\n\nclass Estimator(ABC):\n    \"\"\"Abstract base class for all benchmarks of estimators\"\"\"\n\n    @abstractmethod\n    def make_data(self, params):\n        \"\"\"Return the dataset for a combination of parameters\"\"\"\n        # The datasets are cached using joblib.Memory so it's fast and can be\n        # called for each repeat\n        pass\n\n    @abstractmethod\n    def make_estimator(self, params):\n        \"\"\"Return an instance of the estimator for a combination of parameters\"\"\"\n        pass\n\n    def skip(self, params):\n        \"\"\"Return True if the benchmark should be skipped for these params\"\"\"\n        return False\n\n    def setup_cache(self):\n        \"\"\"Pickle a fitted estimator for all combinations of parameters\"\"\"\n        # This is run once per benchmark class.\n\n        clear_tmp()\n\n        param_grid = list(itertools.product(*self.params))\n\n        for params in param_grid:\n            if self.skip(params):\n                continue\n\n            estimator = self.make_estimator(params)\n            X, _, y, _ = self.make_data(params)\n\n            estimator.fit(X, y)\n\n            est_path = get_estimator_path(\n                self, Benchmark.save_dir, params, Benchmark.save_estimators\n            )\n            with est_path.open(mode=\"wb\") as f:\n                pickle.dump(estimator, f)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n\n        if self.skip(params):\n            raise NotImplementedError\n\n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n\n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\"rb\") as f:\n            self.estimator = pickle.load(f)\n\n        self.make_scorers()\n\n    def time_fit(self, *args):\n "}], "retrieved_count": 10, "cost_time": 1.0168628692626953}
{"question": "Why does Scikit-learn use a validation-based approach for hyperparameter tuning instead of analytical optimization?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn uses a validation-based approach for hyperparameter tuning instead of analytical optimization for several fundamental reasons that align with machine learning best practices and practical considerations:\n\n1. **Generalization Performance Focus**:\n   - **Cross-Validation**: Evaluates how well models generalize to unseen data\n   - **Out-of-Sample Performance**: Measures true predictive performance, not just training performance\n   - **Overfitting Prevention**: Helps identify when models are overfitting to training data\n   - **Robust Evaluation**: Provides more reliable estimates of model performance\n   - **Statistical Validity**: Follows established statistical principles for model evaluation\n\n2. **Algorithmic Complexity and Non-Convexity**:\n   - **Non-Convex Optimization**: Most machine learning problems are non-convex, making analytical optimization difficult\n   - **Multiple Local Optima**: Many hyperparameter spaces have multiple local optima\n   - **Discrete Parameters**: Many hyperparameters are discrete (e.g., number of trees, kernel types)\n   - **Categorical Parameters**: Some parameters are categorical and not amenable to gradient-based optimization\n   - **Mixed Parameter Types**: Hyperparameter spaces often mix continuous, discrete, and categorical parameters\n\n3. **Computational Efficiency**:\n   - **Parallel Evaluation**: Cross-validation can be parallelized across parameter combinations\n   - **Early Stopping**: Can implement early stopping strategies to avoid exhaustive search\n   - **Incremental Learning**: Some algorithms support warm-starting for efficient parameter exploration\n   - **Resource Management**: Better control over computational resources and time\n   - **Scalability**: Works well with large datasets and complex models\n\n4. **Practical Implementation Considerations**:\n   - **Black-Box Nature**: Many algorithms don't provide analytical gradients for hyperparameters\n   - **Implementation Complexity**: Analytical optimization would require significant changes to existing algorithms\n   - **Maintenance Burden**: Would increase code complexity and maintenance overhead\n   - **Backward Compatibility**: Validation-based approach maintains compatibility with existing code\n   - **User Familiarity**: Most users are familiar with cross-validation concepts\n\n5. **Statistical Robustness**:\n   - **Variance Estimation**: Cross-validation provides estimates of performance variance\n   - **Confidence Intervals**: Can estimate confidence intervals for performance metrics\n   - **Multiple Metrics**: Can evaluate multiple performance metrics simultaneously\n   - **Stratified Sampling**: Can maintain class distributions in classification problems\n   - **Time Series Considerations**: Special handling for temporal data through TimeSeriesSplit\n\n6. **Flexibility and Extensibility**:\n   - **Multiple Search Strategies**: GridSearchCV, RandomizedSearchCV, HalvingGridSearchCV\n   - **Custom Scoring**: Users can define custom scoring functions\n   - **Pipeline Integration**: Works seamlessly with Pipeline and FeatureUnion\n   - **Meta-Estimators**: Can be used with any estimator that follows the scikit-learn interface\n   - **Custom CV Splitters**: Users can define custom cross-validation strategies\n\n7. **Interpretability and Debugging**:\n   - **Transparent Process**: Users can see exactly which parameter combinations were evaluated\n   - **Performance Analysis**: Can analyze performance across different parameter ranges\n   - **Error Diagnosis**: Can identify parameter combinations that cause failures\n   - **Learning Curves**: Can generate learning curves to understand model behavior\n   - **Validation Curves**: Can visualize how parameters affect performance\n\n8. **Data Leakage Prevention**:\n   - **Proper Separation**: Ensures proper train/validation/test separation\n   - **Pipeline Safety**: Prevents data leakage in preprocessing pipelines\n   - **Feature Selection**: Handles feature selection within cross-validation folds\n   - **Model Selection**: Prevents overfitting to the test set\n   - **Best Practices**: Enforces proper machine learning workflow\n\n9. **Real-World Applicability**:\n   - **Domain Expertise**: Allows domain experts to specify parameter ranges based on knowledge\n   - **Business Constraints**: Can incorporate business constraints into parameter search\n   - **Resource Constraints**: Can limit search based on computational resources\n   - **Time Constraints**: Can implement time-bounded search strategies\n   - **Production Readiness**: Results are more likely to generalize to production data\n\n10. **Educational and Research Benefits**:\n    - **Understanding**: Helps users understand the relationship between parameters and performance\n    - **Experimentation**: Encourages systematic experimentation with different parameter values\n    - **Reproducibility**: Results are reproducible and can be shared with others\n    - **Documentation**: Process is well-documented and understood by the community\n    - **Best Practices**: Reinforces proper machine learning methodology", "score": null, "retrieved_content": [{"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "plot_nested_cross_validation_iris.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ion/test set splits. In the inner loop (here executed by\n:class:`GridSearchCV <sklearn.model_selection.GridSearchCV>`), the score is\napproximately maximized by fitting a model to each training set, and then\ndirectly maximized in selecting (hyper)parameters over the validation set. In\nthe outer loop (here in :func:`cross_val_score\n<sklearn.model_selection.cross_val_score>`), generalization error is estimated\nby averaging test set scores over several dataset splits.\n\nThe example below uses a support vector classifier with a non-linear kernel to\nbuild a model with optimized hyperparameters by grid search. We compare the\nperformance of non-nested and nested CV strategies by taking the difference\nbetween their scores.\n\n.. seealso::\n\n    - :ref:`cross_validation`\n    - :ref:`grid_search`\n\n.. rubric:: References\n\n.. [1] `Cawley, G.C.; Talbot, N.L.C. On over-fitting in model selection and\n    subsequent selection bias in performance evaluation.\n    J. Mach. Learn. Res 2010,11, 2079-2107.\n    <http://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf>`_\n\n\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.svm import SVC\n\n# Number of random trials\nNUM_TRIALS = 30\n\n# Load the dataset\niris = load_iris()\nX_iris = iris.data\ny_iris = iris.target\n\n# Set up possible values of parameters to optimize over\np_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}\n\n# We will use a Support Vector Classifier with \"rbf\" kernel\nsvm = SVC(kernel=\"rbf\")\n\n# Arrays to store scores\nnon_nested_scores = np.zeros(NUM_TRIALS)\nnested_scores = np.zeros(NUM_TRIALS)\n\n# Loop for each trial\nfor i in range(NUM_TRIALS):\n    # Choose cross-validation techniques for the inner and outer loops,\n    # independently of the dataset.\n    # E.g \"GroupKFold\", \"LeaveOneOut\", \"LeaveOneGroupOut\", etc.\n    in"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "plot_rbf_parameters.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/svm", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " set of support vectors does not change anymore. The radius of the RBF\nkernel alone acts as a good structural regularizer. Increasing ``C`` further\ndoesn't help, likely because there are no more training points in violation\n(inside the margin or wrongly classified), or at least no better solution can\nbe found. Scores being equal, it may make sense to use the smaller ``C``\nvalues, since very high ``C`` values typically increase fitting time.\n\nOn the other hand, lower ``C`` values generally lead to more support vectors,\nwhich may increase prediction time. Therefore, lowering the value of ``C``\ninvolves a trade-off between fitting time and prediction time.\n\nWe should also note that small differences in scores results from the random\nsplits of the cross-validation procedure. Those spurious variations can be\nsmoothed out by increasing the number of CV iterations ``n_splits`` at the\nexpense of compute time. Increasing the value number of ``C_range`` and\n``gamma_range`` steps will increase the resolution of the hyper-parameter heat\nmap.\n\n\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\n# %%\n# Utility class to move the midpoint of a colormap to be around\n# the values of interest.\n\nimport numpy as np\nfrom matplotlib.colors import Normalize\n\n\nclass MidpointNormalize(Normalize):\n    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n        self.midpoint = midpoint\n        Normalize.__init__(self, vmin, vmax, clip)\n\n    def __call__(self, value, clip=None):\n        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n        return np.ma.masked_array(np.interp(value, x, y))\n\n\n# %%\n# Load and prepare data set\n# -------------------------\n#\n# dataset for grid search\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# %%\n# Dataset for decision function visualization: we only keep the first two\n# features in X and sub-sample the dataset to keep only 2 classes and\n# make it a binary c"}, {"start_line": 37000, "end_line": 39000, "belongs_to": {"file_name": "_search_successive_halving.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "g trade-off.\n        However computing the scores on the training set can be computationally\n        expensive and is not strictly required to select the parameters that\n        yield the best generalization performance.\n\n    random_state : int, RandomState instance or None, default=None\n        Pseudo random number generator state used for subsampling the dataset\n        when `resources != 'n_samples'`. Also used for random uniform\n        sampling from lists of possible values instead of scipy.stats\n        distributions.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : int\n        Controls the verbosity: the higher, the more messages.\n\n    Attributes\n    ----------\n    n_resources_ : list of int\n        The amount of resources used at each iteration.\n\n    n_candidates_ : list of int\n        The number of candidate parameters that were evaluated at each\n        iteration.\n\n    n_remaining_candidates_ : int\n        The number of candidate parameters that are left after the last\n        iteration. It corresponds to `ceil(n_candidates[-1] / factor)`\n\n    max_resources_ : int\n        The maximum number of resources that any candidate is allowed to use\n        for a given iteration. Note that since the number of resources used at\n        each iteration must be a multiple of ``min_resources_``, the actual\n        number of resources used at the last iteration may be smaller than\n        ``max_resources_``.\n\n    min_resources_ : int\n        The amount of resources that are allocated for each candidate at the\n        first iteration.\n\n    n_iterations_ : int\n        The actual number of iterations that were run. This is equal to\n   "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "plot_successive_halving_heatmap.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "s=1000, random_state=rng)\n\ngammas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\nCs = [1, 10, 100, 1e3, 1e4, 1e5]\nparam_grid = {\"gamma\": gammas, \"C\": Cs}\n\nclf = SVC(random_state=rng)\n\ntic = time()\ngsh = HalvingGridSearchCV(\n    estimator=clf, param_grid=param_grid, factor=2, random_state=rng\n)\ngsh.fit(X, y)\ngsh_time = time() - tic\n\ntic = time()\ngs = GridSearchCV(estimator=clf, param_grid=param_grid)\ngs.fit(X, y)\ngs_time = time() - tic\n\n# %%\n# We now plot heatmaps for both search estimators.\n\n\ndef make_heatmap(ax, gs, is_sh=False, make_cbar=False):\n    \"\"\"Helper to make a heatmap.\"\"\"\n    results = pd.DataFrame(gs.cv_results_)\n    results[[\"param_C\", \"param_gamma\"]] = results[[\"param_C\", \"param_gamma\"]].astype(\n        np.float64\n    )\n    if is_sh:\n        # SH dataframe: get mean_test_score values for the highest iter\n        scores_matrix = results.sort_values(\"iter\").pivot_table(\n            index=\"param_gamma\",\n            columns=\"param_C\",\n            values=\"mean_test_score\",\n            aggfunc=\"last\",\n        )\n    else:\n        scores_matrix = results.pivot(\n            index=\"param_gamma\", columns=\"param_C\", values=\"mean_test_score\"\n        )\n\n    im = ax.imshow(scores_matrix)\n\n    ax.set_xticks(np.arange(len(Cs)))\n    ax.set_xticklabels([\"{:.0E}\".format(x) for x in Cs])\n    ax.set_xlabel(\"C\", fontsize=15)\n\n    ax.set_yticks(np.arange(len(gammas)))\n    ax.set_yticklabels([\"{:.0E}\".format(x) for x in gammas])\n    ax.set_ylabel(\"gamma\", fontsize=15)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n    if is_sh:\n        iterations = results.pivot_table(\n            index=\"param_gamma\", columns=\"param_C\", values=\"iter\", aggfunc=\"max\"\n        ).values\n        for i in range(len(gammas)):\n            for j in range(len(Cs)):\n                ax.text(\n                    j,\n                    i,\n                    iterations[i, j],\n                    ha=\"center\",\n          "}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "_search_successive_halving.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "uto\"}),\n        ],\n        \"min_resources\": [\n            Interval(Integral, 0, None, closed=\"neither\"),\n            StrOptions({\"exhaust\", \"smallest\"}),\n        ],\n        \"resource\": [str],\n        \"factor\": [Interval(Real, 0, None, closed=\"neither\")],\n        \"aggressive_elimination\": [\"boolean\"],\n    }\n    _parameter_constraints.pop(\"pre_dispatch\")  # not used in this class\n\n    def __init__(\n        self,\n        estimator,\n        *,\n        scoring=None,\n        n_jobs=None,\n        refit=True,\n        cv=5,\n        verbose=0,\n        random_state=None,\n        error_score=np.nan,\n        return_train_score=True,\n        max_resources=\"auto\",\n        min_resources=\"exhaust\",\n        resource=\"n_samples\",\n        factor=3,\n        aggressive_elimination=False,\n    ):\n        super().__init__(\n            estimator,\n            scoring=scoring,\n            n_jobs=n_jobs,\n            refit=refit,\n            cv=cv,\n            verbose=verbose,\n            error_score=error_score,\n            return_train_score=return_train_score,\n        )\n\n        self.random_state = random_state\n        self.max_resources = max_resources\n        self.resource = resource\n        self.factor = factor\n        self.min_resources = min_resources\n        self.aggressive_elimination = aggressive_elimination\n\n    def _check_input_parameters(self, X, y, split_params):\n        # We need to enforce that successive calls to cv.split() yield the same\n        # splits: see https://github.com/scikit-learn/scikit-learn/issues/15149\n        if not _yields_constant_splits(self._checked_cv_orig):\n            raise ValueError(\n                \"The cv parameter must yield consistent folds across \"\n                \"calls to split(). Set its random_state to an int, or set \"\n                \"shuffle=False.\"\n            )\n\n        if (\n            self.resource != \"n_samples\"\n            and self.resource not in self.estimator.get_params()\n        ):\n            raise ValueError(\n                f\"Can"}, {"start_line": 7000, "end_line": 8393, "belongs_to": {"file_name": "plot_rbf_parameters.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/svm", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "bplot(len(C_2d_range), len(gamma_2d_range), k + 1)\n    plt.title(\"gamma=10^%d, C=10^%d\" % (np.log10(gamma), np.log10(C)), size=\"medium\")\n\n    # visualize parameter's effect on decision function\n    plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)\n    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu_r, edgecolors=\"k\")\n    plt.xticks(())\n    plt.yticks(())\n    plt.axis(\"tight\")\n\nscores = grid.cv_results_[\"mean_test_score\"].reshape(len(C_range), len(gamma_range))\n\n# %%\n# Draw heatmap of the validation accuracy as a function of gamma and C\n#\n# The score are encoded as colors with the hot colormap which varies from dark\n# red to bright yellow. As the most interesting scores are all located in the\n# 0.92 to 0.97 range we use a custom normalizer to set the mid-point to 0.92 so\n# as to make it easier to visualize the small variations of score values in the\n# interesting range while not brutally collapsing all the low score values to\n# the same color.\n\nplt.figure(figsize=(8, 6))\nplt.subplots_adjust(left=0.2, right=0.95, bottom=0.15, top=0.95)\nplt.imshow(\n    scores,\n    interpolation=\"nearest\",\n    cmap=plt.cm.hot,\n    norm=MidpointNormalize(vmin=0.2, midpoint=0.92),\n)\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.colorbar()\nplt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\nplt.yticks(np.arange(len(C_range)), C_range)\nplt.title(\"Validation accuracy\")\nplt.show()\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "plot_nested_cross_validation_iris.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"\n=========================================\nNested versus non-nested cross-validation\n=========================================\n\nThis example compares non-nested and nested cross-validation strategies on a\nclassifier of the iris data set. Nested cross-validation (CV) is often used to\ntrain a model in which hyperparameters also need to be optimized. Nested CV\nestimates the generalization error of the underlying model and its\n(hyper)parameter search. Choosing the parameters that maximize non-nested CV\nbiases the model to the dataset, yielding an overly-optimistic score.\n\nModel selection without nested CV uses the same data to tune model parameters\nand evaluate model performance. Information may thus \"leak\" into the model\nand overfit the data. The magnitude of this effect is primarily dependent on\nthe size of the dataset and the stability of the model. See Cawley and Talbot\n[1]_ for an analysis of these issues.\n\nTo avoid this problem, nested CV effectively uses a series of\ntrain/validation/test set splits. In the inner loop (here executed by\n:class:`GridSearchCV <sklearn.model_selection.GridSearchCV>`), the score is\napproximately maximized by fitting a model to each training set, and then\ndirectly maximized in selecting (hyper)parameters over the validation set. In\nthe outer loop (here in :func:`cross_val_score\n<sklearn.model_selection.cross_val_score>`), generalization error is estimated\nby averaging test set scores over several dataset splits.\n\nThe example below uses a support vector classifier with a non-linear kernel to\nbuild a model with optimized hyperparameters by grid search. We compare the\nperformance of non-nested and nested CV strategies by taking the difference\nbetween their scores.\n\n.. seealso::\n\n    - :ref:`cross_validation`\n    - :ref:`grid_search`\n\n.. rubric:: References\n\n.. [1] `Cawley, G.C.; Talbot, N.L.C. On over-fitting in model selection and\n    subsequent selection bias in performance evaluation.\n    J. Mach. Learn. Res 2010,11, 2079-2107.\n    <"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "plot_nested_cross_validation_iris.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "http://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf>`_\n\n\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.svm import SVC\n\n# Number of random trials\nNUM_TRIALS = 30\n\n# Load the dataset\niris = load_iris()\nX_iris = iris.data\ny_iris = iris.target\n\n# Set up possible values of parameters to optimize over\np_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}\n\n# We will use a Support Vector Classifier with \"rbf\" kernel\nsvm = SVC(kernel=\"rbf\")\n\n# Arrays to store scores\nnon_nested_scores = np.zeros(NUM_TRIALS)\nnested_scores = np.zeros(NUM_TRIALS)\n\n# Loop for each trial\nfor i in range(NUM_TRIALS):\n    # Choose cross-validation techniques for the inner and outer loops,\n    # independently of the dataset.\n    # E.g \"GroupKFold\", \"LeaveOneOut\", \"LeaveOneGroupOut\", etc.\n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n\n    # Non_nested parameter search and scoring\n    clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=outer_cv)\n    clf.fit(X_iris, y_iris)\n    non_nested_scores[i] = clf.best_score_\n\n    # Nested CV with parameter optimization\n    clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=inner_cv)\n    nested_score = cross_val_score(clf, X=X_iris, y=y_iris, cv=outer_cv)\n    nested_scores[i] = nested_score.mean()\n\nscore_difference = non_nested_scores - nested_scores\n\nprint(\n    \"Average difference of {:6f} with std. dev. of {:6f}.\".format(\n        score_difference.mean(), score_difference.std()\n    )\n)\n\n# Plot scores on each trial for nested and non-nested CV\nplt.figure()\nplt.subplot(211)\n(non_nested_scores_line,) = plt.plot(non_nested_scores, color=\"r\")\n(nested_line,) = plt.plot(nested_scores, color=\"b\")\nplt.ylabel(\"score\", fontsize=\"14\")\nplt.legend(\n    [non_neste"}, {"start_line": 62000, "end_line": 64000, "belongs_to": {"file_name": "_search.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "mport svm, datasets\n    >>> from sklearn.model_selection import GridSearchCV\n    >>> iris = datasets.load_iris()\n    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n    >>> svc = svm.SVC()\n    >>> clf = GridSearchCV(svc, parameters)\n    >>> clf.fit(iris.data, iris.target)\n    GridSearchCV(estimator=SVC(),\n                 param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n    >>> sorted(clf.cv_results_.keys())\n    ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n     'param_C', 'param_kernel', 'params',...\n     'rank_test_score', 'split0_test_score',...\n     'split2_test_score', ...\n     'std_fit_time', 'std_score_time', 'std_test_score']\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **BaseSearchCV._parameter_constraints,\n        \"param_grid\": [dict, list],\n    }\n\n    def __init__(\n        self,\n        estimator,\n        param_grid,\n        *,\n        scoring=None,\n        n_jobs=None,\n        refit=True,\n        cv=None,\n        verbose=0,\n        pre_dispatch=\"2*n_jobs\",\n        error_score=np.nan,\n        return_train_score=False,\n    ):\n        super().__init__(\n            estimator=estimator,\n            scoring=scoring,\n            n_jobs=n_jobs,\n            refit=refit,\n            cv=cv,\n            verbose=verbose,\n            pre_dispatch=pre_dispatch,\n            error_score=error_score,\n            return_train_score=return_train_score,\n        )\n        self.param_grid = param_grid\n\n    def _run_search(self, evaluate_candidates):\n        \"\"\"Search all candidates in param_grid\"\"\"\n        evaluate_candidates(ParameterGrid(self.param_grid))\n\n\nclass RandomizedSearchCV(BaseSearchCV):\n    \"\"\"Randomized search on hyper parameters.\n\n    RandomizedSearchCV implements a \"fit\" and a \"score\" method.\n    It also implements \"score_samples\", \"predict\", \"predict_proba\",\n    \"decision_function\", \"transform\" and \"inverse_transform\" if they are\n    implemented in the estimator used.\n\n    The parameters of the estimator used to apply the"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "_search.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tion._validation import (\n    _aggregate_score_dicts,\n    _fit_and_score,\n    _insert_error_scores,\n    _normalize_score_results,\n    _warn_or_raise_about_fit_failures,\n)\nfrom sklearn.utils import Bunch, check_random_state\nfrom sklearn.utils._param_validation import HasMethods, Interval, StrOptions\nfrom sklearn.utils._repr_html.estimator import _VisualBlock\nfrom sklearn.utils._tags import get_tags\nfrom sklearn.utils.metadata_routing import (\n    MetadataRouter,\n    MethodMapping,\n    _raise_for_params,\n    _routing_enabled,\n    process_routing,\n)\nfrom sklearn.utils.metaestimators import available_if\nfrom sklearn.utils.parallel import Parallel, delayed\nfrom sklearn.utils.random import sample_without_replacement\nfrom sklearn.utils.validation import _check_method_params, check_is_fitted, indexable\n\n__all__ = [\"GridSearchCV\", \"ParameterGrid\", \"ParameterSampler\", \"RandomizedSearchCV\"]\n\n\nclass ParameterGrid:\n    \"\"\"Grid of parameters with a discrete number of values for each.\n\n    Can be used to iterate over parameter value combinations with the\n    Python built-in function iter.\n    The order of the generated parameter combinations is deterministic.\n\n    Read more in the :ref:`User Guide <grid_search>`.\n\n    Parameters\n    ----------\n    param_grid : dict of str to sequence, or sequence of such\n        The parameter grid to explore, as a dictionary mapping estimator\n        parameters to sequences of allowed values.\n\n        An empty dict signifies default parameters.\n\n        A sequence of dicts signifies a sequence of grids to search, and is\n        useful to avoid exploring parameter combinations that make no sense\n        or have no effect. See the examples below.\n\n    Examples\n    --------\n    >>> from sklearn.model_selection import ParameterGrid\n    >>> param_grid = {'a': [1, 2], 'b': [True, False]}\n    >>> list(ParameterGrid(param_grid)) == (\n    ...    [{'a': 1, 'b': True}, {'a': 1, 'b': False},\n    ...     {'a': 2, 'b': True}, {'a': 2, 'b': False}])\n    True\n\n  "}], "retrieved_count": 10, "cost_time": 1.0536036491394043}
{"question": "Why does Scikit-learn implement sparse matrix support to improve memory efficiency for high-dimensional data?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn implements sparse matrix support to improve memory efficiency for high-dimensional data for several fundamental design reasons that enhance performance, scalability, and practical usability:\n\n1. **Memory Efficiency for Sparse Data**:\n   - **Zero Storage Elimination**: Only stores non-zero values, dramatically reducing memory usage\n   - **Compressed Storage**: Uses compressed formats (CSR, CSC) that store only essential data\n   - **High-Dimensional Support**: Enables handling of datasets with millions of features\n   - **Memory Scaling**: Memory usage scales with actual data content, not feature count\n   - **Storage Optimization**: Optimized storage for datasets with high sparsity ratios\n\n2. **Performance Benefits**:\n   - **Faster Computations**: Only processes non-zero elements in mathematical operations\n   - **Efficient Dot Products**: Sparse dot products are much faster for sparse data\n   - **Reduced I/O**: Less data transfer between memory and CPU\n   - **Cache Efficiency**: Better cache utilization with smaller memory footprint\n   - **Parallel Processing**: Efficient parallel processing of sparse operations\n\n3. **Text and NLP Applications**:\n   - **Document-Term Matrices**: Efficient representation of text document features\n   - **Bag-of-Words**: Natural sparse representation for text classification\n   - **TF-IDF Matrices**: Sparse representation of term frequency-inverse document frequency\n   - **Vocabulary Scaling**: Can handle vocabularies with millions of unique terms\n   - **Memory-Efficient Text Processing**: Enables processing of large text corpora\n\n4. **High-Dimensional Feature Spaces**:\n   - **Feature Hashing**: Efficient representation of high-dimensional feature spaces\n   - **One-Hot Encoding**: Sparse representation of categorical variables\n   - **Polynomial Features**: Efficient handling of polynomial feature expansions\n   - **Kernel Approximations**: Memory-efficient kernel matrix approximations\n   - **Random Projections**: Sparse random projection matrices for dimensionality reduction\n\n5. **Algorithm-Specific Optimizations**:\n   - **Linear Models**: Optimized sparse matrix operations for linear classifiers/regressors\n   - **Sparse Coefficients**: Model coefficients can be stored in sparse format\n   - **Feature Selection**: Efficient handling of feature selection results\n   - **Ensemble Methods**: Sparse representation of tree-based model predictions\n   - **Clustering**: Efficient distance computations for sparse data\n\n6. **Scalability and Big Data**:\n   - **Large-Scale Datasets**: Can handle datasets that don't fit in memory in dense format\n   - **Out-of-Core Processing**: Enables processing of datasets larger than available RAM\n   - **Distributed Computing**: Efficient data distribution in distributed environments\n   - **Cloud Computing**: Reduced memory costs in cloud computing environments\n   - **Production Deployment**: Lower memory requirements for production systems\n\n7. **Format Flexibility**:\n   - **Multiple Formats**: Support for CSR, CSC, COO, LIL, and other sparse formats\n   - **Format Conversion**: Automatic conversion between formats for optimal performance\n   - **Format Selection**: Intelligent selection of optimal format for each operation\n   - **Interoperability**: Seamless integration with scipy.sparse and other libraries\n   - **Backward Compatibility**: Maintains compatibility with dense array operations\n\n8. **Computational Efficiency**:\n   - **Sparse Algorithms**: Specialized algorithms that exploit sparsity patterns\n   - **Efficient Indexing**: Fast indexing and slicing operations on sparse matrices\n   - **Matrix Operations**: Optimized matrix multiplication, addition, and other operations\n   - **Vector Operations**: Efficient vector operations on sparse data\n   - **Statistical Computations**: Fast computation of statistics on sparse data\n\n9. **Real-World Applications**:\n   - **Recommendation Systems**: Sparse user-item matrices for collaborative filtering\n   - **Bioinformatics**: Sparse representation of biological sequence features\n   - **Image Processing**: Sparse representation of image features and descriptors\n   - **Network Analysis**: Sparse adjacency matrices for graph analysis\n   - **Signal Processing**: Sparse representation of signal features\n\n10. **Future-Proofing and Extensibility**:\n    - **Growing Data Sizes**: Prepared for increasingly large datasets\n    - **New Applications**: Enables new applications that require high-dimensional data\n    - **Hardware Optimization**: Takes advantage of hardware optimizations for sparse operations\n    - **Research Support**: Supports cutting-edge research in high-dimensional machine learning\n    - **Industry Adoption**: Enables industry adoption of machine learning for large-scale problems", "score": null, "retrieved_content": [{"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "test_sparse.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/svm/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " does not support large sparse, so we specify int32 indices\n    # In this case, `csr_matrix` automatically uses int32 regardless of the dtypes of\n    # `indices` and `indptr` but `csr_array` may or may not use the same dtype as\n    # `indices` and `indptr`, which would be int64 if not specified\n    indices = np.array([6, 5, 35, 31], dtype=np.int32)\n    indptr = np.array([0] * 8 + [1] * 32 + [2] * 38 + [4] * 3, dtype=np.int32)\n\n    X = csr_container((data, indices, indptr))\n    y = np.array(\n        [\n            1.0,\n            0.0,\n            2.0,\n            2.0,\n            1.0,\n            1.0,\n            1.0,\n            2.0,\n            2.0,\n            0.0,\n            1.0,\n            2.0,\n            2.0,\n            0.0,\n            2.0,\n            0.0,\n            3.0,\n            0.0,\n            3.0,\n            0.0,\n            1.0,\n            1.0,\n            3.0,\n            2.0,\n            3.0,\n            2.0,\n            0.0,\n            3.0,\n            1.0,\n            0.0,\n            2.0,\n            1.0,\n            2.0,\n            0.0,\n            1.0,\n            0.0,\n            2.0,\n            3.0,\n            1.0,\n            3.0,\n            0.0,\n            1.0,\n            0.0,\n            0.0,\n            2.0,\n            0.0,\n            1.0,\n            2.0,\n            2.0,\n            2.0,\n            3.0,\n            2.0,\n            0.0,\n            3.0,\n            2.0,\n            1.0,\n            2.0,\n            3.0,\n            2.0,\n            2.0,\n            0.0,\n            1.0,\n            0.0,\n            1.0,\n            2.0,\n            3.0,\n            0.0,\n            0.0,\n            2.0,\n            2.0,\n            1.0,\n            3.0,\n            1.0,\n            1.0,\n            0.0,\n            1.0,\n            2.0,\n            1.0,\n            1.0,\n            3.0,\n        ]\n    )\n\n    clf = svm.SVC(kernel=\"linear\").fit(X.toarray(), y)\n    sp_clf = svm.SVC(kernel=\"linear\").fit(X.tocoo(), y)\n\n    a"}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "se matrix, which for\n        L1-regularized models can be much more memory- and storage-efficient\n        than the usual numpy.ndarray representation.\n\n        The ``intercept_`` member is not converted.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.\n        \"\"\"\n        msg = \"Estimator, %(name)s, must be fitted before sparsifying.\"\n        check_is_fitted(self, msg=msg)\n        self.coef_ = sp.csr_matrix(self.coef_)\n        return self\n\n\nclass LinearRegression(MultiOutputMixin, RegressorMixin, LinearModel):\n    \"\"\"\n    Ordinary least squares Linear Regression.\n\n    LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n    to minimize the residual sum of squares between the observed targets in\n    the dataset, and the targets predicted by the linear approximation.\n\n    Parameters\n    ----------\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    tol : float, default=1e-6\n        The precision of the solution (`coef_`) is determined by `tol` which\n        specifies a different convergence criterion for the `lsqr` solver.\n        `tol` is set as `atol` and `btol` of `scipy.sparse.linalg.lsqr` when\n        fitting on sparse training data. This parameter has"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "extmath.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "determinant\n        of an array.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils.extmath import fast_logdet\n    >>> a = np.array([[5, 1], [2, 8]])\n    >>> fast_logdet(a)\n    np.float64(3.6375861597263857)\n    \"\"\"\n    xp, _ = get_namespace(A)\n    sign, ld = xp.linalg.slogdet(A)\n    if not sign > 0:\n        return -xp.inf\n    return ld\n\n\ndef density(w):\n    \"\"\"Compute density of a sparse vector.\n\n    Parameters\n    ----------\n    w : {ndarray, sparse matrix}\n        The input data can be numpy ndarray or a sparse matrix.\n\n    Returns\n    -------\n    float\n        The density of w, between 0 and 1.\n\n    Examples\n    --------\n    >>> from scipy import sparse\n    >>> from sklearn.utils.extmath import density\n    >>> X = sparse.random(10, 10, density=0.25, random_state=0)\n    >>> density(X)\n    0.25\n    \"\"\"\n    if hasattr(w, \"toarray\"):\n        d = float(w.nnz) / (w.shape[0] * w.shape[1])\n    else:\n        d = 0 if w is None else float((w != 0).sum()) / w.size\n    return d\n\n\ndef safe_sparse_dot(a, b, *, dense_output=False):\n    \"\"\"Dot product that handle the sparse matrix case correctly.\n\n    Parameters\n    ----------\n    a : {ndarray, sparse matrix}\n    b : {ndarray, sparse matrix}\n    dense_output : bool, default=False\n        When False, ``a`` and ``b`` both being sparse will yield sparse output.\n        When True, output will always be a dense array.\n\n    Returns\n    -------\n    dot_product : {ndarray, sparse matrix}\n        Sparse if ``a`` and ``b`` are sparse and ``dense_output=False``.\n\n    Examples\n    --------\n    >>> from scipy.sparse import csr_matrix\n    >>> from sklearn.utils.extmath import safe_sparse_dot\n    >>> X = csr_matrix([[1, 2], [3, 4], [5, 6]])\n    >>> dot_product = safe_sparse_dot(X, X.T)\n    >>> dot_product.toarray()\n    array([[ 5, 11, 17],\n           [11, 25, 39],\n           [17, 39, 61]])\n    \"\"\"\n    xp, _ = get_namespace(a, b)\n    if a.ndim > 2 or b.ndim > 2:\n        if sparse.issparse(a):\n            # sparse "}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "(prob.shape[0], -1))\n            return prob\n\n\nclass SparseCoefMixin:\n    \"\"\"Mixin for converting coef_ to and from CSR format.\n\n    L1-regularizing estimators should inherit this.\n    \"\"\"\n\n    def densify(self):\n        \"\"\"\n        Convert coefficient matrix to dense array format.\n\n        Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n        default format of ``coef_`` and is required for fitting, so calling\n        this method is only required on models that have previously been\n        sparsified; otherwise, it is a no-op.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        msg = \"Estimator, %(name)s, must be fitted before densifying.\"\n        check_is_fitted(self, msg=msg)\n        if sp.issparse(self.coef_):\n            self.coef_ = self.coef_.toarray()\n        return self\n\n    def sparsify(self):\n        \"\"\"\n        Convert coefficient matrix to sparse format.\n\n        Converts the ``coef_`` member to a scipy.sparse matrix, which for\n        L1-regularized models can be much more memory- and storage-efficient\n        than the usual numpy.ndarray representation.\n\n        The ``intercept_`` member is not converted.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.\n        \"\"\"\n        msg = \"Estimator, %(name)s, must be fitted before sparsifying.\"\n        check_is_fitted(self, msg=msg)\n        self.coef_ = sp.csr_matrix(self.coef_)\n        return self\n\n\nclass Linea"}, {"start_line": 94000, "end_line": 96000, "belongs_to": {"file_name": "_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "X = resample(\n                X, replace=False, n_samples=self.subsample, random_state=random_state\n            )\n\n        self.quantiles_ = np.nanpercentile(X, references, axis=0)\n        # Due to floating-point precision error in `np.nanpercentile`,\n        # make sure that quantiles are monotonically increasing.\n        # Upstream issue in numpy:\n        # https://github.com/numpy/numpy/issues/14685\n        self.quantiles_ = np.maximum.accumulate(self.quantiles_)\n\n    def _sparse_fit(self, X, random_state):\n        \"\"\"Compute percentiles for sparse matrices.\n\n        Parameters\n        ----------\n        X : sparse matrix of shape (n_samples, n_features)\n            The data used to scale along the features axis. The sparse matrix\n            needs to be nonnegative. If a sparse matrix is provided,\n            it will be converted into a sparse ``csc_matrix``.\n        \"\"\"\n        n_samples, n_features = X.shape\n        references = self.references_ * 100\n\n        self.quantiles_ = []\n        for feature_idx in range(n_features):\n            column_nnz_data = X.data[X.indptr[feature_idx] : X.indptr[feature_idx + 1]]\n            if self.subsample is not None and len(column_nnz_data) > self.subsample:\n                column_subsample = self.subsample * len(column_nnz_data) // n_samples\n                if self.ignore_implicit_zeros:\n                    column_data = np.zeros(shape=column_subsample, dtype=X.dtype)\n                else:\n                    column_data = np.zeros(shape=self.subsample, dtype=X.dtype)\n                column_data[:column_subsample] = random_state.choice(\n                    column_nnz_data, size=column_subsample, replace=False\n                )\n            else:\n                if self.ignore_implicit_zeros:\n                    column_data = np.zeros(shape=len(column_nnz_data), dtype=X.dtype)\n                else:\n                    column_data = np.zeros(shape=n_samples, dtype=X.dtype)\n                column_data[: len(column_nnz_data)] "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "extmath.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n        if _is_numpy_namespace(xp):\n            X = np.asarray(X)\n            norms = np.einsum(\"ij,ij->i\", X, X)\n            norms = xp.asarray(norms)\n        else:\n            norms = xp.sum(xp.multiply(X, X), axis=1)\n        if not squared:\n            norms = xp.sqrt(norms)\n    return norms\n\n\ndef fast_logdet(A):\n    \"\"\"Compute logarithm of determinant of a square matrix.\n\n    The (natural) logarithm of the determinant of a square matrix\n    is returned if det(A) is non-negative and well defined.\n    If the determinant is zero or negative returns -Inf.\n\n    Equivalent to : np.log(np.det(A)) but more robust.\n\n    Parameters\n    ----------\n    A : array_like of shape (n, n)\n        The square matrix.\n\n    Returns\n    -------\n    logdet : float\n        When det(A) is strictly positive, log(det(A)) is returned.\n        When det(A) is non-positive or not defined, then -inf is returned.\n\n    See Also\n    --------\n    numpy.linalg.slogdet : Compute the sign and (natural) logarithm of the determinant\n        of an array.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils.extmath import fast_logdet\n    >>> a = np.array([[5, 1], [2, 8]])\n    >>> fast_logdet(a)\n    np.float64(3.6375861597263857)\n    \"\"\"\n    xp, _ = get_namespace(A)\n    sign, ld = xp.linalg.slogdet(A)\n    if not sign > 0:\n        return -xp.inf\n    return ld\n\n\ndef density(w):\n    \"\"\"Compute density of a sparse vector.\n\n    Parameters\n    ----------\n    w : {ndarray, sparse matrix}\n        The input data can be numpy ndarray or a sparse matrix.\n\n    Returns\n    -------\n    float\n        The density of w, between 0 and 1.\n\n    Examples\n    --------\n    >>> from scipy import sparse\n    >>> from sklearn.utils.extmath import density\n    >>> X = sparse.random(10, 10, density=0.25, random_state=0)\n    >>> density(X)\n    0.25\n    \"\"\"\n    if hasattr(w, \"toarray\"):\n        d = float(w.nnz) / (w.shape[0] * w.shape[1])\n    else:\n        d = 0 if w is None else float((w != 0).sum()) / w.siz"}, {"start_line": 21000, "end_line": 23000, "belongs_to": {"file_name": "random_projection.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "to dense random\n    projection matrix that guarantees similar embedding quality while being\n    much more memory efficient and allowing faster computation of the\n    projected data.\n\n    If we note `s = 1 / density` the components of the random matrix are\n    drawn from:\n\n    .. code-block:: text\n\n      -sqrt(s) / sqrt(n_components)   with probability 1 / 2s\n       0                              with probability 1 - 1 / s\n      +sqrt(s) / sqrt(n_components)   with probability 1 / 2s\n\n    Read more in the :ref:`User Guide <sparse_random_matrix>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    n_components : int or 'auto', default='auto'\n        Dimensionality of the target projection space.\n\n        n_components can be automatically adjusted according to the\n        number of samples in the dataset and the bound given by the\n        Johnson-Lindenstrauss lemma. In that case the quality of the\n        embedding is controlled by the ``eps`` parameter.\n\n        It should be noted that Johnson-Lindenstrauss lemma can yield\n        very conservative estimated of the required number of components\n        as it makes no assumption on the structure of the dataset.\n\n    density : float or 'auto', default='auto'\n        Ratio in the range (0, 1] of non-zero component in the random\n        projection matrix.\n\n        If density = 'auto', the value is set to the minimum density\n        as recommended by Ping Li et al.: 1 / sqrt(n_features).\n\n        Use density = 1 / 3.0 if you want to reproduce the results from\n        Achlioptas, 2001.\n\n    eps : float, default=0.1\n        Parameter to control the quality of the embedding according to\n        the Johnson-Lindenstrauss lemma when n_components is set to\n        'auto'. This value should be strictly positive.\n\n        Smaller values lead to better embedding and higher number of\n        dimensions (n_components) in the target projection space.\n\n    dense_output : bool, default=False\n        If True, ensure that the"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "random_projection.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "matrix will be of CSR format.\n\n        \"\"\"\n\n    def _compute_inverse_components(self):\n        \"\"\"Compute the pseudo-inverse of the (densified) components.\"\"\"\n        components = self.components_\n        if sp.issparse(components):\n            components = components.toarray()\n        return linalg.pinv(components, check_finite=False)\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Generate a sparse random projection matrix.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Training set: only the shape is used to find optimal random\n            matrix dimensions based on the theory referenced in the\n            afore mentioned papers.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            BaseRandomProjection class instance.\n        \"\"\"\n        X = validate_data(\n            self, X, accept_sparse=[\"csr\", \"csc\"], dtype=[np.float64, np.float32]\n        )\n\n        n_samples, n_features = X.shape\n\n        if self.n_components == \"auto\":\n            self.n_components_ = johnson_lindenstrauss_min_dim(\n                n_samples=n_samples, eps=self.eps\n            )\n\n            if self.n_components_ <= 0:\n                raise ValueError(\n                    \"eps=%f and n_samples=%d lead to a target dimension of \"\n                    \"%d which is invalid\" % (self.eps, n_samples, self.n_components_)\n                )\n\n            elif self.n_components_ > n_features:\n                raise ValueError(\n                    \"eps=%f and n_samples=%d lead to a target dimension of \"\n                    \"%d which is larger than the original space with \"\n                    \"n_features=%d\"\n                    % (self.eps, n_samples, self.n_components_, n_features)\n                )\n        else:\n            if self.n_components > n_features:\n                warni"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_sparsefuncs.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "import numpy as np\nimport pytest\nimport scipy.sparse as sp\nfrom numpy.random import RandomState\nfrom numpy.testing import assert_array_almost_equal, assert_array_equal\nfrom scipy import linalg\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.utils._testing import assert_allclose\nfrom sklearn.utils.fixes import CSC_CONTAINERS, CSR_CONTAINERS, LIL_CONTAINERS\nfrom sklearn.utils.sparsefuncs import (\n    _implicit_column_offset,\n    count_nonzero,\n    csc_median_axis_0,\n    incr_mean_variance_axis,\n    inplace_column_scale,\n    inplace_row_scale,\n    inplace_swap_column,\n    inplace_swap_row,\n    mean_variance_axis,\n    min_max_axis,\n)\nfrom sklearn.utils.sparsefuncs_fast import (\n    assign_rows_csr,\n    csr_row_norms,\n    inplace_csr_row_normalize_l1,\n    inplace_csr_row_normalize_l2,\n)\n\n\n@pytest.mark.parametrize(\"csc_container\", CSC_CONTAINERS)\n@pytest.mark.parametrize(\"csr_container\", CSR_CONTAINERS)\n@pytest.mark.parametrize(\"lil_container\", LIL_CONTAINERS)\ndef test_mean_variance_axis0(csc_container, csr_container, lil_container):\n    X, _ = make_classification(5, 4, random_state=0)\n    # Sparsify the array a little bit\n    X[0, 0] = 0\n    X[2, 1] = 0\n    X[4, 3] = 0\n    X_lil = lil_container(X)\n    X_lil[1, 0] = 0\n    X[1, 0] = 0\n\n    with pytest.raises(TypeError):\n        mean_variance_axis(X_lil, axis=0)\n\n    X_csr = csr_container(X_lil)\n    X_csc = csc_container(X_lil)\n\n    expected_dtypes = [\n        (np.float32, np.float32),\n        (np.float64, np.float64),\n        (np.int32, np.float64),\n        (np.int64, np.float64),\n    ]\n\n    for input_dtype, output_dtype in expected_dtypes:\n        X_test = X.astype(input_dtype)\n        for X_sparse in (X_csr, X_csc):\n            X_sparse = X_sparse.astype(input_dtype)\n            X_means, X_vars = mean_variance_axis(X_sparse, axis=0)\n            assert X_means.dtype == output_dtype\n            assert X_vars.dtype == output_dtype\n            assert_array_almost_equal(X_means, np.mean(X_test, axis=0))\n       "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_extmath.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport itertools\n\nimport numpy as np\nimport pytest\nfrom scipy import linalg, sparse\nfrom scipy.linalg import eigh\nfrom scipy.sparse.linalg import eigsh\n\nfrom sklearn import config_context\nfrom sklearn.datasets import make_low_rank_matrix, make_sparse_spd_matrix\nfrom sklearn.utils import gen_batches\nfrom sklearn.utils._arpack import _init_arpack_v0\nfrom sklearn.utils._array_api import (\n    _convert_to_numpy,\n    _get_namespace_device_dtype_ids,\n    get_namespace,\n    yield_namespace_device_dtype_combinations,\n)\nfrom sklearn.utils._testing import (\n    _array_api_for_tests,\n    assert_allclose,\n    assert_allclose_dense_sparse,\n    assert_almost_equal,\n    assert_array_almost_equal,\n    assert_array_equal,\n    skip_if_32bit,\n)\nfrom sklearn.utils.extmath import (\n    _approximate_mode,\n    _deterministic_vector_sign_flip,\n    _incremental_mean_and_var,\n    _randomized_eigsh,\n    _safe_accumulator_op,\n    cartesian,\n    density,\n    randomized_range_finder,\n    randomized_svd,\n    row_norms,\n    safe_sparse_dot,\n    softmax,\n    stable_cumsum,\n    svd_flip,\n    weighted_mode,\n)\nfrom sklearn.utils.fixes import (\n    COO_CONTAINERS,\n    CSC_CONTAINERS,\n    CSR_CONTAINERS,\n    DOK_CONTAINERS,\n    LIL_CONTAINERS,\n    _mode,\n)\n\n\n@pytest.mark.parametrize(\n    \"sparse_container\",\n    COO_CONTAINERS + CSC_CONTAINERS + CSR_CONTAINERS + LIL_CONTAINERS,\n)\ndef test_density(sparse_container):\n    rng = np.random.RandomState(0)\n    X = rng.randint(10, size=(10, 5))\n    X[1, 2] = 0\n    X[5, 3] = 0\n\n    assert density(sparse_container(X)) == density(X)\n\n\ndef test_uniform_weights():\n    # with uniform weights, results should be identical to stats.mode\n    rng = np.random.RandomState(0)\n    x = rng.randint(10, size=(10, 5))\n    weights = np.ones(x.shape)\n\n    for axis in (None, 0, 1):\n        mode, score = _mode(x, axis)\n        mode2, score2 = weighted_mode(x, weights, axis=axis)\n\n        assert_array_equal"}], "retrieved_count": 10, "cost_time": 1.0372748374938965}
{"question": "Why does Scikit-learn use joblib for parallel processing instead of Python's multiprocessing directly?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn uses joblib for parallel processing instead of Python's multiprocessing directly for several fundamental design reasons that enhance reliability, performance, and ease of use:\n\n1. **Enhanced Reliability and Safety**:\n   - **Fork-Safety**: Joblib handles fork-safety issues that can cause crashes on macOS and Linux\n   - **Process Management**: Better process lifecycle management and cleanup\n   - **Error Handling**: Robust error handling and recovery mechanisms\n   - **Memory Management**: Intelligent memory management with memmap for large datasets\n   - **Resource Cleanup**: Automatic cleanup of resources and processes\n\n2. **Advanced Backend Support**:\n   - **Multiple Backends**: Supports both multiprocessing and multithreading backends\n   - **Loky Backend**: Default backend that provides better process isolation\n   - **Threading Backend**: Efficient threading when GIL is released\n   - **Custom Backends**: Support for custom parallel processing backends\n   - **Backend Selection**: Automatic selection of optimal backend based on task\n\n3. **Memory Efficiency**:\n   - **Shared Memory**: Efficient shared memory management for large datasets\n   - **Memmap Support**: Automatic use of memory mapping for datasets > 1MB\n   - **Memory Pooling**: Shared memory pools to reduce memory overhead\n   - **Copy Avoidance**: Minimizes unnecessary data copying between processes\n   - **Memory Monitoring**: Built-in memory usage monitoring and optimization\n\n4. **Performance Optimizations**:\n   - **Load Balancing**: Intelligent load balancing across workers\n   - **Batch Processing**: Efficient batch processing of tasks\n   - **Caching**: Built-in caching of expensive computations\n   - **Pre-dispatch**: Configurable pre-dispatch to optimize task distribution\n   - **Worker Pooling**: Efficient worker pool management and reuse\n\n5. **Oversubscription Prevention**:\n   - **Thread Limiting**: Automatic limiting of threads to prevent oversubscription\n   - **Resource Coordination**: Coordinates with OpenMP, BLAS, and other parallel libraries\n   - **Environment Variables**: Automatic setting of thread limits via environment variables\n   - **CPU Awareness**: Aware of CPU topology and limitations\n   - **Performance Tuning**: Automatic performance tuning based on system capabilities\n\n6. **Ease of Use and API Design**:\n   - **Simple API**: Simple and intuitive parallel processing API\n   - **Context Managers**: Easy-to-use context managers for backend selection\n   - **Configuration**: Flexible configuration options\n   - **Debugging Support**: Better debugging and error reporting\n   - **Documentation**: Comprehensive documentation and examples\n\n7. **Cross-Platform Compatibility**:\n   - **Platform Independence**: Works consistently across different operating systems\n   - **Process Start Methods**: Handles different process start methods (fork, spawn, forkserver)\n   - **Environment Adaptation**: Adapts to different computing environments\n   - **Cloud Compatibility**: Optimized for cloud computing environments\n   - **Container Support**: Works well in containerized environments\n\n8. **Integration with Scientific Computing**:\n   - **NumPy Integration**: Optimized integration with NumPy arrays\n   - **SciPy Compatibility**: Compatible with SciPy's parallel processing patterns\n   - **Scientific Workflows**: Designed for scientific computing workflows\n   - **Research Support**: Supports reproducible research and benchmarking\n   - **Academic Use**: Widely used in academic and research environments\n\n9. **Production Readiness**:\n   - **Production Deployment**: Production-ready parallel processing capabilities\n   - **Monitoring**: Built-in monitoring and performance tracking\n   - **Scalability**: Scales well from development to production environments\n   - **Reliability**: Proven reliability in production systems\n   - **Maintenance**: Active maintenance and continuous improvement\n\n10. **Community and Ecosystem Benefits**:\n    - **Wide Adoption**: Widely adopted in the Python scientific computing community\n    - **Active Development**: Active development and community support\n    - **Best Practices**: Implements best practices for parallel processing\n    - **Interoperability**: Works well with other scientific computing tools\n    - **Future-Proofing**: Continuously evolving to support new parallel processing paradigms", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "parallel.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Customizations of :mod:`joblib` and :mod:`threadpoolctl` tools for scikit-learn\nusage.\n\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport functools\nimport warnings\nfrom functools import update_wrapper\n\nimport joblib\nfrom threadpoolctl import ThreadpoolController\n\nfrom sklearn._config import config_context, get_config\n\n# Global threadpool controller instance that can be used to locally limit the number of\n# threads without looping through all shared libraries every time.\n# It should not be accessed directly and _get_threadpool_controller should be used\n# instead.\n_threadpool_controller = None\n\n\ndef _with_config_and_warning_filters(delayed_func, config, warning_filters):\n    \"\"\"Helper function that intends to attach a config to a delayed function.\"\"\"\n    if hasattr(delayed_func, \"with_config_and_warning_filters\"):\n        return delayed_func.with_config_and_warning_filters(config, warning_filters)\n    else:\n        warnings.warn(\n            (\n                \"`sklearn.utils.parallel.Parallel` needs to be used in \"\n                \"conjunction with `sklearn.utils.parallel.delayed` instead of \"\n                \"`joblib.delayed` to correctly propagate the scikit-learn \"\n                \"configuration to the joblib workers.\"\n            ),\n            UserWarning,\n        )\n        return delayed_func\n\n\nclass Parallel(joblib.Parallel):\n    \"\"\"Tweak of :class:`joblib.Parallel` that propagates the scikit-learn configuration.\n\n    This subclass of :class:`joblib.Parallel` ensures that the active configuration\n    (thread-local) of scikit-learn is propagated to the parallel workers for the\n    duration of the execution of the parallel tasks.\n\n    The API does not change and you can refer to :class:`joblib.Parallel`\n    documentation for more details.\n\n    .. versionadded:: 1.3\n    \"\"\"\n\n    def __call__(self, iterable):\n        \"\"\"Dispatch the tasks and return the results.\n\n        Parameters\n        ----------\n        iterable : it"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_parallel.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "import time\nimport warnings\n\nimport joblib\nimport numpy as np\nimport pytest\nfrom numpy.testing import assert_array_equal\n\nfrom sklearn import config_context, get_config\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.fixes import _IS_WASM\nfrom sklearn.utils.parallel import Parallel, delayed\n\n\ndef get_working_memory():\n    return get_config()[\"working_memory\"]\n\n\n@pytest.mark.parametrize(\"n_jobs\", [1, 2])\n@pytest.mark.parametrize(\"backend\", [\"loky\", \"threading\", \"multiprocessing\"])\ndef test_configuration_passes_through_to_joblib(n_jobs, backend):\n    # Tests that the global global configuration is passed to joblib jobs\n\n    with config_context(working_memory=123):\n        results = Parallel(n_jobs=n_jobs, backend=backend)(\n            delayed(get_working_memory)() for _ in range(2)\n        )\n\n    assert_array_equal(results, [123] * 2)\n\n\ndef test_parallel_delayed_warnings():\n    \"\"\"Informative warnings should be raised when mixing sklearn and joblib API\"\"\"\n    # We should issue a warning when one wants to use sklearn.utils.fixes.Parallel\n    # with joblib.delayed. The config will not be propagated to the workers.\n    warn_msg = \"`sklearn.utils.parallel.Parallel` needs to be used in conjunction\"\n    with pytest.warns(UserWarning, match=warn_msg) as records:\n        Parallel()(joblib.delayed(time.sleep)(0) for _ in range(10))\n    assert len(records) == 10\n\n    # We should issue a warning if one wants to use sklearn.utils.fixes.delayed with\n    # joblib.Parallel\n    warn_msg = (\n        \"`sklearn.utils.parallel.delayed` should be used with \"\n        \"`sklearn.utils.parallel.Parallel` to make it possible to propagate\"\n    )\n    with pytest.warns(UserWarning, match=warn_msg)"}, {"start_line": 0, "end_line": 1243, "belongs_to": {"file_name": "bench_plot_parallel_pairwise.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/benchmarks", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport time\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics.pairwise import pairwise_distances, pairwise_kernels\nfrom sklearn.utils import check_random_state\n\n\ndef plot(func):\n    random_state = check_random_state(0)\n    one_core = []\n    multi_core = []\n    sample_sizes = range(1000, 6000, 1000)\n\n    for n_samples in sample_sizes:\n        X = random_state.rand(n_samples, 300)\n\n        start = time.time()\n        func(X, n_jobs=1)\n        one_core.append(time.time() - start)\n\n        start = time.time()\n        func(X, n_jobs=-1)\n        multi_core.append(time.time() - start)\n\n    plt.figure(\"scikit-learn parallel %s benchmark results\" % func.__name__)\n    plt.plot(sample_sizes, one_core, label=\"one core\")\n    plt.plot(sample_sizes, multi_core, label=\"multi core\")\n    plt.xlabel(\"n_samples\")\n    plt.ylabel(\"Time (s)\")\n    plt.title(\"Parallel %s\" % func.__name__)\n    plt.legend()\n\n\ndef euclidean_distances(X, n_jobs):\n    return pairwise_distances(X, metric=\"euclidean\", n_jobs=n_jobs)\n\n\ndef rbf_kernels(X, n_jobs):\n    return pairwise_kernels(X, metric=\"rbf\", n_jobs=n_jobs, gamma=0.1)\n\n\nplot(euclidean_distances)\nplot(rbf_kernels)\nplt.show()\n"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " release after an increment in Y\n#   X.Y.Z   # For bugfix releases\n#\n# Admissible pre-release markers:\n#   X.Y.ZaN   # Alpha release\n#   X.Y.ZbN   # Beta release\n#   X.Y.ZrcN  # Release Candidate\n#   X.Y.Z     # Final release\n#\n# Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.\n# 'X.Y.dev0' is the canonical version of 'X.Y.dev'\n#\n__version__ = \"1.8.dev0\"\n\n\n# On OSX, we can get a runtime error due to multiple OpenMP libraries loaded\n# simultaneously. This can happen for instance when calling BLAS inside a\n# prange. Setting the following environment variable allows multiple OpenMP\n# libraries to be loaded. It should not degrade performances since we manually\n# take care of potential over-subcription performance issues, in sections of\n# the code where nested OpenMP loops can happen, by dynamically reconfiguring\n# the inner OpenMP runtime to temporarily disable it while under the scope of\n# the outer OpenMP parallel section.\nos.environ.setdefault(\"KMP_DUPLICATE_LIB_OK\", \"True\")\n\n# Workaround issue discovered in intel-openmp 2019.5:\n# https://github.com/ContinuumIO/anaconda-issues/issues/11294\nos.environ.setdefault(\"KMP_INIT_AT_FORK\", \"FALSE\")\n\n# `_distributor_init` allows distributors to run custom init code.\n# For instance, for the Windows wheel, this is used to pre-load the\n# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\n# sub-folder.\n# It is necessary to do this prior to importing show_versions as the\n# later is linked to the OpenMP runtime to make it possible to introspect\n# it and importing it first would fail if the OpenMP dll cannot be found.\nfrom sklearn import __check_build, _distributor_init  # noqa: E402 F401\nfrom sklearn.base import clone  # noqa: E402\nfrom sklearn.utils._show_versions import show_versions  # noqa: E402\n\n_submodules = [\n    \"calibration\",\n    \"cluster\",\n    \"covariance\",\n    \"cross_decomposition\",\n    \"datasets\",\n    \"decomposition\",\n    \"dummy\",\n    \"ensemble\",\n    \"exceptions\",\n    \"experimental\""}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " \"True\")\n\n# Workaround issue discovered in intel-openmp 2019.5:\n# https://github.com/ContinuumIO/anaconda-issues/issues/11294\nos.environ.setdefault(\"KMP_INIT_AT_FORK\", \"FALSE\")\n\n# `_distributor_init` allows distributors to run custom init code.\n# For instance, for the Windows wheel, this is used to pre-load the\n# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\n# sub-folder.\n# It is necessary to do this prior to importing show_versions as the\n# later is linked to the OpenMP runtime to make it possible to introspect\n# it and importing it first would fail if the OpenMP dll cannot be found.\nfrom sklearn import __check_build, _distributor_init  # noqa: E402 F401\nfrom sklearn.base import clone  # noqa: E402\nfrom sklearn.utils._show_versions import show_versions  # noqa: E402\n\n_submodules = [\n    \"calibration\",\n    \"cluster\",\n    \"covariance\",\n    \"cross_decomposition\",\n    \"datasets\",\n    \"decomposition\",\n    \"dummy\",\n    \"ensemble\",\n    \"exceptions\",\n    \"experimental\",\n    \"externals\",\n    \"feature_extraction\",\n    \"feature_selection\",\n    \"frozen\",\n    \"gaussian_process\",\n    \"inspection\",\n    \"isotonic\",\n    \"kernel_approximation\",\n    \"kernel_ridge\",\n    \"linear_model\",\n    \"manifold\",\n    \"metrics\",\n    \"mixture\",\n    \"model_selection\",\n    \"multiclass\",\n    \"multioutput\",\n    \"naive_bayes\",\n    \"neighbors\",\n    \"neural_network\",\n    \"pipeline\",\n    \"preprocessing\",\n    \"random_projection\",\n    \"semi_supervised\",\n    \"svm\",\n    \"tree\",\n    \"discriminant_analysis\",\n    \"impute\",\n    \"compose\",\n]\n\n__all__ = _submodules + [\n    # Non-modules:\n    \"clone\",\n    \"get_config\",\n    \"set_config\",\n    \"config_context\",\n    \"show_versions\",\n]\n\n\ndef __dir__():\n    return __all__\n\n\ndef __getattr__(name):\n    if name in _submodules:\n        return _importlib.import_module(f\"sklearn.{name}\")\n    else:\n        try:\n            return globals()[name]\n        except KeyError:\n            raise AttributeError(f\"Module 'sklearn' has no attribute '{name}'\")\n\n\ndef s"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Configure global settings and get information about the working environment.\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\n# Machine learning module for Python\n# ==================================\n#\n# sklearn is a Python module integrating classical machine\n# learning algorithms in the tightly-knit world of scientific Python\n# packages (numpy, scipy, matplotlib).\n#\n# It aims to provide simple and efficient solutions to learning problems\n# that are accessible to everybody and reusable in various contexts:\n# machine-learning as a versatile tool for science and engineering.\n#\n# See https://scikit-learn.org for complete documentation.\n\nimport importlib as _importlib\nimport logging\nimport os\nimport random\n\nfrom sklearn._config import config_context, get_config, set_config\n\nlogger = logging.getLogger(__name__)\n\n\n# PEP0440 compatible formatted version, see:\n# https://www.python.org/dev/peps/pep-0440/\n#\n# Generic release markers:\n#   X.Y.0   # For first release after an increment in Y\n#   X.Y.Z   # For bugfix releases\n#\n# Admissible pre-release markers:\n#   X.Y.ZaN   # Alpha release\n#   X.Y.ZbN   # Beta release\n#   X.Y.ZrcN  # Release Candidate\n#   X.Y.Z     # Final release\n#\n# Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.\n# 'X.Y.dev0' is the canonical version of 'X.Y.dev'\n#\n__version__ = \"1.8.dev0\"\n\n\n# On OSX, we can get a runtime error due to multiple OpenMP libraries loaded\n# simultaneously. This can happen for instance when calling BLAS inside a\n# prange. Setting the following environment variable allows multiple OpenMP\n# libraries to be loaded. It should not degrade performances since we manually\n# take care of potential over-subcription performance issues, in sections of\n# the code where nested OpenMP loops can happen, by dynamically reconfiguring\n# the inner OpenMP runtime to temporarily disable it while under the scope of\n# the outer OpenMP parallel section.\nos.environ.setdefault(\"KMP_DUPLICATE_LIB_OK\","}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "common.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/asv_benchmarks/benchmarks", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "import itertools\nimport json\nimport os\nimport pickle\nimport timeit\nfrom abc import ABC, abstractmethod\nfrom multiprocessing import cpu_count\nfrom pathlib import Path\n\nimport numpy as np\n\n\ndef get_from_config():\n    \"\"\"Get benchmarks configuration from the config.json file\"\"\"\n    current_path = Path(__file__).resolve().parent\n\n    config_path = current_path / \"config.json\"\n    with open(config_path, \"r\") as config_file:\n        config_file = \"\".join(line for line in config_file if line and \"//\" not in line)\n        config = json.loads(config_file)\n\n    profile = os.getenv(\"SKLBENCH_PROFILE\", config[\"profile\"])\n\n    n_jobs_vals_env = os.getenv(\"SKLBENCH_NJOBS\")\n    if n_jobs_vals_env:\n        n_jobs_vals = json.loads(n_jobs_vals_env)\n    else:\n        n_jobs_vals = config[\"n_jobs_vals\"]\n    if not n_jobs_vals:\n        n_jobs_vals = list(range(1, 1 + cpu_count()))\n\n    cache_path = current_path / \"cache\"\n    cache_path.mkdir(exist_ok=True)\n    (cache_path / \"estimators\").mkdir(exist_ok=True)\n    (cache_path / \"tmp\").mkdir(exist_ok=True)\n\n    save_estimators = os.getenv(\"SKLBENCH_SAVE_ESTIMATORS\", config[\"save_estimators\"])\n    save_dir = os.getenv(\"ASV_COMMIT\", \"new\")[:8]\n\n    if save_estimators:\n        (cache_path / \"estimators\" / save_dir).mkdir(exist_ok=True)\n\n    base_commit = os.getenv(\"SKLBENCH_BASE_COMMIT\", config[\"base_commit\"])\n\n    bench_predict = os.getenv(\"SKLBENCH_PREDICT\", config[\"bench_predict\"])\n    bench_transform = os.getenv(\"SKLBENCH_TRANSFORM\", config[\"bench_transform\"])\n\n    return (\n        profile,\n        n_jobs_vals,\n        save_estimators,\n        save_dir,\n        base_commit,\n        bench_predict,\n        bench_transform,\n    )\n\n\ndef get_estimator_path(benchmark, directory, params, save=False):\n    \"\"\"Get path of pickled fitted estimator\"\"\"\n    path = Path(__file__).resolve().parent / \"cache\"\n    path = (path / \"estimators\" / directory) if save else (path / \"tmp\")\n\n    filename = (\n        benchmark.__class__.__name__\n        + \"_estimato"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "_show_versions.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"\nUtility methods to print system info for debugging\n\nadapted from :func:`pandas.show_versions`\n\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport platform\nimport sys\n\nfrom threadpoolctl import threadpool_info\n\nfrom sklearn import __version__\nfrom sklearn.utils._openmp_helpers import _openmp_parallelism_enabled\n\n\ndef _get_sys_info():\n    \"\"\"System information\n\n    Returns\n    -------\n    sys_info : dict\n        system and Python version information\n\n    \"\"\"\n    python = sys.version.replace(\"\\n\", \" \")\n\n    blob = [\n        (\"python\", python),\n        (\"executable\", sys.executable),\n        (\"machine\", platform.platform()),\n    ]\n\n    return dict(blob)\n\n\ndef _get_deps_info():\n    \"\"\"Overview of the installed version of main dependencies\n\n    This function does not import the modules to collect the version numbers\n    but instead relies on standard Python package metadata.\n\n    Returns\n    -------\n    deps_info: dict\n        version information on relevant Python libraries\n\n    \"\"\"\n    deps = [\n        \"pip\",\n        \"setuptools\",\n        \"numpy\",\n        \"scipy\",\n        \"Cython\",\n        \"pandas\",\n        \"matplotlib\",\n        \"joblib\",\n        \"threadpoolctl\",\n    ]\n\n    deps_info = {\n        \"sklearn\": __version__,\n    }\n\n    from importlib.metadata import PackageNotFoundError, version\n\n    for modname in deps:\n        try:\n            deps_info[modname] = version(modname)\n        except PackageNotFoundError:\n            deps_info[modname] = None\n    return deps_info\n\n\ndef show_versions():\n    \"\"\"Print useful debugging information\"\n\n    .. versionadded:: 0.20\n\n    Examples\n    --------\n    >>> from sklearn import show_versions\n    >>> show_versions()  # doctest: +SKIP\n    \"\"\"\n\n    sys_info = _get_sys_info()\n    deps_info = _get_deps_info()\n\n    print(\"\\nSystem:\")\n    for k, stat in sys_info.items():\n        print(\"{k:>10}: {stat}\".format(k=k, stat=stat))\n\n    print(\"\\nPython dependencies:\")\n    for k, stat in deps_info.ite"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "bench_hist_gradient_boosting_threading.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/benchmarks", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " in [\"lightgbm\", \"xgboost\", \"catboost\"]:\n        if getattr(args, libname):\n            print(libname)\n            est = get_equivalent_estimator(\n                sklearn_est, lib=libname, n_classes=args.n_classes\n            )\n            pprint(est.get_params())\n\n\ndef one_run(n_threads, n_samples):\n    X_train = X_train_[:n_samples]\n    X_test = X_test_[:n_samples]\n    y_train = y_train_[:n_samples]\n    y_test = y_test_[:n_samples]\n    if sample_weight is not None:\n        sample_weight_train = sample_weight_train_[:n_samples]\n    else:\n        sample_weight_train = None\n    assert X_train.shape[0] == n_samples\n    assert X_test.shape[0] == n_samples\n    print(\"Fitting a sklearn model...\")\n    tic = time()\n    est = sklearn.base.clone(sklearn_est)\n\n    with threadpool_limits(n_threads, user_api=\"openmp\"):\n        est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        sklearn_fit_duration = time() - tic\n        tic = time()\n        sklearn_score = est.score(X_test, y_test)\n        sklearn_score_duration = time() - tic\n    print(\"score: {:.4f}\".format(sklearn_score))\n    print(\"fit duration: {:.3f}s,\".format(sklearn_fit_duration))\n    print(\"score duration: {:.3f}s,\".format(sklearn_score_duration))\n\n    lightgbm_score = None\n    lightgbm_fit_duration = None\n    lightgbm_score_duration = None\n    if args.lightgbm:\n        print(\"Fitting a LightGBM model...\")\n        lightgbm_est = get_equivalent_estimator(\n            est, lib=\"lightgbm\", n_classes=args.n_classes\n        )\n        lightgbm_est.set_params(num_threads=n_threads)\n\n        tic = time()\n        lightgbm_est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        lightgbm_fit_duration = time() - tic\n        tic = time()\n        lightgbm_score = lightgbm_est.score(X_test, y_test)\n        lightgbm_score_duration = time() - tic\n        print(\"score: {:.4f}\".format(lightgbm_score))\n        print(\"fit duration: {:.3f}s,\".format(lightgbm_fit_duration))\n        print(\"score duration: {:.3f}"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "_forest.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "om sklearn.preprocessing import OneHotEncoder\nfrom sklearn.tree import (\n    BaseDecisionTree,\n    DecisionTreeClassifier,\n    DecisionTreeRegressor,\n    ExtraTreeClassifier,\n    ExtraTreeRegressor,\n)\nfrom sklearn.tree._tree import DOUBLE, DTYPE\nfrom sklearn.utils import check_random_state, compute_sample_weight\nfrom sklearn.utils._param_validation import Interval, RealNotInt, StrOptions\nfrom sklearn.utils._tags import get_tags\nfrom sklearn.utils.multiclass import check_classification_targets, type_of_target\nfrom sklearn.utils.parallel import Parallel, delayed\nfrom sklearn.utils.validation import (\n    _check_feature_names_in,\n    _check_sample_weight,\n    _num_samples,\n    check_is_fitted,\n    validate_data,\n)\n\n__all__ = [\n    \"ExtraTreesClassifier\",\n    \"ExtraTreesRegressor\",\n    \"RandomForestClassifier\",\n    \"RandomForestRegressor\",\n    \"RandomTreesEmbedding\",\n]\n\nMAX_INT = np.iinfo(np.int32).max\n\n\ndef _get_n_samples_bootstrap(n_samples, max_samples):\n    \"\"\"\n    Get the number of samples in a bootstrap sample.\n\n    Parameters\n    ----------\n    n_samples : int\n        Number of samples in the dataset.\n    max_samples : int or float\n        The maximum number of samples to draw from the total available:\n            - if float, this indicates a fraction of the total and should be\n              the interval `(0.0, 1.0]`;\n            - if int, this indicates the exact number of samples;\n            - if None, this indicates the total number of samples.\n\n    Returns\n    -------\n    n_samples_bootstrap : int\n        The total number of samples to draw for the bootstrap sample.\n    \"\"\"\n    if max_samples is None:\n        return n_samples\n\n    if isinstance(max_samples, Integral):\n        if max_samples > n_samples:\n            msg = \"`max_samples` must be <= n_samples={} but got value {}\"\n            raise ValueError(msg.format(n_samples, max_samples))\n        return max_samples\n\n    if isinstance(max_samples, Real):\n        return max(round(n_samples * max_samples), 1)"}], "retrieved_count": 10, "cost_time": 1.044325828552246}
{"question": "Why does Scikit-learn's pipeline system optimize memory usage and performance in large-scale data processing?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn's pipeline system optimizes memory usage and performance in large-scale data processing for several fundamental design reasons that enhance efficiency, scalability, and resource management:\n\n1. **Caching and Memory Management**:\n   - **Transformer Caching**: Pipeline can cache fitted transformers to avoid recomputation\n   - **Memory-Efficient Storage**: Optimized storage of intermediate results\n   - **Selective Caching**: Only caches transformers, not the final estimator\n   - **Disk-Based Caching**: Can use disk storage for large intermediate results\n   - **Memory Pooling**: Shared memory pools for common data structures\n\n2. **Sequential Processing Optimization**:\n   - **Streaming Processing**: Data flows through steps without storing all intermediate results\n   - **Lazy Evaluation**: Transformations are applied only when needed\n   - **Memory-Efficient Chaining**: Each step processes data and passes it to the next\n   - **Garbage Collection**: Automatic cleanup of intermediate results\n   - **Copy Avoidance**: Minimizes unnecessary data copying between steps\n\n3. **Parallel Processing Integration**:\n   - **Parallel Step Execution**: Can parallelize independent pipeline steps\n   - **Joblib Integration**: Efficient parallel processing with joblib\n   - **Resource Management**: Optimal allocation of computational resources\n   - **Load Balancing**: Automatic load balancing across parallel workers\n   - **Memory Distribution**: Distributes memory usage across parallel processes\n\n4. **Large-Scale Data Handling**:\n   - **Chunked Processing**: Can process data in chunks to manage memory usage\n   - **Out-of-Core Support**: Can handle datasets larger than available memory\n   - **Incremental Learning**: Supports incremental learning for large datasets\n   - **Streaming Transformers**: Transformers that can process data incrementally\n   - **Memory Monitoring**: Built-in memory usage monitoring and optimization\n\n5. **Optimized Data Flow**:\n   - **Efficient Data Transfer**: Optimized data transfer between pipeline steps\n   - **Type Preservation**: Maintains optimal data types throughout the pipeline\n   - **Sparse Matrix Support**: Efficient handling of sparse matrices\n   - **Memory Layout Optimization**: Optimized memory layout for better cache performance\n   - **Data Compression**: Automatic data compression where beneficial\n\n6. **Resource Optimization**:\n   - **Working Memory Control**: Configurable working memory limits\n   - **Memory-Efficient Algorithms**: Uses memory-efficient algorithms in each step\n   - **Temporary Storage Management**: Efficient management of temporary storage\n   - **Memory Cleanup**: Automatic cleanup of temporary data structures\n   - **Resource Pooling**: Shared resource pools for common operations\n\n7. **Performance Monitoring and Tuning**:\n   - **Performance Profiling**: Built-in performance monitoring capabilities\n   - **Memory Usage Tracking**: Tracks memory usage throughout the pipeline\n   - **Bottleneck Identification**: Identifies performance bottlenecks\n   - **Automatic Optimization**: Automatic optimization of step execution order\n   - **Performance Metrics**: Provides performance metrics for optimization\n\n8. **Scalability Features**:\n   - **Horizontal Scaling**: Can scale across multiple machines\n   - **Vertical Scaling**: Can utilize multiple CPU cores efficiently\n   - **Distributed Processing**: Support for distributed processing frameworks\n   - **Cloud Integration**: Optimized for cloud computing environments\n   - **Elastic Scaling**: Can scale resources up or down based on demand\n\n9. **Advanced Memory Management**:\n   - **Memory Mapping**: Can use memory mapping for very large datasets\n   - **Virtual Memory Optimization**: Optimized use of virtual memory\n   - **Cache-Aware Processing**: Cache-aware data processing patterns\n   - **Memory Prefetching**: Intelligent memory prefetching strategies\n   - **Memory Defragmentation**: Automatic memory defragmentation\n\n10. **Production-Ready Optimizations**:\n    - **Production Deployment**: Optimized for production deployment scenarios\n    - **Real-Time Processing**: Efficient real-time data processing capabilities\n    - **Batch Processing**: Optimized batch processing for large datasets\n    - **Memory-Efficient Deployment**: Minimal memory footprint for deployment\n    - **Resource Efficiency**: Optimal resource utilization in production environments", "score": null, "retrieved_content": [{"start_line": 59000, "end_line": 61000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "p : Pipeline\n        Returns a scikit-learn :class:`Pipeline` object.\n\n    See Also\n    --------\n    Pipeline : Class for creating a pipeline of transforms with a final\n        estimator.\n\n    Examples\n    --------\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.pipeline import make_pipeline\n    >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('gaussiannb', GaussianNB())])\n    \"\"\"\n    return Pipeline(\n        _name_estimators(steps),\n        transform_input=transform_input,\n        memory=memory,\n        verbose=verbose,\n    )\n\n\ndef _transform_one(transformer, X, y, weight, params):\n    \"\"\"Call transform and apply weight to output.\n\n    Parameters\n    ----------\n    transformer : estimator\n        Estimator to be used for transformation.\n\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Input data to be transformed.\n\n    y : ndarray of shape (n_samples,)\n        Ignored.\n\n    weight : float\n        Weight to be applied to the output of the transformation.\n\n    params : dict\n        Parameters to be passed to the transformer's ``transform`` method.\n\n        This should be of the form ``process_routing()[\"step_name\"]``.\n    \"\"\"\n    res = transformer.transform(X, **params.transform)\n    # if we have a weight for this transformer, multiply output\n    if weight is None:\n        return res\n    return res * weight\n\n\ndef _fit_transform_one(\n    transformer, X, y, weight, message_clsname=\"\", message=None, params=None\n):\n    \"\"\"\n    Fits ``transformer`` to ``X`` and ``y``. The transformed result is returned\n    with the fitted transformer. If ``weight`` is not ``None``, the result will\n    be multiplied by ``weight``.\n\n    ``params`` needs to be of the form ``process_routing()[\"step_name\"]``.\n    \"\"\"\n    params = params or {}\n    with _print_elapsed_time(message_clsname, message)"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ring :term:`fit`. Only defined if the\n        underlying first estimator in `steps` exposes such an attribute\n        when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    make_pipeline : Convenience function for simplified pipeline construction.\n\n    Examples\n    --------\n    >>> from sklearn.svm import SVC\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.pipeline import Pipeline\n    >>> X, y = make_classification(random_state=0)\n    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n    ...                                                     random_state=0)\n    >>> pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n    >>> # The pipeline can be used as any other estimator\n    >>> # and avoids leaking the test set into the train set\n    >>> pipe.fit(X_train, y_train).score(X_test, y_test)\n    0.88\n    >>> # An estimator's parameter can be set using '__' syntax\n    >>> pipe.set_params(svc__C=10).fit(X_train, y_train).score(X_test, y_test)\n    0.76\n    \"\"\"\n\n    # BaseEstimator interface\n    _parameter_constraints: dict = {\n        \"steps\": [list, Hidden(tuple)],\n        \"transform_input\": [list, None],\n        \"memory\": [None, str, HasMethods([\"cache\"])],\n        \"verbose\": [\"boolean\"],\n    }\n\n    def __init__(self, steps, *, transform_input=None, memory=None, verbose=False):\n        self.steps = steps\n        self.transform_input = transform_input\n        self.memory = memory\n        self.verbose = verbose\n\n    def set_output(self, *, transform=None):\n        \"\"\"Set the output container when `\"transform\"` and `\"fit_transform\"` are called.\n\n        Calling"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "mposition):\n    \"\"\"\n    A sequence of data transformers with an optional final predictor.\n\n    `Pipeline` allows you to sequentially apply a list of transformers to\n    preprocess the data and, if desired, conclude the sequence with a final\n    :term:`predictor` for predictive modeling.\n\n    Intermediate steps of the pipeline must be transformers, that is, they\n    must implement `fit` and `transform` methods.\n    The final :term:`estimator` only needs to implement `fit`.\n    The transformers in the pipeline can be cached using ``memory`` argument.\n\n    The purpose of the pipeline is to assemble several steps that can be\n    cross-validated together while setting different parameters. For this, it\n    enables setting parameters of the various steps using their names and the\n    parameter name separated by a `'__'`, as in the example below. A step's\n    estimator may be replaced entirely by setting the parameter with its name\n    to another estimator, or a transformer removed by setting it to\n    `'passthrough'` or `None`.\n\n    For an example use case of `Pipeline` combined with\n    :class:`~sklearn.model_selection.GridSearchCV`, refer to\n    :ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`. The\n    example :ref:`sphx_glr_auto_examples_compose_plot_digits_pipe.py` shows how\n    to grid search on a pipeline using `'__'` as a separator in the parameter names.\n\n    Read more in the :ref:`User Guide <pipeline>`.\n\n    .. versionadded:: 0.5\n\n    Parameters\n    ----------\n    steps : list of tuples\n        List of (name of step, estimator) tuples that are to be chained in\n        sequential order. To be compatible with the scikit-learn API, all steps\n        must define `fit`. All non-last steps must also define `transform`. See\n        :ref:`Combining Estimators <combining_estimators>` for more details.\n\n    transform_input : list of str, default=None\n        The names of the :term:`metadata` parameters that should be transformed by the\n        pipeline befo"}, {"start_line": 58000, "end_line": 60000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e transformer instance given to the\n        pipeline cannot be inspected directly. Use the attribute ``named_steps``\n        or ``steps`` to inspect estimators within the pipeline. Caching the\n        transformers is advantageous when fitting is time consuming.\n\n    transform_input : list of str, default=None\n        This enables transforming some input arguments to ``fit`` (other than ``X``)\n        to be transformed by the steps of the pipeline up to the step which requires\n        them. Requirement is defined via :ref:`metadata routing <metadata_routing>`.\n        This can be used to pass a validation set through the pipeline for instance.\n\n        You can only set this if metadata routing is enabled, which you\n        can enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n\n        .. versionadded:: 1.6\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting each step will be printed as it\n        is completed.\n\n    Returns\n    -------\n    p : Pipeline\n        Returns a scikit-learn :class:`Pipeline` object.\n\n    See Also\n    --------\n    Pipeline : Class for creating a pipeline of transforms with a final\n        estimator.\n\n    Examples\n    --------\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.pipeline import make_pipeline\n    >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('gaussiannb', GaussianNB())])\n    \"\"\"\n    return Pipeline(\n        _name_estimators(steps),\n        transform_input=transform_input,\n        memory=memory,\n        verbose=verbose,\n    )\n\n\ndef _transform_one(transformer, X, y, weight, params):\n    \"\"\"Call transform and apply weight to output.\n\n    Parameters\n    ----------\n    transformer : estimator\n        Estimator to be used for transformation.\n\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        In"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tance given to the\n        pipeline cannot be inspected directly. Use the attribute ``named_steps``\n        or ``steps`` to inspect estimators within the pipeline. Caching the\n        transformers is advantageous when fitting is time consuming. See\n        :ref:`sphx_glr_auto_examples_neighbors_plot_caching_nearest_neighbors.py`\n        for an example on how to enable caching.\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting each step will be printed as it\n        is completed.\n\n    Attributes\n    ----------\n    named_steps : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n        Read-only attribute to access any step parameter by user given name.\n        Keys are step names and values are steps parameters.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels. Only exist if the last step of the pipeline is a\n        classifier.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying first estimator in `steps` exposes such an attribute\n        when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    make_pipeline : Convenience function for simplified pipeline construction.\n\n    Examples\n    --------\n    >>> from sklearn.svm import SVC\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.pipeline import Pipeline\n    >>> X, y = make_classification(random_state=0)\n    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n    ...                                                     random_state=0)\n    >>> pipe = Pipeline([('s"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "thodMapping,\n    _raise_for_params,\n    _routing_enabled,\n    get_routing_for_object,\n    process_routing,\n)\nfrom sklearn.utils.metaestimators import _BaseComposition, available_if\nfrom sklearn.utils.parallel import Parallel, delayed\nfrom sklearn.utils.validation import check_is_fitted, check_memory\n\n__all__ = [\"FeatureUnion\", \"Pipeline\", \"make_pipeline\", \"make_union\"]\n\n\n@contextmanager\ndef _raise_or_warn_if_not_fitted(estimator):\n    \"\"\"A context manager to make sure a NotFittedError is raised, if a sub-estimator\n    raises the error.\n\n    Otherwise, we raise a warning if the pipeline is not fitted, with the deprecation.\n\n    TODO(1.8): remove this context manager and replace with check_is_fitted.\n    \"\"\"\n    try:\n        yield\n    except NotFittedError as exc:\n        raise NotFittedError(\"Pipeline is not fitted yet.\") from exc\n\n    # we only get here if the above didn't raise\n    try:\n        check_is_fitted(estimator)\n    except NotFittedError:\n        warnings.warn(\n            \"This Pipeline instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using other methods such as transform, \"\n            \"predict, etc. This will raise an error in 1.8 instead of the current \"\n            \"warning.\",\n            FutureWarning,\n        )\n\n\ndef _final_estimator_has(attr):\n    \"\"\"Check that final_estimator has `attr`.\n\n    Used together with `available_if` in `Pipeline`.\"\"\"\n\n    def check(self):\n        # raise original `AttributeError` if `attr` does not exist\n        getattr(self._final_estimator, attr)\n        return True\n\n    return check\n\n\ndef _cached_transform(\n    sub_pipeline, *, cache, param_name, param_value, transform_params\n):\n    \"\"\"Transform a parameter value using a sub-pipeline and cache the result.\n\n    Parameters\n    ----------\n    sub_pipeline : Pipeline\n        The sub-pipeline to be used for transformation.\n    cache : dict\n        The cache dictionary to store the transformed values.\n    param_name : str\n        The n"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " it to\n    `'passthrough'` or `None`.\n\n    For an example use case of `Pipeline` combined with\n    :class:`~sklearn.model_selection.GridSearchCV`, refer to\n    :ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`. The\n    example :ref:`sphx_glr_auto_examples_compose_plot_digits_pipe.py` shows how\n    to grid search on a pipeline using `'__'` as a separator in the parameter names.\n\n    Read more in the :ref:`User Guide <pipeline>`.\n\n    .. versionadded:: 0.5\n\n    Parameters\n    ----------\n    steps : list of tuples\n        List of (name of step, estimator) tuples that are to be chained in\n        sequential order. To be compatible with the scikit-learn API, all steps\n        must define `fit`. All non-last steps must also define `transform`. See\n        :ref:`Combining Estimators <combining_estimators>` for more details.\n\n    transform_input : list of str, default=None\n        The names of the :term:`metadata` parameters that should be transformed by the\n        pipeline before passing it to the step consuming it.\n\n        This enables transforming some input arguments to ``fit`` (other than ``X``)\n        to be transformed by the steps of the pipeline up to the step which requires\n        them. Requirement is defined via :ref:`metadata routing <metadata_routing>`.\n        For instance, this can be used to pass a validation set through the pipeline.\n\n        You can only set this if metadata routing is enabled, which you\n        can enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n\n        .. versionadded:: 1.6\n\n    memory : str or object with the joblib.Memory interface, default=None\n        Used to cache the fitted transformers of the pipeline. The last step\n        will never be cached, even if it is a transformer. By default, no\n        caching is performed. If a string is given, it is the path to the\n        caching directory. Enabling caching triggers a clone of the transformers\n        before fitting. Therefore, the transformer ins"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "caler', StandardScaler()), ('svc', SVC())])\n    >>> # The pipeline can be used as any other estimator\n    >>> # and avoids leaking the test set into the train set\n    >>> pipe.fit(X_train, y_train).score(X_test, y_test)\n    0.88\n    >>> # An estimator's parameter can be set using '__' syntax\n    >>> pipe.set_params(svc__C=10).fit(X_train, y_train).score(X_test, y_test)\n    0.76\n    \"\"\"\n\n    # BaseEstimator interface\n    _parameter_constraints: dict = {\n        \"steps\": [list, Hidden(tuple)],\n        \"transform_input\": [list, None],\n        \"memory\": [None, str, HasMethods([\"cache\"])],\n        \"verbose\": [\"boolean\"],\n    }\n\n    def __init__(self, steps, *, transform_input=None, memory=None, verbose=False):\n        self.steps = steps\n        self.transform_input = transform_input\n        self.memory = memory\n        self.verbose = verbose\n\n    def set_output(self, *, transform=None):\n        \"\"\"Set the output container when `\"transform\"` and `\"fit_transform\"` are called.\n\n        Calling `set_output` will set the output of all estimators in `steps`.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.\n\n        Returns\n        -------\n        self : estimator instance\n            Estimator instance.\n        \"\"\"\n        for _, _, step in self._iter():\n            _safe_set_output(step, transform=transform)\n        return self\n\n    def get_params(self, deep=True):\n        \"\"\"Get parameters for this estimator.\n\n        Returns the parameters given in the constructor as well as the\n        estimators contained within the `steps` of the `Pipeline`.\n\n        Parameters\n       "}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_pprint.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "       analyzer=\"word\",\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.int64,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.ngram_range = ngram_range\n        self.vocabulary = vocabulary\n        self.binary = binary\n        self.dtype = dtype\n\n\nclass Pipeline(BaseEstimator):\n    def __init__(self, steps, memory=None):\n        self.steps = steps\n        self.memory = memory\n\n\nclass SVC(BaseEstimator):\n    def __init__(\n        self,\n        C=1.0,\n        kernel=\"rbf\",\n        degree=3,\n        gamma=\"auto_deprecated\",\n        coef0=0.0,\n        shrinking=True,\n        probability=False,\n        tol=1e-3,\n        cache_size=200,\n        class_weight=None,\n        verbose=False,\n        max_iter=-1,\n        decision_function_shape=\"ovr\",\n        random_state=None,\n    ):\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.tol = tol\n        self.C = C\n        self.shrinking = shrinking\n        self.probability = probability\n        self.cache_size = cache_size\n        self.class_weight = class_weight\n        self.verbose = verbose\n        self.max_iter = max_iter\n        self.decision_function_shape = decision_function_shape\n        self.random_state = random_state\n\n\nclass PCA(BaseEstimator):\n    def __init__(\n        self,\n        n_components=None,\n        copy=True,\n        whiten=False,\n        svd_solver=\"auto\",\n        tol=0.0,\n        iterated_power=\"auto\",\n        random_state"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "plot_release_highlights_0_23_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "01)\nglm.fit(X_train, y_train)\ngbdt.fit(X_train, y_train)\nprint(glm.score(X_test, y_test))\nprint(gbdt.score(X_test, y_test))\n\n##############################################################################\n# Rich visual representation of estimators\n# -----------------------------------------\n# Estimators can now be visualized in notebooks by enabling the\n# `display='diagram'` option. This is particularly useful to summarise the\n# structure of pipelines and other composite estimators, with interactivity to\n# provide detail.  Click on the example image below to expand Pipeline\n# elements.  See :ref:`visualizing_composite_estimators` for how you can use\n# this feature.\n\nfrom sklearn import set_config\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nset_config(display=\"diagram\")\n\nnum_proc = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n\ncat_proc = make_pipeline(\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n    OneHotEncoder(handle_unknown=\"ignore\"),\n)\n\npreprocessor = make_column_transformer(\n    (num_proc, (\"feat1\", \"feat3\")), (cat_proc, (\"feat0\", \"feat2\"))\n)\n\nclf = make_pipeline(preprocessor, LogisticRegression())\nclf\n\n##############################################################################\n# Scalability and stability improvements to KMeans\n# ------------------------------------------------\n# The :class:`~sklearn.cluster.KMeans` estimator was entirely re-worked, and it\n# is now significantly faster and more stable. In addition, the Elkan algorithm\n# is now compatible with sparse matrices. The estimator uses OpenMP based\n# parallelism instead of relying on joblib, so the `n_jobs` parameter has no\n# effect anymore. For more details on how to control the number of threads,\n# please refer to our :ref:`parallelism` notes.\nimport nump"}], "retrieved_count": 10, "cost_time": 1.0616953372955322}
{"question": "Why does Scikit-learn implement a separate transform method for preprocessing steps instead of combining it with fit?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn implements a separate transform method for preprocessing steps instead of combining it with fit for several fundamental design reasons that align with machine learning best practices and practical considerations:\n\n1. **Data Leakage Prevention**:\n   - **Training Data Only**: fit() learns parameters only from training data\n   - **Test Data Safety**: transform() applies learned parameters to test data without learning\n   - **Cross-Validation Integrity**: Prevents information from test folds leaking into training\n   - **Pipeline Safety**: Ensures proper train/test separation in preprocessing pipelines\n   - **Production Readiness**: Prevents using future information in real-world applications\n\n2. **Separation of Concerns**:\n   - **Learning Phase (fit)**: Computes and stores transformation parameters (e.g., mean, std)\n   - **Application Phase (transform)**: Applies stored parameters to new data\n   - **Clear Distinction**: Each phase has distinct responsibilities and requirements\n   - **Modularity**: Allows independent optimization of learning and application logic\n   - **Maintainability**: Easier to debug and maintain separate learning and application code\n\n3. **Consistency with Estimator Interface**:\n   - **Unified API**: All estimators follow the same fit/transform pattern\n   - **Pipeline Compatibility**: Works seamlessly with Pipeline and FeatureUnion\n   - **Meta-Estimator Support**: GridSearchCV, RandomizedSearchCV work with transformers\n   - **Method Chaining**: Enables fluent API: scaler.fit(X_train).transform(X_test)\n   - **Extensibility**: Easy to add new transformers following the same pattern\n\n4. **Performance and Efficiency**:\n   - **Fit Once, Transform Many**: Parameters computed once, applied multiple times\n   - **Memory Efficiency**: Stored parameters are typically small compared to data\n   - **Computational Optimization**: Can optimize learning and application separately\n   - **Caching**: Learned parameters can be cached and reused\n   - **Parallel Processing**: Transform operations can be parallelized independently\n\n5. **State Management and Immutability**:\n   - **Parameter Storage**: fit() sets internal state (e.g., mean_, scale_, categories_)\n   - **Immutable Application**: transform() uses stored state without modification\n   - **Reproducibility**: Same transformer produces same results for same input\n   - **Thread Safety**: Multiple threads can use the same fitted transformer\n   - **Model Persistence**: Fitted transformers can be saved and reused\n\n6. **Validation and Error Handling**:\n   - **Training Validation**: fit() validates training data and computes parameters\n   - **Application Validation**: transform() validates input data format and dimensions\n   - **State Checking**: transform() checks if transformer has been fitted\n   - **Error Messages**: Clear error messages for each phase\n   - **Debugging**: Easier to identify whether issues occur during learning or application\n\n7. **Advanced Use Cases**:\n   - **Incremental Learning**: Some transformers support partial_fit for online learning\n   - **Warm Starting**: Can reuse parameters from previous fits\n   - **Parameter Tuning**: Can optimize transformation parameters independently\n   - **Custom Transformations**: Users can implement custom fit/transform logic\n   - **Conditional Transformations**: Can apply different transformations based on data\n\n8. **Integration with Machine Learning Workflow**:\n   - **Cross-Validation**: Each fold learns its own transformation parameters\n   - **Hyperparameter Tuning**: Transformation parameters can be tuned independently\n   - **Feature Engineering**: Complex preprocessing pipelines with multiple steps\n   - **Model Selection**: Different preprocessing strategies can be compared\n   - **Production Deployment**: Fitted transformers can be deployed independently\n\n9. **Educational and Documentation Benefits**:\n   - **Clear Learning Path**: Beginners understand the two-phase process\n   - **Documentation**: Each method can be documented separately\n   - **Examples**: Clear examples for each phase\n   - **Debugging**: Easier to teach debugging for each phase\n   - **Best Practices**: Reinforces proper machine learning workflow\n\n10. **Flexibility and Extensibility**:\n    - **Custom Transformers**: Easy to implement custom transformers following the pattern\n    - **Inheritance**: Base classes provide common functionality for both phases\n    - **Mixin Classes**: TransformerMixin provides specialized behavior\n    - **Method Override**: Subclasses can override specific phases independently\n    - **Plugin Architecture**: New transformers can be added without changing existing code", "score": null, "retrieved_content": [{"start_line": 72000, "end_line": 74000, "belongs_to": {"file_name": "_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "d\n        (ii) be consistent with the scikit-learn transformer API.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data to estimate the normalization parameters.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Fitted transformer.\n        \"\"\"\n        validate_data(self, X, accept_sparse=\"csr\")\n        return self\n\n    def transform(self, X, copy=None):\n        \"\"\"Scale each non zero row of X to unit norm.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data to normalize, row by row. scipy.sparse matrices should be\n            in CSR format to avoid an un-necessary copy.\n\n        copy : bool, default=None\n            Copy the input X or not.\n\n        Returns\n        -------\n        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n        \"\"\"\n        copy = copy if copy is not None else self.copy\n        X = validate_data(\n            self, X, accept_sparse=\"csr\", force_writeable=True, copy=copy, reset=False\n        )\n        return normalize(X, norm=self.norm, axis=1, copy=False)\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.sparse = True\n        tags.requires_fit = False\n        tags.array_api_support = True\n        return tags\n\n\n@validate_params(\n    {\n        \"X\": [\"array-like\", \"sparse matrix\"],\n        \"threshold\": [Interval(Real, None, None, closed=\"neither\")],\n        \"copy\": [\"boolean\"],\n    },\n    prefer_skip_nested_validation=True,\n)\ndef binarize(X, *, threshold=0.0, copy=True):\n    \"\"\"Boolean thresholding of array-like or scipy.sparse matrix.\n\n    Read more in the :ref:`User Guide <preprocessing_binarization>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samp"}, {"start_line": 71000, "end_line": 73000, "belongs_to": {"file_name": "_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "o be fitted.\n    However, we recommend to call :meth:`fit_transform` instead of\n    :meth:`transform`, as parameter validation is only performed in\n    :meth:`fit`.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import Normalizer\n    >>> X = [[4, 1, 2, 2],\n    ...      [1, 3, 9, 3],\n    ...      [5, 7, 5, 1]]\n    >>> transformer = Normalizer().fit(X)  # fit does nothing.\n    >>> transformer\n    Normalizer()\n    >>> transformer.transform(X)\n    array([[0.8, 0.2, 0.4, 0.4],\n           [0.1, 0.3, 0.9, 0.3],\n           [0.5, 0.7, 0.5, 0.1]])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"norm\": [StrOptions({\"l1\", \"l2\", \"max\"})],\n        \"copy\": [\"boolean\"],\n    }\n\n    def __init__(self, norm=\"l2\", *, copy=True):\n        self.norm = norm\n        self.copy = copy\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Only validates estimator's parameters.\n\n        This method allows to: (i) validate the estimator's parameters and\n        (ii) be consistent with the scikit-learn transformer API.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data to estimate the normalization parameters.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Fitted transformer.\n        \"\"\"\n        validate_data(self, X, accept_sparse=\"csr\")\n        return self\n\n    def transform(self, X, copy=None):\n        \"\"\"Scale each non zero row of X to unit norm.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data to normalize, row by row. scipy.sparse matrices should be\n            in CSR format to avoid an un-necessary copy.\n\n        copy : bool, default=None\n            Copy the input X or not.\n\n        Returns\n        -------\n        X_tr : {ndarray, sparse matrix} of shape"}, {"start_line": 79000, "end_line": 81000, "belongs_to": {"file_name": "_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ored.\n\n        Returns\n        -------\n        self : object\n            Fitted transformer.\n        \"\"\"\n        validate_data(self, X, accept_sparse=\"csr\")\n        return self\n\n    def transform(self, X, copy=None):\n        \"\"\"Binarize each element of X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data to binarize, element by element.\n            scipy.sparse matrices should be in CSR format to avoid an\n            un-necessary copy.\n\n        copy : bool\n            Copy the input X or not.\n\n        Returns\n        -------\n        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n        \"\"\"\n        copy = copy if copy is not None else self.copy\n        # TODO: This should be refactored because binarize also calls\n        # check_array\n        X = validate_data(\n            self,\n            X,\n            accept_sparse=[\"csr\", \"csc\"],\n            force_writeable=True,\n            copy=copy,\n            reset=False,\n        )\n        return binarize(X, threshold=self.threshold, copy=False)\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.requires_fit = False\n        tags.array_api_support = True\n        tags.input_tags.sparse = True\n        return tags\n\n\nclass KernelCenterer(ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator):\n    r\"\"\"Center an arbitrary kernel matrix :math:`K`.\n\n    Let define a kernel :math:`K` such that:\n\n    .. math::\n        K(X, Y) = \\phi(X) . \\phi(Y)^{T}\n\n    :math:`\\phi(X)` is a function mapping of rows of :math:`X` to a\n    Hilbert space and :math:`K` is of shape `(n_samples, n_samples)`.\n\n    This class allows to compute :math:`\\tilde{K}(X, Y)` such that:\n\n    .. math::\n        \\tilde{K(X, Y)} = \\tilde{\\phi}(X) . \\tilde{\\phi}(Y)^{T}\n\n    :math:`\\tilde{\\phi}(X)` is the centered mapped data in the Hilbert\n    space.\n\n    `KernelCenterer` centers the features wi"}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ess_routing`, so it has a dict for each\n        # method (e.g. fit, transform, predict), which are the args to be passed to\n        # those methods. We need to transform the parameters which are in the\n        # `transform_input`, before returning these dicts.\n        for method, method_params in step_params.items():\n            transformed_params[method] = Bunch()\n            for param_name, param_value in method_params.items():\n                # An example of `(param_name, param_value)` is\n                # `('sample_weight', array([0.5, 0.5, ...]))`\n                if param_name in self.transform_input:\n                    # This parameter now needs to be transformed by the sub_pipeline, to\n                    # this step. We cache these computations to avoid repeating them.\n                    transformed_params[method][param_name] = _cached_transform(\n                        sub_pipeline,\n                        cache=transformed_cache,\n                        param_name=param_name,\n                        param_value=param_value,\n                        transform_params=transform_params,\n                    )\n                else:\n                    transformed_params[method][param_name] = param_value\n        return transformed_params\n\n    # Estimator interface\n\n    def _fit(self, X, y=None, routed_params=None, raw_params=None):\n        \"\"\"Fit the pipeline except the last step.\n\n        routed_params is the output of `process_routing`\n        raw_params is the parameters passed by the user, used when `transform_input`\n            is set by the user, to transform metadata using a sub-pipeline.\n        \"\"\"\n        # shallow copy of steps - this should really be steps_\n        self.steps = list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory = check_memory(self.memory)\n\n        fit_transform_one_cached = memory.cache(_fit_transform_one)\n\n        for step_idx, name, transformer in self._iter(\n            with_final=False, filt"}, {"start_line": 58000, "end_line": 60000, "belongs_to": {"file_name": "_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "            adjust = stats.norm.ppf(q_max / 100.0) - stats.norm.ppf(q_min / 100.0)\n                self.scale_ = self.scale_ / adjust\n        else:\n            self.scale_ = None\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Center and scale the data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data used to scale along the specified axis.\n\n        Returns\n        -------\n        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n        \"\"\"\n        check_is_fitted(self)\n        X = validate_data(\n            self,\n            X,\n            accept_sparse=(\"csr\", \"csc\"),\n            copy=self.copy,\n            dtype=FLOAT_DTYPES,\n            force_writeable=True,\n            reset=False,\n            ensure_all_finite=\"allow-nan\",\n        )\n\n        if sparse.issparse(X):\n            if self.with_scaling:\n                inplace_column_scale(X, 1.0 / self.scale_)\n        else:\n            if self.with_centering:\n                X -= self.center_\n            if self.with_scaling:\n                X /= self.scale_\n        return X\n\n    def inverse_transform(self, X):\n        \"\"\"Scale back the data to the original representation.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The rescaled data to be transformed back.\n\n        Returns\n        -------\n        X_original : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(\n            X,\n            accept_sparse=(\"csr\", \"csc\"),\n            copy=self.copy,\n            dtype=FLOAT_DTYPES,\n            force_writeable=True,\n            ensure_all_finite=\"allow-nan\",\n        )\n\n        if sparse.issparse(X):\n            if self.with_scaling:\n                inplace_column_scale(X, self.scale_)\n        else:\n "}, {"start_line": 57000, "end_line": 59000, "belongs_to": {"file_name": "_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "          \" instead. See docstring for motivation and alternatives.\"\n                )\n            self.center_ = np.nanmedian(X, axis=0)\n        else:\n            self.center_ = None\n\n        if self.with_scaling:\n            quantiles = []\n            for feature_idx in range(X.shape[1]):\n                if sparse.issparse(X):\n                    column_nnz_data = X.data[\n                        X.indptr[feature_idx] : X.indptr[feature_idx + 1]\n                    ]\n                    column_data = np.zeros(shape=X.shape[0], dtype=X.dtype)\n                    column_data[: len(column_nnz_data)] = column_nnz_data\n                else:\n                    column_data = X[:, feature_idx]\n\n                quantiles.append(np.nanpercentile(column_data, self.quantile_range))\n\n            quantiles = np.transpose(quantiles)\n\n            self.scale_ = quantiles[1] - quantiles[0]\n            self.scale_ = _handle_zeros_in_scale(self.scale_, copy=False)\n            if self.unit_variance:\n                adjust = stats.norm.ppf(q_max / 100.0) - stats.norm.ppf(q_min / 100.0)\n                self.scale_ = self.scale_ / adjust\n        else:\n            self.scale_ = None\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Center and scale the data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data used to scale along the specified axis.\n\n        Returns\n        -------\n        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n        \"\"\"\n        check_is_fitted(self)\n        X = validate_data(\n            self,\n            X,\n            accept_sparse=(\"csr\", \"csc\"),\n            copy=self.copy,\n            dtype=FLOAT_DTYPES,\n            force_writeable=True,\n            reset=False,\n            ensure_all_finite=\"allow-nan\",\n        )\n\n        if sparse.issparse(X):\n            if self.with_scaling:\n                inplace_column_scale(X, 1"}, {"start_line": 114000, "end_line": 116000, "belongs_to": {"file_name": "_forest.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "hild node.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        # Parameters are validated in fit_transform\n        self.fit_transform(X, y, sample_weight=sample_weight)\n        return self\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit_transform(self, X, y=None, sample_weight=None):\n        \"\"\"\n        Fit estimator and transform dataset.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data used to build forests. Use ``dtype=np.float32`` for\n            maximum efficiency.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted. Splits\n            that would create child nodes with net zero or negative weight are\n            ignored while searching for a split in each node. In the case of\n            classification, splits are also ignored if they would result in any\n            single class carrying a negative weight in either child node.\n\n        Returns\n        -------\n        X_transformed : sparse matrix of shape (n_samples, n_out)\n            Transformed dataset.\n        \"\"\"\n        rnd = check_random_state(self.random_state)\n        y = rnd.uniform(size=_num_samples(X))\n        super().fit(X, y, sample_weight=sample_weight)\n\n        self.one_hot_encoder_ = OneHotEncoder(sparse_output=self.sparse_output)\n        output = self.one_hot_encoder_.fit_transform(self.apply(X))\n        self._n_features_out = output.shape[1]\n        return output\n\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in :met"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "_dict_vectorizer.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/feature_extraction", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "nce(v, Mapping):\n                    raise TypeError(\n                        f\"Unsupported value type {type(v)} \"\n                        f\"for {f}: {v}.\\n\"\n                        \"Mapping objects are not supported.\"\n                    )\n                elif isinstance(v, Iterable):\n                    feature_name = None\n                    self._add_iterable_element(f, v, feature_names, vocab)\n\n                if feature_name is not None:\n                    if feature_name not in vocab:\n                        vocab[feature_name] = len(feature_names)\n                        feature_names.append(feature_name)\n\n        if self.sort:\n            feature_names.sort()\n            vocab = {f: i for i, f in enumerate(feature_names)}\n\n        self.feature_names_ = feature_names\n        self.vocabulary_ = vocab\n\n        return self\n\n    def _transform(self, X, fitting):\n        # Sanity check: Python's array has no way of explicitly requesting the\n        # signed 32-bit integers that scipy.sparse needs, so we use the next\n        # best thing: typecode \"i\" (int). However, if that gives larger or\n        # smaller integers than 32-bit ones, np.frombuffer screws up.\n        assert array(\"i\").itemsize == 4, (\n            \"sizeof(int) != 4 on your platform; please report this at\"\n            \" https://github.com/scikit-learn/scikit-learn/issues and\"\n            \" include the output from platform.platform() in your bug report\"\n        )\n\n        dtype = self.dtype\n        if fitting:\n            feature_names = []\n            vocab = {}\n        else:\n            feature_names = self.feature_names_\n            vocab = self.vocabulary_\n\n        transforming = True\n\n        # Process everything as sparse regardless of setting\n        X = [X] if isinstance(X, Mapping) else X\n\n        indices = array(\"i\")\n        indptr = [0]\n        # XXX we could change values to an array.array as well, but it\n        # would require (heuristic) conversion of dtype to typecode...\n        valu"}, {"start_line": 60000, "end_line": 62000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "put data to be transformed.\n\n    y : ndarray of shape (n_samples,)\n        Ignored.\n\n    weight : float\n        Weight to be applied to the output of the transformation.\n\n    params : dict\n        Parameters to be passed to the transformer's ``transform`` method.\n\n        This should be of the form ``process_routing()[\"step_name\"]``.\n    \"\"\"\n    res = transformer.transform(X, **params.transform)\n    # if we have a weight for this transformer, multiply output\n    if weight is None:\n        return res\n    return res * weight\n\n\ndef _fit_transform_one(\n    transformer, X, y, weight, message_clsname=\"\", message=None, params=None\n):\n    \"\"\"\n    Fits ``transformer`` to ``X`` and ``y``. The transformed result is returned\n    with the fitted transformer. If ``weight`` is not ``None``, the result will\n    be multiplied by ``weight``.\n\n    ``params`` needs to be of the form ``process_routing()[\"step_name\"]``.\n    \"\"\"\n    params = params or {}\n    with _print_elapsed_time(message_clsname, message):\n        if hasattr(transformer, \"fit_transform\"):\n            res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n        else:\n            res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\n                X, **params.get(\"transform\", {})\n            )\n\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n\ndef _fit_one(transformer, X, y, weight, message_clsname=\"\", message=None, params=None):\n    \"\"\"\n    Fits ``transformer`` to ``X`` and ``y``.\n    \"\"\"\n    with _print_elapsed_time(message_clsname, message):\n        return transformer.fit(X, y, **params[\"fit\"])\n\n\nclass FeatureUnion(TransformerMixin, _BaseComposition):\n    \"\"\"Concatenates results of multiple transformer objects.\n\n    This estimator applies a list of transformer objects in parallel to the\n    input data, then concatenates the results. This is useful to combine\n    several feature extraction mechanisms into a single transformer.\n\n    Parameters of"}, {"start_line": 45000, "end_line": 47000, "belongs_to": {"file_name": "_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "is=0, ignore_nan=True)\n            max_abs = np.maximum(np.abs(mins), np.abs(maxs))\n        else:\n            max_abs = _array_api._nanmax(xp.abs(X), axis=0, xp=xp)\n\n        if first_pass:\n            self.n_samples_seen_ = X.shape[0]\n        else:\n            max_abs = xp.maximum(self.max_abs_, max_abs)\n            self.n_samples_seen_ += X.shape[0]\n\n        self.max_abs_ = max_abs\n        self.scale_ = _handle_zeros_in_scale(max_abs, copy=True)\n        return self\n\n    def transform(self, X):\n        \"\"\"Scale the data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data that should be scaled.\n\n        Returns\n        -------\n        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n        \"\"\"\n        check_is_fitted(self)\n\n        xp, _ = get_namespace(X)\n\n        X = validate_data(\n            self,\n            X,\n            accept_sparse=(\"csr\", \"csc\"),\n            copy=self.copy,\n            reset=False,\n            dtype=_array_api.supported_float_dtypes(xp),\n            force_writeable=True,\n            ensure_all_finite=\"allow-nan\",\n        )\n\n        if sparse.issparse(X):\n            inplace_column_scale(X, 1.0 / self.scale_)\n            if self.clip:\n                np.clip(X.data, -1.0, 1.0, out=X.data)\n        else:\n            X /= self.scale_\n            if self.clip:\n                device_ = device(X)\n                X = _modify_in_place_if_numpy(\n                    xp,\n                    xp.clip,\n                    X,\n                    xp.asarray(-1.0, dtype=X.dtype, device=device_),\n                    xp.asarray(1.0, dtype=X.dtype, device=device_),\n                    out=X,\n                )\n        return X\n\n    def inverse_transform(self, X):\n        \"\"\"Scale back the data to the original representation.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_feature"}], "retrieved_count": 10, "cost_time": 1.0756194591522217}
{"question": "Why does Scikit-learn use a pipeline system for chaining preprocessing and model steps instead of separate function calls?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn uses a pipeline system for chaining preprocessing and model steps instead of separate function calls for several fundamental design reasons that enhance usability, safety, and maintainability:\n\n1. **Data Leakage Prevention**:\n   - **Automatic Safety**: Pipeline ensures preprocessing is only fitted on training data\n   - **Cross-Validation Safety**: Prevents test data from leaking into preprocessing steps\n   - **Consistent Application**: Same preprocessing is automatically applied to test data\n   - **Production Safety**: Prevents using future information in real-world applications\n   - **Error Prevention**: Reduces risk of accidentally applying fit() to test data\n\n2. **Convenience and Encapsulation**:\n   - **Single Interface**: One fit() and predict() call handles entire workflow\n   - **Reduced Boilerplate**: No need to manually chain multiple function calls\n   - **Cleaner Code**: Eliminates repetitive preprocessing code\n   - **Method Chaining**: Enables fluent API: pipeline.fit(X_train, y_train).predict(X_test)\n   - **Simplified Workflow**: Complex preprocessing chains become single objects\n\n3. **Joint Parameter Selection**:\n   - **Unified Hyperparameter Tuning**: GridSearchCV can optimize parameters across all steps\n   - **Nested Parameter Access**: Parameters accessible via step__parameter syntax\n   - **Cross-Step Optimization**: Can optimize preprocessing and model parameters together\n   - **Consistent Validation**: All parameters validated in cross-validation context\n   - **Efficient Search**: Avoids redundant parameter combinations\n\n4. **Consistency and Reproducibility**:\n   - **Guaranteed Order**: Steps are always executed in the same sequence\n   - **Reproducible Results**: Same pipeline produces same results given same data\n   - **State Management**: Pipeline maintains state across all steps\n   - **Error Handling**: Consistent error handling across all pipeline steps\n   - **Debugging**: Easier to debug issues in complex workflows\n\n5. **Integration with Meta-Estimators**:\n   - **GridSearchCV Compatibility**: Works seamlessly with hyperparameter tuning\n   - **Cross-Validation Integration**: Pipeline steps are properly handled in CV\n   - **FeatureUnion Support**: Can combine pipelines with parallel feature processing\n   - **Model Selection**: Easy comparison of different preprocessing strategies\n   - **Ensemble Methods**: Pipelines can be used in voting and stacking ensembles\n\n6. **Performance and Efficiency**:\n   - **Caching Support**: Can cache expensive preprocessing steps\n   - **Parallel Processing**: Pipeline can be parallelized in meta-estimators\n   - **Memory Efficiency**: Avoids storing intermediate results unnecessarily\n   - **Optimized Execution**: Pipeline can optimize step execution order\n   - **Resource Management**: Better control over computational resources\n\n7. **Advanced Features**:\n   - **Feature Name Tracking**: Preserves feature names through pipeline steps\n   - **Metadata Routing**: Advanced metadata routing through pipeline steps\n   - **Inverse Transform**: Can reverse transformations when needed\n   - **Partial Fitting**: Some pipelines support incremental learning\n   - **Model Persistence**: Entire pipeline can be saved and loaded\n\n8. **Maintainability and Extensibility**:\n   - **Modular Design**: Easy to add, remove, or modify pipeline steps\n   - **Reusable Components**: Pipeline steps can be reused in different contexts\n   - **Testing**: Easier to test complex workflows as single units\n   - **Documentation**: Pipeline structure is self-documenting\n   - **Version Control**: Pipeline configurations can be version controlled\n\n9. **Educational and Debugging Benefits**:\n   - **Clear Workflow**: Pipeline structure makes workflow explicit\n   - **Step Inspection**: Can inspect intermediate results at each step\n   - **Error Localization**: Easier to identify which step causes issues\n   - **Learning**: Helps users understand proper machine learning workflow\n   - **Best Practices**: Reinforces proper preprocessing and modeling practices\n\n10. **Production Readiness**:\n    - **Deployment Safety**: Ensures preprocessing is applied consistently in production\n    - **Model Versioning**: Entire pipeline can be versioned and deployed\n    - **Monitoring**: Easier to monitor pipeline performance and behavior\n    - **Scalability**: Pipeline can be scaled across different environments\n    - **Compliance**: Helps ensure regulatory compliance in sensitive applications", "score": null, "retrieved_content": [{"start_line": 59000, "end_line": 61000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "p : Pipeline\n        Returns a scikit-learn :class:`Pipeline` object.\n\n    See Also\n    --------\n    Pipeline : Class for creating a pipeline of transforms with a final\n        estimator.\n\n    Examples\n    --------\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.pipeline import make_pipeline\n    >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('gaussiannb', GaussianNB())])\n    \"\"\"\n    return Pipeline(\n        _name_estimators(steps),\n        transform_input=transform_input,\n        memory=memory,\n        verbose=verbose,\n    )\n\n\ndef _transform_one(transformer, X, y, weight, params):\n    \"\"\"Call transform and apply weight to output.\n\n    Parameters\n    ----------\n    transformer : estimator\n        Estimator to be used for transformation.\n\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Input data to be transformed.\n\n    y : ndarray of shape (n_samples,)\n        Ignored.\n\n    weight : float\n        Weight to be applied to the output of the transformation.\n\n    params : dict\n        Parameters to be passed to the transformer's ``transform`` method.\n\n        This should be of the form ``process_routing()[\"step_name\"]``.\n    \"\"\"\n    res = transformer.transform(X, **params.transform)\n    # if we have a weight for this transformer, multiply output\n    if weight is None:\n        return res\n    return res * weight\n\n\ndef _fit_transform_one(\n    transformer, X, y, weight, message_clsname=\"\", message=None, params=None\n):\n    \"\"\"\n    Fits ``transformer`` to ``X`` and ``y``. The transformed result is returned\n    with the fitted transformer. If ``weight`` is not ``None``, the result will\n    be multiplied by ``weight``.\n\n    ``params`` needs to be of the form ``process_routing()[\"step_name\"]``.\n    \"\"\"\n    params = params or {}\n    with _print_elapsed_time(message_clsname, message)"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "mposition):\n    \"\"\"\n    A sequence of data transformers with an optional final predictor.\n\n    `Pipeline` allows you to sequentially apply a list of transformers to\n    preprocess the data and, if desired, conclude the sequence with a final\n    :term:`predictor` for predictive modeling.\n\n    Intermediate steps of the pipeline must be transformers, that is, they\n    must implement `fit` and `transform` methods.\n    The final :term:`estimator` only needs to implement `fit`.\n    The transformers in the pipeline can be cached using ``memory`` argument.\n\n    The purpose of the pipeline is to assemble several steps that can be\n    cross-validated together while setting different parameters. For this, it\n    enables setting parameters of the various steps using their names and the\n    parameter name separated by a `'__'`, as in the example below. A step's\n    estimator may be replaced entirely by setting the parameter with its name\n    to another estimator, or a transformer removed by setting it to\n    `'passthrough'` or `None`.\n\n    For an example use case of `Pipeline` combined with\n    :class:`~sklearn.model_selection.GridSearchCV`, refer to\n    :ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`. The\n    example :ref:`sphx_glr_auto_examples_compose_plot_digits_pipe.py` shows how\n    to grid search on a pipeline using `'__'` as a separator in the parameter names.\n\n    Read more in the :ref:`User Guide <pipeline>`.\n\n    .. versionadded:: 0.5\n\n    Parameters\n    ----------\n    steps : list of tuples\n        List of (name of step, estimator) tuples that are to be chained in\n        sequential order. To be compatible with the scikit-learn API, all steps\n        must define `fit`. All non-last steps must also define `transform`. See\n        :ref:`Combining Estimators <combining_estimators>` for more details.\n\n    transform_input : list of str, default=None\n        The names of the :term:`metadata` parameters that should be transformed by the\n        pipeline befo"}, {"start_line": 58000, "end_line": 60000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e transformer instance given to the\n        pipeline cannot be inspected directly. Use the attribute ``named_steps``\n        or ``steps`` to inspect estimators within the pipeline. Caching the\n        transformers is advantageous when fitting is time consuming.\n\n    transform_input : list of str, default=None\n        This enables transforming some input arguments to ``fit`` (other than ``X``)\n        to be transformed by the steps of the pipeline up to the step which requires\n        them. Requirement is defined via :ref:`metadata routing <metadata_routing>`.\n        This can be used to pass a validation set through the pipeline for instance.\n\n        You can only set this if metadata routing is enabled, which you\n        can enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n\n        .. versionadded:: 1.6\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting each step will be printed as it\n        is completed.\n\n    Returns\n    -------\n    p : Pipeline\n        Returns a scikit-learn :class:`Pipeline` object.\n\n    See Also\n    --------\n    Pipeline : Class for creating a pipeline of transforms with a final\n        estimator.\n\n    Examples\n    --------\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.pipeline import make_pipeline\n    >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('gaussiannb', GaussianNB())])\n    \"\"\"\n    return Pipeline(\n        _name_estimators(steps),\n        transform_input=transform_input,\n        memory=memory,\n        verbose=verbose,\n    )\n\n\ndef _transform_one(transformer, X, y, weight, params):\n    \"\"\"Call transform and apply weight to output.\n\n    Parameters\n    ----------\n    transformer : estimator\n        Estimator to be used for transformation.\n\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        In"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ring :term:`fit`. Only defined if the\n        underlying first estimator in `steps` exposes such an attribute\n        when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    make_pipeline : Convenience function for simplified pipeline construction.\n\n    Examples\n    --------\n    >>> from sklearn.svm import SVC\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.pipeline import Pipeline\n    >>> X, y = make_classification(random_state=0)\n    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n    ...                                                     random_state=0)\n    >>> pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n    >>> # The pipeline can be used as any other estimator\n    >>> # and avoids leaking the test set into the train set\n    >>> pipe.fit(X_train, y_train).score(X_test, y_test)\n    0.88\n    >>> # An estimator's parameter can be set using '__' syntax\n    >>> pipe.set_params(svc__C=10).fit(X_train, y_train).score(X_test, y_test)\n    0.76\n    \"\"\"\n\n    # BaseEstimator interface\n    _parameter_constraints: dict = {\n        \"steps\": [list, Hidden(tuple)],\n        \"transform_input\": [list, None],\n        \"memory\": [None, str, HasMethods([\"cache\"])],\n        \"verbose\": [\"boolean\"],\n    }\n\n    def __init__(self, steps, *, transform_input=None, memory=None, verbose=False):\n        self.steps = steps\n        self.transform_input = transform_input\n        self.memory = memory\n        self.verbose = verbose\n\n    def set_output(self, *, transform=None):\n        \"\"\"Set the output container when `\"transform\"` and `\"fit_transform\"` are called.\n\n        Calling"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "plot_release_highlights_0_23_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "01)\nglm.fit(X_train, y_train)\ngbdt.fit(X_train, y_train)\nprint(glm.score(X_test, y_test))\nprint(gbdt.score(X_test, y_test))\n\n##############################################################################\n# Rich visual representation of estimators\n# -----------------------------------------\n# Estimators can now be visualized in notebooks by enabling the\n# `display='diagram'` option. This is particularly useful to summarise the\n# structure of pipelines and other composite estimators, with interactivity to\n# provide detail.  Click on the example image below to expand Pipeline\n# elements.  See :ref:`visualizing_composite_estimators` for how you can use\n# this feature.\n\nfrom sklearn import set_config\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nset_config(display=\"diagram\")\n\nnum_proc = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n\ncat_proc = make_pipeline(\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n    OneHotEncoder(handle_unknown=\"ignore\"),\n)\n\npreprocessor = make_column_transformer(\n    (num_proc, (\"feat1\", \"feat3\")), (cat_proc, (\"feat0\", \"feat2\"))\n)\n\nclf = make_pipeline(preprocessor, LogisticRegression())\nclf\n\n##############################################################################\n# Scalability and stability improvements to KMeans\n# ------------------------------------------------\n# The :class:`~sklearn.cluster.KMeans` estimator was entirely re-worked, and it\n# is now significantly faster and more stable. In addition, the Elkan algorithm\n# is now compatible with sparse matrices. The estimator uses OpenMP based\n# parallelism instead of relying on joblib, so the `n_jobs` parameter has no\n# effect anymore. For more details on how to control the number of threads,\n# please refer to our :ref:`parallelism` notes.\nimport nump"}, {"start_line": 56000, "end_line": 58000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ba\")\n            .add(caller=\"decision_function\", callee=\"decision_function\")\n            .add(caller=\"predict_log_proba\", callee=\"predict_log_proba\")\n            .add(caller=\"transform\", callee=\"transform\")\n            .add(caller=\"inverse_transform\", callee=\"inverse_transform\")\n            .add(caller=\"score\", callee=\"score\")\n        )\n\n        router.add(method_mapping=method_mapping, **{final_name: final_est})\n        return router\n\n\ndef _name_estimators(estimators):\n    \"\"\"Generate names for estimators.\"\"\"\n\n    names = [\n        estimator if isinstance(estimator, str) else type(estimator).__name__.lower()\n        for estimator in estimators\n    ]\n    namecount = defaultdict(int)\n    for est, name in zip(estimators, names):\n        namecount[name] += 1\n\n    for k, v in list(namecount.items()):\n        if v == 1:\n            del namecount[k]\n\n    for i in reversed(range(len(estimators))):\n        name = names[i]\n        if name in namecount:\n            names[i] += \"-%d\" % namecount[name]\n            namecount[name] -= 1\n\n    return list(zip(names, estimators))\n\n\ndef make_pipeline(*steps, memory=None, transform_input=None, verbose=False):\n    \"\"\"Construct a :class:`Pipeline` from the given estimators.\n\n    This is a shorthand for the :class:`Pipeline` constructor; it does not\n    require, and does not permit, naming the estimators. Instead, their names\n    will be set to the lowercase of their types automatically.\n\n    Parameters\n    ----------\n    *steps : list of Estimator objects\n        List of the scikit-learn estimators that are chained together.\n\n    memory : str or object with the joblib.Memory interface, default=None\n        Used to cache the fitted transformers of the pipeline. The last step\n        will never be cached, even if it is a transformer. By default, no\n        caching is performed. If a string is given, it is the path to the\n        caching directory. Enabling caching triggers a clone of the transformers\n        before fitting. Therefore, th"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "plot_column_transformer_mixed_types.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/compose", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ss:`~pipeline.Pipeline`, together with a simple classification\nmodel.\n\n\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\n# %%\nimport numpy as np\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.feature_selection import SelectPercentile, chi2\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nnp.random.seed(0)\n\n# %%\n# Load data from https://www.openml.org/d/40945\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n\n# Alternatively X and y can be obtained directly from the frame attribute:\n# X = titanic.frame.drop('survived', axis=1)\n# y = titanic.frame['survived']\n\n# %%\n# Use ``ColumnTransformer`` by selecting column by names\n#\n# We will train our classifier with the following features:\n#\n# Numeric Features:\n#\n# * ``age``: float;\n# * ``fare``: float.\n#\n# Categorical Features:\n#\n# * ``embarked``: categories encoded as strings ``{'C', 'S', 'Q'}``;\n# * ``sex``: categories encoded as strings ``{'female', 'male'}``;\n# * ``pclass``: ordinal integers ``{1, 2, 3}``.\n#\n# We create the preprocessing pipelines for both numeric and categorical data.\n# Note that ``pclass`` could either be treated as a categorical or numeric\n# feature.\n\nnumeric_features = [\"age\", \"fare\"]\nnumeric_transformer = Pipeline(\n    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n)\n\ncategorical_features = [\"embarked\", \"sex\", \"pclass\"]\ncategorical_transformer = Pipeline(\n    steps=[\n        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n        (\"selector\", SelectPercentile(chi2, percentile=50)),\n    ]\n)\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_features),\n        (\"cat\", categorical_transf"}, {"start_line": 37000, "end_line": 39000, "belongs_to": {"file_name": "api_reference.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/doc", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ernoulliRBM\", \"MLPClassifier\", \"MLPRegressor\"],\n            },\n        ],\n    },\n    \"sklearn.pipeline\": {\n        \"short_summary\": \"Pipeline.\",\n        \"description\": _get_guide(\"combining_estimators\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"FeatureUnion\",\n                    \"Pipeline\",\n                    \"make_pipeline\",\n                    \"make_union\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.preprocessing\": {\n        \"short_summary\": \"Preprocessing and normalization.\",\n        \"description\": _get_guide(\"preprocessing\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"Binarizer\",\n                    \"FunctionTransformer\",\n                    \"KBinsDiscretizer\",\n                    \"KernelCenterer\",\n                    \"LabelBinarizer\",\n                    \"LabelEncoder\",\n                    \"MaxAbsScaler\",\n                    \"MinMaxScaler\",\n                    \"MultiLabelBinarizer\",\n                    \"Normalizer\",\n                    \"OneHotEncoder\",\n                    \"OrdinalEncoder\",\n                    \"PolynomialFeatures\",\n                    \"PowerTransformer\",\n                    \"QuantileTransformer\",\n                    \"RobustScaler\",\n                    \"SplineTransformer\",\n                    \"StandardScaler\",\n                    \"TargetEncoder\",\n                    \"add_dummy_feature\",\n                    \"binarize\",\n                    \"label_binarize\",\n                    \"maxabs_scale\",\n                    \"minmax_scale\",\n                    \"normalize\",\n                    \"power_transform\",\n                    \"quantile_transform\",\n                    \"robust_scale\",\n                    \"scale\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.random_projection\": {\n        \"short_summary\": \"Random projection.\",\n        \"description\": _get_guide(\"r"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "plot_stack_predictors.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "linear model, one needs to\n# one-hot encode the categories. If the ending regressor is a tree-based model\n# an ordinal encoder will be sufficient. Besides, numerical values need to be\n# standardized for a linear model while the raw numerical data can be treated\n# as is by a tree-based model. However, both models need an imputer to\n# handle missing values.\n#\n# We will first design the pipeline required for the tree-based models.\n\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import OrdinalEncoder\n\ncat_tree_processor = OrdinalEncoder(\n    handle_unknown=\"use_encoded_value\",\n    unknown_value=-1,\n    encoded_missing_value=-2,\n)\nnum_tree_processor = SimpleImputer(strategy=\"mean\", add_indicator=True)\n\ntree_preprocessor = make_column_transformer(\n    (num_tree_processor, num_selector), (cat_tree_processor, cat_selector)\n)\ntree_preprocessor\n\n# %%\n# Then, we will now define the preprocessor used when the ending regressor\n# is a linear model.\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\ncat_linear_processor = OneHotEncoder(handle_unknown=\"ignore\")\nnum_linear_processor = make_pipeline(\n    StandardScaler(), SimpleImputer(strategy=\"mean\", add_indicator=True)\n)\n\nlinear_preprocessor = make_column_transformer(\n    (num_linear_processor, num_selector), (cat_linear_processor, cat_selector)\n)\nlinear_preprocessor\n\n# %%\n# Stack of predictors on a single data set\n# ########################################\n#\n# It is sometimes tedious to find the model which will best perform on a given\n# dataset. Stacking provide an alternative by combining the outputs of several\n# learners, without the need to choose a model specifically. The performance of\n# stacking is usually close to the best model and sometimes it can outperform\n# the prediction performance of each individual model.\n#\n# Here, we combine 3 learners (linear and non-linear) and use a ridge regresso"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "plot_linear_model_coefficient_interpretation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/inspection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n# as ridge or lasso work best for a normal distribution of error).\n#\n# The WAGE is increasing when EDUCATION is increasing.\n# Note that the dependence between WAGE and EDUCATION\n# represented here is a marginal dependence, i.e. it describes the behavior\n# of a specific variable without keeping the others fixed.\n#\n# Also, the EXPERIENCE and AGE are strongly linearly correlated.\n#\n# .. _the-pipeline:\n#\n# The machine-learning pipeline\n# -----------------------------\n#\n# To design our machine-learning pipeline, we first manually\n# check the type of data that we are dealing with:\n\nsurvey.data.info()\n\n# %%\n# As seen previously, the dataset contains columns with different data types\n# and we need to apply a specific preprocessing for each data types.\n# In particular categorical variables cannot be included in linear model if not\n# coded as integers first. In addition, to avoid categorical features to be\n# treated as ordered values, we need to one-hot-encode them.\n# Our pre-processor will:\n#\n# - one-hot encode (i.e., generate a column by category) the categorical\n#   columns, only for non-binary categorical variables;\n# - as a first approach (we will see after how the normalisation of numerical\n#   values will affect our discussion), keep numerical values as they are.\n\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_columns = [\"RACE\", \"OCCUPATION\", \"SECTOR\", \"MARR\", \"UNION\", \"SEX\", \"SOUTH\"]\nnumerical_columns = [\"EDUCATION\", \"EXPERIENCE\", \"AGE\"]\n\npreprocessor = make_column_transformer(\n    (OneHotEncoder(drop=\"if_binary\"), categorical_columns),\n    remainder=\"passthrough\",\n    verbose_feature_names_out=False,  # avoid to prepend the preprocessor names\n)\n\n# %%\n# We use a ridge regressor\n# with a very small regularization to model the logarithm of the WAGE.\n\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipel"}], "retrieved_count": 10, "cost_time": 1.088691234588623}
{"question": "Why does Scikit-learn use a consistent parameter naming convention across all estimators?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn uses a consistent parameter naming convention across all estimators for several fundamental reasons that enhance usability, maintainability, and API design:\n\n1. **User Experience and Learning Curve**:\n   - **Familiarity**: Users can transfer knowledge between different estimators\n   - **Intuitive Usage**: Consistent names make parameters self-explanatory\n   - **Reduced Cognitive Load**: Users don't need to remember different parameter names for similar concepts\n   - **Documentation Efficiency**: Users can quickly understand new estimators\n   - **Error Reduction**: Consistent naming reduces parameter-related errors\n\n2. **API Consistency and Predictability**:\n   - **Unified Interface**: All estimators follow the same parameter naming patterns\n   - **Predictable Behavior**: Users can predict parameter names based on functionality\n   - **Cross-Estimator Compatibility**: Parameters work consistently across different estimators\n   - **Meta-Estimator Support**: GridSearchCV, Pipeline, etc. work seamlessly with consistent naming\n   - **Parameter Routing**: Advanced parameter routing relies on consistent naming conventions\n\n3. **Maintenance and Development**:\n   - **Code Reusability**: Common parameter validation logic can be shared\n   - **Testing Efficiency**: Test suites can be designed around consistent parameter patterns\n   - **Documentation Consistency**: Parameter documentation follows established patterns\n   - **Bug Prevention**: Consistent naming reduces parameter-related bugs\n   - **Code Review**: Easier to review and maintain code with consistent conventions\n\n4. **Specific Naming Conventions**:\n   - **Trailing Underscore**: Estimated attributes end with underscore (e.g., coef_, intercept_)\n   - **Leading Underscore**: Private attributes start with underscore (e.g., _intermediate_coefs)\n   - **Common Parameters**: Standard parameters like random_state, n_jobs, verbose\n   - **Algorithm-Specific**: Parameters reflect the underlying algorithm (e.g., C for SVM, alpha for regularization)\n   - **Functional Names**: Parameters describe their function (e.g., max_depth, min_samples_split)\n\n5. **Parameter Validation and Type Safety**:\n   - **Consistent Validation**: Similar parameters can use the same validation logic\n   - **Type Checking**: Consistent types enable better type checking and validation\n   - **Constraint Definition**: Common constraints can be defined once and reused\n   - **Error Messages**: Consistent error messages for similar parameter types\n   - **Documentation Generation**: Automated documentation can follow consistent patterns\n\n6. **Integration and Interoperability**:\n   - **Pipeline Compatibility**: Consistent naming enables seamless pipeline integration\n   - **Meta-Estimator Support**: GridSearchCV, RandomizedSearchCV work with any estimator\n   - **Parameter Inheritance**: Subclasses can inherit parameter handling from base classes\n   - **Third-Party Compatibility**: External libraries can easily integrate with scikit-learn\n   - **API Evolution**: Consistent naming facilitates API evolution and deprecation\n\n7. **Educational and Documentation Benefits**:\n   - **Learning Transfer**: Knowledge transfers between different estimators\n   - **Documentation Clarity**: Consistent documentation patterns across all estimators\n   - **Example Reusability**: Examples can be adapted across different estimators\n   - **Best Practices**: Reinforces good API design practices\n   - **Community Understanding**: Easier for the community to understand and contribute\n\n8. **Advanced Features Support**:\n   - **Parameter Routing**: Metadata routing relies on consistent parameter names\n   - **Nested Parameters**: Double underscore notation (estimator__param) for nested estimators\n   - **Dynamic Parameter Access**: get_params() and set_params() work consistently\n   - **Parameter Cloning**: Deep cloning of estimators preserves parameter structure\n   - **Serialization**: Consistent naming enables proper model serialization\n\n9. **Performance and Optimization**:\n   - **Caching**: Parameter-based caching can be implemented consistently\n   - **Warm Starting**: Consistent parameter names enable warm-starting across estimators\n   - **Parallel Processing**: Parameter-based parallelization works consistently\n   - **Memory Management**: Consistent parameter handling enables better memory management\n   - **Optimization**: Parameter optimization can be generalized across estimators\n\n10. **Future-Proofing and Extensibility**:\n    - **API Evolution**: Consistent naming facilitates future API changes\n    - **Backward Compatibility**: Easier to maintain backward compatibility\n    - **Feature Addition**: New features can follow established naming patterns\n    - **Deprecation Management**: Consistent naming simplifies deprecation processes\n    - **Version Migration**: Users can more easily migrate between versions", "score": null, "retrieved_content": [{"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])\n    \"\"\"\n\n    _html_repr = estimator_html_repr\n\n    @classmethod\n    def _get_param_names(cls):\n        \"\"\"Get parameter names for the estimator\"\"\"\n        # fetch the constructor or the original constructor before\n        # deprecation wrapping if any\n        init = getattr(cls.__init__, \"deprecated_original\", cls.__init__)\n        if init is object.__init__:\n            # No explicit constructor to introspect\n            return []\n\n        # introspect the constructor arguments to find the model parameters\n        # to represent\n        init_signature = inspect.signature(init)\n        # Consider the constructor parameters excluding 'self'\n        parameters = [\n            p\n            for p in init_signature.parameters.values()\n            if p.name != \"self\" and p.kind != p.VAR_KEYWORD\n        ]\n        for p in parameters:\n            if p.kind == p.VAR_POSITIONAL:\n                raise RuntimeError(\n                    \"scikit-learn estimators should always \"\n                    \"specify their para"}, {"start_line": 146000, "end_line": 148000, "belongs_to": {"file_name": "estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "           random_state=0,\n                n_features=2,\n                cluster_std=0.1,\n            )\n            X = _enforce_estimator_tags_X(estimator_orig, X)\n        set_random_state(estimator, 0)\n        estimator.fit(X, y_)\n\n        # These return a n_iter per component.\n        if name in CROSS_DECOMPOSITION:\n            for iter_ in estimator.n_iter_:\n                assert iter_ >= 1\n        else:\n            assert estimator.n_iter_ >= 1\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_get_params_invariance(name, estimator_orig):\n    # Checks if get_params(deep=False) is a subset of get_params(deep=True)\n    e = clone(estimator_orig)\n\n    shallow_params = e.get_params(deep=False)\n    deep_params = e.get_params(deep=True)\n\n    assert all(item in deep_params.items() for item in shallow_params.items())\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_set_params(name, estimator_orig):\n    # Check that get_params() returns the same thing\n    # before and after set_params() with some fuzz\n    estimator = clone(estimator_orig)\n\n    orig_params = estimator.get_params(deep=False)\n    msg = \"get_params result does not match what was passed to set_params\"\n\n    estimator.set_params(**orig_params)\n    curr_params = estimator.get_params(deep=False)\n    assert set(orig_params.keys()) == set(curr_params.keys()), msg\n    for k, v in curr_params.items():\n        assert orig_params[k] is v, msg\n\n    # some fuzz values\n    test_values = [-np.inf, np.inf, None]\n\n    test_params = deepcopy(orig_params)\n    for param_name in orig_params.keys():\n        default_value = orig_params[param_name]\n        for value in test_values:\n            test_params[param_name] = value\n            try:\n                estimator.set_params(**test_params)\n            except (TypeError, ValueError) as e:\n                e_type = e.__class__.__name__\n                # Exception occurred, possibly parameter validation\n                warnings.warn(\n                    \"{0} occurred during s"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "aram2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n\n    # _sklearn_output_config is used by `set_output` to configure the output\n    # container of an estimator.\n    if hasattr(estimator, \"_sklearn_output_config\"):\n        new_object._sklearn_output_config = copy.deepcopy(\n            estimator._sklearn_output_config\n        )\n    return new_object\n\n\nclass BaseEstimator(ReprHTMLMixin, _HTMLDocumentationLinkMixin, _MetadataRequester):\n    \"\"\"Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])\n    \"\"\"\n\n    _html_"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "test_pprint.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    degree=3,\n        gamma=\"auto_deprecated\",\n        coef0=0.0,\n        shrinking=True,\n        probability=False,\n        tol=1e-3,\n        cache_size=200,\n        class_weight=None,\n        verbose=False,\n        max_iter=-1,\n        decision_function_shape=\"ovr\",\n        random_state=None,\n    ):\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.tol = tol\n        self.C = C\n        self.shrinking = shrinking\n        self.probability = probability\n        self.cache_size = cache_size\n        self.class_weight = class_weight\n        self.verbose = verbose\n        self.max_iter = max_iter\n        self.decision_function_shape = decision_function_shape\n        self.random_state = random_state\n\n\nclass PCA(BaseEstimator):\n    def __init__(\n        self,\n        n_components=None,\n        copy=True,\n        whiten=False,\n        svd_solver=\"auto\",\n        tol=0.0,\n        iterated_power=\"auto\",\n        random_state=None,\n    ):\n        self.n_components = n_components\n        self.copy = copy\n        self.whiten = whiten\n        self.svd_solver = svd_solver\n        self.tol = tol\n        self.iterated_power = iterated_power\n        self.random_state = random_state\n\n\nclass NMF(BaseEstimator):\n    def __init__(\n        self,\n        n_components=None,\n        init=None,\n        solver=\"cd\",\n        beta_loss=\"frobenius\",\n        tol=1e-4,\n        max_iter=200,\n        random_state=None,\n        alpha=0.0,\n        l1_ratio=0.0,\n        verbose=0,\n        shuffle=False,\n    ):\n        self.n_components = n_components\n        self.init = init\n        self.solver = solver\n        self.beta_loss = beta_loss\n        self.tol = tol\n        self.max_iter = max_iter\n        self.random_state = random_state\n        self.alpha = alpha\n        self.l1_ratio = l1_ratio\n        self.verbose = verbose\n        self.shuffle = shuffle\n\n\nclass SimpleImputer(BaseEstimator):\n    def __init__(\n        self,\n        mis"}, {"start_line": 24000, "end_line": 26000, "belongs_to": {"file_name": "_logistic.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e: a scorer callable object (e.g., function) with signature\n          ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.\n        - `None`: :ref:`accuracy <accuracy_score>` is used.\n\n    fit_intercept : bool\n        If False, then the bias term is set to zero. Else the last\n        term of each coef_ gives us the intercept.\n\n    max_iter : int\n        Maximum number of iterations for the solver.\n\n    tol : float\n        Tolerance for stopping criteria.\n\n    class_weight : dict or 'balanced'\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n    verbose : int\n        For the liblinear and lbfgs solvers set verbose to any positive\n        number for verbosity.\n\n    solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'}\n        Decides which solver to use.\n\n    penalty : {'l1', 'l2', 'elasticnet'}\n        Used to specify the norm used in the penalization. The 'newton-cg',\n        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n        only supported by the 'saga' solver.\n\n    dual : bool\n        Dual or primal formulation. Dual formulation is only implemented for\n        l2 penalty with liblinear solver. Prefer dual=False when\n        n_samples > n_features.\n\n    intercept_scaling : float\n        Useful only when the solver `liblinear` is used\n        and `self.fit_intercept` is set to `True`. In this case, `x` becomes\n        `[x, self.intercept_scaling]`,\n        i.e. a \"synthetic\" feature with constant value equal to\n        `intercept_scaling` is appended to the ins"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "_perceptron.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "l.\n\n    eta0 : float, default=1\n        Constant by which the updates are multiplied.\n\n    n_jobs : int, default=None\n        The number of CPUs to use to do the OVA (One Versus All, for\n        multi-class problems) computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance or None, default=0\n        Used to shuffle the training data, when ``shuffle`` is set to\n        ``True``. Pass an int for reproducible output across multiple\n        function calls.\n        See :term:`Glossary <random_state>`.\n\n    early_stopping : bool, default=False\n        Whether to use early stopping to terminate training when validation\n        score is not improving. If set to True, it will automatically set aside\n        a stratified fraction of training data as validation and terminate\n        training when validation score is not improving by at least `tol` for\n        `n_iter_no_change` consecutive epochs.\n\n        .. versionadded:: 0.20\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if early_stopping is True.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=5\n        Number of iterations with no improvement to wait before early stopping.\n\n        .. versionadded:: 0.20\n\n    class_weight : dict, {class_label: weight} or \"balanced\", default=None\n        Preset for the class_weight fit parameter.\n\n        Weights associated with classes. If not given, all classes\n        are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n    warm_start : bool, default=False\n        W"}, {"start_line": 46000, "end_line": 48000, "belongs_to": {"file_name": "_ridge.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "d reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``1 / (2C)`` in other linear models such as\n        :class:`~sklearn.linear_model.LogisticRegression` or\n        :class:`~sklearn.svm.LinearSVC`.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set to false, no\n        intercept will be used in calculations (e.g. data is expected to be\n        already centered).\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    max_iter : int, default=None\n        Maximum number of iterations for conjugate gradient solver.\n        The default value is determined by scipy.sparse.linalg.\n\n    tol : float, default=1e-4\n        The precision of the solution (`coef_`) is determined by `tol` which\n        specifies a different convergence criterion for each solver:\n\n        - 'svd': `tol` has no impact.\n\n        - 'cholesky': `tol` has no impact.\n\n        - 'sparse_cg': norm of residuals smaller than `tol`.\n\n        - 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr,\n          which control the norm of the residual vector in terms of the norms of\n          matrix and coefficients.\n\n        - 'sag' and 'saga': relative change of coef smaller than `tol`.\n\n        - 'lbfgs': maximum of the absolute (projected) gradient=max|residuals|\n          smaller than `tol`.\n\n        .. versionchanged:: 1.2\n           Default value changed from 1e-3 to 1e-4 for consistency with other linear\n           models.\n\n    class_weight : dict or 'balanced', default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_pprint.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "   self.intercept_scaling = intercept_scaling\n        self.class_weight = class_weight\n        self.random_state = random_state\n        self.solver = solver\n        self.max_iter = max_iter\n        self.multi_class = multi_class\n        self.verbose = verbose\n        self.warm_start = warm_start\n        self.n_jobs = n_jobs\n        self.l1_ratio = l1_ratio\n\n    def fit(self, X, y):\n        return self\n\n\nclass StandardScaler(TransformerMixin, BaseEstimator):\n    def __init__(self, copy=True, with_mean=True, with_std=True):\n        self.with_mean = with_mean\n        self.with_std = with_std\n        self.copy = copy\n\n    def transform(self, X, copy=None):\n        return self\n\n\nclass RFE(BaseEstimator):\n    def __init__(self, estimator, n_features_to_select=None, step=1, verbose=0):\n        self.estimator = estimator\n        self.n_features_to_select = n_features_to_select\n        self.step = step\n        self.verbose = verbose\n\n\nclass GridSearchCV(BaseEstimator):\n    def __init__(\n        self,\n        estimator,\n        param_grid,\n        scoring=None,\n        n_jobs=None,\n        iid=\"warn\",\n        refit=True,\n        cv=\"warn\",\n        verbose=0,\n        pre_dispatch=\"2*n_jobs\",\n        error_score=\"raise-deprecating\",\n        return_train_score=False,\n    ):\n        self.estimator = estimator\n        self.param_grid = param_grid\n        self.scoring = scoring\n        self.n_jobs = n_jobs\n        self.iid = iid\n        self.refit = refit\n        self.cv = cv\n        self.verbose = verbose\n        self.pre_dispatch = pre_dispatch\n        self.error_score = error_score\n        self.return_train_score = return_train_score\n\n\nclass CountVectorizer(BaseEstimator):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        stop_words=None,\n        token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n        ngram_range=(1, 1),\n "}, {"start_line": 35000, "end_line": 37000, "belongs_to": {"file_name": "_stochastic_gradient.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "assumed to be already centered.\n\n    max_iter : int, default=1000\n        The maximum number of passes over the training data (aka epochs).\n        It only impacts the behavior in the ``fit`` method, and not the\n        :meth:`partial_fit` method.\n        Values must be in the range `[1, inf)`.\n\n        .. versionadded:: 0.19\n\n    tol : float or None, default=1e-3\n        The stopping criterion. If it is not None, training will stop\n        when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n        epochs.\n        Convergence is checked against the training loss or the\n        validation loss depending on the `early_stopping` parameter.\n        Values must be in the range `[0.0, inf)`.\n\n        .. versionadded:: 0.19\n\n    shuffle : bool, default=True\n        Whether or not the training data should be shuffled after each epoch.\n\n    verbose : int, default=0\n        The verbosity level.\n        Values must be in the range `[0, inf)`.\n\n    epsilon : float, default=0.1\n        Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n        For 'huber', determines the threshold at which it becomes less\n        important to get the prediction exactly right.\n        For epsilon-insensitive, any differences between the current prediction\n        and the correct label are ignored if they are less than this threshold.\n        Values must be in the range `[0.0, inf)`.\n\n    n_jobs : int, default=None\n        The number of CPUs to use to do the OVA (One Versus All, for\n        multi-class problems) computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance, default=None\n        Used for shuffling the data, when ``shuffle`` is set to ``True``.\n        Pass an int for reproducible output across multiple fun"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "plot_release_highlights_1_0_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# ruff: noqa: CPY001\n\"\"\"\n=======================================\nRelease Highlights for scikit-learn 1.0\n=======================================\n\n.. currentmodule:: sklearn\n\nWe are very pleased to announce the release of scikit-learn 1.0! The library\nhas been stable for quite some time, releasing version 1.0 is recognizing that\nand signalling it to our users. This release does not include any breaking\nchanges apart from the usual two-release deprecation cycle. For the future, we\ndo our best to keep this pattern.\n\nThis release includes some new key features as well as many improvements and\nbug fixes. We detail below a few of the major features of this release. **For\nan exhaustive list of all the changes**, please refer to the :ref:`release\nnotes <release_notes_1_0>`.\n\nTo install the latest version (with pip)::\n\n    pip install --upgrade scikit-learn\n\nor with conda::\n\n    conda install -c conda-forge scikit-learn\n\n\"\"\"\n\n##############################################################################\n# Keyword and positional arguments\n# ---------------------------------------------------------\n# The scikit-learn API exposes many functions and methods which have many input\n# parameters. For example, before this release, one could instantiate a\n# :class:`~ensemble.HistGradientBoostingRegressor` as::\n#\n#         HistGradientBoostingRegressor(\"squared_error\", 0.1, 100, 31, None,\n#             20, 0.0, 255, None, None, False, \"auto\", \"loss\", 0.1, 10, 1e-7,\n#             0, None)\n#\n# Understanding the above code requires the reader to go to the API\n# documentation and to check each and every parameter for its position and\n# its meaning. To improve the readability of code written based on scikit-learn,\n# now users have to provide most parameters with their names, as keyword\n# arguments, instead of positional arguments. For example, the above code would\n# be::\n#\n#     HistGradientBoostingRegressor(\n#         loss=\"squared_error\",\n#         learning_rate=0.1,\n#         max_iter=10"}], "retrieved_count": 10, "cost_time": 1.109856128692627}
{"question": "Why does Scikit-learn implement a fit/predict interface instead of a single method that handles both training and prediction?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn implements a fit/predict interface instead of a single method for several fundamental design reasons that align with machine learning best practices and software engineering principles:\n\n1. **Separation of Concerns**:\n   - **Training Phase (fit)**: Learns model parameters from training data\n   - **Prediction Phase (predict)**: Applies learned parameters to new data\n   - **Clear Distinction**: Each phase has distinct responsibilities and requirements\n   - **Modularity**: Allows independent optimization of training and prediction logic\n   - **Maintainability**: Easier to debug and maintain separate training and prediction code\n\n2. **Machine Learning Workflow Alignment**:\n   - **Train Once, Predict Many**: Models are typically trained once but used for multiple predictions\n   - **Cross-Validation**: Requires separate fit/predict cycles for different data splits\n   - **Hyperparameter Tuning**: GridSearchCV needs to fit multiple models and evaluate predictions\n   - **Model Persistence**: Trained models can be saved and reused without retraining\n   - **Incremental Learning**: Some models support partial_fit for online learning\n\n3. **Data Leakage Prevention**:\n   - **Clear Boundaries**: Prevents accidental use of test data during training\n   - **Validation Integrity**: Ensures proper train/test separation in cross-validation\n   - **Pipeline Safety**: Prevents data leakage in preprocessing pipelines\n   - **Best Practices**: Enforces proper machine learning workflow\n   - **Error Prevention**: Reduces risk of using future information in predictions\n\n4. **Performance Optimization**:\n   - **Training Optimization**: Can optimize training algorithms independently\n   - **Prediction Optimization**: Can optimize prediction algorithms for speed\n   - **Memory Efficiency**: Training can use more memory, prediction can be optimized for speed\n   - **Caching**: Trained models can be cached and reused\n   - **Parallel Processing**: Different phases can be parallelized differently\n\n5. **API Consistency and Composability**:\n   - **Pipeline Integration**: Works seamlessly with Pipeline and FeatureUnion\n   - **Meta-Estimators**: GridSearchCV, VotingClassifier, etc. rely on separate fit/predict\n   - **Consistent Interface**: All estimators follow the same pattern\n   - **Method Chaining**: Enables fluent API: model.fit(X, y).predict(X_test)\n   - **Extensibility**: Easy to add new estimators that follow the same pattern\n\n6. **State Management**:\n   - **Model State**: fit() sets internal state (coefficients, parameters, etc.)\n   - **Prediction State**: predict() uses stored state without modification\n   - **Immutability**: Predictions don't change the trained model\n   - **Reproducibility**: Same model produces same predictions for same input\n   - **Thread Safety**: Multiple threads can use the same fitted model for predictions\n\n7. **Validation and Error Handling**:\n   - **Training Validation**: fit() validates training data and parameters\n   - **Prediction Validation**: predict() validates input data format and dimensions\n   - **State Checking**: predict() checks if model has been fitted\n   - **Error Messages**: Clear error messages for each phase\n   - **Debugging**: Easier to identify whether issues occur during training or prediction\n\n8. **Flexibility and Extensibility**:\n   - **Custom Estimators**: Easy to implement custom estimators following the pattern\n   - **Inheritance**: Base classes provide common functionality for both phases\n   - **Mixin Classes**: TransformerMixin, ClassifierMixin provide specialized behavior\n   - **Method Override**: Subclasses can override specific phases independently\n   - **Plugin Architecture**: New algorithms can be added without changing existing code\n\n9. **Testing and Quality Assurance**:\n   - **Unit Testing**: Can test training and prediction independently\n   - **Integration Testing**: Can test the complete workflow\n   - **Regression Testing**: Can ensure predictions remain consistent\n   - **Performance Testing**: Can benchmark training and prediction separately\n   - **Validation Testing**: Can test with different data types and formats\n\n10. **Educational and Documentation Benefits**:\n    - **Clear Learning Path**: Beginners understand the two-phase process\n    - **Documentation**: Each method can be documented separately\n    - **Examples**: Clear examples for each phase\n    - **Debugging**: Easier to teach debugging for each phase\n    - **Best Practices**: Reinforces proper machine learning workflow", "score": null, "retrieved_content": [{"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "_classes.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tree", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n            )\n\n        # Use BestFirst if max_leaf_nodes given; use DepthFirst otherwise\n        if max_leaf_nodes < 0:\n            builder = DepthFirstTreeBuilder(\n                splitter,\n                min_samples_split,\n                min_samples_leaf,\n                min_weight_leaf,\n                max_depth,\n                self.min_impurity_decrease,\n            )\n        else:\n            builder = BestFirstTreeBuilder(\n                splitter,\n                min_samples_split,\n                min_samples_leaf,\n                min_weight_leaf,\n                max_depth,\n                max_leaf_nodes,\n                self.min_impurity_decrease,\n            )\n\n        builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n\n        if self.n_outputs_ == 1 and is_classifier(self):\n            self.n_classes_ = self.n_classes_[0]\n            self.classes_ = self.classes_[0]\n\n        self._prune_tree()\n\n        return self\n\n    def _validate_X_predict(self, X, check_input):\n        \"\"\"Validate the training data on predict (probabilities).\"\"\"\n        if check_input:\n            if self._support_missing_values(X):\n                ensure_all_finite = \"allow-nan\"\n            else:\n                ensure_all_finite = True\n            X = validate_data(\n                self,\n                X,\n                dtype=DTYPE,\n                accept_sparse=\"csr\",\n                reset=False,\n                ensure_all_finite=ensure_all_finite,\n            )\n            if issparse(X) and (\n                X.indices.dtype != np.intc or X.indptr.dtype != np.intc\n            ):\n                raise ValueError(\"No support for np.int64 index based sparse matrices\")\n        else:\n            # The number of features is checked regardless of `check_input`\n            _check_n_features(self, X, reset=False)\n        return X\n\n    def predict(self, X, check_input=True):\n        \"\"\"Predict class or regression value for X.\n\n        For a classification m"}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "_stochastic_gradient.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "is not None:\n            self._allocate_parameter_mem(\n                n_classes=n_classes,\n                n_features=n_features,\n                input_dtype=X.dtype,\n                coef_init=coef_init,\n                intercept_init=intercept_init,\n            )\n        elif n_features != self.coef_.shape[-1]:\n            raise ValueError(\n                \"Number of features %d does not match previous data %d.\"\n                % (n_features, self.coef_.shape[-1])\n            )\n\n        self._loss_function_ = self._get_loss_function(loss)\n        if not hasattr(self, \"t_\"):\n            self.t_ = 1.0\n\n        # delegate to concrete training procedure\n        if n_classes > 2:\n            self._fit_multiclass(\n                X,\n                y,\n                alpha=alpha,\n                C=C,\n                learning_rate=learning_rate,\n                sample_weight=sample_weight,\n                max_iter=max_iter,\n            )\n        elif n_classes == 2:\n            self._fit_binary(\n                X,\n                y,\n                alpha=alpha,\n                C=C,\n                learning_rate=learning_rate,\n                sample_weight=sample_weight,\n                max_iter=max_iter,\n            )\n        else:\n            raise ValueError(\n                \"The number of classes has to be greater than one; got %d class\"\n                % n_classes\n            )\n\n        return self\n\n    def _fit(\n        self,\n        X,\n        y,\n        alpha,\n        C,\n        loss,\n        learning_rate,\n        coef_init=None,\n        intercept_init=None,\n        sample_weight=None,\n    ):\n        if hasattr(self, \"classes_\"):\n            # delete the attribute otherwise _partial_fit thinks it's not the first call\n            delattr(self, \"classes_\")\n\n        # labels can be encoded as float, int, or string literals\n        # np.unique sorts in asc order; largest class id is positive class\n        y = validate_data(self, y=y)\n        classes = np.unique(y)\n\n "}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "metadata_routing_common.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "        self, sample_weight=sample_weight, metadata=metadata\n        )\n        _check_partial_fit_first_call(self, classes)\n        return self\n\n    def fit(self, X, y, sample_weight=\"default\", metadata=\"default\"):\n        if self.registry is not None:\n            self.registry.append(self)\n\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n\n        self.classes_ = np.unique(y)\n        self.coef_ = np.ones_like(X)\n        return self\n\n    def predict(self, X, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n        y_score = np.empty(shape=(len(X),), dtype=\"int8\")\n        y_score[len(X) // 2 :] = 0\n        y_score[: len(X) // 2] = 1\n        return y_score\n\n    def predict_proba(self, X, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n        y_proba = np.empty(shape=(len(X), len(self.classes_)), dtype=np.float32)\n        # each row sums up to 1.0:\n        y_proba[:] = np.random.dirichlet(alpha=np.ones(len(self.classes_)), size=len(X))\n        return y_proba\n\n    def predict_log_proba(self, X, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n        return self.predict_proba(X)\n\n    def decision_function(self, X, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n        y_score = np.empty(shape=(len(X),))\n        y_score[len(X) // 2 :] = 0\n        y_score[: len(X) // 2] = 1\n        return y_score\n\n    def score(self, X, y, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n"}, {"start_line": 91000, "end_line": 93000, "belongs_to": {"file_name": "gradient_boosting.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble/_hist_gradient_boosting", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "lidation_fraction=validation_fraction,\n            n_iter_no_change=n_iter_no_change,\n            tol=tol,\n            verbose=verbose,\n            random_state=random_state,\n        )\n        self.class_weight = class_weight\n\n    def _finalize_sample_weight(self, sample_weight, y):\n        \"\"\"Adjust sample_weights with class_weights.\"\"\"\n        if self.class_weight is None:\n            return sample_weight\n\n        expanded_class_weight = compute_sample_weight(self.class_weight, y)\n\n        if sample_weight is not None:\n            return sample_weight * expanded_class_weight\n        else:\n            return expanded_class_weight\n\n    def predict(self, X):\n        \"\"\"Predict classes for X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        y : ndarray, shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        # TODO: This could be done in parallel\n        raw_predictions = self._raw_predict(X)\n        if raw_predictions.shape[1] == 1:\n            # np.argmax([0.5, 0.5]) is 0, not 1. Therefore \"> 0\" not \">= 0\" to be\n            # consistent with the multiclass case.\n            encoded_classes = (raw_predictions.ravel() > 0).astype(int)\n        else:\n            encoded_classes = np.argmax(raw_predictions, axis=1)\n        return self.classes_[encoded_classes]\n\n    def staged_predict(self, X):\n        \"\"\"Predict classes at each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        .. versionadded:: 0.24\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        ------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted classes of the input samples, for each iteration.\n        \"\"\"\n        for raw_predictions in self._staged_raw_predict(X):\n            if "}, {"start_line": 34000, "end_line": 36000, "belongs_to": {"file_name": "discriminant_analysis.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ">>> print(clf.predict([[-0.8, -1]]))\n    [1]\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"priors\": [\"array-like\", None],\n        \"reg_param\": [Interval(Real, 0, 1, closed=\"both\")],\n        \"store_covariance\": [\"boolean\"],\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\n    }\n\n    def __init__(\n        self, *, priors=None, reg_param=0.0, store_covariance=False, tol=1.0e-4\n    ):\n        self.priors = priors\n        self.reg_param = reg_param\n        self.store_covariance = store_covariance\n        self.tol = tol\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y):\n        \"\"\"Fit the model according to the given training data and parameters.\n\n        .. versionchanged:: 0.19\n            ``store_covariances`` has been moved to main constructor as\n            ``store_covariance``.\n\n        .. versionchanged:: 0.19\n            ``tol`` has been moved to main constructor.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values (integers).\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        X, y = validate_data(self, X, y)\n        check_classification_targets(y)\n        self.classes_, y = np.unique(y, return_inverse=True)\n        n_samples, n_features = X.shape\n        n_classes = len(self.classes_)\n        if n_classes < 2:\n            raise ValueError(\n                \"The number of classes has to be greater than one; got %d class\"\n                % (n_classes)\n            )\n        if self.priors is None:\n            self.priors_ = np.bincount(y) / float(n_samples)\n        else:\n            self.priors_ = np.array(self.priors)\n\n        cov = None\n        store_covariance = self.store_covariance\n        if store_covariance:\n            cov"}, {"start_line": 21000, "end_line": 23000, "belongs_to": {"file_name": "_stochastic_gradient.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "nary(\n                X,\n                y,\n                alpha=alpha,\n                C=C,\n                learning_rate=learning_rate,\n                sample_weight=sample_weight,\n                max_iter=max_iter,\n            )\n        else:\n            raise ValueError(\n                \"The number of classes has to be greater than one; got %d class\"\n                % n_classes\n            )\n\n        return self\n\n    def _fit(\n        self,\n        X,\n        y,\n        alpha,\n        C,\n        loss,\n        learning_rate,\n        coef_init=None,\n        intercept_init=None,\n        sample_weight=None,\n    ):\n        if hasattr(self, \"classes_\"):\n            # delete the attribute otherwise _partial_fit thinks it's not the first call\n            delattr(self, \"classes_\")\n\n        # labels can be encoded as float, int, or string literals\n        # np.unique sorts in asc order; largest class id is positive class\n        y = validate_data(self, y=y)\n        classes = np.unique(y)\n\n        if self.warm_start and hasattr(self, \"coef_\"):\n            if coef_init is None:\n                coef_init = self.coef_\n            if intercept_init is None:\n                intercept_init = self.intercept_\n        else:\n            self.coef_ = None\n            self.intercept_ = None\n\n        if self.average > 0:\n            self._standard_coef = self.coef_\n            self._standard_intercept = self.intercept_\n            self._average_coef = None\n            self._average_intercept = None\n\n        # Clear iteration count for multiple call to fit.\n        self.t_ = 1.0\n\n        self._partial_fit(\n            X,\n            y,\n            alpha,\n            C,\n            loss,\n            learning_rate,\n            self.max_iter,\n            classes,\n            sample_weight,\n            coef_init,\n            intercept_init,\n        )\n\n        if (\n            self.tol is not None\n            and self.tol > -np.inf\n            and self.n_iter_ == self.max_iter\n        ):\n  "}, {"start_line": 47000, "end_line": 49000, "belongs_to": {"file_name": "_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ange(len(test_indices))\n\n    if sp.issparse(predictions[0]):\n        predictions = sp.vstack(predictions, format=predictions[0].format)\n    elif encode and isinstance(predictions[0], list):\n        # `predictions` is a list of method outputs from each fold.\n        # If each of those is also a list, then treat this as a\n        # multioutput-multiclass task. We need to separately concatenate\n        # the method outputs for each label into an `n_labels` long list.\n        n_labels = y.shape[1]\n        concat_pred = []\n        for i_label in range(n_labels):\n            label_preds = np.concatenate([p[i_label] for p in predictions])\n            concat_pred.append(label_preds)\n        predictions = concat_pred\n    else:\n        predictions = np.concatenate(predictions)\n\n    if isinstance(predictions, list):\n        return [p[inv_test_indices] for p in predictions]\n    else:\n        return predictions[inv_test_indices]\n\n\ndef _fit_and_predict(estimator, X, y, train, test, fit_params, method):\n    \"\"\"Fit estimator and predict values for a given dataset split.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    estimator : estimator object implementing 'fit' and 'predict'\n        The object to use to fit the data.\n\n    X : array-like of shape (n_samples, n_features)\n        The data to fit.\n\n        .. versionchanged:: 0.20\n            X is only required to be an object with finite length or shape now\n\n    y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n        The target variable to try to predict in the case of\n        supervised learning.\n\n    train : array-like of shape (n_train_samples,)\n        Indices of training samples.\n\n    test : array-like of shape (n_test_samples,)\n        Indices of test samples.\n\n    fit_params : dict or None\n        Parameters that will be passed to ``estimator.fit``.\n\n    method : str\n        Invokes the passed method name of the passed estimator.\n\n    Returns\n    -------\n "}, {"start_line": 19000, "end_line": 21000, "belongs_to": {"file_name": "_stochastic_gradient.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    def _partial_fit(\n        self,\n        X,\n        y,\n        alpha,\n        C,\n        loss,\n        learning_rate,\n        max_iter,\n        classes,\n        sample_weight,\n        coef_init,\n        intercept_init,\n    ):\n        first_call = not hasattr(self, \"classes_\")\n        X, y = validate_data(\n            self,\n            X,\n            y,\n            accept_sparse=\"csr\",\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            accept_large_sparse=False,\n            reset=first_call,\n        )\n\n        n_samples, n_features = X.shape\n\n        _check_partial_fit_first_call(self, classes)\n\n        n_classes = self.classes_.shape[0]\n\n        # Allocate datastructures from input arguments\n        self._expanded_class_weight = compute_class_weight(\n            self.class_weight, classes=self.classes_, y=y\n        )\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        if getattr(self, \"coef_\", None) is None or coef_init is not None:\n            self._allocate_parameter_mem(\n                n_classes=n_classes,\n                n_features=n_features,\n                input_dtype=X.dtype,\n                coef_init=coef_init,\n                intercept_init=intercept_init,\n            )\n        elif n_features != self.coef_.shape[-1]:\n            raise ValueError(\n                \"Number of features %d does not match previous data %d.\"\n                % (n_features, self.coef_.shape[-1])\n            )\n\n        self._loss_function_ = self._get_loss_function(loss)\n        if not hasattr(self, \"t_\"):\n            self.t_ = 1.0\n\n        # delegate to concrete training procedure\n        if n_classes > 2:\n            self._fit_multiclass(\n                X,\n                y,\n                alpha=alpha,\n                C=C,\n                learning_rate=learning_rate,\n                sample_weight=sample_weight,\n                max_iter=max_iter,\n            )\n        elif n_classes == 2:\n            self._fit_bi"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/mixture", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tion is performed upon the first call. Upon consecutive\n        calls, training starts where it left off.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row\n            corresponds to a single data point.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            The fitted mixture.\n        \"\"\"\n        # parameters are validated in fit_predict\n        self.fit_predict(X, y)\n        return self\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit_predict(self, X, y=None):\n        \"\"\"Estimate model parameters using X and predict the labels for X.\n\n        The method fits the model n_init times and sets the parameters with\n        which the model has the largest likelihood or lower bound. Within each\n        trial, the method iterates between E-step and M-step for `max_iter`\n        times until the change of likelihood or lower bound is less than\n        `tol`, otherwise, a :class:`~sklearn.exceptions.ConvergenceWarning` is\n        raised. After fitting, it predicts the most probable label for the\n        input data points.\n\n        .. versionadded:: 0.20\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row\n            corresponds to a single data point.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        labels : array, shape (n_samples,)\n            Component labels.\n        \"\"\"\n        xp, _ = get_namespace(X)\n        X = validate_data(self, X, dtype=[xp.float64, xp.float32], ensure_min_samples=2)\n        if X.shape[0] < self.n_components:\n            raise ValueError(\n                \"Expected n_samples >= n_components \"\n                f\"but got n_components = {self.n_comp"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "_classification_threshold.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "        threshold=\"auto\",\n        pos_label=None,\n        response_method=\"auto\",\n    ):\n        super().__init__(estimator=estimator, response_method=response_method)\n        self.pos_label = pos_label\n        self.threshold = threshold\n\n    @property\n    def classes_(self):\n        if estimator := getattr(self, \"estimator_\", None):\n            return estimator.classes_\n        try:\n            check_is_fitted(self.estimator)\n            return self.estimator.classes_\n        except NotFittedError:\n            raise AttributeError(\n                \"The underlying estimator is not fitted yet.\"\n            ) from NotFittedError\n\n    def _fit(self, X, y, **params):\n        \"\"\"Fit the classifier.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        **params : dict\n            Parameters to pass to the `fit` method of the underlying\n            classifier.\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n        \"\"\"\n        routed_params = process_routing(self, \"fit\", **params)\n        self.estimator_ = clone(self.estimator).fit(X, y, **routed_params.estimator.fit)\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict the target of new samples.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The samples, as accepted by `estimator.predict`.\n\n        Returns\n        -------\n        class_labels : ndarray of shape (n_samples,)\n            The predicted class.\n        \"\"\"\n        _check_is_fitted(self)\n\n        estimator = getattr(self, \"estimator_\", self.estimator)\n\n        y_score, _, response_method_used = _get_response_values_binary(\n            estimator,\n            X,\n            self._get_response_method(),\n            pos_label=self.pos_label,\n            return_respo"}], "retrieved_count": 10, "cost_time": 1.1163103580474854}
{"question": "What is the dependency injection mechanism between Scikit-learn's preprocessing module and the validation module?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn's preprocessing module and validation module interact through a sophisticated dependency injection mechanism that ensures data integrity and consistent validation across all preprocessing operations:\n\n1. **Core Validation Functions as Dependencies**:\n   - **check_array()**: Primary validation function injected into all preprocessing estimators\n   - **check_X_y()**: Used for supervised preprocessing that requires both X and y\n   - **validate_data()**: Comprehensive validation with feature counting and metadata routing\n   - **_check_y()**: Specialized validation for target variables\n   - **check_consistent_length()**: Ensures consistent lengths across multiple arrays\n\n2. **Preprocessing Module Integration**:\n   - **StandardScaler**: Uses check_array() in fit() and transform() methods\n   - **MinMaxScaler**: Injects validation through validate_data() helper\n   - **RobustScaler**: Relies on check_array() for input validation\n   - **OneHotEncoder**: Uses custom _check_X() method that calls check_array() internally\n   - **OrdinalEncoder**: Inherits validation from _BaseEncoder class\n   - **SimpleImputer**: Validates input through check_array() with specific parameters\n\n3. **Validation Parameter Injection**:\n   - **accept_sparse**: Controls sparse matrix acceptance (True/False/specific formats)\n   - **dtype**: Specifies expected data types ('numeric', specific types, or None)\n   - **ensure_all_finite**: Controls handling of NaN/Inf values\n   - **ensure_2d**: Enforces 2D array requirements\n   - **copy**: Controls whether to create copies of input data\n   - **force_all_finite**: Deprecated parameter replaced by ensure_all_finite\n\n4. **Custom Validation Patterns**:\n   - **Encoder Classes**: Implement _check_X() methods that customize validation for categorical data\n   - **Imputer Classes**: Use specific validation parameters for missing value handling\n   - **Scaler Classes**: Apply validation with numeric dtype requirements\n   - **Feature Selection**: Use validation with specific sparse matrix acceptance patterns\n   - **Polynomial Features**: Validate input with ensure_2d=True and numeric dtype\n\n5. **Error Handling and Messaging**:\n   - **Estimator Name Injection**: Validation functions include estimator names in error messages\n   - **Input Name Specification**: Functions specify input names ('X', 'y') for better error messages\n   - **Consistent Error Types**: All validation functions raise consistent exception types\n   - **Helpful Error Messages**: Errors include suggestions for fixing common issues\n   - **Documentation Links**: Error messages link to relevant documentation when appropriate\n\n6. **Feature Name and Count Tracking**:\n   - **n_features_in_**: Validation sets and tracks number of input features\n   - **feature_names_in_**: Preserves and validates feature names from pandas DataFrames\n   - **Consistency Checking**: Ensures feature count consistency across fit/transform calls\n   - **Reset Mechanism**: Allows resetting feature tracking for new datasets\n   - **Metadata Routing**: Supports advanced metadata routing for feature names\n\n7. **Sparse Matrix Handling**:\n   - **Format Validation**: Validates sparse matrix formats (CSR, CSC, COO, etc.)\n   - **Conversion Control**: Controls automatic format conversion\n   - **Large Sparse Support**: Handles large sparse matrices with configurable acceptance\n   - **Index Validation**: Validates sparse matrix indices for compatibility\n   - **Memory Efficiency**: Optimizes memory usage for sparse operations\n\n8. **Data Type Management**:\n   - **Automatic Conversion**: Converts object arrays to numeric when possible\n   - **Type Preservation**: Preserves original types when appropriate\n   - **Mixed Type Handling**: Handles mixed data types through object arrays\n   - **Extension Array Support**: Supports pandas extension arrays and other array-like objects\n   - **Complex Number Handling**: Validates and handles complex number arrays\n\n9. **Performance Optimizations**:\n   - **Skip Validation**: Global configuration allows skipping validation for performance\n   - **Nested Validation Control**: Prevents redundant validation in nested estimators\n   - **Memory Layout Control**: Optimizes memory layout (C/F order) for performance\n   - **Copy Avoidance**: Minimizes unnecessary data copying\n   - **Parallel Processing**: Supports parallel validation for large datasets\n\n10. **Integration with Pipeline System**:\n    - **Pipeline Compatibility**: All validation functions work seamlessly in Pipeline objects\n    - **Parameter Routing**: Supports advanced parameter routing in pipelines\n    - **Feature Name Propagation**: Preserves feature names through pipeline steps\n    - **Consistent Interface**: Provides consistent validation interface across all estimators\n    - **Metadata Support**: Supports metadata routing for advanced use cases", "score": null, "retrieved_content": [{"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "f, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass HasMutableParameters(BaseEstimator):\n    def __init__(self, p=object()):\n        self.p = p\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass HasImmutableParameters(BaseEstimator):\n    # Note that object is an uninitialized class, thus immutable.\n    def __init__(self, p=42, q=np.int32(42), r=object):\n        self.p = p\n        self.q = q\n        self.r = r\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ModifiesValueInsteadOfRaisingError(BaseEstimator):\n    def __init__(self, p=0):\n        self.p = p\n\n    def set_params(self, **kwargs):\n        if \"p\" in kwargs:\n            p = kwargs.pop(\"p\")\n            if p < 0:\n                p = 0\n            self.p = p\n        return super().set_params(**kwargs)\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ModifiesAnotherValue(BaseEstimator):\n    def __init__(self, a=0, b=\"method1\"):\n        self.a = a\n        self.b = b\n\n    def set_params(self, **kwargs):\n        if \"a\" in kwargs:\n            a = kwargs.pop(\"a\")\n            self.a = a\n            if a is None:\n                kwargs.pop(\"b\")\n                self.b = \"method2\"\n        return super().set_params(**kwargs)\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass NoCheckinPredict(BaseBadClassifier):\n    def fit(self, X, y):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass NoSparseClassifier(BaseBadClassifier):\n    def __init__(self, raise_for_type=None):\n        # raise_for_type : str, expects \"sparse_array\" or \"sparse_matrix\"\n        self.raise_for_type = raise_for_type\n\n    def fit(self, X, y):\n        X, y = validate_data(self, X, y, accept_sparse=[\"csr\", \"csc\"])\n        if self.raise_for_type == \"sparse_array\":\n            correct_type = isinstance(X, sp.spar"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ixes import CSR_CONTAINERS, SPARRAY_PRESENT\nfrom sklearn.utils.metaestimators import available_if\nfrom sklearn.utils.multiclass import type_of_target\nfrom sklearn.utils.validation import (\n    check_array,\n    check_is_fitted,\n    check_X_y,\n    validate_data,\n)\n\n\nclass CorrectNotFittedError(ValueError):\n    \"\"\"Exception class to raise if estimator is used before fitting.\n\n    Like NotFittedError, it inherits from ValueError, but not from\n    AttributeError. Used for testing only.\n    \"\"\"\n\n\nclass BaseBadClassifier(ClassifierMixin, BaseEstimator):\n    def fit(self, X, y):\n        return self\n\n    def predict(self, X):\n        return np.ones(X.shape[0])\n\n\nclass ChangesDict(BaseEstimator):\n    def __init__(self, key=0):\n        self.key = key\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n    def predict(self, X):\n        X = check_array(X)\n        self.key = 1000\n        return np.ones(X.shape[0])\n\n\nclass SetsWrongAttribute(BaseEstimator):\n    def __init__(self, acceptable_key=0):\n        self.acceptable_key = acceptable_key\n\n    def fit(self, X, y=None):\n        self.wrong_attribute = 0\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ChangesWrongAttribute(BaseEstimator):\n    def __init__(self, wrong_attribute=0):\n        self.wrong_attribute = wrong_attribute\n\n    def fit(self, X, y=None):\n        self.wrong_attribute = 1\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ChangesUnderscoreAttribute(BaseEstimator):\n    def fit(self, X, y=None):\n        self._good_attribute = 1\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass RaisesErrorInSetParams(BaseEstimator):\n    def __init__(self, p=0):\n        self.p = p\n\n    def set_params(self, **kwargs):\n        if \"p\" in kwargs:\n            p = kwargs.pop(\"p\")\n            if p < 0:\n                raise ValueError(\"p can't be less than 0\")\n            self.p = p\n        return super().set_params(**kwargs)\n\n    def fit(sel"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "test_metaestimators.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tor\",\n                \"base_estimator\",\n                \"regressor\",\n                \"transformer_list\",\n                \"estimators\",\n            }\n        ):\n            continue\n\n        with suppress(SkipTest):\n            for meta_estimator in _construct_instances(Estimator):\n                print(meta_estimator)\n                yield _get_instance_with_pipeline(meta_estimator, sig)\n\n\n# TODO: remove data validation for the following estimators\n# They should be able to work on any data and delegate data validation to\n# their inner estimator(s).\nDATA_VALIDATION_META_ESTIMATORS_TO_IGNORE = [\n    \"AdaBoostClassifier\",\n    \"AdaBoostRegressor\",\n    \"BaggingClassifier\",\n    \"BaggingRegressor\",\n    \"ClassifierChain\",  # data validation is necessary\n    \"FrozenEstimator\",  # this estimator cannot be tested like others.\n    \"IterativeImputer\",\n    \"OneVsOneClassifier\",  # input validation can't be avoided\n    \"RANSACRegressor\",\n    \"RFE\",\n    \"RFECV\",\n    \"RegressorChain\",  # data validation is necessary\n    \"SelfTrainingClassifier\",\n    \"SequentialFeatureSelector\",  # not applicable (2D data mandatory)\n]\n\nDATA_VALIDATION_META_ESTIMATORS = [\n    est\n    for est in _generate_meta_estimator_instances_with_pipeline()\n    if est.__class__.__name__ not in DATA_VALIDATION_META_ESTIMATORS_TO_IGNORE\n]\n\n\ndef _get_meta_estimator_id(estimator):\n    return estimator.__class__.__name__\n\n\n@pytest.mark.parametrize(\n    \"estimator\", DATA_VALIDATION_META_ESTIMATORS, ids=_get_meta_estimator_id\n)\ndef test_meta_estimators_delegate_data_validation(estimator):\n    # Check that meta-estimators delegate data validation to the inner\n    # estimator(s).\n    rng = np.random.RandomState(0)\n    set_random_state(estimator)\n\n    n_samples = 30\n    X = rng.choice(np.array([\"aa\", \"bb\", \"cc\"], dtype=object), size=n_samples)\n\n    if is_regressor(estimator):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(3, size=n_samples)\n\n    # We convert to lists to make sure it works on array-"}, {"start_line": 15000, "end_line": 16737, "belongs_to": {"file_name": "test_public_functions.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "arning\"),\n    (\n        \"sklearn.decomposition.dict_learning_online\",\n        \"sklearn.decomposition.MiniBatchDictionaryLearning\",\n    ),\n    (\"sklearn.decomposition.fastica\", \"sklearn.decomposition.FastICA\"),\n    (\"sklearn.decomposition.non_negative_factorization\", \"sklearn.decomposition.NMF\"),\n    (\"sklearn.preprocessing.maxabs_scale\", \"sklearn.preprocessing.MaxAbsScaler\"),\n    (\"sklearn.preprocessing.minmax_scale\", \"sklearn.preprocessing.MinMaxScaler\"),\n    (\"sklearn.preprocessing.power_transform\", \"sklearn.preprocessing.PowerTransformer\"),\n    (\n        \"sklearn.preprocessing.quantile_transform\",\n        \"sklearn.preprocessing.QuantileTransformer\",\n    ),\n    (\"sklearn.preprocessing.robust_scale\", \"sklearn.preprocessing.RobustScaler\"),\n]\n\n\n@pytest.mark.parametrize(\n    \"func_module, class_module\", PARAM_VALIDATION_CLASS_WRAPPER_LIST\n)\ndef test_class_wrapper_param_validation(func_module, class_module):\n    \"\"\"Check param validation for public functions that are wrappers around\n    estimators.\n    \"\"\"\n    func, func_name, func_params, required_params = _get_func_info(func_module)\n\n    module_name, class_name = class_module.rsplit(\".\", 1)\n    module = import_module(module_name)\n    klass = getattr(module, class_name)\n\n    parameter_constraints_func = getattr(func, \"_skl_parameter_constraints\")\n    parameter_constraints_class = getattr(klass, \"_parameter_constraints\")\n    parameter_constraints = {\n        **parameter_constraints_class,\n        **parameter_constraints_func,\n    }\n    parameter_constraints = {\n        k: v for k, v in parameter_constraints.items() if k in func_params\n    }\n\n    _check_function_param_validation(\n        func, func_name, func_params, required_params, parameter_constraints\n    )\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test the validation module\"\"\"\n\nimport os\nimport re\nimport sys\nimport tempfile\nimport warnings\nfrom functools import partial\nfrom io import StringIO\nfrom time import sleep\n\nimport numpy as np\nimport pytest\nfrom scipy.sparse import issparse\n\nfrom sklearn import config_context\nfrom sklearn.base import BaseEstimator, ClassifierMixin, clone\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import (\n    load_diabetes,\n    load_digits,\n    load_iris,\n    make_classification,\n    make_multilabel_classification,\n    make_regression,\n)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.exceptions import FitFailedWarning, UnsetMetadataPassedError\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import (\n    LogisticRegression,\n    PassiveAggressiveClassifier,\n    Ridge,\n    RidgeClassifier,\n    SGDClassifier,\n)\nfrom sklearn.metrics import (\n    accuracy_score,\n    check_scoring,\n    confusion_matrix,\n    explained_variance_score,\n    make_scorer,\n    mean_squared_error,\n    precision_recall_fscore_support,\n    precision_score,\n    r2_score,\n)\nfrom sklearn.metrics._scorer import _MultimetricScorer\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    GroupKFold,\n    GroupShuffleSplit,\n    KFold,\n    LeaveOneGroupOut,\n    LeaveOneOut,\n    LeavePGroupsOut,\n    ShuffleSplit,\n    StratifiedKFold,\n    cross_val_predict,\n    cross_val_score,\n    cross_validate,\n    learning_curve,\n    permutation_test_score,\n    validation_curve,\n)\nfrom sklearn.model_selection._validation import (\n    _check_is_permutation,\n    _fit_and_score,\n    _score,\n)\nfrom sklearn.model_selection.tests.common import OneTimeSplitter\nfrom sklearn.model_selection.tests.test_search import FailingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, scale\nfrom sklearn.svm import "}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "test_public_functions.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "t wrappers around\n    estimators.\n    \"\"\"\n    func, func_name, func_params, required_params = _get_func_info(func_module)\n\n    parameter_constraints = getattr(func, \"_skl_parameter_constraints\")\n\n    _check_function_param_validation(\n        func, func_name, func_params, required_params, parameter_constraints\n    )\n\n\nPARAM_VALIDATION_CLASS_WRAPPER_LIST = [\n    (\"sklearn.cluster.affinity_propagation\", \"sklearn.cluster.AffinityPropagation\"),\n    (\"sklearn.cluster.dbscan\", \"sklearn.cluster.DBSCAN\"),\n    (\"sklearn.cluster.k_means\", \"sklearn.cluster.KMeans\"),\n    (\"sklearn.cluster.mean_shift\", \"sklearn.cluster.MeanShift\"),\n    (\"sklearn.cluster.spectral_clustering\", \"sklearn.cluster.SpectralClustering\"),\n    (\"sklearn.covariance.graphical_lasso\", \"sklearn.covariance.GraphicalLasso\"),\n    (\"sklearn.covariance.ledoit_wolf\", \"sklearn.covariance.LedoitWolf\"),\n    (\"sklearn.covariance.oas\", \"sklearn.covariance.OAS\"),\n    (\"sklearn.decomposition.dict_learning\", \"sklearn.decomposition.DictionaryLearning\"),\n    (\n        \"sklearn.decomposition.dict_learning_online\",\n        \"sklearn.decomposition.MiniBatchDictionaryLearning\",\n    ),\n    (\"sklearn.decomposition.fastica\", \"sklearn.decomposition.FastICA\"),\n    (\"sklearn.decomposition.non_negative_factorization\", \"sklearn.decomposition.NMF\"),\n    (\"sklearn.preprocessing.maxabs_scale\", \"sklearn.preprocessing.MaxAbsScaler\"),\n    (\"sklearn.preprocessing.minmax_scale\", \"sklearn.preprocessing.MinMaxScaler\"),\n    (\"sklearn.preprocessing.power_transform\", \"sklearn.preprocessing.PowerTransformer\"),\n    (\n        \"sklearn.preprocessing.quantile_transform\",\n        \"sklearn.preprocessing.QuantileTransformer\",\n    ),\n    (\"sklearn.preprocessing.robust_scale\", \"sklearn.preprocessing.RobustScaler\"),\n]\n\n\n@pytest.mark.parametrize(\n    \"func_module, class_module\", PARAM_VALIDATION_CLASS_WRAPPER_LIST\n)\ndef test_class_wrapper_param_validation(func_module, class_module):\n    \"\"\"Check param validation for public functions that are wrappers around\n    e"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  def __init__(self, acceptable_key=0):\n        self.acceptable_key = acceptable_key\n\n    def fit(self, X, y=None):\n        self.wrong_attribute = 0\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ChangesWrongAttribute(BaseEstimator):\n    def __init__(self, wrong_attribute=0):\n        self.wrong_attribute = wrong_attribute\n\n    def fit(self, X, y=None):\n        self.wrong_attribute = 1\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ChangesUnderscoreAttribute(BaseEstimator):\n    def fit(self, X, y=None):\n        self._good_attribute = 1\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass RaisesErrorInSetParams(BaseEstimator):\n    def __init__(self, p=0):\n        self.p = p\n\n    def set_params(self, **kwargs):\n        if \"p\" in kwargs:\n            p = kwargs.pop(\"p\")\n            if p < 0:\n                raise ValueError(\"p can't be less than 0\")\n            self.p = p\n        return super().set_params(**kwargs)\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass HasMutableParameters(BaseEstimator):\n    def __init__(self, p=object()):\n        self.p = p\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass HasImmutableParameters(BaseEstimator):\n    # Note that object is an uninitialized class, thus immutable.\n    def __init__(self, p=42, q=np.int32(42), r=object):\n        self.p = p\n        self.q = q\n        self.r = r\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ModifiesValueInsteadOfRaisingError(BaseEstimator):\n    def __init__(self, p=0):\n        self.p = p\n\n    def set_params(self, **kwargs):\n        if \"p\" in kwargs:\n            p = kwargs.pop(\"p\")\n            if p < 0:\n                p = 0\n            self.p = p\n        return super().set_params(**kwargs)\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass M"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "test_public_functions.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ances_argmin\",\n    \"sklearn.metrics.pairwise_distances_chunked\",\n    \"sklearn.metrics.precision_recall_curve\",\n    \"sklearn.metrics.precision_recall_fscore_support\",\n    \"sklearn.metrics.precision_score\",\n    \"sklearn.metrics.r2_score\",\n    \"sklearn.metrics.rand_score\",\n    \"sklearn.metrics.recall_score\",\n    \"sklearn.metrics.roc_auc_score\",\n    \"sklearn.metrics.roc_curve\",\n    \"sklearn.metrics.root_mean_squared_error\",\n    \"sklearn.metrics.root_mean_squared_log_error\",\n    \"sklearn.metrics.top_k_accuracy_score\",\n    \"sklearn.metrics.v_measure_score\",\n    \"sklearn.metrics.zero_one_loss\",\n    \"sklearn.model_selection.cross_val_predict\",\n    \"sklearn.model_selection.cross_val_score\",\n    \"sklearn.model_selection.cross_validate\",\n    \"sklearn.model_selection.learning_curve\",\n    \"sklearn.model_selection.permutation_test_score\",\n    \"sklearn.model_selection.train_test_split\",\n    \"sklearn.model_selection.validation_curve\",\n    \"sklearn.neighbors.kneighbors_graph\",\n    \"sklearn.neighbors.radius_neighbors_graph\",\n    \"sklearn.neighbors.sort_graph_by_row_values\",\n    \"sklearn.preprocessing.add_dummy_feature\",\n    \"sklearn.preprocessing.binarize\",\n    \"sklearn.preprocessing.label_binarize\",\n    \"sklearn.preprocessing.normalize\",\n    \"sklearn.preprocessing.scale\",\n    \"sklearn.random_projection.johnson_lindenstrauss_min_dim\",\n    \"sklearn.svm.l1_min_c\",\n    \"sklearn.tree.export_graphviz\",\n    \"sklearn.tree.export_text\",\n    \"sklearn.tree.plot_tree\",\n    \"sklearn.utils.gen_batches\",\n    \"sklearn.utils.gen_even_slices\",\n    \"sklearn.utils.resample\",\n    \"sklearn.utils.safe_mask\",\n    \"sklearn.utils.extmath.randomized_svd\",\n    \"sklearn.utils.class_weight.compute_class_weight\",\n    \"sklearn.utils.class_weight.compute_sample_weight\",\n    \"sklearn.utils.graph.single_source_shortest_path_length\",\n]\n\n\n@pytest.mark.parametrize(\"func_module\", PARAM_VALIDATION_FUNCTION_LIST)\ndef test_function_param_validation(func_module):\n    \"\"\"Check param validation for public functions that are no"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "test_public_functions.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "g)\n\n\nPARAM_VALIDATION_FUNCTION_LIST = [\n    \"sklearn.calibration.calibration_curve\",\n    \"sklearn.cluster.cluster_optics_dbscan\",\n    \"sklearn.cluster.compute_optics_graph\",\n    \"sklearn.cluster.estimate_bandwidth\",\n    \"sklearn.cluster.kmeans_plusplus\",\n    \"sklearn.cluster.cluster_optics_xi\",\n    \"sklearn.cluster.ward_tree\",\n    \"sklearn.covariance.empirical_covariance\",\n    \"sklearn.covariance.ledoit_wolf_shrinkage\",\n    \"sklearn.covariance.log_likelihood\",\n    \"sklearn.covariance.shrunk_covariance\",\n    \"sklearn.datasets.clear_data_home\",\n    \"sklearn.datasets.dump_svmlight_file\",\n    \"sklearn.datasets.fetch_20newsgroups\",\n    \"sklearn.datasets.fetch_20newsgroups_vectorized\",\n    \"sklearn.datasets.fetch_california_housing\",\n    \"sklearn.datasets.fetch_covtype\",\n    \"sklearn.datasets.fetch_kddcup99\",\n    \"sklearn.datasets.fetch_lfw_pairs\",\n    \"sklearn.datasets.fetch_lfw_people\",\n    \"sklearn.datasets.fetch_olivetti_faces\",\n    \"sklearn.datasets.fetch_rcv1\",\n    \"sklearn.datasets.fetch_openml\",\n    \"sklearn.datasets.fetch_species_distributions\",\n    \"sklearn.datasets.get_data_home\",\n    \"sklearn.datasets.load_breast_cancer\",\n    \"sklearn.datasets.load_diabetes\",\n    \"sklearn.datasets.load_digits\",\n    \"sklearn.datasets.load_files\",\n    \"sklearn.datasets.load_iris\",\n    \"sklearn.datasets.load_linnerud\",\n    \"sklearn.datasets.load_sample_image\",\n    \"sklearn.datasets.load_svmlight_file\",\n    \"sklearn.datasets.load_svmlight_files\",\n    \"sklearn.datasets.load_wine\",\n    \"sklearn.datasets.make_biclusters\",\n    \"sklearn.datasets.make_blobs\",\n    \"sklearn.datasets.make_checkerboard\",\n    \"sklearn.datasets.make_circles\",\n    \"sklearn.datasets.make_classification\",\n    \"sklearn.datasets.make_friedman1\",\n    \"sklearn.datasets.make_friedman2\",\n    \"sklearn.datasets.make_friedman3\",\n    \"sklearn.datasets.make_gaussian_quantiles\",\n    \"sklearn.datasets.make_hastie_10_2\",\n    \"sklearn.datasets.make_low_rank_matrix\",\n    \"sklearn.datasets.make_moons\",\n    \"sklearn.datasets.make"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tests for input validation functions\"\"\"\n\nimport numbers\nimport re\nimport warnings\nfrom itertools import product\nfrom operator import itemgetter\nfrom tempfile import NamedTemporaryFile\n\nimport numpy as np\nimport pytest\nimport scipy.sparse as sp\nfrom pytest import importorskip\n\nimport sklearn\nfrom sklearn._config import config_context\nfrom sklearn._min_dependencies import dependent_packages\nfrom sklearn.base import BaseEstimator\nfrom sklearn.datasets import make_blobs\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.exceptions import NotFittedError, PositiveSpectrumWarning\nfrom sklearn.linear_model import ARDRegression\n\n# TODO: add this estimator into the _mocking module in a further refactoring\nfrom sklearn.metrics.tests.test_score_objects import EstimatorWithFit\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.random_projection import _sparse_random_matrix\nfrom sklearn.svm import SVR\nfrom sklearn.utils import (\n    _safe_indexing,\n    as_float_array,\n    check_array,\n    check_symmetric,\n    check_X_y,\n    deprecated,\n)\nfrom sklearn.utils._array_api import (\n    _convert_to_numpy,\n    _get_namespace_device_dtype_ids,\n    _is_numpy_namespace,\n    yield_namespace_device_dtype_combinations,\n)\nfrom sklearn.utils._mocking import (\n    MockDataFrame,\n    _MockEstimatorOnOffPrediction,\n)\nfrom sklearn.utils._testing import (\n    SkipTest,\n    TempMemmap,\n    _array_api_for_tests,\n    _convert_container,\n    assert_allclose,\n    assert_allclose_dense_sparse,\n    assert_array_equal,\n    create_memmap_backed_data,\n    skip_if_array_api_compat_not_configured,\n)\nfrom sklearn.utils.estimator_checks import _NotAnArray\nfrom sklearn.utils.fixes import (\n    COO_CONTAINERS,\n    CSC_CONTAINERS,\n    CSR_CONTAINERS,\n    DIA_CONTAINERS,\n    DOK_CONTAINERS,\n)\nfrom sklearn.utils.validation import (\n    FLOAT_DTYPES,\n    _allclose_dense_sparse,\n    _check_feature_names_in,\n    _check_method_params,\n    _check_pos_label_consistency,\n    _check_psd_eigenvalues"}], "retrieved_count": 10, "cost_time": 1.1210064888000488}
{"question": "Why does Scikit-learn implement a unified estimator interface instead of allowing each algorithm to have its own API?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn implements a unified estimator interface instead of allowing each algorithm to have its own API for several fundamental design reasons that enhance usability, maintainability, and ecosystem integration:\n\n1. **User Experience and Learning Curve**:\n   - **Consistent Learning**: Users learn one interface and can apply it to all algorithms\n   - **Reduced Cognitive Load**: No need to remember different API patterns for different algorithms\n   - **Intuitive Usage**: Predictable behavior across all estimators\n   - **Documentation Efficiency**: Single documentation pattern applies to all estimators\n   - **Error Reduction**: Consistent interface reduces user errors and confusion\n\n2. **API Consistency and Predictability**:\n   - **Unified Interface**: All estimators follow the same fit/predict/transform pattern\n   - **Predictable Behavior**: Users can predict how any estimator will behave\n   - **Cross-Algorithm Compatibility**: Same code works with different algorithms\n   - **Method Chaining**: Consistent interface enables fluent API usage\n   - **Parameter Consistency**: Common parameters work the same way across estimators\n\n3. **Ecosystem Integration and Composability**:\n   - **Pipeline Compatibility**: All estimators work seamlessly in Pipeline objects\n   - **Meta-Estimator Support**: GridSearchCV, RandomizedSearchCV work with any estimator\n   - **FeatureUnion Integration**: Transformers can be combined in parallel\n   - **Cross-Validation**: All estimators work with cross-validation tools\n   - **Model Selection**: Easy comparison and selection between different algorithms\n\n4. **Maintenance and Development Efficiency**:\n   - **Code Reusability**: Common functionality can be shared across estimators\n   - **Testing Efficiency**: Single test framework applies to all estimators\n   - **Documentation Consistency**: Unified documentation patterns\n   - **Bug Prevention**: Consistent interface reduces implementation errors\n   - **Code Review**: Easier to review and maintain code with consistent patterns\n\n5. **Advanced Features and Extensibility**:\n   - **Parameter Validation**: Unified parameter validation system\n   - **Metadata Routing**: Advanced metadata routing works across all estimators\n   - **Feature Names**: Consistent feature name handling\n   - **Serialization**: Unified model persistence and loading\n   - **Cloning**: Deep cloning works consistently across all estimators\n\n6. **Performance and Optimization**:\n   - **Parallel Processing**: Unified interface enables parallel processing\n   - **Caching**: Consistent interface enables effective caching strategies\n   - **Memory Management**: Unified memory management patterns\n   - **Optimization**: Performance optimizations can be applied consistently\n   - **Resource Management**: Consistent resource handling across estimators\n\n7. **Educational and Documentation Benefits**:\n   - **Learning Transfer**: Knowledge transfers between different algorithms\n   - **Documentation Clarity**: Single documentation pattern for all estimators\n   - **Example Reusability**: Examples can be adapted across different algorithms\n   - **Best Practices**: Reinforces good API design practices\n   - **Community Understanding**: Easier for community to understand and contribute\n\n8. **Third-Party Integration**:\n   - **Plugin Architecture**: Easy to add new algorithms following the same pattern\n   - **Compatibility**: Third-party libraries can easily integrate with scikit-learn\n   - **Extensibility**: Users can implement custom estimators following the pattern\n   - **Interoperability**: Consistent interface enables better interoperability\n   - **Ecosystem Growth**: Easier for the ecosystem to grow and evolve\n\n9. **Quality Assurance and Testing**:\n   - **Unified Testing**: Single test framework applies to all estimators\n   - **Consistent Validation**: Same validation rules apply to all estimators\n   - **Regression Testing**: Easier to ensure consistent behavior across versions\n   - **Compatibility Testing**: Easier to test compatibility between components\n   - **Performance Testing**: Consistent benchmarking across all estimators\n\n10. **Future-Proofing and Evolution**:\n    - **API Evolution**: Easier to evolve the API consistently across all estimators\n    - **Backward Compatibility**: Consistent interface simplifies backward compatibility\n    - **Feature Addition**: New features can be added consistently across all estimators\n    - **Deprecation Management**: Easier to manage deprecation across the entire library\n    - **Version Migration**: Users can more easily migrate between versions", "score": null, "retrieved_content": [{"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "aram2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n\n    # _sklearn_output_config is used by `set_output` to configure the output\n    # container of an estimator.\n    if hasattr(estimator, \"_sklearn_output_config\"):\n        new_object._sklearn_output_config = copy.deepcopy(\n            estimator._sklearn_output_config\n        )\n    return new_object\n\n\nclass BaseEstimator(ReprHTMLMixin, _HTMLDocumentationLinkMixin, _MetadataRequester):\n    \"\"\"Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])\n    \"\"\"\n\n    _html_"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n estimator instead of a class.\"\n                )\n            else:\n                raise TypeError(\n                    \"Cannot clone object '%s' (type %s): \"\n                    \"it does not seem to be a scikit-learn \"\n                    \"estimator as it does not implement a \"\n                    \"'get_params' method.\" % (repr(estimator), type(estimator))\n                )\n\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        new_object_params[name] = clone(param, safe=False)\n\n    new_object = klass(**new_object_params)\n    try:\n        new_object._metadata_request = copy.deepcopy(estimator._metadata_request)\n    except AttributeError:\n        pass\n\n    params_set = new_object.get_params(deep=False)\n\n    # quick sanity check of the parameters of the clone\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is not param2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n\n    # _sklearn_output_config is used by `set_output` to configure the output\n    # container of an estimator.\n    if hasattr(estimator, \"_sklearn_output_config\"):\n        new_object._sklearn_output_config = copy.deepcopy(\n            estimator._sklearn_output_config\n        )\n    return new_object\n\n\nclass BaseEstimator(ReprHTMLMixin, _HTMLDocumentationLinkMixin, _MetadataRequester):\n    \"\"\"Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User"}, {"start_line": 22000, "end_line": 24000, "belongs_to": {"file_name": "_bagging.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "       caller=\"predict_proba\", callee=\"predict_proba\"\n                )\n            )\n\n        else:\n            (\n                method_mapping.add(caller=\"predict\", callee=\"predict\").add(\n                    caller=\"predict_proba\", callee=\"predict\"\n                )\n            )\n\n        # the router needs to be built depending on whether the sub-estimator has a\n        # `predict_log_proba` method (as BaggingClassifier decides dynamically at\n        # runtime):\n        if hasattr(self._get_estimator(), \"predict_log_proba\"):\n            method_mapping.add(caller=\"predict_log_proba\", callee=\"predict_log_proba\")\n\n        else:\n            # if `predict_log_proba` is not available in BaggingClassifier's\n            # sub-estimator, the routing should go to its `predict_proba` if it is\n            # available or else to its `predict` method; according to how\n            # `sample_weight` is passed to the respective methods dynamically at\n            # runtime:\n            if hasattr(self._get_estimator(), \"predict_proba\"):\n                method_mapping.add(caller=\"predict_log_proba\", callee=\"predict_proba\")\n\n            else:\n                method_mapping.add(caller=\"predict_log_proba\", callee=\"predict\")\n\n        router.add(estimator=self._get_estimator(), method_mapping=method_mapping)\n        return router\n\n    @abstractmethod\n    def _get_estimator(self):\n        \"\"\"Resolve which estimator to return.\"\"\"\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.sparse = get_tags(self._get_estimator()).input_tags.sparse\n        tags.input_tags.allow_nan = get_tags(self._get_estimator()).input_tags.allow_nan\n        return tags\n\n\nclass BaggingClassifier(ClassifierMixin, BaseBagging):\n    \"\"\"A Bagging classifier.\n\n    A Bagging classifier is an ensemble meta-estimator that fits base\n    classifiers each on random subsets of the original dataset and then\n    aggregate their individual predictions (either by voting or by avera"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])\n    \"\"\"\n\n    _html_repr = estimator_html_repr\n\n    @classmethod\n    def _get_param_names(cls):\n        \"\"\"Get parameter names for the estimator\"\"\"\n        # fetch the constructor or the original constructor before\n        # deprecation wrapping if any\n        init = getattr(cls.__init__, \"deprecated_original\", cls.__init__)\n        if init is object.__init__:\n            # No explicit constructor to introspect\n            return []\n\n        # introspect the constructor arguments to find the model parameters\n        # to represent\n        init_signature = inspect.signature(init)\n        # Consider the constructor parameters excluding 'self'\n        parameters = [\n            p\n            for p in init_signature.parameters.values()\n            if p.name != \"self\" and p.kind != p.VAR_KEYWORD\n        ]\n        for p in parameters:\n            if p.kind == p.VAR_POSITIONAL:\n                raise RuntimeError(\n                    \"scikit-learn estimators should always \"\n                    \"specify their para"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "_voting.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "add(\n                **{name: estimator},\n                method_mapping=MethodMapping().add(callee=\"fit\", caller=\"fit\"),\n            )\n        return router\n\n\nclass VotingClassifier(ClassifierMixin, _BaseVoting):\n    \"\"\"Soft Voting/Majority Rule classifier for unfitted estimators.\n\n    Read more in the :ref:`User Guide <voting_classifier>`.\n\n    .. versionadded:: 0.17\n\n    Parameters\n    ----------\n    estimators : list of (str, estimator) tuples\n        Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n        of those original estimators that will be stored in the class attribute\n        ``self.estimators_``. An estimator can be set to ``'drop'`` using\n        :meth:`set_params`.\n\n        .. versionchanged:: 0.21\n            ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n            support was removed in 0.24.\n\n    voting : {'hard', 'soft'}, default='hard'\n        If 'hard', uses predicted class labels for majority rule voting.\n        Else if 'soft', predicts the class label based on the argmax of\n        the sums of the predicted probabilities, which is recommended for\n        an ensemble of well-calibrated classifiers.\n\n    weights : array-like of shape (n_classifiers,), default=None\n        Sequence of weights (`float` or `int`) to weight the occurrences of\n        predicted class labels (`hard` voting) or class probabilities\n        before averaging (`soft` voting). Uses uniform weights if `None`.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel for ``fit``.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.18\n\n    flatten_transform : bool, default=True\n        Affects shape of transform output only when voting='soft'\n        If voting='soft' and flatten_transform=True, transform method returns\n        matrix with shape (n_samples, n_classi"}, {"start_line": 12000, "end_line": 13699, "belongs_to": {"file_name": "_mocking.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  The estimator to wrap.\n    \"\"\"\n\n    def __init__(self, est=None):\n        self.est = est\n\n    def fit(self, X, y):\n        return self.est.fit(X, y)\n\n    def predict(self, X):\n        return self.est.predict(X)\n\n    def predict_proba(self, X):\n        return self.est.predict_proba(X)\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags._skip_test = True\n        return tags\n\n\ndef _check_response(method):\n    def check(self):\n        return self.response_methods is not None and method in self.response_methods\n\n    return check\n\n\nclass _MockEstimatorOnOffPrediction(BaseEstimator):\n    \"\"\"Estimator for which we can turn on/off the prediction methods.\n\n    Parameters\n    ----------\n    response_methods: list of \\\n            {\"predict\", \"predict_proba\", \"decision_function\"}, default=None\n        List containing the response implemented by the estimator. When, the\n        response is in the list, it will return the name of the response method\n        when called. Otherwise, an `AttributeError` is raised. It allows to\n        use `getattr` as any conventional estimator. By default, no response\n        methods are mocked.\n    \"\"\"\n\n    def __init__(self, response_methods=None):\n        self.response_methods = response_methods\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        return self\n\n    @available_if(_check_response(\"predict\"))\n    def predict(self, X):\n        return \"predict\"\n\n    @available_if(_check_response(\"predict_proba\"))\n    def predict_proba(self, X):\n        return \"predict_proba\"\n\n    @available_if(_check_response(\"decision_function\"))\n    def decision_function(self, X):\n        return \"decision_function\"\n"}, {"start_line": 23000, "end_line": 25000, "belongs_to": {"file_name": "_bagging.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "lf._get_estimator(), \"predict_proba\"):\n                method_mapping.add(caller=\"predict_log_proba\", callee=\"predict_proba\")\n\n            else:\n                method_mapping.add(caller=\"predict_log_proba\", callee=\"predict\")\n\n        router.add(estimator=self._get_estimator(), method_mapping=method_mapping)\n        return router\n\n    @abstractmethod\n    def _get_estimator(self):\n        \"\"\"Resolve which estimator to return.\"\"\"\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.sparse = get_tags(self._get_estimator()).input_tags.sparse\n        tags.input_tags.allow_nan = get_tags(self._get_estimator()).input_tags.allow_nan\n        return tags\n\n\nclass BaggingClassifier(ClassifierMixin, BaseBagging):\n    \"\"\"A Bagging classifier.\n\n    A Bagging classifier is an ensemble meta-estimator that fits base\n    classifiers each on random subsets of the original dataset and then\n    aggregate their individual predictions (either by voting or by averaging)\n    to form a final prediction. Such a meta-estimator can typically be used as\n    a way to reduce the variance of a black-box estimator (e.g., a decision\n    tree), by introducing randomization into its construction procedure and\n    then making an ensemble out of it.\n\n    This algorithm encompasses several works from the literature. When random\n    subsets of the dataset are drawn as random subsets of the samples, then\n    this algorithm is known as Pasting [1]_. If samples are drawn with\n    replacement, then the method is known as Bagging [2]_. When random subsets\n    of the dataset are drawn as random subsets of the features, then the method\n    is known as Random Subspaces [3]_. Finally, when base estimators are built\n    on subsets of both samples and features, then the method is known as\n    Random Patches [4]_.\n\n    Read more in the :ref:`User Guide <bagging>`.\n\n    .. versionadded:: 0.15\n\n    Parameters\n    ----------\n    estimator : object, default=None\n        The bas"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "_mocking.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    \"\"\"\n        if self.methods_to_check == \"all\" or \"score\" in self.methods_to_check:\n            self._check_X_y(X, Y)\n        if self.foo_param > 1:\n            score = 1.0\n        else:\n            score = 0.0\n        return score\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags._skip_test = True\n        tags.input_tags.two_d_array = False\n        tags.target_tags.one_d_labels = True\n        return tags\n\n\n# Deactivate key validation for CheckingClassifier because we want to be able to\n# call fit with arbitrary fit_params and record them. Without this change, we\n# would get an error because those arbitrary params are not expected.\nCheckingClassifier.set_fit_request = RequestMethod(  # type: ignore[assignment,method-assign]\n    name=\"fit\", keys=[], validate_keys=False\n)\n\n\nclass NoSampleWeightWrapper(BaseEstimator):\n    \"\"\"Wrap estimator which will not expose `sample_weight`.\n\n    Parameters\n    ----------\n    est : estimator, default=None\n        The estimator to wrap.\n    \"\"\"\n\n    def __init__(self, est=None):\n        self.est = est\n\n    def fit(self, X, y):\n        return self.est.fit(X, y)\n\n    def predict(self, X):\n        return self.est.predict(X)\n\n    def predict_proba(self, X):\n        return self.est.predict_proba(X)\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags._skip_test = True\n        return tags\n\n\ndef _check_response(method):\n    def check(self):\n        return self.response_methods is not None and method in self.response_methods\n\n    return check\n\n\nclass _MockEstimatorOnOffPrediction(BaseEstimator):\n    \"\"\"Estimator for which we can turn on/off the prediction methods.\n\n    Parameters\n    ----------\n    response_methods: list of \\\n            {\"predict\", \"predict_proba\", \"decision_function\"}, default=None\n        List containing the response implemented by the estimator. When, the\n        response is in the list, it will return the name of the response method\n        when"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_metaestimators.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "         if obj.hidden_method == method.__name__:\n                raise AttributeError(\"%r is hidden\" % obj.hidden_method)\n            return functools.partial(method, obj)\n\n        return wrapper\n\n    class SubEstimator(BaseEstimator):\n        def __init__(self, param=1, hidden_method=None):\n            self.param = param\n            self.hidden_method = hidden_method\n\n        def fit(self, X, y=None, *args, **kwargs):\n            self.coef_ = np.arange(X.shape[1])\n            self.classes_ = []\n            return True\n\n        def _check_fit(self):\n            check_is_fitted(self)\n\n        @hides\n        def inverse_transform(self, X, *args, **kwargs):\n            self._check_fit()\n            return X\n\n        @hides\n        def transform(self, X, *args, **kwargs):\n            self._check_fit()\n            return X\n\n        @hides\n        def predict(self, X, *args, **kwargs):\n            self._check_fit()\n            return np.ones(X.shape[0])\n\n        @hides\n        def predict_proba(self, X, *args, **kwargs):\n            self._check_fit()\n            return np.ones(X.shape[0])\n\n        @hides\n        def predict_log_proba(self, X, *args, **kwargs):\n            self._check_fit()\n            return np.ones(X.shape[0])\n\n        @hides\n        def decision_function(self, X, *args, **kwargs):\n            self._check_fit()\n            return np.ones(X.shape[0])\n\n        @hides\n        def score(self, X, y, *args, **kwargs):\n            self._check_fit()\n            return 1.0\n\n    methods = [\n        k\n        for k in SubEstimator.__dict__.keys()\n        if not k.startswith(\"_\") and not k.startswith(\"fit\")\n    ]\n    methods.sort()\n\n    for delegator_data in DELEGATING_METAESTIMATORS:\n        delegate = SubEstimator()\n        delegator = delegator_data.construct(delegate)\n        for method in methods:\n            if method in delegator_data.skip_methods:\n                continue\n            assert hasattr(delegate, method)\n            assert hasattr(delegator, met"}, {"start_line": 0, "end_line": 162, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/frozen", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nfrom sklearn.frozen._frozen import FrozenEstimator\n\n__all__ = [\"FrozenEstimator\"]\n"}], "retrieved_count": 10, "cost_time": 1.1114122867584229}
{"question": "What specific NumPy components does Scikit-learn's core module depend on?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn's core module depends on several specific NumPy components for its fundamental operations:\n\n1. **Core Array Types and Functions**:\n   - **ndarray**: Primary data structure for all numerical operations\n   - **asarray()**: For converting various inputs to NumPy arrays\n   - **array()**: For creating new arrays from existing data\n   - **zeros(), ones(), empty()**: For array initialization\n   - **arange(), linspace()**: For sequence generation\n   - **reshape(), ravel(), flatten()**: For array reshaping operations\n\n2. **Data Type System**:\n   - **dtype objects**: For specifying data types (float64, float32, int32, etc.)\n   - **np.float64, np.float32**: Primary floating-point types\n   - **np.int32, np.int64**: Integer types for indices and counts\n   - **np.bool_**: Boolean type for masks and logical operations\n   - **np.object_**: Object type for mixed data\n   - **np.nan, np.inf**: Special values for missing data and infinity\n\n3. **Mathematical Operations**:\n   - **np.mean(), np.std(), np.var()**: Statistical functions\n   - **np.sum(), np.prod()**: Reduction operations\n   - **np.min(), np.max()**: Extremum functions\n   - **np.argmin(), np.argmax()**: Index finding functions\n   - **np.dot()**: Matrix multiplication and dot products\n   - **np.linalg**: Linear algebra operations (eigenvalues, SVD, etc.)\n\n4. **Array Manipulation**:\n   - **np.concatenate()**: For joining arrays\n   - **np.vstack(), np.hstack()**: For vertical and horizontal stacking\n   - **np.tile(), np.repeat()**: For array repetition\n   - **np.transpose()**: For array transposition\n   - **np.broadcast_to()**: For broadcasting operations\n   - **np.copy()**: For array copying\n\n5. **Indexing and Slicing**:\n   - **Boolean indexing**: For conditional selection\n   - **Integer indexing**: For positional selection\n   - **Fancy indexing**: For advanced selection patterns\n   - **np.where()**: For conditional array creation\n   - **np.take()**: For advanced indexing operations\n\n6. **Random Number Generation**:\n   - **np.random**: For random number generation\n   - **np.random.RandomState**: For controlled randomness\n   - **np.random.seed()**: For reproducibility\n   - **np.random.normal(), np.random.uniform()**: For various distributions\n   - **np.random.shuffle(), np.random.permutation()**: For randomization\n\n7. **Memory and Performance**:\n   - **C-contiguous arrays**: For efficient memory access\n   - **np.ascontiguousarray()**: For ensuring C-contiguous layout\n   - **np.asarray()**: For efficient array conversion\n   - **Memory views**: For zero-copy array access\n   - **Stride manipulation**: For efficient array operations\n\n8. **Validation and Testing**:\n   - **np.isfinite()**: For checking finite values\n   - **np.isnan()**: For checking NaN values\n   - **np.all(), np.any()**: For logical operations\n   - **np.testing**: For array comparison and testing\n   - **np.allclose()**: For approximate equality testing\n\n9. **Specialized Operations**:\n   - **np.einsum()**: For Einstein summation\n   - **np.polyfit(), np.polyval()**: For polynomial operations\n   - **np.interp()**: For interpolation\n   - **np.unique()**: For finding unique elements\n   - **np.sort()**: For sorting operations\n\n10. **Integration Features**:\n    - **Array API compatibility**: For interoperability with other array libraries\n    - **Cython integration**: For performance-critical operations\n    - **Memory mapping**: For large dataset handling\n    - **Parallel processing**: For multi-core operations\n    - **GPU support**: Through array API compatibility", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "extmath.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Utilities to perform optimal mathematical operations in scikit-learn.\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport warnings\nfrom functools import partial\nfrom numbers import Integral\n\nimport numpy as np\nfrom scipy import linalg, sparse\n\nfrom sklearn.utils._array_api import (\n    _average,\n    _is_numpy_namespace,\n    _nanmean,\n    device,\n    get_namespace,\n)\nfrom sklearn.utils._param_validation import Interval, StrOptions, validate_params\nfrom sklearn.utils.sparsefuncs_fast import csr_row_norms\nfrom sklearn.utils.validation import check_array, check_random_state\n\n\ndef squared_norm(x):\n    \"\"\"Squared Euclidean or Frobenius norm of x.\n\n    Faster than norm(x) ** 2.\n\n    Parameters\n    ----------\n    x : array-like\n        The input array which could be either be a vector or a 2 dimensional array.\n\n    Returns\n    -------\n    float\n        The Euclidean norm when x is a vector, the Frobenius norm when x\n        is a matrix (2-d array).\n    \"\"\"\n    x = np.ravel(x, order=\"K\")\n    if np.issubdtype(x.dtype, np.integer):\n        warnings.warn(\n            (\n                \"Array type is integer, np.dot may overflow. \"\n                \"Data should be float type to avoid this issue\"\n            ),\n            UserWarning,\n        )\n    return np.dot(x, x)\n\n\ndef row_norms(X, squared=False):\n    \"\"\"Row-wise (squared) Euclidean norm of X.\n\n    Equivalent to np.sqrt((X * X).sum(axis=1)), but also supports sparse\n    matrices and does not create an X.shape-sized temporary.\n\n    Performs no input validation.\n\n    Parameters\n    ----------\n    X : array-like\n        The input array.\n    squared : bool, default=False\n        If True, return squared norms.\n\n    Returns\n    -------\n    array-like\n        The row-wise (squared) Euclidean norm of X.\n    \"\"\"\n    if sparse.issparse(X):\n        X = X.tocsr()\n        norms = csr_row_norms(X)\n        if not squared:\n            norms = np.sqrt(norms)\n    else:\n        xp, _ = get_namespace(X)"}, {"start_line": 1000, "end_line": 2312, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tTags,\n    RegressorTags,\n    Tags,\n    TargetTags,\n    TransformerTags,\n    get_tags,\n)\nfrom sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\nfrom sklearn.utils.deprecation import deprecated\nfrom sklearn.utils.discovery import all_estimators\nfrom sklearn.utils.extmath import safe_sqr\nfrom sklearn.utils.murmurhash import murmurhash3_32\nfrom sklearn.utils.validation import (\n    as_float_array,\n    assert_all_finite,\n    check_array,\n    check_consistent_length,\n    check_random_state,\n    check_scalar,\n    check_symmetric,\n    check_X_y,\n    column_or_1d,\n    indexable,\n)\n\n__all__ = [\n    \"Bunch\",\n    \"ClassifierTags\",\n    \"DataConversionWarning\",\n    \"InputTags\",\n    \"RegressorTags\",\n    \"Tags\",\n    \"TargetTags\",\n    \"TransformerTags\",\n    \"_safe_indexing\",\n    \"all_estimators\",\n    \"as_float_array\",\n    \"assert_all_finite\",\n    \"check_X_y\",\n    \"check_array\",\n    \"check_consistent_length\",\n    \"check_random_state\",\n    \"check_scalar\",\n    \"check_symmetric\",\n    \"column_or_1d\",\n    \"compute_class_weight\",\n    \"compute_sample_weight\",\n    \"deprecated\",\n    \"estimator_html_repr\",\n    \"gen_batches\",\n    \"gen_even_slices\",\n    \"get_tags\",\n    \"indexable\",\n    \"metadata_routing\",\n    \"murmurhash3_32\",\n    \"resample\",\n    \"safe_mask\",\n    \"safe_sqr\",\n    \"shuffle\",\n]\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fixes.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Compatibility fixes for older version of python, numpy and scipy\n\nIf you add content to this file, please give the version of the package\nat which the fix is no longer needed.\n\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport platform\nimport struct\n\nimport numpy as np\nimport scipy\nimport scipy.sparse.linalg\nimport scipy.stats\nfrom scipy import optimize\n\ntry:\n    import pandas as pd\nexcept ImportError:\n    pd = None\n\nfrom sklearn.externals._packaging.version import parse as parse_version\nfrom sklearn.utils.parallel import _get_threadpool_controller\n\n_IS_32BIT = 8 * struct.calcsize(\"P\") == 32\n_IS_WASM = platform.machine() in [\"wasm32\", \"wasm64\"]\n\nnp_version = parse_version(np.__version__)\nnp_base_version = parse_version(np_version.base_version)\nsp_version = parse_version(scipy.__version__)\nsp_base_version = parse_version(sp_version.base_version)\n\n# TODO: We can consider removing the containers and importing\n# directly from SciPy when sparse matrices will be deprecated.\nCSR_CONTAINERS = [scipy.sparse.csr_matrix, scipy.sparse.csr_array]\nCSC_CONTAINERS = [scipy.sparse.csc_matrix, scipy.sparse.csc_array]\nCOO_CONTAINERS = [scipy.sparse.coo_matrix, scipy.sparse.coo_array]\nLIL_CONTAINERS = [scipy.sparse.lil_matrix, scipy.sparse.lil_array]\nDOK_CONTAINERS = [scipy.sparse.dok_matrix, scipy.sparse.dok_array]\nBSR_CONTAINERS = [scipy.sparse.bsr_matrix, scipy.sparse.bsr_array]\nDIA_CONTAINERS = [scipy.sparse.dia_matrix, scipy.sparse.dia_array]\n\n# Remove when minimum scipy version is 1.11.0\ntry:\n    from scipy.sparse import sparray  # noqa: F401\n\n    SPARRAY_PRESENT = True\nexcept ImportError:\n    SPARRAY_PRESENT = False\n\n\ndef _object_dtype_isnan(X):\n    return X != X\n\n\n# TODO: Remove when SciPy 1.11 is the minimum supported version\ndef _mode(a, axis=0):\n    if sp_version >= parse_version(\"1.9.0\"):\n        mode = scipy.stats.mode(a, axis=axis, keepdims=True)\n        if sp_version >= parse_version(\"1.10.999\"):\n            # scipy.stats.mode "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  check_array,\n    check_symmetric,\n    check_X_y,\n    deprecated,\n)\nfrom sklearn.utils._array_api import (\n    _convert_to_numpy,\n    _get_namespace_device_dtype_ids,\n    _is_numpy_namespace,\n    yield_namespace_device_dtype_combinations,\n)\nfrom sklearn.utils._mocking import (\n    MockDataFrame,\n    _MockEstimatorOnOffPrediction,\n)\nfrom sklearn.utils._testing import (\n    SkipTest,\n    TempMemmap,\n    _array_api_for_tests,\n    _convert_container,\n    assert_allclose,\n    assert_allclose_dense_sparse,\n    assert_array_equal,\n    create_memmap_backed_data,\n    skip_if_array_api_compat_not_configured,\n)\nfrom sklearn.utils.estimator_checks import _NotAnArray\nfrom sklearn.utils.fixes import (\n    COO_CONTAINERS,\n    CSC_CONTAINERS,\n    CSR_CONTAINERS,\n    DIA_CONTAINERS,\n    DOK_CONTAINERS,\n)\nfrom sklearn.utils.validation import (\n    FLOAT_DTYPES,\n    _allclose_dense_sparse,\n    _check_feature_names_in,\n    _check_method_params,\n    _check_pos_label_consistency,\n    _check_psd_eigenvalues,\n    _check_response_method,\n    _check_sample_weight,\n    _check_y,\n    _deprecate_positional_args,\n    _estimator_has,\n    _get_feature_names,\n    _is_fitted,\n    _is_pandas_df,\n    _is_polars_df,\n    _num_features,\n    _num_samples,\n    _to_object_array,\n    assert_all_finite,\n    check_consistent_length,\n    check_is_fitted,\n    check_memory,\n    check_non_negative,\n    check_random_state,\n    check_scalar,\n    column_or_1d,\n    has_fit_parameter,\n    validate_data,\n)\n\n\ndef test_make_rng():\n    # Check the check_random_state utility function behavior\n    assert check_random_state(None) is np.random.mtrand._rand\n    assert check_random_state(np.random) is np.random.mtrand._rand\n\n    rng_42 = np.random.RandomState(42)\n    assert check_random_state(42).randint(100) == rng_42.randint(100)\n\n    rng_42 = np.random.RandomState(42)\n    assert check_random_state(rng_42) is rng_42\n\n    rng_42 = np.random.RandomState(42)\n    assert check_random_state(43).randint(100) != rng_42.randint(100)\n\n"}, {"start_line": 43000, "end_line": 45000, "belongs_to": {"file_name": "api_reference.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/doc", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ils\", \"class_weight\"),\n                \"autosummary\": [\n                    \"class_weight.compute_class_weight\",\n                    \"class_weight.compute_sample_weight\",\n                ],\n            },\n            {\n                \"title\": \"Dealing with multiclass target in classifiers\",\n                \"description\": _get_submodule(\"sklearn.utils\", \"multiclass\"),\n                \"autosummary\": [\n                    \"multiclass.is_multilabel\",\n                    \"multiclass.type_of_target\",\n                    \"multiclass.unique_labels\",\n                ],\n            },\n            {\n                \"title\": \"Optimal mathematical operations\",\n                \"description\": _get_submodule(\"sklearn.utils\", \"extmath\"),\n                \"autosummary\": [\n                    \"extmath.density\",\n                    \"extmath.fast_logdet\",\n                    \"extmath.randomized_range_finder\",\n                    \"extmath.randomized_svd\",\n                    \"extmath.safe_sparse_dot\",\n                    \"extmath.weighted_mode\",\n                ],\n            },\n            {\n                \"title\": \"Working with sparse matrices and arrays\",\n                \"description\": _get_submodule(\"sklearn.utils\", \"sparsefuncs\"),\n                \"autosummary\": [\n                    \"sparsefuncs.incr_mean_variance_axis\",\n                    \"sparsefuncs.inplace_column_scale\",\n                    \"sparsefuncs.inplace_csr_column_scale\",\n                    \"sparsefuncs.inplace_row_scale\",\n                    \"sparsefuncs.inplace_swap_column\",\n                    \"sparsefuncs.inplace_swap_row\",\n                    \"sparsefuncs.mean_variance_axis\",\n                ],\n            },\n            {\n                \"title\": None,\n                \"description\": _get_submodule(\"sklearn.utils\", \"sparsefuncs_fast\"),\n                \"autosummary\": [\n                    \"sparsefuncs_fast.inplace_csr_row_normalize_l1\",\n                    \"sparsefuncs_fast.inplace_csr_row_normalize_l2\",\n         "}, {"start_line": 34000, "end_line": 36000, "belongs_to": {"file_name": "_array_api.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ion. This function only supports 2D arrays.\n    \"\"\"\n    from sklearn.utils.sparsefuncs import count_nonzero\n\n    xp, _ = get_namespace(X, sample_weight, xp=xp)\n    if _is_numpy_namespace(xp) and sp.issparse(X):\n        return count_nonzero(X, axis=axis, sample_weight=sample_weight)\n\n    assert X.ndim == 2\n\n    weights = xp.ones_like(X, device=device)\n    if sample_weight is not None:\n        sample_weight = xp.asarray(sample_weight, device=device)\n        sample_weight = xp.reshape(sample_weight, (sample_weight.shape[0], 1))\n        weights = xp.astype(weights, sample_weight.dtype) * sample_weight\n\n    zero_scalar = xp.asarray(0, device=device, dtype=weights.dtype)\n    return xp.sum(xp.where(X != 0, weights, zero_scalar), axis=axis)\n\n\ndef _modify_in_place_if_numpy(xp, func, *args, out=None, **kwargs):\n    if _is_numpy_namespace(xp):\n        func(*args, out=out, **kwargs)\n    else:\n        out = func(*args, **kwargs)\n    return out\n\n\ndef _bincount(array, weights=None, minlength=None, xp=None):\n    # TODO: update if bincount is ever adopted in a future version of the standard:\n    # https://github.com/data-apis/array-api/issues/812\n    xp, _ = get_namespace(array, xp=xp)\n    if hasattr(xp, \"bincount\"):\n        return xp.bincount(array, weights=weights, minlength=minlength)\n\n    array_np = _convert_to_numpy(array, xp=xp)\n    if weights is not None:\n        weights_np = _convert_to_numpy(weights, xp=xp)\n    else:\n        weights_np = None\n    bin_out = numpy.bincount(array_np, weights=weights_np, minlength=minlength)\n    return xp.asarray(bin_out, device=device(array))\n\n\ndef _tolist(array, xp=None):\n    xp, _ = get_namespace(array, xp=xp)\n    if _is_numpy_namespace(xp):\n        return array.tolist()\n    array_np = _convert_to_numpy(array, xp=xp)\n    return [element.item() for element in array_np]\n\n\ndef _logsumexp(array, axis=None, xp=None):\n    # TODO replace by scipy.special.logsumexp when\n    # https://github.com/scipy/scipy/pull/22683 is part of a release.\n    # The "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_array_api.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "import os\nfrom functools import partial\n\nimport numpy\nimport pytest\nimport scipy\nfrom numpy.testing import assert_allclose\n\nfrom sklearn._config import config_context\nfrom sklearn.base import BaseEstimator\nfrom sklearn.utils._array_api import (\n    _add_to_diagonal,\n    _asarray_with_order,\n    _atol_for_type,\n    _average,\n    _convert_to_numpy,\n    _count_nonzero,\n    _estimator_with_converted_arrays,\n    _fill_diagonal,\n    _get_namespace_device_dtype_ids,\n    _is_numpy_namespace,\n    _isin,\n    _logsumexp,\n    _max_precision_float_dtype,\n    _median,\n    _nanmax,\n    _nanmean,\n    _nanmin,\n    _ravel,\n    _validate_diagonal_args,\n    device,\n    get_namespace,\n    get_namespace_and_device,\n    indexing_dtype,\n    np_compat,\n    supported_float_dtypes,\n    yield_namespace_device_dtype_combinations,\n)\nfrom sklearn.utils._testing import (\n    SkipTest,\n    _array_api_for_tests,\n    assert_array_equal,\n    skip_if_array_api_compat_not_configured,\n)\nfrom sklearn.utils.fixes import _IS_32BIT, CSR_CONTAINERS, np_version, parse_version\n\n\n@pytest.mark.parametrize(\"X\", [numpy.asarray([1, 2, 3]), [1, 2, 3]])\ndef test_get_namespace_ndarray_default(X):\n    \"\"\"Check that get_namespace returns NumPy wrapper\"\"\"\n    xp_out, is_array_api_compliant = get_namespace(X)\n    assert xp_out is np_compat\n    assert not is_array_api_compliant\n\n\ndef test_get_namespace_ndarray_creation_device():\n    \"\"\"Check expected behavior with device and creation functions.\"\"\"\n    X = numpy.asarray([1, 2, 3])\n    xp_out, _ = get_namespace(X)\n\n    full_array = xp_out.full(10, fill_value=2.0, device=\"cpu\")\n    assert_allclose(full_array, [2.0] * 10)\n\n    with pytest.raises(ValueError, match=\"Unsupported device\"):\n        xp_out.zeros(10, device=\"cuda\")\n\n\n@skip_if_array_api_compat_not_configured\ndef test_get_namespace_ndarray_with_dispatch():\n    \"\"\"Test get_namespace on NumPy ndarrays.\"\"\"\n\n    X_np = numpy.asarray([[1, 2, 3]])\n\n    with config_context(array_api_dispatch=True):\n        xp_out, is_array_api"}, {"start_line": 24000, "end_line": 26000, "belongs_to": {"file_name": "_array_api.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "sk = xp.all(mask, axis=axis)\n        if xp.any(mask):\n            X = xp.where(mask, xp.asarray(xp.nan, dtype=X.dtype, device=device_), X)\n        return X\n\n\ndef _nanmax(X, axis=None, xp=None):\n    # TODO: refactor once nan-aware reductions are standardized:\n    # https://github.com/data-apis/array-api/issues/621\n    xp, _, device_ = get_namespace_and_device(X, xp=xp)\n    if _is_numpy_namespace(xp):\n        return xp.asarray(numpy.nanmax(X, axis=axis))\n\n    else:\n        mask = xp.isnan(X)\n        X = xp.max(\n            xp.where(mask, xp.asarray(-xp.inf, dtype=X.dtype, device=device_), X),\n            axis=axis,\n        )\n        # Replace Infs from all NaN slices with NaN again\n        mask = xp.all(mask, axis=axis)\n        if xp.any(mask):\n            X = xp.where(mask, xp.asarray(xp.nan, dtype=X.dtype, device=device_), X)\n        return X\n\n\ndef _nanmean(X, axis=None, xp=None):\n    # TODO: refactor once nan-aware reductions are standardized:\n    # https://github.com/data-apis/array-api/issues/621\n    xp, _, device_ = get_namespace_and_device(X, xp=xp)\n    if _is_numpy_namespace(xp):\n        return xp.asarray(numpy.nanmean(X, axis=axis))\n    else:\n        mask = xp.isnan(X)\n        total = xp.sum(\n            xp.where(mask, xp.asarray(0.0, dtype=X.dtype, device=device_), X), axis=axis\n        )\n        count = xp.sum(xp.astype(xp.logical_not(mask), X.dtype), axis=axis)\n        return total / count\n\n\ndef _asarray_with_order(\n    array, dtype=None, order=None, copy=None, *, xp=None, device=None\n):\n    \"\"\"Helper to support the order kwarg only for NumPy-backed arrays\n\n    Memory layout parameter `order` is not exposed in the Array API standard,\n    however some input validation code in scikit-learn needs to work both\n    for classes and functions that will leverage Array API only operations\n    and for code that inherently relies on NumPy backed data containers with\n    specific memory layout constraints (e.g. our own Cython code). The\n    purpose of this helper is t"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "_array_api.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tools to support array_api.\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport itertools\nimport math\nimport os\n\nimport numpy\nimport scipy\nimport scipy.sparse as sp\nimport scipy.special as special\n\nfrom sklearn._config import get_config\nfrom sklearn.externals import array_api_compat\nfrom sklearn.externals import array_api_extra as xpx\nfrom sklearn.externals.array_api_compat import numpy as np_compat\nfrom sklearn.utils.fixes import parse_version\n\n# TODO: complete __all__\n__all__ = [\"xpx\"]  # we import xpx here just to re-export it, need this to appease ruff\n\n_NUMPY_NAMESPACE_NAMES = {\"numpy\", \"sklearn.externals.array_api_compat.numpy\"}\n\n\ndef yield_namespaces(include_numpy_namespaces=True):\n    \"\"\"Yield supported namespace.\n\n    This is meant to be used for testing purposes only.\n\n    Parameters\n    ----------\n    include_numpy_namespaces : bool, default=True\n        If True, also yield numpy namespaces.\n\n    Returns\n    -------\n    array_namespace : str\n        The name of the Array API namespace.\n    \"\"\"\n    for array_namespace in [\n        # The following is used to test the array_api_compat wrapper when\n        # array_api_dispatch is enabled: in particular, the arrays used in the\n        # tests are regular numpy arrays without any \"device\" attribute.\n        \"numpy\",\n        # Stricter NumPy-based Array API implementation. The\n        # array_api_strict.Array instances always have a dummy \"device\" attribute.\n        \"array_api_strict\",\n        \"cupy\",\n        \"torch\",\n    ]:\n        if not include_numpy_namespaces and array_namespace in _NUMPY_NAMESPACE_NAMES:\n            continue\n        yield array_namespace\n\n\ndef yield_namespace_device_dtype_combinations(include_numpy_namespaces=True):\n    \"\"\"Yield supported namespace, device, dtype tuples for testing.\n\n    Use this to test that an estimator works with all combinations.\n    Use in conjunction with `ids=_get_namespace_device_dtype_ids` to give\n    clearer pytest parame"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "fixes.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ".int64\n            if arr.size == 0:\n                # a bigger type not needed yet, let's look at the next array\n                continue\n            else:\n                maxval = arr.max()\n                minval = arr.min()\n                if minval < int32min or maxval > int32max:\n                    # a big index type is actually needed\n                    return np.int64\n\n    return np.int32\n\n\n# TODO: Remove when Scipy 1.12 is the minimum supported version\nif sp_version < parse_version(\"1.12\"):\n    from sklearn.externals._scipy.sparse.csgraph import laplacian\nelse:\n    from scipy.sparse.csgraph import (\n        laplacian,  # noqa: F401  # pragma: no cover\n    )\n\n\n# TODO: Remove when Python min version >= 3.12.\ndef tarfile_extractall(tarfile, path):\n    try:\n        # Use filter=\"data\" to prevent the most dangerous security issues.\n        # For more details, see\n        # https://docs.python.org/3/library/tarfile.html#tarfile.TarFile.extractall\n        tarfile.extractall(path, filter=\"data\")\n    except TypeError:\n        tarfile.extractall(path)\n\n\ndef _in_unstable_openblas_configuration():\n    \"\"\"Return True if in an unstable configuration for OpenBLAS\"\"\"\n\n    # Import libraries which might load OpenBLAS.\n    import numpy  # noqa: F401\n    import scipy  # noqa: F401\n\n    modules_info = _get_threadpool_controller().info()\n\n    open_blas_used = any(info[\"internal_api\"] == \"openblas\" for info in modules_info)\n    if not open_blas_used:\n        return False\n\n    # OpenBLAS 0.3.16 fixed instability for arm64, see:\n    # https://github.com/xianyi/OpenBLAS/blob/1b6db3dbba672b4f8af935bd43a1ff6cff4d20b7/Changelog.txt#L56-L58\n    openblas_arm64_stable_version = parse_version(\"0.3.16\")\n    for info in modules_info:\n        if info[\"internal_api\"] != \"openblas\":\n            continue\n        openblas_version = info.get(\"version\")\n        openblas_architecture = info.get(\"architecture\")\n        if openblas_version is None or openblas_architecture is None:\n            # Cann"}], "retrieved_count": 10, "cost_time": 1.1235320568084717}
{"question": "Where in Scikit-learn's cross-validation process does the data flow from splitting through model fitting to performance evaluation?", "answer": null, "relative_code_list": null, "ground_truth": "In scikit-learn's cross-validation process, data flows through a well-defined sequence of stages from splitting through model fitting to performance evaluation. Here's the detailed data flow:\n\n1. **Data Splitting Stage**:\n   - **CV Splitter Selection**: Cross-validation splitter (e.g., KFold, StratifiedKFold) is selected\n   - **Index Generation**: Splitter generates train/test indices for each fold\n   - **Data Partitioning**: Original dataset is partitioned into training and validation subsets\n   - **Fold Iteration**: Process repeats for each fold (typically 5-fold by default)\n   - **Data Isolation**: Each fold ensures proper separation of training and validation data\n\n2. **Model Preparation Stage**:\n   - **Estimator Cloning**: Original estimator is cloned for each fold to ensure independence\n   - **Parameter Setup**: Model parameters are configured for each fold\n   - **State Initialization**: Each cloned estimator starts with a clean state\n   - **Memory Allocation**: Resources are allocated for each fold's computation\n   - **Parallel Preparation**: Multiple folds can be prepared in parallel\n\n3. **Training Data Flow**:\n   - **Data Subsetting**: Training indices are used to extract training data from original dataset\n   - **Data Validation**: Training data is validated using check_array() and check_X_y()\n   - **Preprocessing Application**: If using pipelines, preprocessing is applied to training data\n   - **Feature Engineering**: Any feature engineering steps are applied to training data\n   - **Model Fitting**: Estimator learns parameters from the training subset\n\n4. **Model Fitting Stage**:\n   - **Parameter Learning**: Model learns optimal parameters from training data\n   - **Internal State**: Model stores learned parameters and internal state\n   - **Validation Checks**: Model validates that it can work with the training data\n   - **Convergence**: Model training continues until convergence criteria are met\n   - **State Preservation**: Fitted model state is preserved for prediction\n\n5. **Validation Data Flow**:\n   - **Data Subsetting**: Validation indices are used to extract validation data\n   - **Data Validation**: Validation data is validated for consistency with training data\n   - **Preprocessing Application**: Same preprocessing is applied to validation data\n   - **Feature Consistency**: Ensures validation data has same features as training data\n   - **Prediction Preparation**: Validation data is prepared for model prediction\n\n6. **Prediction Stage**:\n   - **Model Prediction**: Fitted model generates predictions on validation data\n   - **Output Generation**: Model produces predictions, probabilities, or decision functions\n   - **Result Collection**: Predictions are collected for performance evaluation\n   - **Error Handling**: Any prediction errors are handled gracefully\n   - **Memory Management**: Prediction results are managed efficiently\n\n7. **Performance Evaluation Stage**:\n   - **Scoring Function**: Appropriate scoring function is applied to predictions\n   - **Metric Computation**: Performance metrics (accuracy, precision, recall, etc.) are computed\n   - **Result Storage**: Performance results are stored for each fold\n   - **Error Handling**: Failed evaluations are handled with error_score parameter\n   - **Timing Measurement**: Fit and score times are measured for each fold\n\n8. **Result Aggregation Stage**:\n   - **Score Collection**: Performance scores from all folds are collected\n   - **Statistical Summary**: Mean, standard deviation, and other statistics are computed\n   - **Result Formatting**: Results are formatted into appropriate data structures\n   - **Additional Metrics**: Multiple metrics can be computed if specified\n   - **Metadata Storage**: Additional information (timing, indices) is stored if requested\n\n9. **Parallel Processing Integration**:\n   - **Job Distribution**: Folds can be processed in parallel using joblib\n   - **Resource Management**: Computational resources are managed across parallel jobs\n   - **Result Synchronization**: Results from parallel jobs are synchronized\n   - **Memory Coordination**: Memory usage is coordinated across parallel processes\n   - **Error Propagation**: Errors in parallel jobs are properly handled and reported\n\n10. **Advanced Features**:\n    - **Multiple Metrics**: Can evaluate multiple performance metrics simultaneously\n    - **Custom Scoring**: Supports custom scoring functions for specialized evaluation\n    - **Metadata Routing**: Advanced metadata can flow through the CV process\n    - **Group CV**: Supports group-based cross-validation for dependent samples\n    - **Time Series CV**: Specialized CV for time series data with temporal dependencies", "score": null, "retrieved_content": [{"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "        # the user is not calling `fit` directly, so we change the message\n            # to make it more suitable for this case.\n            raise UnsetMetadataPassedError(\n                message=str(e).replace(\"cross_validate.fit\", \"cross_validate\"),\n                unrequested_params=e.unrequested_params,\n                routed_params=e.routed_params,\n            )\n    else:\n        routed_params = Bunch()\n        routed_params.splitter = Bunch(split={\"groups\": groups})\n        routed_params.estimator = Bunch(fit=params)\n        routed_params.scorer = Bunch(score={})\n\n    indices = cv.split(X, y, **routed_params.splitter.split)\n    if return_indices:\n        # materialize the indices since we need to store them in the returned dict\n        indices = list(indices)\n\n    # We clone the estimator to make sure that all the folds are\n    # independent, and that it is pickle-able.\n    parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n    results = parallel(\n        delayed(_fit_and_score)(\n            clone(estimator),\n            X,\n            y,\n            scorer=scorers,\n            train=train,\n            test=test,\n            verbose=verbose,\n            parameters=None,\n            fit_params=routed_params.estimator.fit,\n            score_params=routed_params.scorer.score,\n            return_train_score=return_train_score,\n            return_times=True,\n            return_estimator=return_estimator,\n            error_score=error_score,\n        )\n        for train, test in indices\n    )\n\n    _warn_or_raise_about_fit_failures(results, error_score)\n\n    # For callable scoring, the return type is only know after calling. If the\n    # return type is a dictionary, the error scores can now be inserted with\n    # the correct key.\n    if callable(scoring):\n        _insert_error_scores(results, error_score)\n\n    results = _aggregate_score_dicts(results)\n\n    ret = {}\n    ret[\"fit_time\"] = results[\"fit_time\"]\n    ret[\"score_time\"] = results[\""}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "thods. For these router methods, we create the router to use\n        # `process_routing` on it.\n        router = (\n            MetadataRouter(owner=\"cross_validate\")\n            .add(\n                splitter=cv,\n                method_mapping=MethodMapping().add(caller=\"fit\", callee=\"split\"),\n            )\n            .add(\n                estimator=estimator,\n                # TODO(SLEP6): also pass metadata to the predict method for\n                # scoring?\n                method_mapping=MethodMapping().add(caller=\"fit\", callee=\"fit\"),\n            )\n            .add(\n                scorer=scorers,\n                method_mapping=MethodMapping().add(caller=\"fit\", callee=\"score\"),\n            )\n        )\n        try:\n            routed_params = process_routing(router, \"fit\", **params)\n        except UnsetMetadataPassedError as e:\n            # The default exception would mention `fit` since in the above\n            # `process_routing` code, we pass `fit` as the caller. However,\n            # the user is not calling `fit` directly, so we change the message\n            # to make it more suitable for this case.\n            raise UnsetMetadataPassedError(\n                message=str(e).replace(\"cross_validate.fit\", \"cross_validate\"),\n                unrequested_params=e.unrequested_params,\n                routed_params=e.routed_params,\n            )\n    else:\n        routed_params = Bunch()\n        routed_params.splitter = Bunch(split={\"groups\": groups})\n        routed_params.estimator = Bunch(fit=params)\n        routed_params.scorer = Bunch(score={})\n\n    indices = cv.split(X, y, **routed_params.splitter.split)\n    if return_indices:\n        # materialize the indices since we need to store them in the returned dict\n        indices = list(indices)\n\n    # We clone the estimator to make sure that all the folds are\n    # independent, and that it is pickle-able.\n    parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n    results = parallel(\n   "}, {"start_line": 25000, "end_line": 27000, "belongs_to": {"file_name": "_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "t datasets, linear_model\n    >>> from sklearn.model_selection import cross_val_score\n    >>> diabetes = datasets.load_diabetes()\n    >>> X = diabetes.data[:150]\n    >>> y = diabetes.target[:150]\n    >>> lasso = linear_model.Lasso()\n    >>> print(cross_val_score(lasso, X, y, cv=3))\n    [0.3315057  0.08022103 0.03531816]\n    \"\"\"\n    # To ensure multimetric format is not supported\n    scorer = check_scoring(estimator, scoring=scoring)\n\n    cv_results = cross_validate(\n        estimator=estimator,\n        X=X,\n        y=y,\n        groups=groups,\n        scoring={\"score\": scorer},\n        cv=cv,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        params=params,\n        pre_dispatch=pre_dispatch,\n        error_score=error_score,\n    )\n    return cv_results[\"test_score\"]\n\n\ndef _fit_and_score(\n    estimator,\n    X,\n    y,\n    *,\n    scorer,\n    train,\n    test,\n    verbose,\n    parameters,\n    fit_params,\n    score_params,\n    return_train_score=False,\n    return_parameters=False,\n    return_n_test_samples=False,\n    return_times=False,\n    return_estimator=False,\n    split_progress=None,\n    candidate_progress=None,\n    error_score=np.nan,\n):\n    \"\"\"Fit estimator and compute scores for a given dataset split.\n\n    Parameters\n    ----------\n    estimator : estimator object implementing 'fit'\n        The object to use to fit the data.\n\n    X : array-like of shape (n_samples, n_features)\n        The data to fit.\n\n    y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n        The target variable to try to predict in the case of\n        supervised learning.\n\n    scorer : A single callable or dict mapping scorer name to the callable\n        If it is a single callable, the return value for ``train_scores`` and\n        ``test_scores`` is a single float.\n\n        For a dict, it should be one mapping the scorer name to the scorer\n        callable object / function.\n\n        The callable object / fn should have signature\n        ``scorer(estimator, X, y)``.\n\n  "}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "stance(estimators, list)\n    assert all(isinstance(estimator, Pipeline) for estimator in estimators)\n\n\n@pytest.mark.parametrize(\"use_sparse\", [False, True])\n@pytest.mark.parametrize(\"csr_container\", CSR_CONTAINERS)\ndef test_cross_validate(use_sparse: bool, csr_container):\n    # Compute train and test mse/r2 scores\n    cv = KFold()\n\n    # Regression\n    X_reg, y_reg = make_regression(n_samples=30, random_state=0)\n    reg = Ridge(random_state=0)\n\n    # Classification\n    X_clf, y_clf = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n\n    if use_sparse:\n        X_reg = csr_container(X_reg)\n        X_clf = csr_container(X_clf)\n\n    for X, y, est in ((X_reg, y_reg, reg), (X_clf, y_clf, clf)):\n        # It's okay to evaluate regression metrics on classification too\n        mse_scorer = check_scoring(est, scoring=\"neg_mean_squared_error\")\n        r2_scorer = check_scoring(est, scoring=\"r2\")\n        train_mse_scores = []\n        test_mse_scores = []\n        train_r2_scores = []\n        test_r2_scores = []\n        fitted_estimators = []\n\n        for train, test in cv.split(X, y):\n            est = clone(est).fit(X[train], y[train])\n            train_mse_scores.append(mse_scorer(est, X[train], y[train]))\n            train_r2_scores.append(r2_scorer(est, X[train], y[train]))\n            test_mse_scores.append(mse_scorer(est, X[test], y[test]))\n            test_r2_scores.append(r2_scorer(est, X[test], y[test]))\n            fitted_estimators.append(est)\n\n        train_mse_scores = np.array(train_mse_scores)\n        test_mse_scores = np.array(test_mse_scores)\n        train_r2_scores = np.array(train_r2_scores)\n        test_r2_scores = np.array(test_r2_scores)\n        fitted_estimators = np.array(fitted_estimators)\n\n        scores = (\n            train_mse_scores,\n            test_mse_scores,\n            train_r2_scores,\n            test_r2_scores,\n            fitted_estimators,\n        )\n\n        # To ensure that the test does not "}, {"start_line": 30000, "end_line": 32000, "belongs_to": {"file_name": "_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     progress_msg = f\" {split_progress[0] + 1}/{split_progress[1]}\"\n        if candidate_progress and verbose > 9:\n            progress_msg += f\"; {candidate_progress[0] + 1}/{candidate_progress[1]}\"\n\n    if verbose > 1:\n        if parameters is None:\n            params_msg = \"\"\n        else:\n            sorted_keys = sorted(parameters)  # Ensure deterministic o/p\n            params_msg = \", \".join(f\"{k}={parameters[k]}\" for k in sorted_keys)\n    if verbose > 9:\n        start_msg = f\"[CV{progress_msg}] START {params_msg}\"\n        print(f\"{start_msg}{(80 - len(start_msg)) * '.'}\")\n\n    # Adjust length of sample weights\n    fit_params = fit_params if fit_params is not None else {}\n    fit_params = _check_method_params(X, params=fit_params, indices=train)\n    score_params = score_params if score_params is not None else {}\n    score_params_train = _check_method_params(X, params=score_params, indices=train)\n    score_params_test = _check_method_params(X, params=score_params, indices=test)\n\n    if parameters is not None:\n        # here we clone the parameters, since sometimes the parameters\n        # themselves might be estimators, e.g. when we search over different\n        # estimators in a pipeline.\n        # ref: https://github.com/scikit-learn/scikit-learn/pull/26786\n        estimator = estimator.set_params(**clone(parameters, safe=False))\n\n    start_time = time.time()\n\n    X_train, y_train = _safe_split(estimator, X, y, train)\n    X_test, y_test = _safe_split(estimator, X, y, test, train)\n\n    result = {}\n    try:\n        if y_train is None:\n            estimator.fit(X_train, **fit_params)\n        else:\n            estimator.fit(X_train, y_train, **fit_params)\n\n    except Exception:\n        # Note fit time as time until error\n        fit_time = time.time() - start_time\n        score_time = 0.0\n        if error_score == \"raise\":\n            raise\n        elif isinstance(error_score, numbers.Number):\n            if isinstance(scorer, _MultimetricScorer):\n              "}, {"start_line": 83000, "end_line": 85000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    cv = KFold(n_splits=3, shuffle=True, random_state=global_random_seed)\n    cv_results = cross_validate(estimator, X, y, cv=cv, n_jobs=2, return_indices=False)\n    assert \"indices\" not in cv_results\n\n    cv_results = cross_validate(estimator, X, y, cv=cv, n_jobs=2, return_indices=True)\n    assert \"indices\" in cv_results\n    train_indices = cv_results[\"indices\"][\"train\"]\n    test_indices = cv_results[\"indices\"][\"test\"]\n    assert len(train_indices) == cv.n_splits\n    assert len(test_indices) == cv.n_splits\n\n    assert_array_equal([indices.size for indices in train_indices], 100)\n    assert_array_equal([indices.size for indices in test_indices], 50)\n\n    for split_idx, (expected_train_idx, expected_test_idx) in enumerate(cv.split(X, y)):\n        assert_array_equal(train_indices[split_idx], expected_train_idx)\n        assert_array_equal(test_indices[split_idx], expected_test_idx)\n\n\n# Tests for metadata routing in cross_val* and in *curve\n# ======================================================\n\n\n# TODO(1.8): remove `learning_curve`, `validation_curve` and `permutation_test_score`.\n@pytest.mark.parametrize(\n    \"func, extra_args\",\n    [\n        (learning_curve, {}),\n        (permutation_test_score, {}),\n        (validation_curve, {\"param_name\": \"alpha\", \"param_range\": np.array([1])}),\n    ],\n)\ndef test_fit_param_deprecation(func, extra_args):\n    \"\"\"Check that we warn about deprecating `fit_params`.\"\"\"\n    with pytest.warns(FutureWarning, match=\"`fit_params` is deprecated\"):\n        func(\n            estimator=ConsumingClassifier(), X=X, y=y, cv=2, fit_params={}, **extra_args\n        )\n\n    with pytest.raises(\n        ValueError, match=\"`params` and `fit_params` cannot both be provided\"\n    ):\n        func(\n            estimator=ConsumingClassifier(),\n            X=X,\n            y=y,\n            fit_params={},\n            params={},\n            **extra_args,\n        )\n\n\n@pytest.mark.parametrize(\n    \"func, extra_args\",\n    [\n        (cross_validate, {}),\n        (cro"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "train_score`` changes to a specific\n            metric like ``train_r2`` or ``train_auc`` if there are\n            multiple scoring metrics in the scoring parameter.\n            This is available only if ``return_train_score`` parameter\n            is ``True``.\n        ``fit_time``\n            The time for fitting the estimator on the train\n            set for each cv split.\n        ``score_time``\n            The time for scoring the estimator on the test set for each\n            cv split. (Note: time for scoring on the train set is not\n            included even if ``return_train_score`` is set to ``True``).\n        ``estimator``\n            The estimator objects for each cv split.\n            This is available only if ``return_estimator`` parameter\n            is set to ``True``.\n        ``indices``\n            The train/test positional indices for each cv split. A dictionary\n            is returned where the keys are either `\"train\"` or `\"test\"`\n            and the associated values are a list of integer-dtyped NumPy\n            arrays with the indices. Available only if `return_indices=True`.\n\n    See Also\n    --------\n    cross_val_score : Run cross-validation for single metric evaluation.\n\n    cross_val_predict : Get predictions from each split of cross-validation for\n        diagnostic purposes.\n\n    sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n        loss function.\n\n    Examples\n    --------\n    >>> from sklearn import datasets, linear_model\n    >>> from sklearn.model_selection import cross_validate\n    >>> from sklearn.metrics import make_scorer\n    >>> from sklearn.metrics import confusion_matrix\n    >>> from sklearn.svm import LinearSVC\n    >>> diabetes = datasets.load_diabetes()\n    >>> X = diabetes.data[:150]\n    >>> y = diabetes.target[:150]\n    >>> lasso = linear_model.Lasso()\n\n    Single metric evaluation using ``cross_validate``\n\n    >>> cv_results = cross_validate(lasso, X, y, cv=3)\n    >>> sorted(cv_results.keys())\n    "}, {"start_line": 38000, "end_line": 40000, "belongs_to": {"file_name": "_search.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "                  n_splits, n_candidates, n_candidates * n_splits\n                        )\n                    )\n\n                out = parallel(\n                    delayed(_fit_and_score)(\n                        clone(base_estimator),\n                        X,\n                        y,\n                        train=train,\n                        test=test,\n                        parameters=parameters,\n                        split_progress=(split_idx, n_splits),\n                        candidate_progress=(cand_idx, n_candidates),\n                        **fit_and_score_kwargs,\n                    )\n                    for (cand_idx, parameters), (split_idx, (train, test)) in product(\n                        enumerate(candidate_params),\n                        enumerate(cv.split(X, y, **routed_params.splitter.split)),\n                    )\n                )\n\n                if len(out) < 1:\n                    raise ValueError(\n                        \"No fits were performed. \"\n                        \"Was the CV iterator empty? \"\n                        \"Were there no candidates?\"\n                    )\n                elif len(out) != n_candidates * n_splits:\n                    raise ValueError(\n                        \"cv.split and cv.get_n_splits returned \"\n                        \"inconsistent results. Expected {} \"\n                        \"splits, got {}\".format(n_splits, len(out) // n_candidates)\n                    )\n\n                _warn_or_raise_about_fit_failures(out, self.error_score)\n\n                # For callable self.scoring, the return type is only know after\n                # calling. If the return type is a dictionary, the error scores\n                # can now be inserted with the correct key. The type checking\n                # of out will be done in `_insert_error_scores`.\n                if callable(self.scoring):\n                    _insert_error_scores(out, self.error_score)\n\n                all_candidate_params.extend(candidate_param"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_squared_error,\n    precision_recall_fscore_support,\n    precision_score,\n    r2_score,\n)\nfrom sklearn.metrics._scorer import _MultimetricScorer\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    GroupKFold,\n    GroupShuffleSplit,\n    KFold,\n    LeaveOneGroupOut,\n    LeaveOneOut,\n    LeavePGroupsOut,\n    ShuffleSplit,\n    StratifiedKFold,\n    cross_val_predict,\n    cross_val_score,\n    cross_validate,\n    learning_curve,\n    permutation_test_score,\n    validation_curve,\n)\nfrom sklearn.model_selection._validation import (\n    _check_is_permutation,\n    _fit_and_score,\n    _score,\n)\nfrom sklearn.model_selection.tests.common import OneTimeSplitter\nfrom sklearn.model_selection.tests.test_search import FailingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, scale\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tests.metadata_routing_common import (\n    ConsumingClassifier,\n    ConsumingScorer,\n    ConsumingSplitter,\n    _Registry,\n    check_recorded_metadata,\n)\nfrom sklearn.utils import shuffle\nfrom sklearn.utils._mocking import CheckingClassifier, MockDataFrame\nfrom sklearn.utils._testing import (\n    assert_allclose,\n    assert_almost_equal,\n    assert_array_almost_equal,\n    assert_array_equal,\n)\nfrom sklearn.utils.fixes import COO_CONTAINERS, CSR_CONTAINERS\nfrom sklearn.utils.validation import _num_samples\n\n\nclass MockImprovingEstimator(BaseEstimator):\n    \"\"\"Dummy classifier to test the learning curve\"\"\"\n\n    def __init__(self, n_max_train_sizes):\n        self.n_max_train_sizes = n_max_train_sizes\n        self.train_sizes = 0\n        self.X_subset = None\n\n    def fit(self, X_subset, y_subset=None):\n        self.X_subset = X_subset\n        self.train_sizes = X_subset.shape[0]\n        return self\n\n    def predict(self, X):\n        raise NotImplementedError\n\n   "}, {"start_line": 44000, "end_line": 46000, "belongs_to": {"file_name": "_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ing_enabled():\n        # For estimators, a MetadataRouter is created in get_metadata_routing\n        # methods. For these router methods, we create the router to use\n        # `process_routing` on it.\n        router = (\n            MetadataRouter(owner=\"cross_val_predict\")\n            .add(\n                splitter=cv,\n                method_mapping=MethodMapping().add(caller=\"fit\", callee=\"split\"),\n            )\n            .add(\n                estimator=estimator,\n                # TODO(SLEP6): also pass metadata for the predict method.\n                method_mapping=MethodMapping().add(caller=\"fit\", callee=\"fit\"),\n            )\n        )\n        try:\n            routed_params = process_routing(router, \"fit\", **params)\n        except UnsetMetadataPassedError as e:\n            # The default exception would mention `fit` since in the above\n            # `process_routing` code, we pass `fit` as the caller. However,\n            # the user is not calling `fit` directly, so we change the message\n            # to make it more suitable for this case.\n            raise UnsetMetadataPassedError(\n                message=str(e).replace(\"cross_val_predict.fit\", \"cross_val_predict\"),\n                unrequested_params=e.unrequested_params,\n                routed_params=e.routed_params,\n            )\n    else:\n        routed_params = Bunch()\n        routed_params.splitter = Bunch(split={\"groups\": groups})\n        routed_params.estimator = Bunch(fit=params)\n\n    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n    splits = list(cv.split(X, y, **routed_params.splitter.split))\n\n    test_indices = np.concatenate([test for _, test in splits])\n    if not _check_is_permutation(test_indices, _num_samples(X)):\n        raise ValueError(\"cross_val_predict only works for partitions\")\n\n    # If classification methods produce multiple columns of output,\n    # we need to manually encode classes to ensure consistent column ordering.\n    encode = (\n        method in [\"decision_function"}], "retrieved_count": 10, "cost_time": 1.192544937133789}
{"question": "Where in Scikit-learn's estimator pipeline does the data flow from input through preprocessing to model training and prediction?", "answer": null, "relative_code_list": null, "ground_truth": "In scikit-learn's estimator pipeline, data flows through a well-defined sequence of stages from input through preprocessing to model training and prediction. Here's the detailed data flow:\n\n1. **Input Data Stage**:\n   - **Raw Data Input**: Data enters the pipeline as X (features) and optionally y (targets)\n   - **Data Validation**: Input data is validated using check_array() and check_X_y()\n   - **Type Conversion**: Data is converted to appropriate NumPy arrays or sparse matrices\n   - **Shape Validation**: Ensures data has correct dimensions and structure\n   - **Feature Names**: Preserves feature names if available\n\n2. **Preprocessing Stage (Transformers)**:\n   - **Sequential Processing**: Data flows through each transformer in the pipeline sequentially\n   - **Fit Phase**: Each transformer learns parameters from training data (e.g., mean, std for StandardScaler)\n   - **Transform Phase**: Each transformer applies learned parameters to transform the data\n   - **Data Flow**: Output of one transformer becomes input to the next transformer\n   - **State Preservation**: Each transformer maintains its fitted state for later use\n\n3. **Intermediate Data Flow**:\n   - **Data Transformation**: Each step transforms data according to its specific algorithm\n   - **Feature Engineering**: Features may be added, removed, or modified\n   - **Dimensionality Changes**: Some transformers may change the number of features\n   - **Data Type Preservation**: Maintains appropriate data types (dense/sparse) throughout\n   - **Memory Management**: Efficient memory handling for large datasets\n\n4. **Model Training Stage (Final Estimator)**:\n   - **Preprocessed Data**: Final transformer output is passed to the model\n   - **Model Fitting**: The model learns from the preprocessed training data\n   - **Parameter Learning**: Model parameters are optimized based on the transformed features\n   - **State Storage**: Model stores learned parameters and internal state\n   - **Validation**: Model validates that it can work with the transformed data\n\n5. **Prediction Stage**:\n   - **New Data Input**: New data enters the pipeline for prediction\n   - **Preprocessing Application**: Same preprocessing steps are applied to new data\n   - **Transformation Consistency**: Uses stored parameters from training phase\n   - **Model Prediction**: Preprocessed data is passed to the model for prediction\n   - **Output Generation**: Model generates predictions based on learned parameters\n\n6. **Pipeline Integration Points**:\n   - **Unified Interface**: Pipeline provides fit(), predict(), transform() methods\n   - **Method Chaining**: Each step's output flows seamlessly to the next step\n   - **Error Handling**: Errors at any stage are properly handled and reported\n   - **State Management**: Pipeline maintains state across all steps\n   - **Caching**: Intermediate results can be cached for efficiency\n\n7. **Data Flow Control**:\n   - **Sequential Execution**: Steps are executed in the order defined in the pipeline\n   - **Conditional Processing**: Some steps may be skipped based on conditions\n   - **Parallel Processing**: Independent steps can be parallelized where possible\n   - **Memory Optimization**: Data flows efficiently to minimize memory usage\n   - **Performance Monitoring**: Pipeline can track performance at each stage\n\n8. **Cross-Validation Integration**:\n   - **Fold-Specific Processing**: Each CV fold has its own preprocessing pipeline\n   - **Data Isolation**: Training and validation data are properly separated\n   - **Consistent Application**: Same preprocessing is applied consistently across folds\n   - **Performance Evaluation**: Model performance is evaluated on properly preprocessed data\n   - **Parameter Optimization**: Hyperparameters can be optimized across the entire pipeline\n\n9. **Production Deployment**:\n   - **Fitted Pipeline**: Complete pipeline with all fitted transformers and model\n   - **New Data Processing**: New data flows through the same preprocessing steps\n   - **Prediction Generation**: Consistent predictions using the trained pipeline\n   - **Monitoring**: Pipeline performance can be monitored in production\n   - **Updates**: Pipeline can be updated with new data while maintaining consistency\n\n10. **Advanced Data Flow Features**:\n    - **Metadata Routing**: Advanced metadata can flow through the pipeline\n    - **Feature Names**: Feature names are preserved and tracked throughout\n    - **Inverse Transform**: Some pipelines support inverse transformation\n    - **Partial Fitting**: Some pipelines support incremental learning\n    - **Model Persistence**: Complete pipeline state can be saved and loaded", "score": null, "retrieved_content": [{"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "mposition):\n    \"\"\"\n    A sequence of data transformers with an optional final predictor.\n\n    `Pipeline` allows you to sequentially apply a list of transformers to\n    preprocess the data and, if desired, conclude the sequence with a final\n    :term:`predictor` for predictive modeling.\n\n    Intermediate steps of the pipeline must be transformers, that is, they\n    must implement `fit` and `transform` methods.\n    The final :term:`estimator` only needs to implement `fit`.\n    The transformers in the pipeline can be cached using ``memory`` argument.\n\n    The purpose of the pipeline is to assemble several steps that can be\n    cross-validated together while setting different parameters. For this, it\n    enables setting parameters of the various steps using their names and the\n    parameter name separated by a `'__'`, as in the example below. A step's\n    estimator may be replaced entirely by setting the parameter with its name\n    to another estimator, or a transformer removed by setting it to\n    `'passthrough'` or `None`.\n\n    For an example use case of `Pipeline` combined with\n    :class:`~sklearn.model_selection.GridSearchCV`, refer to\n    :ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`. The\n    example :ref:`sphx_glr_auto_examples_compose_plot_digits_pipe.py` shows how\n    to grid search on a pipeline using `'__'` as a separator in the parameter names.\n\n    Read more in the :ref:`User Guide <pipeline>`.\n\n    .. versionadded:: 0.5\n\n    Parameters\n    ----------\n    steps : list of tuples\n        List of (name of step, estimator) tuples that are to be chained in\n        sequential order. To be compatible with the scikit-learn API, all steps\n        must define `fit`. All non-last steps must also define `transform`. See\n        :ref:`Combining Estimators <combining_estimators>` for more details.\n\n    transform_input : list of str, default=None\n        The names of the :term:`metadata` parameters that should be transformed by the\n        pipeline befo"}, {"start_line": 59000, "end_line": 61000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "p : Pipeline\n        Returns a scikit-learn :class:`Pipeline` object.\n\n    See Also\n    --------\n    Pipeline : Class for creating a pipeline of transforms with a final\n        estimator.\n\n    Examples\n    --------\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.pipeline import make_pipeline\n    >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('gaussiannb', GaussianNB())])\n    \"\"\"\n    return Pipeline(\n        _name_estimators(steps),\n        transform_input=transform_input,\n        memory=memory,\n        verbose=verbose,\n    )\n\n\ndef _transform_one(transformer, X, y, weight, params):\n    \"\"\"Call transform and apply weight to output.\n\n    Parameters\n    ----------\n    transformer : estimator\n        Estimator to be used for transformation.\n\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Input data to be transformed.\n\n    y : ndarray of shape (n_samples,)\n        Ignored.\n\n    weight : float\n        Weight to be applied to the output of the transformation.\n\n    params : dict\n        Parameters to be passed to the transformer's ``transform`` method.\n\n        This should be of the form ``process_routing()[\"step_name\"]``.\n    \"\"\"\n    res = transformer.transform(X, **params.transform)\n    # if we have a weight for this transformer, multiply output\n    if weight is None:\n        return res\n    return res * weight\n\n\ndef _fit_transform_one(\n    transformer, X, y, weight, message_clsname=\"\", message=None, params=None\n):\n    \"\"\"\n    Fits ``transformer`` to ``X`` and ``y``. The transformed result is returned\n    with the fitted transformer. If ``weight`` is not ``None``, the result will\n    be multiplied by ``weight``.\n\n    ``params`` needs to be of the form ``process_routing()[\"step_name\"]``.\n    \"\"\"\n    params = params or {}\n    with _print_elapsed_time(message_clsname, message)"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_metadata_routing.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_is_valid,\n)\nfrom sklearn.utils.metadata_routing import (\n    MetadataRequest,\n    MetadataRouter,\n    MethodMapping,\n    _RoutingNotSupportedMixin,\n    get_routing_for_object,\n    process_routing,\n)\nfrom sklearn.utils.validation import check_is_fitted\n\nrng = np.random.RandomState(42)\nN, M = 100, 4\nX = rng.rand(N, M)\ny = rng.randint(0, 2, size=N)\nmy_groups = rng.randint(0, 10, size=N)\nmy_weights = rng.rand(N)\nmy_other_weights = rng.rand(N)\n\n\nclass SimplePipeline(BaseEstimator):\n    \"\"\"A very simple pipeline, assuming the last step is always a predictor.\n\n    Parameters\n    ----------\n    steps : iterable of objects\n        An iterable of transformers with the last step being a predictor.\n    \"\"\"\n\n    def __init__(self, steps):\n        self.steps = steps\n\n    def fit(self, X, y, **fit_params):\n        self.steps_ = []\n        params = process_routing(self, \"fit\", **fit_params)\n        X_transformed = X\n        for i, step in enumerate(self.steps[:-1]):\n            transformer = clone(step).fit(\n                X_transformed, y, **params.get(f\"step_{i}\").fit\n            )\n            self.steps_.append(transformer)\n            X_transformed = transformer.transform(\n                X_transformed, **params.get(f\"step_{i}\").transform\n            )\n\n        self.steps_.append(\n            clone(self.steps[-1]).fit(X_transformed, y, **params.predictor.fit)\n        )\n        return self\n\n    def predict(self, X, **predict_params):\n        check_is_fitted(self)\n        X_transformed = X\n        params = process_routing(self, \"predict\", **predict_params)\n        for i, step in enumerate(self.steps_[:-1]):\n            X_transformed = step.transform(X, **params.get(f\"step_{i}\").transform)\n\n        return self.steps_[-1].predict(X_transformed, **params.predictor.predict)\n\n    def get_metadata_routing(self):\n        router = MetadataRouter(owner=self.__class__.__name__)\n        for i, step in enumerate(self.steps[:-1]):\n            router.add(\n                **{f\"step_{i}\": step"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ring :term:`fit`. Only defined if the\n        underlying first estimator in `steps` exposes such an attribute\n        when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    make_pipeline : Convenience function for simplified pipeline construction.\n\n    Examples\n    --------\n    >>> from sklearn.svm import SVC\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.pipeline import Pipeline\n    >>> X, y = make_classification(random_state=0)\n    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n    ...                                                     random_state=0)\n    >>> pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n    >>> # The pipeline can be used as any other estimator\n    >>> # and avoids leaking the test set into the train set\n    >>> pipe.fit(X_train, y_train).score(X_test, y_test)\n    0.88\n    >>> # An estimator's parameter can be set using '__' syntax\n    >>> pipe.set_params(svc__C=10).fit(X_train, y_train).score(X_test, y_test)\n    0.76\n    \"\"\"\n\n    # BaseEstimator interface\n    _parameter_constraints: dict = {\n        \"steps\": [list, Hidden(tuple)],\n        \"transform_input\": [list, None],\n        \"memory\": [None, str, HasMethods([\"cache\"])],\n        \"verbose\": [\"boolean\"],\n    }\n\n    def __init__(self, steps, *, transform_input=None, memory=None, verbose=False):\n        self.steps = steps\n        self.transform_input = transform_input\n        self.memory = memory\n        self.verbose = verbose\n\n    def set_output(self, *, transform=None):\n        \"\"\"Set the output container when `\"transform\"` and `\"fit_transform\"` are called.\n\n        Calling"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "plot_release_highlights_1_6_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "er methods and properties are left unchanged. An interesting\n# use case for this is to use a pre-fitted model as a transformer step in a pipeline\n# or to pass a pre-fitted model to some of the meta-estimators. Here's a short example:\n\nimport time\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.frozen import FrozenEstimator\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import FixedThresholdClassifier\n\nX, y = make_classification(n_samples=1000, random_state=0)\n\nstart = time.time()\nclassifier = SGDClassifier().fit(X, y)\nprint(f\"Fitting the classifier took {(time.time() - start) * 1_000:.2f} milliseconds\")\n\nstart = time.time()\nthreshold_classifier = FixedThresholdClassifier(\n    estimator=FrozenEstimator(classifier), threshold=0.9\n).fit(X, y)\nprint(\n    f\"Fitting the threshold classifier took {(time.time() - start) * 1_000:.2f} \"\n    \"milliseconds\"\n)\n\n# %%\n# Fitting the threshold classifier skipped fitting the inner `SGDClassifier`. For more\n# details refer to the example :ref:`sphx_glr_auto_examples_frozen_plot_frozen_examples.py`.\n\n# %%\n# Transforming data other than X in a Pipeline\n# --------------------------------------------\n#\n# The :class:`~pipeline.Pipeline` now supports transforming passed data other than `X`\n# if necessary. This can be done by setting the new `transform_input` parameter. This\n# is particularly useful when passing a validation set through the pipeline.\n#\n# As an example, imagine `EstimatorWithValidationSet` is an estimator which accepts\n# a validation set. We can now have a pipeline which will transform the validation set\n# and pass it to the estimator::\n#\n#     with sklearn.config_context(enable_metadata_routing=True):\n#         est_gs = GridSearchCV(\n#             Pipeline(\n#                 (\n#                     StandardScaler(),\n#                     EstimatorWithValidationSet(...).set_fit_request(X_val=True, y_val=True),\n#                 ),\n#                 # telling pipeline to transform th"}, {"start_line": 19000, "end_line": 21000, "belongs_to": {"file_name": "plot_metadata_routing.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/miscellaneous", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    router = (\n            MetadataRouter(owner=self.__class__.__name__)\n            # We add the routing for the transformer.\n            .add(\n                transformer=self.transformer,\n                method_mapping=MethodMapping()\n                # The metadata is routed such that it retraces how\n                # `SimplePipeline` internally calls the transformer's `fit` and\n                # `transform` methods in its own methods (`fit` and `predict`).\n                .add(caller=\"fit\", callee=\"fit\")\n                .add(caller=\"fit\", callee=\"transform\")\n                .add(caller=\"predict\", callee=\"transform\"),\n            )\n            # We add the routing for the classifier.\n            .add(\n                classifier=self.classifier,\n                method_mapping=MethodMapping()\n                .add(caller=\"fit\", callee=\"fit\")\n                .add(caller=\"predict\", callee=\"predict\"),\n            )\n        )\n        return router\n\n    def fit(self, X, y, **fit_params):\n        routed_params = process_routing(self, \"fit\", **fit_params)\n\n        self.transformer_ = clone(self.transformer).fit(\n            X, y, **routed_params.transformer.fit\n        )\n        X_transformed = self.transformer_.transform(\n            X, **routed_params.transformer.transform\n        )\n\n        self.classifier_ = clone(self.classifier).fit(\n            X_transformed, y, **routed_params.classifier.fit\n        )\n        return self\n\n    def predict(self, X, **predict_params):\n        routed_params = process_routing(self, \"predict\", **predict_params)\n\n        X_transformed = self.transformer_.transform(\n            X, **routed_params.transformer.transform\n        )\n        return self.classifier_.predict(\n            X_transformed, **routed_params.classifier.predict\n        )\n\n\n# %%\n# Note the usage of :class:`~utils.metadata_routing.MethodMapping` to\n# declare which methods of the child estimator (callee) are used in which\n# methods of the meta estimator (caller). As you can "}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "plot_metadata_routing.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/miscellaneous", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      routed_params = process_routing(self, \"fit\", **fit_params)\n\n        self.transformer_ = clone(self.transformer).fit(\n            X, y, **routed_params.transformer.fit\n        )\n        X_transformed = self.transformer_.transform(\n            X, **routed_params.transformer.transform\n        )\n\n        self.classifier_ = clone(self.classifier).fit(\n            X_transformed, y, **routed_params.classifier.fit\n        )\n        return self\n\n    def predict(self, X, **predict_params):\n        routed_params = process_routing(self, \"predict\", **predict_params)\n\n        X_transformed = self.transformer_.transform(\n            X, **routed_params.transformer.transform\n        )\n        return self.classifier_.predict(\n            X_transformed, **routed_params.classifier.predict\n        )\n\n\n# %%\n# Note the usage of :class:`~utils.metadata_routing.MethodMapping` to\n# declare which methods of the child estimator (callee) are used in which\n# methods of the meta estimator (caller). As you can see, `SimplePipeline` uses\n# the transformer's ``transform`` and ``fit`` methods in ``fit``, and its\n# ``transform`` method in ``predict``, and that's what you see implemented in\n# the routing structure of the pipeline class.\n#\n# Another difference in the above example with the previous ones is the usage\n# of :func:`~utils.metadata_routing.process_routing`, which processes the input\n# parameters, does the required validation, and returns the `routed_params`\n# which we had created in previous examples. This reduces the boilerplate code\n# a developer needs to write in each meta-estimator's method. Developers are\n# strongly recommended to use this function unless there is a good reason\n# against it.\n#\n# In order to test the above pipeline, let's add an example transformer.\n\n\nclass ExampleTransformer(TransformerMixin, BaseEstimator):\n    def fit(self, X, y, sample_weight=None):\n        check_metadata(self, sample_weight=sample_weight)\n        return self\n\n    def transform(self, X, groups="}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "plot_release_highlights_0_23_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "01)\nglm.fit(X_train, y_train)\ngbdt.fit(X_train, y_train)\nprint(glm.score(X_test, y_test))\nprint(gbdt.score(X_test, y_test))\n\n##############################################################################\n# Rich visual representation of estimators\n# -----------------------------------------\n# Estimators can now be visualized in notebooks by enabling the\n# `display='diagram'` option. This is particularly useful to summarise the\n# structure of pipelines and other composite estimators, with interactivity to\n# provide detail.  Click on the example image below to expand Pipeline\n# elements.  See :ref:`visualizing_composite_estimators` for how you can use\n# this feature.\n\nfrom sklearn import set_config\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nset_config(display=\"diagram\")\n\nnum_proc = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n\ncat_proc = make_pipeline(\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n    OneHotEncoder(handle_unknown=\"ignore\"),\n)\n\npreprocessor = make_column_transformer(\n    (num_proc, (\"feat1\", \"feat3\")), (cat_proc, (\"feat0\", \"feat2\"))\n)\n\nclf = make_pipeline(preprocessor, LogisticRegression())\nclf\n\n##############################################################################\n# Scalability and stability improvements to KMeans\n# ------------------------------------------------\n# The :class:`~sklearn.cluster.KMeans` estimator was entirely re-worked, and it\n# is now significantly faster and more stable. In addition, the Elkan algorithm\n# is now compatible with sparse matrices. The estimator uses OpenMP based\n# parallelism instead of relying on joblib, so the `n_jobs` parameter has no\n# effect anymore. For more details on how to control the number of threads,\n# please refer to our :ref:`parallelism` notes.\nimport nump"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Utilities to build a composite estimator as a chain of transforms and estimators.\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport warnings\nfrom collections import Counter, defaultdict\nfrom contextlib import contextmanager\nfrom copy import deepcopy\nfrom itertools import chain, islice\n\nimport numpy as np\nfrom scipy import sparse\n\nfrom sklearn.base import TransformerMixin, _fit_context, clone\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.utils import Bunch\nfrom sklearn.utils._metadata_requests import METHODS\nfrom sklearn.utils._param_validation import HasMethods, Hidden\nfrom sklearn.utils._repr_html.estimator import _VisualBlock\nfrom sklearn.utils._set_output import _get_container_adapter, _safe_set_output\nfrom sklearn.utils._tags import get_tags\nfrom sklearn.utils._user_interface import _print_elapsed_time\nfrom sklearn.utils.metadata_routing import (\n    MetadataRouter,\n    MethodMapping,\n    _raise_for_params,\n    _routing_enabled,\n    get_routing_for_object,\n    process_routing,\n)\nfrom sklearn.utils.metaestimators import _BaseComposition, available_if\nfrom sklearn.utils.parallel import Parallel, delayed\nfrom sklearn.utils.validation import check_is_fitted, check_memory\n\n__all__ = [\"FeatureUnion\", \"Pipeline\", \"make_pipeline\", \"make_union\"]\n\n\n@contextmanager\ndef _raise_or_warn_if_not_fitted(estimator):\n    \"\"\"A context manager to make sure a NotFittedError is raised, if a sub-estimator\n    raises the error.\n\n    Otherwise, we raise a warning if the pipeline is not fitted, with the deprecation.\n\n    TODO(1.8): remove this context manager and replace with check_is_fitted.\n    \"\"\"\n    try:\n        yield\n    except NotFittedError as exc:\n        raise NotFittedError(\"Pipeline is not fitted yet.\") from exc\n\n    # we only get here if the above didn't raise\n    try:\n        check_is_fitted(estimator)\n    except NotFittedError:\n        warnings.warn(\n            \"T"}, {"start_line": 22000, "end_line": 24000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "er_passthrough=False\n        ):\n            if transformer is None or transformer == \"passthrough\":\n                with _print_elapsed_time(\"Pipeline\", self._log_message(step_idx)):\n                    continue\n\n            if hasattr(memory, \"location\") and memory.location is None:\n                # we do not clone when caching is disabled to\n                # preserve backward compatibility\n                cloned_transformer = transformer\n            else:\n                cloned_transformer = clone(transformer)\n            # Fit or load from cache the current transformer\n            step_params = self._get_metadata_for_step(\n                step_idx=step_idx,\n                step_params=routed_params[name],\n                all_params=raw_params,\n            )\n\n            X, fitted_transformer = fit_transform_one_cached(\n                cloned_transformer,\n                X,\n                y,\n                weight=None,\n                message_clsname=\"Pipeline\",\n                message=self._log_message(step_idx),\n                params=step_params,\n            )\n            # Replace the transformer of the step with the fitted\n            # transformer. This is necessary when loading the transformer\n            # from the cache.\n            self.steps[step_idx] = (name, fitted_transformer)\n        return X\n\n    @_fit_context(\n        # estimators in Pipeline.steps are not validated yet\n        prefer_skip_nested_validation=False\n    )\n    def fit(self, X, y=None, **params):\n        \"\"\"Fit the model.\n\n        Fit all the transformers one after the other and sequentially transform the\n        data. Finally, fit the transformed data using the final estimator.\n\n        Parameters\n        ----------\n        X : iterable\n            Training data. Must fulfill input requirements of first step of the\n            pipeline.\n\n        y : iterable, default=None\n            Training targets. Must fulfill label requirements for all steps of\n            the pipeline.\n\n    "}], "retrieved_count": 10, "cost_time": 1.2143621444702148}
{"question": "Where does Scikit-learn store its algorithm implementations?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn stores its algorithm implementations across multiple specialized modules organized by algorithm type and functionality. Here's where the different algorithm implementations are located:\n\n1. **Linear Models (linear_model/)**:\n   - **LogisticRegression**: _logistic.py\n   - **LinearRegression**: _base.py\n   - **Ridge/Lasso/ElasticNet**: _ridge.py, _coordinate_descent.py\n   - **SGD**: _stochastic_gradient.py\n   - **Perceptron**: _perceptron.py\n   - **Passive Aggressive**: _passive_aggressive.py\n\n2. **Ensemble Methods (ensemble/)**:\n   - **Random Forest**: _forest.py\n   - **Gradient Boosting**: _gb.py\n   - **Histogram Gradient Boosting**: _hist_gradient_boosting/\n   - **AdaBoost**: _weight_boosting.py\n   - **Voting**: _voting.py\n   - **Stacking**: _stacking.py\n\n3. **Tree-Based Algorithms (tree/)**:\n   - **Decision Trees**: _tree.py\n   - **Extra Trees**: _tree.py\n   - **Tree Visualization**: export_graphviz\n   - **Tree Export**: export_text\n\n4. **Support Vector Machines (svm/)**:\n   - **SVC/SVR**: _classes.py\n   - **LinearSVC**: _classes.py\n   - **NuSVC/NuSVR**: _classes.py\n   - **OneClassSVM**: _classes.py\n   - **LibSVM Integration**: _libsvm/\n\n5. **Neural Networks (neural_network/)**:\n   - **MLP**: _multilayer_perceptron.py\n   - **BernoulliRBM**: _rbm.py\n\n6. **Clustering Algorithms (cluster/)**:\n   - **K-Means**: _kmeans.py\n   - **DBSCAN**: _dbscan.py\n   - **Agglomerative**: _agglomerative.py\n   - **Spectral**: _spectral.py\n   - **Mean Shift**: _mean_shift.py\n   - **OPTICS**: _optics.py\n   - **HDBSCAN**: _hdbscan.py\n\n7. **Dimensionality Reduction (decomposition/)**:\n   - **PCA**: _pca.py\n   - **NMF**: _nmf.py\n   - **FastICA**: _fastica.py\n   - **Kernel PCA**: _kernel_pca.py\n   - **Truncated SVD**: _truncated_svd.py\n   - **Dictionary Learning**: _dict_learning.py\n\n8. **Feature Selection (feature_selection/)**:\n   - **Variance Threshold**: _variance_threshold.py\n   - **SelectKBest**: _univariate_selection.py\n   - **RFE**: _rfe.py\n   - **SelectFromModel**: _from_model.py\n   - **Sequential Feature Selection**: _sequential.py\n\n9. **Preprocessing (preprocessing/)**:\n   - **StandardScaler**: _data.py\n   - **MinMaxScaler**: _data.py\n   - **RobustScaler**: _data.py\n   - **LabelEncoder**: _label.py\n   - **OneHotEncoder**: _encoders.py\n   - **PolynomialFeatures**: _polynomial.py\n\n10. **Neighbors (neighbors/)**:\n    - **KNeighborsClassifier/Regressor**: _classification.py, _regression.py\n    - **NearestNeighbors**: _unsupervised.py\n    - **Kernel Density**: _kde.py\n    - **KDTree/BallTree**: _binary_tree.py\n\n11. **Naive Bayes (naive_bayes/)**:\n    - **GaussianNB**: _gaussian.py\n    - **MultinomialNB**: _multinomial.py\n    - **BernoulliNB**: _bernoulli.py\n    - **ComplementNB**: _complement.py\n\n12. **Cross-Decomposition (cross_decomposition/)**:\n    - **PLS**: _pls.py\n    - **CCA**: _pls.py\n\n13. **Semi-Supervised Learning (semi_supervised/)**:\n    - **LabelPropagation**: _label_propagation.py\n    - **LabelSpreading**: _label_propagation.py\n\n14. **Isotonic Regression (isotonic/)**:\n    - **IsotonicRegression**: _isotonic.py\n\n15. **Kernel Approximation (kernel_approximation/)**:\n    - **RBFSampler**: _rbf.py\n    - **Nystroem**: _nystroem.py\n    - **PolynomialCountSketch**: _polynomial.py\n\n16. **Manifold Learning (manifold/)**:\n    - **MDS**: _mds.py\n    - **Isomap**: _isomap.py\n    - **Locally Linear Embedding**: _locally_linear.py\n    - **Spectral Embedding**: _spectral_embedding.py\n    - **TSNE**: _t_sne.py\n\n17. **Mixture Models (mixture/)**:\n    - **GaussianMixture**: _gaussian_mixture.py\n    - **BayesianGaussianMixture**: _bayesian_mixture.py\n\n18. **Covariance Estimation (covariance/)**:\n    - **EmpiricalCovariance**: _empirical_covariance.py\n    - **ShrunkCovariance**: _shrunk_covariance.py\n    - **EllipticEnvelope**: _robust_covariance.py\n\n19. **Outlier Detection (outlier_detection/)**:\n    - **IsolationForest**: _iforest.py\n    - **LocalOutlierFactor**: _lof.py\n    - **OneClassSVM**: Inherited from svm module\n\n20. **Random Projection (random_projection/)**:\n    - **GaussianRandomProjection**: random_projection.py\n    - **SparseRandomProjection**: random_projection.py", "score": null, "retrieved_content": [{"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "instance_generator.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/_test_common", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "sklearn.kernel_approximation import (\n    Nystroem,\n    PolynomialCountSketch,\n    RBFSampler,\n    SkewedChi2Sampler,\n)\nfrom sklearn.linear_model import (\n    ARDRegression,\n    BayesianRidge,\n    ElasticNet,\n    ElasticNetCV,\n    GammaRegressor,\n    HuberRegressor,\n    LarsCV,\n    Lasso,\n    LassoCV,\n    LassoLars,\n    LassoLarsCV,\n    LassoLarsIC,\n    LinearRegression,\n    LogisticRegression,\n    LogisticRegressionCV,\n    MultiTaskElasticNet,\n    MultiTaskElasticNetCV,\n    MultiTaskLasso,\n    MultiTaskLassoCV,\n    OrthogonalMatchingPursuitCV,\n    PassiveAggressiveClassifier,\n    PassiveAggressiveRegressor,\n    Perceptron,\n    PoissonRegressor,\n    QuantileRegressor,\n    RANSACRegressor,\n    Ridge,\n    RidgeClassifier,\n    SGDClassifier,\n    SGDOneClassSVM,\n    SGDRegressor,\n    TheilSenRegressor,\n    TweedieRegressor,\n)\nfrom sklearn.manifold import (\n    MDS,\n    TSNE,\n    Isomap,\n    LocallyLinearEmbedding,\n    SpectralEmbedding,\n)\nfrom sklearn.mixture import BayesianGaussianMixture, GaussianMixture\nfrom sklearn.model_selection import (\n    FixedThresholdClassifier,\n    GridSearchCV,\n    HalvingGridSearchCV,\n    HalvingRandomSearchCV,\n    RandomizedSearchCV,\n    TunedThresholdClassifierCV,\n)\nfrom sklearn.multiclass import (\n    OneVsOneClassifier,\n    OneVsRestClassifier,\n    OutputCodeClassifier,\n)\nfrom sklearn.multioutput import (\n    ClassifierChain,\n    MultiOutputClassifier,\n    MultiOutputRegressor,\n    RegressorChain,\n)\nfrom sklearn.neighbors import (\n    KernelDensity,\n    KNeighborsClassifier,\n    KNeighborsRegressor,\n    KNeighborsTransformer,\n    NeighborhoodComponentsAnalysis,\n    RadiusNeighborsTransformer,\n)\nfrom sklearn.neural_network import BernoulliRBM, MLPClassifier, MLPRegressor\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.preprocessing import (\n    KBinsDiscretizer,\n    OneHotEncoder,\n    SplineTransformer,\n    StandardScaler,\n    TargetEncoder,\n)\nfrom sklearn.random_projection import GaussianRandomProjection, SparseRandomP"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "instance_generator.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/_test_common", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  MiniBatchSparsePCA,\n    SparseCoder,\n    SparsePCA,\n    TruncatedSVD,\n)\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.ensemble import (\n    AdaBoostClassifier,\n    AdaBoostRegressor,\n    BaggingClassifier,\n    BaggingRegressor,\n    ExtraTreesClassifier,\n    ExtraTreesRegressor,\n    GradientBoostingClassifier,\n    GradientBoostingRegressor,\n    HistGradientBoostingClassifier,\n    HistGradientBoostingRegressor,\n    IsolationForest,\n    RandomForestClassifier,\n    RandomForestRegressor,\n    RandomTreesEmbedding,\n    StackingClassifier,\n    StackingRegressor,\n    VotingClassifier,\n    VotingRegressor,\n)\nfrom sklearn.exceptions import SkipTestWarning\nfrom sklearn.experimental import enable_halving_search_cv  # noqa: F401\nfrom sklearn.feature_selection import (\n    RFE,\n    RFECV,\n    SelectFdr,\n    SelectFromModel,\n    SelectKBest,\n    SequentialFeatureSelector,\n)\nfrom sklearn.frozen import FrozenEstimator\nfrom sklearn.kernel_approximation import (\n    Nystroem,\n    PolynomialCountSketch,\n    RBFSampler,\n    SkewedChi2Sampler,\n)\nfrom sklearn.linear_model import (\n    ARDRegression,\n    BayesianRidge,\n    ElasticNet,\n    ElasticNetCV,\n    GammaRegressor,\n    HuberRegressor,\n    LarsCV,\n    Lasso,\n    LassoCV,\n    LassoLars,\n    LassoLarsCV,\n    LassoLarsIC,\n    LinearRegression,\n    LogisticRegression,\n    LogisticRegressionCV,\n    MultiTaskElasticNet,\n    MultiTaskElasticNetCV,\n    MultiTaskLasso,\n    MultiTaskLassoCV,\n    OrthogonalMatchingPursuitCV,\n    PassiveAggressiveClassifier,\n    PassiveAggressiveRegressor,\n    Perceptron,\n    PoissonRegressor,\n    QuantileRegressor,\n    RANSACRegressor,\n    Ridge,\n    RidgeClassifier,\n    SGDClassifier,\n    SGDOneClassSVM,\n    SGDRegressor,\n    TheilSenRegressor,\n    TweedieRegressor,\n)\nfrom sklearn.manifold import (\n    MDS,\n    TSNE,\n    Isomap,\n    LocallyLinearEmbedding,\n    SpectralEmbedding,\n)\nfrom sklearn.mixture import BayesianGaussianMixture"}, {"start_line": 0, "end_line": 509, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/svm", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Support vector machine algorithms.\"\"\"\n\n# See http://scikit-learn.sourceforge.net/modules/svm.html for complete\n# documentation.\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nfrom sklearn.svm._bounds import l1_min_c\nfrom sklearn.svm._classes import (\n    SVC,\n    SVR,\n    LinearSVC,\n    LinearSVR,\n    NuSVC,\n    NuSVR,\n    OneClassSVM,\n)\n\n__all__ = [\n    \"SVC\",\n    \"SVR\",\n    \"LinearSVC\",\n    \"LinearSVR\",\n    \"NuSVC\",\n    \"NuSVR\",\n    \"OneClassSVM\",\n    \"l1_min_c\",\n]\n"}, {"start_line": 40000, "end_line": 42000, "belongs_to": {"file_name": "api_reference.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/doc", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     \"LinearSVR\",\n                    \"NuSVC\",\n                    \"NuSVR\",\n                    \"OneClassSVM\",\n                    \"SVC\",\n                    \"SVR\",\n                    \"l1_min_c\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.tree\": {\n        \"short_summary\": \"Decision trees.\",\n        \"description\": _get_guide(\"tree\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"DecisionTreeClassifier\",\n                    \"DecisionTreeRegressor\",\n                    \"ExtraTreeClassifier\",\n                    \"ExtraTreeRegressor\",\n                ],\n            },\n            {\n                \"title\": \"Exporting\",\n                \"autosummary\": [\"export_graphviz\", \"export_text\"],\n            },\n            {\n                \"title\": \"Plotting\",\n                \"autosummary\": [\"plot_tree\"],\n            },\n        ],\n    },\n    \"sklearn.utils\": {\n        \"short_summary\": \"Utilities.\",\n        \"description\": _get_guide(\"developers-utils\", is_developer=True),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"Bunch\",\n                    \"_safe_indexing\",\n                    \"as_float_array\",\n                    \"assert_all_finite\",\n                    \"deprecated\",\n                    \"estimator_html_repr\",\n                    \"gen_batches\",\n                    \"gen_even_slices\",\n                    \"indexable\",\n                    \"murmurhash3_32\",\n                    \"resample\",\n                    \"safe_mask\",\n                    \"safe_sqr\",\n                    \"shuffle\",\n                    \"Tags\",\n                    \"InputTags\",\n                    \"TargetTags\",\n                    \"ClassifierTags\",\n                    \"RegressorTags\",\n                    \"TransformerTags\",\n                    \"get_tags\",\n                ],\n            },\n            {\n                \"title\": \"Input and parameter v"}, {"start_line": 39000, "end_line": 41000, "belongs_to": {"file_name": "api_reference.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/doc", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "andom_projection\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"GaussianRandomProjection\",\n                    \"SparseRandomProjection\",\n                    \"johnson_lindenstrauss_min_dim\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.semi_supervised\": {\n        \"short_summary\": \"Semi-supervised learning.\",\n        \"description\": _get_guide(\"semi_supervised\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"LabelPropagation\",\n                    \"LabelSpreading\",\n                    \"SelfTrainingClassifier\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.svm\": {\n        \"short_summary\": \"Support vector machines.\",\n        \"description\": _get_guide(\"svm\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"LinearSVC\",\n                    \"LinearSVR\",\n                    \"NuSVC\",\n                    \"NuSVR\",\n                    \"OneClassSVM\",\n                    \"SVC\",\n                    \"SVR\",\n                    \"l1_min_c\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.tree\": {\n        \"short_summary\": \"Decision trees.\",\n        \"description\": _get_guide(\"tree\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"DecisionTreeClassifier\",\n                    \"DecisionTreeRegressor\",\n                    \"ExtraTreeClassifier\",\n                    \"ExtraTreeRegressor\",\n                ],\n            },\n            {\n                \"title\": \"Exporting\",\n                \"autosummary\": [\"export_graphviz\", \"export_text\"],\n            },\n            {\n                \"title\": \"Plotting\",\n                \"autosummary\": [\"plot_tree\"],\n            },\n        ],\n    },\n    \"sklearn.utils\": {\n        \"short_summary\": \"Utilities.\",\n      "}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "api_reference.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/doc", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"fastica\",\n                    \"non_negative_factorization\",\n                    \"sparse_encode\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.discriminant_analysis\": {\n        \"short_summary\": \"Discriminant analysis.\",\n        \"description\": _get_guide(\"lda_qda\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"LinearDiscriminantAnalysis\",\n                    \"QuadraticDiscriminantAnalysis\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.dummy\": {\n        \"short_summary\": \"Dummy estimators.\",\n        \"description\": _get_guide(\"model_evaluation\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\"DummyClassifier\", \"DummyRegressor\"],\n            },\n        ],\n    },\n    \"sklearn.ensemble\": {\n        \"short_summary\": \"Ensemble methods.\",\n        \"description\": _get_guide(\"ensemble\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"AdaBoostClassifier\",\n                    \"AdaBoostRegressor\",\n                    \"BaggingClassifier\",\n                    \"BaggingRegressor\",\n                    \"ExtraTreesClassifier\",\n                    \"ExtraTreesRegressor\",\n                    \"GradientBoostingClassifier\",\n                    \"GradientBoostingRegressor\",\n                    \"HistGradientBoostingClassifier\",\n                    \"HistGradientBoostingRegressor\",\n                    \"IsolationForest\",\n                    \"RandomForestClassifier\",\n                    \"RandomForestRegressor\",\n                    \"RandomTreesEmbedding\",\n                    \"StackingClassifier\",\n                    \"StackingRegressor\",\n                    \"VotingClassifier\",\n                    \"VotingRegressor\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.exceptions\": {\n        \"short_summary\": \"Exceptions and warnings.\",\n        \"description"}, {"start_line": 0, "end_line": 162, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/frozen", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nfrom sklearn.frozen._frozen import FrozenEstimator\n\n__all__ = [\"FrozenEstimator\"]\n"}, {"start_line": 35000, "end_line": 37000, "belongs_to": {"file_name": "api_reference.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/doc", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " {\n                \"title\": None,\n                \"autosummary\": [\n                    \"ClassifierChain\",\n                    \"MultiOutputClassifier\",\n                    \"MultiOutputRegressor\",\n                    \"RegressorChain\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.naive_bayes\": {\n        \"short_summary\": \"Naive Bayes.\",\n        \"description\": _get_guide(\"naive_bayes\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"BernoulliNB\",\n                    \"CategoricalNB\",\n                    \"ComplementNB\",\n                    \"GaussianNB\",\n                    \"MultinomialNB\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.neighbors\": {\n        \"short_summary\": \"Nearest neighbors.\",\n        \"description\": _get_guide(\"neighbors\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"BallTree\",\n                    \"KDTree\",\n                    \"KNeighborsClassifier\",\n                    \"KNeighborsRegressor\",\n                    \"KNeighborsTransformer\",\n                    \"KernelDensity\",\n                    \"LocalOutlierFactor\",\n                    \"NearestCentroid\",\n                    \"NearestNeighbors\",\n                    \"NeighborhoodComponentsAnalysis\",\n                    \"RadiusNeighborsClassifier\",\n                    \"RadiusNeighborsRegressor\",\n                    \"RadiusNeighborsTransformer\",\n                    \"kneighbors_graph\",\n                    \"radius_neighbors_graph\",\n                    \"sort_graph_by_row_values\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.neural_network\": {\n        \"short_summary\": \"Neural network models.\",\n        \"description\": _get_guide(\n            \"neural_networks_supervised\", \"neural_networks_unsupervised\"\n        ),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\"B"}, {"start_line": 36000, "end_line": 38000, "belongs_to": {"file_name": "api_reference.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/doc", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "          \"KDTree\",\n                    \"KNeighborsClassifier\",\n                    \"KNeighborsRegressor\",\n                    \"KNeighborsTransformer\",\n                    \"KernelDensity\",\n                    \"LocalOutlierFactor\",\n                    \"NearestCentroid\",\n                    \"NearestNeighbors\",\n                    \"NeighborhoodComponentsAnalysis\",\n                    \"RadiusNeighborsClassifier\",\n                    \"RadiusNeighborsRegressor\",\n                    \"RadiusNeighborsTransformer\",\n                    \"kneighbors_graph\",\n                    \"radius_neighbors_graph\",\n                    \"sort_graph_by_row_values\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.neural_network\": {\n        \"short_summary\": \"Neural network models.\",\n        \"description\": _get_guide(\n            \"neural_networks_supervised\", \"neural_networks_unsupervised\"\n        ),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\"BernoulliRBM\", \"MLPClassifier\", \"MLPRegressor\"],\n            },\n        ],\n    },\n    \"sklearn.pipeline\": {\n        \"short_summary\": \"Pipeline.\",\n        \"description\": _get_guide(\"combining_estimators\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"FeatureUnion\",\n                    \"Pipeline\",\n                    \"make_pipeline\",\n                    \"make_union\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.preprocessing\": {\n        \"short_summary\": \"Preprocessing and normalization.\",\n        \"description\": _get_guide(\"preprocessing\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"Binarizer\",\n                    \"FunctionTransformer\",\n                    \"KBinsDiscretizer\",\n                    \"KernelCenterer\",\n                    \"LabelBinarizer\",\n                    \"LabelEncoder\",\n                    \"MaxAbsScaler"}, {"start_line": 0, "end_line": 424, "belongs_to": {"file_name": "_distributor_init.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Distributor init file\n\nDistributors: you can add custom code here to support particular distributions\nof scikit-learn.\n\nFor example, this is a good place to put any checks for hardware requirements.\n\nThe scikit-learn standard source distribution will not put code in this file,\nso you can safely replace this file with your own version.\n\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n"}], "retrieved_count": 10, "cost_time": 1.2067289352416992}
{"question": "Where are Scikit-learn's built-in metrics defined?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn's built-in metrics are defined across multiple specialized modules within the metrics package. Here's where the different types of metrics are implemented:\n\n1. **Classification Metrics (metrics/_classification.py)**:\n   - **accuracy_score**: Basic accuracy calculation\n   - **balanced_accuracy_score**: Balanced accuracy for imbalanced datasets\n   - **precision_score, recall_score, f1_score**: Precision, recall, and F1-score\n   - **confusion_matrix**: Confusion matrix computation\n   - **roc_auc_score**: ROC AUC calculation\n   - **log_loss**: Logarithmic loss\n   - **matthews_corrcoef**: Matthews correlation coefficient\n   - **jaccard_score**: Jaccard similarity coefficient\n   - **hamming_loss**: Hamming loss for multilabel\n   - **hinge_loss**: Hinge loss for SVM-like classifiers\n\n2. **Regression Metrics (metrics/_regression.py)**:\n   - **mean_squared_error**: Mean squared error\n   - **mean_absolute_error**: Mean absolute error\n   - **r2_score**: R-squared coefficient of determination\n   - **explained_variance_score**: Explained variance score\n   - **max_error**: Maximum error\n   - **median_absolute_error**: Median absolute error\n   - **mean_absolute_percentage_error**: Mean absolute percentage error\n   - **root_mean_squared_error**: Root mean squared error\n   - **mean_poisson_deviance**: Poisson deviance\n   - **mean_gamma_deviance**: Gamma deviance\n\n3. **Clustering Metrics (metrics/cluster/)**:\n   - **Supervised Metrics (_supervised.py)**:\n     - **adjusted_rand_score**: Adjusted Rand index\n     - **mutual_info_score**: Mutual information score\n     - **homogeneity_score, completeness_score**: Homogeneity and completeness\n     - **v_measure_score**: V-measure score\n     - **fowlkes_mallows_score**: Fowlkes-Mallows score\n   - **Unsupervised Metrics (_unsupervised.py)**:\n     - **silhouette_score**: Silhouette coefficient\n     - **calinski_harabasz_score**: Calinski-Harabasz index\n     - **davies_bouldin_score**: Davies-Bouldin index\n\n4. **Ranking Metrics (metrics/_ranking.py)**:\n   - **average_precision_score**: Average precision score\n   - **coverage_error**: Coverage error\n   - **label_ranking_loss**: Label ranking loss\n   - **dcg_score, ndcg_score**: Discounted cumulative gain\n   - **label_ranking_average_precision_score**: Label ranking average precision\n\n5. **Distance Metrics (metrics/_dist_metrics.py)**:\n   - **DistanceMetric**: Base class for distance metrics\n   - **Euclidean distance**: L2 norm distance\n   - **Manhattan distance**: L1 norm distance\n   - **Chebyshev distance**: L norm distance\n   - **Minkowski distance**: Generalized Minkowski distance\n   - **Cosine distance**: Cosine similarity distance\n   - **Hamming distance**: Hamming distance for categorical data\n\n6. **Pairwise Metrics (metrics/_pairwise.py)**:\n   - **pairwise_distances**: Compute pairwise distances between samples\n   - **pairwise_kernels**: Compute pairwise kernels between samples\n   - **cosine_similarity**: Cosine similarity matrix\n   - **linear_kernel**: Linear kernel matrix\n   - **rbf_kernel**: Radial basis function kernel\n   - **polynomial_kernel**: Polynomial kernel\n\n7. **Scorer Objects (metrics/_scorer.py)**:\n   - **make_scorer**: Create custom scorer objects\n   - **get_scorer**: Get scorer by name\n   - **check_scoring**: Check and validate scoring parameter\n   - **get_scorer_names**: Get list of available scorer names\n   - **Built-in scorers**: Pre-defined scorer objects for all metrics\n\n8. **Plotting Functions (metrics/_plot/)**:\n   - **ConfusionMatrixDisplay**: Confusion matrix visualization\n   - **RocCurveDisplay**: ROC curve plotting\n   - **PrecisionRecallDisplay**: Precision-recall curve plotting\n   - **DetCurveDisplay**: DET curve plotting\n   - **PredictionErrorDisplay**: Prediction error plotting\n\n9. **Specialized Metrics**:\n   - **Biclustering metrics**: consensus_score for biclustering evaluation\n   - **Time series metrics**: Specialized metrics for time series data\n   - **Multilabel metrics**: Metrics specifically for multilabel classification\n   - **Multioutput metrics**: Metrics for multioutput regression/classification\n\n10. **Utility Functions**:\n    - **Metric aggregation**: Functions for combining multiple metrics\n    - **Metric validation**: Input validation for metric functions\n    - **Metric computation**: Core computation functions for metrics\n    - **Metric documentation**: Comprehensive documentation for each metric\n    - **Metric testing**: Extensive test suites for all metrics", "score": null, "retrieved_content": [{"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "test_public_functions.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "loss\",\n    \"sklearn.metrics.homogeneity_score\",\n    \"sklearn.metrics.jaccard_score\",\n    \"sklearn.metrics.label_ranking_average_precision_score\",\n    \"sklearn.metrics.label_ranking_loss\",\n    \"sklearn.metrics.log_loss\",\n    \"sklearn.metrics.make_scorer\",\n    \"sklearn.metrics.matthews_corrcoef\",\n    \"sklearn.metrics.max_error\",\n    \"sklearn.metrics.mean_absolute_error\",\n    \"sklearn.metrics.mean_absolute_percentage_error\",\n    \"sklearn.metrics.mean_gamma_deviance\",\n    \"sklearn.metrics.mean_pinball_loss\",\n    \"sklearn.metrics.mean_poisson_deviance\",\n    \"sklearn.metrics.mean_squared_error\",\n    \"sklearn.metrics.mean_squared_log_error\",\n    \"sklearn.metrics.mean_tweedie_deviance\",\n    \"sklearn.metrics.median_absolute_error\",\n    \"sklearn.metrics.multilabel_confusion_matrix\",\n    \"sklearn.metrics.mutual_info_score\",\n    \"sklearn.metrics.ndcg_score\",\n    \"sklearn.metrics.pair_confusion_matrix\",\n    \"sklearn.metrics.adjusted_rand_score\",\n    \"sklearn.metrics.pairwise.additive_chi2_kernel\",\n    \"sklearn.metrics.pairwise.chi2_kernel\",\n    \"sklearn.metrics.pairwise.cosine_distances\",\n    \"sklearn.metrics.pairwise.cosine_similarity\",\n    \"sklearn.metrics.pairwise.euclidean_distances\",\n    \"sklearn.metrics.pairwise.haversine_distances\",\n    \"sklearn.metrics.pairwise.laplacian_kernel\",\n    \"sklearn.metrics.pairwise.linear_kernel\",\n    \"sklearn.metrics.pairwise.manhattan_distances\",\n    \"sklearn.metrics.pairwise.nan_euclidean_distances\",\n    \"sklearn.metrics.pairwise.paired_cosine_distances\",\n    \"sklearn.metrics.pairwise.paired_distances\",\n    \"sklearn.metrics.pairwise.paired_euclidean_distances\",\n    \"sklearn.metrics.pairwise.paired_manhattan_distances\",\n    \"sklearn.metrics.pairwise.pairwise_distances_argmin_min\",\n    \"sklearn.metrics.pairwise.pairwise_kernels\",\n    \"sklearn.metrics.pairwise.polynomial_kernel\",\n    \"sklearn.metrics.pairwise.rbf_kernel\",\n    \"sklearn.metrics.pairwise.sigmoid_kernel\",\n    \"sklearn.metrics.pairwise_distances\",\n    \"sklearn.metrics.pairwise_dist"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "test_public_functions.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "metrics.cluster.contingency_matrix\",\n    \"sklearn.metrics.cluster.fowlkes_mallows_score\",\n    \"sklearn.metrics.cluster.homogeneity_completeness_v_measure\",\n    \"sklearn.metrics.cluster.normalized_mutual_info_score\",\n    \"sklearn.metrics.cluster.silhouette_samples\",\n    \"sklearn.metrics.cluster.silhouette_score\",\n    \"sklearn.metrics.cohen_kappa_score\",\n    \"sklearn.metrics.confusion_matrix\",\n    \"sklearn.metrics.consensus_score\",\n    \"sklearn.metrics.coverage_error\",\n    \"sklearn.metrics.d2_absolute_error_score\",\n    \"sklearn.metrics.d2_brier_score\",\n    \"sklearn.metrics.d2_log_loss_score\",\n    \"sklearn.metrics.d2_pinball_score\",\n    \"sklearn.metrics.d2_tweedie_score\",\n    \"sklearn.metrics.davies_bouldin_score\",\n    \"sklearn.metrics.dcg_score\",\n    \"sklearn.metrics.det_curve\",\n    \"sklearn.metrics.explained_variance_score\",\n    \"sklearn.metrics.f1_score\",\n    \"sklearn.metrics.fbeta_score\",\n    \"sklearn.metrics.get_scorer\",\n    \"sklearn.metrics.hamming_loss\",\n    \"sklearn.metrics.hinge_loss\",\n    \"sklearn.metrics.homogeneity_score\",\n    \"sklearn.metrics.jaccard_score\",\n    \"sklearn.metrics.label_ranking_average_precision_score\",\n    \"sklearn.metrics.label_ranking_loss\",\n    \"sklearn.metrics.log_loss\",\n    \"sklearn.metrics.make_scorer\",\n    \"sklearn.metrics.matthews_corrcoef\",\n    \"sklearn.metrics.max_error\",\n    \"sklearn.metrics.mean_absolute_error\",\n    \"sklearn.metrics.mean_absolute_percentage_error\",\n    \"sklearn.metrics.mean_gamma_deviance\",\n    \"sklearn.metrics.mean_pinball_loss\",\n    \"sklearn.metrics.mean_poisson_deviance\",\n    \"sklearn.metrics.mean_squared_error\",\n    \"sklearn.metrics.mean_squared_log_error\",\n    \"sklearn.metrics.mean_tweedie_deviance\",\n    \"sklearn.metrics.median_absolute_error\",\n    \"sklearn.metrics.multilabel_confusion_matrix\",\n    \"sklearn.metrics.mutual_info_score\",\n    \"sklearn.metrics.ndcg_score\",\n    \"sklearn.metrics.pair_confusion_matrix\",\n    \"sklearn.metrics.adjusted_rand_score\",\n    \"sklearn.metrics.pairwise.additive_chi2_kernel\",\n"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "test_public_functions.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    \"sklearn.metrics.pairwise.chi2_kernel\",\n    \"sklearn.metrics.pairwise.cosine_distances\",\n    \"sklearn.metrics.pairwise.cosine_similarity\",\n    \"sklearn.metrics.pairwise.euclidean_distances\",\n    \"sklearn.metrics.pairwise.haversine_distances\",\n    \"sklearn.metrics.pairwise.laplacian_kernel\",\n    \"sklearn.metrics.pairwise.linear_kernel\",\n    \"sklearn.metrics.pairwise.manhattan_distances\",\n    \"sklearn.metrics.pairwise.nan_euclidean_distances\",\n    \"sklearn.metrics.pairwise.paired_cosine_distances\",\n    \"sklearn.metrics.pairwise.paired_distances\",\n    \"sklearn.metrics.pairwise.paired_euclidean_distances\",\n    \"sklearn.metrics.pairwise.paired_manhattan_distances\",\n    \"sklearn.metrics.pairwise.pairwise_distances_argmin_min\",\n    \"sklearn.metrics.pairwise.pairwise_kernels\",\n    \"sklearn.metrics.pairwise.polynomial_kernel\",\n    \"sklearn.metrics.pairwise.rbf_kernel\",\n    \"sklearn.metrics.pairwise.sigmoid_kernel\",\n    \"sklearn.metrics.pairwise_distances\",\n    \"sklearn.metrics.pairwise_distances_argmin\",\n    \"sklearn.metrics.pairwise_distances_chunked\",\n    \"sklearn.metrics.precision_recall_curve\",\n    \"sklearn.metrics.precision_recall_fscore_support\",\n    \"sklearn.metrics.precision_score\",\n    \"sklearn.metrics.r2_score\",\n    \"sklearn.metrics.rand_score\",\n    \"sklearn.metrics.recall_score\",\n    \"sklearn.metrics.roc_auc_score\",\n    \"sklearn.metrics.roc_curve\",\n    \"sklearn.metrics.root_mean_squared_error\",\n    \"sklearn.metrics.root_mean_squared_log_error\",\n    \"sklearn.metrics.top_k_accuracy_score\",\n    \"sklearn.metrics.v_measure_score\",\n    \"sklearn.metrics.zero_one_loss\",\n    \"sklearn.model_selection.cross_val_predict\",\n    \"sklearn.model_selection.cross_val_score\",\n    \"sklearn.model_selection.cross_validate\",\n    \"sklearn.model_selection.learning_curve\",\n    \"sklearn.model_selection.permutation_test_score\",\n    \"sklearn.model_selection.train_test_split\",\n    \"sklearn.model_selection.validation_curve\",\n    \"sklearn.neighbors.kneighbors_graph\",\n    \"sklearn.neighbors.ra"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/metrics", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ".metrics._plot.regression import PredictionErrorDisplay\nfrom sklearn.metrics._plot.roc_curve import RocCurveDisplay\nfrom sklearn.metrics._ranking import (\n    auc,\n    average_precision_score,\n    coverage_error,\n    dcg_score,\n    det_curve,\n    label_ranking_average_precision_score,\n    label_ranking_loss,\n    ndcg_score,\n    precision_recall_curve,\n    roc_auc_score,\n    roc_curve,\n    top_k_accuracy_score,\n)\nfrom sklearn.metrics._regression import (\n    d2_absolute_error_score,\n    d2_pinball_score,\n    d2_tweedie_score,\n    explained_variance_score,\n    max_error,\n    mean_absolute_error,\n    mean_absolute_percentage_error,\n    mean_gamma_deviance,\n    mean_pinball_loss,\n    mean_poisson_deviance,\n    mean_squared_error,\n    mean_squared_log_error,\n    mean_tweedie_deviance,\n    median_absolute_error,\n    r2_score,\n    root_mean_squared_error,\n    root_mean_squared_log_error,\n)\nfrom sklearn.metrics._scorer import (\n    check_scoring,\n    get_scorer,\n    get_scorer_names,\n    make_scorer,\n)\nfrom sklearn.metrics.cluster import (\n    adjusted_mutual_info_score,\n    adjusted_rand_score,\n    calinski_harabasz_score,\n    completeness_score,\n    consensus_score,\n    davies_bouldin_score,\n    fowlkes_mallows_score,\n    homogeneity_completeness_v_measure,\n    homogeneity_score,\n    mutual_info_score,\n    normalized_mutual_info_score,\n    pair_confusion_matrix,\n    rand_score,\n    silhouette_samples,\n    silhouette_score,\n    v_measure_score,\n)\nfrom sklearn.metrics.pairwise import (\n    euclidean_distances,\n    nan_euclidean_distances,\n    pairwise_distances,\n    pairwise_distances_argmin,\n    pairwise_distances_argmin_min,\n    pairwise_distances_chunked,\n    pairwise_kernels,\n)\n\n__all__ = [\n    \"ConfusionMatrixDisplay\",\n    \"DetCurveDisplay\",\n    \"DistanceMetric\",\n    \"PrecisionRecallDisplay\",\n    \"PredictionErrorDisplay\",\n    \"RocCurveDisplay\",\n    \"accuracy_score\",\n    \"adjusted_mutual_info_score\",\n    \"adjusted_rand_score\",\n    \"auc\",\n    \"average_precision_score\",\n "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/metrics", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "scorer,\n)\nfrom sklearn.metrics.cluster import (\n    adjusted_mutual_info_score,\n    adjusted_rand_score,\n    calinski_harabasz_score,\n    completeness_score,\n    consensus_score,\n    davies_bouldin_score,\n    fowlkes_mallows_score,\n    homogeneity_completeness_v_measure,\n    homogeneity_score,\n    mutual_info_score,\n    normalized_mutual_info_score,\n    pair_confusion_matrix,\n    rand_score,\n    silhouette_samples,\n    silhouette_score,\n    v_measure_score,\n)\nfrom sklearn.metrics.pairwise import (\n    euclidean_distances,\n    nan_euclidean_distances,\n    pairwise_distances,\n    pairwise_distances_argmin,\n    pairwise_distances_argmin_min,\n    pairwise_distances_chunked,\n    pairwise_kernels,\n)\n\n__all__ = [\n    \"ConfusionMatrixDisplay\",\n    \"DetCurveDisplay\",\n    \"DistanceMetric\",\n    \"PrecisionRecallDisplay\",\n    \"PredictionErrorDisplay\",\n    \"RocCurveDisplay\",\n    \"accuracy_score\",\n    \"adjusted_mutual_info_score\",\n    \"adjusted_rand_score\",\n    \"auc\",\n    \"average_precision_score\",\n    \"balanced_accuracy_score\",\n    \"brier_score_loss\",\n    \"calinski_harabasz_score\",\n    \"check_scoring\",\n    \"class_likelihood_ratios\",\n    \"classification_report\",\n    \"cluster\",\n    \"cohen_kappa_score\",\n    \"completeness_score\",\n    \"confusion_matrix\",\n    \"consensus_score\",\n    \"coverage_error\",\n    \"d2_absolute_error_score\",\n    \"d2_brier_score\",\n    \"d2_log_loss_score\",\n    \"d2_pinball_score\",\n    \"d2_tweedie_score\",\n    \"davies_bouldin_score\",\n    \"dcg_score\",\n    \"det_curve\",\n    \"euclidean_distances\",\n    \"explained_variance_score\",\n    \"f1_score\",\n    \"fbeta_score\",\n    \"fowlkes_mallows_score\",\n    \"get_scorer\",\n    \"get_scorer_names\",\n    \"hamming_loss\",\n    \"hinge_loss\",\n    \"homogeneity_completeness_v_measure\",\n    \"homogeneity_score\",\n    \"jaccard_score\",\n    \"label_ranking_average_precision_score\",\n    \"label_ranking_loss\",\n    \"log_loss\",\n    \"make_scorer\",\n    \"matthews_corrcoef\",\n    \"max_error\",\n    \"mean_absolute_error\",\n    \"mean_absolute_percentage_error\",\n    \"me"}, {"start_line": 3000, "end_line": 4890, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/metrics", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "   \"balanced_accuracy_score\",\n    \"brier_score_loss\",\n    \"calinski_harabasz_score\",\n    \"check_scoring\",\n    \"class_likelihood_ratios\",\n    \"classification_report\",\n    \"cluster\",\n    \"cohen_kappa_score\",\n    \"completeness_score\",\n    \"confusion_matrix\",\n    \"consensus_score\",\n    \"coverage_error\",\n    \"d2_absolute_error_score\",\n    \"d2_brier_score\",\n    \"d2_log_loss_score\",\n    \"d2_pinball_score\",\n    \"d2_tweedie_score\",\n    \"davies_bouldin_score\",\n    \"dcg_score\",\n    \"det_curve\",\n    \"euclidean_distances\",\n    \"explained_variance_score\",\n    \"f1_score\",\n    \"fbeta_score\",\n    \"fowlkes_mallows_score\",\n    \"get_scorer\",\n    \"get_scorer_names\",\n    \"hamming_loss\",\n    \"hinge_loss\",\n    \"homogeneity_completeness_v_measure\",\n    \"homogeneity_score\",\n    \"jaccard_score\",\n    \"label_ranking_average_precision_score\",\n    \"label_ranking_loss\",\n    \"log_loss\",\n    \"make_scorer\",\n    \"matthews_corrcoef\",\n    \"max_error\",\n    \"mean_absolute_error\",\n    \"mean_absolute_percentage_error\",\n    \"mean_gamma_deviance\",\n    \"mean_pinball_loss\",\n    \"mean_poisson_deviance\",\n    \"mean_squared_error\",\n    \"mean_squared_log_error\",\n    \"mean_tweedie_deviance\",\n    \"median_absolute_error\",\n    \"multilabel_confusion_matrix\",\n    \"mutual_info_score\",\n    \"nan_euclidean_distances\",\n    \"ndcg_score\",\n    \"normalized_mutual_info_score\",\n    \"pair_confusion_matrix\",\n    \"pairwise_distances\",\n    \"pairwise_distances_argmin\",\n    \"pairwise_distances_argmin_min\",\n    \"pairwise_distances_chunked\",\n    \"pairwise_kernels\",\n    \"precision_recall_curve\",\n    \"precision_recall_fscore_support\",\n    \"precision_score\",\n    \"r2_score\",\n    \"rand_score\",\n    \"recall_score\",\n    \"roc_auc_score\",\n    \"roc_curve\",\n    \"root_mean_squared_error\",\n    \"root_mean_squared_log_error\",\n    \"silhouette_samples\",\n    \"silhouette_score\",\n    \"top_k_accuracy_score\",\n    \"v_measure_score\",\n    \"zero_one_loss\",\n]\n"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "test_common.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/metrics/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ared_log_error\": mean_squared_log_error,\n    \"mean_pinball_loss\": mean_pinball_loss,\n    \"median_absolute_error\": median_absolute_error,\n    \"mean_absolute_percentage_error\": mean_absolute_percentage_error,\n    \"explained_variance_score\": explained_variance_score,\n    \"r2_score\": partial(r2_score, multioutput=\"variance_weighted\"),\n    \"root_mean_squared_error\": root_mean_squared_error,\n    \"root_mean_squared_log_error\": root_mean_squared_log_error,\n    \"mean_normal_deviance\": partial(mean_tweedie_deviance, power=0),\n    \"mean_poisson_deviance\": mean_poisson_deviance,\n    \"mean_gamma_deviance\": mean_gamma_deviance,\n    \"mean_compound_poisson_deviance\": partial(mean_tweedie_deviance, power=1.4),\n    \"d2_tweedie_score\": partial(d2_tweedie_score, power=1.4),\n    \"d2_pinball_score\": d2_pinball_score,\n    \"d2_absolute_error_score\": d2_absolute_error_score,\n}\n\nCLASSIFICATION_METRICS = {\n    \"accuracy_score\": accuracy_score,\n    \"balanced_accuracy_score\": balanced_accuracy_score,\n    \"adjusted_balanced_accuracy_score\": partial(balanced_accuracy_score, adjusted=True),\n    \"unnormalized_accuracy_score\": partial(accuracy_score, normalize=False),\n    # `confusion_matrix` returns absolute values and hence behaves unnormalized\n    # . Naming it with an unnormalized_ prefix is necessary for this module to\n    # skip sample_weight scaling checks which will fail for unnormalized\n    # metrics.\n    \"unnormalized_confusion_matrix\": confusion_matrix,\n    \"normalized_confusion_matrix\": lambda *args, **kwargs: (\n        confusion_matrix(*args, **kwargs).astype(\"float\")\n        / confusion_matrix(*args, **kwargs).sum(axis=1)[:, np.newaxis]\n    ),\n    \"unnormalized_multilabel_confusion_matrix\": multilabel_confusion_matrix,\n    \"unnormalized_multilabel_confusion_matrix_sample\": partial(\n        multilabel_confusion_matrix, samplewise=True\n    ),\n    \"hamming_loss\": hamming_loss,\n    \"zero_one_loss\": zero_one_loss,\n    \"unnormalized_zero_one_loss\": partial(zero_one_loss, normalize=False),\n  "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/metrics", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Score functions, performance metrics, pairwise metrics and distance computations.\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nfrom sklearn.metrics import cluster\nfrom sklearn.metrics._classification import (\n    accuracy_score,\n    balanced_accuracy_score,\n    brier_score_loss,\n    class_likelihood_ratios,\n    classification_report,\n    cohen_kappa_score,\n    confusion_matrix,\n    d2_brier_score,\n    d2_log_loss_score,\n    f1_score,\n    fbeta_score,\n    hamming_loss,\n    hinge_loss,\n    jaccard_score,\n    log_loss,\n    matthews_corrcoef,\n    multilabel_confusion_matrix,\n    precision_recall_fscore_support,\n    precision_score,\n    recall_score,\n    zero_one_loss,\n)\nfrom sklearn.metrics._dist_metrics import DistanceMetric\nfrom sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\nfrom sklearn.metrics._plot.det_curve import DetCurveDisplay\nfrom sklearn.metrics._plot.precision_recall_curve import PrecisionRecallDisplay\nfrom sklearn.metrics._plot.regression import PredictionErrorDisplay\nfrom sklearn.metrics._plot.roc_curve import RocCurveDisplay\nfrom sklearn.metrics._ranking import (\n    auc,\n    average_precision_score,\n    coverage_error,\n    dcg_score,\n    det_curve,\n    label_ranking_average_precision_score,\n    label_ranking_loss,\n    ndcg_score,\n    precision_recall_curve,\n    roc_auc_score,\n    roc_curve,\n    top_k_accuracy_score,\n)\nfrom sklearn.metrics._regression import (\n    d2_absolute_error_score,\n    d2_pinball_score,\n    d2_tweedie_score,\n    explained_variance_score,\n    max_error,\n    mean_absolute_error,\n    mean_absolute_percentage_error,\n    mean_gamma_deviance,\n    mean_pinball_loss,\n    mean_poisson_deviance,\n    mean_squared_error,\n    mean_squared_log_error,\n    mean_tweedie_deviance,\n    median_absolute_error,\n    r2_score,\n    root_mean_squared_error,\n    root_mean_squared_log_error,\n)\nfrom sklearn.metrics._scorer import (\n    check_scoring,\n    get_scorer,\n    get_scorer_names,\n    make_"}, {"start_line": 29000, "end_line": 31000, "belongs_to": {"file_name": "_scorer.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/metrics", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "utual_info_score)\nadjusted_mutual_info_scorer = make_scorer(adjusted_mutual_info_score)\nnormalized_mutual_info_scorer = make_scorer(normalized_mutual_info_score)\nfowlkes_mallows_scorer = make_scorer(fowlkes_mallows_score)\n\n\n_SCORERS = dict(\n    explained_variance=explained_variance_scorer,\n    r2=r2_scorer,\n    neg_max_error=neg_max_error_scorer,\n    matthews_corrcoef=matthews_corrcoef_scorer,\n    neg_median_absolute_error=neg_median_absolute_error_scorer,\n    neg_mean_absolute_error=neg_mean_absolute_error_scorer,\n    neg_mean_absolute_percentage_error=neg_mean_absolute_percentage_error_scorer,\n    neg_mean_squared_error=neg_mean_squared_error_scorer,\n    neg_mean_squared_log_error=neg_mean_squared_log_error_scorer,\n    neg_root_mean_squared_error=neg_root_mean_squared_error_scorer,\n    neg_root_mean_squared_log_error=neg_root_mean_squared_log_error_scorer,\n    neg_mean_poisson_deviance=neg_mean_poisson_deviance_scorer,\n    neg_mean_gamma_deviance=neg_mean_gamma_deviance_scorer,\n    d2_absolute_error_score=d2_absolute_error_scorer,\n    accuracy=accuracy_scorer,\n    top_k_accuracy=top_k_accuracy_scorer,\n    roc_auc=roc_auc_scorer,\n    roc_auc_ovr=roc_auc_ovr_scorer,\n    roc_auc_ovo=roc_auc_ovo_scorer,\n    roc_auc_ovr_weighted=roc_auc_ovr_weighted_scorer,\n    roc_auc_ovo_weighted=roc_auc_ovo_weighted_scorer,\n    balanced_accuracy=balanced_accuracy_scorer,\n    average_precision=average_precision_scorer,\n    neg_log_loss=neg_log_loss_scorer,\n    neg_brier_score=neg_brier_score_scorer,\n    positive_likelihood_ratio=positive_likelihood_ratio_scorer,\n    neg_negative_likelihood_ratio=neg_negative_likelihood_ratio_scorer,\n    # Cluster metrics that use supervised evaluation\n    adjusted_rand_score=adjusted_rand_scorer,\n    rand_score=rand_scorer,\n    homogeneity_score=homogeneity_scorer,\n    completeness_score=completeness_scorer,\n    v_measure_score=v_measure_scorer,\n    mutual_info_score=mutual_info_scorer,\n    adjusted_mutual_info_score=adjusted_mutual_info_scorer,\n    "}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "test_public_functions.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "increasing\",\n    \"sklearn.isotonic.isotonic_regression\",\n    \"sklearn.linear_model.enet_path\",\n    \"sklearn.linear_model.lars_path\",\n    \"sklearn.linear_model.lars_path_gram\",\n    \"sklearn.linear_model.lasso_path\",\n    \"sklearn.linear_model.orthogonal_mp\",\n    \"sklearn.linear_model.orthogonal_mp_gram\",\n    \"sklearn.linear_model.ridge_regression\",\n    \"sklearn.manifold.locally_linear_embedding\",\n    \"sklearn.manifold.smacof\",\n    \"sklearn.manifold.spectral_embedding\",\n    \"sklearn.manifold.trustworthiness\",\n    \"sklearn.metrics.accuracy_score\",\n    \"sklearn.metrics.auc\",\n    \"sklearn.metrics.average_precision_score\",\n    \"sklearn.metrics.balanced_accuracy_score\",\n    \"sklearn.metrics.brier_score_loss\",\n    \"sklearn.metrics.calinski_harabasz_score\",\n    \"sklearn.metrics.check_scoring\",\n    \"sklearn.metrics.completeness_score\",\n    \"sklearn.metrics.class_likelihood_ratios\",\n    \"sklearn.metrics.classification_report\",\n    \"sklearn.metrics.cluster.adjusted_mutual_info_score\",\n    \"sklearn.metrics.cluster.contingency_matrix\",\n    \"sklearn.metrics.cluster.fowlkes_mallows_score\",\n    \"sklearn.metrics.cluster.homogeneity_completeness_v_measure\",\n    \"sklearn.metrics.cluster.normalized_mutual_info_score\",\n    \"sklearn.metrics.cluster.silhouette_samples\",\n    \"sklearn.metrics.cluster.silhouette_score\",\n    \"sklearn.metrics.cohen_kappa_score\",\n    \"sklearn.metrics.confusion_matrix\",\n    \"sklearn.metrics.consensus_score\",\n    \"sklearn.metrics.coverage_error\",\n    \"sklearn.metrics.d2_absolute_error_score\",\n    \"sklearn.metrics.d2_brier_score\",\n    \"sklearn.metrics.d2_log_loss_score\",\n    \"sklearn.metrics.d2_pinball_score\",\n    \"sklearn.metrics.d2_tweedie_score\",\n    \"sklearn.metrics.davies_bouldin_score\",\n    \"sklearn.metrics.dcg_score\",\n    \"sklearn.metrics.det_curve\",\n    \"sklearn.metrics.explained_variance_score\",\n    \"sklearn.metrics.f1_score\",\n    \"sklearn.metrics.fbeta_score\",\n    \"sklearn.metrics.get_scorer\",\n    \"sklearn.metrics.hamming_loss\",\n    \"sklearn.metrics.hinge_"}], "retrieved_count": 10, "cost_time": 1.204033374786377}
{"question": "Why does Scikit-learn implement a separate validation module for input checking instead of embedding validation in each estimator?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn implements a separate validation module for input checking instead of embedding validation in each estimator for several fundamental design reasons that enhance maintainability, consistency, and performance:\n\n1. **Code Reusability and DRY Principle**:\n   - **Centralized Logic**: Common validation logic is implemented once and reused\n   - **Reduced Duplication**: Eliminates code duplication across all estimators\n   - **Consistent Behavior**: All estimators use the same validation rules\n   - **Maintenance Efficiency**: Changes to validation logic only need to be made in one place\n   - **Bug Prevention**: Reduces risk of inconsistent validation across estimators\n\n2. **Consistency and Standardization**:\n   - **Unified Validation**: All estimators follow the same validation patterns\n   - **Standardized Error Messages**: Consistent error messages across all estimators\n   - **Uniform Behavior**: Same validation behavior regardless of estimator type\n   - **Predictable Interface**: Users can expect consistent validation behavior\n   - **API Consistency**: Validation follows the same patterns as other scikit-learn utilities\n\n3. **Performance Optimization**:\n   - **Optimized Implementation**: Validation functions are highly optimized\n   - **Efficient Algorithms**: Uses the most efficient validation algorithms\n   - **Memory Management**: Optimized memory usage for validation operations\n   - **Caching**: Can implement caching strategies for repeated validations\n   - **Parallel Processing**: Can be optimized for parallel validation when needed\n\n4. **Maintainability and Evolution**:\n   - **Centralized Updates**: New validation features can be added centrally\n   - **Bug Fixes**: Validation bugs are fixed once and benefit all estimators\n   - **Feature Addition**: New validation capabilities can be added systematically\n   - **Backward Compatibility**: Easier to maintain backward compatibility\n   - **Version Management**: Validation logic can be versioned independently\n\n5. **Testing and Quality Assurance**:\n   - **Comprehensive Testing**: Validation functions can be thoroughly tested\n   - **Edge Case Coverage**: All edge cases can be tested in one place\n   - **Regression Testing**: Easier to ensure validation behavior doesn't regress\n   - **Performance Testing**: Can benchmark validation performance independently\n   - **Integration Testing**: Can test validation integration with all estimators\n\n6. **Flexibility and Customization**:\n   - **Configurable Validation**: Validation behavior can be customized per estimator\n   - **Parameter Control**: Estimators can control which validations to apply\n   - **Skip Options**: Can skip validation for performance-critical operations\n   - **Custom Validators**: Can add custom validation functions to the module\n   - **Validation Levels**: Different levels of validation can be applied\n\n7. **Error Handling and Debugging**:\n   - **Centralized Error Handling**: Consistent error handling across all estimators\n   - **Detailed Error Messages**: Rich error messages with helpful suggestions\n   - **Debugging Support**: Better debugging information for validation issues\n   - **Error Categorization**: Systematic categorization of validation errors\n   - **Documentation Links**: Error messages can link to relevant documentation\n\n8. **Integration and Interoperability**:\n   - **Third-Party Compatibility**: Third-party estimators can use the same validation\n   - **Plugin Architecture**: Easy to extend validation for new data types\n   - **Framework Integration**: Can integrate with other validation frameworks\n   - **Standard Compliance**: Can ensure compliance with data validation standards\n   - **Interoperability**: Works with various data formats and sources\n\n9. **Educational and Documentation Benefits**:\n   - **Clear Documentation**: Validation logic is well-documented in one place\n   - **Learning Resource**: Users can learn validation patterns from the module\n   - **Best Practices**: Demonstrates best practices for data validation\n   - **Examples**: Provides clear examples of validation usage\n   - **Community Understanding**: Easier for community to understand and contribute\n\n10. **Future-Proofing and Extensibility**:\n    - **API Evolution**: Validation API can evolve independently of estimators\n    - **New Data Types**: Can add support for new data types centrally\n    - **Advanced Features**: Can add advanced validation features systematically\n    - **Performance Improvements**: Can optimize validation performance independently\n    - **Standards Compliance**: Can ensure compliance with evolving data standards", "score": null, "retrieved_content": [{"start_line": 45000, "end_line": 47000, "belongs_to": {"file_name": "validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "eck_X_y(\n    X,\n    y,\n    accept_sparse=False,\n    *,\n    accept_large_sparse=True,\n    dtype=\"numeric\",\n    order=None,\n    copy=False,\n    force_writeable=False,\n    force_all_finite=\"deprecated\",\n    ensure_all_finite=None,\n    ensure_2d=True,\n    allow_nd=False,\n    multi_output=False,\n    ensure_min_samples=1,\n    ensure_min_features=1,\n    y_numeric=False,\n    estimator=None,\n):\n    \"\"\"Input validation for standard estimators.\n\n    Checks X and y for consistent length, enforces X to be 2D and y 1D. By\n    default, X is checked to be non-empty and containing only finite values.\n    Standard input checks are also applied to y, such as checking that y\n    does not have np.nan or np.inf targets. For multi-label y, set\n    multi_output=True to allow 2D and sparse y. If the dtype of X is\n    object, attempt converting to float, raising on failure.\n\n    Parameters\n    ----------\n    X : {ndarray, list, sparse matrix}\n        Input data.\n\n    y : {ndarray, list, sparse matrix}\n        Labels.\n\n    accept_sparse : str, bool or list of str, default=False\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc. If the input is sparse but not in the allowed format,\n        it will be converted to the first listed format. True allows the input\n        to be any format. False means that a sparse matrix input will\n        raise an error.\n\n    accept_large_sparse : bool, default=True\n        If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\n        accept_sparse, accept_large_sparse will cause it to be accepted only\n        if its indices are stored with a 32-bit dtype.\n\n        .. versionadded:: 0.20\n\n    dtype : 'numeric', type, list of type or None, default='numeric'\n        Data type of result. If None, the dtype of the input is preserved.\n        If \"numeric\", dtype is preserved unless array.dtype is object.\n        If dtype is a list of types, conversion on the first type is only\n        performed if the dtype o"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ixes import CSR_CONTAINERS, SPARRAY_PRESENT\nfrom sklearn.utils.metaestimators import available_if\nfrom sklearn.utils.multiclass import type_of_target\nfrom sklearn.utils.validation import (\n    check_array,\n    check_is_fitted,\n    check_X_y,\n    validate_data,\n)\n\n\nclass CorrectNotFittedError(ValueError):\n    \"\"\"Exception class to raise if estimator is used before fitting.\n\n    Like NotFittedError, it inherits from ValueError, but not from\n    AttributeError. Used for testing only.\n    \"\"\"\n\n\nclass BaseBadClassifier(ClassifierMixin, BaseEstimator):\n    def fit(self, X, y):\n        return self\n\n    def predict(self, X):\n        return np.ones(X.shape[0])\n\n\nclass ChangesDict(BaseEstimator):\n    def __init__(self, key=0):\n        self.key = key\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n    def predict(self, X):\n        X = check_array(X)\n        self.key = 1000\n        return np.ones(X.shape[0])\n\n\nclass SetsWrongAttribute(BaseEstimator):\n    def __init__(self, acceptable_key=0):\n        self.acceptable_key = acceptable_key\n\n    def fit(self, X, y=None):\n        self.wrong_attribute = 0\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ChangesWrongAttribute(BaseEstimator):\n    def __init__(self, wrong_attribute=0):\n        self.wrong_attribute = wrong_attribute\n\n    def fit(self, X, y=None):\n        self.wrong_attribute = 1\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ChangesUnderscoreAttribute(BaseEstimator):\n    def fit(self, X, y=None):\n        self._good_attribute = 1\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass RaisesErrorInSetParams(BaseEstimator):\n    def __init__(self, p=0):\n        self.p = p\n\n    def set_params(self, **kwargs):\n        if \"p\" in kwargs:\n            p = kwargs.pop(\"p\")\n            if p < 0:\n                raise ValueError(\"p can't be less than 0\")\n            self.p = p\n        return super().set_params(**kwargs)\n\n    def fit(sel"}, {"start_line": 105000, "end_line": 107000, "belongs_to": {"file_name": "validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " in an estimator that requires input\n    validation. This mutates the estimator and sets the `n_features_in_` and\n    `feature_names_in_` attributes if `reset=True`.\n\n    .. versionadded:: 1.6\n\n    Parameters\n    ----------\n    _estimator : estimator instance\n        The estimator to validate the input for.\n\n    X : {array-like, sparse matrix, dataframe} of shape \\\n            (n_samples, n_features), default='no validation'\n        The input samples.\n        If `'no_validation'`, no validation is performed on `X`. This is\n        useful for meta-estimator which can delegate input validation to\n        their underlying estimator(s). In that case `y` must be passed and\n        the only accepted `check_params` are `multi_output` and\n        `y_numeric`.\n\n    y : array-like of shape (n_samples,), default='no_validation'\n        The targets.\n\n        - If `None`, :func:`~sklearn.utils.check_array` is called on `X`. If\n          the estimator's `requires_y` tag is True, then an error will be raised.\n        - If `'no_validation'`, :func:`~sklearn.utils.check_array` is called\n          on `X` and the estimator's `requires_y` tag is ignored. This is a default\n          placeholder and is never meant to be explicitly set. In that case `X` must be\n          passed.\n        - Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with\n          either :func:`~sklearn.utils.check_array` or\n          :func:`~sklearn.utils.check_X_y` depending on `validate_separately`.\n\n    reset : bool, default=True\n        Whether to reset the `n_features_in_` attribute.\n        If False, the input will be checked for consistency with data\n        provided when reset was last True.\n\n        .. note::\n\n           It is recommended to call `reset=True` in `fit` and in the first\n           call to `partial_fit`. All other methods that validate `X`\n           should set `reset=False`.\n\n    validate_separately : False or tuple of dicts, default=False\n        Only used if `y` is not `No"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ataframe_column_names_consistency,\n    check_decision_proba_consistency,\n    check_dict_unchanged,\n    check_dont_overwrite_parameters,\n    check_estimator,\n    check_estimator_cloneable,\n    check_estimator_repr,\n    check_estimator_sparse_array,\n    check_estimator_sparse_matrix,\n    check_estimator_sparse_tag,\n    check_estimator_tags_renamed,\n    check_estimators_nan_inf,\n    check_estimators_overwrite_params,\n    check_estimators_unfitted,\n    check_fit_check_is_fitted,\n    check_fit_score_takes_y,\n    check_methods_sample_order_invariance,\n    check_methods_subset_invariance,\n    check_mixin_order,\n    check_no_attributes_set_in_init,\n    check_outlier_contamination,\n    check_outlier_corruption,\n    check_parameters_default_constructible,\n    check_positive_only_tag_during_fit,\n    check_regressor_data_not_an_array,\n    check_requires_y_none,\n    check_sample_weights_pandas_series,\n    check_set_params,\n    estimator_checks_generator,\n    set_random_state,\n)\nfrom sklearn.utils.fixes import CSR_CONTAINERS, SPARRAY_PRESENT\nfrom sklearn.utils.metaestimators import available_if\nfrom sklearn.utils.multiclass import type_of_target\nfrom sklearn.utils.validation import (\n    check_array,\n    check_is_fitted,\n    check_X_y,\n    validate_data,\n)\n\n\nclass CorrectNotFittedError(ValueError):\n    \"\"\"Exception class to raise if estimator is used before fitting.\n\n    Like NotFittedError, it inherits from ValueError, but not from\n    AttributeError. Used for testing only.\n    \"\"\"\n\n\nclass BaseBadClassifier(ClassifierMixin, BaseEstimator):\n    def fit(self, X, y):\n        return self\n\n    def predict(self, X):\n        return np.ones(X.shape[0])\n\n\nclass ChangesDict(BaseEstimator):\n    def __init__(self, key=0):\n        self.key = key\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n    def predict(self, X):\n        X = check_array(X)\n        self.key = 1000\n        return np.ones(X.shape[0])\n\n\nclass SetsWrongAttribute(BaseEstimator):\n  "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "put import _get_output_config\nfrom sklearn.utils._testing import (\n    _convert_container,\n    assert_array_equal,\n)\nfrom sklearn.utils.validation import _check_n_features, validate_data\n\n\n#############################################################################\n# A few test classes\nclass MyEstimator(BaseEstimator):\n    def __init__(self, l1=0, empty=None):\n        self.l1 = l1\n        self.empty = empty\n\n\nclass K(BaseEstimator):\n    def __init__(self, c=None, d=None):\n        self.c = c\n        self.d = d\n\n\nclass T(BaseEstimator):\n    def __init__(self, a=None, b=None):\n        self.a = a\n        self.b = b\n\n\nclass NaNTag(BaseEstimator):\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.allow_nan = True\n        return tags\n\n\nclass NoNaNTag(BaseEstimator):\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.allow_nan = False\n        return tags\n\n\nclass OverrideTag(NaNTag):\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.allow_nan = False\n        return tags\n\n\nclass DiamondOverwriteTag(NaNTag, NoNaNTag):\n    pass\n\n\nclass InheritDiamondOverwriteTag(DiamondOverwriteTag):\n    pass\n\n\nclass ModifyInitParams(BaseEstimator):\n    \"\"\"Deprecated behavior.\n    Equal parameters but with a type cast.\n    Doesn't fulfill a is a\n    \"\"\"\n\n    def __init__(self, a=np.array([0])):\n        self.a = a.copy()\n\n\nclass Buggy(BaseEstimator):\n    \"A buggy estimator that does not set its parameters right.\"\n\n    def __init__(self, a=None):\n        self.a = 1\n\n\nclass NoEstimator:\n    def __init__(self):\n        pass\n\n    def fit(self, X=None, y=None):\n        return self\n\n    def predict(self, X=None):\n        return None\n\n\nclass VargEstimator(BaseEstimator):\n    \"\"\"scikit-learn estimators shouldn't have vargs.\"\"\"\n\n    def __init__(self, *vargs):\n        pass\n\n\n#############################################################################\n# The tests"}, {"start_line": 106000, "end_line": 108000, "belongs_to": {"file_name": "validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e raised.\n        - If `'no_validation'`, :func:`~sklearn.utils.check_array` is called\n          on `X` and the estimator's `requires_y` tag is ignored. This is a default\n          placeholder and is never meant to be explicitly set. In that case `X` must be\n          passed.\n        - Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with\n          either :func:`~sklearn.utils.check_array` or\n          :func:`~sklearn.utils.check_X_y` depending on `validate_separately`.\n\n    reset : bool, default=True\n        Whether to reset the `n_features_in_` attribute.\n        If False, the input will be checked for consistency with data\n        provided when reset was last True.\n\n        .. note::\n\n           It is recommended to call `reset=True` in `fit` and in the first\n           call to `partial_fit`. All other methods that validate `X`\n           should set `reset=False`.\n\n    validate_separately : False or tuple of dicts, default=False\n        Only used if `y` is not `None`.\n        If `False`, call :func:`~sklearn.utils.check_X_y`. Else, it must be a tuple of\n        kwargs to be used for calling :func:`~sklearn.utils.check_array` on `X` and `y`\n        respectively.\n\n        `estimator=self` is automatically added to these dicts to generate\n        more informative error message in case of invalid input data.\n\n    skip_check_array : bool, default=False\n        If `True`, `X` and `y` are unchanged and only `feature_names_in_` and\n        `n_features_in_` are checked. Otherwise, :func:`~sklearn.utils.check_array`\n        is called on `X` and `y`.\n\n    **check_params : kwargs\n        Parameters passed to :func:`~sklearn.utils.check_array` or\n        :func:`~sklearn.utils.check_X_y`. Ignored if validate_separately\n        is not False.\n\n        `estimator=self` is automatically added to these params to generate\n        more informative error message in case of invalid input data.\n\n    Returns\n    -------\n    out : {ndarray, sparse matrix} or tuple of th"}, {"start_line": 25000, "end_line": 27000, "belongs_to": {"file_name": "validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "be converted\n        # first to map pd.NA to np.nan\n        return True\n    elif is_integer_dtype(pd_dtype):\n        # XXX: Warn when converting from a high integer to a float\n        return True\n\n    return False\n\n\ndef _is_extension_array_dtype(array):\n    # Pandas extension arrays have a dtype with an na_value\n    return hasattr(array, \"dtype\") and hasattr(array.dtype, \"na_value\")\n\n\ndef check_array(\n    array,\n    accept_sparse=False,\n    *,\n    accept_large_sparse=True,\n    dtype=\"numeric\",\n    order=None,\n    copy=False,\n    force_writeable=False,\n    force_all_finite=\"deprecated\",\n    ensure_all_finite=None,\n    ensure_non_negative=False,\n    ensure_2d=True,\n    allow_nd=False,\n    ensure_min_samples=1,\n    ensure_min_features=1,\n    estimator=None,\n    input_name=\"\",\n):\n    \"\"\"Input validation on an array, list, sparse matrix or similar.\n\n    By default, the input is checked to be a non-empty 2D array containing\n    only finite values. If the dtype of the array is object, attempt\n    converting to float, raising on failure.\n\n    Parameters\n    ----------\n    array : object\n        Input object to check / convert.\n\n    accept_sparse : str, bool or list/tuple of str, default=False\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc. If the input is sparse but not in the allowed format,\n        it will be converted to the first listed format. True allows the input\n        to be any format. False means that a sparse matrix input will\n        raise an error.\n\n    accept_large_sparse : bool, default=True\n        If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\n        accept_sparse, accept_large_sparse=False will cause it to be accepted\n        only if its indices are stored with a 32-bit dtype.\n\n        .. versionadded:: 0.20\n\n    dtype : 'numeric', type, list of type or None, default='numeric'\n        Data type of result. If None, the dtype of the input is preserved.\n        If \"numeric\", dtype is pres"}, {"start_line": 42000, "end_line": 44000, "belongs_to": {"file_name": "api_reference.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/doc", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "alidation\",\n                \"description\": _get_submodule(\"sklearn.utils\", \"validation\"),\n                \"autosummary\": [\n                    \"check_X_y\",\n                    \"check_array\",\n                    \"check_consistent_length\",\n                    \"check_random_state\",\n                    \"check_scalar\",\n                    \"validation.check_is_fitted\",\n                    \"validation.check_memory\",\n                    \"validation.check_symmetric\",\n                    \"validation.column_or_1d\",\n                    \"validation.has_fit_parameter\",\n                    \"validation.validate_data\",\n                ],\n            },\n            {\n                \"title\": \"Meta-estimators\",\n                \"description\": _get_submodule(\"sklearn.utils\", \"metaestimators\"),\n                \"autosummary\": [\"metaestimators.available_if\"],\n            },\n            {\n                \"title\": \"Weight handling based on class labels\",\n                \"description\": _get_submodule(\"sklearn.utils\", \"class_weight\"),\n                \"autosummary\": [\n                    \"class_weight.compute_class_weight\",\n                    \"class_weight.compute_sample_weight\",\n                ],\n            },\n            {\n                \"title\": \"Dealing with multiclass target in classifiers\",\n                \"description\": _get_submodule(\"sklearn.utils\", \"multiclass\"),\n                \"autosummary\": [\n                    \"multiclass.is_multilabel\",\n                    \"multiclass.type_of_target\",\n                    \"multiclass.unique_labels\",\n                ],\n            },\n            {\n                \"title\": \"Optimal mathematical operations\",\n                \"description\": _get_submodule(\"sklearn.utils\", \"extmath\"),\n                \"autosummary\": [\n                    \"extmath.density\",\n                    \"extmath.fast_logdet\",\n                    \"extmath.randomized_range_finder\",\n                    \"extmath.randomized_svd\",\n                    \"extmath.safe_sparse_dot\",\n     "}, {"start_line": 103000, "end_line": 105000, "belongs_to": {"file_name": "validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "he input for.\n\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The input samples.\n\n    reset : bool\n        Whether to reset the `n_features_in_` attribute.\n        If True, the `n_features_in_` attribute is set to `X.shape[1]`.\n        If False and the attribute exists, then check that it is equal to\n        `X.shape[1]`. If False and the attribute does *not* exist, then\n        the check is skipped.\n\n        .. note::\n           It is recommended to call `reset=True` in `fit` and in the first\n           call to `partial_fit`. All other methods that validate `X`\n           should set `reset=False`.\n    \"\"\"\n    try:\n        n_features = _num_features(X)\n    except TypeError as e:\n        if not reset and hasattr(estimator, \"n_features_in_\"):\n            raise ValueError(\n                \"X does not contain any features, but \"\n                f\"{estimator.__class__.__name__} is expecting \"\n                f\"{estimator.n_features_in_} features\"\n            ) from e\n        # If the number of features is not defined and reset=True,\n        # then we skip this check\n        return\n\n    if reset:\n        estimator.n_features_in_ = n_features\n        return\n\n    if not hasattr(estimator, \"n_features_in_\"):\n        # Skip this check if the expected number of expected input features\n        # was not recorded by calling fit first. This is typically the case\n        # for stateless transformers.\n        return\n\n    if n_features != estimator.n_features_in_:\n        raise ValueError(\n            f\"X has {n_features} features, but {estimator.__class__.__name__} \"\n            f\"is expecting {estimator.n_features_in_} features as input.\"\n        )\n\n\ndef validate_data(\n    _estimator,\n    /,\n    X=\"no_validation\",\n    y=\"no_validation\",\n    reset=True,\n    validate_separately=False,\n    skip_check_array=False,\n    **check_params,\n):\n    \"\"\"Validate input data and set or check feature names and counts of the input.\n\n    This helper function should be used"}, {"start_line": 48000, "end_line": 50000, "belongs_to": {"file_name": "validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " in X.\n        - 'allow-nan': accepts only np.nan or pd.NA values in X. Values cannot\n          be infinite.\n\n        .. versionadded:: 0.20\n           ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n        .. versionchanged:: 0.23\n           Accepts `pd.NA` and converts it into `np.nan`\n\n        .. deprecated:: 1.6\n           `force_all_finite` was renamed to `ensure_all_finite` and will be removed\n           in 1.8.\n\n    ensure_all_finite : bool or 'allow-nan', default=True\n        Whether to raise an error on np.inf, np.nan, pd.NA in array. This parameter\n        does not influence whether y can have np.inf, np.nan, pd.NA values.\n        The possibilities are:\n\n        - True: Force all values of X to be finite.\n        - False: accepts np.inf, np.nan, pd.NA in X.\n        - 'allow-nan': accepts only np.nan or pd.NA values in X. Values cannot\n          be infinite.\n\n        .. versionadded:: 1.6\n           `force_all_finite` was renamed to `ensure_all_finite`.\n\n    ensure_2d : bool, default=True\n        Whether to raise a value error if X is not 2D.\n\n    allow_nd : bool, default=False\n        Whether to allow X.ndim > 2.\n\n    multi_output : bool, default=False\n        Whether to allow 2D y (array or sparse matrix). If false, y will be\n        validated as a vector. y cannot have np.nan or np.inf values if\n        multi_output=True.\n\n    ensure_min_samples : int, default=1\n        Make sure that X has a minimum number of samples in its first\n        axis (rows for a 2D array).\n\n    ensure_min_features : int, default=1\n        Make sure that the 2D array has some minimum number of features\n        (columns). The default value of 1 rejects empty datasets.\n        This check is only enforced when X has effectively 2 dimensions or\n        is originally 1D and ``ensure_2d`` is True. Setting to 0 disables\n        this check.\n\n    y_numeric : bool, default=False\n        Whether to ensure that y has a numeric type. If dtype of y is object,\n        it is converte"}], "retrieved_count": 10, "cost_time": 1.2365937232971191}
{"question": "Why does Scikit-learn provide built-in cross-validation utilities instead of requiring users to implement their own?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn provides built-in cross-validation utilities instead of requiring users to implement their own for several fundamental reasons that enhance usability, reliability, and best practices:\n\n1. **User Experience and Accessibility**:\n   - **Ease of Use**: Users can perform cross-validation with simple function calls\n   - **Reduced Learning Curve**: No need to understand complex CV implementation details\n   - **Immediate Availability**: CV utilities are ready to use without additional coding\n   - **Consistent Interface**: Standardized API across all CV functions\n   - **Error Prevention**: Built-in utilities prevent common CV implementation mistakes\n\n2. **Reliability and Correctness**:\n   - **Thoroughly Tested**: CV utilities are extensively tested and validated\n   - **Edge Case Handling**: Proper handling of edge cases and error conditions\n   - **Statistical Validity**: Correct implementation of statistical CV principles\n   - **Bug Prevention**: Avoids common bugs in manual CV implementations\n   - **Consistent Results**: Reproducible results across different use cases\n\n3. **Performance and Efficiency**:\n   - **Optimized Implementation**: Highly optimized for performance and memory usage\n   - **Parallel Processing**: Built-in support for parallel CV execution\n   - **Memory Management**: Efficient memory handling for large datasets\n   - **Caching**: Intelligent caching of intermediate results\n   - **Scalability**: Handles large datasets and complex models efficiently\n\n4. **Integration and Compatibility**:\n   - **Estimator Compatibility**: Works seamlessly with all scikit-learn estimators\n   - **Pipeline Integration**: CV works correctly with Pipeline and FeatureUnion\n   - **Meta-Estimator Support**: Compatible with GridSearchCV, RandomizedSearchCV\n   - **Scoring Integration**: Works with all built-in and custom scoring functions\n   - **Data Type Support**: Handles various data types (dense, sparse, etc.)\n\n5. **Advanced Features and Flexibility**:\n   - **Multiple CV Strategies**: KFold, StratifiedKFold, TimeSeriesSplit, etc.\n   - **Custom Splitters**: Support for custom cross-validation strategies\n   - **Multiple Metrics**: Evaluate multiple metrics simultaneously\n   - **Metadata Routing**: Advanced metadata handling through CV splits\n   - **Error Handling**: Graceful handling of fitting failures\n\n6. **Best Practices Enforcement**:\n   - **Data Leakage Prevention**: Ensures proper train/test separation\n   - **Statistical Rigor**: Follows established statistical principles\n   - **Reproducibility**: Consistent results with proper random state handling\n   - **Validation Integrity**: Maintains validation set independence\n   - **Model Selection**: Prevents overfitting to test set\n\n7. **Educational and Documentation Benefits**:\n   - **Learning Resource**: Examples and documentation teach proper CV usage\n   - **Best Practices**: Reinforces correct machine learning methodology\n   - **Common Patterns**: Demonstrates common CV patterns and use cases\n   - **Debugging Help**: Clear error messages and debugging information\n   - **Community Knowledge**: Leverages collective knowledge and experience\n\n8. **Maintenance and Evolution**:\n   - **Continuous Improvement**: CV utilities are continuously improved\n   - **Bug Fixes**: Issues are fixed promptly by the development team\n   - **Feature Updates**: New CV strategies and features are added regularly\n   - **Backward Compatibility**: Maintains compatibility across versions\n   - **Performance Optimization**: Ongoing performance improvements\n\n9. **Production Readiness**:\n   - **Robust Implementation**: Production-ready code with proper error handling\n   - **Scalability**: Handles production-scale datasets and workloads\n   - **Monitoring**: Built-in timing and performance monitoring\n   - **Reliability**: Stable and reliable for production use\n   - **Compliance**: Helps ensure regulatory compliance in sensitive applications\n\n10. **Ecosystem Benefits**:\n    - **Standardization**: Establishes standard CV practices across the community\n    - **Interoperability**: Enables interoperability between different tools\n    - **Research Support**: Supports reproducible research and benchmarking\n    - **Community Growth**: Facilitates community learning and collaboration\n    - **Tool Integration**: Enables integration with other machine learning tools", "score": null, "retrieved_content": [{"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "       return splitter.split(X, y, groups=groups)\n    else:\n        return splitter.split(X, y)\n\n\ndef test_cross_validator_with_default_params():\n    n_samples = 4\n    n_unique_groups = 4\n    n_splits = 2\n    p = 2\n    n_shuffle_splits = 10  # (the default value)\n\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    X_1d = np.array([1, 2, 3, 4])\n    y = np.array([1, 1, 2, 2])\n    groups = np.array([1, 2, 3, 4])\n    loo = LeaveOneOut()\n    lpo = LeavePOut(p)\n    kf = KFold(n_splits)\n    skf = StratifiedKFold(n_splits)\n    lolo = LeaveOneGroupOut()\n    lopo = LeavePGroupsOut(p)\n    ss = ShuffleSplit(random_state=0)\n    ps = PredefinedSplit([1, 1, 2, 2])  # n_splits = np of unique folds = 2\n    sgkf = StratifiedGroupKFold(n_splits)\n\n    loo_repr = \"LeaveOneOut()\"\n    lpo_repr = \"LeavePOut(p=2)\"\n    kf_repr = \"KFold(n_splits=2, random_state=None, shuffle=False)\"\n    skf_repr = \"StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\"\n    lolo_repr = \"LeaveOneGroupOut()\"\n    lopo_repr = \"LeavePGroupsOut(n_groups=2)\"\n    ss_repr = (\n        \"ShuffleSplit(n_splits=10, random_state=0, test_size=None, train_size=None)\"\n    )\n    ps_repr = \"PredefinedSplit(test_fold=array([1, 1, 2, 2]))\"\n    sgkf_repr = \"StratifiedGroupKFold(n_splits=2, random_state=None, shuffle=False)\"\n\n    n_splits_expected = [\n        n_samples,\n        comb(n_samples, p),\n        n_splits,\n        n_splits,\n        n_unique_groups,\n        comb(n_unique_groups, p),\n        n_shuffle_splits,\n        2,\n        n_splits,\n    ]\n\n    for i, (cv, cv_repr) in enumerate(\n        zip(\n            [loo, lpo, kf, skf, lolo, lopo, ss, ps, sgkf],\n            [\n                loo_repr,\n                lpo_repr,\n                kf_repr,\n                skf_repr,\n                lolo_repr,\n                lopo_repr,\n                ss_repr,\n                ps_repr,\n                sgkf_repr,\n            ],\n        )\n    ):\n        # Test if get_n_splits works correctly\n        assert n_splits_expected[i] == cv"}, {"start_line": 18000, "end_line": 20000, "belongs_to": {"file_name": "_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " len(results)\n        fit_errors_counter = Counter(fit_errors)\n        delimiter = \"-\" * 80 + \"\\n\"\n        fit_errors_summary = \"\\n\".join(\n            f\"{delimiter}{n} fits failed with the following error:\\n{error}\"\n            for error, n in fit_errors_counter.items()\n        )\n\n        if num_failed_fits == num_fits:\n            all_fits_failed_message = (\n                f\"\\nAll the {num_fits} fits failed.\\n\"\n                \"It is very likely that your model is misconfigured.\\n\"\n                \"You can try to debug the error by setting error_score='raise'.\\n\\n\"\n                f\"Below are more details about the failures:\\n{fit_errors_summary}\"\n            )\n            raise ValueError(all_fits_failed_message)\n\n        else:\n            some_fits_failed_message = (\n                f\"\\n{num_failed_fits} fits failed out of a total of {num_fits}.\\n\"\n                \"The score on these train-test partitions for these parameters\"\n                f\" will be set to {error_score}.\\n\"\n                \"If these failures are not expected, you can try to debug them \"\n                \"by setting error_score='raise'.\\n\\n\"\n                f\"Below are more details about the failures:\\n{fit_errors_summary}\"\n            )\n            warnings.warn(some_fits_failed_message, FitFailedWarning)\n\n\n@validate_params(\n    {\n        \"estimator\": [HasMethods(\"fit\")],\n        \"X\": [\"array-like\", \"sparse matrix\"],\n        \"y\": [\"array-like\", None],\n        \"groups\": [\"array-like\", None],\n        \"scoring\": [StrOptions(set(get_scorer_names())), callable, None],\n        \"cv\": [\"cv_object\"],\n        \"n_jobs\": [Integral, None],\n        \"verbose\": [\"verbose\"],\n        \"params\": [dict, None],\n        \"pre_dispatch\": [Integral, str, None],\n        \"error_score\": [StrOptions({\"raise\"}), Real],\n    },\n    prefer_skip_nested_validation=False,  # estimator is not validated yet\n)\ndef cross_val_score(\n    estimator,\n    X,\n    y=None,\n    *,\n    groups=None,\n    scoring=None,\n    cv=None,\n    n_jobs=No"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "plot_nested_cross_validation_iris.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"\n=========================================\nNested versus non-nested cross-validation\n=========================================\n\nThis example compares non-nested and nested cross-validation strategies on a\nclassifier of the iris data set. Nested cross-validation (CV) is often used to\ntrain a model in which hyperparameters also need to be optimized. Nested CV\nestimates the generalization error of the underlying model and its\n(hyper)parameter search. Choosing the parameters that maximize non-nested CV\nbiases the model to the dataset, yielding an overly-optimistic score.\n\nModel selection without nested CV uses the same data to tune model parameters\nand evaluate model performance. Information may thus \"leak\" into the model\nand overfit the data. The magnitude of this effect is primarily dependent on\nthe size of the dataset and the stability of the model. See Cawley and Talbot\n[1]_ for an analysis of these issues.\n\nTo avoid this problem, nested CV effectively uses a series of\ntrain/validation/test set splits. In the inner loop (here executed by\n:class:`GridSearchCV <sklearn.model_selection.GridSearchCV>`), the score is\napproximately maximized by fitting a model to each training set, and then\ndirectly maximized in selecting (hyper)parameters over the validation set. In\nthe outer loop (here in :func:`cross_val_score\n<sklearn.model_selection.cross_val_score>`), generalization error is estimated\nby averaging test set scores over several dataset splits.\n\nThe example below uses a support vector classifier with a non-linear kernel to\nbuild a model with optimized hyperparameters by grid search. We compare the\nperformance of non-nested and nested CV strategies by taking the difference\nbetween their scores.\n\n.. seealso::\n\n    - :ref:`cross_validation`\n    - :ref:`grid_search`\n\n.. rubric:: References\n\n.. [1] `Cawley, G.C.; Talbot, N.L.C. On over-fitting in model selection and\n    subsequent selection bias in performance evaluation.\n    J. Mach. Learn. Res 2010,11, 2079-2107.\n    <"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "plot_nested_cross_validation_iris.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ion/test set splits. In the inner loop (here executed by\n:class:`GridSearchCV <sklearn.model_selection.GridSearchCV>`), the score is\napproximately maximized by fitting a model to each training set, and then\ndirectly maximized in selecting (hyper)parameters over the validation set. In\nthe outer loop (here in :func:`cross_val_score\n<sklearn.model_selection.cross_val_score>`), generalization error is estimated\nby averaging test set scores over several dataset splits.\n\nThe example below uses a support vector classifier with a non-linear kernel to\nbuild a model with optimized hyperparameters by grid search. We compare the\nperformance of non-nested and nested CV strategies by taking the difference\nbetween their scores.\n\n.. seealso::\n\n    - :ref:`cross_validation`\n    - :ref:`grid_search`\n\n.. rubric:: References\n\n.. [1] `Cawley, G.C.; Talbot, N.L.C. On over-fitting in model selection and\n    subsequent selection bias in performance evaluation.\n    J. Mach. Learn. Res 2010,11, 2079-2107.\n    <http://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf>`_\n\n\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.svm import SVC\n\n# Number of random trials\nNUM_TRIALS = 30\n\n# Load the dataset\niris = load_iris()\nX_iris = iris.data\ny_iris = iris.target\n\n# Set up possible values of parameters to optimize over\np_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}\n\n# We will use a Support Vector Classifier with \"rbf\" kernel\nsvm = SVC(kernel=\"rbf\")\n\n# Arrays to store scores\nnon_nested_scores = np.zeros(NUM_TRIALS)\nnested_scores = np.zeros(NUM_TRIALS)\n\n# Loop for each trial\nfor i in range(NUM_TRIALS):\n    # Choose cross-validation techniques for the inner and outer loops,\n    # independently of the dataset.\n    # E.g \"GroupKFold\", \"LeaveOneOut\", \"LeaveOneGroupOut\", etc.\n    in"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "train_score`` changes to a specific\n            metric like ``train_r2`` or ``train_auc`` if there are\n            multiple scoring metrics in the scoring parameter.\n            This is available only if ``return_train_score`` parameter\n            is ``True``.\n        ``fit_time``\n            The time for fitting the estimator on the train\n            set for each cv split.\n        ``score_time``\n            The time for scoring the estimator on the test set for each\n            cv split. (Note: time for scoring on the train set is not\n            included even if ``return_train_score`` is set to ``True``).\n        ``estimator``\n            The estimator objects for each cv split.\n            This is available only if ``return_estimator`` parameter\n            is set to ``True``.\n        ``indices``\n            The train/test positional indices for each cv split. A dictionary\n            is returned where the keys are either `\"train\"` or `\"test\"`\n            and the associated values are a list of integer-dtyped NumPy\n            arrays with the indices. Available only if `return_indices=True`.\n\n    See Also\n    --------\n    cross_val_score : Run cross-validation for single metric evaluation.\n\n    cross_val_predict : Get predictions from each split of cross-validation for\n        diagnostic purposes.\n\n    sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n        loss function.\n\n    Examples\n    --------\n    >>> from sklearn import datasets, linear_model\n    >>> from sklearn.model_selection import cross_validate\n    >>> from sklearn.metrics import make_scorer\n    >>> from sklearn.metrics import confusion_matrix\n    >>> from sklearn.svm import LinearSVC\n    >>> diabetes = datasets.load_diabetes()\n    >>> X = diabetes.data[:150]\n    >>> y = diabetes.target[:150]\n    >>> lasso = linear_model.Lasso()\n\n    Single metric evaluation using ``cross_validate``\n\n    >>> cv_results = cross_validate(lasso, X, y, cv=3)\n    >>> sorted(cv_results.keys())\n    "}, {"start_line": 98000, "end_line": 100000, "belongs_to": {"file_name": "_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  Examples\n    --------\n    >>> from sklearn.model_selection import check_cv\n    >>> check_cv(cv=5, y=None, classifier=False)\n    KFold(...)\n    >>> check_cv(cv=5, y=[1, 1, 0, 0, 0, 0], classifier=True)\n    StratifiedKFold(...)\n    \"\"\"\n    cv = 5 if cv is None else cv\n    if isinstance(cv, numbers.Integral):\n        if (\n            classifier\n            and (y is not None)\n            and (type_of_target(y, input_name=\"y\") in (\"binary\", \"multiclass\"))\n        ):\n            return StratifiedKFold(cv)\n        else:\n            return KFold(cv)\n\n    if not hasattr(cv, \"split\") or isinstance(cv, str):\n        if not isinstance(cv, Iterable) or isinstance(cv, str):\n            raise ValueError(\n                \"Expected cv as an integer, cross-validation \"\n                \"object (from sklearn.model_selection) \"\n                \"or an iterable. Got %s.\" % cv\n            )\n        return _CVIterableWrapper(cv)\n\n    return cv  # New style cv objects are passed without any modification\n\n\n@validate_params(\n    {\n        \"test_size\": [\n            Interval(RealNotInt, 0, 1, closed=\"neither\"),\n            Interval(numbers.Integral, 1, None, closed=\"left\"),\n            None,\n        ],\n        \"train_size\": [\n            Interval(RealNotInt, 0, 1, closed=\"neither\"),\n            Interval(numbers.Integral, 1, None, closed=\"left\"),\n            None,\n        ],\n        \"random_state\": [\"random_state\"],\n        \"shuffle\": [\"boolean\"],\n        \"stratify\": [\"array-like\", None],\n    },\n    prefer_skip_nested_validation=True,\n)\ndef train_test_split(\n    *arrays,\n    test_size=None,\n    train_size=None,\n    random_state=None,\n    shuffle=True,\n    stratify=None,\n):\n    \"\"\"Split arrays or matrices into random train and test subsets.\n\n    Quick utility that wraps input validation,\n    ``next(ShuffleSplit().split(X, y))``, and application to input data\n    into a single call for splitting (and optionally subsampling) data into a\n    one-liner.\n\n    Read more in the :ref:`User Guide <cros"}, {"start_line": 0, "end_line": 641, "belongs_to": {"file_name": "common.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"\nCommon utilities for testing model selection.\n\"\"\"\n\nimport numpy as np\n\nfrom sklearn.model_selection import KFold\n\n\nclass OneTimeSplitter:\n    \"\"\"A wrapper to make KFold single entry cv iterator\"\"\"\n\n    def __init__(self, n_splits=4, n_samples=99):\n        self.n_splits = n_splits\n        self.n_samples = n_samples\n        self.indices = iter(KFold(n_splits=n_splits).split(np.ones(n_samples)))\n\n    def split(self, X=None, y=None, groups=None):\n        \"\"\"Split can be called only once\"\"\"\n        for index in self.indices:\n            yield index\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        return self.n_splits\n"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "stance(estimators, list)\n    assert all(isinstance(estimator, Pipeline) for estimator in estimators)\n\n\n@pytest.mark.parametrize(\"use_sparse\", [False, True])\n@pytest.mark.parametrize(\"csr_container\", CSR_CONTAINERS)\ndef test_cross_validate(use_sparse: bool, csr_container):\n    # Compute train and test mse/r2 scores\n    cv = KFold()\n\n    # Regression\n    X_reg, y_reg = make_regression(n_samples=30, random_state=0)\n    reg = Ridge(random_state=0)\n\n    # Classification\n    X_clf, y_clf = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n\n    if use_sparse:\n        X_reg = csr_container(X_reg)\n        X_clf = csr_container(X_clf)\n\n    for X, y, est in ((X_reg, y_reg, reg), (X_clf, y_clf, clf)):\n        # It's okay to evaluate regression metrics on classification too\n        mse_scorer = check_scoring(est, scoring=\"neg_mean_squared_error\")\n        r2_scorer = check_scoring(est, scoring=\"r2\")\n        train_mse_scores = []\n        test_mse_scores = []\n        train_r2_scores = []\n        test_r2_scores = []\n        fitted_estimators = []\n\n        for train, test in cv.split(X, y):\n            est = clone(est).fit(X[train], y[train])\n            train_mse_scores.append(mse_scorer(est, X[train], y[train]))\n            train_r2_scores.append(r2_scorer(est, X[train], y[train]))\n            test_mse_scores.append(mse_scorer(est, X[test], y[test]))\n            test_r2_scores.append(r2_scorer(est, X[test], y[test]))\n            fitted_estimators.append(est)\n\n        train_mse_scores = np.array(train_mse_scores)\n        test_mse_scores = np.array(test_mse_scores)\n        train_r2_scores = np.array(train_r2_scores)\n        test_r2_scores = np.array(test_r2_scores)\n        fitted_estimators = np.array(fitted_estimators)\n\n        scores = (\n            train_mse_scores,\n            test_mse_scores,\n            train_r2_scores,\n            test_r2_scores,\n            fitted_estimators,\n        )\n\n        # To ensure that the test does not "}, {"start_line": 95000, "end_line": 97000, "belongs_to": {"file_name": "_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        return len(self.unique_folds)\n\n\nclass _CVIterableWrapper(BaseCrossValidator):\n    \"\"\"Wrapper class for old style cv objects and iterables.\"\"\"\n\n    def __init__(self, cv):\n        self.cv = list(cv)\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator.\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        return len(self.cv)\n\n    def split(self, X=None, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        for train, test in self.cv:\n            yield train, test\n\n\ndef check_cv(cv=5, y=None, *, classifier=False):\n    \"\"\"Input checker utility for building a cross-validator.\n\n    Parameters\n    ----------\n    cv : int, cross-validation generator, iterable or None, default=5\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n        - None, to use the default 5-fold cross validation,\n        "}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "= []\n        train_r2_scores = []\n        test_r2_scores = []\n        fitted_estimators = []\n\n        for train, test in cv.split(X, y):\n            est = clone(est).fit(X[train], y[train])\n            train_mse_scores.append(mse_scorer(est, X[train], y[train]))\n            train_r2_scores.append(r2_scorer(est, X[train], y[train]))\n            test_mse_scores.append(mse_scorer(est, X[test], y[test]))\n            test_r2_scores.append(r2_scorer(est, X[test], y[test]))\n            fitted_estimators.append(est)\n\n        train_mse_scores = np.array(train_mse_scores)\n        test_mse_scores = np.array(test_mse_scores)\n        train_r2_scores = np.array(train_r2_scores)\n        test_r2_scores = np.array(test_r2_scores)\n        fitted_estimators = np.array(fitted_estimators)\n\n        scores = (\n            train_mse_scores,\n            test_mse_scores,\n            train_r2_scores,\n            test_r2_scores,\n            fitted_estimators,\n        )\n\n        # To ensure that the test does not suffer from\n        # large statistical fluctuations due to slicing small datasets,\n        # we pass the cross-validation instance\n        check_cross_validate_single_metric(est, X, y, scores, cv)\n        check_cross_validate_multi_metric(est, X, y, scores, cv)\n\n\ndef check_cross_validate_single_metric(clf, X, y, scores, cv):\n    (\n        train_mse_scores,\n        test_mse_scores,\n        train_r2_scores,\n        test_r2_scores,\n        fitted_estimators,\n    ) = scores\n    # Test single metric evaluation when scoring is string or singleton list\n    for return_train_score, dict_len in ((True, 4), (False, 3)):\n        # Single metric passed as a string\n        if return_train_score:\n            mse_scores_dict = cross_validate(\n                clf,\n                X,\n                y,\n                scoring=\"neg_mean_squared_error\",\n                return_train_score=True,\n                cv=cv,\n            )\n            assert_array_almost_equal(mse_scores_dict[\"train_score\"], train"}], "retrieved_count": 10, "cost_time": 1.2586100101470947}
{"question": "Where in Scikit-learn's codebase does the pipeline system optimize memory usage and performance in large-scale data processing?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn's pipeline system optimizes memory usage and performance in large-scale data processing through several key mechanisms implemented throughout the codebase. Here's where and how these optimizations are achieved:\n\n1. **Pipeline Caching System (pipeline.py)**:\n   - **Transformer Caching**: Pipeline caches fitted transformers to avoid recomputation\n   - **Memory Parameter**: Configurable memory storage for intermediate results\n   - **Disk-Based Caching**: Can use disk storage for large intermediate results\n   - **Selective Caching**: Only caches transformers, not the final estimator\n   - **Cache Invalidation**: Intelligent cache invalidation when parameters change\n\n2. **Chunked Processing (_chunking.py)**:\n   - **gen_even_slices()**: Generates even slices for chunked processing\n   - **get_chunk_n_rows()**: Calculates optimal chunk size based on working memory\n   - **Memory-Aware Chunking**: Chunks data to fit within available memory\n   - **Configurable Working Memory**: User-configurable working memory limits\n   - **Automatic Chunk Size**: Automatic calculation of optimal chunk sizes\n\n3. **Working Memory Management (_config.py)**:\n   - **Global Configuration**: set_config() and get_config() for memory limits\n   - **Working Memory Limits**: Configurable working memory (default 1GB)\n   - **Context Managers**: config_context for temporary memory settings\n   - **Memory Monitoring**: Built-in memory usage monitoring\n   - **Thread-Local Storage**: Thread-specific memory configurations\n\n4. **Out-of-Core Learning Support**:\n   - **Streaming Data**: Support for data that doesn't fit in memory\n   - **Incremental Learning**: Incremental algorithms for large datasets\n   - **Feature Extraction**: Memory-efficient feature extraction methods\n   - **Batch Processing**: Processing data in manageable batches\n   - **Memory Mapping**: Support for memory-mapped files\n\n5. **Sparse Matrix Optimizations**:\n   - **Sparse Format Support**: Efficient handling of sparse matrices\n   - **Format Conversion**: Automatic conversion between sparse formats\n   - **Memory-Efficient Operations**: Optimized operations for sparse data\n   - **Sparsity Preservation**: Maintains sparsity throughout pipeline\n   - **Compressed Storage**: Uses compressed sparse formats (CSR, CSC)\n\n6. **Parallel Processing Integration**:\n   - **Joblib Integration**: Efficient parallel processing with joblib\n   - **Memory Distribution**: Distributes memory usage across parallel processes\n   - **Shared Memory**: Efficient shared memory management\n   - **Process Pooling**: Reuses processes to reduce overhead\n   - **Load Balancing**: Automatic load balancing across workers\n\n7. **Model Compression Features**:\n   - **Sparse Models**: Support for sparse model coefficients\n   - **Model Sparsification**: sparsify() method for linear models\n   - **Feature Selection**: Automatic feature selection to reduce memory\n   - **Model Reshaping**: Removing unused features from models\n   - **Memory-Efficient Storage**: Optimized storage of model parameters\n\n8. **Data Flow Optimizations**:\n   - **Streaming Processing**: Data flows through pipeline without storing all intermediate results\n   - **Lazy Evaluation**: Transformations are applied only when needed\n   - **Copy Avoidance**: Minimizes unnecessary data copying between steps\n   - **Memory-Efficient Transfers**: Optimized data transfer between pipeline steps\n   - **Garbage Collection**: Automatic cleanup of intermediate results\n\n9. **Large-Scale Data Handling**:\n   - **Chunked Operations**: Operations that can be performed in chunks\n   - **Memory Monitoring**: Built-in memory usage tracking\n   - **Out-of-Memory Prevention**: Prevents out-of-memory errors\n   - **Scalable Algorithms**: Algorithms designed for large-scale data\n   - **Distributed Processing**: Support for distributed processing frameworks\n\n10. **Performance Monitoring and Tuning**:\n    - **Memory Profiling**: Built-in memory profiling capabilities\n    - **Performance Metrics**: Tracks performance metrics throughout pipeline\n    - **Bottleneck Identification**: Identifies performance bottlenecks\n    - **Automatic Optimization**: Automatic optimization of step execution order\n    - **Resource Management**: Optimal allocation of computational resources", "score": null, "retrieved_content": [{"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "thodMapping,\n    _raise_for_params,\n    _routing_enabled,\n    get_routing_for_object,\n    process_routing,\n)\nfrom sklearn.utils.metaestimators import _BaseComposition, available_if\nfrom sklearn.utils.parallel import Parallel, delayed\nfrom sklearn.utils.validation import check_is_fitted, check_memory\n\n__all__ = [\"FeatureUnion\", \"Pipeline\", \"make_pipeline\", \"make_union\"]\n\n\n@contextmanager\ndef _raise_or_warn_if_not_fitted(estimator):\n    \"\"\"A context manager to make sure a NotFittedError is raised, if a sub-estimator\n    raises the error.\n\n    Otherwise, we raise a warning if the pipeline is not fitted, with the deprecation.\n\n    TODO(1.8): remove this context manager and replace with check_is_fitted.\n    \"\"\"\n    try:\n        yield\n    except NotFittedError as exc:\n        raise NotFittedError(\"Pipeline is not fitted yet.\") from exc\n\n    # we only get here if the above didn't raise\n    try:\n        check_is_fitted(estimator)\n    except NotFittedError:\n        warnings.warn(\n            \"This Pipeline instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using other methods such as transform, \"\n            \"predict, etc. This will raise an error in 1.8 instead of the current \"\n            \"warning.\",\n            FutureWarning,\n        )\n\n\ndef _final_estimator_has(attr):\n    \"\"\"Check that final_estimator has `attr`.\n\n    Used together with `available_if` in `Pipeline`.\"\"\"\n\n    def check(self):\n        # raise original `AttributeError` if `attr` does not exist\n        getattr(self._final_estimator, attr)\n        return True\n\n    return check\n\n\ndef _cached_transform(\n    sub_pipeline, *, cache, param_name, param_value, transform_params\n):\n    \"\"\"Transform a parameter value using a sub-pipeline and cache the result.\n\n    Parameters\n    ----------\n    sub_pipeline : Pipeline\n        The sub-pipeline to be used for transformation.\n    cache : dict\n        The cache dictionary to store the transformed values.\n    param_name : str\n        The n"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ring :term:`fit`. Only defined if the\n        underlying first estimator in `steps` exposes such an attribute\n        when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    make_pipeline : Convenience function for simplified pipeline construction.\n\n    Examples\n    --------\n    >>> from sklearn.svm import SVC\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.pipeline import Pipeline\n    >>> X, y = make_classification(random_state=0)\n    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n    ...                                                     random_state=0)\n    >>> pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n    >>> # The pipeline can be used as any other estimator\n    >>> # and avoids leaking the test set into the train set\n    >>> pipe.fit(X_train, y_train).score(X_test, y_test)\n    0.88\n    >>> # An estimator's parameter can be set using '__' syntax\n    >>> pipe.set_params(svc__C=10).fit(X_train, y_train).score(X_test, y_test)\n    0.76\n    \"\"\"\n\n    # BaseEstimator interface\n    _parameter_constraints: dict = {\n        \"steps\": [list, Hidden(tuple)],\n        \"transform_input\": [list, None],\n        \"memory\": [None, str, HasMethods([\"cache\"])],\n        \"verbose\": [\"boolean\"],\n    }\n\n    def __init__(self, steps, *, transform_input=None, memory=None, verbose=False):\n        self.steps = steps\n        self.transform_input = transform_input\n        self.memory = memory\n        self.verbose = verbose\n\n    def set_output(self, *, transform=None):\n        \"\"\"Set the output container when `\"transform\"` and `\"fit_transform\"` are called.\n\n        Calling"}, {"start_line": 59000, "end_line": 61000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "p : Pipeline\n        Returns a scikit-learn :class:`Pipeline` object.\n\n    See Also\n    --------\n    Pipeline : Class for creating a pipeline of transforms with a final\n        estimator.\n\n    Examples\n    --------\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.pipeline import make_pipeline\n    >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('gaussiannb', GaussianNB())])\n    \"\"\"\n    return Pipeline(\n        _name_estimators(steps),\n        transform_input=transform_input,\n        memory=memory,\n        verbose=verbose,\n    )\n\n\ndef _transform_one(transformer, X, y, weight, params):\n    \"\"\"Call transform and apply weight to output.\n\n    Parameters\n    ----------\n    transformer : estimator\n        Estimator to be used for transformation.\n\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Input data to be transformed.\n\n    y : ndarray of shape (n_samples,)\n        Ignored.\n\n    weight : float\n        Weight to be applied to the output of the transformation.\n\n    params : dict\n        Parameters to be passed to the transformer's ``transform`` method.\n\n        This should be of the form ``process_routing()[\"step_name\"]``.\n    \"\"\"\n    res = transformer.transform(X, **params.transform)\n    # if we have a weight for this transformer, multiply output\n    if weight is None:\n        return res\n    return res * weight\n\n\ndef _fit_transform_one(\n    transformer, X, y, weight, message_clsname=\"\", message=None, params=None\n):\n    \"\"\"\n    Fits ``transformer`` to ``X`` and ``y``. The transformed result is returned\n    with the fitted transformer. If ``weight`` is not ``None``, the result will\n    be multiplied by ``weight``.\n\n    ``params`` needs to be of the form ``process_routing()[\"step_name\"]``.\n    \"\"\"\n    params = params or {}\n    with _print_elapsed_time(message_clsname, message)"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "his Pipeline instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using other methods such as transform, \"\n            \"predict, etc. This will raise an error in 1.8 instead of the current \"\n            \"warning.\",\n            FutureWarning,\n        )\n\n\ndef _final_estimator_has(attr):\n    \"\"\"Check that final_estimator has `attr`.\n\n    Used together with `available_if` in `Pipeline`.\"\"\"\n\n    def check(self):\n        # raise original `AttributeError` if `attr` does not exist\n        getattr(self._final_estimator, attr)\n        return True\n\n    return check\n\n\ndef _cached_transform(\n    sub_pipeline, *, cache, param_name, param_value, transform_params\n):\n    \"\"\"Transform a parameter value using a sub-pipeline and cache the result.\n\n    Parameters\n    ----------\n    sub_pipeline : Pipeline\n        The sub-pipeline to be used for transformation.\n    cache : dict\n        The cache dictionary to store the transformed values.\n    param_name : str\n        The name of the parameter to be transformed.\n    param_value : object\n        The value of the parameter to be transformed.\n    transform_params : dict\n        The metadata to be used for transformation. This passed to the\n        `transform` method of the sub-pipeline.\n\n    Returns\n    -------\n    transformed_value : object\n        The transformed value of the parameter.\n    \"\"\"\n    if param_name not in cache:\n        # If the parameter is a tuple, transform each element of the\n        # tuple. This is needed to support the pattern present in\n        # `lightgbm` and `xgboost` where users can pass multiple\n        # validation sets.\n        if isinstance(param_value, tuple):\n            cache[param_name] = tuple(\n                sub_pipeline.transform(element, **transform_params)\n                for element in param_value\n            )\n        else:\n            cache[param_name] = sub_pipeline.transform(param_value, **transform_params)\n\n    return cache[param_name]\n\n\nclass Pipeline(_BaseCo"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "caler', StandardScaler()), ('svc', SVC())])\n    >>> # The pipeline can be used as any other estimator\n    >>> # and avoids leaking the test set into the train set\n    >>> pipe.fit(X_train, y_train).score(X_test, y_test)\n    0.88\n    >>> # An estimator's parameter can be set using '__' syntax\n    >>> pipe.set_params(svc__C=10).fit(X_train, y_train).score(X_test, y_test)\n    0.76\n    \"\"\"\n\n    # BaseEstimator interface\n    _parameter_constraints: dict = {\n        \"steps\": [list, Hidden(tuple)],\n        \"transform_input\": [list, None],\n        \"memory\": [None, str, HasMethods([\"cache\"])],\n        \"verbose\": [\"boolean\"],\n    }\n\n    def __init__(self, steps, *, transform_input=None, memory=None, verbose=False):\n        self.steps = steps\n        self.transform_input = transform_input\n        self.memory = memory\n        self.verbose = verbose\n\n    def set_output(self, *, transform=None):\n        \"\"\"Set the output container when `\"transform\"` and `\"fit_transform\"` are called.\n\n        Calling `set_output` will set the output of all estimators in `steps`.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.\n\n        Returns\n        -------\n        self : estimator instance\n            Estimator instance.\n        \"\"\"\n        for _, _, step in self._iter():\n            _safe_set_output(step, transform=transform)\n        return self\n\n    def get_params(self, deep=True):\n        \"\"\"Get parameters for this estimator.\n\n        Returns the parameters given in the constructor as well as the\n        estimators contained within the `steps` of the `Pipeline`.\n\n        Parameters\n       "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Utilities to build a composite estimator as a chain of transforms and estimators.\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport warnings\nfrom collections import Counter, defaultdict\nfrom contextlib import contextmanager\nfrom copy import deepcopy\nfrom itertools import chain, islice\n\nimport numpy as np\nfrom scipy import sparse\n\nfrom sklearn.base import TransformerMixin, _fit_context, clone\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.utils import Bunch\nfrom sklearn.utils._metadata_requests import METHODS\nfrom sklearn.utils._param_validation import HasMethods, Hidden\nfrom sklearn.utils._repr_html.estimator import _VisualBlock\nfrom sklearn.utils._set_output import _get_container_adapter, _safe_set_output\nfrom sklearn.utils._tags import get_tags\nfrom sklearn.utils._user_interface import _print_elapsed_time\nfrom sklearn.utils.metadata_routing import (\n    MetadataRouter,\n    MethodMapping,\n    _raise_for_params,\n    _routing_enabled,\n    get_routing_for_object,\n    process_routing,\n)\nfrom sklearn.utils.metaestimators import _BaseComposition, available_if\nfrom sklearn.utils.parallel import Parallel, delayed\nfrom sklearn.utils.validation import check_is_fitted, check_memory\n\n__all__ = [\"FeatureUnion\", \"Pipeline\", \"make_pipeline\", \"make_union\"]\n\n\n@contextmanager\ndef _raise_or_warn_if_not_fitted(estimator):\n    \"\"\"A context manager to make sure a NotFittedError is raised, if a sub-estimator\n    raises the error.\n\n    Otherwise, we raise a warning if the pipeline is not fitted, with the deprecation.\n\n    TODO(1.8): remove this context manager and replace with check_is_fitted.\n    \"\"\"\n    try:\n        yield\n    except NotFittedError as exc:\n        raise NotFittedError(\"Pipeline is not fitted yet.\") from exc\n\n    # we only get here if the above didn't raise\n    try:\n        check_is_fitted(estimator)\n    except NotFittedError:\n        warnings.warn(\n            \"T"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " it to\n    `'passthrough'` or `None`.\n\n    For an example use case of `Pipeline` combined with\n    :class:`~sklearn.model_selection.GridSearchCV`, refer to\n    :ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`. The\n    example :ref:`sphx_glr_auto_examples_compose_plot_digits_pipe.py` shows how\n    to grid search on a pipeline using `'__'` as a separator in the parameter names.\n\n    Read more in the :ref:`User Guide <pipeline>`.\n\n    .. versionadded:: 0.5\n\n    Parameters\n    ----------\n    steps : list of tuples\n        List of (name of step, estimator) tuples that are to be chained in\n        sequential order. To be compatible with the scikit-learn API, all steps\n        must define `fit`. All non-last steps must also define `transform`. See\n        :ref:`Combining Estimators <combining_estimators>` for more details.\n\n    transform_input : list of str, default=None\n        The names of the :term:`metadata` parameters that should be transformed by the\n        pipeline before passing it to the step consuming it.\n\n        This enables transforming some input arguments to ``fit`` (other than ``X``)\n        to be transformed by the steps of the pipeline up to the step which requires\n        them. Requirement is defined via :ref:`metadata routing <metadata_routing>`.\n        For instance, this can be used to pass a validation set through the pipeline.\n\n        You can only set this if metadata routing is enabled, which you\n        can enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n\n        .. versionadded:: 1.6\n\n    memory : str or object with the joblib.Memory interface, default=None\n        Used to cache the fitted transformers of the pipeline. The last step\n        will never be cached, even if it is a transformer. By default, no\n        caching is performed. If a string is given, it is the path to the\n        caching directory. Enabling caching triggers a clone of the transformers\n        before fitting. Therefore, the transformer ins"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "mposition):\n    \"\"\"\n    A sequence of data transformers with an optional final predictor.\n\n    `Pipeline` allows you to sequentially apply a list of transformers to\n    preprocess the data and, if desired, conclude the sequence with a final\n    :term:`predictor` for predictive modeling.\n\n    Intermediate steps of the pipeline must be transformers, that is, they\n    must implement `fit` and `transform` methods.\n    The final :term:`estimator` only needs to implement `fit`.\n    The transformers in the pipeline can be cached using ``memory`` argument.\n\n    The purpose of the pipeline is to assemble several steps that can be\n    cross-validated together while setting different parameters. For this, it\n    enables setting parameters of the various steps using their names and the\n    parameter name separated by a `'__'`, as in the example below. A step's\n    estimator may be replaced entirely by setting the parameter with its name\n    to another estimator, or a transformer removed by setting it to\n    `'passthrough'` or `None`.\n\n    For an example use case of `Pipeline` combined with\n    :class:`~sklearn.model_selection.GridSearchCV`, refer to\n    :ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`. The\n    example :ref:`sphx_glr_auto_examples_compose_plot_digits_pipe.py` shows how\n    to grid search on a pipeline using `'__'` as a separator in the parameter names.\n\n    Read more in the :ref:`User Guide <pipeline>`.\n\n    .. versionadded:: 0.5\n\n    Parameters\n    ----------\n    steps : list of tuples\n        List of (name of step, estimator) tuples that are to be chained in\n        sequential order. To be compatible with the scikit-learn API, all steps\n        must define `fit`. All non-last steps must also define `transform`. See\n        :ref:`Combining Estimators <combining_estimators>` for more details.\n\n    transform_input : list of str, default=None\n        The names of the :term:`metadata` parameters that should be transformed by the\n        pipeline befo"}, {"start_line": 58000, "end_line": 60000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e transformer instance given to the\n        pipeline cannot be inspected directly. Use the attribute ``named_steps``\n        or ``steps`` to inspect estimators within the pipeline. Caching the\n        transformers is advantageous when fitting is time consuming.\n\n    transform_input : list of str, default=None\n        This enables transforming some input arguments to ``fit`` (other than ``X``)\n        to be transformed by the steps of the pipeline up to the step which requires\n        them. Requirement is defined via :ref:`metadata routing <metadata_routing>`.\n        This can be used to pass a validation set through the pipeline for instance.\n\n        You can only set this if metadata routing is enabled, which you\n        can enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n\n        .. versionadded:: 1.6\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting each step will be printed as it\n        is completed.\n\n    Returns\n    -------\n    p : Pipeline\n        Returns a scikit-learn :class:`Pipeline` object.\n\n    See Also\n    --------\n    Pipeline : Class for creating a pipeline of transforms with a final\n        estimator.\n\n    Examples\n    --------\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.pipeline import make_pipeline\n    >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('gaussiannb', GaussianNB())])\n    \"\"\"\n    return Pipeline(\n        _name_estimators(steps),\n        transform_input=transform_input,\n        memory=memory,\n        verbose=verbose,\n    )\n\n\ndef _transform_one(transformer, X, y, weight, params):\n    \"\"\"Call transform and apply weight to output.\n\n    Parameters\n    ----------\n    transformer : estimator\n        Estimator to be used for transformation.\n\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        In"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tance given to the\n        pipeline cannot be inspected directly. Use the attribute ``named_steps``\n        or ``steps`` to inspect estimators within the pipeline. Caching the\n        transformers is advantageous when fitting is time consuming. See\n        :ref:`sphx_glr_auto_examples_neighbors_plot_caching_nearest_neighbors.py`\n        for an example on how to enable caching.\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting each step will be printed as it\n        is completed.\n\n    Attributes\n    ----------\n    named_steps : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n        Read-only attribute to access any step parameter by user given name.\n        Keys are step names and values are steps parameters.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels. Only exist if the last step of the pipeline is a\n        classifier.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying first estimator in `steps` exposes such an attribute\n        when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    make_pipeline : Convenience function for simplified pipeline construction.\n\n    Examples\n    --------\n    >>> from sklearn.svm import SVC\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.pipeline import Pipeline\n    >>> X, y = make_classification(random_state=0)\n    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n    ...                                                     random_state=0)\n    >>> pipe = Pipeline([('s"}], "retrieved_count": 10, "cost_time": 1.2469274997711182}
{"question": "Why does Scikit-learn's estimator interface improve performance compared to algorithm-specific APIs?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn's estimator interface improves performance compared to algorithm-specific APIs for several fundamental design reasons that optimize execution, memory usage, and computational efficiency:\n\n1. **Optimized Base Class Implementation**:\n   - **Efficient Parameter Management**: BaseEstimator provides optimized get_params() and set_params() methods\n   - **Fast Cloning**: Optimized clone() function for efficient estimator copying\n   - **Memory-Efficient State Management**: Efficient storage and retrieval of estimator state\n   - **Optimized Serialization**: Fast pickle-based serialization and deserialization\n   - **Reduced Overhead**: Minimal overhead from the unified interface\n\n2. **Shared Optimization Strategies**:\n   - **Common Performance Patterns**: Shared optimization patterns across all estimators\n   - **Efficient Validation**: Centralized, optimized input validation functions\n   - **Memory Layout Optimization**: Consistent memory layout patterns for better cache performance\n   - **Vectorized Operations**: Shared vectorized operations where applicable\n   - **Parallel Processing**: Unified parallel processing capabilities\n\n3. **Meta-Estimator Performance Benefits**:\n   - **Pipeline Optimization**: Pipeline can optimize step execution order and caching\n   - **GridSearchCV Efficiency**: Efficient parameter grid exploration with shared validation\n   - **Cross-Validation Optimization**: Optimized CV execution with parallel processing\n   - **Ensemble Methods**: Efficient ensemble construction and prediction\n   - **Feature Union**: Optimized parallel feature processing\n\n4. **Memory Management Optimizations**:\n   - **Efficient State Storage**: Optimized storage of fitted parameters and attributes\n   - **Memory Pooling**: Shared memory pools for common data structures\n   - **Garbage Collection**: Optimized garbage collection patterns\n   - **Copy Avoidance**: Minimizes unnecessary data copying\n   - **Memory Layout**: Optimized memory layout for better cache performance\n\n5. **Caching and Reuse Strategies**:\n   - **Result Caching**: Built-in caching of expensive computations\n   - **Parameter Caching**: Efficient caching of parameter validation results\n   - **Model Persistence**: Fast model saving and loading\n   - **Intermediate Result Reuse**: Reuse of intermediate results in pipelines\n   - **Warm Starting**: Efficient warm-starting capabilities\n\n6. **Parallel Processing Integration**:\n   - **Unified Parallelization**: Consistent parallel processing across all estimators\n   - **Joblib Integration**: Optimized integration with joblib for parallel execution\n   - **Thread Safety**: Thread-safe operations for concurrent access\n   - **Resource Management**: Efficient resource allocation and deallocation\n   - **Load Balancing**: Automatic load balancing for parallel operations\n\n7. **Algorithm-Specific Optimizations**:\n   - **Specialized Implementations**: Algorithm-specific optimizations within the unified interface\n   - **Efficient Data Structures**: Optimized data structures for each algorithm type\n   - **Fast Prediction**: Optimized prediction methods for each estimator type\n   - **Memory-Efficient Training**: Optimized training algorithms for each estimator\n   - **Incremental Learning**: Efficient incremental learning where supported\n\n8. **Validation and Error Handling Efficiency**:\n   - **Fast Validation**: Optimized input validation with minimal overhead\n   - **Efficient Error Checking**: Fast error detection and handling\n   - **Early Termination**: Quick failure detection to avoid wasted computation\n   - **Optimized Error Messages**: Fast error message generation\n   - **State Validation**: Efficient validation of estimator state\n\n9. **Integration Performance Benefits**:\n   - **Seamless Integration**: No performance penalty for integration with other tools\n   - **Efficient Interoperability**: Fast interoperability with NumPy, pandas, etc.\n   - **Optimized Data Flow**: Efficient data flow between components\n   - **Reduced Serialization Overhead**: Minimal overhead for data serialization\n   - **Fast Type Conversion**: Optimized type conversion between different data formats\n\n10. **Future Performance Improvements**:\n    - **Continuous Optimization**: Ongoing performance improvements to the unified interface\n    - **New Optimization Techniques**: Easy integration of new optimization techniques\n    - **Hardware Acceleration**: Unified interface enables hardware acceleration\n    - **Algorithm Improvements**: Performance improvements benefit all estimators\n    - **Scalability**: Unified interface enables better scalability across different hardware", "score": null, "retrieved_content": [{"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "bench_hist_gradient_boosting_threading.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/benchmarks", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " in [\"lightgbm\", \"xgboost\", \"catboost\"]:\n        if getattr(args, libname):\n            print(libname)\n            est = get_equivalent_estimator(\n                sklearn_est, lib=libname, n_classes=args.n_classes\n            )\n            pprint(est.get_params())\n\n\ndef one_run(n_threads, n_samples):\n    X_train = X_train_[:n_samples]\n    X_test = X_test_[:n_samples]\n    y_train = y_train_[:n_samples]\n    y_test = y_test_[:n_samples]\n    if sample_weight is not None:\n        sample_weight_train = sample_weight_train_[:n_samples]\n    else:\n        sample_weight_train = None\n    assert X_train.shape[0] == n_samples\n    assert X_test.shape[0] == n_samples\n    print(\"Fitting a sklearn model...\")\n    tic = time()\n    est = sklearn.base.clone(sklearn_est)\n\n    with threadpool_limits(n_threads, user_api=\"openmp\"):\n        est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        sklearn_fit_duration = time() - tic\n        tic = time()\n        sklearn_score = est.score(X_test, y_test)\n        sklearn_score_duration = time() - tic\n    print(\"score: {:.4f}\".format(sklearn_score))\n    print(\"fit duration: {:.3f}s,\".format(sklearn_fit_duration))\n    print(\"score duration: {:.3f}s,\".format(sklearn_score_duration))\n\n    lightgbm_score = None\n    lightgbm_fit_duration = None\n    lightgbm_score_duration = None\n    if args.lightgbm:\n        print(\"Fitting a LightGBM model...\")\n        lightgbm_est = get_equivalent_estimator(\n            est, lib=\"lightgbm\", n_classes=args.n_classes\n        )\n        lightgbm_est.set_params(num_threads=n_threads)\n\n        tic = time()\n        lightgbm_est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        lightgbm_fit_duration = time() - tic\n        tic = time()\n        lightgbm_score = lightgbm_est.score(X_test, y_test)\n        lightgbm_score_duration = time() - tic\n        print(\"score: {:.4f}\".format(lightgbm_score))\n        print(\"fit duration: {:.3f}s,\".format(lightgbm_fit_duration))\n        print(\"score duration: {:.3f}"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "bench_hist_gradient_boosting_higgsboson.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/benchmarks", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " time()\n    print(f\"Loaded {df.values.nbytes / 1e9:0.3f} GB in {toc - tic:0.3f}s\")\n    return df\n\n\ndef fit(est, data_train, target_train, libname):\n    print(f\"Fitting a {libname} model...\")\n    tic = time()\n    est.fit(data_train, target_train)\n    toc = time()\n    print(f\"fitted in {toc - tic:.3f}s\")\n\n\ndef predict(est, data_test, target_test):\n    if args.no_predict:\n        return\n    tic = time()\n    predicted_test = est.predict(data_test)\n    predicted_proba_test = est.predict_proba(data_test)\n    toc = time()\n    roc_auc = roc_auc_score(target_test, predicted_proba_test[:, 1])\n    acc = accuracy_score(target_test, predicted_test)\n    print(f\"predicted in {toc - tic:.3f}s, ROC AUC: {roc_auc:.4f}, ACC: {acc:.4f}\")\n\n\ndf = load_data()\ntarget = df.values[:, 0]\ndata = np.ascontiguousarray(df.values[:, 1:])\ndata_train, data_test, target_train, target_test = train_test_split(\n    data, target, test_size=0.2, random_state=0\n)\nn_classes = len(np.unique(target))\n\nif subsample is not None:\n    data_train, target_train = data_train[:subsample], target_train[:subsample]\n\nn_samples, n_features = data_train.shape\nprint(f\"Training set with {n_samples} records with {n_features} features.\")\n\nif args.no_interactions:\n    interaction_cst = [[i] for i in range(n_features)]\nelse:\n    interaction_cst = None\n\nest = HistGradientBoostingClassifier(\n    loss=\"log_loss\",\n    learning_rate=lr,\n    max_iter=n_trees,\n    max_bins=max_bins,\n    max_leaf_nodes=n_leaf_nodes,\n    early_stopping=False,\n    random_state=0,\n    verbose=1,\n    interaction_cst=interaction_cst,\n    max_features=max_features,\n)\nfit(est, data_train, target_train, \"sklearn\")\npredict(est, data_test, target_test)\n\nif args.lightgbm:\n    est = get_equivalent_estimator(est, lib=\"lightgbm\", n_classes=n_classes)\n    fit(est, data_train, target_train, \"lightgbm\")\n    predict(est, data_test, target_test)\n\nif args.xgboost:\n    est = get_equivalent_estimator(est, lib=\"xgboost\", n_classes=n_classes)\n    fit(est, data_train, target_t"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "plot_release_highlights_1_6_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# ruff: noqa: CPY001, E501\n\"\"\"\n=======================================\nRelease Highlights for scikit-learn 1.6\n=======================================\n\n.. currentmodule:: sklearn\n\nWe are pleased to announce the release of scikit-learn 1.6! Many bug fixes\nand improvements were added, as well as some key new features. Below we\ndetail the highlights of this release. **For an exhaustive list of\nall the changes**, please refer to the :ref:`release notes <release_notes_1_6>`.\n\nTo install the latest version (with pip)::\n\n    pip install --upgrade scikit-learn\n\nor with conda::\n\n    conda install -c conda-forge scikit-learn\n\n\"\"\"\n\n# %%\n# FrozenEstimator: Freezing an estimator\n# --------------------------------------\n#\n# This meta-estimator allows you to take an estimator and freeze its fit method, meaning\n# that calling `fit` does not perform any operations; also, `fit_predict` and\n# `fit_transform` call `predict` and `transform` respectively without calling `fit`. The\n# original estimator's other methods and properties are left unchanged. An interesting\n# use case for this is to use a pre-fitted model as a transformer step in a pipeline\n# or to pass a pre-fitted model to some of the meta-estimators. Here's a short example:\n\nimport time\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.frozen import FrozenEstimator\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import FixedThresholdClassifier\n\nX, y = make_classification(n_samples=1000, random_state=0)\n\nstart = time.time()\nclassifier = SGDClassifier().fit(X, y)\nprint(f\"Fitting the classifier took {(time.time() - start) * 1_000:.2f} milliseconds\")\n\nstart = time.time()\nthreshold_classifier = FixedThresholdClassifier(\n    estimator=FrozenEstimator(classifier), threshold=0.9\n).fit(X, y)\nprint(\n    f\"Fitting the threshold classifier took {(time.time() - start) * 1_000:.2f} \"\n    \"milliseconds\"\n)\n\n# %%\n# Fitting the threshold classifier skipped fitting the inner `SGDClassifier`. For more\n# d"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "bench_hist_gradient_boosting_threading.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/benchmarks", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "t)\n        sklearn_score_duration = time() - tic\n    print(\"score: {:.4f}\".format(sklearn_score))\n    print(\"fit duration: {:.3f}s,\".format(sklearn_fit_duration))\n    print(\"score duration: {:.3f}s,\".format(sklearn_score_duration))\n\n    lightgbm_score = None\n    lightgbm_fit_duration = None\n    lightgbm_score_duration = None\n    if args.lightgbm:\n        print(\"Fitting a LightGBM model...\")\n        lightgbm_est = get_equivalent_estimator(\n            est, lib=\"lightgbm\", n_classes=args.n_classes\n        )\n        lightgbm_est.set_params(num_threads=n_threads)\n\n        tic = time()\n        lightgbm_est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        lightgbm_fit_duration = time() - tic\n        tic = time()\n        lightgbm_score = lightgbm_est.score(X_test, y_test)\n        lightgbm_score_duration = time() - tic\n        print(\"score: {:.4f}\".format(lightgbm_score))\n        print(\"fit duration: {:.3f}s,\".format(lightgbm_fit_duration))\n        print(\"score duration: {:.3f}s,\".format(lightgbm_score_duration))\n\n    xgb_score = None\n    xgb_fit_duration = None\n    xgb_score_duration = None\n    if args.xgboost:\n        print(\"Fitting an XGBoost model...\")\n        xgb_est = get_equivalent_estimator(est, lib=\"xgboost\", n_classes=args.n_classes)\n        xgb_est.set_params(nthread=n_threads)\n\n        tic = time()\n        xgb_est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        xgb_fit_duration = time() - tic\n        tic = time()\n        xgb_score = xgb_est.score(X_test, y_test)\n        xgb_score_duration = time() - tic\n        print(\"score: {:.4f}\".format(xgb_score))\n        print(\"fit duration: {:.3f}s,\".format(xgb_fit_duration))\n        print(\"score duration: {:.3f}s,\".format(xgb_score_duration))\n\n    cat_score = None\n    cat_fit_duration = None\n    cat_score_duration = None\n    if args.catboost:\n        print(\"Fitting a CatBoost model...\")\n        cat_est = get_equivalent_estimator(\n            est, lib=\"catboost\", n_classes=args.n_classes\n  "}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "plot_model_complexity_influence.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/applications", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "](**conf[\"tuned_params\"])\n\n        print(\"Benchmarking %s\" % estimator)\n        estimator.fit(conf[\"data\"][\"X_train\"], conf[\"data\"][\"y_train\"])\n        conf[\"postfit_hook\"](estimator)\n        complexity = conf[\"complexity_computer\"](estimator)\n        complexities.append(complexity)\n        start_time = time.time()\n        for _ in range(conf[\"n_samples\"]):\n            y_pred = estimator.predict(conf[\"data\"][\"X_test\"])\n        elapsed_time = (time.time() - start_time) / float(conf[\"n_samples\"])\n        prediction_times.append(elapsed_time)\n        pred_score = conf[\"prediction_performance_computer\"](\n            conf[\"data\"][\"y_test\"], y_pred\n        )\n        prediction_powers.append(pred_score)\n        print(\n            \"Complexity: %d | %s: %.4f | Pred. Time: %fs\\n\"\n            % (\n                complexity,\n                conf[\"prediction_performance_label\"],\n                pred_score,\n                elapsed_time,\n            )\n        )\n    return prediction_powers, prediction_times, complexities\n\n\n##############################################################################\n# Choose parameters\n# -----------------\n#\n# We choose the parameters for each of our estimators by making\n# a dictionary with all the necessary values.\n# ``changing_param`` is the name of the parameter which will vary in each\n# estimator.\n# Complexity will be defined by the ``complexity_label`` and calculated using\n# `complexity_computer`.\n# Also note that depending on the estimator type we are passing\n# different data.\n#\n\n\ndef _count_nonzero_coefficients(estimator):\n    a = estimator.coef_.toarray()\n    return np.count_nonzero(a)\n\n\nconfigurations = [\n    {\n        \"estimator\": SGDClassifier,\n        \"tuned_params\": {\n            \"penalty\": \"elasticnet\",\n            \"alpha\": 0.001,\n            \"loss\": \"modified_huber\",\n            \"fit_intercept\": True,\n            \"tol\": 1e-1,\n            \"n_iter_no_change\": 2,\n        },\n        \"changing_param\": \"l1_ratio\",\n        \"changing_param_"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "bench_hist_gradient_boosting_threading.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/benchmarks", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ights:\n    sample_weight = np.random.rand(len(X)) * 10\nelse:\n    sample_weight = None\n\nif sample_weight is not None:\n    (X_train_, X_test_, y_train_, y_test_, sample_weight_train_, _) = train_test_split(\n        X, y, sample_weight, test_size=0.5, random_state=0\n    )\nelse:\n    X_train_, X_test_, y_train_, y_test_ = train_test_split(\n        X, y, test_size=0.5, random_state=0\n    )\n    sample_weight_train_ = None\n\n\nsklearn_est = Estimator(\n    learning_rate=lr,\n    max_iter=n_trees,\n    max_bins=max_bins,\n    max_leaf_nodes=n_leaf_nodes,\n    early_stopping=False,\n    random_state=0,\n    verbose=0,\n)\nloss = args.loss\nif args.problem == \"classification\":\n    if loss == \"default\":\n        # loss='auto' does not work with get_equivalent_estimator()\n        loss = \"log_loss\"\nelse:\n    # regression\n    if loss == \"default\":\n        loss = \"squared_error\"\nsklearn_est.set_params(loss=loss)\n\n\nif args.print_params:\n    print(\"scikit-learn\")\n    pprint(sklearn_est.get_params())\n\n    for libname in [\"lightgbm\", \"xgboost\", \"catboost\"]:\n        if getattr(args, libname):\n            print(libname)\n            est = get_equivalent_estimator(\n                sklearn_est, lib=libname, n_classes=args.n_classes\n            )\n            pprint(est.get_params())\n\n\ndef one_run(n_threads, n_samples):\n    X_train = X_train_[:n_samples]\n    X_test = X_test_[:n_samples]\n    y_train = y_train_[:n_samples]\n    y_test = y_test_[:n_samples]\n    if sample_weight is not None:\n        sample_weight_train = sample_weight_train_[:n_samples]\n    else:\n        sample_weight_train = None\n    assert X_train.shape[0] == n_samples\n    assert X_test.shape[0] == n_samples\n    print(\"Fitting a sklearn model...\")\n    tic = time()\n    est = sklearn.base.clone(sklearn_est)\n\n    with threadpool_limits(n_threads, user_api=\"openmp\"):\n        est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        sklearn_fit_duration = time() - tic\n        tic = time()\n        sklearn_score = est.score(X_test, y_tes"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "bench_20newsgroups.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/benchmarks", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "(\n        \"-e\", \"--estimators\", nargs=\"+\", required=True, choices=ESTIMATORS\n    )\n    args = vars(parser.parse_args())\n\n    data_train = fetch_20newsgroups_vectorized(subset=\"train\")\n    data_test = fetch_20newsgroups_vectorized(subset=\"test\")\n    X_train = check_array(data_train.data, dtype=np.float32, accept_sparse=\"csc\")\n    X_test = check_array(data_test.data, dtype=np.float32, accept_sparse=\"csr\")\n    y_train = data_train.target\n    y_test = data_test.target\n\n    print(\"20 newsgroups\")\n    print(\"=============\")\n    print(f\"X_train.shape = {X_train.shape}\")\n    print(f\"X_train.format = {X_train.format}\")\n    print(f\"X_train.dtype = {X_train.dtype}\")\n    print(f\"X_train density = {X_train.nnz / np.prod(X_train.shape)}\")\n    print(f\"y_train {y_train.shape}\")\n    print(f\"X_test {X_test.shape}\")\n    print(f\"X_test.format = {X_test.format}\")\n    print(f\"X_test.dtype = {X_test.dtype}\")\n    print(f\"y_test {y_test.shape}\")\n    print()\n    print(\"Classifier Training\")\n    print(\"===================\")\n    accuracy, train_time, test_time = {}, {}, {}\n    for name in sorted(args[\"estimators\"]):\n        clf = ESTIMATORS[name]\n        try:\n            clf.set_params(random_state=0)\n        except (TypeError, ValueError):\n            pass\n\n        print(\"Training %s ... \" % name, end=\"\")\n        t0 = time()\n        clf.fit(X_train, y_train)\n        train_time[name] = time() - t0\n        t0 = time()\n        y_pred = clf.predict(X_test)\n        test_time[name] = time() - t0\n        accuracy[name] = accuracy_score(y_test, y_pred)\n        print(\"done\")\n\n    print()\n    print(\"Classification performance:\")\n    print(\"===========================\")\n    print()\n    print(\"%s %s %s %s\" % (\"Classifier  \", \"train-time\", \"test-time\", \"Accuracy\"))\n    print(\"-\" * 44)\n    for name in sorted(accuracy, key=accuracy.get):\n        print(\n            \"%s %s %s %s\"\n            % (\n                name.ljust(16),\n                (\"%.4fs\" % train_time[name]).center(10),\n                (\"%.4fs\" %"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "bench_hist_gradient_boosting.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/benchmarks", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e() - tic\n    tic = time()\n    sklearn_score = est.score(X_test, y_test)\n    sklearn_score_duration = time() - tic\n    print(\"score: {:.4f}\".format(sklearn_score))\n    print(\"fit duration: {:.3f}s,\".format(sklearn_fit_duration))\n    print(\"score duration: {:.3f}s,\".format(sklearn_score_duration))\n\n    lightgbm_score = None\n    lightgbm_fit_duration = None\n    lightgbm_score_duration = None\n    if args.lightgbm:\n        print(\"Fitting a LightGBM model...\")\n        lightgbm_est = get_equivalent_estimator(\n            est, lib=\"lightgbm\", n_classes=args.n_classes\n        )\n\n        tic = time()\n        lightgbm_est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        lightgbm_fit_duration = time() - tic\n        tic = time()\n        lightgbm_score = lightgbm_est.score(X_test, y_test)\n        lightgbm_score_duration = time() - tic\n        print(\"score: {:.4f}\".format(lightgbm_score))\n        print(\"fit duration: {:.3f}s,\".format(lightgbm_fit_duration))\n        print(\"score duration: {:.3f}s,\".format(lightgbm_score_duration))\n\n    xgb_score = None\n    xgb_fit_duration = None\n    xgb_score_duration = None\n    if args.xgboost:\n        print(\"Fitting an XGBoost model...\")\n        xgb_est = get_equivalent_estimator(est, lib=\"xgboost\", n_classes=args.n_classes)\n\n        tic = time()\n        xgb_est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        xgb_fit_duration = time() - tic\n        tic = time()\n        xgb_score = xgb_est.score(X_test, y_test)\n        xgb_score_duration = time() - tic\n        print(\"score: {:.4f}\".format(xgb_score))\n        print(\"fit duration: {:.3f}s,\".format(xgb_fit_duration))\n        print(\"score duration: {:.3f}s,\".format(xgb_score_duration))\n\n    cat_score = None\n    cat_fit_duration = None\n    cat_score_duration = None\n    if args.catboost:\n        print(\"Fitting a CatBoost model...\")\n        cat_est = get_equivalent_estimator(\n            est, lib=\"catboost\", n_classes=args.n_classes\n        )\n\n        tic = time()\n     "}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "bench_hist_gradient_boosting.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/benchmarks", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ion: {:.3f}s,\".format(lightgbm_score_duration))\n\n    xgb_score = None\n    xgb_fit_duration = None\n    xgb_score_duration = None\n    if args.xgboost:\n        print(\"Fitting an XGBoost model...\")\n        xgb_est = get_equivalent_estimator(est, lib=\"xgboost\", n_classes=args.n_classes)\n\n        tic = time()\n        xgb_est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        xgb_fit_duration = time() - tic\n        tic = time()\n        xgb_score = xgb_est.score(X_test, y_test)\n        xgb_score_duration = time() - tic\n        print(\"score: {:.4f}\".format(xgb_score))\n        print(\"fit duration: {:.3f}s,\".format(xgb_fit_duration))\n        print(\"score duration: {:.3f}s,\".format(xgb_score_duration))\n\n    cat_score = None\n    cat_fit_duration = None\n    cat_score_duration = None\n    if args.catboost:\n        print(\"Fitting a CatBoost model...\")\n        cat_est = get_equivalent_estimator(\n            est, lib=\"catboost\", n_classes=args.n_classes\n        )\n\n        tic = time()\n        cat_est.fit(X_train, y_train, sample_weight=sample_weight_train)\n        cat_fit_duration = time() - tic\n        tic = time()\n        cat_score = cat_est.score(X_test, y_test)\n        cat_score_duration = time() - tic\n        print(\"score: {:.4f}\".format(cat_score))\n        print(\"fit duration: {:.3f}s,\".format(cat_fit_duration))\n        print(\"score duration: {:.3f}s,\".format(cat_score_duration))\n\n    return (\n        sklearn_score,\n        sklearn_fit_duration,\n        sklearn_score_duration,\n        lightgbm_score,\n        lightgbm_fit_duration,\n        lightgbm_score_duration,\n        xgb_score,\n        xgb_fit_duration,\n        xgb_score_duration,\n        cat_score,\n        cat_fit_duration,\n        cat_score_duration,\n    )\n\n\nn_samples_list = [1000, 10000, 100000, 500000, 1000000, 5000000, 10000000]\nn_samples_list = [\n    n_samples for n_samples in n_samples_list if n_samples <= args.n_samples_max\n]\n\nsklearn_scores = []\nsklearn_fit_durations = []\nsklearn_score_durations = []\nl"}, {"start_line": 0, "end_line": 49, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/asv_benchmarks/benchmarks", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Benchmark suite for scikit-learn using ASV\"\"\"\n"}], "retrieved_count": 10, "cost_time": 1.2646372318267822}
{"question": "Where in the Scikit-learn codebase is the core estimator interface implemented?", "answer": null, "relative_code_list": null, "ground_truth": "The core estimator interface in scikit-learn is implemented across several key files and modules throughout the codebase. Here's where the core interface components are located:\n\n1. **Base Estimator Class (base.py)**:\n   - **BaseEstimator**: The fundamental base class that all estimators inherit from\n   - **get_params() and set_params()**: Core parameter management methods\n   - **clone()**: Function for creating copies of estimators\n   - **Parameter Validation**: Built-in parameter validation framework\n   - **Serialization Support**: Pickle-based serialization capabilities\n\n2. **Estimator Mixins (base.py)**:\n   - **ClassifierMixin**: Provides classifier-specific functionality\n   - **RegressorMixin**: Provides regressor-specific functionality\n   - **TransformerMixin**: Provides transformer-specific functionality\n   - **ClusterMixin**: Provides clustering-specific functionality\n   - **OutlierMixin**: Provides outlier detection functionality\n\n3. **Core Interface Methods**:\n   - **fit()**: The primary method that all estimators must implement\n   - **predict()**: Method for making predictions (predictors)\n   - **transform()**: Method for data transformation (transformers)\n   - **score()**: Method for model evaluation\n   - **fit_predict()**: Combined fit and predict for some estimators\n   - **fit_transform()**: Combined fit and transform for transformers\n\n4. **Validation Framework (utils/validation.py)**:\n   - **check_is_fitted()**: Validates that estimator has been fitted\n   - **validate_data()**: Comprehensive data validation\n   - **check_array()**: Array validation and conversion\n   - **check_X_y()**: Validation for supervised learning data\n   - **Input validation utilities**: Various validation helper functions\n\n5. **Parameter Management System**:\n   - **_get_param_names()**: Extracts parameter names from __init__\n   - **_validate_params()**: Validates parameter types and values\n   - **Parameter constraints**: _parameter_constraints dictionary\n   - **validate_params decorator**: Runtime parameter validation\n   - **Parameter routing**: Advanced parameter routing system\n\n6. **Estimator Tags System (utils/_tags.py)**:\n   - **Tags class**: Defines estimator capabilities and properties\n   - **__sklearn_tags__()**: Method for defining estimator tags\n   - **Tag inheritance**: Automatic tag inheritance from mixins\n   - **Runtime tag determination**: Tags that depend on parameters\n   - **Tag validation**: Validation of estimator tags\n\n7. **Cloning and Serialization**:\n   - **clone() function**: Creates independent copies of estimators\n   - **__sklearn_clone__()**: Customizable cloning behavior\n   - **Pickle support**: Built-in serialization support\n   - **State preservation**: Maintains estimator state during cloning\n   - **Deep cloning**: Support for nested estimator structures\n\n8. **Metadata Routing System**:\n   - **_MetadataRequester**: Base class for metadata routing\n   - **get_metadata_routing()**: Defines metadata requirements\n   - **process_routing()**: Processes metadata routing requests\n   - **Metadata validation**: Validates metadata requirements\n   - **Routing propagation**: Propagates metadata through pipelines\n\n9. **HTML Representation**:\n   - **ReprHTMLMixin**: Provides HTML representation for estimators\n   - **estimator_html_repr()**: Generates HTML representation\n   - **Jupyter integration**: Rich display in Jupyter notebooks\n   - **Documentation links**: Links to documentation\n   - **Parameter display**: HTML display of parameters\n\n10. **Interface Validation and Testing**:\n    - **check_estimator()**: Validates estimator interface compliance\n    - **parametrize_with_checks**: Pytest decorator for testing\n    - **Common checks**: Standardized tests for all estimators\n    - **Interface compliance**: Ensures adherence to scikit-learn API\n    - **Regression testing**: Prevents interface regressions", "score": null, "retrieved_content": [{"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "aram2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n\n    # _sklearn_output_config is used by `set_output` to configure the output\n    # container of an estimator.\n    if hasattr(estimator, \"_sklearn_output_config\"):\n        new_object._sklearn_output_config = copy.deepcopy(\n            estimator._sklearn_output_config\n        )\n    return new_object\n\n\nclass BaseEstimator(ReprHTMLMixin, _HTMLDocumentationLinkMixin, _MetadataRequester):\n    \"\"\"Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])\n    \"\"\"\n\n    _html_"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n estimator instead of a class.\"\n                )\n            else:\n                raise TypeError(\n                    \"Cannot clone object '%s' (type %s): \"\n                    \"it does not seem to be a scikit-learn \"\n                    \"estimator as it does not implement a \"\n                    \"'get_params' method.\" % (repr(estimator), type(estimator))\n                )\n\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        new_object_params[name] = clone(param, safe=False)\n\n    new_object = klass(**new_object_params)\n    try:\n        new_object._metadata_request = copy.deepcopy(estimator._metadata_request)\n    except AttributeError:\n        pass\n\n    params_set = new_object.get_params(deep=False)\n\n    # quick sanity check of the parameters of the clone\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is not param2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n\n    # _sklearn_output_config is used by `set_output` to configure the output\n    # container of an estimator.\n    if hasattr(estimator, \"_sklearn_output_config\"):\n        new_object._sklearn_output_config = copy.deepcopy(\n            estimator._sklearn_output_config\n        )\n    return new_object\n\n\nclass BaseEstimator(ReprHTMLMixin, _HTMLDocumentationLinkMixin, _MetadataRequester):\n    \"\"\"Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])\n    \"\"\"\n\n    _html_repr = estimator_html_repr\n\n    @classmethod\n    def _get_param_names(cls):\n        \"\"\"Get parameter names for the estimator\"\"\"\n        # fetch the constructor or the original constructor before\n        # deprecation wrapping if any\n        init = getattr(cls.__init__, \"deprecated_original\", cls.__init__)\n        if init is object.__init__:\n            # No explicit constructor to introspect\n            return []\n\n        # introspect the constructor arguments to find the model parameters\n        # to represent\n        init_signature = inspect.signature(init)\n        # Consider the constructor parameters excluding 'self'\n        parameters = [\n            p\n            for p in init_signature.parameters.values()\n            if p.name != \"self\" and p.kind != p.VAR_KEYWORD\n        ]\n        for p in parameters:\n            if p.kind == p.VAR_POSITIONAL:\n                raise RuntimeError(\n                    \"scikit-learn estimators should always \"\n                    \"specify their para"}, {"start_line": 0, "end_line": 162, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/frozen", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nfrom sklearn.frozen._frozen import FrozenEstimator\n\n__all__ = [\"FrozenEstimator\"]\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Base classes for all estimators and various utility functions.\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport copy\nimport functools\nimport inspect\nimport platform\nimport re\nimport warnings\nfrom collections import defaultdict\n\nimport numpy as np\n\nfrom sklearn import __version__\nfrom sklearn._config import config_context, get_config\nfrom sklearn.exceptions import InconsistentVersionWarning\nfrom sklearn.utils._metadata_requests import _MetadataRequester, _routing_enabled\nfrom sklearn.utils._missing import is_scalar_nan\nfrom sklearn.utils._param_validation import validate_parameter_constraints\nfrom sklearn.utils._repr_html.base import ReprHTMLMixin, _HTMLDocumentationLinkMixin\nfrom sklearn.utils._repr_html.estimator import estimator_html_repr\nfrom sklearn.utils._repr_html.params import ParamsDict\nfrom sklearn.utils._set_output import _SetOutputMixin\nfrom sklearn.utils._tags import (\n    ClassifierTags,\n    RegressorTags,\n    Tags,\n    TargetTags,\n    TransformerTags,\n    get_tags,\n)\nfrom sklearn.utils.fixes import _IS_32BIT\nfrom sklearn.utils.validation import (\n    _check_feature_names_in,\n    _generate_get_feature_names_out,\n    _is_fitted,\n    check_array,\n    check_is_fitted,\n)\n\n\ndef clone(estimator, *, safe=True):\n    \"\"\"Construct a new unfitted estimator with the same parameters.\n\n    Clone does a deep copy of the model in an estimator\n    without actually copying attached data. It returns a new estimator\n    with the same parameters that has not been fitted on any data.\n\n    .. versionchanged:: 1.3\n        Delegates to `estimator.__sklearn_clone__` if the method exists.\n\n    Parameters\n    ----------\n    estimator : {list, tuple, set} of estimator instance or a single \\\n            estimator instance\n        The estimator or group of estimators to be cloned.\n    safe : bool, default=True\n        If safe is False, clone will fall back to a deep copy on objects\n        that are not estimators. Ignored if `estimator.__s"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ators = n_estimators\n        self.estimator_params = estimator_params\n\n        # Don't instantiate estimators now! Parameters of estimator might\n        # still change. Eg., when grid-searching with the nested object syntax.\n        # self.estimators_ needs to be filled by the derived classes in fit.\n\n    def _validate_estimator(self, default=None):\n        \"\"\"Check the base estimator.\n\n        Sets the `estimator_` attributes.\n        \"\"\"\n        if self.estimator is not None:\n            self.estimator_ = self.estimator\n        else:\n            self.estimator_ = default\n\n    def _make_estimator(self, append=True, random_state=None):\n        \"\"\"Make and configure a copy of the `estimator_` attribute.\n\n        Warning: This method should be used to properly instantiate new\n        sub-estimators.\n        \"\"\"\n        estimator = clone(self.estimator_)\n        estimator.set_params(**{p: getattr(self, p) for p in self.estimator_params})\n\n        if random_state is not None:\n            _set_random_states(estimator, random_state)\n\n        if append:\n            self.estimators_.append(estimator)\n\n        return estimator\n\n    def __len__(self):\n        \"\"\"Return the number of estimators in the ensemble.\"\"\"\n        return len(self.estimators_)\n\n    def __getitem__(self, index):\n        \"\"\"Return the index'th estimator in the ensemble.\"\"\"\n        return self.estimators_[index]\n\n    def __iter__(self):\n        \"\"\"Return iterator over estimators in the ensemble.\"\"\"\n        return iter(self.estimators_)\n\n\ndef _partition_estimators(n_estimators, n_jobs):\n    \"\"\"Private function used to partition estimators between jobs.\"\"\"\n    # Compute the number of jobs\n    n_jobs = min(effective_n_jobs(n_jobs), n_estimators)\n\n    # Partition estimators between jobs\n    n_estimators_per_job = np.full(n_jobs, n_estimators // n_jobs, dtype=int)\n    n_estimators_per_job[: n_estimators % n_jobs] += 1\n    starts = np.cumsum(n_estimators_per_job)\n\n    return n_jobs, n_estimators_per_job.tolist()"}, {"start_line": 21000, "end_line": 23000, "belongs_to": {"file_name": "multioutput.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "r of outputs of Y for fit {0} and\"\n                \" score {1} should be same\".format(n_outputs_, y.shape[1])\n            )\n        y_pred = self.predict(X)\n        return np.mean(np.all(y == y_pred, axis=1))\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        # FIXME\n        tags._skip_test = True\n        return tags\n\n\ndef _available_if_base_estimator_has(attr):\n    \"\"\"Return a function to check if `base_estimator` or `estimators_` has `attr`.\n\n    Helper for Chain implementations.\n    \"\"\"\n\n    def _check(self):\n        return hasattr(self._get_estimator(), attr) or all(\n            hasattr(est, attr) for est in self.estimators_\n        )\n\n    return available_if(_check)\n\n\nclass _BaseChain(BaseEstimator, metaclass=ABCMeta):\n    _parameter_constraints: dict = {\n        \"base_estimator\": [\n            HasMethods([\"fit\", \"predict\"]),\n            StrOptions({\"deprecated\"}),\n        ],\n        \"estimator\": [\n            HasMethods([\"fit\", \"predict\"]),\n            Hidden(None),\n        ],\n        \"order\": [\"array-like\", StrOptions({\"random\"}), None],\n        \"cv\": [\"cv_object\", StrOptions({\"prefit\"})],\n        \"random_state\": [\"random_state\"],\n        \"verbose\": [\"boolean\"],\n    }\n\n    # TODO(1.9): Remove base_estimator\n    def __init__(\n        self,\n        estimator=None,\n        *,\n        order=None,\n        cv=None,\n        random_state=None,\n        verbose=False,\n        base_estimator=\"deprecated\",\n    ):\n        self.estimator = estimator\n        self.base_estimator = base_estimator\n        self.order = order\n        self.cv = cv\n        self.random_state = random_state\n        self.verbose = verbose\n\n    # TODO(1.8): This is a temporary getter method to validate input wrt deprecation.\n    # It was only included to avoid relying on the presence of self.estimator_\n    def _get_estimator(self):\n        \"\"\"Get and validate estimator.\"\"\"\n\n        if self.estimator is not None and (self.base_estimator != \"deprecated\"):\n            rais"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ataframe_column_names_consistency,\n    check_decision_proba_consistency,\n    check_dict_unchanged,\n    check_dont_overwrite_parameters,\n    check_estimator,\n    check_estimator_cloneable,\n    check_estimator_repr,\n    check_estimator_sparse_array,\n    check_estimator_sparse_matrix,\n    check_estimator_sparse_tag,\n    check_estimator_tags_renamed,\n    check_estimators_nan_inf,\n    check_estimators_overwrite_params,\n    check_estimators_unfitted,\n    check_fit_check_is_fitted,\n    check_fit_score_takes_y,\n    check_methods_sample_order_invariance,\n    check_methods_subset_invariance,\n    check_mixin_order,\n    check_no_attributes_set_in_init,\n    check_outlier_contamination,\n    check_outlier_corruption,\n    check_parameters_default_constructible,\n    check_positive_only_tag_during_fit,\n    check_regressor_data_not_an_array,\n    check_requires_y_none,\n    check_sample_weights_pandas_series,\n    check_set_params,\n    estimator_checks_generator,\n    set_random_state,\n)\nfrom sklearn.utils.fixes import CSR_CONTAINERS, SPARRAY_PRESENT\nfrom sklearn.utils.metaestimators import available_if\nfrom sklearn.utils.multiclass import type_of_target\nfrom sklearn.utils.validation import (\n    check_array,\n    check_is_fitted,\n    check_X_y,\n    validate_data,\n)\n\n\nclass CorrectNotFittedError(ValueError):\n    \"\"\"Exception class to raise if estimator is used before fitting.\n\n    Like NotFittedError, it inherits from ValueError, but not from\n    AttributeError. Used for testing only.\n    \"\"\"\n\n\nclass BaseBadClassifier(ClassifierMixin, BaseEstimator):\n    def fit(self, X, y):\n        return self\n\n    def predict(self, X):\n        return np.ones(X.shape[0])\n\n\nclass ChangesDict(BaseEstimator):\n    def __init__(self, key=0):\n        self.key = key\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n    def predict(self, X):\n        X = check_array(X)\n        self.key = 1000\n        return np.ones(X.shape[0])\n\n\nclass SetsWrongAttribute(BaseEstimator):\n  "}, {"start_line": 22000, "end_line": 24000, "belongs_to": {"file_name": "_bagging.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "       caller=\"predict_proba\", callee=\"predict_proba\"\n                )\n            )\n\n        else:\n            (\n                method_mapping.add(caller=\"predict\", callee=\"predict\").add(\n                    caller=\"predict_proba\", callee=\"predict\"\n                )\n            )\n\n        # the router needs to be built depending on whether the sub-estimator has a\n        # `predict_log_proba` method (as BaggingClassifier decides dynamically at\n        # runtime):\n        if hasattr(self._get_estimator(), \"predict_log_proba\"):\n            method_mapping.add(caller=\"predict_log_proba\", callee=\"predict_log_proba\")\n\n        else:\n            # if `predict_log_proba` is not available in BaggingClassifier's\n            # sub-estimator, the routing should go to its `predict_proba` if it is\n            # available or else to its `predict` method; according to how\n            # `sample_weight` is passed to the respective methods dynamically at\n            # runtime:\n            if hasattr(self._get_estimator(), \"predict_proba\"):\n                method_mapping.add(caller=\"predict_log_proba\", callee=\"predict_proba\")\n\n            else:\n                method_mapping.add(caller=\"predict_log_proba\", callee=\"predict\")\n\n        router.add(estimator=self._get_estimator(), method_mapping=method_mapping)\n        return router\n\n    @abstractmethod\n    def _get_estimator(self):\n        \"\"\"Resolve which estimator to return.\"\"\"\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.sparse = get_tags(self._get_estimator()).input_tags.sparse\n        tags.input_tags.allow_nan = get_tags(self._get_estimator()).input_tags.allow_nan\n        return tags\n\n\nclass BaggingClassifier(ClassifierMixin, BaseBagging):\n    \"\"\"A Bagging classifier.\n\n    A Bagging classifier is an ensemble meta-estimator that fits base\n    classifiers each on random subsets of the original dataset and then\n    aggregate their individual predictions (either by voting or by avera"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "_classification_threshold.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "target\nfrom sklearn.utils.parallel import Parallel, delayed\nfrom sklearn.utils.validation import (\n    _check_method_params,\n    _estimator_has,\n    _num_samples,\n    check_is_fitted,\n    indexable,\n)\n\n\ndef _check_is_fitted(estimator):\n    try:\n        check_is_fitted(estimator.estimator)\n    except NotFittedError:\n        check_is_fitted(estimator, \"estimator_\")\n\n\nclass BaseThresholdClassifier(ClassifierMixin, MetaEstimatorMixin, BaseEstimator):\n    \"\"\"Base class for binary classifiers that set a non-default decision threshold.\n\n    In this base class, we define the following interface:\n\n    - the validation of common parameters in `fit`;\n    - the different prediction methods that can be used with the classifier.\n\n    .. versionadded:: 1.5\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        The binary classifier, fitted or not, for which we want to optimize\n        the decision threshold used during `predict`.\n\n    response_method : {\"auto\", \"decision_function\", \"predict_proba\"}, default=\"auto\"\n        Methods by the classifier `estimator` corresponding to the\n        decision function for which we want to find a threshold. It can be:\n\n        * if `\"auto\"`, it will try to invoke, for each classifier,\n          `\"predict_proba\"` or `\"decision_function\"` in that order.\n        * otherwise, one of `\"predict_proba\"` or `\"decision_function\"`.\n          If the method is not implemented by the classifier, it will raise an\n          error.\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"estimator\": [\n            HasMethods([\"fit\", \"predict_proba\"]),\n            HasMethods([\"fit\", \"decision_function\"]),\n        ],\n        \"response_method\": [StrOptions({\"auto\", \"predict_proba\", \"decision_function\"})],\n    }\n\n    def __init__(self, estimator, *, response_method=\"auto\"):\n        self.estimator = estimator\n        self.response_method = response_method\n\n    def _get_response_method(self):\n        \"\"\"Define the response method.\"\"\"\n        if self."}], "retrieved_count": 10, "cost_time": 1.247986078262329}
{"question": "Where does Scikit-learn implement its cross-validation logic?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn implements its cross-validation logic across multiple specialized modules and components. Here's where the cross-validation system is implemented:\n\n1. **Core Cross-Validation Functions (model_selection/_validation.py)**:\n   - **cross_validate()**: Main function for cross-validation with multiple metrics\n   - **cross_val_score()**: Simplified cross-validation for single metrics\n   - **cross_val_predict()**: Cross-validation predictions for diagnostics\n   - **_fit_and_score()**: Internal function for fitting and scoring in each fold\n   - **_aggregate_score_dicts()**: Aggregates results from multiple CV folds\n\n2. **Cross-Validation Splitters (model_selection/_split.py)**:\n   - **KFold**: Basic k-fold cross-validation\n   - **StratifiedKFold**: Stratified k-fold for classification\n   - **ShuffleSplit**: Random train/test splits\n   - **StratifiedShuffleSplit**: Stratified random splits\n   - **TimeSeriesSplit**: Time series cross-validation\n   - **GroupKFold**: Group-based cross-validation\n   - **LeaveOneOut**: Leave-one-out cross-validation\n   - **LeavePOut**: Leave-p-out cross-validation\n\n3. **Cross-Validation Estimators**:\n   - **GridSearchCV**: Exhaustive parameter search with CV\n   - **RandomizedSearchCV**: Random parameter search with CV\n   - **HalvingGridSearchCV**: Successive halving with CV\n   - **HalvingRandomSearchCV**: Random search with successive halving\n   - **Cross-validation estimators**: EstimatorCV classes (e.g., LogisticRegressionCV)\n\n4. **Validation Curves and Learning Curves**:\n   - **validation_curve()**: Validation curves for parameter analysis\n   - **learning_curve()**: Learning curves for model analysis\n   - **Learning curve computation**: Internal learning curve logic\n   - **Curve plotting**: Visualization of validation and learning curves\n\n5. **Parallel Processing Integration**:\n   - **Joblib Integration**: Parallel processing of CV folds\n   - **n_jobs parameter**: Configurable parallelization\n   - **Pre-dispatch**: Control over job dispatching\n   - **Memory management**: Efficient memory usage during parallel CV\n   - **Error handling**: Robust error handling in parallel execution\n\n6. **Scoring and Metrics Integration**:\n   - **Multiple metrics**: Support for multiple evaluation metrics\n   - **Custom scoring**: Custom scoring functions\n   - **Metric aggregation**: Statistical aggregation of CV results\n   - **Score timing**: Measurement of fit and score times\n   - **Error scoring**: Handling of failed CV folds\n\n7. **Data Splitting Utilities**:\n   - **train_test_split()**: Simple train/test splitting\n   - **Stratification**: Automatic stratification for classification\n   - **Group splitting**: Support for group-based splitting\n   - **Time series splitting**: Specialized splitting for temporal data\n   - **Random state management**: Reproducible splitting\n\n8. **Advanced Cross-Validation Features**:\n   - **Metadata routing**: Advanced metadata handling in CV\n   - **Parameter passing**: Parameter routing to estimators and scorers\n   - **Indices return**: Option to return train/test indices\n   - **Estimator return**: Option to return fitted estimators\n   - **Train score return**: Option to return training scores\n\n9. **Algorithm-Specific CV**:\n   - **SVM CV**: LibSVM cross-validation implementation\n   - **Linear model CV**: Built-in CV for linear models\n   - **Neural network CV**: Early stopping validation\n   - **Gradient boosting CV**: Validation fraction handling\n   - **Ensemble CV**: CV strategies for ensemble methods\n\n10. **Performance and Optimization**:\n    - **Memory efficiency**: Optimized memory usage for large datasets\n    - **Caching**: Caching of intermediate results\n    - **Early stopping**: Early stopping in iterative algorithms\n    - **Warm starting**: Warm starting for efficient CV\n    - **Resource management**: Optimal allocation of computational resources", "score": null, "retrieved_content": [{"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rn.utils.validation import _num_samples, check_array, column_or_1d\n\n__all__ = [\n    \"BaseCrossValidator\",\n    \"GroupKFold\",\n    \"GroupShuffleSplit\",\n    \"KFold\",\n    \"LeaveOneGroupOut\",\n    \"LeaveOneOut\",\n    \"LeavePGroupsOut\",\n    \"LeavePOut\",\n    \"PredefinedSplit\",\n    \"RepeatedKFold\",\n    \"RepeatedStratifiedKFold\",\n    \"ShuffleSplit\",\n    \"StratifiedGroupKFold\",\n    \"StratifiedKFold\",\n    \"StratifiedShuffleSplit\",\n    \"check_cv\",\n    \"train_test_split\",\n]\n\n\nclass _UnsupportedGroupCVMixin:\n    \"\"\"Mixin for splitters that do not support Groups.\"\"\"\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            The target variable for supervised learning problems.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is not None:\n            warnings.warn(\n                f\"The groups parameter is ignored by {self.__class__.__name__}\",\n                UserWarning,\n            )\n        return super().split(X, y, groups=groups)\n\n\nclass GroupsConsumerMixin(_MetadataRequester):\n    \"\"\"A Mixin to ``groups`` by default.\n\n    This Mixin makes the object to request ``groups`` by default as ``True``.\n\n    .. versionadded:: 1.3\n    \"\"\"\n\n    __metadata_request__split = {\"groups\": True}\n\n\nclass BaseCrossValidator(_MetadataRequester, metaclass=ABCMeta):\n    \"\"\"Base class for all cross-validators.\n\n    Implementations must define `_iter_test_masks` or `_iter_test_indices`.\n    \"\"\"\n\n    # This indicates that by default C"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_squared_error,\n    precision_recall_fscore_support,\n    precision_score,\n    r2_score,\n)\nfrom sklearn.metrics._scorer import _MultimetricScorer\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    GroupKFold,\n    GroupShuffleSplit,\n    KFold,\n    LeaveOneGroupOut,\n    LeaveOneOut,\n    LeavePGroupsOut,\n    ShuffleSplit,\n    StratifiedKFold,\n    cross_val_predict,\n    cross_val_score,\n    cross_validate,\n    learning_curve,\n    permutation_test_score,\n    validation_curve,\n)\nfrom sklearn.model_selection._validation import (\n    _check_is_permutation,\n    _fit_and_score,\n    _score,\n)\nfrom sklearn.model_selection.tests.common import OneTimeSplitter\nfrom sklearn.model_selection.tests.test_search import FailingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, scale\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tests.metadata_routing_common import (\n    ConsumingClassifier,\n    ConsumingScorer,\n    ConsumingSplitter,\n    _Registry,\n    check_recorded_metadata,\n)\nfrom sklearn.utils import shuffle\nfrom sklearn.utils._mocking import CheckingClassifier, MockDataFrame\nfrom sklearn.utils._testing import (\n    assert_allclose,\n    assert_almost_equal,\n    assert_array_almost_equal,\n    assert_array_equal,\n)\nfrom sklearn.utils.fixes import COO_CONTAINERS, CSR_CONTAINERS\nfrom sklearn.utils.validation import _num_samples\n\n\nclass MockImprovingEstimator(BaseEstimator):\n    \"\"\"Dummy classifier to test the learning curve\"\"\"\n\n    def __init__(self, n_max_train_sizes):\n        self.n_max_train_sizes = n_max_train_sizes\n        self.train_sizes = 0\n        self.X_subset = None\n\n    def fit(self, X_subset, y_subset=None):\n        self.X_subset = X_subset\n        self.train_sizes = X_subset.shape[0]\n        return self\n\n    def predict(self, X):\n        raise NotImplementedError\n\n   "}, {"start_line": 57000, "end_line": 59000, "belongs_to": {"file_name": "test_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ")\n    cv = check_cv(3, y_multioutput, classifier=True)\n    np.testing.assert_equal(list(KFold(3).split(X)), list(cv.split(X)))\n\n    with pytest.raises(ValueError):\n        check_cv(cv=\"lolo\")\n\n\ndef test_cv_iterable_wrapper():\n    kf_iter = KFold().split(X, y)\n    kf_iter_wrapped = check_cv(kf_iter)\n    # Since the wrapped iterable is enlisted and stored,\n    # split can be called any number of times to produce\n    # consistent results.\n    np.testing.assert_equal(\n        list(kf_iter_wrapped.split(X, y)), list(kf_iter_wrapped.split(X, y))\n    )\n    # If the splits are randomized, successive calls to split yields different\n    # results\n    kf_randomized_iter = KFold(shuffle=True, random_state=0).split(X, y)\n    kf_randomized_iter_wrapped = check_cv(kf_randomized_iter)\n    # numpy's assert_array_equal properly compares nested lists\n    np.testing.assert_equal(\n        list(kf_randomized_iter_wrapped.split(X, y)),\n        list(kf_randomized_iter_wrapped.split(X, y)),\n    )\n\n    try:\n        splits_are_equal = True\n        np.testing.assert_equal(\n            list(kf_iter_wrapped.split(X, y)),\n            list(kf_randomized_iter_wrapped.split(X, y)),\n        )\n    except AssertionError:\n        splits_are_equal = False\n    assert not splits_are_equal, (\n        \"If the splits are randomized, \"\n        \"successive calls to split should yield different results\"\n    )\n\n\n@pytest.mark.parametrize(\"kfold\", [GroupKFold, StratifiedGroupKFold])\n@pytest.mark.parametrize(\"shuffle\", [True, False])\ndef test_group_kfold(kfold, shuffle, global_random_seed):\n    rng = np.random.RandomState(global_random_seed)\n\n    # Parameters of the test\n    n_groups = 15\n    n_samples = 1000\n    n_splits = 5\n\n    X = y = np.ones(n_samples)\n\n    # Construct the test data\n    tolerance = 0.05 * n_samples  # 5 percent error allowed\n    groups = rng.randint(0, n_groups, n_samples)\n\n    ideal_n_groups_per_fold = n_samples // n_splits\n\n    len(np.unique(groups))\n    # Get the test fold indices from the t"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "       return splitter.split(X, y, groups=groups)\n    else:\n        return splitter.split(X, y)\n\n\ndef test_cross_validator_with_default_params():\n    n_samples = 4\n    n_unique_groups = 4\n    n_splits = 2\n    p = 2\n    n_shuffle_splits = 10  # (the default value)\n\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    X_1d = np.array([1, 2, 3, 4])\n    y = np.array([1, 1, 2, 2])\n    groups = np.array([1, 2, 3, 4])\n    loo = LeaveOneOut()\n    lpo = LeavePOut(p)\n    kf = KFold(n_splits)\n    skf = StratifiedKFold(n_splits)\n    lolo = LeaveOneGroupOut()\n    lopo = LeavePGroupsOut(p)\n    ss = ShuffleSplit(random_state=0)\n    ps = PredefinedSplit([1, 1, 2, 2])  # n_splits = np of unique folds = 2\n    sgkf = StratifiedGroupKFold(n_splits)\n\n    loo_repr = \"LeaveOneOut()\"\n    lpo_repr = \"LeavePOut(p=2)\"\n    kf_repr = \"KFold(n_splits=2, random_state=None, shuffle=False)\"\n    skf_repr = \"StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\"\n    lolo_repr = \"LeaveOneGroupOut()\"\n    lopo_repr = \"LeavePGroupsOut(n_groups=2)\"\n    ss_repr = (\n        \"ShuffleSplit(n_splits=10, random_state=0, test_size=None, train_size=None)\"\n    )\n    ps_repr = \"PredefinedSplit(test_fold=array([1, 1, 2, 2]))\"\n    sgkf_repr = \"StratifiedGroupKFold(n_splits=2, random_state=None, shuffle=False)\"\n\n    n_splits_expected = [\n        n_samples,\n        comb(n_samples, p),\n        n_splits,\n        n_splits,\n        n_unique_groups,\n        comb(n_unique_groups, p),\n        n_shuffle_splits,\n        2,\n        n_splits,\n    ]\n\n    for i, (cv, cv_repr) in enumerate(\n        zip(\n            [loo, lpo, kf, skf, lolo, lopo, ss, ps, sgkf],\n            [\n                loo_repr,\n                lpo_repr,\n                kf_repr,\n                skf_repr,\n                lolo_repr,\n                lopo_repr,\n                ss_repr,\n                ps_repr,\n                sgkf_repr,\n            ],\n        )\n    ):\n        # Test if get_n_splits works correctly\n        assert n_splits_expected[i] == cv"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Tools for model selection, such as cross validation and hyper-parameter tuning.\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport typing\n\nfrom sklearn.model_selection._classification_threshold import (\n    FixedThresholdClassifier,\n    TunedThresholdClassifierCV,\n)\nfrom sklearn.model_selection._plot import LearningCurveDisplay, ValidationCurveDisplay\nfrom sklearn.model_selection._search import (\n    GridSearchCV,\n    ParameterGrid,\n    ParameterSampler,\n    RandomizedSearchCV,\n)\nfrom sklearn.model_selection._split import (\n    BaseCrossValidator,\n    BaseShuffleSplit,\n    GroupKFold,\n    GroupShuffleSplit,\n    KFold,\n    LeaveOneGroupOut,\n    LeaveOneOut,\n    LeavePGroupsOut,\n    LeavePOut,\n    PredefinedSplit,\n    RepeatedKFold,\n    RepeatedStratifiedKFold,\n    ShuffleSplit,\n    StratifiedGroupKFold,\n    StratifiedKFold,\n    StratifiedShuffleSplit,\n    TimeSeriesSplit,\n    check_cv,\n    train_test_split,\n)\nfrom sklearn.model_selection._validation import (\n    cross_val_predict,\n    cross_val_score,\n    cross_validate,\n    learning_curve,\n    permutation_test_score,\n    validation_curve,\n)\n\nif typing.TYPE_CHECKING:\n    # Avoid errors in type checkers (e.g. mypy) for experimental estimators.\n    # TODO: remove this check once the estimator is no longer experimental.\n    from sklearn.model_selection._search_successive_halving import (\n        HalvingGridSearchCV,\n        HalvingRandomSearchCV,\n    )\n\n\n__all__ = [\n    \"BaseCrossValidator\",\n    \"BaseShuffleSplit\",\n    \"FixedThresholdClassifier\",\n    \"GridSearchCV\",\n    \"GroupKFold\",\n    \"GroupShuffleSplit\",\n    \"HalvingGridSearchCV\",\n    \"HalvingRandomSearchCV\",\n    \"KFold\",\n    \"LearningCurveDisplay\",\n    \"LeaveOneGroupOut\",\n    \"LeaveOneOut\",\n    \"LeavePGroupsOut\",\n    \"LeavePOut\",\n    \"ParameterGrid\",\n    \"ParameterSampler\",\n    \"PredefinedSplit\",\n    \"RandomizedSearchCV\",\n    \"RepeatedKFold\",\n    \"RepeatedStratifiedKFold\",\n    \"ShuffleSplit\",\n    \"StratifiedGroupKFold\",\n  "}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "        # the user is not calling `fit` directly, so we change the message\n            # to make it more suitable for this case.\n            raise UnsetMetadataPassedError(\n                message=str(e).replace(\"cross_validate.fit\", \"cross_validate\"),\n                unrequested_params=e.unrequested_params,\n                routed_params=e.routed_params,\n            )\n    else:\n        routed_params = Bunch()\n        routed_params.splitter = Bunch(split={\"groups\": groups})\n        routed_params.estimator = Bunch(fit=params)\n        routed_params.scorer = Bunch(score={})\n\n    indices = cv.split(X, y, **routed_params.splitter.split)\n    if return_indices:\n        # materialize the indices since we need to store them in the returned dict\n        indices = list(indices)\n\n    # We clone the estimator to make sure that all the folds are\n    # independent, and that it is pickle-able.\n    parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n    results = parallel(\n        delayed(_fit_and_score)(\n            clone(estimator),\n            X,\n            y,\n            scorer=scorers,\n            train=train,\n            test=test,\n            verbose=verbose,\n            parameters=None,\n            fit_params=routed_params.estimator.fit,\n            score_params=routed_params.scorer.score,\n            return_train_score=return_train_score,\n            return_times=True,\n            return_estimator=return_estimator,\n            error_score=error_score,\n        )\n        for train, test in indices\n    )\n\n    _warn_or_raise_about_fit_failures(results, error_score)\n\n    # For callable scoring, the return type is only know after calling. If the\n    # return type is a dictionary, the error scores can now be inserted with\n    # the correct key.\n    if callable(scoring):\n        _insert_error_scores(results, error_score)\n\n    results = _aggregate_score_dicts(results)\n\n    ret = {}\n    ret[\"fit_time\"] = results[\"fit_time\"]\n    ret[\"score_time\"] = results[\""}, {"start_line": 56000, "end_line": 58000, "belongs_to": {"file_name": "test_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ry))\n    )\n\n    y_multiclass = np.array([0, 1, 0, 1, 2, 1, 2, 0, 2])\n    cv = check_cv(3, y_multiclass, classifier=True)\n    np.testing.assert_equal(\n        list(StratifiedKFold(3).split(X, y_multiclass)), list(cv.split(X, y_multiclass))\n    )\n    # also works with 2d multiclass\n    y_multiclass_2d = y_multiclass.reshape(-1, 1)\n    cv = check_cv(3, y_multiclass_2d, classifier=True)\n    np.testing.assert_equal(\n        list(StratifiedKFold(3).split(X, y_multiclass_2d)),\n        list(cv.split(X, y_multiclass_2d)),\n    )\n\n    assert not np.all(\n        next(StratifiedKFold(3).split(X, y_multiclass_2d))[0]\n        == next(KFold(3).split(X, y_multiclass_2d))[0]\n    )\n\n    X = np.ones(5)\n    y_multilabel = np.array(\n        [[0, 0, 0, 0], [0, 1, 1, 0], [0, 0, 0, 1], [1, 1, 0, 1], [0, 0, 1, 0]]\n    )\n    cv = check_cv(3, y_multilabel, classifier=True)\n    np.testing.assert_equal(list(KFold(3).split(X)), list(cv.split(X)))\n\n    y_multioutput = np.array([[1, 2], [0, 3], [0, 0], [3, 1], [2, 0]])\n    cv = check_cv(3, y_multioutput, classifier=True)\n    np.testing.assert_equal(list(KFold(3).split(X)), list(cv.split(X)))\n\n    with pytest.raises(ValueError):\n        check_cv(cv=\"lolo\")\n\n\ndef test_cv_iterable_wrapper():\n    kf_iter = KFold().split(X, y)\n    kf_iter_wrapped = check_cv(kf_iter)\n    # Since the wrapped iterable is enlisted and stored,\n    # split can be called any number of times to produce\n    # consistent results.\n    np.testing.assert_equal(\n        list(kf_iter_wrapped.split(X, y)), list(kf_iter_wrapped.split(X, y))\n    )\n    # If the splits are randomized, successive calls to split yields different\n    # results\n    kf_randomized_iter = KFold(shuffle=True, random_state=0).split(X, y)\n    kf_randomized_iter_wrapped = check_cv(kf_randomized_iter)\n    # numpy's assert_array_equal properly compares nested lists\n    np.testing.assert_equal(\n        list(kf_randomized_iter_wrapped.split(X, y)),\n        list(kf_randomized_iter_wrapped.split(X, y)),\n    )\n\n    try:\n   "}, {"start_line": 1000, "end_line": 2861, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ation import (\n    cross_val_predict,\n    cross_val_score,\n    cross_validate,\n    learning_curve,\n    permutation_test_score,\n    validation_curve,\n)\n\nif typing.TYPE_CHECKING:\n    # Avoid errors in type checkers (e.g. mypy) for experimental estimators.\n    # TODO: remove this check once the estimator is no longer experimental.\n    from sklearn.model_selection._search_successive_halving import (\n        HalvingGridSearchCV,\n        HalvingRandomSearchCV,\n    )\n\n\n__all__ = [\n    \"BaseCrossValidator\",\n    \"BaseShuffleSplit\",\n    \"FixedThresholdClassifier\",\n    \"GridSearchCV\",\n    \"GroupKFold\",\n    \"GroupShuffleSplit\",\n    \"HalvingGridSearchCV\",\n    \"HalvingRandomSearchCV\",\n    \"KFold\",\n    \"LearningCurveDisplay\",\n    \"LeaveOneGroupOut\",\n    \"LeaveOneOut\",\n    \"LeavePGroupsOut\",\n    \"LeavePOut\",\n    \"ParameterGrid\",\n    \"ParameterSampler\",\n    \"PredefinedSplit\",\n    \"RandomizedSearchCV\",\n    \"RepeatedKFold\",\n    \"RepeatedStratifiedKFold\",\n    \"ShuffleSplit\",\n    \"StratifiedGroupKFold\",\n    \"StratifiedKFold\",\n    \"StratifiedShuffleSplit\",\n    \"TimeSeriesSplit\",\n    \"TunedThresholdClassifierCV\",\n    \"ValidationCurveDisplay\",\n    \"check_cv\",\n    \"cross_val_predict\",\n    \"cross_val_score\",\n    \"cross_validate\",\n    \"learning_curve\",\n    \"permutation_test_score\",\n    \"train_test_split\",\n    \"validation_curve\",\n]\n\n\n# TODO: remove this check once the estimator is no longer experimental.\ndef __getattr__(name):\n    if name in {\"HalvingGridSearchCV\", \"HalvingRandomSearchCV\"}:\n        raise ImportError(\n            f\"{name} is experimental and the API might change without any \"\n            \"deprecation cycle. To use it, you need to explicitly import \"\n            \"enable_halving_search_cv:\\n\"\n            \"from sklearn.experimental import enable_halving_search_cv\"\n        )\n    raise AttributeError(f\"module {__name__} has no attribute {name}\")\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test the validation module\"\"\"\n\nimport os\nimport re\nimport sys\nimport tempfile\nimport warnings\nfrom functools import partial\nfrom io import StringIO\nfrom time import sleep\n\nimport numpy as np\nimport pytest\nfrom scipy.sparse import issparse\n\nfrom sklearn import config_context\nfrom sklearn.base import BaseEstimator, ClassifierMixin, clone\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import (\n    load_diabetes,\n    load_digits,\n    load_iris,\n    make_classification,\n    make_multilabel_classification,\n    make_regression,\n)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.exceptions import FitFailedWarning, UnsetMetadataPassedError\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import (\n    LogisticRegression,\n    PassiveAggressiveClassifier,\n    Ridge,\n    RidgeClassifier,\n    SGDClassifier,\n)\nfrom sklearn.metrics import (\n    accuracy_score,\n    check_scoring,\n    confusion_matrix,\n    explained_variance_score,\n    make_scorer,\n    mean_squared_error,\n    precision_recall_fscore_support,\n    precision_score,\n    r2_score,\n)\nfrom sklearn.metrics._scorer import _MultimetricScorer\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    GroupKFold,\n    GroupShuffleSplit,\n    KFold,\n    LeaveOneGroupOut,\n    LeaveOneOut,\n    LeavePGroupsOut,\n    ShuffleSplit,\n    StratifiedKFold,\n    cross_val_predict,\n    cross_val_score,\n    cross_validate,\n    learning_curve,\n    permutation_test_score,\n    validation_curve,\n)\nfrom sklearn.model_selection._validation import (\n    _check_is_permutation,\n    _fit_and_score,\n    _score,\n)\nfrom sklearn.model_selection.tests.common import OneTimeSplitter\nfrom sklearn.model_selection.tests.test_search import FailingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, scale\nfrom sklearn.svm import "}, {"start_line": 34000, "end_line": 36000, "belongs_to": {"file_name": "_search_successive_halving.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ger, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used. These splitters are instantiated\n        with `shuffle=False` so the splits will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. note::\n            Due to implementation details, the folds produced by `cv` must be\n            the same across multiple calls to `cv.split()`. For\n            built-in `scikit-learn` iterators, this can be achieved by\n            deactivating shuffling (`shuffle=False`), or by setting the\n            `cv`'s `random_state` parameter to an integer.\n\n    scoring : str or callable, default=None\n        Scoring method to use to evaluate the predictions on the test set.\n\n        - str: see :ref:`scoring_string_names` for options.\n        - callable: a scorer callable object (e.g., function) with signature\n          ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.\n        - `None`: the `estimator`'s\n          :ref:`default evaluation criterion <scoring_api_overview>` is used.\n\n    refit : bool or callable, default=True\n        Refit an estimator using the best found parameters on the whole\n        dataset.\n\n        Where there are considerations other than maximum score in\n        choosing a best estimator, ``refit`` can be set to a function which\n        returns the selected ``best_index_`` given ``cv_results_``. In that\n        case, the ``best_estimator_`` and ``best_params_`` will be set\n        according to the returned ``best_index_`` while the ``best_score_``\n        attribute will not be available.\n\n        The refitted estimator is made "}], "retrieved_count": 10, "cost_time": 1.2408978939056396}
{"question": "Where in Scikit-learn is the data validation system implemented?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn's data validation system is implemented primarily in the utils/validation.py module, with additional validation components distributed across the codebase. Here's where the validation system is implemented:\n\n1. **Core Validation Module (utils/validation.py)**:\n   - **check_array()**: Primary function for validating and converting input arrays\n   - **check_X_y()**: Validates X and y for consistency in supervised learning\n   - **validate_data()**: Comprehensive validation with feature counting and metadata routing\n   - **_check_y()**: Validates target variables (y) for classification and regression\n   - **check_consistent_length()**: Ensures consistent lengths across multiple arrays\n\n2. **Data Type and Format Validation**:\n   - **Dtype Conversion**: Automatic conversion to appropriate NumPy dtypes\n   - **Sparse Matrix Support**: Validation and conversion of sparse matrices\n   - **DataFrame Support**: Validation of pandas DataFrames and Polars DataFrames\n   - **Array-like Objects**: Support for various array-like input types\n   - **Feature Names**: Validation and preservation of feature names\n\n3. **Shape and Dimension Validation**:\n   - **_check_n_features()**: Validates number of features consistency\n   - **n_features_in_**: Attribute tracking for feature count validation\n   - **feature_names_in_**: Attribute tracking for feature name validation\n   - **Shape Validation**: Ensures correct array shapes and dimensions\n   - **Sample Count Validation**: Validates consistent sample counts\n\n4. **Input Validation Decorators**:\n   - **validate_params**: Decorator for parameter validation\n   - **check_is_fitted**: Validates that estimators have been fitted\n   - **Input validation utilities**: Various helper functions for validation\n   - **Type checking**: Runtime type validation for parameters\n   - **Range validation**: Parameter range and constraint validation\n\n5. **Feature Name Validation**:\n   - **_check_feature_names()**: Validates feature name consistency\n   - **Feature name preservation**: Maintains feature names throughout pipeline\n   - **Feature name conversion**: Converts feature names to appropriate formats\n   - **Feature name tracking**: Tracks feature names across transformations\n   - **Feature name validation**: Ensures feature name consistency\n\n6. **Target Variable Validation**:\n   - **Classification targets**: Validates classification target formats\n   - **Regression targets**: Validates regression target formats\n   - **Multi-output targets**: Validates multi-output target formats\n   - **Target type conversion**: Converts targets to appropriate formats\n   - **Target consistency**: Ensures target consistency across operations\n\n7. **Sparse Matrix Validation**:\n   - **Sparse format validation**: Validates sparse matrix formats\n   - **Format conversion**: Automatic conversion between sparse formats\n   - **Sparsity preservation**: Maintains sparsity throughout validation\n   - **Sparse matrix operations**: Optimized operations for sparse data\n   - **Sparse matrix compatibility**: Ensures compatibility with algorithms\n\n8. **Validation in Estimators**:\n   - **BaseEstimator**: Built-in validation framework\n   - **validate_data()**: Standard validation function for estimators\n   - **Input validation**: Automatic validation in fit(), predict(), transform()\n   - **Parameter validation**: Validation of estimator parameters\n   - **State validation**: Validation of estimator state\n\n9. **Pipeline Validation**:\n   - **Pipeline validation**: Validation across pipeline steps\n   - **Feature consistency**: Ensures feature consistency through pipeline\n   - **Data flow validation**: Validates data flow through pipeline\n   - **Step validation**: Validation of individual pipeline steps\n   - **Output validation**: Validation of pipeline outputs\n\n10. **Advanced Validation Features**:\n    - **Metadata routing**: Advanced metadata validation and routing\n    - **Custom validation**: Support for custom validation functions\n    - **Validation caching**: Caching of validation results\n    - **Performance optimization**: Optimized validation for large datasets\n    - **Error reporting**: Comprehensive error messages and diagnostics", "score": null, "retrieved_content": [{"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ixes import CSR_CONTAINERS, SPARRAY_PRESENT\nfrom sklearn.utils.metaestimators import available_if\nfrom sklearn.utils.multiclass import type_of_target\nfrom sklearn.utils.validation import (\n    check_array,\n    check_is_fitted,\n    check_X_y,\n    validate_data,\n)\n\n\nclass CorrectNotFittedError(ValueError):\n    \"\"\"Exception class to raise if estimator is used before fitting.\n\n    Like NotFittedError, it inherits from ValueError, but not from\n    AttributeError. Used for testing only.\n    \"\"\"\n\n\nclass BaseBadClassifier(ClassifierMixin, BaseEstimator):\n    def fit(self, X, y):\n        return self\n\n    def predict(self, X):\n        return np.ones(X.shape[0])\n\n\nclass ChangesDict(BaseEstimator):\n    def __init__(self, key=0):\n        self.key = key\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n    def predict(self, X):\n        X = check_array(X)\n        self.key = 1000\n        return np.ones(X.shape[0])\n\n\nclass SetsWrongAttribute(BaseEstimator):\n    def __init__(self, acceptable_key=0):\n        self.acceptable_key = acceptable_key\n\n    def fit(self, X, y=None):\n        self.wrong_attribute = 0\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ChangesWrongAttribute(BaseEstimator):\n    def __init__(self, wrong_attribute=0):\n        self.wrong_attribute = wrong_attribute\n\n    def fit(self, X, y=None):\n        self.wrong_attribute = 1\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ChangesUnderscoreAttribute(BaseEstimator):\n    def fit(self, X, y=None):\n        self._good_attribute = 1\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass RaisesErrorInSetParams(BaseEstimator):\n    def __init__(self, p=0):\n        self.p = p\n\n    def set_params(self, **kwargs):\n        if \"p\" in kwargs:\n            p = kwargs.pop(\"p\")\n            if p < 0:\n                raise ValueError(\"p can't be less than 0\")\n            self.p = p\n        return super().set_params(**kwargs)\n\n    def fit(sel"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ataframe_column_names_consistency,\n    check_decision_proba_consistency,\n    check_dict_unchanged,\n    check_dont_overwrite_parameters,\n    check_estimator,\n    check_estimator_cloneable,\n    check_estimator_repr,\n    check_estimator_sparse_array,\n    check_estimator_sparse_matrix,\n    check_estimator_sparse_tag,\n    check_estimator_tags_renamed,\n    check_estimators_nan_inf,\n    check_estimators_overwrite_params,\n    check_estimators_unfitted,\n    check_fit_check_is_fitted,\n    check_fit_score_takes_y,\n    check_methods_sample_order_invariance,\n    check_methods_subset_invariance,\n    check_mixin_order,\n    check_no_attributes_set_in_init,\n    check_outlier_contamination,\n    check_outlier_corruption,\n    check_parameters_default_constructible,\n    check_positive_only_tag_during_fit,\n    check_regressor_data_not_an_array,\n    check_requires_y_none,\n    check_sample_weights_pandas_series,\n    check_set_params,\n    estimator_checks_generator,\n    set_random_state,\n)\nfrom sklearn.utils.fixes import CSR_CONTAINERS, SPARRAY_PRESENT\nfrom sklearn.utils.metaestimators import available_if\nfrom sklearn.utils.multiclass import type_of_target\nfrom sklearn.utils.validation import (\n    check_array,\n    check_is_fitted,\n    check_X_y,\n    validate_data,\n)\n\n\nclass CorrectNotFittedError(ValueError):\n    \"\"\"Exception class to raise if estimator is used before fitting.\n\n    Like NotFittedError, it inherits from ValueError, but not from\n    AttributeError. Used for testing only.\n    \"\"\"\n\n\nclass BaseBadClassifier(ClassifierMixin, BaseEstimator):\n    def fit(self, X, y):\n        return self\n\n    def predict(self, X):\n        return np.ones(X.shape[0])\n\n\nclass ChangesDict(BaseEstimator):\n    def __init__(self, key=0):\n        self.key = key\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n    def predict(self, X):\n        X = check_array(X)\n        self.key = 1000\n        return np.ones(X.shape[0])\n\n\nclass SetsWrongAttribute(BaseEstimator):\n  "}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e_matrix\"\n        self.raise_for_type = raise_for_type\n\n    def fit(self, X, y):\n        X, y = validate_data(\n            self,\n            X,\n            y,\n            accept_sparse=(\"csr\", \"csc\", \"coo\"),\n            accept_large_sparse=True,\n            multi_output=True,\n            y_numeric=True,\n        )\n        if self.raise_for_type == \"sparse_array\":\n            correct_type = isinstance(X, sp.sparray)\n        elif self.raise_for_type == \"sparse_matrix\":\n            correct_type = isinstance(X, sp.spmatrix)\n        if correct_type:\n            if X.format == \"coo\":\n                if X.row.dtype == \"int64\" or X.col.dtype == \"int64\":\n                    raise ValueError(\"Estimator doesn't support 64-bit indices\")\n            elif X.format in [\"csc\", \"csr\"]:\n                assert \"int64\" not in (\n                    X.indices.dtype,\n                    X.indptr.dtype,\n                ), \"Estimator doesn't support 64-bit indices\"\n\n        return self\n\n\nclass SparseTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self, sparse_container=None):\n        self.sparse_container = sparse_container\n\n    def fit(self, X, y=None):\n        validate_data(self, X)\n        return self\n\n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X)\n\n    def transform(self, X):\n        check_is_fitted(self)\n        X = validate_data(self, X, accept_sparse=True, reset=False)\n        return self.sparse_container(X)\n\n\nclass EstimatorInconsistentForPandas(BaseEstimator):\n    def fit(self, X, y):\n        try:\n            from pandas import DataFrame\n\n            if isinstance(X, DataFrame):\n                self.value_ = X.iloc[0, 0]\n            else:\n                X = check_array(X)\n                self.value_ = X[1, 0]\n            return self\n\n        except ImportError:\n            X = check_array(X)\n            self.value_ = X[1, 0]\n            return self\n\n    def predict(self, X):\n        X = check_array(X)\n        return np.array([self."}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "f, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass HasMutableParameters(BaseEstimator):\n    def __init__(self, p=object()):\n        self.p = p\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass HasImmutableParameters(BaseEstimator):\n    # Note that object is an uninitialized class, thus immutable.\n    def __init__(self, p=42, q=np.int32(42), r=object):\n        self.p = p\n        self.q = q\n        self.r = r\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ModifiesValueInsteadOfRaisingError(BaseEstimator):\n    def __init__(self, p=0):\n        self.p = p\n\n    def set_params(self, **kwargs):\n        if \"p\" in kwargs:\n            p = kwargs.pop(\"p\")\n            if p < 0:\n                p = 0\n            self.p = p\n        return super().set_params(**kwargs)\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ModifiesAnotherValue(BaseEstimator):\n    def __init__(self, a=0, b=\"method1\"):\n        self.a = a\n        self.b = b\n\n    def set_params(self, **kwargs):\n        if \"a\" in kwargs:\n            a = kwargs.pop(\"a\")\n            self.a = a\n            if a is None:\n                kwargs.pop(\"b\")\n                self.b = \"method2\"\n        return super().set_params(**kwargs)\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass NoCheckinPredict(BaseBadClassifier):\n    def fit(self, X, y):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass NoSparseClassifier(BaseBadClassifier):\n    def __init__(self, raise_for_type=None):\n        # raise_for_type : str, expects \"sparse_array\" or \"sparse_matrix\"\n        self.raise_for_type = raise_for_type\n\n    def fit(self, X, y):\n        X, y = validate_data(self, X, y, accept_sparse=[\"csr\", \"csc\"])\n        if self.raise_for_type == \"sparse_array\":\n            correct_type = isinstance(X, sp.spar"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "put import _get_output_config\nfrom sklearn.utils._testing import (\n    _convert_container,\n    assert_array_equal,\n)\nfrom sklearn.utils.validation import _check_n_features, validate_data\n\n\n#############################################################################\n# A few test classes\nclass MyEstimator(BaseEstimator):\n    def __init__(self, l1=0, empty=None):\n        self.l1 = l1\n        self.empty = empty\n\n\nclass K(BaseEstimator):\n    def __init__(self, c=None, d=None):\n        self.c = c\n        self.d = d\n\n\nclass T(BaseEstimator):\n    def __init__(self, a=None, b=None):\n        self.a = a\n        self.b = b\n\n\nclass NaNTag(BaseEstimator):\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.allow_nan = True\n        return tags\n\n\nclass NoNaNTag(BaseEstimator):\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.allow_nan = False\n        return tags\n\n\nclass OverrideTag(NaNTag):\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.allow_nan = False\n        return tags\n\n\nclass DiamondOverwriteTag(NaNTag, NoNaNTag):\n    pass\n\n\nclass InheritDiamondOverwriteTag(DiamondOverwriteTag):\n    pass\n\n\nclass ModifyInitParams(BaseEstimator):\n    \"\"\"Deprecated behavior.\n    Equal parameters but with a type cast.\n    Doesn't fulfill a is a\n    \"\"\"\n\n    def __init__(self, a=np.array([0])):\n        self.a = a.copy()\n\n\nclass Buggy(BaseEstimator):\n    \"A buggy estimator that does not set its parameters right.\"\n\n    def __init__(self, a=None):\n        self.a = 1\n\n\nclass NoEstimator:\n    def __init__(self):\n        pass\n\n    def fit(self, X=None, y=None):\n        return self\n\n    def predict(self, X=None):\n        return None\n\n\nclass VargEstimator(BaseEstimator):\n    \"\"\"scikit-learn estimators shouldn't have vargs.\"\"\"\n\n    def __init__(self, *vargs):\n        pass\n\n\n#############################################################################\n# The tests"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  check_array,\n    check_symmetric,\n    check_X_y,\n    deprecated,\n)\nfrom sklearn.utils._array_api import (\n    _convert_to_numpy,\n    _get_namespace_device_dtype_ids,\n    _is_numpy_namespace,\n    yield_namespace_device_dtype_combinations,\n)\nfrom sklearn.utils._mocking import (\n    MockDataFrame,\n    _MockEstimatorOnOffPrediction,\n)\nfrom sklearn.utils._testing import (\n    SkipTest,\n    TempMemmap,\n    _array_api_for_tests,\n    _convert_container,\n    assert_allclose,\n    assert_allclose_dense_sparse,\n    assert_array_equal,\n    create_memmap_backed_data,\n    skip_if_array_api_compat_not_configured,\n)\nfrom sklearn.utils.estimator_checks import _NotAnArray\nfrom sklearn.utils.fixes import (\n    COO_CONTAINERS,\n    CSC_CONTAINERS,\n    CSR_CONTAINERS,\n    DIA_CONTAINERS,\n    DOK_CONTAINERS,\n)\nfrom sklearn.utils.validation import (\n    FLOAT_DTYPES,\n    _allclose_dense_sparse,\n    _check_feature_names_in,\n    _check_method_params,\n    _check_pos_label_consistency,\n    _check_psd_eigenvalues,\n    _check_response_method,\n    _check_sample_weight,\n    _check_y,\n    _deprecate_positional_args,\n    _estimator_has,\n    _get_feature_names,\n    _is_fitted,\n    _is_pandas_df,\n    _is_polars_df,\n    _num_features,\n    _num_samples,\n    _to_object_array,\n    assert_all_finite,\n    check_consistent_length,\n    check_is_fitted,\n    check_memory,\n    check_non_negative,\n    check_random_state,\n    check_scalar,\n    column_or_1d,\n    has_fit_parameter,\n    validate_data,\n)\n\n\ndef test_make_rng():\n    # Check the check_random_state utility function behavior\n    assert check_random_state(None) is np.random.mtrand._rand\n    assert check_random_state(np.random) is np.random.mtrand._rand\n\n    rng_42 = np.random.RandomState(42)\n    assert check_random_state(42).randint(100) == rng_42.randint(100)\n\n    rng_42 = np.random.RandomState(42)\n    assert check_random_state(rng_42) is rng_42\n\n    rng_42 = np.random.RandomState(42)\n    assert check_random_state(43).randint(100) != rng_42.randint(100)\n\n"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "odifiesAnotherValue(BaseEstimator):\n    def __init__(self, a=0, b=\"method1\"):\n        self.a = a\n        self.b = b\n\n    def set_params(self, **kwargs):\n        if \"a\" in kwargs:\n            a = kwargs.pop(\"a\")\n            self.a = a\n            if a is None:\n                kwargs.pop(\"b\")\n                self.b = \"method2\"\n        return super().set_params(**kwargs)\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass NoCheckinPredict(BaseBadClassifier):\n    def fit(self, X, y):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass NoSparseClassifier(BaseBadClassifier):\n    def __init__(self, raise_for_type=None):\n        # raise_for_type : str, expects \"sparse_array\" or \"sparse_matrix\"\n        self.raise_for_type = raise_for_type\n\n    def fit(self, X, y):\n        X, y = validate_data(self, X, y, accept_sparse=[\"csr\", \"csc\"])\n        if self.raise_for_type == \"sparse_array\":\n            correct_type = isinstance(X, sp.sparray)\n        elif self.raise_for_type == \"sparse_matrix\":\n            correct_type = isinstance(X, sp.spmatrix)\n        if correct_type:\n            raise ValueError(\"Nonsensical Error\")\n        return self\n\n    def predict(self, X):\n        X = check_array(X)\n        return np.ones(X.shape[0])\n\n\nclass CorrectNotFittedErrorClassifier(BaseBadClassifier):\n    def fit(self, X, y):\n        X, y = validate_data(self, X, y)\n        self.coef_ = np.ones(X.shape[1])\n        return self\n\n    def predict(self, X):\n        check_is_fitted(self)\n        X = check_array(X)\n        return np.ones(X.shape[0])\n\n\nclass NoSampleWeightPandasSeriesType(BaseEstimator):\n    def fit(self, X, y, sample_weight=None):\n        # Convert data\n        X, y = validate_data(\n            self, X, y, accept_sparse=(\"csr\", \"csc\"), multi_output=True, y_numeric=True\n        )\n        # Function is only called after we verify that pandas is installed\n        from pandas import Series\n\n        if isinstance(sample_weight, "}, {"start_line": 25000, "end_line": 27000, "belongs_to": {"file_name": "test_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " pytest.raises(TypeError, match=msg):\n        trans.transform(df_mixed)\n\n\ndef test_validate_data_skip_check_array():\n    \"\"\"Check skip_check_array option of _validate_data.\"\"\"\n\n    pd = pytest.importorskip(\"pandas\")\n    iris = datasets.load_iris()\n    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n    y = pd.Series(iris.target)\n\n    class NoOpTransformer(TransformerMixin, BaseEstimator):\n        pass\n\n    no_op = NoOpTransformer()\n    X_np_out = validate_data(no_op, df, skip_check_array=False)\n    assert isinstance(X_np_out, np.ndarray)\n    assert_allclose(X_np_out, df.to_numpy())\n\n    X_df_out = validate_data(no_op, df, skip_check_array=True)\n    assert X_df_out is df\n\n    y_np_out = validate_data(no_op, y=y, skip_check_array=False)\n    assert isinstance(y_np_out, np.ndarray)\n    assert_allclose(y_np_out, y.to_numpy())\n\n    y_series_out = validate_data(no_op, y=y, skip_check_array=True)\n    assert y_series_out is y\n\n    X_np_out, y_np_out = validate_data(no_op, df, y, skip_check_array=False)\n    assert isinstance(X_np_out, np.ndarray)\n    assert_allclose(X_np_out, df.to_numpy())\n    assert isinstance(y_np_out, np.ndarray)\n    assert_allclose(y_np_out, y.to_numpy())\n\n    X_df_out, y_series_out = validate_data(no_op, df, y, skip_check_array=True)\n    assert X_df_out is df\n    assert y_series_out is y\n\n    msg = \"Validation should be done on X, y or both.\"\n    with pytest.raises(ValueError, match=msg):\n        validate_data(no_op)\n\n\ndef test_clone_keeps_output_config():\n    \"\"\"Check that clone keeps the set_output config.\"\"\"\n\n    ss = StandardScaler().set_output(transform=\"pandas\")\n    config = _get_output_config(\"transform\", ss)\n\n    ss_clone = clone(ss)\n    config_clone = _get_output_config(\"transform\", ss_clone)\n    assert config == config_clone\n\n\nclass _Empty:\n    pass\n\n\nclass EmptyEstimator(_Empty, BaseEstimator):\n    pass\n\n\n@pytest.mark.parametrize(\"estimator\", [BaseEstimator(), EmptyEstimator()])\ndef test_estimator_empty_instance_dict(estimator):\n    "}, {"start_line": 10000, "end_line": 11471, "belongs_to": {"file_name": "test_metaestimators.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n is necessary\n    \"SelfTrainingClassifier\",\n    \"SequentialFeatureSelector\",  # not applicable (2D data mandatory)\n]\n\nDATA_VALIDATION_META_ESTIMATORS = [\n    est\n    for est in _generate_meta_estimator_instances_with_pipeline()\n    if est.__class__.__name__ not in DATA_VALIDATION_META_ESTIMATORS_TO_IGNORE\n]\n\n\ndef _get_meta_estimator_id(estimator):\n    return estimator.__class__.__name__\n\n\n@pytest.mark.parametrize(\n    \"estimator\", DATA_VALIDATION_META_ESTIMATORS, ids=_get_meta_estimator_id\n)\ndef test_meta_estimators_delegate_data_validation(estimator):\n    # Check that meta-estimators delegate data validation to the inner\n    # estimator(s).\n    rng = np.random.RandomState(0)\n    set_random_state(estimator)\n\n    n_samples = 30\n    X = rng.choice(np.array([\"aa\", \"bb\", \"cc\"], dtype=object), size=n_samples)\n\n    if is_regressor(estimator):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(3, size=n_samples)\n\n    # We convert to lists to make sure it works on array-like\n    X = _enforce_estimator_tags_X(estimator, X).tolist()\n    y = _enforce_estimator_tags_y(estimator, y).tolist()\n\n    # Calling fit should not raise any data validation exception since X is a\n    # valid input datastructure for the first step of the pipeline passed as\n    # base estimator to the meta estimator.\n    estimator.fit(X, y)\n\n    # n_features_in_ should not be defined since data is not tabular data.\n    assert not hasattr(estimator, \"n_features_in_\")\n"}, {"start_line": 42000, "end_line": 44000, "belongs_to": {"file_name": "api_reference.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/doc", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "alidation\",\n                \"description\": _get_submodule(\"sklearn.utils\", \"validation\"),\n                \"autosummary\": [\n                    \"check_X_y\",\n                    \"check_array\",\n                    \"check_consistent_length\",\n                    \"check_random_state\",\n                    \"check_scalar\",\n                    \"validation.check_is_fitted\",\n                    \"validation.check_memory\",\n                    \"validation.check_symmetric\",\n                    \"validation.column_or_1d\",\n                    \"validation.has_fit_parameter\",\n                    \"validation.validate_data\",\n                ],\n            },\n            {\n                \"title\": \"Meta-estimators\",\n                \"description\": _get_submodule(\"sklearn.utils\", \"metaestimators\"),\n                \"autosummary\": [\"metaestimators.available_if\"],\n            },\n            {\n                \"title\": \"Weight handling based on class labels\",\n                \"description\": _get_submodule(\"sklearn.utils\", \"class_weight\"),\n                \"autosummary\": [\n                    \"class_weight.compute_class_weight\",\n                    \"class_weight.compute_sample_weight\",\n                ],\n            },\n            {\n                \"title\": \"Dealing with multiclass target in classifiers\",\n                \"description\": _get_submodule(\"sklearn.utils\", \"multiclass\"),\n                \"autosummary\": [\n                    \"multiclass.is_multilabel\",\n                    \"multiclass.type_of_target\",\n                    \"multiclass.unique_labels\",\n                ],\n            },\n            {\n                \"title\": \"Optimal mathematical operations\",\n                \"description\": _get_submodule(\"sklearn.utils\", \"extmath\"),\n                \"autosummary\": [\n                    \"extmath.density\",\n                    \"extmath.fast_logdet\",\n                    \"extmath.randomized_range_finder\",\n                    \"extmath.randomized_svd\",\n                    \"extmath.safe_sparse_dot\",\n     "}], "retrieved_count": 10, "cost_time": 1.2418792247772217}
{"question": "Where is the \"transform\" method defined in the transformer hierarchy?", "answer": null, "relative_code_list": null, "ground_truth": "The \"transform\" method in scikit-learn's transformer hierarchy is defined across multiple levels of the inheritance structure. Here's where and how the transform method is defined:\n\n1. **TransformerMixin (base.py)**:\n   - **Base Definition**: TransformerMixin provides the foundational interface for transformers\n   - **fit_transform Method**: Implements fit_transform() that delegates to fit() and transform()\n   - **Default Implementation**: Provides a default implementation of fit_transform\n   - **Interface Contract**: Defines the expected behavior for all transformers\n   - **Metadata Routing**: Handles metadata routing for transform operations\n\n2. **Abstract Interface Definition**:\n   - **Method Signature**: transform(X, **kwargs) where X is the input data\n   - **Return Value**: Must return transformed data with same number of samples\n   - **Sample Preservation**: Should not change the number of input samples\n   - **Order Preservation**: Output should correspond to input samples in same order\n   - **Feature Transformation**: Can change number and nature of features\n\n3. **Concrete Transformer Implementations**:\n   - **Preprocessing Transformers**: StandardScaler, MinMaxScaler, RobustScaler\n   - **Feature Selection**: SelectKBest, VarianceThreshold, SelectFromModel\n   - **Dimensionality Reduction**: PCA, NMF, TruncatedSVD, FastICA\n   - **Feature Extraction**: CountVectorizer, TfidfVectorizer, FeatureHasher\n   - **Encoding Transformers**: OneHotEncoder, LabelEncoder, OrdinalEncoder\n\n4. **Pipeline Integration (pipeline.py)**:\n   - **Pipeline.transform()**: Chains transform methods of multiple transformers\n   - **_transform_one()**: Helper function for individual transformer transformation\n   - **Sequential Processing**: Applies transformers in sequence through pipeline\n   - **Parameter Routing**: Routes parameters to appropriate transformer steps\n   - **Metadata Handling**: Handles metadata routing through pipeline steps\n\n5. **Meta-Transformer Implementations**:\n   - **FeatureUnion**: Parallel transformation of multiple transformers\n   - **ColumnTransformer**: Column-specific transformations\n   - **TransformedTargetRegressor**: Transforms target variables\n   - **MetaTransformer**: Wrapper for other transformers\n   - **Custom Meta-Transformers**: User-defined transformer compositions\n\n6. **Transform Method Requirements**:\n   - **Input Validation**: Must validate input data format and shape\n   - **Fitted State**: Must be called after fit() or fit_transform()\n   - **Consistency**: Must produce consistent transformations\n   - **Memory Efficiency**: Should be memory-efficient for large datasets\n   - **Sparse Support**: Should support sparse matrices where appropriate\n\n7. **Specialized Transform Methods**:\n   - **inverse_transform**: Reverses the transformation (for some transformers)\n   - **fit_transform**: Combined fitting and transformation\n   - **transform_sample**: Transforms individual samples\n   - **batch_transform**: Transforms data in batches\n   - **online_transform**: Transforms streaming data\n\n8. **Feature Name Handling**:\n   - **get_feature_names_out**: Returns output feature names\n   - **Feature Name Preservation**: Maintains feature names through transformations\n   - **Feature Name Generation**: Generates new feature names for created features\n   - **Feature Name Validation**: Validates feature name consistency\n   - **Feature Name Routing**: Routes feature names through pipelines\n\n9. **Transform Method Patterns**:\n   - **One-to-One Transformation**: Each input feature maps to one output feature\n   - **Many-to-One Transformation**: Multiple input features map to one output feature\n   - **One-to-Many Transformation**: One input feature maps to multiple output features\n   - **Many-to-Many Transformation**: Complex mappings between input and output features\n   - **Conditional Transformation**: Transformation depends on data characteristics\n\n10. **Advanced Transform Features**:\n    - **Metadata Routing**: Advanced metadata handling during transformation\n    - **Parameter Passing**: Passing parameters to transform methods\n    - **Error Handling**: Robust error handling for transformation failures\n    - **Performance Optimization**: Optimized transformation for large datasets\n    - **Caching**: Caching of transformation results where appropriate", "score": null, "retrieved_content": [{"start_line": 42000, "end_line": 44000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "of each transformer in the pipeline. The transformed\n        data are finally passed to the final estimator that calls\n        `transform` method. Only valid if the final estimator\n        implements `transform`.\n\n        This also works where final estimator is `None` in which case all prior\n        transformations are applied.\n\n        Parameters\n        ----------\n        X : iterable\n            Data to transform. Must fulfill input requirements of first step\n            of the pipeline.\n\n        **params : dict of str -> object\n            Parameters requested and accepted by steps. Each step must have\n            requested certain metadata for these parameters to be forwarded to\n            them.\n\n            .. versionadded:: 1.4\n                Only available if `enable_metadata_routing=True`. See\n                :ref:`Metadata Routing User Guide <metadata_routing>` for more\n                details.\n\n        Returns\n        -------\n        Xt : ndarray of shape (n_samples, n_transformed_features)\n            Transformed data.\n        \"\"\"\n        # TODO(1.8): Remove the context manager and use check_is_fitted(self)\n        with _raise_or_warn_if_not_fitted(self):\n            _raise_for_params(params, self, \"transform\")\n\n            # not branching here since params is only available if\n            # enable_metadata_routing=True\n            routed_params = process_routing(self, \"transform\", **params)\n            Xt = X\n            for _, name, transform in self._iter():\n                Xt = transform.transform(Xt, **routed_params[name].transform)\n            return Xt\n\n    def _can_inverse_transform(self):\n        return all(hasattr(t, \"inverse_transform\") for _, _, t in self._iter())\n\n    @available_if(_can_inverse_transform)\n    def inverse_transform(self, X, **params):\n        \"\"\"Apply `inverse_transform` for each step in a reverse order.\n\n        All estimators in the pipeline must support `inverse_transform`.\n\n        Parameters\n        ----------\n       "}, {"start_line": 29000, "end_line": 31000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n` are helpful mixins for\n    defining :term:`get_feature_names_out`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, TransformerMixin\n    >>> class MyTransformer(TransformerMixin, BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         return self\n    ...     def transform(self, X):\n    ...         return np.full(shape=len(X), fill_value=self.param)\n    >>> transformer = MyTransformer()\n    >>> X = [[1, 2], [2, 3], [3, 4]]\n    >>> transformer.fit_transform(X)\n    array([1, 1, 1])\n    \"\"\"\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.transformer_tags = TransformerTags()\n        return tags\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n                default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.\n            Pass only if the estimator accepts additional params in its `fit` method.\n\n        Returns\n        -------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.\n        \"\"\"\n        # non-optimized default implementation; override when a better\n        # method is possible for a given clustering algorithm\n\n        # we do not route parameters here, since consumers don't route. But\n        # since it's possible for a `transform` method to also consume\n        # metadata, we check if that's the case, and we raise a warning telling\n        # users that they "}, {"start_line": 41000, "end_line": 43000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "lf):\n            Xt = X\n\n            if not _routing_enabled():\n                for _, name, transform in self._iter(with_final=False):\n                    Xt = transform.transform(Xt)\n                return self.steps[-1][1].predict_log_proba(Xt, **params)\n\n            # metadata routing enabled\n            routed_params = process_routing(self, \"predict_log_proba\", **params)\n            for _, name, transform in self._iter(with_final=False):\n                Xt = transform.transform(Xt, **routed_params[name].transform)\n            return self.steps[-1][1].predict_log_proba(\n                Xt, **routed_params[self.steps[-1][0]].predict_log_proba\n            )\n\n    def _can_transform(self):\n        return self._final_estimator == \"passthrough\" or hasattr(\n            self._final_estimator, \"transform\"\n        )\n\n    @available_if(_can_transform)\n    def transform(self, X, **params):\n        \"\"\"Transform the data, and apply `transform` with the final estimator.\n\n        Call `transform` of each transformer in the pipeline. The transformed\n        data are finally passed to the final estimator that calls\n        `transform` method. Only valid if the final estimator\n        implements `transform`.\n\n        This also works where final estimator is `None` in which case all prior\n        transformations are applied.\n\n        Parameters\n        ----------\n        X : iterable\n            Data to transform. Must fulfill input requirements of first step\n            of the pipeline.\n\n        **params : dict of str -> object\n            Parameters requested and accepted by steps. Each step must have\n            requested certain metadata for these parameters to be forwarded to\n            them.\n\n            .. versionadded:: 1.4\n                Only available if `enable_metadata_routing=True`. See\n                :ref:`Metadata Routing User Guide <metadata_routing>` for more\n                details.\n\n        Returns\n        -------\n        Xt : ndarray of shape (n_samples, n_tr"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ame of the parameter to be transformed.\n    param_value : object\n        The value of the parameter to be transformed.\n    transform_params : dict\n        The metadata to be used for transformation. This passed to the\n        `transform` method of the sub-pipeline.\n\n    Returns\n    -------\n    transformed_value : object\n        The transformed value of the parameter.\n    \"\"\"\n    if param_name not in cache:\n        # If the parameter is a tuple, transform each element of the\n        # tuple. This is needed to support the pattern present in\n        # `lightgbm` and `xgboost` where users can pass multiple\n        # validation sets.\n        if isinstance(param_value, tuple):\n            cache[param_name] = tuple(\n                sub_pipeline.transform(element, **transform_params)\n                for element in param_value\n            )\n        else:\n            cache[param_name] = sub_pipeline.transform(param_value, **transform_params)\n\n    return cache[param_name]\n\n\nclass Pipeline(_BaseComposition):\n    \"\"\"\n    A sequence of data transformers with an optional final predictor.\n\n    `Pipeline` allows you to sequentially apply a list of transformers to\n    preprocess the data and, if desired, conclude the sequence with a final\n    :term:`predictor` for predictive modeling.\n\n    Intermediate steps of the pipeline must be transformers, that is, they\n    must implement `fit` and `transform` methods.\n    The final :term:`estimator` only needs to implement `fit`.\n    The transformers in the pipeline can be cached using ``memory`` argument.\n\n    The purpose of the pipeline is to assemble several steps that can be\n    cross-validated together while setting different parameters. For this, it\n    enables setting parameters of the various steps using their names and the\n    parameter name separated by a `'__'`, as in the example below. A step's\n    estimator may be replaced entirely by setting the parameter with its name\n    to another estimator, or a transformer removed by setting"}, {"start_line": 28000, "end_line": 30000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " : ndarray of shape (n_rows, n_cols)\n            The submatrix corresponding to bicluster `i`.\n\n        Notes\n        -----\n        Works with sparse matrices. Only works if ``rows_`` and\n        ``columns_`` attributes exist.\n        \"\"\"\n\n        data = check_array(data, accept_sparse=\"csr\")\n        row_ind, col_ind = self.get_indices(i)\n        return data[row_ind[:, np.newaxis], col_ind]\n\n\nclass TransformerMixin(_SetOutputMixin):\n    \"\"\"Mixin class for all transformers in scikit-learn.\n\n    This mixin defines the following functionality:\n\n    - a `fit_transform` method that delegates to `fit` and `transform`;\n    - a `set_output` method to output `X` as a specific container type.\n\n    If :term:`get_feature_names_out` is defined, then :class:`BaseEstimator` will\n    automatically wrap `transform` and `fit_transform` to follow the `set_output`\n    API. See the :ref:`developer_api_set_output` for details.\n\n    :class:`OneToOneFeatureMixin` and\n    :class:`ClassNamePrefixFeaturesOutMixin` are helpful mixins for\n    defining :term:`get_feature_names_out`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, TransformerMixin\n    >>> class MyTransformer(TransformerMixin, BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         return self\n    ...     def transform(self, X):\n    ...         return np.full(shape=len(X), fill_value=self.param)\n    >>> transformer = MyTransformer()\n    >>> X = [[1, 2], [2, 3], [3, 4]]\n    >>> transformer.fit_transform(X)\n    array([1, 1, 1])\n    \"\"\"\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.transformer_tags = TransformerTags()\n        return tags\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transf"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "_function_transformer.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "]])\n    >>> transformer.transform(X)\n    array([[0.       , 0.6931],\n           [1.0986, 1.3862]])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"func\": [callable, None],\n        \"inverse_func\": [callable, None],\n        \"validate\": [\"boolean\"],\n        \"accept_sparse\": [\"boolean\"],\n        \"check_inverse\": [\"boolean\"],\n        \"feature_names_out\": [callable, StrOptions({\"one-to-one\"}), None],\n        \"kw_args\": [dict, None],\n        \"inv_kw_args\": [dict, None],\n    }\n\n    def __init__(\n        self,\n        func=None,\n        inverse_func=None,\n        *,\n        validate=False,\n        accept_sparse=False,\n        check_inverse=True,\n        feature_names_out=None,\n        kw_args=None,\n        inv_kw_args=None,\n    ):\n        self.func = func\n        self.inverse_func = inverse_func\n        self.validate = validate\n        self.accept_sparse = accept_sparse\n        self.check_inverse = check_inverse\n        self.feature_names_out = feature_names_out\n        self.kw_args = kw_args\n        self.inv_kw_args = inv_kw_args\n\n    def _check_inverse_transform(self, X):\n        \"\"\"Check that func and inverse_func are the inverse.\"\"\"\n        idx_selected = slice(None, None, max(1, X.shape[0] // 100))\n        X_round_trip = self.inverse_transform(self.transform(X[idx_selected]))\n\n        if hasattr(X, \"dtype\"):\n            dtypes = [X.dtype]\n        elif hasattr(X, \"dtypes\"):\n            # Dataframes can have multiple dtypes\n            dtypes = X.dtypes\n\n        # Not all dtypes are numpy dtypes, they can be pandas dtypes as well\n        if not all(\n            isinstance(d, np.dtype) and np.issubdtype(d, np.number) for d in dtypes\n        ):\n            raise ValueError(\n                \"'check_inverse' is only supported when all the elements in `X` is\"\n                \" numerical.\"\n            )\n\n        if not _allclose_dense_sparse(X[idx_selected], X_round_trip):\n            warnings.warn(\n                (\n                    \"The provided functions are not st"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "_target.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/compose", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    such as the :class:`~sklearn.preprocessing.QuantileTransformer` or as a\n    function and its inverse such as `np.log` and `np.exp`.\n\n    The computation during :meth:`fit` is::\n\n        regressor.fit(X, func(y))\n\n    or::\n\n        regressor.fit(X, transformer.transform(y))\n\n    The computation during :meth:`predict` is::\n\n        inverse_func(regressor.predict(X))\n\n    or::\n\n        transformer.inverse_transform(regressor.predict(X))\n\n    Read more in the :ref:`User Guide <transformed_target_regressor>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    regressor : object, default=None\n        Regressor object such as derived from\n        :class:`~sklearn.base.RegressorMixin`. This regressor will\n        automatically be cloned each time prior to fitting. If `regressor is\n        None`, :class:`~sklearn.linear_model.LinearRegression` is created and used.\n\n    transformer : object, default=None\n        Estimator object such as derived from\n        :class:`~sklearn.base.TransformerMixin`. Cannot be set at the same time\n        as `func` and `inverse_func`. If `transformer is None` as well as\n        `func` and `inverse_func`, the transformer will be an identity\n        transformer. Note that the transformer will be cloned during fitting.\n        Also, the transformer is restricting `y` to be a numpy array.\n\n    func : function, default=None\n        Function to apply to `y` before passing to :meth:`fit`. Cannot be set\n        at the same time as `transformer`. If `func is None`, the function used will be\n        the identity function. If `func` is set, `inverse_func` also needs to be\n        provided. The function needs to return a 2-dimensional array.\n\n    inverse_func : function, default=None\n        Function to apply to the prediction of the regressor. Cannot be set at\n        the same time as `transformer`. The inverse function is used to return\n        predictions to the same space of the original training labels. If\n        `inverse_func` is set,"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/decomposition", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "var)),\n        )\n        precision = components_ @ components_.T / self.noise_variance_\n        _add_to_diagonal(precision, 1.0 / exp_var_diff, xp)\n        precision = components_.T @ linalg_inv(precision) @ components_\n        precision /= -(self.noise_variance_**2)\n        _add_to_diagonal(precision, 1.0 / self.noise_variance_, xp)\n        return precision\n\n    @abstractmethod\n    def fit(self, X, y=None):\n        \"\"\"Placeholder for fit. Subclasses should implement this method!\n\n        Fit the model with X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n\n    def transform(self, X):\n        \"\"\"Apply dimensionality reduction to X.\n\n        X is projected on the first principal components previously extracted\n        from a training set.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        Returns\n        -------\n        X_new : array-like of shape (n_samples, n_components)\n            Projection of X in the first principal components, where `n_samples`\n            is the number of samples and `n_components` is the number of the components.\n        \"\"\"\n        xp, _ = get_namespace(X, self.components_, self.explained_variance_)\n\n        check_is_fitted(self)\n\n        X = validate_data(\n            self,\n            X,\n            dtype=[xp.float64, xp.float32],\n            accept_sparse=(\"csr\", \"csc\"),\n            reset=False,\n        )\n        return self._transform(X, xp=xp, x_is_centered=False)\n\n    def _transform(self, X, xp, x_is_centered=False):\n        X_transformed = X @ self.components_.T\n      "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "_target.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/compose", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "TransformerMixin`. Cannot be set at the same time\n        as `func` and `inverse_func`. If `transformer is None` as well as\n        `func` and `inverse_func`, the transformer will be an identity\n        transformer. Note that the transformer will be cloned during fitting.\n        Also, the transformer is restricting `y` to be a numpy array.\n\n    func : function, default=None\n        Function to apply to `y` before passing to :meth:`fit`. Cannot be set\n        at the same time as `transformer`. If `func is None`, the function used will be\n        the identity function. If `func` is set, `inverse_func` also needs to be\n        provided. The function needs to return a 2-dimensional array.\n\n    inverse_func : function, default=None\n        Function to apply to the prediction of the regressor. Cannot be set at\n        the same time as `transformer`. The inverse function is used to return\n        predictions to the same space of the original training labels. If\n        `inverse_func` is set, `func` also needs to be provided. The inverse\n        function needs to return a 2-dimensional array.\n\n    check_inverse : bool, default=True\n        Whether to check that `transform` followed by `inverse_transform`\n        or `func` followed by `inverse_func` leads to the original targets.\n\n    Attributes\n    ----------\n    regressor_ : object\n        Fitted regressor.\n\n    transformer_ : object\n        Transformer used in :meth:`fit` and :meth:`predict`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying regressor exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    sklearn.preprocessing.FunctionTransformer : Construct a transformer from an\n        arbitrary call"}, {"start_line": 81000, "end_line": 83000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " def _sk_visual_block_(self):\n        names, transformers = zip(*self.transformer_list)\n        return _VisualBlock(\"parallel\", transformers, names=names)\n\n    def __getitem__(self, name):\n        \"\"\"Return transformer with name.\"\"\"\n        if not isinstance(name, str):\n            raise KeyError(\"Only string keys are supported\")\n        return self.named_transformers[name]\n\n    def get_metadata_routing(self):\n        \"\"\"Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.5\n\n        Returns\n        -------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.\n        \"\"\"\n        router = MetadataRouter(owner=self.__class__.__name__)\n\n        for name, transformer in self.transformer_list:\n            router.add(\n                **{name: transformer},\n                method_mapping=MethodMapping()\n                .add(caller=\"fit\", callee=\"fit\")\n                .add(caller=\"fit_transform\", callee=\"fit_transform\")\n                .add(caller=\"fit_transform\", callee=\"fit\")\n                .add(caller=\"fit_transform\", callee=\"transform\")\n                .add(caller=\"transform\", callee=\"transform\"),\n            )\n\n        return router\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        try:\n            tags.input_tags.sparse = all(\n                get_tags(trans).input_tags.sparse\n                for name, trans in self.transformer_list\n                if trans not in {\"passthrough\", \"drop\"}\n            )\n        except Exception:\n            # If `transformer_list` does not comply with our API (list of tuples)\n            # then it will fail. In this case, we assume that `sparse` is False\n            # but the parameter validation will raise an error during `fit`.\n            pass  # pragma: no cover\n        return tags\n\n\ndef mak"}], "retrieved_count": 10, "cost_time": 0.3339567184448242}
{"question": "Where is the \"score\" method defined in the estimator interface?", "answer": null, "relative_code_list": null, "ground_truth": "The \"score\" method in scikit-learn's estimator interface is defined across multiple levels of the inheritance hierarchy. Here's where and how the score method is defined:\n\n1. **Mixin Class Implementations (base.py)**:\n   - **ClassifierMixin**: Implements score method using accuracy_score for classifiers\n   - **RegressorMixin**: Implements score method using r2_score for regressors\n   - **DensityMixin**: Provides a no-op score method for density estimators\n   - **ClusterMixin**: Does not provide a default score method\n   - **OutlierMixin**: Does not provide a default score method\n\n2. **ClassifierMixin Score Implementation**:\n   - **Default Metric**: Uses accuracy_score as the default scoring metric\n   - **Method Signature**: score(X, y, sample_weight=None)\n   - **Multi-label Support**: Handles multi-label classification with subset accuracy\n   - **Sample Weighting**: Supports sample weights for weighted accuracy calculation\n   - **Return Value**: Returns mean accuracy of predictions w.r.t. true labels\n\n3. **RegressorMixin Score Implementation**:\n   - **Default Metric**: Uses r2_score as the default scoring metric\n   - **Method Signature**: score(X, y, sample_weight=None)\n   - **R Calculation**: Implements coefficient of determination (R)\n   - **Multi-output Support**: Uses uniform_average for multi-output regression\n   - **Return Value**: Returns R score of predictions w.r.t. true values\n\n4. **Concrete Estimator Overrides**:\n   - **Individual Estimators**: Many estimators override the default score method\n   - **Algorithm-Specific Metrics**: Use metrics specific to the algorithm\n   - **Likelihood-Based Scoring**: Some estimators use likelihood-based scoring\n   - **Custom Scoring Logic**: Implement custom scoring logic when appropriate\n   - **Performance Optimization**: Optimize scoring for specific algorithms\n\n5. **Meta-Estimator Score Implementations**:\n   - **Pipeline (pipeline.py)**: Chains score through transformers to final estimator\n   - **BaseSearchCV (model_selection/_search.py)**: Uses best_estimator_.score method\n   - **MultiOutputEstimator**: Aggregates scores across multiple outputs\n   - **Voting/Stacking**: Combines scores from multiple base estimators\n   - **FeatureUnion**: Does not provide a score method\n\n6. **Score Method Requirements**:\n   - **Signature**: score(X, y, sample_weight=None) where X is features and y is targets\n   - **Fitted State**: Must be called after fit() or fit_transform()\n   - **Return Value**: Must return a single float representing the score\n   - **Higher is Better**: Convention that higher scores indicate better performance\n   - **Consistency**: Must produce consistent scores for same input\n\n7. **Score Method Integration**:\n   - **Predict Integration**: Score methods typically use predict() internally\n   - **Metric Functions**: Use functions from sklearn.metrics module\n   - **Sample Weighting**: Support sample weights for weighted evaluation\n   - **Multi-output Handling**: Handle multiple target variables appropriately\n   - **Error Handling**: Provide clear error messages for invalid inputs\n\n8. **Specialized Score Methods**:\n   - **score_samples**: Returns per-sample scores (for some estimators)\n   - **Custom Scoring**: Estimator-specific scoring methods\n   - **Cross-validation Scoring**: Integration with cross-validation tools\n   - **Grid Search Scoring**: Integration with hyperparameter search\n   - **Model Selection Scoring**: Integration with model selection tools\n\n9. **Score Method Patterns**:\n   - **Classification Scoring**: Accuracy, precision, recall, F1-score patterns\n   - **Regression Scoring**: R, MSE, MAE, explained variance patterns\n   - **Clustering Scoring**: Silhouette, Calinski-Harabasz, Davies-Bouldin patterns\n   - **Density Scoring**: Log-likelihood, probability density patterns\n   - **Outlier Scoring**: Anomaly score, outlier factor patterns\n\n10. **Advanced Score Features**:\n    - **Metadata Routing**: Handle metadata during scoring\n    - **Parameter Passing**: Pass parameters to underlying scoring methods\n    - **Error Handling**: Robust error handling for scoring failures\n    - **Performance Optimization**: Optimized scoring for large datasets\n    - **Caching**: Caching of scoring results where appropriate", "score": null, "retrieved_content": [{"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "_self_training.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/semi_supervised", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "       more details.\n\n        Returns\n        -------\n        score : float\n            Result of calling score on the `estimator`.\n        \"\"\"\n        check_is_fitted(self)\n        _raise_for_params(params, self, \"score\")\n\n        if _routing_enabled():\n            # metadata routing is enabled.\n            routed_params = process_routing(self, \"score\", **params)\n        else:\n            routed_params = Bunch(estimator=Bunch(score={}))\n\n        X = validate_data(\n            self,\n            X,\n            accept_sparse=True,\n            ensure_all_finite=False,\n            reset=False,\n        )\n        return self.estimator_.score(X, y, **routed_params.estimator.score)\n\n    def get_metadata_routing(self):\n        \"\"\"Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.6\n\n        Returns\n        -------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.\n        \"\"\"\n        router = MetadataRouter(owner=self.__class__.__name__)\n        router.add(\n            estimator=self.estimator,\n            method_mapping=(\n                MethodMapping()\n                .add(callee=\"fit\", caller=\"fit\")\n                .add(callee=\"score\", caller=\"fit\")\n                .add(callee=\"predict\", caller=\"predict\")\n                .add(callee=\"predict_proba\", caller=\"predict_proba\")\n                .add(callee=\"decision_function\", caller=\"decision_function\")\n                .add(callee=\"predict_log_proba\", caller=\"predict_log_proba\")\n                .add(callee=\"score\", caller=\"score\")\n            ),\n        )\n        return router\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        # TODO(1.8): remove the condition check together with base_estimator\n        if self.estimator is not None:\n            tags.input_tags.sparse = get_tags(self"}, {"start_line": 24000, "end_line": 25801, "belongs_to": {"file_name": "_ransac.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tadata_routing=True)` is set. See\n                :ref:`Metadata Routing User Guide <metadata_routing>` for more\n                details.\n\n        Returns\n        -------\n        z : float\n            Score of the prediction.\n        \"\"\"\n        check_is_fitted(self)\n        X = validate_data(\n            self,\n            X,\n            ensure_all_finite=False,\n            accept_sparse=True,\n            reset=False,\n        )\n\n        _raise_for_params(params, self, \"score\")\n        if _routing_enabled():\n            score_params = process_routing(self, \"score\", **params).estimator[\"score\"]\n        else:\n            score_params = {}\n\n        return self.estimator_.score(X, y, **score_params)\n\n    def get_metadata_routing(self):\n        \"\"\"Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.5\n\n        Returns\n        -------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.\n        \"\"\"\n        router = MetadataRouter(owner=self.__class__.__name__).add(\n            estimator=self.estimator,\n            method_mapping=MethodMapping()\n            .add(caller=\"fit\", callee=\"fit\")\n            .add(caller=\"fit\", callee=\"score\")\n            .add(caller=\"score\", callee=\"score\")\n            .add(caller=\"predict\", callee=\"predict\"),\n        )\n        return router\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        if self.estimator is None:\n            tags.input_tags.sparse = True  # default estimator is LinearRegression\n        else:\n            tags.input_tags.sparse = get_tags(self.estimator).input_tags.sparse\n        return tags\n"}, {"start_line": 19000, "end_line": 21000, "belongs_to": {"file_name": "_self_training.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/semi_supervised", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "   accept_sparse=True,\n            ensure_all_finite=False,\n            reset=False,\n        )\n        return self.estimator_.predict_log_proba(\n            X, **routed_params.estimator.predict_log_proba\n        )\n\n    @available_if(_estimator_has(\"score\"))\n    def score(self, X, y, **params):\n        \"\"\"Call score on the `estimator`.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Array representing the data.\n\n        y : array-like of shape (n_samples,)\n            Array representing the labels.\n\n        **params : dict of str -> object\n            Parameters to pass to the underlying estimator's ``score`` method.\n\n            .. versionadded:: 1.6\n                Only available if `enable_metadata_routing=True`,\n                which can be set by using\n                ``sklearn.set_config(enable_metadata_routing=True)``.\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n                more details.\n\n        Returns\n        -------\n        score : float\n            Result of calling score on the `estimator`.\n        \"\"\"\n        check_is_fitted(self)\n        _raise_for_params(params, self, \"score\")\n\n        if _routing_enabled():\n            # metadata routing is enabled.\n            routed_params = process_routing(self, \"score\", **params)\n        else:\n            routed_params = Bunch(estimator=Bunch(score={}))\n\n        X = validate_data(\n            self,\n            X,\n            accept_sparse=True,\n            ensure_all_finite=False,\n            reset=False,\n        )\n        return self.estimator_.score(X, y, **routed_params.estimator.score)\n\n    def get_metadata_routing(self):\n        \"\"\"Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.6\n\n        Returns\n        -------\n        routing : MetadataRouter\n            A :class:`~sklearn.u"}, {"start_line": 21000, "end_line": 23000, "belongs_to": {"file_name": "_search.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "       y_score : ndarray of shape (n_samples,)\n            The ``best_estimator_.score_samples`` method.\n        \"\"\"\n        check_is_fitted(self)\n        return self.best_estimator_.score_samples(X)\n\n    @available_if(_search_estimator_has(\"predict\"))\n    def predict(self, X):\n        \"\"\"Call predict on the estimator with the best found parameters.\n\n        Only available if ``refit=True`` and the underlying estimator supports\n        ``predict``.\n\n        Parameters\n        ----------\n        X : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The predicted labels or values for `X` based on the estimator with\n            the best found parameters.\n        \"\"\"\n        check_is_fitted(self)\n        return self.best_estimator_.predict(X)\n\n    @available_if(_search_estimator_has(\"predict_proba\"))\n    def predict_proba(self, X):\n        \"\"\"Call predict_proba on the estimator with the best found parameters.\n\n        Only available if ``refit=True`` and the underlying estimator supports\n        ``predict_proba``.\n\n        Parameters\n        ----------\n        X : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Predicted class probabilities for `X` based on the estimator with\n            the best found parameters. The order of the classes corresponds\n            to that in the fitted attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        return self.best_estimator_.predict_proba(X)\n\n    @available_if(_search_estimator_has(\"predict_log_proba\"))\n    def predict_log_proba(self, X):\n        \"\"\"Call predict_log_proba on the estimator with the best found parameters.\n\n        Only available if ``refit=True`` and the "}, {"start_line": 23000, "end_line": 25000, "belongs_to": {"file_name": "_ransac.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_sparse=True,\n            reset=False,\n        )\n\n        _raise_for_params(params, self, \"predict\")\n\n        if _routing_enabled():\n            predict_params = process_routing(self, \"predict\", **params).estimator[\n                \"predict\"\n            ]\n        else:\n            predict_params = {}\n\n        return self.estimator_.predict(X, **predict_params)\n\n    def score(self, X, y, **params):\n        \"\"\"Return the score of the prediction.\n\n        This is a wrapper for `estimator_.score(X, y)`.\n\n        Parameters\n        ----------\n        X : (array-like or sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        **params : dict\n            Parameters routed to the `score` method of the sub-estimator via\n            the metadata routing API.\n\n            .. versionadded:: 1.5\n\n                Only available if\n                `sklearn.set_config(enable_metadata_routing=True)` is set. See\n                :ref:`Metadata Routing User Guide <metadata_routing>` for more\n                details.\n\n        Returns\n        -------\n        z : float\n            Score of the prediction.\n        \"\"\"\n        check_is_fitted(self)\n        X = validate_data(\n            self,\n            X,\n            ensure_all_finite=False,\n            accept_sparse=True,\n            reset=False,\n        )\n\n        _raise_for_params(params, self, \"score\")\n        if _routing_enabled():\n            score_params = process_routing(self, \"score\", **params).estimator[\"score\"]\n        else:\n            score_params = {}\n\n        return self.estimator_.score(X, y, **score_params)\n\n    def get_metadata_routing(self):\n        \"\"\"Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.5\n\n        Returns\n        -------\n        routing : MetadataRouter\n           "}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "_scorer.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/metrics", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     y_true : array-like\n            Gold standard target values for X. These must be class labels,\n            not decision function values.\n\n        **kwargs : dict\n            Other parameters passed to the scorer. Refer to\n            :func:`set_score_request` for more details.\n\n        Returns\n        -------\n        score : float\n            Score function applied to prediction of estimator on X.\n        \"\"\"\n        self._warn_overlap(\n            message=(\n                \"There is an overlap between set kwargs of this scorer instance and\"\n                \" passed metadata. Please pass them either as kwargs to `make_scorer`\"\n                \" or metadata, but not both.\"\n            ),\n            kwargs=kwargs,\n        )\n\n        pos_label = None if is_regressor(estimator) else self._get_pos_label()\n        response_method = _check_response_method(estimator, self._response_method)\n        y_pred = method_caller(\n            estimator,\n            _get_response_method_name(response_method),\n            X,\n            pos_label=pos_label,\n        )\n\n        scoring_kwargs = {**self._kwargs, **kwargs}\n        return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n\n\n@validate_params(\n    {\n        \"scoring\": [str, callable, None],\n    },\n    prefer_skip_nested_validation=True,\n)\ndef get_scorer(scoring):\n    \"\"\"Get a scorer from string.\n\n    Read more in the :ref:`User Guide <scoring_parameter>`.\n    :func:`~sklearn.metrics.get_scorer_names` can be used to retrieve the names\n    of all available scorers.\n\n    Parameters\n    ----------\n    scoring : str, callable or None\n        Scoring method as string. If callable it is returned as is.\n        If None, returns None.\n\n    Returns\n    -------\n    scorer : callable\n        The scorer.\n\n    Notes\n    -----\n    When passed a string, this function always returns a copy of the scorer\n    object. Calling `get_scorer` twice for the same scorer results in two\n    separate scorer objects.\n\n    Examples\n    -"}, {"start_line": 19000, "end_line": 21000, "belongs_to": {"file_name": "_search.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "assification or regression;\n            None for unsupervised learning.\n\n        **params : dict\n            Parameters to be passed to the underlying scorer(s).\n\n            .. versionadded:: 1.4\n                Only available if `enable_metadata_routing=True`. See\n                :ref:`Metadata Routing User Guide <metadata_routing>` for more\n                details.\n\n        Returns\n        -------\n        score : float\n            The score defined by ``scoring`` if provided, and the\n            ``best_estimator_.score`` method otherwise.\n        \"\"\"\n        _check_refit(self, \"score\")\n        check_is_fitted(self)\n\n        _raise_for_params(params, self, \"score\")\n\n        if _routing_enabled():\n            score_params = process_routing(self, \"score\", **params).scorer[\"score\"]\n        else:\n            score_params = dict()\n\n        if self.scorer_ is None:\n            raise ValueError(\n                \"No score function explicitly defined, \"\n                \"and the estimator doesn't provide one %s\" % self.best_estimator_\n            )\n        if isinstance(self.scorer_, dict):\n            if self.multimetric_:\n                scorer = self.scorer_[self.refit]\n            else:\n                scorer = self.scorer_\n            return scorer(self.best_estimator_, X, y, **score_params)\n\n        # callable\n        score = self.scorer_(self.best_estimator_, X, y, **score_params)\n        if self.multimetric_:\n            score = score[self.refit]\n        return score\n\n    @available_if(_search_estimator_has(\"score_samples\"))\n    def score_samples(self, X):\n        \"\"\"Call score_samples on the estimator with the best found parameters.\n\n        Only available if ``refit=True`` and the underlying estimator supports\n        ``score_samples``.\n\n        .. versionadded:: 0.24\n\n        Parameters\n        ----------\n        X : iterable\n            Data to predict on. Must fulfill input requirements\n            of the underlying estimator.\n\n        Returns\n        -------\n "}, {"start_line": 47000, "end_line": 49000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "sult of calling `score` on the final estimator.\n        \"\"\"\n        # TODO(1.8): Remove the context manager and use check_is_fitted(self)\n        with _raise_or_warn_if_not_fitted(self):\n            Xt = X\n            if not _routing_enabled():\n                for _, name, transform in self._iter(with_final=False):\n                    Xt = transform.transform(Xt)\n                score_params = {}\n                if sample_weight is not None:\n                    score_params[\"sample_weight\"] = sample_weight\n                return self.steps[-1][1].score(Xt, y, **score_params)\n\n            # metadata routing is enabled.\n            routed_params = process_routing(\n                self, \"score\", sample_weight=sample_weight, **params\n            )\n\n            Xt = X\n            for _, name, transform in self._iter(with_final=False):\n                Xt = transform.transform(Xt, **routed_params[name].transform)\n            return self.steps[-1][1].score(\n                Xt, y, **routed_params[self.steps[-1][0]].score\n            )\n\n    @property\n    def classes_(self):\n        \"\"\"The classes labels. Only exist if the last step is a classifier.\"\"\"\n        return self.steps[-1][1].classes_\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n\n        if not self.steps:\n            return tags\n\n        try:\n            if self.steps[0][1] is not None and self.steps[0][1] != \"passthrough\":\n                tags.input_tags.pairwise = get_tags(\n                    self.steps[0][1]\n                ).input_tags.pairwise\n            # WARNING: the sparse tag can be incorrect.\n            # Some Pipelines accepting sparse data are wrongly tagged sparse=False.\n            # For example Pipeline([PCA(), estimator]) accepts sparse data\n            # even if the estimator doesn't as PCA outputs a dense array.\n            tags.input_tags.sparse = all(\n                get_tags(step).input_tags.sparse\n                for name, step in self.steps\n                if st"}, {"start_line": 17000, "end_line": 19000, "belongs_to": {"file_name": "_search.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_train_score\": [\"boolean\"],\n    }\n\n    @abstractmethod\n    def __init__(\n        self,\n        estimator,\n        *,\n        scoring=None,\n        n_jobs=None,\n        refit=True,\n        cv=None,\n        verbose=0,\n        pre_dispatch=\"2*n_jobs\",\n        error_score=np.nan,\n        return_train_score=True,\n    ):\n        self.scoring = scoring\n        self.estimator = estimator\n        self.n_jobs = n_jobs\n        self.refit = refit\n        self.cv = cv\n        self.verbose = verbose\n        self.pre_dispatch = pre_dispatch\n        self.error_score = error_score\n        self.return_train_score = return_train_score\n\n    @property\n    # TODO(1.8) remove this property\n    def _estimator_type(self):\n        return self.estimator._estimator_type\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        sub_estimator_tags = get_tags(self.estimator)\n        tags.estimator_type = sub_estimator_tags.estimator_type\n        tags.classifier_tags = deepcopy(sub_estimator_tags.classifier_tags)\n        tags.regressor_tags = deepcopy(sub_estimator_tags.regressor_tags)\n        # allows cross-validation to see 'precomputed' metrics\n        tags.input_tags.pairwise = sub_estimator_tags.input_tags.pairwise\n        tags.input_tags.sparse = sub_estimator_tags.input_tags.sparse\n        tags.array_api_support = sub_estimator_tags.array_api_support\n        return tags\n\n    def score(self, X, y=None, **params):\n        \"\"\"Return the score on the given data, if the estimator has been refit.\n\n        This uses the score defined by ``scoring`` where provided, and the\n        ``best_estimator_.score`` method otherwise.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples, n_output) \\\n            or (n_samples,), default=None\n            Target relative to X for cl"}, {"start_line": 22000, "end_line": 24000, "belongs_to": {"file_name": "_ransac.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "lier_mask_best\n        return self\n\n    def predict(self, X, **params):\n        \"\"\"Predict using the estimated model.\n\n        This is a wrapper for `estimator_.predict(X)`.\n\n        Parameters\n        ----------\n        X : {array-like or sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        **params : dict\n            Parameters routed to the `predict` method of the sub-estimator via\n            the metadata routing API.\n\n            .. versionadded:: 1.5\n\n                Only available if\n                `sklearn.set_config(enable_metadata_routing=True)` is set. See\n                :ref:`Metadata Routing User Guide <metadata_routing>` for more\n                details.\n\n        Returns\n        -------\n        y : array, shape = [n_samples] or [n_samples, n_targets]\n            Returns predicted values.\n        \"\"\"\n        check_is_fitted(self)\n        X = validate_data(\n            self,\n            X,\n            ensure_all_finite=False,\n            accept_sparse=True,\n            reset=False,\n        )\n\n        _raise_for_params(params, self, \"predict\")\n\n        if _routing_enabled():\n            predict_params = process_routing(self, \"predict\", **params).estimator[\n                \"predict\"\n            ]\n        else:\n            predict_params = {}\n\n        return self.estimator_.predict(X, **predict_params)\n\n    def score(self, X, y, **params):\n        \"\"\"Return the score of the prediction.\n\n        This is a wrapper for `estimator_.score(X, y)`.\n\n        Parameters\n        ----------\n        X : (array-like or sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        **params : dict\n            Parameters routed to the `score` method of the sub-estimator via\n            the metadata routing API.\n\n            .. versionadded:: 1.5\n\n                Only available if\n                `sklearn.set_config(enable_me"}], "retrieved_count": 10, "cost_time": 0.3272984027862549}
{"question": "Where in Scikit-learn's codebase is the \"predict\" method defined for classifiers?", "answer": null, "relative_code_list": null, "ground_truth": "The \"predict\" method for classifiers in scikit-learn is defined across multiple levels of the inheritance hierarchy. Here's where and how the predict method is defined:\n\n1. **ClassifierMixin (base.py)**:\n   - **Interface Definition**: ClassifierMixin provides the foundational interface for classifiers\n   - **Method Contract**: Defines the expected behavior for all classifier predict methods\n   - **Score Method**: Provides default score method using accuracy_score\n   - **Tag Management**: Sets appropriate estimator tags for classifiers\n   - **Type Checking**: Provides is_classifier() function for type checking\n\n2. **Base Class Implementations**:\n   - **LinearClassifierMixin (linear_model/_base.py)**: Implements predict for linear classifiers\n   - **BaseDecisionTree (tree/_classes.py)**: Implements predict for decision tree classifiers\n   - **ForestClassifier (ensemble/_forest.py)**: Implements predict for ensemble classifiers\n   - **BaseSVC (svm/_base.py)**: Implements predict for SVM classifiers\n   - **BaseMultilayerPerceptron (neural_network/_multilayer_perceptron.py)**: Implements predict for neural network classifiers\n\n3. **Concrete Classifier Implementations**:\n   - **Individual Classifier Classes**: Each classifier implements its own predict method\n   - **Algorithm-Specific Logic**: Predict methods contain the specific prediction logic for each algorithm\n   - **Class Label Mapping**: Predict methods map internal predictions to class labels\n   - **Multi-Output Support**: Handle multi-output classification scenarios\n   - **Probability Integration**: Use predict_proba when available for better predictions\n\n4. **Predict Method Requirements**:\n   - **Signature**: predict(X) where X is the input features\n   - **Return Value**: Must return class labels from classes_ attribute\n   - **Fitted State**: Must be called after fit() or fit_transform()\n   - **Input Validation**: Must validate input data format and shape\n   - **Consistency**: Must produce consistent predictions for same input\n\n5. **Decision Function Integration**:\n   - **decision_function**: Many classifiers implement decision_function for raw scores\n   - **Score to Label Mapping**: predict methods often use decision_function internally\n   - **Threshold Application**: Apply thresholds to convert scores to class labels\n   - **Multi-class Handling**: Handle multi-class scenarios appropriately\n   - **Binary vs Multi-class**: Different logic for binary vs multi-class classification\n\n6. **Probability-Based Prediction**:\n   - **predict_proba Integration**: Use probability estimates when available\n   - **Argmax Selection**: Select class with highest probability\n   - **Ensemble Averaging**: Average probabilities across ensemble members\n   - **Voting Mechanisms**: Use voting when probabilities not available\n   - **Confidence Estimation**: Provide confidence estimates through probabilities\n\n7. **Specialized Predict Methods**:\n   - **Multi-Output Prediction**: Handle multiple target variables\n   - **Sparse Prediction**: Efficient prediction for sparse inputs\n   - **Batch Prediction**: Predict on batches of data\n   - **Online Prediction**: Predict on streaming data\n   - **Parallel Prediction**: Parallel prediction for large datasets\n\n8. **Class Label Management**:\n   - **classes_ Attribute**: Store class labels in classes_ attribute\n   - **Label Mapping**: Map internal indices to actual class labels\n   - **Label Preservation**: Preserve original class label types\n   - **Label Ordering**: Maintain consistent label ordering\n   - **Label Validation**: Validate class labels during fitting\n\n9. **Predict Method Patterns**:\n   - **Binary Classification**: Special handling for binary classification\n   - **Multi-class Classification**: Handle multiple classes appropriately\n   - **Multi-label Classification**: Handle multiple labels per sample\n   - **Multi-output Classification**: Handle multiple target variables\n   - **Imbalanced Classification**: Handle imbalanced class distributions\n\n10. **Advanced Predict Features**:\n    - **Metadata Routing**: Handle metadata during prediction\n    - **Parameter Passing**: Pass parameters to underlying prediction methods\n    - **Error Handling**: Robust error handling for prediction failures\n    - **Performance Optimization**: Optimized prediction for large datasets\n    - **Caching**: Caching of prediction results where appropriate", "score": null, "retrieved_content": [{"start_line": 18000, "end_line": 20000, "belongs_to": {"file_name": "_classes.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tree", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "odel, the predicted class for each sample in X is\n        returned. For a regression model, the predicted value based on X is\n        returned.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        check_input : bool, default=True\n            Allow to bypass several input checking.\n            Don't use this parameter unless you know what you're doing.\n\n        Returns\n        -------\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            The predicted classes, or the predict values.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_X_predict(X, check_input)\n        proba = self.tree_.predict(X)\n        n_samples = X.shape[0]\n\n        # Classification\n        if is_classifier(self):\n            if self.n_outputs_ == 1:\n                return self.classes_.take(np.argmax(proba, axis=1), axis=0)\n\n            else:\n                class_type = self.classes_[0].dtype\n                predictions = np.zeros((n_samples, self.n_outputs_), dtype=class_type)\n                for k in range(self.n_outputs_):\n                    predictions[:, k] = self.classes_[k].take(\n                        np.argmax(proba[:, k], axis=1), axis=0\n                    )\n\n                return predictions\n\n        # Regression\n        else:\n            if self.n_outputs_ == 1:\n                return proba[:, 0]\n\n            else:\n                return proba[:, :, 0]\n\n    def apply(self, X, check_input=True):\n        \"\"\"Return the index of the leaf that each sample is predicted as.\n\n        .. versionadded:: 0.17\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np"}, {"start_line": 63000, "end_line": 65000, "belongs_to": {"file_name": "_gb.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "t in the attribute\n            :term:`classes_`. Regression and binary classification produce an\n            array of shape (n_samples,).\n        \"\"\"\n        X = validate_data(\n            self, X, dtype=DTYPE, order=\"C\", accept_sparse=\"csr\", reset=False\n        )\n        raw_predictions = self._raw_predict(X)\n        if raw_predictions.shape[1] == 1:\n            return raw_predictions.ravel()\n        return raw_predictions\n\n    def staged_decision_function(self, X):\n        \"\"\"Compute decision function of ``X`` for each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Yields\n        ------\n        score : generator of ndarray of shape (n_samples, k)\n            The decision function of the input samples, which corresponds to\n            the raw values predicted from the trees of the ensemble . The\n            classes corresponds to that in the attribute :term:`classes_`.\n            Regression and binary classification are special cases with\n            ``k == 1``, otherwise ``k==n_classes``.\n        \"\"\"\n        yield from self._staged_raw_predict(X)\n\n    def predict(self, X):\n        \"\"\"Predict class for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        raw_predictions = self.decision_function(X)\n        if raw_predictions.ndim == 1:  # decision_fu"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "idate_data(self, X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n        coef_ = self.coef_\n        if coef_.ndim == 1:\n            return X @ coef_ + self.intercept_\n        else:\n            return X @ coef_.T + self.intercept_\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : array, shape (n_samples,)\n            Returns predicted values.\n        \"\"\"\n        return self._decision_function(X)\n\n    def _set_intercept(self, X_offset, y_offset, X_scale=None):\n        \"\"\"Set the intercept_\"\"\"\n        xp, _ = get_namespace(X_offset, y_offset, X_scale)\n\n        if self.fit_intercept:\n            # We always want coef_.dtype=X.dtype. For instance, X.dtype can differ from\n            # coef_.dtype if warm_start=True.\n            self.coef_ = xp.astype(self.coef_, X_offset.dtype, copy=False)\n            if X_scale is not None:\n                self.coef_ = xp.divide(self.coef_, X_scale)\n\n            if self.coef_.ndim == 1:\n                self.intercept_ = y_offset - X_offset @ self.coef_\n            else:\n                self.intercept_ = y_offset - X_offset @ self.coef_.T\n\n        else:\n            self.intercept_ = 0.0\n\n\n# XXX Should this derive from LinearModel? It should be a mixin, not an ABC.\n# Maybe the n_features checking can be moved to LinearModel.\nclass LinearClassifierMixin(ClassifierMixin):\n    \"\"\"Mixin for linear classifiers.\n\n    Handles prediction for sparse and dense X.\n    \"\"\"\n\n    def decision_function(self, X):\n        \"\"\"\n        Predict confidence scores for samples.\n\n        The confidence score for a sample is proportional to the signed\n        distance of that sample to the hyperplane.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want "}, {"start_line": 64000, "end_line": 66000, "belongs_to": {"file_name": "_gb.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "hape (n_samples, k)\n            The decision function of the input samples, which corresponds to\n            the raw values predicted from the trees of the ensemble . The\n            classes corresponds to that in the attribute :term:`classes_`.\n            Regression and binary classification are special cases with\n            ``k == 1``, otherwise ``k==n_classes``.\n        \"\"\"\n        yield from self._staged_raw_predict(X)\n\n    def predict(self, X):\n        \"\"\"Predict class for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        raw_predictions = self.decision_function(X)\n        if raw_predictions.ndim == 1:  # decision_function already squeezed it\n            encoded_classes = (raw_predictions >= 0).astype(int)\n        else:\n            encoded_classes = np.argmax(raw_predictions, axis=1)\n        return self.classes_[encoded_classes]\n\n    def staged_predict(self, X):\n        \"\"\"Predict class at each stage for X.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Yields\n        ------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted value of the input samples.\n        \"\"\"\n        if self.n_classes_ == 2:  # n_trees_per_iteration_ = 1\n            for raw_predictions in self._staged_raw_predict(X):\n                encoded_classes = "}, {"start_line": 44000, "end_line": 46000, "belongs_to": {"file_name": "_ridge.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "sample_weight = sample_weight * compute_sample_weight(self.class_weight, y)\n        return X, y, sample_weight, Y\n\n    def predict(self, X):\n        \"\"\"Predict class labels for samples in `X`.\n\n        Parameters\n        ----------\n        X : {array-like, spare matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to predict the targets.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n            Vector or matrix containing the predictions. In binary and\n            multiclass problems, this is a vector containing `n_samples`. In\n            a multilabel problem, it returns a matrix of shape\n            `(n_samples, n_outputs)`.\n        \"\"\"\n        check_is_fitted(self, attributes=[\"_label_binarizer\"])\n        if self._label_binarizer.y_type_.startswith(\"multilabel\"):\n            # Threshold such that the negative label is -1 and positive label\n            # is 1 to use the inverse transform of the label binarizer fitted\n            # during fit.\n            scores = 2 * (self.decision_function(X) > 0) - 1\n            return self._label_binarizer.inverse_transform(scores)\n        return super().predict(X)\n\n    @property\n    def classes_(self):\n        \"\"\"Classes labels.\"\"\"\n        return self._label_binarizer.classes_\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.classifier_tags.multi_label = True\n        return tags\n\n\nclass RidgeClassifier(_RidgeClassifierMixin, _BaseRidge):\n    \"\"\"Classifier using Ridge regression.\n\n    This classifier first converts the target values into ``{-1, 1}`` and\n    then treats the problem as a regression task (multi-output regression in\n    the multiclass case).\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alpha : float, default=1.0\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem an"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "_nearest_centroid.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/neighbors", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ations_)\n            self.deviations_ *= signs\n            # Now adjust the centroids using the deviation\n            msd = ms * self.deviations_\n            self.centroids_ = np.array(dataset_centroid_ + msd, copy=False)\n        return self\n\n    def predict(self, X):\n        \"\"\"Perform classification on an array of test vectors `X`.\n\n        The predicted class `C` for each sample in `X` is returned.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        check_is_fitted(self)\n        if np.isclose(self.class_prior_, 1 / len(self.classes_)).all():\n            # `validate_data` is called here since we are not calling `super()`\n            ensure_all_finite = (\n                \"allow-nan\" if get_tags(self).input_tags.allow_nan else True\n            )\n            X = validate_data(\n                self,\n                X,\n                ensure_all_finite=ensure_all_finite,\n                accept_sparse=\"csr\",\n                reset=False,\n            )\n            return self.classes_[\n                pairwise_distances_argmin(X, self.centroids_, metric=self.metric)\n            ]\n        else:\n            return super().predict(X)\n\n    def _decision_function(self, X):\n        # return discriminant scores, see eq. (18.2) p. 652 of the ESL.\n        check_is_fitted(self, \"centroids_\")\n\n        X_normalized = validate_data(\n            self, X, copy=True, reset=False, accept_sparse=\"csr\", dtype=np.float64\n        )\n\n        discriminant_score = np.empty(\n            (X_normalized.shape[0], self.classes_.size), dtype=np.float64\n        )\n\n        mask = self.within_class_std_dev_ != 0\n        X_normalized[:, mask] /= self.within_class_std_dev_[mask]\n        centroids_normalized = self.centroids_.copy()\n        centroids_normalized[:, mask] /= self.within_class_s"}, {"start_line": 91000, "end_line": 93000, "belongs_to": {"file_name": "gradient_boosting.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble/_hist_gradient_boosting", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "lidation_fraction=validation_fraction,\n            n_iter_no_change=n_iter_no_change,\n            tol=tol,\n            verbose=verbose,\n            random_state=random_state,\n        )\n        self.class_weight = class_weight\n\n    def _finalize_sample_weight(self, sample_weight, y):\n        \"\"\"Adjust sample_weights with class_weights.\"\"\"\n        if self.class_weight is None:\n            return sample_weight\n\n        expanded_class_weight = compute_sample_weight(self.class_weight, y)\n\n        if sample_weight is not None:\n            return sample_weight * expanded_class_weight\n        else:\n            return expanded_class_weight\n\n    def predict(self, X):\n        \"\"\"Predict classes for X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        y : ndarray, shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        # TODO: This could be done in parallel\n        raw_predictions = self._raw_predict(X)\n        if raw_predictions.shape[1] == 1:\n            # np.argmax([0.5, 0.5]) is 0, not 1. Therefore \"> 0\" not \">= 0\" to be\n            # consistent with the multiclass case.\n            encoded_classes = (raw_predictions.ravel() > 0).astype(int)\n        else:\n            encoded_classes = np.argmax(raw_predictions, axis=1)\n        return self.classes_[encoded_classes]\n\n    def staged_predict(self, X):\n        \"\"\"Predict classes at each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        .. versionadded:: 0.24\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        ------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted classes of the input samples, for each iteration.\n        \"\"\"\n        for raw_predictions in self._staged_raw_predict(X):\n            if "}, {"start_line": 32000, "end_line": 34000, "belongs_to": {"file_name": "_bagging.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ask = ~indices_to_mask(samples, n_samples)\n\n            if hasattr(estimator, \"predict_proba\"):\n                predictions[mask, :] += estimator.predict_proba(\n                    (X[mask, :])[:, features]\n                )\n\n            else:\n                p = estimator.predict((X[mask, :])[:, features])\n                j = 0\n\n                for i in range(n_samples):\n                    if mask[i]:\n                        predictions[i, p[j]] += 1\n                        j += 1\n\n        if (predictions.sum(axis=1) == 0).any():\n            warn(\n                \"Some inputs do not have OOB scores. \"\n                \"This probably means too few estimators were used \"\n                \"to compute any reliable oob estimates.\"\n            )\n\n        oob_decision_function = predictions / predictions.sum(axis=1)[:, np.newaxis]\n        oob_score = accuracy_score(y, np.argmax(predictions, axis=1))\n\n        self.oob_decision_function_ = oob_decision_function\n        self.oob_score_ = oob_score\n\n    def _validate_y(self, y):\n        y = column_or_1d(y, warn=True)\n        check_classification_targets(y)\n        self.classes_, y = np.unique(y, return_inverse=True)\n        self.n_classes_ = len(self.classes_)\n\n        return y\n\n    def predict(self, X, **params):\n        \"\"\"Predict class for X.\n\n        The predicted class of an input sample is computed as the class with\n        the highest mean predicted probability. If base estimators do not\n        implement a ``predict_proba`` method, then it resorts to voting.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if\n            they are supported by the base estimator.\n\n        **params : dict\n            Parameters routed to the `predict_proba` (if available) or the `predict`\n            method (otherwise) of the sub-estimators via the metadata routing API.\n\n            .. versionadded:: 1."}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "_classification_threshold.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ay of shape (n_samples, n_classes)\n            The class probabilities of the input samples.\n        \"\"\"\n        _check_is_fitted(self)\n        estimator = getattr(self, \"estimator_\", self.estimator)\n        return estimator.predict_proba(X)\n\n    @available_if(_estimator_has(\"predict_log_proba\"))\n    def predict_log_proba(self, X):\n        \"\"\"Predict logarithm class probabilities for `X` using the fitted estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        log_probabilities : ndarray of shape (n_samples, n_classes)\n            The logarithm class probabilities of the input samples.\n        \"\"\"\n        _check_is_fitted(self)\n        estimator = getattr(self, \"estimator_\", self.estimator)\n        return estimator.predict_log_proba(X)\n\n    @available_if(_estimator_has(\"decision_function\"))\n    def decision_function(self, X):\n        \"\"\"Decision function for samples in `X` using the fitted estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        decisions : ndarray of shape (n_samples,)\n            The decision function computed the fitted estimator.\n        \"\"\"\n        _check_is_fitted(self)\n        estimator = getattr(self, \"estimator_\", self.estimator)\n        return estimator.decision_function(X)\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.classifier_tags.multi_class = False\n        tags.input_tags.sparse = get_tags(self.estimator).input_tags.sparse\n        return tags\n\n\nclass FixedThresholdClassifier(BaseThresholdClassifier):\n    \"\"\"Binary classifier that"}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "_weight_boosting.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "sample_weight, 1.0, 0.0\n\n        n_classes = self.n_classes_\n\n        # Stop if the error is at least as bad as random guessing\n        if estimator_error >= 1.0 - (1.0 / n_classes):\n            self.estimators_.pop(-1)\n            if len(self.estimators_) == 0:\n                raise ValueError(\n                    \"BaseClassifier in AdaBoostClassifier \"\n                    \"ensemble is worse than random, ensemble \"\n                    \"can not be fit.\"\n                )\n            return None, None, None\n\n        # Boost weight using multi-class AdaBoost SAMME alg\n        estimator_weight = self.learning_rate * (\n            np.log((1.0 - estimator_error) / estimator_error) + np.log(n_classes - 1.0)\n        )\n\n        # Only boost the weights if it will fit again\n        if not iboost == self.n_estimators - 1:\n            # Only boost positive weights\n            sample_weight = np.exp(\n                np.log(sample_weight)\n                + estimator_weight * incorrect * (sample_weight > 0)\n            )\n\n        return sample_weight, estimator_weight, estimator_error\n\n    def predict(self, X):\n        \"\"\"Predict classes for X.\n\n        The predicted class of an input sample is computed as the weighted mean\n        prediction of the classifiers in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        pred = self.decision_function(X)\n\n        if self.n_classes_ == 2:\n            return self.classes_.take(pred > 0, axis=0)\n\n        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n\n    def staged_predict(self, X):\n        \"\"\"Return staged predictions for X.\n\n        The predicted class of an input sample is computed as "}], "retrieved_count": 10, "cost_time": 0.33679747581481934}
{"question": "Where in Scikit-learn's codebase is the \"fit\" method defined?", "answer": null, "relative_code_list": null, "ground_truth": "The \"fit\" method in scikit-learn is defined as an abstract interface that must be implemented by all estimators. Here's where and how the fit method is defined throughout the codebase:\n\n1. **Abstract Interface Definition**:\n   - **BaseEstimator**: The fit method is not defined in BaseEstimator but is expected to be implemented by all estimators\n   - **Abstract Base Classes**: Various ABCs define fit as an abstract method that must be implemented\n   - **Interface Specification**: The fit method signature and behavior are specified in the scikit-learn API documentation\n   - **Method Contract**: All estimators must implement fit(X, y=None, **kwargs) with specific behavior\n\n2. **Base Class Implementations**:\n   - **LinearModel (linear_model/_base.py)**: Abstract base class with abstract fit method\n   - **BaseForest (ensemble/_forest.py)**: Concrete fit implementation for ensemble methods\n   - **BaseMultilayerPerceptron (neural_network/_multilayer_perceptron.py)**: Fit implementation for neural networks\n   - **BaseLibSVM (svm/_base.py)**: Fit implementation for SVM models\n   - **BaseMixture (mixture/_base.py)**: Fit implementation for mixture models\n\n3. **Concrete Estimator Implementations**:\n   - **Individual Estimator Classes**: Each estimator implements its own fit method\n   - **Algorithm-Specific Logic**: Fit methods contain the specific training logic for each algorithm\n   - **Parameter Learning**: Fit methods learn model parameters from training data\n   - **State Management**: Fit methods set fitted state and learned attributes\n   - **Validation**: Fit methods validate input data and parameters\n\n4. **Meta-Estimator Implementations**:\n   - **Pipeline (pipeline.py)**: Fit method that chains multiple estimators\n   - **BaseSearchCV (model_selection/_search.py)**: Fit method for hyperparameter search\n   - **MultiOutputEstimator (multioutput.py)**: Fit method for multi-output problems\n   - **Voting/Stacking**: Fit methods for ensemble meta-estimators\n   - **FeatureUnion**: Fit method for parallel feature processing\n\n5. **Fit Method Requirements**:\n   - **Signature**: fit(X, y=None, **kwargs) where X is features and y is optional targets\n   - **Return Value**: Must return self for method chaining\n   - **State Setting**: Must set fitted state and learned attributes\n   - **Data Validation**: Must validate input data and parameters\n   - **Parameter Learning**: Must learn model parameters from training data\n\n6. **Fit Context and Decorators**:\n   - **_fit_context**: Decorator that provides context for fit methods\n   - **Validation Control**: Controls when validation occurs during fitting\n   - **Metadata Routing**: Handles metadata routing during fitting\n   - **Error Handling**: Provides consistent error handling across fit methods\n   - **Performance Optimization**: Optimizes validation and computation during fitting\n\n7. **Specialized Fit Methods**:\n   - **fit_predict**: Combined fit and predict for some estimators\n   - **fit_transform**: Combined fit and transform for transformers\n   - **partial_fit**: Incremental fitting for some estimators\n   - **warm_start**: Warm starting for iterative algorithms\n   - **Online Learning**: Fit methods for online/streaming algorithms\n\n8. **Fit Method Patterns**:\n   - **Supervised Learning**: fit(X, y) for classification and regression\n   - **Unsupervised Learning**: fit(X) for clustering and dimensionality reduction\n   - **Semi-Supervised Learning**: fit(X, y) where y may contain missing values\n   - **Multi-Output Learning**: fit(X, y) where y has multiple columns\n   - **Sample Weighting**: fit(X, y, sample_weight) for weighted learning\n\n9. **Fit Method Validation**:\n   - **Input Validation**: Validates X and y shapes, types, and values\n   - **Parameter Validation**: Validates estimator parameters\n   - **Consistency Checks**: Ensures data consistency across fit calls\n   - **Error Handling**: Provides clear error messages for invalid inputs\n   - **State Management**: Manages estimator state during and after fitting\n\n10. **Fit Method Documentation**:\n    - **API Documentation**: Comprehensive documentation of fit method behavior\n    - **Examples**: Code examples showing proper fit method usage\n    - **Parameter Descriptions**: Detailed descriptions of all fit parameters\n    - **Return Value Documentation**: Clear documentation of what fit returns\n    - **Best Practices**: Guidelines for implementing and using fit methods", "score": null, "retrieved_content": [{"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/neighbors", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "SyntaxWarning,\n                    stacklevel=3,\n                )\n\n    def _fit(self, X, y=None):\n        ensure_all_finite = \"allow-nan\" if get_tags(self).input_tags.allow_nan else True\n        if self.__sklearn_tags__().target_tags.required:\n            if not isinstance(X, (KDTree, BallTree, NeighborsBase)):\n                X, y = validate_data(\n                    self,\n                    X,\n                    y,\n                    accept_sparse=\"csr\",\n                    multi_output=True,\n                    order=\"C\",\n                    ensure_all_finite=ensure_all_finite,\n                )\n\n            if is_classifier(self):\n                # Classification targets require a specific format\n                if y.ndim == 1 or (y.ndim == 2 and y.shape[1] == 1):\n                    if y.ndim != 1:\n                        warnings.warn(\n                            (\n                                \"A column-vector y was passed when a \"\n                                \"1d array was expected. Please change \"\n                                \"the shape of y to (n_samples,), for \"\n                                \"example using ravel().\"\n                            ),\n                            DataConversionWarning,\n                            stacklevel=2,\n                        )\n\n                    self.outputs_2d_ = False\n                    y = y.reshape((-1, 1))\n                else:\n                    self.outputs_2d_ = True\n\n                check_classification_targets(y)\n                self.classes_ = []\n                # Using `dtype=np.intp` is necessary since `np.bincount`\n                # (called in _classification.py) fails when dealing\n                # with a float64 array on 32bit systems.\n                self._y = np.empty(y.shape, dtype=np.intp)\n                for k in range(self._y.shape[1]):\n                    classes, self._y[:, k] = np.unique(y[:, k], return_inverse=True)\n                    self.classes_.append(classes)\n\n         "}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "_nca.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/neighbors", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "a.fit(X_train, y_train)\n    NeighborhoodComponentsAnalysis(...)\n    >>> knn = KNeighborsClassifier(n_neighbors=3)\n    >>> knn.fit(X_train, y_train)\n    KNeighborsClassifier(...)\n    >>> print(knn.score(X_test, y_test))\n    0.933333...\n    >>> knn.fit(nca.transform(X_train), y_train)\n    KNeighborsClassifier(...)\n    >>> print(knn.score(nca.transform(X_test), y_test))\n    0.961904...\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"n_components\": [\n            Interval(Integral, 1, None, closed=\"left\"),\n            None,\n        ],\n        \"init\": [\n            StrOptions({\"auto\", \"pca\", \"lda\", \"identity\", \"random\"}),\n            np.ndarray,\n        ],\n        \"warm_start\": [\"boolean\"],\n        \"max_iter\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\n        \"callback\": [callable, None],\n        \"verbose\": [\"verbose\"],\n        \"random_state\": [\"random_state\"],\n    }\n\n    def __init__(\n        self,\n        n_components=None,\n        *,\n        init=\"auto\",\n        warm_start=False,\n        max_iter=50,\n        tol=1e-5,\n        callback=None,\n        verbose=0,\n        random_state=None,\n    ):\n        self.n_components = n_components\n        self.init = init\n        self.warm_start = warm_start\n        self.max_iter = max_iter\n        self.tol = tol\n        self.callback = callback\n        self.verbose = verbose\n        self.random_state = random_state\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y):\n        \"\"\"Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The training samples.\n\n        y : array-like of shape (n_samples,)\n            The corresponding training labels.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        # Validate the inputs X and y, and converts y to numerical classes.\n        X, y = validate"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/svm", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "same in the dense case. Need to factor\n    # this out.\n    _sparse_kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"]\n\n    @abstractmethod\n    def __init__(\n        self,\n        kernel,\n        degree,\n        gamma,\n        coef0,\n        tol,\n        C,\n        nu,\n        epsilon,\n        shrinking,\n        probability,\n        cache_size,\n        class_weight,\n        verbose,\n        max_iter,\n        random_state,\n    ):\n        if self._impl not in LIBSVM_IMPL:\n            raise ValueError(\n                \"impl should be one of %s, %s was given\" % (LIBSVM_IMPL, self._impl)\n            )\n\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.tol = tol\n        self.C = C\n        self.nu = nu\n        self.epsilon = epsilon\n        self.shrinking = shrinking\n        self.probability = probability\n        self.cache_size = cache_size\n        self.class_weight = class_weight\n        self.verbose = verbose\n        self.max_iter = max_iter\n        self.random_state = random_state\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        # Used by cross_val_score.\n        tags.input_tags.pairwise = self.kernel == \"precomputed\"\n        tags.input_tags.sparse = self.kernel != \"precomputed\"\n        return tags\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the SVM model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) \\\n                or (n_samples, n_samples)\n            Training vectors, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n            For kernel=\"precomputed\", the expected shape of X is\n            (n_samples, n_samples).\n\n        y : array-like of shape (n_samples,)\n            Target values (class labels in classification, real num"}, {"start_line": 21000, "end_line": 23000, "belongs_to": {"file_name": "_stochastic_gradient.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "nary(\n                X,\n                y,\n                alpha=alpha,\n                C=C,\n                learning_rate=learning_rate,\n                sample_weight=sample_weight,\n                max_iter=max_iter,\n            )\n        else:\n            raise ValueError(\n                \"The number of classes has to be greater than one; got %d class\"\n                % n_classes\n            )\n\n        return self\n\n    def _fit(\n        self,\n        X,\n        y,\n        alpha,\n        C,\n        loss,\n        learning_rate,\n        coef_init=None,\n        intercept_init=None,\n        sample_weight=None,\n    ):\n        if hasattr(self, \"classes_\"):\n            # delete the attribute otherwise _partial_fit thinks it's not the first call\n            delattr(self, \"classes_\")\n\n        # labels can be encoded as float, int, or string literals\n        # np.unique sorts in asc order; largest class id is positive class\n        y = validate_data(self, y=y)\n        classes = np.unique(y)\n\n        if self.warm_start and hasattr(self, \"coef_\"):\n            if coef_init is None:\n                coef_init = self.coef_\n            if intercept_init is None:\n                intercept_init = self.intercept_\n        else:\n            self.coef_ = None\n            self.intercept_ = None\n\n        if self.average > 0:\n            self._standard_coef = self.coef_\n            self._standard_intercept = self.intercept_\n            self._average_coef = None\n            self._average_intercept = None\n\n        # Clear iteration count for multiple call to fit.\n        self.t_ = 1.0\n\n        self._partial_fit(\n            X,\n            y,\n            alpha,\n            C,\n            loss,\n            learning_rate,\n            self.max_iter,\n            classes,\n            sample_weight,\n            coef_init,\n            intercept_init,\n        )\n\n        if (\n            self.tol is not None\n            and self.tol > -np.inf\n            and self.n_iter_ == self.max_iter\n        ):\n  "}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "_rfe.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/feature_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "            - If `enable_metadata_routing=True`: Parameters safely routed to the ``fit``\n              method of the underlying estimator.\n\n            .. versionchanged:: 1.6\n                See :ref:`Metadata Routing User Guide <metadata_routing>`\n                for more details.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        if _routing_enabled():\n            routed_params = process_routing(self, \"fit\", **fit_params)\n        else:\n            routed_params = Bunch(estimator=Bunch(fit=fit_params))\n\n        return self._fit(X, y, **routed_params.estimator.fit)\n\n    def _fit(self, X, y, step_score=None, **fit_params):\n        # Parameter step_score controls the calculation of self.step_scores_\n        # step_score is not exposed to users and is used when implementing RFECV\n        # self.step_scores_ will not be calculated when calling _fit through fit\n\n        X, y = validate_data(\n            self,\n            X,\n            y,\n            accept_sparse=\"csc\",\n            ensure_min_features=2,\n            ensure_all_finite=False,\n            multi_output=True,\n        )\n\n        # Initialization\n        n_features = X.shape[1]\n        if self.n_features_to_select is None:\n            n_features_to_select = n_features // 2\n        elif isinstance(self.n_features_to_select, Integral):  # int\n            n_features_to_select = self.n_features_to_select\n            if n_features_to_select > n_features:\n                warnings.warn(\n                    (\n                        f\"Found {n_features_to_select=} > {n_features=}. There will be\"\n                        \" no feature selection and all features will be kept.\"\n                    ),\n                    UserWarning,\n                )\n        else:  # float\n            n_features_to_select = int(n_features * self.n_features_to_select)\n\n        if 0.0 < self.step < 1.0:\n            step = int(max(1, self.step * n_features))\n        else:\n            step"}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "_stochastic_gradient.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/linear_model", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "is not None:\n            self._allocate_parameter_mem(\n                n_classes=n_classes,\n                n_features=n_features,\n                input_dtype=X.dtype,\n                coef_init=coef_init,\n                intercept_init=intercept_init,\n            )\n        elif n_features != self.coef_.shape[-1]:\n            raise ValueError(\n                \"Number of features %d does not match previous data %d.\"\n                % (n_features, self.coef_.shape[-1])\n            )\n\n        self._loss_function_ = self._get_loss_function(loss)\n        if not hasattr(self, \"t_\"):\n            self.t_ = 1.0\n\n        # delegate to concrete training procedure\n        if n_classes > 2:\n            self._fit_multiclass(\n                X,\n                y,\n                alpha=alpha,\n                C=C,\n                learning_rate=learning_rate,\n                sample_weight=sample_weight,\n                max_iter=max_iter,\n            )\n        elif n_classes == 2:\n            self._fit_binary(\n                X,\n                y,\n                alpha=alpha,\n                C=C,\n                learning_rate=learning_rate,\n                sample_weight=sample_weight,\n                max_iter=max_iter,\n            )\n        else:\n            raise ValueError(\n                \"The number of classes has to be greater than one; got %d class\"\n                % n_classes\n            )\n\n        return self\n\n    def _fit(\n        self,\n        X,\n        y,\n        alpha,\n        C,\n        loss,\n        learning_rate,\n        coef_init=None,\n        intercept_init=None,\n        sample_weight=None,\n    ):\n        if hasattr(self, \"classes_\"):\n            # delete the attribute otherwise _partial_fit thinks it's not the first call\n            delattr(self, \"classes_\")\n\n        # labels can be encoded as float, int, or string literals\n        # np.unique sorts in asc order; largest class id is positive class\n        y = validate_data(self, y=y)\n        classes = np.unique(y)\n\n "}, {"start_line": 23000, "end_line": 25000, "belongs_to": {"file_name": "_classes.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/svm", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ata(\n            self,\n            X,\n            y,\n            accept_sparse=\"csr\",\n            dtype=np.float64,\n            order=\"C\",\n            accept_large_sparse=False,\n        )\n        penalty = \"l2\"  # SVR only accepts l2 penalty\n\n        _dual = _validate_dual_parameter(self.dual, self.loss, penalty, \"ovr\", X)\n\n        self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n            X,\n            y,\n            self.C,\n            self.fit_intercept,\n            self.intercept_scaling,\n            None,\n            penalty,\n            _dual,\n            self.verbose,\n            self.max_iter,\n            self.tol,\n            self.random_state,\n            loss=self.loss,\n            epsilon=self.epsilon,\n            sample_weight=sample_weight,\n        )\n        self.coef_ = self.coef_.ravel()\n        # Backward compatibility: _fit_liblinear is used both by LinearSVC/R\n        # and LogisticRegression but LogisticRegression sets a structured\n        # `n_iter_` attribute with information about the underlying OvR fits\n        # while LinearSVC/R only reports the maximum value.\n        self.n_iter_ = n_iter_.max().item()\n\n        return self\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.sparse = True\n        return tags\n\n\nclass SVC(BaseSVC):\n    \"\"\"C-Support Vector Classification.\n\n    The implementation is based on libsvm. The fit time scales at least\n    quadratically with the number of samples and may be impractical\n    beyond tens of thousands of samples. For large datasets\n    consider using :class:`~sklearn.svm.LinearSVC` or\n    :class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a\n    :class:`~sklearn.kernel_approximation.Nystroem` transformer or\n    other :ref:`kernel_approximation`.\n\n    The multiclass support is handled according to a one-vs-one scheme.\n\n    For details on the precise mathematical formulation of the provided\n    kernel functions and how `gamma`, `coef0` "}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "_iforest.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "=100,\n        max_samples=\"auto\",\n        contamination=\"auto\",\n        max_features=1.0,\n        bootstrap=False,\n        n_jobs=None,\n        random_state=None,\n        verbose=0,\n        warm_start=False,\n    ):\n        super().__init__(\n            estimator=None,\n            # here above max_features has no links with self.max_features\n            bootstrap=bootstrap,\n            bootstrap_features=False,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            warm_start=warm_start,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n        )\n\n        self.contamination = contamination\n\n    def _get_estimator(self):\n        return ExtraTreeRegressor(\n            # here max_features has no links with self.max_features\n            max_features=1,\n            splitter=\"random\",\n            random_state=self.random_state,\n        )\n\n    def _set_oob_score(self, X, y):\n        raise NotImplementedError(\"OOB score not supported by iforest\")\n\n    def _parallel_args(self):\n        # ExtraTreeRegressor releases the GIL, so it's more efficient to use\n        # a thread-based backend rather than a process-based backend so as\n        # to avoid suffering from communication overhead and extra memory\n        # copies. This is only used in the fit method.\n        return {\"prefer\": \"threads\"}\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"\n        Fit estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Use ``dtype=np.float32`` for maximum\n            efficiency. Sparse matrices are also supported, use sparse\n            ``csc_matrix`` for maximum efficiency.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        sample_weight : array-like of shape (n_sample"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/svm", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "vm.set_verbosity_wrap(self.verbose)\n\n        # we don't pass **self.get_params() to allow subclasses to\n        # add other parameters to __init__\n        (\n            self.support_,\n            self.support_vectors_,\n            self._n_support,\n            self.dual_coef_,\n            self.intercept_,\n            self._probA,\n            self._probB,\n            self.fit_status_,\n            self._num_iter,\n        ) = libsvm.fit(\n            X,\n            y,\n            svm_type=solver_type,\n            sample_weight=sample_weight,\n            class_weight=getattr(self, \"class_weight_\", np.empty(0)),\n            kernel=kernel,\n            C=self.C,\n            nu=self.nu,\n            probability=self.probability,\n            degree=self.degree,\n            shrinking=self.shrinking,\n            tol=self.tol,\n            cache_size=self.cache_size,\n            coef0=self.coef0,\n            gamma=self._gamma,\n            epsilon=self.epsilon,\n            max_iter=self.max_iter,\n            random_seed=random_seed,\n        )\n\n        self._warn_from_fit_status()\n\n    def _sparse_fit(self, X, y, sample_weight, solver_type, kernel, random_seed):\n        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")\n        X.sort_indices()\n\n        kernel_type = self._sparse_kernels.index(kernel)\n\n        libsvm_sparse.set_verbosity_wrap(self.verbose)\n\n        (\n            self.support_,\n            self.support_vectors_,\n            dual_coef_data,\n            self.intercept_,\n            self._n_support,\n            self._probA,\n            self._probB,\n            self.fit_status_,\n            self._num_iter,\n        ) = libsvm_sparse.libsvm_sparse_train(\n            X.shape[1],\n            X.data,\n            X.indices,\n            X.indptr,\n            y,\n            solver_type,\n            kernel_type,\n            self.degree,\n            self._gamma,\n            self.coef0,\n            self.tol,\n            self.C,\n            getattr(self, \"class_weight_\", np.emp"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "_classes.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/svm", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ts(y)\n        self.classes_ = np.unique(y)\n\n        _dual = _validate_dual_parameter(\n            self.dual, self.loss, self.penalty, self.multi_class, X\n        )\n\n        self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n            X,\n            y,\n            self.C,\n            self.fit_intercept,\n            self.intercept_scaling,\n            self.class_weight,\n            self.penalty,\n            _dual,\n            self.verbose,\n            self.max_iter,\n            self.tol,\n            self.random_state,\n            self.multi_class,\n            self.loss,\n            sample_weight=sample_weight,\n        )\n        # Backward compatibility: _fit_liblinear is used both by LinearSVC/R\n        # and LogisticRegression but LogisticRegression sets a structured\n        # `n_iter_` attribute with information about the underlying OvR fits\n        # while LinearSVC/R only reports the maximum value.\n        self.n_iter_ = n_iter_.max().item()\n\n        if self.multi_class == \"crammer_singer\" and len(self.classes_) == 2:\n            self.coef_ = (self.coef_[1] - self.coef_[0]).reshape(1, -1)\n            if self.fit_intercept:\n                intercept = self.intercept_[1] - self.intercept_[0]\n                self.intercept_ = np.array([intercept])\n\n        return self\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.sparse = True\n        return tags\n\n\nclass LinearSVR(RegressorMixin, LinearModel):\n    \"\"\"Linear Support Vector Regression.\n\n    Similar to SVR with parameter kernel='linear', but implemented in terms of\n    liblinear rather than libsvm, so it has more flexibility in the choice of\n    penalties and loss functions and should scale better to large numbers of\n    samples.\n\n    The main differences between :class:`~sklearn.svm.LinearSVR` and\n    :class:`~sklearn.svm.SVR` lie in the loss function used by default, and in\n    the handling of intercept regularization between those two implementations.\n\n    Th"}], "retrieved_count": 10, "cost_time": 0.34598612785339355}
{"question": "How could Scikit-learn's validation system be redesigned to support custom validation rules while maintaining type safety?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn's validation system could be redesigned to support custom validation rules while maintaining type safety through several architectural improvements and design patterns:\n\n1. **Extensible Validation Framework**:\n   - **BaseValidator Class**: Abstract base class for all validators\n   - **Custom Validator Interface**: Standard interface for custom validation rules\n   - **Validator Registry**: Registry system for managing custom validators\n   - **Validator Composition**: Ability to compose multiple validators\n   - **Validator Inheritance**: Support for validator inheritance and specialization\n\n2. **Type-Safe Validation System**:\n   - **Type Annotations**: Comprehensive type annotations for all validation functions\n   - **Generic Validators**: Generic validators that work with different data types\n   - **Type Checking Integration**: Integration with static type checkers (mypy, pyright)\n   - **Runtime Type Validation**: Runtime type checking with detailed error messages\n   - **Type Inference**: Automatic type inference for validation rules\n\n3. **Custom Validation Rule Framework**:\n   - **Rule Definition DSL**: Domain-specific language for defining validation rules\n   - **Rule Composition**: Ability to combine multiple validation rules\n   - **Conditional Validation**: Validation rules that depend on other parameters\n   - **Cross-Parameter Validation**: Validation rules that involve multiple parameters\n   - **Dynamic Validation**: Validation rules that can be modified at runtime\n\n4. **Validation Decorator System**:\n   - **@validate_params**: Enhanced decorator with custom validation support\n   - **@custom_validator**: Decorator for defining custom validation functions\n   - **@conditional_validator**: Decorator for conditional validation rules\n   - **@cross_parameter_validator**: Decorator for cross-parameter validation\n   - **@dynamic_validator**: Decorator for dynamic validation rules\n\n5. **Advanced Validation Patterns**:\n   - **Schema Validation**: JSON Schema-like validation for complex data structures\n   - **Business Logic Validation**: Validation rules that encode business logic\n   - **Data Quality Validation**: Validation rules for data quality checks\n   - **Performance Validation**: Validation rules that consider performance constraints\n   - **Security Validation**: Validation rules for security-sensitive parameters\n\n6. **Validation Rule Management**:\n   - **Rule Versioning**: Version control for validation rules\n   - **Rule Testing**: Testing framework for validation rules\n   - **Rule Documentation**: Automatic documentation generation for validation rules\n   - **Rule Performance**: Performance monitoring for validation rules\n   - **Rule Debugging**: Debugging tools for validation rule execution\n\n7. **Integration with Existing Systems**:\n   - **Backward Compatibility**: Maintain compatibility with existing validation\n   - **Gradual Migration**: Gradual migration path for existing validators\n   - **Legacy Support**: Support for legacy validation patterns\n   - **API Consistency**: Consistent API across old and new validation systems\n   - **Documentation**: Clear documentation for migration and usage\n\n8. **Error Handling and Reporting**:\n   - **Detailed Error Messages**: Comprehensive error messages for validation failures\n   - **Error Context**: Context information for validation errors\n   - **Error Aggregation**: Aggregation of multiple validation errors\n   - **Error Recovery**: Recovery mechanisms for validation failures\n   - **Error Logging**: Comprehensive logging of validation activities\n\n9. **Performance Optimization**:\n   - **Lazy Validation**: Lazy evaluation of validation rules when possible\n   - **Caching**: Caching of validation results for repeated validations\n   - **Parallel Validation**: Parallel execution of independent validation rules\n   - **Early Termination**: Early termination of validation on first failure\n   - **Optimization**: Automatic optimization of validation rule execution order\n\n10. **Implementation Strategy**:\n    - **Phased Rollout**: Phased rollout of new validation system\n    - **Feature Flags**: Feature flags to enable/disable new validation features\n    - **Testing Framework**: Comprehensive testing framework for new validation system\n    - **Documentation**: Extensive documentation and examples\n    - **Community Feedback**: Gather feedback from the community during development", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "_param_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport functools\nimport math\nimport operator\nimport re\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Iterable\nfrom inspect import signature\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, issparse\n\nfrom sklearn._config import config_context, get_config\nfrom sklearn.utils.validation import _is_arraylike_not_scalar\n\n\nclass InvalidParameterError(ValueError, TypeError):\n    \"\"\"Custom exception to be raised when the parameter of a class/method/function\n    does not have a valid type or value.\n    \"\"\"\n\n    # Inherits from ValueError and TypeError to keep backward compatibility.\n\n\ndef validate_parameter_constraints(parameter_constraints, params, caller_name):\n    \"\"\"Validate types and values of given parameters.\n\n    Parameters\n    ----------\n    parameter_constraints : dict or {\"no_validation\"}\n        If \"no_validation\", validation is skipped for this parameter.\n\n        If a dict, it must be a dictionary `param_name: list of constraints`.\n        A parameter is valid if it satisfies one of the constraints from the list.\n        Constraints can be:\n        - an Interval object, representing a continuous or discrete range of numbers\n        - the string \"array-like\"\n        - the string \"sparse matrix\"\n        - the string \"random_state\"\n        - callable\n        - None, meaning that None is a valid value for the parameter\n        - any type, meaning that any instance of this type is valid\n        - an Options object, representing a set of elements of a given type\n        - a StrOptions object, representing a set of strings\n        - the string \"boolean\"\n        - the string \"verbose\"\n        - the string \"cv_object\"\n        - the string \"nan\"\n        - a MissingValues object representing markers for missing values\n        - a HasMethods object, representing method(s) an object must have\n        - a Hidden object, representing a co"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "_param_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "            try:\n                with config_context(\n                    skip_parameter_validation=(\n                        prefer_skip_nested_validation or global_skip_validation\n                    )\n                ):\n                    return func(*args, **kwargs)\n            except InvalidParameterError as e:\n                # When the function is just a wrapper around an estimator, we allow\n                # the function to delegate validation to the estimator, but we replace\n                # the name of the estimator by the name of the function in the error\n                # message to avoid confusion.\n                msg = re.sub(\n                    r\"parameter of \\w+ must be\",\n                    f\"parameter of {func.__qualname__} must be\",\n                    str(e),\n                )\n                raise InvalidParameterError(msg) from e\n\n        return wrapper\n\n    return decorator\n\n\nclass RealNotInt(Real):\n    \"\"\"A type that represents reals that are not instances of int.\n\n    Behaves like float, but also works with values extracted from numpy arrays.\n    isintance(1, RealNotInt) -> False\n    isinstance(1.0, RealNotInt) -> True\n    \"\"\"\n\n\nRealNotInt.register(float)\n\n\ndef _type_name(t):\n    \"\"\"Convert type into human readable string.\"\"\"\n    module = t.__module__\n    qualname = t.__qualname__\n    if module == \"builtins\":\n        return qualname\n    elif t == Real:\n        return \"float\"\n    elif t == Integral:\n        return \"int\"\n    return f\"{module}.{qualname}\"\n\n\nclass _Constraint(ABC):\n    \"\"\"Base class for the constraint objects.\"\"\"\n\n    def __init__(self):\n        self.hidden = False\n\n    @abstractmethod\n    def is_satisfied_by(self, val):\n        \"\"\"Whether or not a value satisfies the constraint.\n\n        Parameters\n        ----------\n        val : object\n            The value to check.\n\n        Returns\n        -------\n        is_satisfied : bool\n            Whether or not the constraint is satisfied by this value.\n        \"\"\"\n\n    @abstractm"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "_param_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "straint, str) and constraint == \"cv_object\":\n        return _CVObjects()\n    if isinstance(constraint, Hidden):\n        constraint = make_constraint(constraint.constraint)\n        constraint.hidden = True\n        return constraint\n    if (isinstance(constraint, str) and constraint == \"nan\") or (\n        isinstance(constraint, float) and np.isnan(constraint)\n    ):\n        return _NanConstraint()\n    raise ValueError(f\"Unknown constraint type: {constraint}\")\n\n\ndef validate_params(parameter_constraints, *, prefer_skip_nested_validation):\n    \"\"\"Decorator to validate types and values of functions and methods.\n\n    Parameters\n    ----------\n    parameter_constraints : dict\n        A dictionary `param_name: list of constraints`. See the docstring of\n        `validate_parameter_constraints` for a description of the accepted constraints.\n\n        Note that the *args and **kwargs parameters are not validated and must not be\n        present in the parameter_constraints dictionary.\n\n    prefer_skip_nested_validation : bool\n        If True, the validation of parameters of inner estimators or functions\n        called by the decorated function will be skipped.\n\n        This is useful to avoid validating many times the parameters passed by the\n        user from the public facing API. It's also useful to avoid validating\n        parameters that we pass internally to inner functions that are guaranteed to\n        be valid by the test suite.\n\n        It should be set to True for most functions, except for those that receive\n        non-validated objects as parameters or that are just wrappers around classes\n        because they only perform a partial validation.\n\n    Returns\n    -------\n    decorated_function : function or method\n        The decorated function.\n    \"\"\"\n\n    def decorator(func):\n        # The dict of parameter constraints is set as an attribute of the function\n        # to make it possible to dynamically introspect the constraints for\n        # automatic testing.\n   "}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "_param_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     setattr(func, \"_skl_parameter_constraints\", parameter_constraints)\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            global_skip_validation = get_config()[\"skip_parameter_validation\"]\n            if global_skip_validation:\n                return func(*args, **kwargs)\n\n            func_sig = signature(func)\n\n            # Map *args/**kwargs to the function signature\n            params = func_sig.bind(*args, **kwargs)\n            params.apply_defaults()\n\n            # ignore self/cls and positional/keyword markers\n            to_ignore = [\n                p.name\n                for p in func_sig.parameters.values()\n                if p.kind in (p.VAR_POSITIONAL, p.VAR_KEYWORD)\n            ]\n            to_ignore += [\"self\", \"cls\"]\n            params = {k: v for k, v in params.arguments.items() if k not in to_ignore}\n\n            validate_parameter_constraints(\n                parameter_constraints, params, caller_name=func.__qualname__\n            )\n\n            try:\n                with config_context(\n                    skip_parameter_validation=(\n                        prefer_skip_nested_validation or global_skip_validation\n                    )\n                ):\n                    return func(*args, **kwargs)\n            except InvalidParameterError as e:\n                # When the function is just a wrapper around an estimator, we allow\n                # the function to delegate validation to the estimator, but we replace\n                # the name of the estimator by the name of the function in the error\n                # message to avoid confusion.\n                msg = re.sub(\n                    r\"parameter of \\w+ must be\",\n                    f\"parameter of {func.__qualname__} must be\",\n                    str(e),\n                )\n                raise InvalidParameterError(msg) from e\n\n        return wrapper\n\n    return decorator\n\n\nclass RealNotInt(Real):\n    \"\"\"A type that represents reals that are not instances of"}, {"start_line": 21000, "end_line": 23000, "belongs_to": {"file_name": "test_param_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "is_satisfied_by(5)\n    assert constraint.is_satisfied_by(LeaveOneOut())\n    assert constraint.is_satisfied_by([([1, 2], [3, 4]), ([3, 4], [1, 2])])\n    assert constraint.is_satisfied_by(None)\n    assert not constraint.is_satisfied_by(\"not a CV object\")\n\n\ndef test_third_party_estimator():\n    \"\"\"Check that the validation from a scikit-learn estimator inherited by a third\n    party estimator does not impose a match between the dict of constraints and the\n    parameters of the estimator.\n    \"\"\"\n\n    class ThirdPartyEstimator(_Estimator):\n        def __init__(self, b):\n            self.b = b\n            super().__init__(a=0)\n\n        def fit(self, X=None, y=None):\n            super().fit(X, y)\n\n    # does not raise, even though \"b\" is not in the constraints dict and \"a\" is not\n    # a parameter of the estimator.\n    ThirdPartyEstimator(b=0).fit()\n\n\ndef test_interval_real_not_int():\n    \"\"\"Check for the type RealNotInt in the Interval constraint.\"\"\"\n    constraint = Interval(RealNotInt, 0, 1, closed=\"both\")\n    assert constraint.is_satisfied_by(1.0)\n    assert not constraint.is_satisfied_by(1)\n\n\ndef test_real_not_int():\n    \"\"\"Check for the RealNotInt type.\"\"\"\n    assert isinstance(1.0, RealNotInt)\n    assert not isinstance(1, RealNotInt)\n    assert isinstance(np.float64(1), RealNotInt)\n    assert not isinstance(np.int64(1), RealNotInt)\n\n\ndef test_skip_param_validation():\n    \"\"\"Check that param validation can be skipped using config_context.\"\"\"\n\n    @validate_params({\"a\": [int]}, prefer_skip_nested_validation=True)\n    def f(a):\n        pass\n\n    with pytest.raises(InvalidParameterError, match=\"The 'a' parameter\"):\n        f(a=\"1\")\n\n    # does not raise\n    with config_context(skip_parameter_validation=True):\n        f(a=\"1\")\n\n\n@pytest.mark.parametrize(\"prefer_skip_nested_validation\", [True, False])\ndef test_skip_nested_validation(prefer_skip_nested_validation):\n    \"\"\"Check that nested validation can be skipped.\"\"\"\n\n    @validate_params({\"a\": [int]}, prefer_skip_neste"}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "test_param_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "arameterError, match=\"The 'param1' parameter\"):\n        f(param1=\"wrong\")\n\n    # param2 is not validated: any type is valid.\n    class SomeType:\n        pass\n\n    f(param2=SomeType)\n    f(param2=SomeType())\n\n\ndef test_pandas_na_constraint_with_pd_na():\n    \"\"\"Add a specific test for checking support for `pandas.NA`.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    na_constraint = _PandasNAConstraint()\n    assert na_constraint.is_satisfied_by(pd.NA)\n    assert not na_constraint.is_satisfied_by(np.array([1, 2, 3]))\n\n\ndef test_iterable_not_string():\n    \"\"\"Check that a string does not satisfy the _IterableNotString constraint.\"\"\"\n    constraint = _IterablesNotString()\n    assert constraint.is_satisfied_by([1, 2, 3])\n    assert constraint.is_satisfied_by(range(10))\n    assert not constraint.is_satisfied_by(\"some string\")\n\n\ndef test_cv_objects():\n    \"\"\"Check that the _CVObjects constraint accepts all current ways\n    to pass cv objects.\"\"\"\n    constraint = _CVObjects()\n    assert constraint.is_satisfied_by(5)\n    assert constraint.is_satisfied_by(LeaveOneOut())\n    assert constraint.is_satisfied_by([([1, 2], [3, 4]), ([3, 4], [1, 2])])\n    assert constraint.is_satisfied_by(None)\n    assert not constraint.is_satisfied_by(\"not a CV object\")\n\n\ndef test_third_party_estimator():\n    \"\"\"Check that the validation from a scikit-learn estimator inherited by a third\n    party estimator does not impose a match between the dict of constraints and the\n    parameters of the estimator.\n    \"\"\"\n\n    class ThirdPartyEstimator(_Estimator):\n        def __init__(self, b):\n            self.b = b\n            super().__init__(a=0)\n\n        def fit(self, X=None, y=None):\n            super().fit(X, y)\n\n    # does not raise, even though \"b\" is not in the constraints dict and \"a\" is not\n    # a parameter of the estimator.\n    ThirdPartyEstimator(b=0).fit()\n\n\ndef test_interval_real_not_int():\n    \"\"\"Check for the type RealNotInt in the Interval constraint.\"\"\"\n    constraint = Interval(RealNotInt, 0,"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_param_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "from numbers import Integral, Real\n\nimport numpy as np\nimport pytest\nfrom scipy.sparse import csr_matrix\n\nfrom sklearn._config import config_context, get_config\nfrom sklearn.base import BaseEstimator, _fit_context\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.utils import deprecated\nfrom sklearn.utils._param_validation import (\n    HasMethods,\n    Hidden,\n    Interval,\n    InvalidParameterError,\n    MissingValues,\n    Options,\n    RealNotInt,\n    StrOptions,\n    _ArrayLikes,\n    _Booleans,\n    _Callables,\n    _CVObjects,\n    _InstancesOf,\n    _IterablesNotString,\n    _NanConstraint,\n    _NoneConstraint,\n    _PandasNAConstraint,\n    _RandomStates,\n    _SparseMatrices,\n    _VerboseHelper,\n    generate_invalid_param_val,\n    generate_valid_param,\n    make_constraint,\n    validate_params,\n)\nfrom sklearn.utils.fixes import CSR_CONTAINERS\n\n\n# Some helpers for the tests\n@validate_params(\n    {\"a\": [Real], \"b\": [Real], \"c\": [Real], \"d\": [Real]},\n    prefer_skip_nested_validation=True,\n)\ndef _func(a, b=0, *args, c, d=0, **kwargs):\n    \"\"\"A function to test the validation of functions.\"\"\"\n\n\nclass _Class:\n    \"\"\"A class to test the _InstancesOf constraint and the validation of methods.\"\"\"\n\n    @validate_params({\"a\": [Real]}, prefer_skip_nested_validation=True)\n    def _method(self, a):\n        \"\"\"A validated method\"\"\"\n\n    @deprecated()\n    @validate_params({\"a\": [Real]}, prefer_skip_nested_validation=True)\n    def _deprecated_method(self, a):\n        \"\"\"A deprecated validated method\"\"\"\n\n\nclass _Estimator(BaseEstimator):\n    \"\"\"An estimator to test the validation of estimator parameters.\"\"\"\n\n    _parameter_constraints: dict = {\"a\": [Real]}\n\n    def __init__(self, a):\n        self.a = a\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X=None, y=None):\n        pass\n\n\n@pytest.mark.parametrize(\"interval_type\", [Integral, Real])\ndef test_interval_range(interval_type):\n    \"\"\"Check the range of values depending on closed.\"\"\"\n    interval = Inte"}, {"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      else:\n            return state\n\n    def __setstate__(self, state):\n        if type(self).__module__.startswith(\"sklearn.\"):\n            pickle_version = state.pop(\"_sklearn_version\", \"pre-0.18\")\n            if pickle_version != __version__:\n                warnings.warn(\n                    InconsistentVersionWarning(\n                        estimator_name=self.__class__.__name__,\n                        current_sklearn_version=__version__,\n                        original_sklearn_version=pickle_version,\n                    ),\n                )\n        try:\n            super().__setstate__(state)\n        except AttributeError:\n            self.__dict__.update(state)\n\n    def __sklearn_tags__(self):\n        return Tags(\n            estimator_type=None,\n            target_tags=TargetTags(required=False),\n            transformer_tags=None,\n            regressor_tags=None,\n            classifier_tags=None,\n        )\n\n    def _validate_params(self):\n        \"\"\"Validate types and values of constructor parameters\n\n        The expected type and values must be defined in the `_parameter_constraints`\n        class attribute, which is a dictionary `param_name: list of constraints`. See\n        the docstring of `validate_parameter_constraints` for a description of the\n        accepted constraints.\n        \"\"\"\n        validate_parameter_constraints(\n            self._parameter_constraints,\n            self.get_params(deep=False),\n            caller_name=self.__class__.__name__,\n        )\n\n\nclass ClassifierMixin:\n    \"\"\"Mixin class for all classifiers in scikit-learn.\n\n    This mixin defines the following functionality:\n\n    - set estimator type to `\"classifier\"` through the `estimator_type` tag;\n    - `score` method that default to :func:`~sklearn.metrics.accuracy_score`.\n    - enforce that `fit` requires `y` to be passed through the `requires_y` tag,\n      which is done by setting the classifier type tag.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "f, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass HasMutableParameters(BaseEstimator):\n    def __init__(self, p=object()):\n        self.p = p\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass HasImmutableParameters(BaseEstimator):\n    # Note that object is an uninitialized class, thus immutable.\n    def __init__(self, p=42, q=np.int32(42), r=object):\n        self.p = p\n        self.q = q\n        self.r = r\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ModifiesValueInsteadOfRaisingError(BaseEstimator):\n    def __init__(self, p=0):\n        self.p = p\n\n    def set_params(self, **kwargs):\n        if \"p\" in kwargs:\n            p = kwargs.pop(\"p\")\n            if p < 0:\n                p = 0\n            self.p = p\n        return super().set_params(**kwargs)\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ModifiesAnotherValue(BaseEstimator):\n    def __init__(self, a=0, b=\"method1\"):\n        self.a = a\n        self.b = b\n\n    def set_params(self, **kwargs):\n        if \"a\" in kwargs:\n            a = kwargs.pop(\"a\")\n            self.a = a\n            if a is None:\n                kwargs.pop(\"b\")\n                self.b = \"method2\"\n        return super().set_params(**kwargs)\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass NoCheckinPredict(BaseBadClassifier):\n    def fit(self, X, y):\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass NoSparseClassifier(BaseBadClassifier):\n    def __init__(self, raise_for_type=None):\n        # raise_for_type : str, expects \"sparse_array\" or \"sparse_matrix\"\n        self.raise_for_type = raise_for_type\n\n    def fit(self, X, y):\n        X, y = validate_data(self, X, y, accept_sparse=[\"csr\", \"csc\"])\n        if self.raise_for_type == \"sparse_array\":\n            correct_type = isinstance(X, sp.spar"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_public_functions.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "from importlib import import_module\nfrom inspect import signature\nfrom numbers import Integral, Real\n\nimport pytest\n\nfrom sklearn.utils._param_validation import (\n    Interval,\n    InvalidParameterError,\n    generate_invalid_param_val,\n    generate_valid_param,\n    make_constraint,\n)\n\n\ndef _get_func_info(func_module):\n    module_name, func_name = func_module.rsplit(\".\", 1)\n    module = import_module(module_name)\n    func = getattr(module, func_name)\n\n    func_sig = signature(func)\n    func_params = [\n        p.name\n        for p in func_sig.parameters.values()\n        if p.kind not in (p.VAR_POSITIONAL, p.VAR_KEYWORD)\n    ]\n\n    # The parameters `*args` and `**kwargs` are ignored since we cannot generate\n    # constraints.\n    required_params = [\n        p.name\n        for p in func_sig.parameters.values()\n        if p.default is p.empty and p.kind not in (p.VAR_POSITIONAL, p.VAR_KEYWORD)\n    ]\n\n    return func, func_name, func_params, required_params\n\n\ndef _check_function_param_validation(\n    func, func_name, func_params, required_params, parameter_constraints\n):\n    \"\"\"Check that an informative error is raised when the value of a parameter does not\n    have an appropriate type or value.\n    \"\"\"\n    # generate valid values for the required parameters\n    valid_required_params = {}\n    for param_name in required_params:\n        if parameter_constraints[param_name] == \"no_validation\":\n            valid_required_params[param_name] = 1\n        else:\n            valid_required_params[param_name] = generate_valid_param(\n                make_constraint(parameter_constraints[param_name][0])\n            )\n\n    # check that there is a constraint for each parameter\n    if func_params:\n        validation_params = parameter_constraints.keys()\n        unexpected_params = set(validation_params) - set(func_params)\n        missing_params = set(func_params) - set(validation_params)\n        err_msg = (\n            \"Mismatch between _parameter_constraints and the parameters of\"\n     "}], "retrieved_count": 10, "cost_time": 0.33419179916381836}
{"question": "How could Scikit-learn's estimator interface be refactored to support async/await without breaking backward compatibility with synchronous estimators?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn's estimator interface could be refactored to support async/await while maintaining backward compatibility through several design patterns and implementation strategies:\n\n1. **Dual Interface Pattern**:\n   - **Synchronous Interface**: Maintain existing sync methods (fit, predict, transform)\n   - **Asynchronous Interface**: Add async versions (afit, apredict, atransform)\n   - **Method Detection**: Use hasattr() to check for async method availability\n   - **Graceful Fallback**: Fall back to sync methods when async not available\n   - **Interface Consistency**: Ensure both interfaces have same signatures\n\n2. **Abstract Base Class Extension**:\n   - **AsyncBaseEstimator**: New base class for async estimators\n   - **AsyncMixin**: Mixin providing async method implementations\n   - **Hybrid Base Classes**: Classes that support both sync and async\n   - **Method Resolution**: Clear precedence rules for sync vs async methods\n   - **Inheritance Hierarchy**: Extend existing hierarchy without breaking changes\n\n3. **Method Wrapper Pattern**:\n   - **AsyncWrapper**: Wrapper class that provides async interface for sync estimators\n   - **SyncWrapper**: Wrapper class that provides sync interface for async estimators\n   - **Automatic Wrapping**: Detect estimator type and wrap appropriately\n   - **Transparent Interface**: Wrappers maintain same API as wrapped estimators\n   - **Performance Optimization**: Avoid unnecessary wrapping when possible\n\n4. **Context Manager Integration**:\n   - **AsyncContext**: Context manager for async operations\n   - **Resource Management**: Proper cleanup of async resources\n   - **Error Handling**: Consistent error handling across sync/async\n   - **Cancellation Support**: Support for cancelling async operations\n   - **Timeout Management**: Built-in timeout handling for async operations\n\n5. **Pipeline and Meta-Estimator Support**:\n   - **AsyncPipeline**: Pipeline that supports async estimators\n   - **Mixed Pipelines**: Pipelines with both sync and async estimators\n   - **AsyncGridSearchCV**: Async version of hyperparameter search\n   - **AsyncCrossValidation**: Async cross-validation utilities\n   - **Composition Patterns**: Patterns for composing async and sync estimators\n\n6. **Backward Compatibility Strategies**:\n   - **Deprecation Warnings**: Warn about deprecated sync methods in async estimators\n   - **Gradual Migration**: Allow gradual migration from sync to async\n   - **Feature Detection**: Detect async support at runtime\n   - **Default Behavior**: Maintain existing behavior for existing code\n   - **Documentation**: Clear documentation of migration paths\n\n7. **Performance Considerations**:\n   - **Async Overhead**: Minimize overhead of async/await for simple operations\n   - **Batch Processing**: Support async batch processing for large datasets\n   - **Parallel Execution**: Leverage async for parallel execution where beneficial\n   - **Memory Management**: Efficient memory usage in async context\n   - **Resource Pooling**: Pool async resources for better performance\n\n8. **Error Handling and Debugging**:\n   - **Async Exception Handling**: Proper exception handling in async context\n   - **Stack Trace Preservation**: Maintain useful stack traces for debugging\n   - **Timeout Handling**: Graceful handling of timeouts\n   - **Cancellation Support**: Support for cancelling long-running operations\n   - **Error Propagation**: Consistent error propagation across sync/async boundary\n\n9. **Testing and Validation**:\n   - **Async Test Framework**: Testing framework for async estimators\n   - **Compatibility Tests**: Tests ensuring sync/async compatibility\n   - **Performance Benchmarks**: Benchmarks comparing sync vs async performance\n   - **Integration Tests**: Tests for mixed sync/async pipelines\n   - **Regression Testing**: Ensure no regressions in existing functionality\n\n10. **Implementation Guidelines**:\n    - **Method Signatures**: Keep async method signatures identical to sync versions\n    - **Return Values**: Ensure consistent return value types\n    - **Parameter Validation**: Apply same validation rules to both interfaces\n    - **Documentation**: Comprehensive documentation for both interfaces\n    - **Examples**: Provide examples showing both sync and async usage", "score": null, "retrieved_content": [{"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "_rfe.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/feature_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ", **routed_params.estimator.score\n        )\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n        return self.support_\n\n    @available_if(_estimator_has(\"decision_function\"))\n    def decision_function(self, X):\n        \"\"\"Compute the decision function of ``X``.\n\n        Parameters\n        ----------\n        X : {array-like or sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        score : array, shape = [n_samples, n_classes] or [n_samples]\n            The decision function of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n            Regression and binary classification produce an array of shape\n            [n_samples].\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.decision_function(self.transform(X))\n\n    @available_if(_estimator_has(\"predict_proba\"))\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like or sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : array of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.predict_proba(self.transform(X))\n\n    @available_if(_estimator_has(\"predict_log_proba\"))\n    def predict_log_proba(self, X):\n        \"\"\"Predict class log-probabilities for X.\n\n        Parameters\n        ----------\n        X : array of shape [n_sa"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "_classification_threshold.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ay of shape (n_samples, n_classes)\n            The class probabilities of the input samples.\n        \"\"\"\n        _check_is_fitted(self)\n        estimator = getattr(self, \"estimator_\", self.estimator)\n        return estimator.predict_proba(X)\n\n    @available_if(_estimator_has(\"predict_log_proba\"))\n    def predict_log_proba(self, X):\n        \"\"\"Predict logarithm class probabilities for `X` using the fitted estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        log_probabilities : ndarray of shape (n_samples, n_classes)\n            The logarithm class probabilities of the input samples.\n        \"\"\"\n        _check_is_fitted(self)\n        estimator = getattr(self, \"estimator_\", self.estimator)\n        return estimator.predict_log_proba(X)\n\n    @available_if(_estimator_has(\"decision_function\"))\n    def decision_function(self, X):\n        \"\"\"Decision function for samples in `X` using the fitted estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        decisions : ndarray of shape (n_samples,)\n            The decision function computed the fitted estimator.\n        \"\"\"\n        _check_is_fitted(self)\n        estimator = getattr(self, \"estimator_\", self.estimator)\n        return estimator.decision_function(X)\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.classifier_tags.multi_class = False\n        tags.input_tags.sparse = get_tags(self.estimator).input_tags.sparse\n        return tags\n\n\nclass FixedThresholdClassifier(BaseThresholdClassifier):\n    \"\"\"Binary classifier that"}, {"start_line": 40000, "end_line": 42000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " metadata to `predict`. Please implement a custom \"\n                        \"`fit_predict` method to forward metadata to `predict` as well.\"\n                        \"Alternatively, you can explicitly do `set_predict_request`\"\n                        \"and set all values to `False` to disable metadata routed to \"\n                        \"`predict`, if that's an option.\"\n                    ),\n                    UserWarning,\n                )\n\n        # override for transductive outlier detectors like LocalOulierFactor\n        return self.fit(X, **kwargs).predict(X)\n\n\nclass MetaEstimatorMixin:\n    \"\"\"Mixin class for all meta estimators in scikit-learn.\n\n    This mixin is empty, and only exists to indicate that the estimator is a\n    meta-estimator.\n\n    .. versionchanged:: 1.6\n        The `_required_parameters` is now removed and is unnecessary since tests are\n        refactored and don't use this anymore.\n\n    Examples\n    --------\n    >>> from sklearn.base import MetaEstimatorMixin\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> class MyEstimator(MetaEstimatorMixin):\n    ...     def __init__(self, *, estimator=None):\n    ...         self.estimator = estimator\n    ...     def fit(self, X, y=None):\n    ...         if self.estimator is None:\n    ...             self.estimator_ = LogisticRegression()\n    ...         else:\n    ...             self.estimator_ = self.estimator\n    ...         return self\n    >>> X, y = load_iris(return_X_y=True)\n    >>> estimator = MyEstimator().fit(X, y)\n    >>> estimator.estimator_\n    LogisticRegression()\n    \"\"\"\n\n\nclass MultiOutputMixin:\n    \"\"\"Mixin to mark estimators that support multioutput.\"\"\"\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.target_tags.multi_output = True\n        return tags\n\n\nclass _UnstableArchMixin:\n    \"\"\"Mark estimators that are non-determinstic on 32bit or PowerPC\"\"\"\n\n    def __sklearn_tags__(self):\n    "}, {"start_line": 21000, "end_line": 23000, "belongs_to": {"file_name": "multioutput.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "r of outputs of Y for fit {0} and\"\n                \" score {1} should be same\".format(n_outputs_, y.shape[1])\n            )\n        y_pred = self.predict(X)\n        return np.mean(np.all(y == y_pred, axis=1))\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        # FIXME\n        tags._skip_test = True\n        return tags\n\n\ndef _available_if_base_estimator_has(attr):\n    \"\"\"Return a function to check if `base_estimator` or `estimators_` has `attr`.\n\n    Helper for Chain implementations.\n    \"\"\"\n\n    def _check(self):\n        return hasattr(self._get_estimator(), attr) or all(\n            hasattr(est, attr) for est in self.estimators_\n        )\n\n    return available_if(_check)\n\n\nclass _BaseChain(BaseEstimator, metaclass=ABCMeta):\n    _parameter_constraints: dict = {\n        \"base_estimator\": [\n            HasMethods([\"fit\", \"predict\"]),\n            StrOptions({\"deprecated\"}),\n        ],\n        \"estimator\": [\n            HasMethods([\"fit\", \"predict\"]),\n            Hidden(None),\n        ],\n        \"order\": [\"array-like\", StrOptions({\"random\"}), None],\n        \"cv\": [\"cv_object\", StrOptions({\"prefit\"})],\n        \"random_state\": [\"random_state\"],\n        \"verbose\": [\"boolean\"],\n    }\n\n    # TODO(1.9): Remove base_estimator\n    def __init__(\n        self,\n        estimator=None,\n        *,\n        order=None,\n        cv=None,\n        random_state=None,\n        verbose=False,\n        base_estimator=\"deprecated\",\n    ):\n        self.estimator = estimator\n        self.base_estimator = base_estimator\n        self.order = order\n        self.cv = cv\n        self.random_state = random_state\n        self.verbose = verbose\n\n    # TODO(1.8): This is a temporary getter method to validate input wrt deprecation.\n    # It was only included to avoid relying on the presence of self.estimator_\n    def _get_estimator(self):\n        \"\"\"Get and validate estimator.\"\"\"\n\n        if self.estimator is not None and (self.base_estimator != \"deprecated\"):\n            rais"}, {"start_line": 15000, "end_line": 17000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"is {y_type}.\"\n            )\n        return super().fit(X, y)\n\n    # Toy classifier that only supports binary classification.\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.classifier_tags.multi_class = False\n        return tags\n\n\nclass RequiresPositiveXRegressor(LinearRegression):\n    def fit(self, X, y):\n        # reject sparse X to be able to call (X < 0).any()\n        X, y = validate_data(self, X, y, accept_sparse=False, multi_output=True)\n        if (X < 0).any():\n            raise ValueError(\"Negative values in data passed to X.\")\n        return super().fit(X, y)\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.positive_only = True\n        # reject sparse X to be able to call (X < 0).any()\n        tags.input_tags.sparse = False\n        return tags\n\n\nclass RequiresPositiveYRegressor(LinearRegression):\n    def fit(self, X, y):\n        X, y = validate_data(self, X, y, accept_sparse=True, multi_output=True)\n        if (y <= 0).any():\n            raise ValueError(\"negative y values not supported!\")\n        return super().fit(X, y)\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.target_tags.positive_only = True\n        return tags\n\n\nclass PoorScoreLogisticRegression(LogisticRegression):\n    def decision_function(self, X):\n        return super().decision_function(X) + 1\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.classifier_tags.poor_score = True\n        return tags\n\n\nclass PartialFitChecksName(BaseEstimator):\n    def fit(self, X, y):\n        validate_data(self, X, y)\n        return self\n\n    def partial_fit(self, X, y):\n        reset = not hasattr(self, \"_fitted\")\n        validate_data(self, X, y, reset=reset)\n        self._fitted = True\n        return self\n\n\nclass BrokenArrayAPI(BaseEstimator):\n    \"\"\"Make different predictions when using Numpy and the Array API\"\"\"\n\n    def fit(self, X, y):\n   "}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "_classification_threshold.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  raise ValueError(\n                f\"Only binary classification is supported. Unknown label type: {y_type}\"\n            )\n\n        self._fit(X, y, **params)\n\n        if hasattr(self.estimator_, \"n_features_in_\"):\n            self.n_features_in_ = self.estimator_.n_features_in_\n        if hasattr(self.estimator_, \"feature_names_in_\"):\n            self.feature_names_in_ = self.estimator_.feature_names_in_\n\n        return self\n\n    @property\n    def classes_(self):\n        \"\"\"Classes labels.\"\"\"\n        return self.estimator_.classes_\n\n    @available_if(_estimator_has(\"predict_proba\"))\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for `X` using the fitted estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        probabilities : ndarray of shape (n_samples, n_classes)\n            The class probabilities of the input samples.\n        \"\"\"\n        _check_is_fitted(self)\n        estimator = getattr(self, \"estimator_\", self.estimator)\n        return estimator.predict_proba(X)\n\n    @available_if(_estimator_has(\"predict_log_proba\"))\n    def predict_log_proba(self, X):\n        \"\"\"Predict logarithm class probabilities for `X` using the fitted estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        log_probabilities : ndarray of shape (n_samples, n_classes)\n            The logarithm class probabilities of the input samples.\n        \"\"\"\n        _check_is_fitted(self)\n        estimator = getattr(self, \"estimator_\", self.estimator)\n        return estimator.predict_log_proba(X)\n\n    @available_if"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "multioutput.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tial_fit_params\n    if first_time:\n        estimator = clone(estimator)\n\n    if classes is not None:\n        estimator.partial_fit(X, y, classes=classes, **partial_fit_params)\n    else:\n        estimator.partial_fit(X, y, **partial_fit_params)\n    return estimator\n\n\ndef _available_if_estimator_has(attr):\n    \"\"\"Return a function to check if the sub-estimator(s) has(have) `attr`.\n\n    Helper for Chain implementations.\n    \"\"\"\n\n    def _check(self):\n        if hasattr(self, \"estimators_\"):\n            return all(hasattr(est, attr) for est in self.estimators_)\n\n        if hasattr(self.estimator, attr):\n            return True\n\n        return False\n\n    return available_if(_check)\n\n\nclass _MultiOutputEstimator(MetaEstimatorMixin, BaseEstimator, metaclass=ABCMeta):\n    _parameter_constraints: dict = {\n        \"estimator\": [HasMethods([\"fit\", \"predict\"])],\n        \"n_jobs\": [Integral, None],\n    }\n\n    @abstractmethod\n    def __init__(self, estimator, *, n_jobs=None):\n        self.estimator = estimator\n        self.n_jobs = n_jobs\n\n    @_available_if_estimator_has(\"partial_fit\")\n    @_fit_context(\n        # MultiOutput*.estimator is not validated yet\n        prefer_skip_nested_validation=False\n    )\n    def partial_fit(self, X, y, classes=None, sample_weight=None, **partial_fit_params):\n        \"\"\"Incrementally fit a separate model for each class output.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input data.\n\n        y : {array-like, sparse matrix} of shape (n_samples, n_outputs)\n            Multi-output targets.\n\n        classes : list of ndarray of shape (n_outputs,), default=None\n            Each array is unique classes for one output in str/int.\n            Can be obtained via\n            ``[np.unique(y[:, i]) for i in range(y.shape[1])]``, where `y`\n            is the target matrix of the entire dataset.\n            This argument is required for the first call to partial_fit\n        "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "multioutput.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "uter,\n    MethodMapping,\n    _raise_for_params,\n    _routing_enabled,\n    process_routing,\n)\nfrom sklearn.utils.metaestimators import available_if\nfrom sklearn.utils.multiclass import check_classification_targets\nfrom sklearn.utils.parallel import Parallel, delayed\nfrom sklearn.utils.validation import (\n    _check_method_params,\n    _check_response_method,\n    check_is_fitted,\n    has_fit_parameter,\n    validate_data,\n)\n\n__all__ = [\n    \"ClassifierChain\",\n    \"MultiOutputClassifier\",\n    \"MultiOutputRegressor\",\n    \"RegressorChain\",\n]\n\n\ndef _fit_estimator(estimator, X, y, sample_weight=None, **fit_params):\n    estimator = clone(estimator)\n    if sample_weight is not None:\n        estimator.fit(X, y, sample_weight=sample_weight, **fit_params)\n    else:\n        estimator.fit(X, y, **fit_params)\n    return estimator\n\n\ndef _partial_fit_estimator(\n    estimator, X, y, classes=None, partial_fit_params=None, first_time=True\n):\n    partial_fit_params = {} if partial_fit_params is None else partial_fit_params\n    if first_time:\n        estimator = clone(estimator)\n\n    if classes is not None:\n        estimator.partial_fit(X, y, classes=classes, **partial_fit_params)\n    else:\n        estimator.partial_fit(X, y, **partial_fit_params)\n    return estimator\n\n\ndef _available_if_estimator_has(attr):\n    \"\"\"Return a function to check if the sub-estimator(s) has(have) `attr`.\n\n    Helper for Chain implementations.\n    \"\"\"\n\n    def _check(self):\n        if hasattr(self, \"estimators_\"):\n            return all(hasattr(est, attr) for est in self.estimators_)\n\n        if hasattr(self.estimator, attr):\n            return True\n\n        return False\n\n    return available_if(_check)\n\n\nclass _MultiOutputEstimator(MetaEstimatorMixin, BaseEstimator, metaclass=ABCMeta):\n    _parameter_constraints: dict = {\n        \"estimator\": [HasMethods([\"fit\", \"predict\"])],\n        \"n_jobs\": [Integral, None],\n    }\n\n    @abstractmethod\n    def __init__(self, estimator, *, n_jobs=None):\n        self.estimator "}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "(TransformerMixin, BaseEstimator):\n    def __init__(self, sparse_container=None):\n        self.sparse_container = sparse_container\n\n    def fit(self, X, y=None):\n        validate_data(self, X)\n        return self\n\n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X)\n\n    def transform(self, X):\n        check_is_fitted(self)\n        X = validate_data(self, X, accept_sparse=True, reset=False)\n        return self.sparse_container(X)\n\n\nclass EstimatorInconsistentForPandas(BaseEstimator):\n    def fit(self, X, y):\n        try:\n            from pandas import DataFrame\n\n            if isinstance(X, DataFrame):\n                self.value_ = X.iloc[0, 0]\n            else:\n                X = check_array(X)\n                self.value_ = X[1, 0]\n            return self\n\n        except ImportError:\n            X = check_array(X)\n            self.value_ = X[1, 0]\n            return self\n\n    def predict(self, X):\n        X = check_array(X)\n        return np.array([self.value_] * X.shape[0])\n\n\nclass UntaggedBinaryClassifier(SGDClassifier):\n    # Toy classifier that only supports binary classification, will fail tests.\n    def fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None):\n        super().fit(X, y, coef_init, intercept_init, sample_weight)\n        if len(self.classes_) > 2:\n            raise ValueError(\"Only 2 classes are supported\")\n        return self\n\n    def partial_fit(self, X, y, classes=None, sample_weight=None):\n        super().partial_fit(X=X, y=y, classes=classes, sample_weight=sample_weight)\n        if len(self.classes_) > 2:\n            raise ValueError(\"Only 2 classes are supported\")\n        return self\n\n\nclass TaggedBinaryClassifier(UntaggedBinaryClassifier):\n    def fit(self, X, y):\n        y_type = type_of_target(y, input_name=\"y\", raise_unknown=True)\n        if y_type != \"binary\":\n            raise ValueError(\n                \"Only binary classification is supported. The type of the target \"\n                f"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "_mocking.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    \"\"\"\n        if self.methods_to_check == \"all\" or \"score\" in self.methods_to_check:\n            self._check_X_y(X, Y)\n        if self.foo_param > 1:\n            score = 1.0\n        else:\n            score = 0.0\n        return score\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags._skip_test = True\n        tags.input_tags.two_d_array = False\n        tags.target_tags.one_d_labels = True\n        return tags\n\n\n# Deactivate key validation for CheckingClassifier because we want to be able to\n# call fit with arbitrary fit_params and record them. Without this change, we\n# would get an error because those arbitrary params are not expected.\nCheckingClassifier.set_fit_request = RequestMethod(  # type: ignore[assignment,method-assign]\n    name=\"fit\", keys=[], validate_keys=False\n)\n\n\nclass NoSampleWeightWrapper(BaseEstimator):\n    \"\"\"Wrap estimator which will not expose `sample_weight`.\n\n    Parameters\n    ----------\n    est : estimator, default=None\n        The estimator to wrap.\n    \"\"\"\n\n    def __init__(self, est=None):\n        self.est = est\n\n    def fit(self, X, y):\n        return self.est.fit(X, y)\n\n    def predict(self, X):\n        return self.est.predict(X)\n\n    def predict_proba(self, X):\n        return self.est.predict_proba(X)\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags._skip_test = True\n        return tags\n\n\ndef _check_response(method):\n    def check(self):\n        return self.response_methods is not None and method in self.response_methods\n\n    return check\n\n\nclass _MockEstimatorOnOffPrediction(BaseEstimator):\n    \"\"\"Estimator for which we can turn on/off the prediction methods.\n\n    Parameters\n    ----------\n    response_methods: list of \\\n            {\"predict\", \"predict_proba\", \"decision_function\"}, default=None\n        List containing the response implemented by the estimator. When, the\n        response is in the list, it will return the name of the response method\n        when"}], "retrieved_count": 10, "cost_time": 0.34715914726257324}
{"question": "How could Scikit-learn's pipeline system be refactored to support more flexible data flow patterns while maintaining the simple interface?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn's pipeline system could be refactored to support more flexible data flow patterns while maintaining the simple interface through several architectural improvements and design patterns:\n\n1. **Graph-Based Pipeline Architecture**:\n   - **Directed Acyclic Graph (DAG)**: Replace linear pipeline with DAG structure\n   - **Node Types**: Support different node types (transformer, predictor, splitter, merger)\n   - **Edge Types**: Define different types of data flow between nodes\n   - **Conditional Flow**: Support conditional branching based on data characteristics\n   - **Parallel Processing**: Enable parallel execution of independent branches\n\n2. **Enhanced Data Flow Patterns**:\n   - **Split-Apply-Combine**: Support splitting data, applying different transformations, then combining\n   - **Branching Pipelines**: Multiple paths through the pipeline based on conditions\n   - **Feedback Loops**: Allow data to flow back to previous steps when needed\n   - **Data Fusion**: Combine multiple data streams at different points\n   - **Streaming Processing**: Support for streaming data with windowing and buffering\n\n3. **Backward Compatibility Layer**:\n   - **Linear Pipeline Wrapper**: Maintain existing Pipeline interface for simple cases\n   - **Automatic Conversion**: Convert linear pipelines to graph representation internally\n   - **Interface Preservation**: Keep existing fit/predict/transform methods unchanged\n   - **Parameter Compatibility**: Maintain existing parameter setting mechanisms\n   - **Documentation**: Clear migration guide from linear to graph pipelines\n\n4. **Node and Edge Abstraction**:\n   - **BaseNode Class**: Abstract base class for all pipeline nodes\n   - **TransformerNode**: Specialized node for data transformations\n   - **PredictorNode**: Specialized node for making predictions\n   - **SplitterNode**: Node that splits data into multiple streams\n   - **MergerNode**: Node that combines multiple data streams\n\n5. **Advanced Composition Patterns**:\n   - **Nested Pipelines**: Allow pipelines within pipelines\n   - **Conditional Steps**: Steps that execute based on data conditions\n   - **Parallel Branches**: Independent processing paths that can run in parallel\n   - **Cross-Validation Integration**: Built-in support for cross-validation splits\n   - **Ensemble Methods**: Native support for ensemble learning patterns\n\n6. **Data Flow Control**:\n   - **Data Routing**: Intelligent routing of data between pipeline steps\n   - **Type Checking**: Automatic type checking and conversion between steps\n   - **Memory Management**: Efficient memory usage for large data flows\n   - **Caching Strategy**: Intelligent caching of intermediate results\n   - **Error Handling**: Robust error handling and recovery mechanisms\n\n7. **Visualization and Debugging**:\n   - **Pipeline Visualization**: Visual representation of pipeline structure\n   - **Data Flow Tracking**: Track data as it flows through the pipeline\n   - **Performance Profiling**: Identify bottlenecks in pipeline execution\n   - **Debugging Tools**: Tools for debugging pipeline execution\n   - **Interactive Development**: Interactive pipeline building and testing\n\n8. **Optimization and Performance**:\n   - **Automatic Optimization**: Automatic optimization of pipeline execution order\n   - **Parallel Execution**: Parallel execution of independent pipeline steps\n   - **Memory Optimization**: Optimize memory usage across pipeline steps\n   - **Lazy Evaluation**: Lazy evaluation of pipeline steps when possible\n   - **Compilation**: Compile pipelines to optimized execution plans\n\n9. **Extensibility Framework**:\n   - **Custom Node Types**: Framework for creating custom node types\n   - **Plugin Architecture**: Plugin system for extending pipeline capabilities\n   - **Custom Data Types**: Support for custom data types and transformations\n   - **External Integration**: Easy integration with external tools and libraries\n   - **API Extensions**: Extensible API for advanced use cases\n\n10. **Implementation Strategy**:\n    - **Gradual Migration**: Gradual migration from linear to graph pipelines\n    - **Feature Flags**: Feature flags to enable/disable new functionality\n    - **Testing Framework**: Comprehensive testing framework for new features\n    - **Documentation**: Extensive documentation and examples\n    - **Community Feedback**: Gather feedback from the community during development", "score": null, "retrieved_content": [{"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_metadata_routing.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_is_valid,\n)\nfrom sklearn.utils.metadata_routing import (\n    MetadataRequest,\n    MetadataRouter,\n    MethodMapping,\n    _RoutingNotSupportedMixin,\n    get_routing_for_object,\n    process_routing,\n)\nfrom sklearn.utils.validation import check_is_fitted\n\nrng = np.random.RandomState(42)\nN, M = 100, 4\nX = rng.rand(N, M)\ny = rng.randint(0, 2, size=N)\nmy_groups = rng.randint(0, 10, size=N)\nmy_weights = rng.rand(N)\nmy_other_weights = rng.rand(N)\n\n\nclass SimplePipeline(BaseEstimator):\n    \"\"\"A very simple pipeline, assuming the last step is always a predictor.\n\n    Parameters\n    ----------\n    steps : iterable of objects\n        An iterable of transformers with the last step being a predictor.\n    \"\"\"\n\n    def __init__(self, steps):\n        self.steps = steps\n\n    def fit(self, X, y, **fit_params):\n        self.steps_ = []\n        params = process_routing(self, \"fit\", **fit_params)\n        X_transformed = X\n        for i, step in enumerate(self.steps[:-1]):\n            transformer = clone(step).fit(\n                X_transformed, y, **params.get(f\"step_{i}\").fit\n            )\n            self.steps_.append(transformer)\n            X_transformed = transformer.transform(\n                X_transformed, **params.get(f\"step_{i}\").transform\n            )\n\n        self.steps_.append(\n            clone(self.steps[-1]).fit(X_transformed, y, **params.predictor.fit)\n        )\n        return self\n\n    def predict(self, X, **predict_params):\n        check_is_fitted(self)\n        X_transformed = X\n        params = process_routing(self, \"predict\", **predict_params)\n        for i, step in enumerate(self.steps_[:-1]):\n            X_transformed = step.transform(X, **params.get(f\"step_{i}\").transform)\n\n        return self.steps_[-1].predict(X_transformed, **params.predictor.predict)\n\n    def get_metadata_routing(self):\n        router = MetadataRouter(owner=self.__class__.__name__)\n        for i, step in enumerate(self.steps[:-1]):\n            router.add(\n                **{f\"step_{i}\": step"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "caler', StandardScaler()), ('svc', SVC())])\n    >>> # The pipeline can be used as any other estimator\n    >>> # and avoids leaking the test set into the train set\n    >>> pipe.fit(X_train, y_train).score(X_test, y_test)\n    0.88\n    >>> # An estimator's parameter can be set using '__' syntax\n    >>> pipe.set_params(svc__C=10).fit(X_train, y_train).score(X_test, y_test)\n    0.76\n    \"\"\"\n\n    # BaseEstimator interface\n    _parameter_constraints: dict = {\n        \"steps\": [list, Hidden(tuple)],\n        \"transform_input\": [list, None],\n        \"memory\": [None, str, HasMethods([\"cache\"])],\n        \"verbose\": [\"boolean\"],\n    }\n\n    def __init__(self, steps, *, transform_input=None, memory=None, verbose=False):\n        self.steps = steps\n        self.transform_input = transform_input\n        self.memory = memory\n        self.verbose = verbose\n\n    def set_output(self, *, transform=None):\n        \"\"\"Set the output container when `\"transform\"` and `\"fit_transform\"` are called.\n\n        Calling `set_output` will set the output of all estimators in `steps`.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.\n\n        Returns\n        -------\n        self : estimator instance\n            Estimator instance.\n        \"\"\"\n        for _, _, step in self._iter():\n            _safe_set_output(step, transform=transform)\n        return self\n\n    def get_params(self, deep=True):\n        \"\"\"Get parameters for this estimator.\n\n        Returns the parameters given in the constructor as well as the\n        estimators contained within the `steps` of the `Pipeline`.\n\n        Parameters\n       "}, {"start_line": 59000, "end_line": 61000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "p : Pipeline\n        Returns a scikit-learn :class:`Pipeline` object.\n\n    See Also\n    --------\n    Pipeline : Class for creating a pipeline of transforms with a final\n        estimator.\n\n    Examples\n    --------\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.pipeline import make_pipeline\n    >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('gaussiannb', GaussianNB())])\n    \"\"\"\n    return Pipeline(\n        _name_estimators(steps),\n        transform_input=transform_input,\n        memory=memory,\n        verbose=verbose,\n    )\n\n\ndef _transform_one(transformer, X, y, weight, params):\n    \"\"\"Call transform and apply weight to output.\n\n    Parameters\n    ----------\n    transformer : estimator\n        Estimator to be used for transformation.\n\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Input data to be transformed.\n\n    y : ndarray of shape (n_samples,)\n        Ignored.\n\n    weight : float\n        Weight to be applied to the output of the transformation.\n\n    params : dict\n        Parameters to be passed to the transformer's ``transform`` method.\n\n        This should be of the form ``process_routing()[\"step_name\"]``.\n    \"\"\"\n    res = transformer.transform(X, **params.transform)\n    # if we have a weight for this transformer, multiply output\n    if weight is None:\n        return res\n    return res * weight\n\n\ndef _fit_transform_one(\n    transformer, X, y, weight, message_clsname=\"\", message=None, params=None\n):\n    \"\"\"\n    Fits ``transformer`` to ``X`` and ``y``. The transformed result is returned\n    with the fitted transformer. If ``weight`` is not ``None``, the result will\n    be multiplied by ``weight``.\n\n    ``params`` needs to be of the form ``process_routing()[\"step_name\"]``.\n    \"\"\"\n    params = params or {}\n    with _print_elapsed_time(message_clsname, message)"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "mposition):\n    \"\"\"\n    A sequence of data transformers with an optional final predictor.\n\n    `Pipeline` allows you to sequentially apply a list of transformers to\n    preprocess the data and, if desired, conclude the sequence with a final\n    :term:`predictor` for predictive modeling.\n\n    Intermediate steps of the pipeline must be transformers, that is, they\n    must implement `fit` and `transform` methods.\n    The final :term:`estimator` only needs to implement `fit`.\n    The transformers in the pipeline can be cached using ``memory`` argument.\n\n    The purpose of the pipeline is to assemble several steps that can be\n    cross-validated together while setting different parameters. For this, it\n    enables setting parameters of the various steps using their names and the\n    parameter name separated by a `'__'`, as in the example below. A step's\n    estimator may be replaced entirely by setting the parameter with its name\n    to another estimator, or a transformer removed by setting it to\n    `'passthrough'` or `None`.\n\n    For an example use case of `Pipeline` combined with\n    :class:`~sklearn.model_selection.GridSearchCV`, refer to\n    :ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`. The\n    example :ref:`sphx_glr_auto_examples_compose_plot_digits_pipe.py` shows how\n    to grid search on a pipeline using `'__'` as a separator in the parameter names.\n\n    Read more in the :ref:`User Guide <pipeline>`.\n\n    .. versionadded:: 0.5\n\n    Parameters\n    ----------\n    steps : list of tuples\n        List of (name of step, estimator) tuples that are to be chained in\n        sequential order. To be compatible with the scikit-learn API, all steps\n        must define `fit`. All non-last steps must also define `transform`. See\n        :ref:`Combining Estimators <combining_estimators>` for more details.\n\n    transform_input : list of str, default=None\n        The names of the :term:`metadata` parameters that should be transformed by the\n        pipeline befo"}, {"start_line": 18000, "end_line": 20000, "belongs_to": {"file_name": "plot_metadata_routing.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/miscellaneous", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_fit_request(sample_weight=\"aliased_sample_weight\")\n)\nprint_routing(meta_est)\n# %%\n# The meta-estimator cannot use `aliased_sample_weight`, because it expects\n# it passed as `sample_weight`. This would apply even if\n# `set_fit_request(sample_weight=True)` was set on it.\n\n# %%\n# Simple Pipeline\n# ---------------\n# A slightly more complicated use-case is a meta-estimator resembling a\n# :class:`~pipeline.Pipeline`. Here is a meta-estimator, which accepts a\n# transformer and a classifier. When calling its `fit` method, it applies the\n# transformer's `fit` and `transform` before running the classifier on the\n# transformed data. Upon `predict`, it applies the transformer's `transform`\n# before predicting with the classifier's `predict` method on the transformed\n# new data.\n\n\nclass SimplePipeline(ClassifierMixin, BaseEstimator):\n    def __init__(self, transformer, classifier):\n        self.transformer = transformer\n        self.classifier = classifier\n\n    def get_metadata_routing(self):\n        router = (\n            MetadataRouter(owner=self.__class__.__name__)\n            # We add the routing for the transformer.\n            .add(\n                transformer=self.transformer,\n                method_mapping=MethodMapping()\n                # The metadata is routed such that it retraces how\n                # `SimplePipeline` internally calls the transformer's `fit` and\n                # `transform` methods in its own methods (`fit` and `predict`).\n                .add(caller=\"fit\", callee=\"fit\")\n                .add(caller=\"fit\", callee=\"transform\")\n                .add(caller=\"predict\", callee=\"transform\"),\n            )\n            # We add the routing for the classifier.\n            .add(\n                classifier=self.classifier,\n                method_mapping=MethodMapping()\n                .add(caller=\"fit\", callee=\"fit\")\n                .add(caller=\"predict\", callee=\"predict\"),\n            )\n        )\n        return router\n\n    def fit(self, X, y, **fit_params):\n  "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "thodMapping,\n    _raise_for_params,\n    _routing_enabled,\n    get_routing_for_object,\n    process_routing,\n)\nfrom sklearn.utils.metaestimators import _BaseComposition, available_if\nfrom sklearn.utils.parallel import Parallel, delayed\nfrom sklearn.utils.validation import check_is_fitted, check_memory\n\n__all__ = [\"FeatureUnion\", \"Pipeline\", \"make_pipeline\", \"make_union\"]\n\n\n@contextmanager\ndef _raise_or_warn_if_not_fitted(estimator):\n    \"\"\"A context manager to make sure a NotFittedError is raised, if a sub-estimator\n    raises the error.\n\n    Otherwise, we raise a warning if the pipeline is not fitted, with the deprecation.\n\n    TODO(1.8): remove this context manager and replace with check_is_fitted.\n    \"\"\"\n    try:\n        yield\n    except NotFittedError as exc:\n        raise NotFittedError(\"Pipeline is not fitted yet.\") from exc\n\n    # we only get here if the above didn't raise\n    try:\n        check_is_fitted(estimator)\n    except NotFittedError:\n        warnings.warn(\n            \"This Pipeline instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using other methods such as transform, \"\n            \"predict, etc. This will raise an error in 1.8 instead of the current \"\n            \"warning.\",\n            FutureWarning,\n        )\n\n\ndef _final_estimator_has(attr):\n    \"\"\"Check that final_estimator has `attr`.\n\n    Used together with `available_if` in `Pipeline`.\"\"\"\n\n    def check(self):\n        # raise original `AttributeError` if `attr` does not exist\n        getattr(self._final_estimator, attr)\n        return True\n\n    return check\n\n\ndef _cached_transform(\n    sub_pipeline, *, cache, param_name, param_value, transform_params\n):\n    \"\"\"Transform a parameter value using a sub-pipeline and cache the result.\n\n    Parameters\n    ----------\n    sub_pipeline : Pipeline\n        The sub-pipeline to be used for transformation.\n    cache : dict\n        The cache dictionary to store the transformed values.\n    param_name : str\n        The n"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ring :term:`fit`. Only defined if the\n        underlying first estimator in `steps` exposes such an attribute\n        when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    make_pipeline : Convenience function for simplified pipeline construction.\n\n    Examples\n    --------\n    >>> from sklearn.svm import SVC\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.pipeline import Pipeline\n    >>> X, y = make_classification(random_state=0)\n    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n    ...                                                     random_state=0)\n    >>> pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n    >>> # The pipeline can be used as any other estimator\n    >>> # and avoids leaking the test set into the train set\n    >>> pipe.fit(X_train, y_train).score(X_test, y_test)\n    0.88\n    >>> # An estimator's parameter can be set using '__' syntax\n    >>> pipe.set_params(svc__C=10).fit(X_train, y_train).score(X_test, y_test)\n    0.76\n    \"\"\"\n\n    # BaseEstimator interface\n    _parameter_constraints: dict = {\n        \"steps\": [list, Hidden(tuple)],\n        \"transform_input\": [list, None],\n        \"memory\": [None, str, HasMethods([\"cache\"])],\n        \"verbose\": [\"boolean\"],\n    }\n\n    def __init__(self, steps, *, transform_input=None, memory=None, verbose=False):\n        self.steps = steps\n        self.transform_input = transform_input\n        self.memory = memory\n        self.verbose = verbose\n\n    def set_output(self, *, transform=None):\n        \"\"\"Set the output container when `\"transform\"` and `\"fit_transform\"` are called.\n\n        Calling"}, {"start_line": 56000, "end_line": 58000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ba\")\n            .add(caller=\"decision_function\", callee=\"decision_function\")\n            .add(caller=\"predict_log_proba\", callee=\"predict_log_proba\")\n            .add(caller=\"transform\", callee=\"transform\")\n            .add(caller=\"inverse_transform\", callee=\"inverse_transform\")\n            .add(caller=\"score\", callee=\"score\")\n        )\n\n        router.add(method_mapping=method_mapping, **{final_name: final_est})\n        return router\n\n\ndef _name_estimators(estimators):\n    \"\"\"Generate names for estimators.\"\"\"\n\n    names = [\n        estimator if isinstance(estimator, str) else type(estimator).__name__.lower()\n        for estimator in estimators\n    ]\n    namecount = defaultdict(int)\n    for est, name in zip(estimators, names):\n        namecount[name] += 1\n\n    for k, v in list(namecount.items()):\n        if v == 1:\n            del namecount[k]\n\n    for i in reversed(range(len(estimators))):\n        name = names[i]\n        if name in namecount:\n            names[i] += \"-%d\" % namecount[name]\n            namecount[name] -= 1\n\n    return list(zip(names, estimators))\n\n\ndef make_pipeline(*steps, memory=None, transform_input=None, verbose=False):\n    \"\"\"Construct a :class:`Pipeline` from the given estimators.\n\n    This is a shorthand for the :class:`Pipeline` constructor; it does not\n    require, and does not permit, naming the estimators. Instead, their names\n    will be set to the lowercase of their types automatically.\n\n    Parameters\n    ----------\n    *steps : list of Estimator objects\n        List of the scikit-learn estimators that are chained together.\n\n    memory : str or object with the joblib.Memory interface, default=None\n        Used to cache the fitted transformers of the pipeline. The last step\n        will never be cached, even if it is a transformer. By default, no\n        caching is performed. If a string is given, it is the path to the\n        caching directory. Enabling caching triggers a clone of the transformers\n        before fitting. Therefore, th"}, {"start_line": 19000, "end_line": 21000, "belongs_to": {"file_name": "plot_metadata_routing.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/miscellaneous", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    router = (\n            MetadataRouter(owner=self.__class__.__name__)\n            # We add the routing for the transformer.\n            .add(\n                transformer=self.transformer,\n                method_mapping=MethodMapping()\n                # The metadata is routed such that it retraces how\n                # `SimplePipeline` internally calls the transformer's `fit` and\n                # `transform` methods in its own methods (`fit` and `predict`).\n                .add(caller=\"fit\", callee=\"fit\")\n                .add(caller=\"fit\", callee=\"transform\")\n                .add(caller=\"predict\", callee=\"transform\"),\n            )\n            # We add the routing for the classifier.\n            .add(\n                classifier=self.classifier,\n                method_mapping=MethodMapping()\n                .add(caller=\"fit\", callee=\"fit\")\n                .add(caller=\"predict\", callee=\"predict\"),\n            )\n        )\n        return router\n\n    def fit(self, X, y, **fit_params):\n        routed_params = process_routing(self, \"fit\", **fit_params)\n\n        self.transformer_ = clone(self.transformer).fit(\n            X, y, **routed_params.transformer.fit\n        )\n        X_transformed = self.transformer_.transform(\n            X, **routed_params.transformer.transform\n        )\n\n        self.classifier_ = clone(self.classifier).fit(\n            X_transformed, y, **routed_params.classifier.fit\n        )\n        return self\n\n    def predict(self, X, **predict_params):\n        routed_params = process_routing(self, \"predict\", **predict_params)\n\n        X_transformed = self.transformer_.transform(\n            X, **routed_params.transformer.transform\n        )\n        return self.classifier_.predict(\n            X_transformed, **routed_params.classifier.predict\n        )\n\n\n# %%\n# Note the usage of :class:`~utils.metadata_routing.MethodMapping` to\n# declare which methods of the child estimator (callee) are used in which\n# methods of the meta estimator (caller). As you can "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Utilities to build a composite estimator as a chain of transforms and estimators.\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport warnings\nfrom collections import Counter, defaultdict\nfrom contextlib import contextmanager\nfrom copy import deepcopy\nfrom itertools import chain, islice\n\nimport numpy as np\nfrom scipy import sparse\n\nfrom sklearn.base import TransformerMixin, _fit_context, clone\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.utils import Bunch\nfrom sklearn.utils._metadata_requests import METHODS\nfrom sklearn.utils._param_validation import HasMethods, Hidden\nfrom sklearn.utils._repr_html.estimator import _VisualBlock\nfrom sklearn.utils._set_output import _get_container_adapter, _safe_set_output\nfrom sklearn.utils._tags import get_tags\nfrom sklearn.utils._user_interface import _print_elapsed_time\nfrom sklearn.utils.metadata_routing import (\n    MetadataRouter,\n    MethodMapping,\n    _raise_for_params,\n    _routing_enabled,\n    get_routing_for_object,\n    process_routing,\n)\nfrom sklearn.utils.metaestimators import _BaseComposition, available_if\nfrom sklearn.utils.parallel import Parallel, delayed\nfrom sklearn.utils.validation import check_is_fitted, check_memory\n\n__all__ = [\"FeatureUnion\", \"Pipeline\", \"make_pipeline\", \"make_union\"]\n\n\n@contextmanager\ndef _raise_or_warn_if_not_fitted(estimator):\n    \"\"\"A context manager to make sure a NotFittedError is raised, if a sub-estimator\n    raises the error.\n\n    Otherwise, we raise a warning if the pipeline is not fitted, with the deprecation.\n\n    TODO(1.8): remove this context manager and replace with check_is_fitted.\n    \"\"\"\n    try:\n        yield\n    except NotFittedError as exc:\n        raise NotFittedError(\"Pipeline is not fitted yet.\") from exc\n\n    # we only get here if the above didn't raise\n    try:\n        check_is_fitted(estimator)\n    except NotFittedError:\n        warnings.warn(\n            \"T"}], "retrieved_count": 10, "cost_time": 0.3477451801300049}
{"question": "How could Scikit-learn's cross-validation framework be extended to support nested cross-validation patterns?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn's cross-validation framework could be extended to support nested cross-validation patterns through several architectural improvements and design patterns:\n\n1. **Nested Cross-Validation Classes**:\n   - **NestedCrossValidator**: Base class for nested cross-validation\n   - **NestedKFold**: Nested k-fold cross-validation implementation\n   - **NestedStratifiedKFold**: Nested stratified k-fold cross-validation\n   - **NestedTimeSeriesSplit**: Nested time series cross-validation\n   - **NestedGroupKFold**: Nested group-based cross-validation\n\n2. **Nested Cross-Validation Architecture**:\n   - **Inner Loop**: Hyperparameter optimization and model selection\n   - **Outer Loop**: Performance estimation and model evaluation\n   - **Data Splitting**: Proper data splitting to avoid information leakage\n   - **Result Aggregation**: Statistical aggregation of nested CV results\n   - **Performance Metrics**: Multiple performance metrics for nested CV\n\n3. **Enhanced Cross-Validation Functions**:\n   - **nested_cross_validate**: Main function for nested cross-validation\n   - **nested_cross_val_score**: Simplified nested cross-validation scoring\n   - **nested_cross_val_predict**: Nested cross-validation predictions\n   - **nested_learning_curve**: Nested learning curves\n   - **nested_validation_curve**: Nested validation curves\n\n4. **Nested Grid Search Integration**:\n   - **NestedGridSearchCV**: Grid search with nested cross-validation\n   - **NestedRandomizedSearchCV**: Randomized search with nested CV\n   - **NestedHalvingGridSearchCV**: Successive halving with nested CV\n   - **NestedHalvingRandomSearchCV**: Random search with successive halving and nested CV\n   - **NestedBayesianSearchCV**: Bayesian optimization with nested CV\n\n5. **Advanced Nested CV Patterns**:\n   - **Multi-Level Nesting**: Support for multiple levels of nesting\n   - **Conditional Nesting**: Conditional nesting based on data characteristics\n   - **Adaptive Nesting**: Adaptive nesting based on model complexity\n   - **Ensemble Nesting**: Nested CV for ensemble methods\n   - **Pipeline Nesting**: Nested CV for complex pipelines\n\n6. **Performance and Memory Optimization**:\n   - **Caching Strategy**: Intelligent caching of nested CV results\n   - **Parallel Processing**: Parallel execution of nested CV folds\n   - **Memory Management**: Efficient memory usage for large nested CV\n   - **Early Stopping**: Early stopping for nested CV when appropriate\n   - **Resource Pooling**: Pool resources across nested CV iterations\n\n7. **Result Analysis and Visualization**:\n   - **Nested CV Results**: Comprehensive results from nested CV\n   - **Statistical Analysis**: Statistical analysis of nested CV results\n   - **Visualization Tools**: Tools for visualizing nested CV results\n   - **Performance Comparison**: Comparison of nested vs non-nested CV\n   - **Bias Analysis**: Analysis of bias in nested CV results\n\n8. **Integration with Existing Systems**:\n   - **Backward Compatibility**: Maintain compatibility with existing CV\n   - **Gradual Migration**: Gradual migration path for existing CV\n   - **API Consistency**: Consistent API across nested and non-nested CV\n   - **Documentation**: Clear documentation for nested CV usage\n   - **Examples**: Comprehensive examples of nested CV usage\n\n9. **Advanced Features**:\n   - **Custom Nesting Strategies**: Support for custom nesting strategies\n   - **Multi-Objective Nesting**: Multi-objective optimization in nested CV\n   - **Bayesian Nesting**: Bayesian optimization in nested CV\n   - **Active Learning Nesting**: Active learning in nested CV\n   - **Transfer Learning Nesting**: Transfer learning in nested CV\n\n10. **Implementation Strategy**:\n    - **Phased Rollout**: Phased rollout of nested CV features\n    - **Feature Flags**: Feature flags to enable/disable nested CV features\n    - **Testing Framework**: Comprehensive testing framework for nested CV\n    - **Documentation**: Extensive documentation and examples\n    - **Community Feedback**: Gather feedback from the community during development", "score": null, "retrieved_content": [{"start_line": 68000, "end_line": 70000, "belongs_to": {"file_name": "test_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "lits=3, random_state=0),\n    ]\n\n    for inner_cv, outer_cv in combinations_with_replacement(cvs, 2):\n        gs = GridSearchCV(\n            DummyClassifier(),\n            param_grid={\"strategy\": [\"stratified\", \"most_frequent\"]},\n            cv=inner_cv,\n            error_score=\"raise\",\n        )\n        cross_val_score(\n            gs, X=X, y=y, groups=groups, cv=outer_cv, params={\"groups\": groups}\n        )\n\n\ndef test_build_repr():\n    class MockSplitter:\n        def __init__(self, a, b=0, c=None):\n            self.a = a\n            self.b = b\n            self.c = c\n\n        def __repr__(self):\n            return _build_repr(self)\n\n    assert repr(MockSplitter(5, 6)) == \"MockSplitter(a=5, b=6, c=None)\"\n\n\n@pytest.mark.parametrize(\n    \"CVSplitter\", (ShuffleSplit, GroupShuffleSplit, StratifiedShuffleSplit)\n)\ndef test_shuffle_split_empty_trainset(CVSplitter):\n    cv = CVSplitter(test_size=0.99)\n    X, y = [[1]], [0]  # 1 sample\n    with pytest.raises(\n        ValueError,\n        match=(\n            \"With n_samples=1, test_size=0.99 and train_size=None, \"\n            \"the resulting train set will be empty\"\n        ),\n    ):\n        next(_split(cv, X, y, groups=[1]))\n\n\ndef test_train_test_split_empty_trainset():\n    (X,) = [[1]]  # 1 sample\n    with pytest.raises(\n        ValueError,\n        match=(\n            \"With n_samples=1, test_size=0.99 and train_size=None, \"\n            \"the resulting train set will be empty\"\n        ),\n    ):\n        train_test_split(X, test_size=0.99)\n\n    X = [[1], [1], [1]]  # 3 samples, ask for more than 2 thirds\n    with pytest.raises(\n        ValueError,\n        match=(\n            \"With n_samples=3, test_size=0.67 and train_size=None, \"\n            \"the resulting train set will be empty\"\n        ),\n    ):\n        train_test_split(X, test_size=0.67)\n\n\ndef test_leave_one_out_empty_trainset():\n    # LeaveOneGroup out expect at least 2 groups so no need to check\n    cv = LeaveOneOut()\n    X, y = [[1]], [0]  # 1 sample\n    with pytest.raises"}, {"start_line": 67000, "end_line": 69000, "belongs_to": {"file_name": "test_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "additional test_size\n    splits = TimeSeriesSplit(n_splits=2, gap=2, test_size=3).split(X)\n\n    train, test = next(splits)\n    assert_array_equal(train, [0, 1])\n    assert_array_equal(test, [4, 5, 6])\n\n    train, test = next(splits)\n    assert_array_equal(train, [0, 1, 2, 3, 4])\n    assert_array_equal(test, [7, 8, 9])\n\n    # Verify proper error is thrown\n    with pytest.raises(ValueError, match=\"Too many splits.*and gap\"):\n        splits = TimeSeriesSplit(n_splits=4, gap=2).split(X)\n        next(splits)\n\n\n@ignore_warnings\ndef test_nested_cv():\n    # Test if nested cross validation works with different combinations of cv\n    rng = np.random.RandomState(0)\n\n    X, y = make_classification(n_samples=15, n_classes=2, random_state=0)\n    groups = rng.randint(0, 5, 15)\n\n    cvs = [\n        LeaveOneGroupOut(),\n        StratifiedKFold(n_splits=2),\n        LeaveOneOut(),\n        GroupKFold(n_splits=3),\n        StratifiedKFold(),\n        StratifiedGroupKFold(),\n        StratifiedShuffleSplit(n_splits=3, random_state=0),\n    ]\n\n    for inner_cv, outer_cv in combinations_with_replacement(cvs, 2):\n        gs = GridSearchCV(\n            DummyClassifier(),\n            param_grid={\"strategy\": [\"stratified\", \"most_frequent\"]},\n            cv=inner_cv,\n            error_score=\"raise\",\n        )\n        cross_val_score(\n            gs, X=X, y=y, groups=groups, cv=outer_cv, params={\"groups\": groups}\n        )\n\n\ndef test_build_repr():\n    class MockSplitter:\n        def __init__(self, a, b=0, c=None):\n            self.a = a\n            self.b = b\n            self.c = c\n\n        def __repr__(self):\n            return _build_repr(self)\n\n    assert repr(MockSplitter(5, 6)) == \"MockSplitter(a=5, b=6, c=None)\"\n\n\n@pytest.mark.parametrize(\n    \"CVSplitter\", (ShuffleSplit, GroupShuffleSplit, StratifiedShuffleSplit)\n)\ndef test_shuffle_split_empty_trainset(CVSplitter):\n    cv = CVSplitter(test_size=0.99)\n    X, y = [[1]], [0]  # 1 sample\n    with pytest.raises(\n        ValueError,\n        match=(\n"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "plot_release_highlights_1_4_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e top right corner of the diagram.\n#\n# In addition, the display changes color, from orange to blue, when the estimator is\n# fitted. You can also get this information by hovering on the icon \"i\".\nfrom sklearn.base import clone\n\nclone(forest)  # the clone is not fitted\n\n# %%\n# Metadata Routing Support\n# ------------------------\n# Many meta-estimators and cross-validation routines now support metadata\n# routing, which are listed in the :ref:`user guide\n# <metadata_routing_models>`. For instance, this is how you can do a nested\n# cross-validation with sample weights and :class:`~model_selection.GroupKFold`:\nimport sklearn\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import get_scorer\nfrom sklearn.model_selection import GridSearchCV, GroupKFold, cross_validate\n\n# For now by default metadata routing is disabled, and need to be explicitly\n# enabled.\nsklearn.set_config(enable_metadata_routing=True)\n\nn_samples = 100\nX, y = make_regression(n_samples=n_samples, n_features=5, noise=0.5)\nrng = np.random.RandomState(7)\ngroups = rng.randint(0, 10, size=n_samples)\nsample_weights = rng.rand(n_samples)\nestimator = Lasso().set_fit_request(sample_weight=True)\nhyperparameter_grid = {\"alpha\": [0.1, 0.5, 1.0, 2.0]}\nscoring_inner_cv = get_scorer(\"neg_mean_squared_error\").set_score_request(\n    sample_weight=True\n)\ninner_cv = GroupKFold(n_splits=5)\n\ngrid_search = GridSearchCV(\n    estimator=estimator,\n    param_grid=hyperparameter_grid,\n    cv=inner_cv,\n    scoring=scoring_inner_cv,\n)\n\nouter_cv = GroupKFold(n_splits=5)\nscorers = {\n    \"mse\": get_scorer(\"neg_mean_squared_error\").set_score_request(sample_weight=True)\n}\nresults = cross_validate(\n    grid_search,\n    X,\n    y,\n    cv=outer_cv,\n    scoring=scorers,\n    return_estimator=True,\n    params={\"sample_weight\": sample_weights, \"groups\": groups},\n)\nprint(\"cv error on test sets:\", results[\"test_mse\"])\n\n# Setting the flag to the default `False` to avoid interference with other\n# s"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ore(clf, X_3d, y2, error_score=\"raise\")\n\n\ndef test_cross_validate_many_jobs():\n    # regression test for #12154: cv='warn' with n_jobs>1 trigger a copy of\n    # the parameters leading to a failure in check_cv due to cv is 'warn'\n    # instead of cv == 'warn'.\n    X, y = load_iris(return_X_y=True)\n    clf = SVC(gamma=\"auto\")\n    grid = GridSearchCV(clf, param_grid={\"C\": [1, 10]})\n    cross_validate(grid, X, y, n_jobs=2)\n\n\ndef test_cross_validate_invalid_scoring_param():\n    X, y = make_classification(random_state=0)\n    estimator = MockClassifier()\n\n    # Test the errors\n    error_message_regexp = \".*must be unique strings.*\"\n\n    # List/tuple of callables should raise a message advising users to use\n    # dict of names to callables mapping\n    with pytest.raises(ValueError, match=error_message_regexp):\n        cross_validate(\n            estimator,\n            X,\n            y,\n            scoring=(make_scorer(precision_score), make_scorer(accuracy_score)),\n        )\n    with pytest.raises(ValueError, match=error_message_regexp):\n        cross_validate(estimator, X, y, scoring=(make_scorer(precision_score),))\n\n    # So should empty lists/tuples\n    with pytest.raises(ValueError, match=error_message_regexp + \"Empty list.*\"):\n        cross_validate(estimator, X, y, scoring=())\n\n    # So should duplicated entries\n    with pytest.raises(ValueError, match=error_message_regexp + \"Duplicate.*\"):\n        cross_validate(estimator, X, y, scoring=(\"f1_micro\", \"f1_micro\"))\n\n    # Nested Lists should raise a generic error message\n    with pytest.raises(ValueError, match=error_message_regexp):\n        cross_validate(estimator, X, y, scoring=[[make_scorer(precision_score)]])\n\n    # Empty dict should raise invalid scoring error\n    with pytest.raises(ValueError, match=\"An empty dict\"):\n        cross_validate(estimator, X, y, scoring=(dict()))\n\n    multiclass_scorer = make_scorer(precision_recall_fscore_support)\n\n    # Multiclass Scorers that return multiple values are not supported"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ises(ValueError, match=error_message_regexp):\n        cross_validate(estimator, X, y, scoring=(make_scorer(precision_score),))\n\n    # So should empty lists/tuples\n    with pytest.raises(ValueError, match=error_message_regexp + \"Empty list.*\"):\n        cross_validate(estimator, X, y, scoring=())\n\n    # So should duplicated entries\n    with pytest.raises(ValueError, match=error_message_regexp + \"Duplicate.*\"):\n        cross_validate(estimator, X, y, scoring=(\"f1_micro\", \"f1_micro\"))\n\n    # Nested Lists should raise a generic error message\n    with pytest.raises(ValueError, match=error_message_regexp):\n        cross_validate(estimator, X, y, scoring=[[make_scorer(precision_score)]])\n\n    # Empty dict should raise invalid scoring error\n    with pytest.raises(ValueError, match=\"An empty dict\"):\n        cross_validate(estimator, X, y, scoring=(dict()))\n\n    multiclass_scorer = make_scorer(precision_recall_fscore_support)\n\n    # Multiclass Scorers that return multiple values are not supported yet\n    # the warning message we're expecting to see\n    warning_message = (\n        \"Scoring failed. The score on this train-test \"\n        f\"partition for these parameters will be set to {np.nan}. \"\n        \"Details: \\n\"\n    )\n\n    with pytest.warns(UserWarning, match=warning_message):\n        cross_validate(estimator, X, y, scoring=multiclass_scorer)\n\n    with pytest.warns(UserWarning, match=warning_message):\n        cross_validate(estimator, X, y, scoring={\"foo\": multiclass_scorer})\n\n\ndef test_cross_validate_nested_estimator():\n    # Non-regression test to ensure that nested\n    # estimators are properly returned in a list\n    # https://github.com/scikit-learn/scikit-learn/pull/17745\n    (X, y) = load_iris(return_X_y=True)\n    pipeline = Pipeline(\n        [\n            (\"imputer\", SimpleImputer()),\n            (\"classifier\", MockClassifier()),\n        ]\n    )\n\n    results = cross_validate(pipeline, X, y, return_estimator=True)\n    estimators = results[\"estimator\"]\n\n    assert isin"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " yet\n    # the warning message we're expecting to see\n    warning_message = (\n        \"Scoring failed. The score on this train-test \"\n        f\"partition for these parameters will be set to {np.nan}. \"\n        \"Details: \\n\"\n    )\n\n    with pytest.warns(UserWarning, match=warning_message):\n        cross_validate(estimator, X, y, scoring=multiclass_scorer)\n\n    with pytest.warns(UserWarning, match=warning_message):\n        cross_validate(estimator, X, y, scoring={\"foo\": multiclass_scorer})\n\n\ndef test_cross_validate_nested_estimator():\n    # Non-regression test to ensure that nested\n    # estimators are properly returned in a list\n    # https://github.com/scikit-learn/scikit-learn/pull/17745\n    (X, y) = load_iris(return_X_y=True)\n    pipeline = Pipeline(\n        [\n            (\"imputer\", SimpleImputer()),\n            (\"classifier\", MockClassifier()),\n        ]\n    )\n\n    results = cross_validate(pipeline, X, y, return_estimator=True)\n    estimators = results[\"estimator\"]\n\n    assert isinstance(estimators, list)\n    assert all(isinstance(estimator, Pipeline) for estimator in estimators)\n\n\n@pytest.mark.parametrize(\"use_sparse\", [False, True])\n@pytest.mark.parametrize(\"csr_container\", CSR_CONTAINERS)\ndef test_cross_validate(use_sparse: bool, csr_container):\n    # Compute train and test mse/r2 scores\n    cv = KFold()\n\n    # Regression\n    X_reg, y_reg = make_regression(n_samples=30, random_state=0)\n    reg = Ridge(random_state=0)\n\n    # Classification\n    X_clf, y_clf = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n\n    if use_sparse:\n        X_reg = csr_container(X_reg)\n        X_clf = csr_container(X_clf)\n\n    for X, y, est in ((X_reg, y_reg, reg), (X_clf, y_clf, clf)):\n        # It's okay to evaluate regression metrics on classification too\n        mse_scorer = check_scoring(est, scoring=\"neg_mean_squared_error\")\n        r2_scorer = check_scoring(est, scoring=\"r2\")\n        train_mse_scores = []\n        test_mse_scores "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Test the validation module\"\"\"\n\nimport os\nimport re\nimport sys\nimport tempfile\nimport warnings\nfrom functools import partial\nfrom io import StringIO\nfrom time import sleep\n\nimport numpy as np\nimport pytest\nfrom scipy.sparse import issparse\n\nfrom sklearn import config_context\nfrom sklearn.base import BaseEstimator, ClassifierMixin, clone\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import (\n    load_diabetes,\n    load_digits,\n    load_iris,\n    make_classification,\n    make_multilabel_classification,\n    make_regression,\n)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.exceptions import FitFailedWarning, UnsetMetadataPassedError\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import (\n    LogisticRegression,\n    PassiveAggressiveClassifier,\n    Ridge,\n    RidgeClassifier,\n    SGDClassifier,\n)\nfrom sklearn.metrics import (\n    accuracy_score,\n    check_scoring,\n    confusion_matrix,\n    explained_variance_score,\n    make_scorer,\n    mean_squared_error,\n    precision_recall_fscore_support,\n    precision_score,\n    r2_score,\n)\nfrom sklearn.metrics._scorer import _MultimetricScorer\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    GroupKFold,\n    GroupShuffleSplit,\n    KFold,\n    LeaveOneGroupOut,\n    LeaveOneOut,\n    LeavePGroupsOut,\n    ShuffleSplit,\n    StratifiedKFold,\n    cross_val_predict,\n    cross_val_score,\n    cross_validate,\n    learning_curve,\n    permutation_test_score,\n    validation_curve,\n)\nfrom sklearn.model_selection._validation import (\n    _check_is_permutation,\n    _fit_and_score,\n    _score,\n)\nfrom sklearn.model_selection.tests.common import OneTimeSplitter\nfrom sklearn.model_selection.tests.test_search import FailingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, scale\nfrom sklearn.svm import "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_squared_error,\n    precision_recall_fscore_support,\n    precision_score,\n    r2_score,\n)\nfrom sklearn.metrics._scorer import _MultimetricScorer\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    GroupKFold,\n    GroupShuffleSplit,\n    KFold,\n    LeaveOneGroupOut,\n    LeaveOneOut,\n    LeavePGroupsOut,\n    ShuffleSplit,\n    StratifiedKFold,\n    cross_val_predict,\n    cross_val_score,\n    cross_validate,\n    learning_curve,\n    permutation_test_score,\n    validation_curve,\n)\nfrom sklearn.model_selection._validation import (\n    _check_is_permutation,\n    _fit_and_score,\n    _score,\n)\nfrom sklearn.model_selection.tests.common import OneTimeSplitter\nfrom sklearn.model_selection.tests.test_search import FailingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, scale\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tests.metadata_routing_common import (\n    ConsumingClassifier,\n    ConsumingScorer,\n    ConsumingSplitter,\n    _Registry,\n    check_recorded_metadata,\n)\nfrom sklearn.utils import shuffle\nfrom sklearn.utils._mocking import CheckingClassifier, MockDataFrame\nfrom sklearn.utils._testing import (\n    assert_allclose,\n    assert_almost_equal,\n    assert_array_almost_equal,\n    assert_array_equal,\n)\nfrom sklearn.utils.fixes import COO_CONTAINERS, CSR_CONTAINERS\nfrom sklearn.utils.validation import _num_samples\n\n\nclass MockImprovingEstimator(BaseEstimator):\n    \"\"\"Dummy classifier to test the learning curve\"\"\"\n\n    def __init__(self, n_max_train_sizes):\n        self.n_max_train_sizes = n_max_train_sizes\n        self.train_sizes = 0\n        self.X_subset = None\n\n    def fit(self, X_subset, y_subset=None):\n        self.X_subset = X_subset\n        self.train_sizes = X_subset.shape[0]\n        return self\n\n    def predict(self, X):\n        raise NotImplementedError\n\n   "}, {"start_line": 57000, "end_line": 59000, "belongs_to": {"file_name": "test_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ")\n    cv = check_cv(3, y_multioutput, classifier=True)\n    np.testing.assert_equal(list(KFold(3).split(X)), list(cv.split(X)))\n\n    with pytest.raises(ValueError):\n        check_cv(cv=\"lolo\")\n\n\ndef test_cv_iterable_wrapper():\n    kf_iter = KFold().split(X, y)\n    kf_iter_wrapped = check_cv(kf_iter)\n    # Since the wrapped iterable is enlisted and stored,\n    # split can be called any number of times to produce\n    # consistent results.\n    np.testing.assert_equal(\n        list(kf_iter_wrapped.split(X, y)), list(kf_iter_wrapped.split(X, y))\n    )\n    # If the splits are randomized, successive calls to split yields different\n    # results\n    kf_randomized_iter = KFold(shuffle=True, random_state=0).split(X, y)\n    kf_randomized_iter_wrapped = check_cv(kf_randomized_iter)\n    # numpy's assert_array_equal properly compares nested lists\n    np.testing.assert_equal(\n        list(kf_randomized_iter_wrapped.split(X, y)),\n        list(kf_randomized_iter_wrapped.split(X, y)),\n    )\n\n    try:\n        splits_are_equal = True\n        np.testing.assert_equal(\n            list(kf_iter_wrapped.split(X, y)),\n            list(kf_randomized_iter_wrapped.split(X, y)),\n        )\n    except AssertionError:\n        splits_are_equal = False\n    assert not splits_are_equal, (\n        \"If the splits are randomized, \"\n        \"successive calls to split should yield different results\"\n    )\n\n\n@pytest.mark.parametrize(\"kfold\", [GroupKFold, StratifiedGroupKFold])\n@pytest.mark.parametrize(\"shuffle\", [True, False])\ndef test_group_kfold(kfold, shuffle, global_random_seed):\n    rng = np.random.RandomState(global_random_seed)\n\n    # Parameters of the test\n    n_groups = 15\n    n_samples = 1000\n    n_splits = 5\n\n    X = y = np.ones(n_samples)\n\n    # Construct the test data\n    tolerance = 0.05 * n_samples  # 5 percent error allowed\n    groups = rng.randint(0, n_groups, n_samples)\n\n    ideal_n_groups_per_fold = n_samples // n_splits\n\n    len(np.unique(groups))\n    # Get the test fold indices from the t"}, {"start_line": 56000, "end_line": 58000, "belongs_to": {"file_name": "test_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ry))\n    )\n\n    y_multiclass = np.array([0, 1, 0, 1, 2, 1, 2, 0, 2])\n    cv = check_cv(3, y_multiclass, classifier=True)\n    np.testing.assert_equal(\n        list(StratifiedKFold(3).split(X, y_multiclass)), list(cv.split(X, y_multiclass))\n    )\n    # also works with 2d multiclass\n    y_multiclass_2d = y_multiclass.reshape(-1, 1)\n    cv = check_cv(3, y_multiclass_2d, classifier=True)\n    np.testing.assert_equal(\n        list(StratifiedKFold(3).split(X, y_multiclass_2d)),\n        list(cv.split(X, y_multiclass_2d)),\n    )\n\n    assert not np.all(\n        next(StratifiedKFold(3).split(X, y_multiclass_2d))[0]\n        == next(KFold(3).split(X, y_multiclass_2d))[0]\n    )\n\n    X = np.ones(5)\n    y_multilabel = np.array(\n        [[0, 0, 0, 0], [0, 1, 1, 0], [0, 0, 0, 1], [1, 1, 0, 1], [0, 0, 1, 0]]\n    )\n    cv = check_cv(3, y_multilabel, classifier=True)\n    np.testing.assert_equal(list(KFold(3).split(X)), list(cv.split(X)))\n\n    y_multioutput = np.array([[1, 2], [0, 3], [0, 0], [3, 1], [2, 0]])\n    cv = check_cv(3, y_multioutput, classifier=True)\n    np.testing.assert_equal(list(KFold(3).split(X)), list(cv.split(X)))\n\n    with pytest.raises(ValueError):\n        check_cv(cv=\"lolo\")\n\n\ndef test_cv_iterable_wrapper():\n    kf_iter = KFold().split(X, y)\n    kf_iter_wrapped = check_cv(kf_iter)\n    # Since the wrapped iterable is enlisted and stored,\n    # split can be called any number of times to produce\n    # consistent results.\n    np.testing.assert_equal(\n        list(kf_iter_wrapped.split(X, y)), list(kf_iter_wrapped.split(X, y))\n    )\n    # If the splits are randomized, successive calls to split yields different\n    # results\n    kf_randomized_iter = KFold(shuffle=True, random_state=0).split(X, y)\n    kf_randomized_iter_wrapped = check_cv(kf_randomized_iter)\n    # numpy's assert_array_equal properly compares nested lists\n    np.testing.assert_equal(\n        list(kf_randomized_iter_wrapped.split(X, y)),\n        list(kf_randomized_iter_wrapped.split(X, y)),\n    )\n\n    try:\n   "}], "retrieved_count": 10, "cost_time": 0.33483290672302246}
{"question": "How does Scikit-learn implement its estimator interface?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn implements its estimator interface through a well-defined hierarchy of base classes, mixins, and conventions that ensure consistency across all estimators. Here's how the estimator interface is implemented:\n\n1. **BaseEstimator Class (base.py)**:\n   - **Core Foundation**: All estimators inherit from BaseEstimator\n   - **Parameter Management**: Implements get_params() and set_params() methods\n   - **Serialization**: Provides pickle-based serialization support\n   - **Parameter Validation**: Built-in parameter validation framework\n   - **Data Validation**: Integration with validation utilities\n   - **Feature Names**: Support for feature name validation and preservation\n\n2. **Mixin Classes for Specialization**:\n   - **ClassifierMixin**: Provides score method using accuracy_score for classifiers\n   - **RegressorMixin**: Provides score method using r2_score for regressors\n   - **TransformerMixin**: Provides fit_transform method for transformers\n   - **ClusterMixin**: Provides fit_predict method for clustering algorithms\n   - **DensityMixin**: Provides score method for density estimators\n   - **OutlierMixin**: Provides fit_predict method for outlier detection\n\n3. **Core Interface Methods**:\n   - **fit(X, y=None)**: Required method for all estimators to learn from data\n   - **predict(X)**: Method for making predictions (classifiers, regressors)\n   - **transform(X)**: Method for data transformation (transformers)\n   - **score(X, y)**: Method for evaluating model performance\n   - **get_params()**: Method for retrieving estimator parameters\n   - **set_params()**: Method for setting estimator parameters\n\n4. **Estimator Tags System**:\n   - **Tags Class**: Comprehensive tagging system for estimator capabilities\n   - **Runtime Tags**: Tags that can be determined at runtime based on parameters\n   - **Capability Detection**: Programmatic detection of estimator capabilities\n   - **Input Tags**: Tags describing supported input types (sparse, multi-output, etc.)\n   - **Target Tags**: Tags describing supported target types (binary, multiclass, etc.)\n\n5. **Validation and Error Handling**:\n   - **Input Validation**: Automatic validation of input data and parameters\n   - **Type Checking**: Runtime type checking with detailed error messages\n   - **Shape Validation**: Validation of data shapes and dimensions\n   - **Feature Count Validation**: Validation of feature count consistency\n   - **Error Messages**: Clear and informative error messages\n\n6. **State Management**:\n   - **Fitted State**: Tracking whether estimator has been fitted\n   - **Learned Attributes**: Attributes ending with underscore (_) for learned parameters\n   - **Private Attributes**: Attributes starting with underscore (_) for internal state\n   - **State Validation**: Validation of estimator state before operations\n   - **State Reset**: Proper state management during refitting\n\n7. **Parameter System**:\n   - **Parameter Constraints**: Validation of parameter types and values\n   - **Parameter Documentation**: Automatic documentation of parameters\n   - **Parameter Inheritance**: Proper parameter inheritance in meta-estimators\n   - **Parameter Cloning**: Safe parameter cloning for meta-estimators\n   - **Parameter Validation**: Runtime validation of parameter values\n\n8. **Compatibility and Testing**:\n   - **check_estimator()**: Comprehensive testing function for estimator compliance\n   - **parametrize_with_checks()**: Pytest decorator for testing multiple estimators\n   - **Estimator Checks**: Extensive test suite for estimator compliance\n   - **API Compliance**: Ensuring adherence to scikit-learn API conventions\n   - **Backward Compatibility**: Maintaining compatibility across versions\n\n9. **Advanced Features**:\n   - **Metadata Routing**: Advanced metadata handling and routing\n   - **Feature Names**: Support for feature name preservation and validation\n   - **Array API Support**: Support for Array API compatible inputs\n   - **HTML Representation**: Rich HTML representation for estimators\n   - **Documentation Integration**: Automatic documentation generation\n\n10. **Implementation Guidelines**:\n    - **Method Signatures**: Consistent method signatures across all estimators\n    - **Return Values**: Consistent return value types and formats\n    - **Error Handling**: Consistent error handling patterns\n    - **Documentation**: Comprehensive documentation requirements\n    - **Testing**: Extensive testing requirements for all estimators", "score": null, "retrieved_content": [{"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])\n    \"\"\"\n\n    _html_repr = estimator_html_repr\n\n    @classmethod\n    def _get_param_names(cls):\n        \"\"\"Get parameter names for the estimator\"\"\"\n        # fetch the constructor or the original constructor before\n        # deprecation wrapping if any\n        init = getattr(cls.__init__, \"deprecated_original\", cls.__init__)\n        if init is object.__init__:\n            # No explicit constructor to introspect\n            return []\n\n        # introspect the constructor arguments to find the model parameters\n        # to represent\n        init_signature = inspect.signature(init)\n        # Consider the constructor parameters excluding 'self'\n        parameters = [\n            p\n            for p in init_signature.parameters.values()\n            if p.name != \"self\" and p.kind != p.VAR_KEYWORD\n        ]\n        for p in parameters:\n            if p.kind == p.VAR_POSITIONAL:\n                raise RuntimeError(\n                    \"scikit-learn estimators should always \"\n                    \"specify their para"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "aram2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n\n    # _sklearn_output_config is used by `set_output` to configure the output\n    # container of an estimator.\n    if hasattr(estimator, \"_sklearn_output_config\"):\n        new_object._sklearn_output_config = copy.deepcopy(\n            estimator._sklearn_output_config\n        )\n    return new_object\n\n\nclass BaseEstimator(ReprHTMLMixin, _HTMLDocumentationLinkMixin, _MetadataRequester):\n    \"\"\"Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])\n    \"\"\"\n\n    _html_"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "sklearn_is_fitted.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/developing_estimators", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"\n========================================\n`__sklearn_is_fitted__` as Developer API\n========================================\n\nThe `__sklearn_is_fitted__` method is a convention used in scikit-learn for\nchecking whether an estimator object has been fitted or not. This method is\ntypically implemented in custom estimator classes that are built on top of\nscikit-learn's base classes like `BaseEstimator` or its subclasses.\n\nDevelopers should use :func:`~sklearn.utils.validation.check_is_fitted`\nat the beginning of all methods except `fit`. If they need to customize or\nspeed-up the check, they can implement the `__sklearn_is_fitted__` method as\nshown below.\n\nIn this example the custom estimator showcases the usage of the\n`__sklearn_is_fitted__` method and the `check_is_fitted` utility function\nas developer APIs. The `__sklearn_is_fitted__` method checks fitted status\nby verifying the presence of the `_is_fitted` attribute.\n\"\"\"\n\n# %%\n# An example custom estimator implementing a simple classifier\n# ------------------------------------------------------------\n# This code snippet defines a custom estimator class called `CustomEstimator`\n# that extends both the `BaseEstimator` and `ClassifierMixin` classes from\n# scikit-learn and showcases the usage of the `__sklearn_is_fitted__` method\n# and the `check_is_fitted` utility function.\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_is_fitted\n\n\nclass CustomEstimator(BaseEstimator, ClassifierMixin):\n    def __init__(self, parameter=1):\n        self.parameter = parameter\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the estimator to the training data.\n        \"\"\"\n        self.classes_ = sorted(set(y))\n        # Custom attribute to track if the estimator is fitted\n        self._is_fitted = True\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Perform Predictions\n\n        If the est"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n estimator instead of a class.\"\n                )\n            else:\n                raise TypeError(\n                    \"Cannot clone object '%s' (type %s): \"\n                    \"it does not seem to be a scikit-learn \"\n                    \"estimator as it does not implement a \"\n                    \"'get_params' method.\" % (repr(estimator), type(estimator))\n                )\n\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        new_object_params[name] = clone(param, safe=False)\n\n    new_object = klass(**new_object_params)\n    try:\n        new_object._metadata_request = copy.deepcopy(estimator._metadata_request)\n    except AttributeError:\n        pass\n\n    params_set = new_object.get_params(deep=False)\n\n    # quick sanity check of the parameters of the clone\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is not param2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n\n    # _sklearn_output_config is used by `set_output` to configure the output\n    # container of an estimator.\n    if hasattr(estimator, \"_sklearn_output_config\"):\n        new_object._sklearn_output_config = copy.deepcopy(\n            estimator._sklearn_output_config\n        )\n    return new_object\n\n\nclass BaseEstimator(ReprHTMLMixin, _HTMLDocumentationLinkMixin, _MetadataRequester):\n    \"\"\"Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_pprint.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "   self.intercept_scaling = intercept_scaling\n        self.class_weight = class_weight\n        self.random_state = random_state\n        self.solver = solver\n        self.max_iter = max_iter\n        self.multi_class = multi_class\n        self.verbose = verbose\n        self.warm_start = warm_start\n        self.n_jobs = n_jobs\n        self.l1_ratio = l1_ratio\n\n    def fit(self, X, y):\n        return self\n\n\nclass StandardScaler(TransformerMixin, BaseEstimator):\n    def __init__(self, copy=True, with_mean=True, with_std=True):\n        self.with_mean = with_mean\n        self.with_std = with_std\n        self.copy = copy\n\n    def transform(self, X, copy=None):\n        return self\n\n\nclass RFE(BaseEstimator):\n    def __init__(self, estimator, n_features_to_select=None, step=1, verbose=0):\n        self.estimator = estimator\n        self.n_features_to_select = n_features_to_select\n        self.step = step\n        self.verbose = verbose\n\n\nclass GridSearchCV(BaseEstimator):\n    def __init__(\n        self,\n        estimator,\n        param_grid,\n        scoring=None,\n        n_jobs=None,\n        iid=\"warn\",\n        refit=True,\n        cv=\"warn\",\n        verbose=0,\n        pre_dispatch=\"2*n_jobs\",\n        error_score=\"raise-deprecating\",\n        return_train_score=False,\n    ):\n        self.estimator = estimator\n        self.param_grid = param_grid\n        self.scoring = scoring\n        self.n_jobs = n_jobs\n        self.iid = iid\n        self.refit = refit\n        self.cv = cv\n        self.verbose = verbose\n        self.pre_dispatch = pre_dispatch\n        self.error_score = error_score\n        self.return_train_score = return_train_score\n\n\nclass CountVectorizer(BaseEstimator):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        stop_words=None,\n        token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n        ngram_range=(1, 1),\n "}, {"start_line": 1000, "end_line": 2607, "belongs_to": {"file_name": "sklearn_is_fitted.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/developing_estimators", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "fier\n# ------------------------------------------------------------\n# This code snippet defines a custom estimator class called `CustomEstimator`\n# that extends both the `BaseEstimator` and `ClassifierMixin` classes from\n# scikit-learn and showcases the usage of the `__sklearn_is_fitted__` method\n# and the `check_is_fitted` utility function.\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_is_fitted\n\n\nclass CustomEstimator(BaseEstimator, ClassifierMixin):\n    def __init__(self, parameter=1):\n        self.parameter = parameter\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the estimator to the training data.\n        \"\"\"\n        self.classes_ = sorted(set(y))\n        # Custom attribute to track if the estimator is fitted\n        self._is_fitted = True\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Perform Predictions\n\n        If the estimator is not fitted, then raise NotFittedError\n        \"\"\"\n        check_is_fitted(self)\n        # Perform prediction logic\n        predictions = [self.classes_[0]] * len(X)\n        return predictions\n\n    def score(self, X, y):\n        \"\"\"\n        Calculate Score\n\n        If the estimator is not fitted, then raise NotFittedError\n        \"\"\"\n        check_is_fitted(self)\n        # Perform scoring logic\n        return 0.5\n\n    def __sklearn_is_fitted__(self):\n        \"\"\"\n        Check fitted status and return a Boolean value.\n        \"\"\"\n        return hasattr(self, \"_is_fitted\") and self._is_fitted\n"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "_mocking.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    \"\"\"\n        if self.methods_to_check == \"all\" or \"score\" in self.methods_to_check:\n            self._check_X_y(X, Y)\n        if self.foo_param > 1:\n            score = 1.0\n        else:\n            score = 0.0\n        return score\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags._skip_test = True\n        tags.input_tags.two_d_array = False\n        tags.target_tags.one_d_labels = True\n        return tags\n\n\n# Deactivate key validation for CheckingClassifier because we want to be able to\n# call fit with arbitrary fit_params and record them. Without this change, we\n# would get an error because those arbitrary params are not expected.\nCheckingClassifier.set_fit_request = RequestMethod(  # type: ignore[assignment,method-assign]\n    name=\"fit\", keys=[], validate_keys=False\n)\n\n\nclass NoSampleWeightWrapper(BaseEstimator):\n    \"\"\"Wrap estimator which will not expose `sample_weight`.\n\n    Parameters\n    ----------\n    est : estimator, default=None\n        The estimator to wrap.\n    \"\"\"\n\n    def __init__(self, est=None):\n        self.est = est\n\n    def fit(self, X, y):\n        return self.est.fit(X, y)\n\n    def predict(self, X):\n        return self.est.predict(X)\n\n    def predict_proba(self, X):\n        return self.est.predict_proba(X)\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags._skip_test = True\n        return tags\n\n\ndef _check_response(method):\n    def check(self):\n        return self.response_methods is not None and method in self.response_methods\n\n    return check\n\n\nclass _MockEstimatorOnOffPrediction(BaseEstimator):\n    \"\"\"Estimator for which we can turn on/off the prediction methods.\n\n    Parameters\n    ----------\n    response_methods: list of \\\n            {\"predict\", \"predict_proba\", \"decision_function\"}, default=None\n        List containing the response implemented by the estimator. When, the\n        response is in the list, it will return the name of the response method\n        when"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_pipeline.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "TransfFitParams(Transf):\n    def fit(self, X, y=None, **fit_params):\n        self.fit_params = fit_params\n        return self\n\n\nclass Mult(TransformerMixin, BaseEstimator):\n    def __init__(self, mult=1):\n        self.mult = mult\n\n    def __sklearn_is_fitted__(self):\n        return True\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return np.asarray(X) * self.mult\n\n    def inverse_transform(self, X):\n        return np.asarray(X) / self.mult\n\n    def predict(self, X):\n        return (np.asarray(X) * self.mult).sum(axis=1)\n\n    predict_proba = predict_log_proba = decision_function = predict\n\n    def score(self, X, y=None):\n        return np.sum(X)\n\n\nclass FitParamT(BaseEstimator):\n    \"\"\"Mock classifier\"\"\"\n\n    def __init__(self):\n        self.successful = False\n\n    def fit(self, X, y, should_succeed=False):\n        self.successful = should_succeed\n        self.fitted_ = True\n\n    def predict(self, X):\n        return self.successful\n\n    def fit_predict(self, X, y, should_succeed=False):\n        self.fit(X, y, should_succeed=should_succeed)\n        return self.predict(X)\n\n    def score(self, X, y=None, sample_weight=None):\n        if sample_weight is not None:\n            X = X * sample_weight\n        return np.sum(X)\n\n\nclass DummyTransf(Transf):\n    \"\"\"Transformer which store the column means\"\"\"\n\n    def fit(self, X, y):\n        self.means_ = np.mean(X, axis=0)\n        # store timestamp to figure out whether the result of 'fit' has been\n        # cached or not\n        self.timestamp_ = time.time()\n        return self\n\n\nclass DummyEstimatorParams(BaseEstimator):\n    \"\"\"Mock classifier that takes params on predict\"\"\"\n\n    def __sklearn_is_fitted__(self):\n        return True\n\n    def fit(self, X, y):\n        return self\n\n    def predict(self, X, got_attribute=False):\n        self.got_attribute = got_attribute\n        return self\n\n    def predict_proba(self, X, got_attribute=False):\n        self.got_attribute = got_attribute\n   "}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "metadata_routing_common.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "        self, sample_weight=sample_weight, metadata=metadata\n        )\n        _check_partial_fit_first_call(self, classes)\n        return self\n\n    def fit(self, X, y, sample_weight=\"default\", metadata=\"default\"):\n        if self.registry is not None:\n            self.registry.append(self)\n\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n\n        self.classes_ = np.unique(y)\n        self.coef_ = np.ones_like(X)\n        return self\n\n    def predict(self, X, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n        y_score = np.empty(shape=(len(X),), dtype=\"int8\")\n        y_score[len(X) // 2 :] = 0\n        y_score[: len(X) // 2] = 1\n        return y_score\n\n    def predict_proba(self, X, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n        y_proba = np.empty(shape=(len(X), len(self.classes_)), dtype=np.float32)\n        # each row sums up to 1.0:\n        y_proba[:] = np.random.dirichlet(alpha=np.ones(len(self.classes_)), size=len(X))\n        return y_proba\n\n    def predict_log_proba(self, X, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n        return self.predict_proba(X)\n\n    def decision_function(self, X, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n        y_score = np.empty(shape=(len(X),))\n        y_score[len(X) // 2 :] = 0\n        y_score[: len(X) // 2] = 1\n        return y_score\n\n    def score(self, X, y, sample_weight=\"default\", metadata=\"default\"):\n        record_metadata_not_default(\n            self, sample_weight=sample_weight, metadata=metadata\n        )\n"}, {"start_line": 12000, "end_line": 13699, "belongs_to": {"file_name": "_mocking.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  The estimator to wrap.\n    \"\"\"\n\n    def __init__(self, est=None):\n        self.est = est\n\n    def fit(self, X, y):\n        return self.est.fit(X, y)\n\n    def predict(self, X):\n        return self.est.predict(X)\n\n    def predict_proba(self, X):\n        return self.est.predict_proba(X)\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags._skip_test = True\n        return tags\n\n\ndef _check_response(method):\n    def check(self):\n        return self.response_methods is not None and method in self.response_methods\n\n    return check\n\n\nclass _MockEstimatorOnOffPrediction(BaseEstimator):\n    \"\"\"Estimator for which we can turn on/off the prediction methods.\n\n    Parameters\n    ----------\n    response_methods: list of \\\n            {\"predict\", \"predict_proba\", \"decision_function\"}, default=None\n        List containing the response implemented by the estimator. When, the\n        response is in the list, it will return the name of the response method\n        when called. Otherwise, an `AttributeError` is raised. It allows to\n        use `getattr` as any conventional estimator. By default, no response\n        methods are mocked.\n    \"\"\"\n\n    def __init__(self, response_methods=None):\n        self.response_methods = response_methods\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        return self\n\n    @available_if(_check_response(\"predict\"))\n    def predict(self, X):\n        return \"predict\"\n\n    @available_if(_check_response(\"predict_proba\"))\n    def predict_proba(self, X):\n        return \"predict_proba\"\n\n    @available_if(_check_response(\"decision_function\"))\n    def decision_function(self, X):\n        return \"decision_function\"\n"}], "retrieved_count": 10, "cost_time": 0.3341202735900879}
{"question": "How does Scikit-learn ensure consistency across different algorithms?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn ensures consistency across different algorithms through a comprehensive system of standards, conventions, and validation mechanisms. Here's how consistency is maintained:\n\n1. **Unified Estimator Interface**:\n   - **BaseEstimator**: All algorithms inherit from the same base class\n   - **Standard Methods**: Consistent fit(), predict(), transform(), score() methods\n   - **Parameter Management**: Uniform get_params() and set_params() across all estimators\n   - **State Management**: Consistent fitted state tracking and validation\n   - **Error Handling**: Standardized error handling and validation patterns\n\n2. **API Conventions and Standards**:\n   - **Method Signatures**: Consistent method signatures across all estimators\n   - **Parameter Naming**: Standardized parameter naming conventions\n   - **Return Values**: Consistent return value types and formats\n   - **Documentation**: Uniform documentation standards for all estimators\n   - **Examples**: Consistent example patterns and usage demonstrations\n\n3. **Data Validation System**:\n   - **Input Validation**: Standardized input validation across all algorithms\n   - **Type Checking**: Consistent type checking and conversion\n   - **Shape Validation**: Uniform shape validation for input data\n   - **Feature Count Validation**: Consistent feature count validation\n   - **Data Type Support**: Standardized support for different data types (sparse, dense)\n\n4. **Estimator Tags System**:\n   - **Capability Tags**: Programmatic detection of estimator capabilities\n   - **Input Tags**: Standardized tags for supported input types\n   - **Target Tags**: Consistent tags for supported target types\n   - **Runtime Tags**: Tags that can be determined at runtime\n   - **Compatibility Tags**: Tags for algorithm compatibility and requirements\n\n5. **Testing and Validation Framework**:\n   - **check_estimator()**: Comprehensive testing function for all estimators\n   - **parametrize_with_checks()**: Standardized testing decorator\n   - **Estimator Checks**: Extensive test suite ensuring API compliance\n   - **Regression Tests**: Tests ensuring consistent behavior across versions\n   - **Performance Tests**: Standardized performance benchmarking\n\n6. **Parameter Validation System**:\n   - **Parameter Constraints**: Consistent parameter validation across algorithms\n   - **Type Validation**: Standardized type checking for parameters\n   - **Range Validation**: Consistent range validation for numerical parameters\n   - **Constraint Validation**: Uniform constraint validation patterns\n   - **Error Messages**: Standardized error messages for parameter validation\n\n7. **Random State Management**:\n   - **Consistent Randomization**: Standardized random state handling\n   - **Reproducibility**: Consistent reproducibility across algorithms\n   - **Random State Validation**: Uniform random state validation\n   - **Cross-Validation Consistency**: Consistent randomization in cross-validation\n   - **Algorithm-Specific Randomization**: Standardized randomization for each algorithm type\n\n8. **Performance and Memory Consistency**:\n   - **Memory Management**: Consistent memory usage patterns\n   - **Performance Standards**: Standardized performance expectations\n   - **Scalability**: Consistent scalability patterns across algorithms\n   - **Resource Management**: Uniform resource management and cleanup\n   - **Optimization Standards**: Consistent optimization approaches\n\n9. **Documentation and Examples**:\n   - **API Documentation**: Consistent API documentation across all estimators\n   - **Parameter Documentation**: Standardized parameter documentation\n   - **Example Consistency**: Uniform example patterns and usage\n   - **Best Practices**: Consistent best practices documentation\n   - **Migration Guides**: Standardized migration and upgrade guides\n\n10. **Quality Assurance Processes**:\n    - **Code Review**: Consistent code review standards\n    - **Testing Requirements**: Uniform testing requirements for all algorithms\n    - **Performance Benchmarks**: Standardized performance benchmarking\n    - **Compatibility Testing**: Consistent compatibility testing\n    - **Release Standards**: Uniform release and versioning standards", "score": null, "retrieved_content": [{"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ataframe_column_names_consistency,\n    check_decision_proba_consistency,\n    check_dict_unchanged,\n    check_dont_overwrite_parameters,\n    check_estimator,\n    check_estimator_cloneable,\n    check_estimator_repr,\n    check_estimator_sparse_array,\n    check_estimator_sparse_matrix,\n    check_estimator_sparse_tag,\n    check_estimator_tags_renamed,\n    check_estimators_nan_inf,\n    check_estimators_overwrite_params,\n    check_estimators_unfitted,\n    check_fit_check_is_fitted,\n    check_fit_score_takes_y,\n    check_methods_sample_order_invariance,\n    check_methods_subset_invariance,\n    check_mixin_order,\n    check_no_attributes_set_in_init,\n    check_outlier_contamination,\n    check_outlier_corruption,\n    check_parameters_default_constructible,\n    check_positive_only_tag_during_fit,\n    check_regressor_data_not_an_array,\n    check_requires_y_none,\n    check_sample_weights_pandas_series,\n    check_set_params,\n    estimator_checks_generator,\n    set_random_state,\n)\nfrom sklearn.utils.fixes import CSR_CONTAINERS, SPARRAY_PRESENT\nfrom sklearn.utils.metaestimators import available_if\nfrom sklearn.utils.multiclass import type_of_target\nfrom sklearn.utils.validation import (\n    check_array,\n    check_is_fitted,\n    check_X_y,\n    validate_data,\n)\n\n\nclass CorrectNotFittedError(ValueError):\n    \"\"\"Exception class to raise if estimator is used before fitting.\n\n    Like NotFittedError, it inherits from ValueError, but not from\n    AttributeError. Used for testing only.\n    \"\"\"\n\n\nclass BaseBadClassifier(ClassifierMixin, BaseEstimator):\n    def fit(self, X, y):\n        return self\n\n    def predict(self, X):\n        return np.ones(X.shape[0])\n\n\nclass ChangesDict(BaseEstimator):\n    def __init__(self, key=0):\n        self.key = key\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n    def predict(self, X):\n        X = check_array(X)\n        self.key = 1000\n        return np.ones(X.shape[0])\n\n\nclass SetsWrongAttribute(BaseEstimator):\n  "}, {"start_line": 146000, "end_line": 148000, "belongs_to": {"file_name": "estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "           random_state=0,\n                n_features=2,\n                cluster_std=0.1,\n            )\n            X = _enforce_estimator_tags_X(estimator_orig, X)\n        set_random_state(estimator, 0)\n        estimator.fit(X, y_)\n\n        # These return a n_iter per component.\n        if name in CROSS_DECOMPOSITION:\n            for iter_ in estimator.n_iter_:\n                assert iter_ >= 1\n        else:\n            assert estimator.n_iter_ >= 1\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_get_params_invariance(name, estimator_orig):\n    # Checks if get_params(deep=False) is a subset of get_params(deep=True)\n    e = clone(estimator_orig)\n\n    shallow_params = e.get_params(deep=False)\n    deep_params = e.get_params(deep=True)\n\n    assert all(item in deep_params.items() for item in shallow_params.items())\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_set_params(name, estimator_orig):\n    # Check that get_params() returns the same thing\n    # before and after set_params() with some fuzz\n    estimator = clone(estimator_orig)\n\n    orig_params = estimator.get_params(deep=False)\n    msg = \"get_params result does not match what was passed to set_params\"\n\n    estimator.set_params(**orig_params)\n    curr_params = estimator.get_params(deep=False)\n    assert set(orig_params.keys()) == set(curr_params.keys()), msg\n    for k, v in curr_params.items():\n        assert orig_params[k] is v, msg\n\n    # some fuzz values\n    test_values = [-np.inf, np.inf, None]\n\n    test_params = deepcopy(orig_params)\n    for param_name in orig_params.keys():\n        default_value = orig_params[param_name]\n        for value in test_values:\n            test_params[param_name] = value\n            try:\n                estimator.set_params(**test_params)\n            except (TypeError, ValueError) as e:\n                e_type = e.__class__.__name__\n                # Exception occurred, possibly parameter validation\n                warnings.warn(\n                    \"{0} occurred during s"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport pickle\nimport re\nimport warnings\n\nimport numpy as np\nimport pytest\nimport scipy.sparse as sp\nfrom numpy.testing import assert_allclose\n\nimport sklearn\nfrom sklearn import config_context, datasets\nfrom sklearn.base import (\n    BaseEstimator,\n    OutlierMixin,\n    TransformerMixin,\n    clone,\n    is_classifier,\n    is_clusterer,\n    is_outlier_detector,\n    is_regressor,\n)\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.exceptions import InconsistentVersionWarning\nfrom sklearn.metrics import get_scorer\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.utils._mocking import MockDataFrame\nfrom sklearn.utils._set_output import _get_output_config\nfrom sklearn.utils._testing import (\n    _convert_container,\n    assert_array_equal,\n)\nfrom sklearn.utils.validation import _check_n_features, validate_data\n\n\n#############################################################################\n# A few test classes\nclass MyEstimator(BaseEstimator):\n    def __init__(self, l1=0, empty=None):\n        self.l1 = l1\n        self.empty = empty\n\n\nclass K(BaseEstimator):\n    def __init__(self, c=None, d=None):\n        self.c = c\n        self.d = d\n\n\nclass T(BaseEstimator):\n    def __init__(self, a=None, b=None):\n        self.a = a\n        self.b = b\n\n\nclass NaNTag(BaseEstimator):\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.allow_nan = True\n        return tags\n\n\nclass NoNaNTag(BaseEstimator):\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.allow_nan = False\n        return tags\n\n\nclass OverrideTag(NaNTag):\n    def __sklear"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC, NuSVC\nfrom sklearn.utils import _array_api, all_estimators, deprecated\nfrom sklearn.utils._param_validation import Interval, StrOptions\nfrom sklearn.utils._test_common.instance_generator import (\n    _construct_instances,\n    _get_expected_failed_checks,\n)\nfrom sklearn.utils._testing import (\n    MinimalClassifier,\n    MinimalRegressor,\n    MinimalTransformer,\n    SkipTest,\n    ignore_warnings,\n    raises,\n)\nfrom sklearn.utils.estimator_checks import (\n    _check_name,\n    _NotAnArray,\n    _yield_all_checks,\n    check_array_api_input,\n    check_class_weight_balanced_linear_classifier,\n    check_classifier_data_not_an_array,\n    check_classifier_not_supporting_multiclass,\n    check_classifiers_multilabel_output_format_decision_function,\n    check_classifiers_multilabel_output_format_predict,\n    check_classifiers_multilabel_output_format_predict_proba,\n    check_classifiers_one_label_sample_weights,\n    check_dataframe_column_names_consistency,\n    check_decision_proba_consistency,\n    check_dict_unchanged,\n    check_dont_overwrite_parameters,\n    check_estimator,\n    check_estimator_cloneable,\n    check_estimator_repr,\n    check_estimator_sparse_array,\n    check_estimator_sparse_matrix,\n    check_estimator_sparse_tag,\n    check_estimator_tags_renamed,\n    check_estimators_nan_inf,\n    check_estimators_overwrite_params,\n    check_estimators_unfitted,\n    check_fit_check_is_fitted,\n    check_fit_score_takes_y,\n    check_methods_sample_order_invariance,\n    check_methods_subset_invariance,\n    check_mixin_order,\n    check_no_attributes_set_in_init,\n    check_outlier_contamination,\n    check_outlier_corruption,\n    check_parameters_default_constructible,\n    check_positive_only_tag_during_fit,\n    check_regressor_data_not_an_array,\n    check_requires_y_none,\n    check_sample_weights_pandas_series,\n    check_set_params,\n    estimator_checks_generator,\n    set_random_state,\n)\nfrom sklearn.utils.f"}, {"start_line": 34000, "end_line": 36000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ature_names_in_ attribute\"\n    )\n    with raises(ValueError, match=err_msg):\n        check_dataframe_column_names_consistency(lr.__class__.__name__, lr)\n\n\nclass _BaseMultiLabelClassifierMock(ClassifierMixin, BaseEstimator):\n    def __init__(self, response_output):\n        self.response_output = response_output\n\n    def fit(self, X, y):\n        return self\n\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.classifier_tags.multi_label = True\n        return tags\n\n\ndef test_check_classifiers_multilabel_output_format_predict():\n    n_samples, test_size, n_outputs = 100, 25, 5\n    _, y = make_multilabel_classification(\n        n_samples=n_samples,\n        n_features=2,\n        n_classes=n_outputs,\n        n_labels=3,\n        length=50,\n        allow_unlabeled=True,\n        random_state=0,\n    )\n    y_test = y[-test_size:]\n\n    class MultiLabelClassifierPredict(_BaseMultiLabelClassifierMock):\n        def predict(self, X):\n            return self.response_output\n\n    # 1. inconsistent array type\n    clf = MultiLabelClassifierPredict(response_output=y_test.tolist())\n    err_msg = (\n        r\"MultiLabelClassifierPredict.predict is expected to output a \"\n        r\"NumPy array. Got <class 'list'> instead.\"\n    )\n    with raises(AssertionError, match=err_msg):\n        check_classifiers_multilabel_output_format_predict(clf.__class__.__name__, clf)\n    # 2. inconsistent shape\n    clf = MultiLabelClassifierPredict(response_output=y_test[:, :-1])\n    err_msg = (\n        r\"MultiLabelClassifierPredict.predict outputs a NumPy array of \"\n        r\"shape \\(25, 4\\) instead of \\(25, 5\\).\"\n    )\n    with raises(AssertionError, match=err_msg):\n        check_classifiers_multilabel_output_format_predict(clf.__class__.__name__, clf)\n    # 3. inconsistent dtype\n    clf = MultiLabelClassifierPredict(response_output=y_test.astype(np.float64))\n    err_msg = (\n        r\"MultiLabelClassifierPredict.predict does not output the same \"\n        r\"dtype than the target"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ixes import CSR_CONTAINERS, SPARRAY_PRESENT\nfrom sklearn.utils.metaestimators import available_if\nfrom sklearn.utils.multiclass import type_of_target\nfrom sklearn.utils.validation import (\n    check_array,\n    check_is_fitted,\n    check_X_y,\n    validate_data,\n)\n\n\nclass CorrectNotFittedError(ValueError):\n    \"\"\"Exception class to raise if estimator is used before fitting.\n\n    Like NotFittedError, it inherits from ValueError, but not from\n    AttributeError. Used for testing only.\n    \"\"\"\n\n\nclass BaseBadClassifier(ClassifierMixin, BaseEstimator):\n    def fit(self, X, y):\n        return self\n\n    def predict(self, X):\n        return np.ones(X.shape[0])\n\n\nclass ChangesDict(BaseEstimator):\n    def __init__(self, key=0):\n        self.key = key\n\n    def fit(self, X, y=None):\n        X, y = validate_data(self, X, y)\n        return self\n\n    def predict(self, X):\n        X = check_array(X)\n        self.key = 1000\n        return np.ones(X.shape[0])\n\n\nclass SetsWrongAttribute(BaseEstimator):\n    def __init__(self, acceptable_key=0):\n        self.acceptable_key = acceptable_key\n\n    def fit(self, X, y=None):\n        self.wrong_attribute = 0\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ChangesWrongAttribute(BaseEstimator):\n    def __init__(self, wrong_attribute=0):\n        self.wrong_attribute = wrong_attribute\n\n    def fit(self, X, y=None):\n        self.wrong_attribute = 1\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass ChangesUnderscoreAttribute(BaseEstimator):\n    def fit(self, X, y=None):\n        self._good_attribute = 1\n        X, y = validate_data(self, X, y)\n        return self\n\n\nclass RaisesErrorInSetParams(BaseEstimator):\n    def __init__(self, p=0):\n        self.p = p\n\n    def set_params(self, **kwargs):\n        if \"p\" in kwargs:\n            p = kwargs.pop(\"p\")\n            if p < 0:\n                raise ValueError(\"p can't be less than 0\")\n            self.p = p\n        return super().set_params(**kwargs)\n\n    def fit(sel"}, {"start_line": 73000, "end_line": 75000, "belongs_to": {"file_name": "estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "form and transform outcomes not consistent in %s\"\n                % transformer,\n                atol=1e-2,\n            )\n            assert_allclose_dense_sparse(\n                X_pred,\n                X_pred3,\n                atol=1e-2,\n                err_msg=\"consecutive fit_transform outcomes not consistent in %s\"\n                % transformer,\n            )\n            assert _num_samples(X_pred2) == n_samples\n            assert _num_samples(X_pred3) == n_samples\n\n        # raises error on malformed input for transform\n        if (\n            hasattr(X, \"shape\")\n            and get_tags(transformer).requires_fit\n            and X.ndim == 2\n            and X.shape[1] > 1\n        ):\n            # If it's not an array, it does not have a 'T' property\n            with raises(\n                ValueError,\n                err_msg=(\n                    f\"The transformer {name} does not raise an error \"\n                    \"when the number of features in transform is different from \"\n                    \"the number of features in fit.\"\n                ),\n            ):\n                transformer.transform(X[:, :-1])\n\n\n@ignore_warnings\ndef check_pipeline_consistency(name, estimator_orig):\n    if get_tags(estimator_orig).non_deterministic:\n        msg = name + \" is non deterministic\"\n        raise SkipTest(msg)\n\n    # check that make_pipeline(est) gives same score as est\n    X, y = make_blobs(\n        n_samples=30,\n        centers=[[0, 0, 0], [1, 1, 1]],\n        random_state=0,\n        n_features=2,\n        cluster_std=0.1,\n    )\n    X = _enforce_estimator_tags_X(estimator_orig, X, kernel=rbf_kernel)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n    set_random_state(estimator)\n    pipeline = make_pipeline(estimator)\n    estimator.fit(X, y)\n    pipeline.fit(X, y)\n\n    funcs = [\"score\", \"fit_transform\"]\n\n    for func_name in funcs:\n        func = getattr(estimator, func_name, None)\n        if func is not None:\n            func_pip"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "put import _get_output_config\nfrom sklearn.utils._testing import (\n    _convert_container,\n    assert_array_equal,\n)\nfrom sklearn.utils.validation import _check_n_features, validate_data\n\n\n#############################################################################\n# A few test classes\nclass MyEstimator(BaseEstimator):\n    def __init__(self, l1=0, empty=None):\n        self.l1 = l1\n        self.empty = empty\n\n\nclass K(BaseEstimator):\n    def __init__(self, c=None, d=None):\n        self.c = c\n        self.d = d\n\n\nclass T(BaseEstimator):\n    def __init__(self, a=None, b=None):\n        self.a = a\n        self.b = b\n\n\nclass NaNTag(BaseEstimator):\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.allow_nan = True\n        return tags\n\n\nclass NoNaNTag(BaseEstimator):\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.allow_nan = False\n        return tags\n\n\nclass OverrideTag(NaNTag):\n    def __sklearn_tags__(self):\n        tags = super().__sklearn_tags__()\n        tags.input_tags.allow_nan = False\n        return tags\n\n\nclass DiamondOverwriteTag(NaNTag, NoNaNTag):\n    pass\n\n\nclass InheritDiamondOverwriteTag(DiamondOverwriteTag):\n    pass\n\n\nclass ModifyInitParams(BaseEstimator):\n    \"\"\"Deprecated behavior.\n    Equal parameters but with a type cast.\n    Doesn't fulfill a is a\n    \"\"\"\n\n    def __init__(self, a=np.array([0])):\n        self.a = a.copy()\n\n\nclass Buggy(BaseEstimator):\n    \"A buggy estimator that does not set its parameters right.\"\n\n    def __init__(self, a=None):\n        self.a = 1\n\n\nclass NoEstimator:\n    def __init__(self):\n        pass\n\n    def fit(self, X=None, y=None):\n        return self\n\n    def predict(self, X=None):\n        return None\n\n\nclass VargEstimator(BaseEstimator):\n    \"\"\"scikit-learn estimators shouldn't have vargs.\"\"\"\n\n    def __init__(self, *vargs):\n        pass\n\n\n#############################################################################\n# The tests"}, {"start_line": 65000, "end_line": 67000, "belongs_to": {"file_name": "estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "(estimator_orig)\n    if tags.classifier_tags is not None and not tags.classifier_tags.multi_class:\n        y[y == 2] = 1\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 2\n\n    set_random_state(estimator, 1)\n    estimator.fit(X, y)\n\n    idx = np.random.permutation(X.shape[0])\n\n    for method in [\n        \"predict\",\n        \"transform\",\n        \"decision_function\",\n        \"score_samples\",\n        \"predict_proba\",\n    ]:\n        msg = (\n            \"{method} of {name} is not invariant when applied to a dataset\"\n            \"with different sample order.\"\n        ).format(method=method, name=name)\n\n        if hasattr(estimator, method):\n            assert_allclose_dense_sparse(\n                _safe_indexing(getattr(estimator, method)(X), idx),\n                getattr(estimator, method)(_safe_indexing(X, idx)),\n                atol=1e-9,\n                err_msg=msg,\n            )\n\n\n@ignore_warnings\ndef check_fit2d_1sample(name, estimator_orig):\n    # Check that fitting a 2d array with only one sample either works or\n    # returns an informative message. The error message should either mention\n    # the number of samples or the number of classes.\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(1, 10))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n\n    y = X[:, 0].astype(int)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n\n    # min_cluster_size cannot be less than the data size for OPTICS.\n    if name == \"OPTICS\":\n        estimator.set_params(min_samples=1.0)\n\n    # perplexity cannot be more than the number of samples for TSNE.\n    if nam"}, {"start_line": 149000, "end_line": 151000, "belongs_to": {"file_name": "estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " set(curr_params.keys()), msg\n                for k, v in curr_params.items():\n                    assert test_params[k] is v, msg\n        test_params[param_name] = default_value\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_classifiers_regression_target(name, estimator_orig):\n    # Check if classifier throws an exception when fed regression targets\n\n    X, y = _regression_dataset()\n\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    e = clone(estimator_orig)\n    err_msg = (\n        \"When a classifier is passed a continuous target, it should raise a ValueError\"\n        \" with a message containing 'Unknown label type: ' or a message indicating that\"\n        \" a continuous target is passed and the message should include the word\"\n        \" 'continuous'\"\n    )\n    msg = \"Unknown label type: |continuous\"\n    if not get_tags(e).no_validation:\n        with raises(ValueError, match=msg, err_msg=err_msg):\n            e.fit(X, y)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_decision_proba_consistency(name, estimator_orig):\n    # Check whether an estimator having both decision_function and\n    # predict_proba methods has outputs with perfect rank correlation.\n\n    centers = [(2, 2), (4, 4)]\n    X, y = make_blobs(\n        n_samples=100,\n        random_state=0,\n        n_features=4,\n        centers=centers,\n        cluster_std=1.0,\n        shuffle=True,\n    )\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=0\n    )\n    estimator = clone(estimator_orig)\n\n    if hasattr(estimator, \"decision_function\") and hasattr(estimator, \"predict_proba\"):\n        estimator.fit(X_train, y_train)\n        # Since the link function from decision_function() to predict_proba()\n        # is sometimes not precise enough (typically expit), we round to the\n        # 10th decimal to avoid numerical issues: we compare the rank\n        # with deterministic ties rather than get platform specific rank\n        # inversions in case of machi"}], "retrieved_count": 10, "cost_time": 0.3422715663909912}
{"question": "How does Scikit-learn handle cross-validation?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn handles cross-validation through a comprehensive system of cross-validation splitters, functions, and utilities. Here's how cross-validation is implemented and managed:\n\n1. **Cross-Validation Functions**:\n   - **cross_validate()**: Main function for cross-validation with multiple metrics\n   - **cross_val_score()**: Simplified cross-validation for single metrics\n   - **cross_val_predict()**: Cross-validation predictions for diagnostics\n   - **_fit_and_score()**: Internal function for fitting and scoring in each fold\n   - **_aggregate_score_dicts()**: Aggregates results from multiple CV folds\n\n2. **Cross-Validation Splitters**:\n   - **KFold**: Basic k-fold cross-validation\n   - **StratifiedKFold**: Stratified k-fold for classification\n   - **ShuffleSplit**: Random train/test splits\n   - **StratifiedShuffleSplit**: Stratified random splits\n   - **TimeSeriesSplit**: Time series cross-validation\n   - **GroupKFold**: Group-based cross-validation\n   - **LeaveOneOut**: Leave-one-out cross-validation\n   - **LeavePOut**: Leave-p-out cross-validation\n\n3. **Data Splitting and Management**:\n   - **Index Generation**: Splitters generate train/test indices for each fold\n   - **Data Partitioning**: Original dataset is partitioned into train/test sets\n   - **Stratification**: Automatic stratification for classification problems\n   - **Group Handling**: Support for group-based splitting strategies\n   - **Time Series Handling**: Specialized splitting for temporal data\n\n4. **Parallel Processing Integration**:\n   - **Joblib Integration**: Parallel processing of CV folds\n   - **n_jobs Parameter**: Configurable parallelization\n   - **Pre-dispatch**: Control over job dispatching\n   - **Memory Management**: Efficient memory usage during parallel CV\n   - **Error Handling**: Robust error handling in parallel execution\n\n5. **Scoring and Metrics Integration**:\n   - **Multiple Metrics**: Support for multiple evaluation metrics\n   - **Custom Scoring**: Custom scoring functions\n   - **Metric Aggregation**: Statistical aggregation of CV results\n   - **Score Timing**: Measurement of fit and score times\n   - **Error Scoring**: Handling of failed CV folds\n\n6. **Estimator Management**:\n   - **Estimator Cloning**: Safe cloning of estimators for each fold\n   - **State Management**: Proper state management across folds\n   - **Parameter Passing**: Parameter routing to estimators and scorers\n   - **Metadata Routing**: Advanced metadata handling in CV\n   - **Fitted Estimators**: Option to return fitted estimators\n\n7. **Result Aggregation and Analysis**:\n   - **Score Aggregation**: Statistical aggregation of fold scores\n   - **Time Aggregation**: Aggregation of fit and score times\n   - **Result Formatting**: Consistent result format across different CV functions\n   - **Performance Analysis**: Analysis of CV performance metrics\n   - **Diagnostic Information**: Comprehensive diagnostic information\n\n8. **Advanced Cross-Validation Features**:\n   - **Nested Cross-Validation**: Support for nested CV patterns\n   - **Custom Splitters**: Support for custom splitting strategies\n   - **Conditional Splitting**: Conditional splitting based on data characteristics\n   - **Adaptive Splitting**: Adaptive splitting strategies\n   - **Ensemble CV**: CV strategies for ensemble methods\n\n9. **Error Handling and Robustness**:\n   - **Error Score Handling**: Configurable handling of CV failures\n   - **Warning Management**: Comprehensive warning system\n   - **Failure Recovery**: Recovery mechanisms for failed folds\n   - **Validation**: Input validation and error checking\n   - **Debugging Support**: Support for debugging CV issues\n\n10. **Integration with Other Systems**:\n    - **Pipeline Integration**: Seamless integration with scikit-learn pipelines\n    - **Grid Search Integration**: Integration with hyperparameter search\n    - **Model Selection**: Integration with model selection tools\n    - **Performance Monitoring**: Integration with performance monitoring\n    - **Documentation**: Comprehensive documentation and examples", "score": null, "retrieved_content": [{"start_line": 34000, "end_line": 36000, "belongs_to": {"file_name": "_search_successive_halving.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ger, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used. These splitters are instantiated\n        with `shuffle=False` so the splits will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. note::\n            Due to implementation details, the folds produced by `cv` must be\n            the same across multiple calls to `cv.split()`. For\n            built-in `scikit-learn` iterators, this can be achieved by\n            deactivating shuffling (`shuffle=False`), or by setting the\n            `cv`'s `random_state` parameter to an integer.\n\n    scoring : str or callable, default=None\n        Scoring method to use to evaluate the predictions on the test set.\n\n        - str: see :ref:`scoring_string_names` for options.\n        - callable: a scorer callable object (e.g., function) with signature\n          ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.\n        - `None`: the `estimator`'s\n          :ref:`default evaluation criterion <scoring_api_overview>` is used.\n\n    refit : bool or callable, default=True\n        Refit an estimator using the best found parameters on the whole\n        dataset.\n\n        Where there are considerations other than maximum score in\n        choosing a best estimator, ``refit`` can be set to a function which\n        returns the selected ``best_index_`` given ``cv_results_``. In that\n        case, the ``best_estimator_`` and ``best_params_`` will be set\n        according to the returned ``best_index_`` while the ``best_score_``\n        attribute will not be available.\n\n        The refitted estimator is made "}, {"start_line": 22000, "end_line": 24000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ean masks\n    svm = SVC(kernel=\"linear\")\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    kfold = KFold(5)\n    scores_indices = cross_val_score(svm, X, y, cv=kfold)\n    kfold = KFold(5)\n    cv_masks = []\n    for train, test in kfold.split(X, y):\n        mask_train = np.zeros(len(y), dtype=bool)\n        mask_test = np.zeros(len(y), dtype=bool)\n        mask_train[train] = 1\n        mask_test[test] = 1\n        cv_masks.append((train, test))\n    scores_masks = cross_val_score(svm, X, y, cv=cv_masks)\n    assert_array_equal(scores_indices, scores_masks)\n\n\ndef test_cross_val_score_precomputed():\n    # test for svm with precomputed kernel\n    svm = SVC(kernel=\"precomputed\")\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    linear_kernel = np.dot(X, X.T)\n    score_precomputed = cross_val_score(svm, linear_kernel, y)\n    svm = SVC(kernel=\"linear\")\n    score_linear = cross_val_score(svm, X, y)\n    assert_array_almost_equal(score_precomputed, score_linear)\n\n    # test with callable\n    svm = SVC(kernel=lambda x, y: np.dot(x, y.T))\n    score_callable = cross_val_score(svm, X, y)\n    assert_array_almost_equal(score_precomputed, score_callable)\n\n    # Error raised for non-square X\n    svm = SVC(kernel=\"precomputed\")\n    with pytest.raises(ValueError):\n        cross_val_score(svm, X, y)\n\n    # test error is raised when the precomputed kernel is not array-like\n    # or sparse\n    with pytest.raises(ValueError):\n        cross_val_score(svm, linear_kernel.tolist(), y)\n\n\n@pytest.mark.parametrize(\"coo_container\", COO_CONTAINERS)\ndef test_cross_val_score_fit_params(coo_container):\n    clf = MockClassifier()\n    n_samples = X.shape[0]\n    n_classes = len(np.unique(y))\n\n    W_sparse = coo_container(\n        (np.array([1]), (np.array([1]), np.array([0]))), shape=(15, 1)\n    )\n    P_sparse = coo_container(np.eye(5))\n\n    DUMMY_INT = 42\n    DUMMY_STR = \"42\"\n    DUMMY_OBJ = object()\n\n    def assert_fit_params(clf):\n        # Function to test that the values are passed co"}, {"start_line": 69000, "end_line": 71000, "belongs_to": {"file_name": "_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "on the number of samples usually has to\n        be big enough to contain at least one sample from each class.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross validation,\n        - int, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For int/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used. These splitters are instantiated\n        with `shuffle=False` so the splits will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    scoring : str or callable, default=None\n        Scoring method to use to evaluate the training and test sets.\n\n        - str: see :ref:`scoring_string_names` for options.\n        - callable: a scorer callable object (e.g., function) with signature\n          ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.\n        - `None`: the `estimator`'s\n          :ref:`default evaluation criterion <scoring_api_overview>` is used.\n\n    exploit_incremental_learning : bool, default=False\n        If the estimator supports incremental learning, this will be\n        used to speed up fitting for different training set sizes.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel. Training the estimator and computing\n        the score are parallelized over the different training and test sets.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processor"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "_sequential.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/feature_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "o use for cross-validation. Options:\n\n        - str: see :ref:`scoring_string_names` for options.\n        - callable: a scorer callable object (e.g., function) with signature\n          ``scorer(estimator, X, y)`` that returns a single value.\n          See :ref:`scoring_callable` for details.\n        - `None`: the `estimator`'s\n          :ref:`default evaluation criterion <scoring_api_overview>` is used.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass,\n        :class:`~sklearn.model_selection.StratifiedKFold` is used. In all other\n        cases, :class:`~sklearn.model_selection.KFold` is used. These splitters\n        are instantiated with `shuffle=False` so the splits will be the same\n        across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel. When evaluating a new feature to\n        add or remove, the cross-validation procedure is parallel over the\n        folds.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of fea"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "stance(estimators, list)\n    assert all(isinstance(estimator, Pipeline) for estimator in estimators)\n\n\n@pytest.mark.parametrize(\"use_sparse\", [False, True])\n@pytest.mark.parametrize(\"csr_container\", CSR_CONTAINERS)\ndef test_cross_validate(use_sparse: bool, csr_container):\n    # Compute train and test mse/r2 scores\n    cv = KFold()\n\n    # Regression\n    X_reg, y_reg = make_regression(n_samples=30, random_state=0)\n    reg = Ridge(random_state=0)\n\n    # Classification\n    X_clf, y_clf = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n\n    if use_sparse:\n        X_reg = csr_container(X_reg)\n        X_clf = csr_container(X_clf)\n\n    for X, y, est in ((X_reg, y_reg, reg), (X_clf, y_clf, clf)):\n        # It's okay to evaluate regression metrics on classification too\n        mse_scorer = check_scoring(est, scoring=\"neg_mean_squared_error\")\n        r2_scorer = check_scoring(est, scoring=\"r2\")\n        train_mse_scores = []\n        test_mse_scores = []\n        train_r2_scores = []\n        test_r2_scores = []\n        fitted_estimators = []\n\n        for train, test in cv.split(X, y):\n            est = clone(est).fit(X[train], y[train])\n            train_mse_scores.append(mse_scorer(est, X[train], y[train]))\n            train_r2_scores.append(r2_scorer(est, X[train], y[train]))\n            test_mse_scores.append(mse_scorer(est, X[test], y[test]))\n            test_r2_scores.append(r2_scorer(est, X[test], y[test]))\n            fitted_estimators.append(est)\n\n        train_mse_scores = np.array(train_mse_scores)\n        test_mse_scores = np.array(test_mse_scores)\n        train_r2_scores = np.array(train_r2_scores)\n        test_r2_scores = np.array(test_r2_scores)\n        fitted_estimators = np.array(fitted_estimators)\n\n        scores = (\n            train_mse_scores,\n            test_mse_scores,\n            train_r2_scores,\n            test_r2_scores,\n            fitted_estimators,\n        )\n\n        # To ensure that the test does not "}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     T = T.reshape(len(T), -1)\n        return T[:, 0]\n\n    def predict_proba(self, T):\n        return T\n\n    def score(self, X=None, Y=None):\n        return 1.0 / (1 + np.abs(self.a))\n\n    def get_params(self, deep=False):\n        return {\"a\": self.a, \"allow_nd\": self.allow_nd}\n\n\n# XXX: use 2D array, since 1D X is being detected as a single sample in\n# check_consistent_length\nX = np.ones((15, 2))\ny = np.array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 6])\n# The number of samples per class needs to be > n_splits,\n# for StratifiedKFold(n_splits=3)\ny2 = np.array([1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])\nP = np.eye(5)\n\n\n@pytest.mark.parametrize(\"coo_container\", COO_CONTAINERS)\ndef test_cross_val_score(coo_container):\n    clf = MockClassifier()\n    X_sparse = coo_container(X)\n\n    for a in range(-10, 10):\n        clf.a = a\n        # Smoke test\n        scores = cross_val_score(clf, X, y2)\n        assert_array_equal(scores, clf.score(X, y2))\n\n        # test with multioutput y\n        multioutput_y = np.column_stack([y2, y2[::-1]])\n        scores = cross_val_score(clf, X_sparse, multioutput_y)\n        assert_array_equal(scores, clf.score(X_sparse, multioutput_y))\n\n        scores = cross_val_score(clf, X_sparse, y2)\n        assert_array_equal(scores, clf.score(X_sparse, y2))\n\n        # test with multioutput y\n        scores = cross_val_score(clf, X_sparse, multioutput_y)\n        assert_array_equal(scores, clf.score(X_sparse, multioutput_y))\n\n    # test with X and y as list\n    list_check = lambda x: isinstance(x, list)\n    clf = CheckingClassifier(check_X=list_check)\n    scores = cross_val_score(clf, X.tolist(), y2.tolist(), cv=3)\n\n    clf = CheckingClassifier(check_y=list_check)\n    scores = cross_val_score(clf, X, y2.tolist(), cv=3)\n\n    # test with 3d X and\n    X_3d = X[:, :, np.newaxis]\n    clf = MockClassifier(allow_nd=True)\n    scores = cross_val_score(clf, X_3d, y2)\n\n    clf = MockClassifier(allow_nd=False)\n    with pytest.raises(ValueError):\n        cross_val_sc"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "       return splitter.split(X, y, groups=groups)\n    else:\n        return splitter.split(X, y)\n\n\ndef test_cross_validator_with_default_params():\n    n_samples = 4\n    n_unique_groups = 4\n    n_splits = 2\n    p = 2\n    n_shuffle_splits = 10  # (the default value)\n\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    X_1d = np.array([1, 2, 3, 4])\n    y = np.array([1, 1, 2, 2])\n    groups = np.array([1, 2, 3, 4])\n    loo = LeaveOneOut()\n    lpo = LeavePOut(p)\n    kf = KFold(n_splits)\n    skf = StratifiedKFold(n_splits)\n    lolo = LeaveOneGroupOut()\n    lopo = LeavePGroupsOut(p)\n    ss = ShuffleSplit(random_state=0)\n    ps = PredefinedSplit([1, 1, 2, 2])  # n_splits = np of unique folds = 2\n    sgkf = StratifiedGroupKFold(n_splits)\n\n    loo_repr = \"LeaveOneOut()\"\n    lpo_repr = \"LeavePOut(p=2)\"\n    kf_repr = \"KFold(n_splits=2, random_state=None, shuffle=False)\"\n    skf_repr = \"StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\"\n    lolo_repr = \"LeaveOneGroupOut()\"\n    lopo_repr = \"LeavePGroupsOut(n_groups=2)\"\n    ss_repr = (\n        \"ShuffleSplit(n_splits=10, random_state=0, test_size=None, train_size=None)\"\n    )\n    ps_repr = \"PredefinedSplit(test_fold=array([1, 1, 2, 2]))\"\n    sgkf_repr = \"StratifiedGroupKFold(n_splits=2, random_state=None, shuffle=False)\"\n\n    n_splits_expected = [\n        n_samples,\n        comb(n_samples, p),\n        n_splits,\n        n_splits,\n        n_unique_groups,\n        comb(n_unique_groups, p),\n        n_shuffle_splits,\n        2,\n        n_splits,\n    ]\n\n    for i, (cv, cv_repr) in enumerate(\n        zip(\n            [loo, lpo, kf, skf, lolo, lopo, ss, ps, sgkf],\n            [\n                loo_repr,\n                lpo_repr,\n                kf_repr,\n                skf_repr,\n                lolo_repr,\n                lopo_repr,\n                ss_repr,\n                ps_repr,\n                sgkf_repr,\n            ],\n        )\n    ):\n        # Test if get_n_splits works correctly\n        assert n_splits_expected[i] == cv"}, {"start_line": 21000, "end_line": 23000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ge):\n            cross_val_score(estimator=clf, X=X, y=y, cv=cv)\n        with pytest.raises(ValueError, match=error_message):\n            cross_val_predict(estimator=clf, X=X, y=y, cv=cv)\n\n\ndef test_cross_val_score_pandas():\n    # check cross_val_score doesn't destroy pandas dataframe\n    types = [(MockDataFrame, MockDataFrame)]\n    try:\n        from pandas import DataFrame, Series\n\n        types.append((Series, DataFrame))\n    except ImportError:\n        pass\n    for TargetType, InputFeatureType in types:\n        # X dataframe, y series\n        # 3 fold cross val is used so we need at least 3 samples per class\n        X_df, y_ser = InputFeatureType(X), TargetType(y2)\n        check_df = lambda x: isinstance(x, InputFeatureType)\n        check_series = lambda x: isinstance(x, TargetType)\n        clf = CheckingClassifier(check_X=check_df, check_y=check_series)\n        cross_val_score(clf, X_df, y_ser, cv=3)\n\n\ndef test_cross_val_score_mask():\n    # test that cross_val_score works with boolean masks\n    svm = SVC(kernel=\"linear\")\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    kfold = KFold(5)\n    scores_indices = cross_val_score(svm, X, y, cv=kfold)\n    kfold = KFold(5)\n    cv_masks = []\n    for train, test in kfold.split(X, y):\n        mask_train = np.zeros(len(y), dtype=bool)\n        mask_test = np.zeros(len(y), dtype=bool)\n        mask_train[train] = 1\n        mask_test[test] = 1\n        cv_masks.append((train, test))\n    scores_masks = cross_val_score(svm, X, y, cv=cv_masks)\n    assert_array_equal(scores_indices, scores_masks)\n\n\ndef test_cross_val_score_precomputed():\n    # test for svm with precomputed kernel\n    svm = SVC(kernel=\"precomputed\")\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    linear_kernel = np.dot(X, X.T)\n    score_precomputed = cross_val_score(svm, linear_kernel, y)\n    svm = SVC(kernel=\"linear\")\n    score_linear = cross_val_score(svm, X, y)\n    assert_array_almost_equal(score_precomputed, score_linear)\n\n    # test with c"}, {"start_line": 69000, "end_line": 71000, "belongs_to": {"file_name": "_search.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "d. In all\n        other cases, :class:`KFold` is used. These splitters are instantiated\n        with `shuffle=False` so the splits will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    verbose : int\n        Controls the verbosity: the higher, the more messages.\n\n        - >1 : the computation time for each fold and parameter candidate is\n          displayed;\n        - >2 : the score is also displayed;\n        - >3 : the fold and candidate parameter indexes are also displayed\n          together with the starting time of the computation.\n\n    pre_dispatch : int, or str, default='2*n_jobs'\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n        - None, in which case all the jobs are immediately created and spawned. Use\n          this for lightweight and fast-running jobs, to avoid delays due to on-demand\n          spawning of the jobs\n        - An int, giving the exact number of total jobs that are spawned\n        - A str, giving an expression as a function of n_jobs, as in '2*n_jobs'\n\n    random_state : int, RandomState instance or None, default=None\n        Pseudo random number generator state used for random uniform sampling\n        from lists of possible values instead of scipy.stats distributions.\n        Pass an int for reproducible output across multiple\n        function calls.\n        See :term:`Glossary <random_state>`.\n\n    error_score : 'raise' or numeric, default=np.nan\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised. If a numeric value is given,\n        FitFa"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_squared_error,\n    precision_recall_fscore_support,\n    precision_score,\n    r2_score,\n)\nfrom sklearn.metrics._scorer import _MultimetricScorer\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    GroupKFold,\n    GroupShuffleSplit,\n    KFold,\n    LeaveOneGroupOut,\n    LeaveOneOut,\n    LeavePGroupsOut,\n    ShuffleSplit,\n    StratifiedKFold,\n    cross_val_predict,\n    cross_val_score,\n    cross_validate,\n    learning_curve,\n    permutation_test_score,\n    validation_curve,\n)\nfrom sklearn.model_selection._validation import (\n    _check_is_permutation,\n    _fit_and_score,\n    _score,\n)\nfrom sklearn.model_selection.tests.common import OneTimeSplitter\nfrom sklearn.model_selection.tests.test_search import FailingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, scale\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tests.metadata_routing_common import (\n    ConsumingClassifier,\n    ConsumingScorer,\n    ConsumingSplitter,\n    _Registry,\n    check_recorded_metadata,\n)\nfrom sklearn.utils import shuffle\nfrom sklearn.utils._mocking import CheckingClassifier, MockDataFrame\nfrom sklearn.utils._testing import (\n    assert_allclose,\n    assert_almost_equal,\n    assert_array_almost_equal,\n    assert_array_equal,\n)\nfrom sklearn.utils.fixes import COO_CONTAINERS, CSR_CONTAINERS\nfrom sklearn.utils.validation import _num_samples\n\n\nclass MockImprovingEstimator(BaseEstimator):\n    \"\"\"Dummy classifier to test the learning curve\"\"\"\n\n    def __init__(self, n_max_train_sizes):\n        self.n_max_train_sizes = n_max_train_sizes\n        self.train_sizes = 0\n        self.X_subset = None\n\n    def fit(self, X_subset, y_subset=None):\n        self.X_subset = X_subset\n        self.train_sizes = X_subset.shape[0]\n        return self\n\n    def predict(self, X):\n        raise NotImplementedError\n\n   "}], "retrieved_count": 10, "cost_time": 0.3398566246032715}
{"question": "How does Scikit-learn integrate with NumPy arrays and pandas DataFrames for seamless data handling?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn integrates seamlessly with NumPy arrays and pandas DataFrames through a comprehensive system of input validation, automatic conversion, and output formatting. Here's how scikit-learn handles this integration:\n\n1. **Array-Like Input Support**:\n   - **Universal Acceptance**: All scikit-learn estimators accept 'array-like' inputs\n   - **NumPy Arrays**: Native support for numpy.ndarray as primary data structure\n   - **Pandas DataFrames**: Automatic conversion from pandas.DataFrame to NumPy arrays\n   - **Pandas Series**: Support for pandas.Series as target variables\n   - **List Conversion**: Automatic conversion from Python lists to NumPy arrays\n\n2. **Input Validation and Conversion**:\n   - **check_array()**: Core function that validates and converts inputs to NumPy arrays\n   - **Automatic Conversion**: Converts various input types to appropriate NumPy arrays\n   - **Type Validation**: Ensures inputs are numeric and have correct shapes\n   - **Memory Efficiency**: Optimized conversion to avoid unnecessary memory copies\n   - **Error Handling**: Clear error messages for invalid input types\n\n3. **Pandas DataFrame Integration**:\n   - **Direct Acceptance**: DataFrames are accepted as input without manual conversion\n   - **Column Preservation**: Feature names from DataFrame columns are preserved when possible\n   - **Index Handling**: DataFrame indices are handled appropriately in operations\n   - **Categorical Support**: Automatic handling of categorical columns\n   - **Mixed Data Types**: Support for heterogeneous DataFrames with numeric and categorical columns\n\n4. **Output Formatting with set_output API**:\n   - **Configurable Output**: set_output() method controls output format\n   - **DataFrame Output**: Can return results as pandas DataFrames with preserved column names\n   - **Series Output**: Target predictions can be returned as pandas Series\n   - **Index Preservation**: Maintains original DataFrame indices in outputs\n   - **Feature Names**: Preserves feature names in transformed outputs\n\n5. **ColumnTransformer for Heterogeneous Data**:\n   - **Mixed Data Types**: Handles DataFrames with both numeric and categorical columns\n   - **Column Selection**: Selects columns by name, dtype, or position\n   - **Separate Processing**: Applies different transformers to different column types\n   - **Feature Names**: Preserves feature names through transformations\n   - **Pipeline Integration**: Seamlessly integrates with scikit-learn pipelines\n\n6. **Sparse Matrix Support**:\n   - **Sparse DataFrames**: Support for sparse pandas DataFrames\n   - **Automatic Detection**: Automatically detects and handles sparse data\n   - **Efficient Conversion**: Efficient conversion between sparse formats\n   - **Memory Optimization**: Optimized memory usage for sparse data\n   - **Format Preservation**: Preserves sparse format when appropriate\n\n7. **Cross-Validation Integration**:\n   - **DataFrame Splitting**: Cross-validation works directly with DataFrames\n   - **Index Preservation**: Maintains DataFrame indices through CV splits\n   - **Feature Names**: Preserves feature names in CV results\n   - **Stratified Splitting**: Works with DataFrame targets for stratified CV\n   - **Parallel Processing**: Compatible with parallel CV execution\n\n8. **Pipeline Integration**:\n   - **DataFrame Pipelines**: Pipelines work seamlessly with DataFrames\n   - **Feature Name Propagation**: Feature names flow through pipeline steps\n   - **Mixed Data Processing**: Handles mixed data types in pipeline steps\n   - **Output Formatting**: Pipeline outputs can be formatted as DataFrames\n   - **Parameter Access**: Hierarchical parameter access works with DataFrame inputs\n\n9. **Performance Optimizations**:\n   - **Efficient Conversion**: Optimized conversion from DataFrames to NumPy arrays\n   - **Memory Management**: Efficient memory usage for large DataFrames\n   - **Lazy Evaluation**: Avoids unnecessary conversions when possible\n   - **Parallel Processing**: Compatible with parallel processing for large datasets\n   - **Caching**: Efficient caching of converted data structures\n\n10. **Best Practices and Limitations**:\n    - **Numeric Data**: Ensure all data is numeric for most estimators\n    - **Categorical Encoding**: Use appropriate encoders for categorical data\n    - **Missing Values**: Handle missing values before passing to estimators\n    - **Memory Considerations**: Be aware of memory usage with large DataFrames\n    - **Performance**: NumPy arrays may be faster for large-scale computations", "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "_indexing.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport numbers\nimport sys\nimport warnings\nfrom collections import UserList\nfrom itertools import compress, islice\n\nimport numpy as np\nfrom scipy.sparse import issparse\n\nfrom sklearn.utils._array_api import _is_numpy_namespace, get_namespace\nfrom sklearn.utils._param_validation import Interval, validate_params\nfrom sklearn.utils.extmath import _approximate_mode\nfrom sklearn.utils.fixes import PYARROW_VERSION_BELOW_17\nfrom sklearn.utils.validation import (\n    _check_sample_weight,\n    _is_arraylike_not_scalar,\n    _is_pandas_df,\n    _is_polars_df_or_series,\n    _is_pyarrow_data,\n    _use_interchange_protocol,\n    check_array,\n    check_consistent_length,\n    check_random_state,\n)\n\n\ndef _array_indexing(array, key, key_dtype, axis):\n    \"\"\"Index an array or scipy.sparse consistently across NumPy version.\"\"\"\n    xp, is_array_api = get_namespace(array)\n    if is_array_api:\n        return xp.take(array, key, axis=axis)\n    if issparse(array) and key_dtype == \"bool\":\n        key = np.asarray(key)\n    if isinstance(key, tuple):\n        key = list(key)\n    return array[key, ...] if axis == 0 else array[:, key]\n\n\ndef _pandas_indexing(X, key, key_dtype, axis):\n    \"\"\"Index a pandas dataframe or a series.\"\"\"\n    if _is_arraylike_not_scalar(key):\n        key = np.asarray(key)\n\n    if key_dtype == \"int\" and not (isinstance(key, slice) or np.isscalar(key)):\n        # using take() instead of iloc[] ensures the return value is a \"proper\"\n        # copy that will not raise SettingWithCopyWarning\n        return X.take(key, axis=axis)\n    else:\n        # check whether we should index with loc or iloc\n        indexer = X.iloc if key_dtype == \"int\" else X.loc\n        return indexer[:, key] if axis else indexer[key]\n\n\ndef _list_indexing(X, key, key_dtype):\n    \"\"\"Index a Python list.\"\"\"\n    if np.isscalar(key) or isinstance(key, slice):\n        # key is a slice or a scalar\n        return X[key]\n    if key_dtyp"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "test_estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e_matrix\"\n        self.raise_for_type = raise_for_type\n\n    def fit(self, X, y):\n        X, y = validate_data(\n            self,\n            X,\n            y,\n            accept_sparse=(\"csr\", \"csc\", \"coo\"),\n            accept_large_sparse=True,\n            multi_output=True,\n            y_numeric=True,\n        )\n        if self.raise_for_type == \"sparse_array\":\n            correct_type = isinstance(X, sp.sparray)\n        elif self.raise_for_type == \"sparse_matrix\":\n            correct_type = isinstance(X, sp.spmatrix)\n        if correct_type:\n            if X.format == \"coo\":\n                if X.row.dtype == \"int64\" or X.col.dtype == \"int64\":\n                    raise ValueError(\"Estimator doesn't support 64-bit indices\")\n            elif X.format in [\"csc\", \"csr\"]:\n                assert \"int64\" not in (\n                    X.indices.dtype,\n                    X.indptr.dtype,\n                ), \"Estimator doesn't support 64-bit indices\"\n\n        return self\n\n\nclass SparseTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self, sparse_container=None):\n        self.sparse_container = sparse_container\n\n    def fit(self, X, y=None):\n        validate_data(self, X)\n        return self\n\n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X)\n\n    def transform(self, X):\n        check_is_fitted(self)\n        X = validate_data(self, X, accept_sparse=True, reset=False)\n        return self.sparse_container(X)\n\n\nclass EstimatorInconsistentForPandas(BaseEstimator):\n    def fit(self, X, y):\n        try:\n            from pandas import DataFrame\n\n            if isinstance(X, DataFrame):\n                self.value_ = X.iloc[0, 0]\n            else:\n                X = check_array(X)\n                self.value_ = X[1, 0]\n            return self\n\n        except ImportError:\n            X = check_array(X)\n            self.value_ = X[1, 0]\n            return self\n\n    def predict(self, X):\n        X = check_array(X)\n        return np.array([self."}, {"start_line": 63000, "end_line": 65000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "seArray([1, 0, 1], dtype=ntype2, fill_value=0),\n        }\n    )\n    check_array(df, accept_sparse=[\"csr\", \"csc\"])\n\n\n@pytest.mark.parametrize(\n    \"ntype1, ntype2, expected_subtype\",\n    [\n        (\"double\", \"longdouble\", np.floating),\n        (\"single\", \"float32\", np.floating),\n        (\"double\", \"float64\", np.floating),\n        (\"int8\", \"byte\", np.integer),\n        (\"short\", \"int16\", np.integer),\n        (\"intc\", \"int32\", np.integer),\n        (\"intp\", \"long\", np.integer),\n        (\"int\", \"long\", np.integer),\n        (\"int64\", \"longlong\", np.integer),\n        (\"int_\", \"intp\", np.integer),\n        (\"ubyte\", \"uint8\", np.unsignedinteger),\n        (\"uint16\", \"ushort\", np.unsignedinteger),\n        (\"uintc\", \"uint32\", np.unsignedinteger),\n        (\"uint\", \"uint64\", np.unsignedinteger),\n        (\"uintp\", \"ulonglong\", np.unsignedinteger),\n    ],\n)\ndef test_check_pandas_sparse_valid(ntype1, ntype2, expected_subtype):\n    # check that we support the conversion of sparse dataframe with mixed\n    # type which can be converted safely.\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame(\n        {\n            \"col1\": pd.arrays.SparseArray([0, 1, 0], dtype=ntype1, fill_value=0),\n            \"col2\": pd.arrays.SparseArray([1, 0, 1], dtype=ntype2, fill_value=0),\n        }\n    )\n    arr = check_array(df, accept_sparse=[\"csr\", \"csc\"])\n    assert np.issubdtype(arr.dtype, expected_subtype)\n\n\n@pytest.mark.parametrize(\n    \"constructor_name\",\n    [\"list\", \"tuple\", \"array\", \"dataframe\", \"sparse_csr\", \"sparse_csc\"],\n)\ndef test_num_features(constructor_name):\n    \"\"\"Check _num_features for array-likes.\"\"\"\n    X = [[1, 2, 3], [4, 5, 6]]\n    X = _convert_container(X, constructor_name)\n    assert _num_features(X) == 3\n\n\n@pytest.mark.parametrize(\n    \"X\",\n    [\n        [1, 2, 3],\n        [\"a\", \"b\", \"c\"],\n        [False, True, False],\n        [1.0, 3.4, 4.0],\n        [{\"a\": 1}, {\"b\": 2}, {\"c\": 3}],\n    ],\n    ids=[\"int\", \"str\", \"bool\", \"float\", \"dict\"],\n)\n@pytest.mark.parametrize(\"constru"}, {"start_line": 84000, "end_line": 86000, "belongs_to": {"file_name": "validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "arams.items():\n        if (\n            not _is_arraylike(param_value) and not sp.issparse(param_value)\n        ) or _num_samples(param_value) != _num_samples(X):\n            # Non-indexable pass-through (for now for backward-compatibility).\n            # https://github.com/scikit-learn/scikit-learn/issues/15805\n            method_params_validated[param_key] = param_value\n        else:\n            # Any other method_params should support indexing\n            # (e.g. for cross-validation).\n            method_params_validated[param_key] = _make_indexable(param_value)\n            method_params_validated[param_key] = _safe_indexing(\n                method_params_validated[param_key], indices\n            )\n\n    return method_params_validated\n\n\ndef _is_pandas_df_or_series(X):\n    \"\"\"Return True if the X is a pandas dataframe or series.\"\"\"\n    try:\n        pd = sys.modules[\"pandas\"]\n    except KeyError:\n        return False\n    return isinstance(X, (pd.DataFrame, pd.Series))\n\n\ndef _is_pandas_df(X):\n    \"\"\"Return True if the X is a pandas dataframe.\"\"\"\n    try:\n        pd = sys.modules[\"pandas\"]\n    except KeyError:\n        return False\n    return isinstance(X, pd.DataFrame)\n\n\ndef _is_pyarrow_data(X):\n    \"\"\"Return True if the X is a pyarrow Table, RecordBatch, Array or ChunkedArray.\"\"\"\n    try:\n        pa = sys.modules[\"pyarrow\"]\n    except KeyError:\n        return False\n    return isinstance(X, (pa.Table, pa.RecordBatch, pa.Array, pa.ChunkedArray))\n\n\ndef _is_polars_df_or_series(X):\n    \"\"\"Return True if the X is a polars dataframe or series.\"\"\"\n    try:\n        pl = sys.modules[\"polars\"]\n    except KeyError:\n        return False\n    return isinstance(X, (pl.DataFrame, pl.Series))\n\n\ndef _is_polars_df(X):\n    \"\"\"Return True if the X is a polars dataframe.\"\"\"\n    try:\n        pl = sys.modules[\"polars\"]\n    except KeyError:\n        return False\n    return isinstance(X, pl.DataFrame)\n\n\ndef _get_feature_names(X):\n    \"\"\"Get feature names from X.\n\n    Support for other array con"}, {"start_line": 24000, "end_line": 26000, "belongs_to": {"file_name": "validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " early.\"\"\"\n    # Check these early for pandas versions without extension dtypes\n    from pandas import SparseDtype\n    from pandas.api.types import (\n        is_bool_dtype,\n        is_float_dtype,\n        is_integer_dtype,\n    )\n\n    if is_bool_dtype(pd_dtype):\n        # bool and extension booleans need early conversion because __array__\n        # converts mixed dtype dataframes into object dtypes\n        return True\n\n    if isinstance(pd_dtype, SparseDtype):\n        # Sparse arrays will be converted later in `check_array`\n        return False\n\n    try:\n        from pandas.api.types import is_extension_array_dtype\n    except ImportError:\n        return False\n\n    if isinstance(pd_dtype, SparseDtype) or not is_extension_array_dtype(pd_dtype):\n        # Sparse arrays will be converted later in `check_array`\n        # Only handle extension arrays for integer and floats\n        return False\n    elif is_float_dtype(pd_dtype):\n        # Float ndarrays can normally support nans. They need to be converted\n        # first to map pd.NA to np.nan\n        return True\n    elif is_integer_dtype(pd_dtype):\n        # XXX: Warn when converting from a high integer to a float\n        return True\n\n    return False\n\n\ndef _is_extension_array_dtype(array):\n    # Pandas extension arrays have a dtype with an na_value\n    return hasattr(array, \"dtype\") and hasattr(array.dtype, \"na_value\")\n\n\ndef check_array(\n    array,\n    accept_sparse=False,\n    *,\n    accept_large_sparse=True,\n    dtype=\"numeric\",\n    order=None,\n    copy=False,\n    force_writeable=False,\n    force_all_finite=\"deprecated\",\n    ensure_all_finite=None,\n    ensure_non_negative=False,\n    ensure_2d=True,\n    allow_nd=False,\n    ensure_min_samples=1,\n    ensure_min_features=1,\n    estimator=None,\n    input_name=\"\",\n):\n    \"\"\"Input validation on an array, list, sparse matrix or similar.\n\n    By default, the input is checked to be a non-empty 2D array containing\n    only finite values. If the dtype of the array is object, attempt"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_data.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/preprocessing/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "VR\nfrom sklearn.utils import gen_batches, shuffle\nfrom sklearn.utils._array_api import (\n    _convert_to_numpy,\n    _get_namespace_device_dtype_ids,\n    yield_namespace_device_dtype_combinations,\n)\nfrom sklearn.utils._test_common.instance_generator import _get_check_estimator_ids\nfrom sklearn.utils._testing import (\n    _array_api_for_tests,\n    _convert_container,\n    assert_allclose,\n    assert_allclose_dense_sparse,\n    assert_almost_equal,\n    assert_array_almost_equal,\n    assert_array_equal,\n    assert_array_less,\n    skip_if_32bit,\n)\nfrom sklearn.utils.estimator_checks import (\n    check_array_api_input_and_values,\n)\nfrom sklearn.utils.fixes import (\n    COO_CONTAINERS,\n    CSC_CONTAINERS,\n    CSR_CONTAINERS,\n    LIL_CONTAINERS,\n    sp_version,\n)\nfrom sklearn.utils.sparsefuncs import mean_variance_axis\n\niris = datasets.load_iris()\n\n# Make some data to be used many times\nrng = np.random.RandomState(0)\nn_features = 30\nn_samples = 1000\noffsets = rng.uniform(-1, 1, size=n_features)\nscales = rng.uniform(1, 10, size=n_features)\nX_2d = rng.randn(n_samples, n_features) * scales + offsets\nX_1row = X_2d[0, :].reshape(1, n_features)\nX_1col = X_2d[:, 0].reshape(n_samples, 1)\nX_list_1row = X_1row.tolist()\nX_list_1col = X_1col.tolist()\n\n\ndef toarray(a):\n    if hasattr(a, \"toarray\"):\n        a = a.toarray()\n    return a\n\n\ndef _check_dim_1axis(a):\n    return np.asarray(a).shape[0]\n\n\ndef assert_correct_incr(i, batch_start, batch_stop, n, chunk_size, n_samples_seen):\n    if batch_stop != n:\n        assert (i + 1) * chunk_size == n_samples_seen\n    else:\n        assert i * chunk_size + (batch_stop - batch_start) == n_samples_seen\n\n\ndef test_raises_value_error_if_sample_weights_greater_than_1d():\n    # Sample weights must be either scalar or 1D\n\n    n_sampless = [2, 3]\n    n_featuress = [3, 2]\n\n    for n_samples, n_features in zip(n_sampless, n_featuress):\n        X = rng.randn(n_samples, n_features)\n        y = rng.randn(n_samples)\n\n        scaler = StandardScaler()\n\n        # "}, {"start_line": 24000, "end_line": 26000, "belongs_to": {"file_name": "test_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n on transform\n    Xs = [X_np, df_int_names]\n    for X in Xs:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\", UserWarning)\n            trans.transform(X)\n\n    # fit on dataframe with feature names that are mixed raises an error:\n    df_mixed = pd.DataFrame(X_np, columns=[\"a\", \"b\", 1, 2])\n    trans = NoOpTransformer()\n    msg = re.escape(\n        \"Feature names are only supported if all input features have string names, \"\n        \"but your input has ['int', 'str'] as feature name / column name types. \"\n        \"If you want feature names to be stored and validated, you must convert \"\n        \"them all to strings, by using X.columns = X.columns.astype(str) for \"\n        \"example. Otherwise you can remove feature / column names from your input \"\n        \"data, or convert them all to a non-string data type.\"\n    )\n    with pytest.raises(TypeError, match=msg):\n        trans.fit(df_mixed)\n\n    # transform on feature names that are mixed also raises:\n    with pytest.raises(TypeError, match=msg):\n        trans.transform(df_mixed)\n\n\ndef test_validate_data_skip_check_array():\n    \"\"\"Check skip_check_array option of _validate_data.\"\"\"\n\n    pd = pytest.importorskip(\"pandas\")\n    iris = datasets.load_iris()\n    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n    y = pd.Series(iris.target)\n\n    class NoOpTransformer(TransformerMixin, BaseEstimator):\n        pass\n\n    no_op = NoOpTransformer()\n    X_np_out = validate_data(no_op, df, skip_check_array=False)\n    assert isinstance(X_np_out, np.ndarray)\n    assert_allclose(X_np_out, df.to_numpy())\n\n    X_df_out = validate_data(no_op, df, skip_check_array=True)\n    assert X_df_out is df\n\n    y_np_out = validate_data(no_op, y=y, skip_check_array=False)\n    assert isinstance(y_np_out, np.ndarray)\n    assert_allclose(y_np_out, y.to_numpy())\n\n    y_series_out = validate_data(no_op, y=y, skip_check_array=True)\n    assert y_series_out is y\n\n    X_np_out, y_np_out = validate_data(no_op, df, y, skip_c"}, {"start_line": 37000, "end_line": 39000, "belongs_to": {"file_name": "test_forest.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "        )\n\n    if name in FOREST_TRANSFORMERS:\n        assert_array_almost_equal(\n            sparse.transform(X).toarray(), dense.transform(X).toarray()\n        )\n        assert_array_almost_equal(\n            sparse.fit_transform(X).toarray(), dense.fit_transform(X).toarray()\n        )\n\n\n@pytest.mark.parametrize(\"name\", FOREST_CLASSIFIERS_REGRESSORS)\n@pytest.mark.parametrize(\"dtype\", (np.float64, np.float32))\ndef test_memory_layout(name, dtype):\n    # Test that it works no matter the memory layout\n    est = FOREST_ESTIMATORS[name](random_state=0, bootstrap=False)\n\n    # Dense\n    for container, kwargs in (\n        (np.asarray, {}),  # Nothing\n        (np.asarray, {\"order\": \"C\"}),  # C-order\n        (np.asarray, {\"order\": \"F\"}),  # F-order\n        (np.ascontiguousarray, {}),  # Contiguous\n    ):\n        X = container(iris.data, dtype=dtype, **kwargs)\n        y = iris.target\n        assert_array_almost_equal(est.fit(X, y).predict(X), y)\n\n    # Sparse (if applicable)\n    if est.estimator.splitter in SPARSE_SPLITTERS:\n        for sparse_container in COO_CONTAINERS + CSC_CONTAINERS + CSR_CONTAINERS:\n            X = sparse_container(iris.data, dtype=dtype)\n            y = iris.target\n            assert_array_almost_equal(est.fit(X, y).predict(X), y)\n\n    # Strided\n    X = np.asarray(iris.data[::3], dtype=dtype)\n    y = iris.target[::3]\n    assert_array_almost_equal(est.fit(X, y).predict(X), y)\n\n\n@pytest.mark.parametrize(\"name\", FOREST_ESTIMATORS)\ndef test_1d_input(name):\n    X = iris.data[:, 0]\n    X_2d = iris.data[:, 0].reshape((-1, 1))\n    y = iris.target\n\n    with ignore_warnings():\n        ForestEstimator = FOREST_ESTIMATORS[name]\n        with pytest.raises(ValueError):\n            ForestEstimator(n_estimators=1, random_state=0).fit(X, y)\n\n        est = ForestEstimator(random_state=0)\n        est.fit(X_2d, y)\n\n        if name in FOREST_CLASSIFIERS or name in FOREST_REGRESSORS:\n            with pytest.raises(ValueError):\n                est.predict(X)\n\n\n@pytest.mark."}, {"start_line": 64000, "end_line": 66000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# type which can be converted safely.\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame(\n        {\n            \"col1\": pd.arrays.SparseArray([0, 1, 0], dtype=ntype1, fill_value=0),\n            \"col2\": pd.arrays.SparseArray([1, 0, 1], dtype=ntype2, fill_value=0),\n        }\n    )\n    arr = check_array(df, accept_sparse=[\"csr\", \"csc\"])\n    assert np.issubdtype(arr.dtype, expected_subtype)\n\n\n@pytest.mark.parametrize(\n    \"constructor_name\",\n    [\"list\", \"tuple\", \"array\", \"dataframe\", \"sparse_csr\", \"sparse_csc\"],\n)\ndef test_num_features(constructor_name):\n    \"\"\"Check _num_features for array-likes.\"\"\"\n    X = [[1, 2, 3], [4, 5, 6]]\n    X = _convert_container(X, constructor_name)\n    assert _num_features(X) == 3\n\n\n@pytest.mark.parametrize(\n    \"X\",\n    [\n        [1, 2, 3],\n        [\"a\", \"b\", \"c\"],\n        [False, True, False],\n        [1.0, 3.4, 4.0],\n        [{\"a\": 1}, {\"b\": 2}, {\"c\": 3}],\n    ],\n    ids=[\"int\", \"str\", \"bool\", \"float\", \"dict\"],\n)\n@pytest.mark.parametrize(\"constructor_name\", [\"list\", \"tuple\", \"array\", \"series\"])\ndef test_num_features_errors_1d_containers(X, constructor_name):\n    X = _convert_container(X, constructor_name)\n    if constructor_name == \"array\":\n        expected_type_name = \"numpy.ndarray\"\n    elif constructor_name == \"series\":\n        expected_type_name = \"pandas.*Series\"\n    else:\n        expected_type_name = constructor_name\n    message = (\n        f\"Unable to find the number of features from X of type {expected_type_name}\"\n    )\n    if hasattr(X, \"shape\"):\n        message += re.escape(\" with shape (3,)\")\n    elif isinstance(X[0], str):\n        message += \" where the samples are of type str\"\n    elif isinstance(X[0], dict):\n        message += \" where the samples are of type dict\"\n    with pytest.raises(TypeError, match=message):\n        _num_features(X)\n\n\n@pytest.mark.parametrize(\"X\", [1, \"b\", False, 3.0], ids=[\"int\", \"str\", \"bool\", \"float\"])\ndef test_num_features_errors_scalars(X):\n    msg = f\"Unable to find the number of featu"}, {"start_line": 76000, "end_line": 78000, "belongs_to": {"file_name": "test_validation.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ")\ndef test_check_array_multiple_extensions(\n    extension_dtype, regular_dtype, include_object\n):\n    \"\"\"Check pandas extension arrays give the same result as non-extension arrays.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X_regular = pd.DataFrame(\n        {\n            \"a\": pd.Series([1, 0, 1, 0], dtype=regular_dtype),\n            \"c\": pd.Series([9, 8, 7, 6], dtype=\"int64\"),\n        }\n    )\n    if include_object:\n        X_regular[\"b\"] = pd.Series([\"a\", \"b\", \"c\", \"d\"], dtype=\"object\")\n\n    X_extension = X_regular.assign(a=X_regular[\"a\"].astype(extension_dtype))\n\n    X_regular_checked = check_array(X_regular, dtype=None)\n    X_extension_checked = check_array(X_extension, dtype=None)\n    assert_array_equal(X_regular_checked, X_extension_checked)\n\n\ndef test_num_samples_dataframe_protocol():\n    \"\"\"Use the DataFrame interchange protocol to get n_samples from polars.\"\"\"\n    pl = pytest.importorskip(\"polars\")\n\n    df = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n    assert _num_samples(df) == 3\n\n\n@pytest.mark.parametrize(\n    \"sparse_container\",\n    CSR_CONTAINERS + CSC_CONTAINERS + COO_CONTAINERS + DIA_CONTAINERS,\n)\n@pytest.mark.parametrize(\"output_format\", [\"csr\", \"csc\", \"coo\"])\ndef test_check_array_dia_to_int32_indexed_csr_csc_coo(sparse_container, output_format):\n    \"\"\"Check the consistency of the indices dtype with sparse matrices/arrays.\"\"\"\n    X = sparse_container([[0, 1], [1, 0]], dtype=np.float64)\n\n    # Explicitly set the dtype of the indexing arrays\n    if hasattr(X, \"offsets\"):  # DIA matrix\n        X.offsets = X.offsets.astype(np.int32)\n    elif hasattr(X, \"row\") and hasattr(X, \"col\"):  # COO matrix\n        X.row = X.row.astype(np.int32)\n    elif hasattr(X, \"indices\") and hasattr(X, \"indptr\"):  # CSR or CSC matrix\n        X.indices = X.indices.astype(np.int32)\n        X.indptr = X.indptr.astype(np.int32)\n\n    X_checked = check_array(X, accept_sparse=output_format)\n    if output_format == \"coo\":\n        assert X_checked.row.dtype == np.int32\n        a"}], "retrieved_count": 10, "cost_time": 0.3397374153137207}
{"question": "How does Scikit-learn implement different splitting strategies (KFold, StratifiedKFold, etc.)?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn implements different splitting strategies through a well-designed hierarchy of cross-validation splitters. Here's how the various splitting strategies are implemented:\n\n1. **Base Cross-Validator Architecture**:\n   - **BaseCrossValidator**: Abstract base class for all CV splitters\n   - **split() Method**: Core method that yields train/test index pairs\n   - **get_n_splits() Method**: Returns the number of splitting iterations\n   - **Common Interface**: All splitters follow the same interface\n   - **Parameter Validation**: Standardized parameter validation across all splitters\n\n2. **KFold Implementation**:\n   - **Basic K-Fold**: Divides samples into k consecutive folds\n   - **Equal Size Folds**: Attempts to create folds of equal size\n   - **Sequential Splitting**: Splits data sequentially without shuffling by default\n   - **Shuffle Option**: Optional shuffling before splitting\n   - **Random State**: Reproducible randomization when shuffle=True\n\n3. **StratifiedKFold Implementation**:\n   - **Class Preservation**: Preserves percentage of samples for each class\n   - **Stratification Logic**: Ensures each fold has similar class distribution\n   - **Target Variable**: Uses y (target) for stratification\n   - **Multi-class Support**: Handles binary and multiclass classification\n   - **Balanced Folds**: Creates folds with balanced class distributions\n\n4. **Group-Based Splitting Strategies**:\n   - **GroupKFold**: Ensures same group not in both train and test sets\n   - **StratifiedGroupKFold**: Combines stratification with group constraints\n   - **GroupShuffleSplit**: Random group-based splitting\n   - **Group Handling**: Uses groups parameter for group identification\n   - **Non-overlapping Groups**: Prevents group leakage between folds\n\n5. **Shuffle-Based Splitting Strategies**:\n   - **ShuffleSplit**: Random train/test splits with shuffling\n   - **StratifiedShuffleSplit**: Stratified version of ShuffleSplit\n   - **Random Sampling**: Independent random sampling for each split\n   - **Configurable Sizes**: Configurable train/test split sizes\n   - **Multiple Iterations**: Support for multiple splitting iterations\n\n6. **Leave-Based Splitting Strategies**:\n   - **LeaveOneOut**: Leaves one sample out for each fold\n   - **LeavePOut**: Leaves p samples out for each fold\n   - **LeaveOneGroupOut**: Leaves one group out for each fold\n   - **LeavePGroupsOut**: Leaves p groups out for each fold\n   - **Exhaustive Splitting**: Creates all possible combinations\n\n7. **Time Series Splitting Strategies**:\n   - **TimeSeriesSplit**: Forward chaining for time series data\n   - **Temporal Ordering**: Respects temporal ordering of data\n   - **Expanding Windows**: Training set expands over time\n   - **Fixed Test Size**: Fixed-size test sets for evaluation\n   - **No Future Leakage**: Prevents future information leakage\n\n8. **Specialized Splitting Strategies**:\n   - **PredefinedSplit**: Uses pre-defined train/test splits\n   - **RepeatedKFold**: Repeats KFold with different randomizations\n   - **RepeatedStratifiedKFold**: Repeats StratifiedKFold\n   - **Custom Splitters**: Support for custom splitting strategies\n   - **Iterable Support**: Support for iterable-based splitting\n\n9. **Implementation Features**:\n   - **Index Generation**: Efficient index generation for large datasets\n   - **Memory Efficiency**: Memory-efficient implementation\n   - **Parallel Support**: Support for parallel processing\n   - **Validation**: Comprehensive input validation\n   - **Error Handling**: Robust error handling and edge cases\n\n10. **Integration and Compatibility**:\n    - **Cross-Validation Functions**: Integration with cross_validate, cross_val_score\n    - **Grid Search**: Integration with hyperparameter search\n    - **Pipeline Support**: Seamless integration with pipelines\n    - **Metadata Routing**: Support for metadata routing\n    - **Documentation**: Comprehensive documentation and examples", "score": null, "retrieved_content": [{"start_line": 23000, "end_line": 25000, "belongs_to": {"file_name": "_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        return super().split(X, y, groups)\n\n\nclass StratifiedKFold(_BaseKFold):\n    \"\"\"Class-wise stratified K-Fold cross-validator.\n\n    Provides train/test indices to split data in train/test sets.\n\n    This cross-validation object is a variation of KFold that returns\n    stratified folds. The folds are made by preserving the percentage of\n    samples for each class in `y` in a binary or multiclass classification\n    setting.\n\n    Read more in the :ref:`User Guide <stratified_k_fold>`.\n\n    For visualisation of cross-validation behaviour and\n    comparison between common scikit-learn split methods\n    refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n\n    .. note::\n\n        Stratification on the class label solves an engineering problem rather\n        than a statistical one. See :ref:`stratification` for more details.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n\n        .. versionchanged:: 0.22\n            ``n_splits`` default value changed from 3 to 5.\n\n    shuffle : bool, default=False\n        Whether to shuffle each class's samples before splitting into batches.\n        Note that the samples within each split will not be shuffled.\n\n    random_state : int, RandomState instance or None, default=None\n        When `shuffle` is True, `random_state` affects the ordering of the\n        indices, which controls the randomness of each fold for each class.\n        Otherwise, leave `random_state` as `None`.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import StratifiedKFold\n    >>> X = np.a"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rn.utils.validation import _num_samples, check_array, column_or_1d\n\n__all__ = [\n    \"BaseCrossValidator\",\n    \"GroupKFold\",\n    \"GroupShuffleSplit\",\n    \"KFold\",\n    \"LeaveOneGroupOut\",\n    \"LeaveOneOut\",\n    \"LeavePGroupsOut\",\n    \"LeavePOut\",\n    \"PredefinedSplit\",\n    \"RepeatedKFold\",\n    \"RepeatedStratifiedKFold\",\n    \"ShuffleSplit\",\n    \"StratifiedGroupKFold\",\n    \"StratifiedKFold\",\n    \"StratifiedShuffleSplit\",\n    \"check_cv\",\n    \"train_test_split\",\n]\n\n\nclass _UnsupportedGroupCVMixin:\n    \"\"\"Mixin for splitters that do not support Groups.\"\"\"\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            The target variable for supervised learning problems.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is not None:\n            warnings.warn(\n                f\"The groups parameter is ignored by {self.__class__.__name__}\",\n                UserWarning,\n            )\n        return super().split(X, y, groups=groups)\n\n\nclass GroupsConsumerMixin(_MetadataRequester):\n    \"\"\"A Mixin to ``groups`` by default.\n\n    This Mixin makes the object to request ``groups`` by default as ``True``.\n\n    .. versionadded:: 1.3\n    \"\"\"\n\n    __metadata_request__split = {\"groups\": True}\n\n\nclass BaseCrossValidator(_MetadataRequester, metaclass=ABCMeta):\n    \"\"\"Base class for all cross-validators.\n\n    Implementations must define `_iter_test_masks` or `_iter_test_indices`.\n    \"\"\"\n\n    # This indicates that by default C"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "       return splitter.split(X, y, groups=groups)\n    else:\n        return splitter.split(X, y)\n\n\ndef test_cross_validator_with_default_params():\n    n_samples = 4\n    n_unique_groups = 4\n    n_splits = 2\n    p = 2\n    n_shuffle_splits = 10  # (the default value)\n\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    X_1d = np.array([1, 2, 3, 4])\n    y = np.array([1, 1, 2, 2])\n    groups = np.array([1, 2, 3, 4])\n    loo = LeaveOneOut()\n    lpo = LeavePOut(p)\n    kf = KFold(n_splits)\n    skf = StratifiedKFold(n_splits)\n    lolo = LeaveOneGroupOut()\n    lopo = LeavePGroupsOut(p)\n    ss = ShuffleSplit(random_state=0)\n    ps = PredefinedSplit([1, 1, 2, 2])  # n_splits = np of unique folds = 2\n    sgkf = StratifiedGroupKFold(n_splits)\n\n    loo_repr = \"LeaveOneOut()\"\n    lpo_repr = \"LeavePOut(p=2)\"\n    kf_repr = \"KFold(n_splits=2, random_state=None, shuffle=False)\"\n    skf_repr = \"StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\"\n    lolo_repr = \"LeaveOneGroupOut()\"\n    lopo_repr = \"LeavePGroupsOut(n_groups=2)\"\n    ss_repr = (\n        \"ShuffleSplit(n_splits=10, random_state=0, test_size=None, train_size=None)\"\n    )\n    ps_repr = \"PredefinedSplit(test_fold=array([1, 1, 2, 2]))\"\n    sgkf_repr = \"StratifiedGroupKFold(n_splits=2, random_state=None, shuffle=False)\"\n\n    n_splits_expected = [\n        n_samples,\n        comb(n_samples, p),\n        n_splits,\n        n_splits,\n        n_unique_groups,\n        comb(n_unique_groups, p),\n        n_shuffle_splits,\n        2,\n        n_splits,\n    ]\n\n    for i, (cv, cv_repr) in enumerate(\n        zip(\n            [loo, lpo, kf, skf, lolo, lopo, ss, ps, sgkf],\n            [\n                loo_repr,\n                lpo_repr,\n                kf_repr,\n                skf_repr,\n                lolo_repr,\n                lopo_repr,\n                ss_repr,\n                ps_repr,\n                sgkf_repr,\n            ],\n        )\n    ):\n        # Test if get_n_splits works correctly\n        assert n_splits_expected[i] == cv"}, {"start_line": 24000, "end_line": 26000, "belongs_to": {"file_name": "_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ing problem rather\n        than a statistical one. See :ref:`stratification` for more details.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n\n        .. versionchanged:: 0.22\n            ``n_splits`` default value changed from 3 to 5.\n\n    shuffle : bool, default=False\n        Whether to shuffle each class's samples before splitting into batches.\n        Note that the samples within each split will not be shuffled.\n\n    random_state : int, RandomState instance or None, default=None\n        When `shuffle` is True, `random_state` affects the ordering of the\n        indices, which controls the randomness of each fold for each class.\n        Otherwise, leave `random_state` as `None`.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import StratifiedKFold\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> skf = StratifiedKFold(n_splits=2)\n    >>> skf.get_n_splits(X, y)\n    2\n    >>> print(skf)\n    StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n    >>> for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n    ...     print(f\"Fold {i}:\")\n    ...     print(f\"  Train: index={train_index}\")\n    ...     print(f\"  Test:  index={test_index}\")\n    Fold 0:\n      Train: index=[1 3]\n      Test:  index=[0 2]\n    Fold 1:\n      Train: index=[0 2]\n      Test:  index=[1 3]\n\n    Notes\n    -----\n    The implementation is designed to:\n\n    * Generate test sets such that all contain the same distribution of\n      classes, or as close as possible.\n    * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n      ``y = [1, 0]`` should not change the indices generated.\n    * Preserve order dependencies in the dataset ordering, when\n      ``shuffle=False``: all samples from class k in some t"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "sOut(n_groups=1),\n    StratifiedGroupKFold(),\n    LeaveOneGroupOut(),\n    GroupShuffleSplit(),\n]\nGROUP_SPLITTER_NAMES = set(splitter.__class__.__name__ for splitter in GROUP_SPLITTERS)\n\nALL_SPLITTERS = NO_GROUP_SPLITTERS + GROUP_SPLITTERS  # type: ignore[list-item]\n\nSPLITTERS_REQUIRING_TARGET = [\n    StratifiedKFold(),\n    StratifiedShuffleSplit(),\n    RepeatedStratifiedKFold(),\n]\n\nX = np.ones(10)\ny = np.arange(10) // 2\ntest_groups = (\n    np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]),\n    np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]),\n    np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]),\n    np.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]),\n    [1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3],\n    [\"1\", \"1\", \"1\", \"1\", \"2\", \"2\", \"2\", \"3\", \"3\", \"3\", \"3\", \"3\"],\n)\ndigits = load_digits()\n\npytestmark = pytest.mark.filterwarnings(\n    \"error:The groups parameter:UserWarning:sklearn.*\"\n)\n\n\ndef _split(splitter, X, y, groups):\n    if splitter.__class__.__name__ in GROUP_SPLITTER_NAMES:\n        return splitter.split(X, y, groups=groups)\n    else:\n        return splitter.split(X, y)\n\n\ndef test_cross_validator_with_default_params():\n    n_samples = 4\n    n_unique_groups = 4\n    n_splits = 2\n    p = 2\n    n_shuffle_splits = 10  # (the default value)\n\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    X_1d = np.array([1, 2, 3, 4])\n    y = np.array([1, 1, 2, 2])\n    groups = np.array([1, 2, 3, 4])\n    loo = LeaveOneOut()\n    lpo = LeavePOut(p)\n    kf = KFold(n_splits)\n    skf = StratifiedKFold(n_splits)\n    lolo = LeaveOneGroupOut()\n    lopo = LeavePGroupsOut(p)\n    ss = ShuffleSplit(random_state=0)\n    ps = PredefinedSplit([1, 1, 2, 2])  # n_splits = np of unique folds = 2\n    sgkf = StratifiedGroupKFold(n_splits)\n\n    loo_repr = \"LeaveOneOut()\"\n    lpo_repr = \"LeavePOut(p=2)\"\n    kf_repr = \"KFold(n_splits=2, random_state=None, shuffle=False)\"\n    skf_repr = \"StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\"\n    lolo_repr = \"LeaveOneGroupOut()\"\n    lopo_re"}, {"start_line": 25000, "end_line": 27000, "belongs_to": {"file_name": "_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rray([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> skf = StratifiedKFold(n_splits=2)\n    >>> skf.get_n_splits(X, y)\n    2\n    >>> print(skf)\n    StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n    >>> for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n    ...     print(f\"Fold {i}:\")\n    ...     print(f\"  Train: index={train_index}\")\n    ...     print(f\"  Test:  index={test_index}\")\n    Fold 0:\n      Train: index=[1 3]\n      Test:  index=[0 2]\n    Fold 1:\n      Train: index=[0 2]\n      Test:  index=[1 3]\n\n    Notes\n    -----\n    The implementation is designed to:\n\n    * Generate test sets such that all contain the same distribution of\n      classes, or as close as possible.\n    * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n      ``y = [1, 0]`` should not change the indices generated.\n    * Preserve order dependencies in the dataset ordering, when\n      ``shuffle=False``: all samples from class k in some test set were\n      contiguous in y, or separated in y by samples from classes other than k.\n    * Generate test sets where the smallest and largest differ by at most one\n      sample.\n\n    .. versionchanged:: 0.22\n        The previous implementation did not follow the last constraint.\n\n    See Also\n    --------\n    RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.\n    \"\"\"\n\n    def __init__(self, n_splits=5, *, shuffle=False, random_state=None):\n        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n\n    def _make_test_folds(self, X, y=None):\n        rng = check_random_state(self.random_state)\n        # XXX: as of now, cross-validation splitters only operate in NumPy-land\n        # without attempting to leverage array API namespace features. However\n        # they might be fed by array API inputs, e.g. in CV-enabled estimators so\n        # we need the following explicit conversion:\n        xp, is_array_api = get_namespace(y)\n        if is_arra"}, {"start_line": 98000, "end_line": 100000, "belongs_to": {"file_name": "_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  Examples\n    --------\n    >>> from sklearn.model_selection import check_cv\n    >>> check_cv(cv=5, y=None, classifier=False)\n    KFold(...)\n    >>> check_cv(cv=5, y=[1, 1, 0, 0, 0, 0], classifier=True)\n    StratifiedKFold(...)\n    \"\"\"\n    cv = 5 if cv is None else cv\n    if isinstance(cv, numbers.Integral):\n        if (\n            classifier\n            and (y is not None)\n            and (type_of_target(y, input_name=\"y\") in (\"binary\", \"multiclass\"))\n        ):\n            return StratifiedKFold(cv)\n        else:\n            return KFold(cv)\n\n    if not hasattr(cv, \"split\") or isinstance(cv, str):\n        if not isinstance(cv, Iterable) or isinstance(cv, str):\n            raise ValueError(\n                \"Expected cv as an integer, cross-validation \"\n                \"object (from sklearn.model_selection) \"\n                \"or an iterable. Got %s.\" % cv\n            )\n        return _CVIterableWrapper(cv)\n\n    return cv  # New style cv objects are passed without any modification\n\n\n@validate_params(\n    {\n        \"test_size\": [\n            Interval(RealNotInt, 0, 1, closed=\"neither\"),\n            Interval(numbers.Integral, 1, None, closed=\"left\"),\n            None,\n        ],\n        \"train_size\": [\n            Interval(RealNotInt, 0, 1, closed=\"neither\"),\n            Interval(numbers.Integral, 1, None, closed=\"left\"),\n            None,\n        ],\n        \"random_state\": [\"random_state\"],\n        \"shuffle\": [\"boolean\"],\n        \"stratify\": [\"array-like\", None],\n    },\n    prefer_skip_nested_validation=True,\n)\ndef train_test_split(\n    *arrays,\n    test_size=None,\n    train_size=None,\n    random_state=None,\n    shuffle=True,\n    stratify=None,\n):\n    \"\"\"Split arrays or matrices into random train and test subsets.\n\n    Quick utility that wraps input validation,\n    ``next(ShuffleSplit().split(X, y))``, and application to input data\n    into a single call for splitting (and optionally subsampling) data into a\n    one-liner.\n\n    Read more in the :ref:`User Guide <cros"}, {"start_line": 80000, "end_line": 82000, "belongs_to": {"file_name": "_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "es,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting `random_state`\n        to an integer.\n        \"\"\"\n        return super().split(X, y, groups)\n\n\nclass StratifiedShuffleSplit(BaseShuffleSplit):\n    \"\"\"Class-wise stratified ShuffleSplit cross-validator.\n\n    Provides train/test indices to split data in train/test sets.\n\n    This cross-validation object is a merge of :class:`StratifiedKFold` and\n    :class:`ShuffleSplit`, which returns stratified randomized folds. The folds\n    are made by preserving the percentage of samples for each class in `y` in a\n    binary or multiclass classification setting.\n\n    Note: like the :class:`ShuffleSplit` strategy, stratified random splits\n    do not guarantee that test sets across all folds will be mutually exclusive,\n    and might include overlapping samples. However, this is still very likely for\n    sizeable datasets.\n\n    Read more in the :ref:`User Guide <stratified_shuffle_split>`.\n\n    For visualisation of cross-validation behaviour and\n    comparison between common scikit-learn split methods\n    refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n\n    .. note::\n\n        Stratification on the class label solves an engineering problem rather\n        than a statistical one. See :ref:`stratification` for more details.\n\n    Parameters\n    ----------\n    n_splits : int, default=10\n        Number of re-shuffling & splitting iterations.\n\n    test_size : float or int, default=None\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test spl"}, {"start_line": 17000, "end_line": 19000, "belongs_to": {"file_name": "test_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rlap\n            assert len(np.intersect1d(tr_a, tr_b)) != len(tr1)\n\n        # Set all test indices in successive iterations of kf2 to 1\n        all_folds[te2] = 1\n\n    # Check that all indices are returned in the different test folds\n    assert sum(all_folds) == 300\n\n\n@pytest.mark.parametrize(\"kfold\", [KFold, StratifiedKFold, StratifiedGroupKFold])\ndef test_shuffle_kfold_stratifiedkfold_reproducibility(kfold):\n    X = np.ones(15)  # Divisible by 3\n    y = [0] * 7 + [1] * 8\n    groups_1 = np.arange(len(y))\n    X2 = np.ones(16)  # Not divisible by 3\n    y2 = [0] * 8 + [1] * 8\n    groups_2 = np.arange(len(y2))\n\n    # Check that when the shuffle is True, multiple split calls produce the\n    # same split when random_state is int\n    kf = kfold(3, shuffle=True, random_state=0)\n\n    np.testing.assert_equal(\n        list(_split(kf, X, y, groups_1)), list(_split(kf, X, y, groups_1))\n    )\n\n    # Check that when the shuffle is True, multiple split calls often\n    # (not always) produce different splits when random_state is\n    # RandomState instance or None\n    kf = kfold(3, shuffle=True, random_state=np.random.RandomState(0))\n    for data in zip((X, X2), (y, y2), (groups_1, groups_2)):\n        # Test if the two splits are different cv\n        for (_, test_a), (_, test_b) in zip(_split(kf, *data), _split(kf, *data)):\n            # cv.split(...) returns an array of tuples, each tuple\n            # consisting of an array with train indices and test indices\n            # Ensure that the splits for data are not same\n            # when random state is not set\n            with pytest.raises(AssertionError):\n                np.testing.assert_array_equal(test_a, test_b)\n\n\ndef test_shuffle_stratifiedkfold():\n    # Check that shuffling is happening when requested, and for proper\n    # sample coverage\n    X_40 = np.ones(40)\n    y = [0] * 20 + [1] * 20\n    kf0 = StratifiedKFold(5, shuffle=True, random_state=0)\n    kf1 = StratifiedKFold(5, shuffle=True, random_state=1)\n    for (_, test0)"}, {"start_line": 15000, "end_line": 17000, "belongs_to": {"file_name": "test_split.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "np.arange(len(y))\n\n    def get_splits(y):\n        random_state = None if not shuffle else 0\n        return [\n            (list(train), list(test))\n            for train, test in _split(\n                kfold(k, random_state=random_state, shuffle=shuffle),\n                X,\n                y,\n                groups=groups,\n            )\n        ]\n\n    splits_base = get_splits(y)\n    for perm in permutations([0, 1, 2]):\n        y_perm = np.take(perm, y)\n        splits_perm = get_splits(y_perm)\n        assert splits_perm == splits_base\n\n\ndef test_kfold_balance():\n    # Check that KFold returns folds with balanced sizes\n    for i in range(11, 17):\n        kf = KFold(5).split(X=np.ones(i))\n        sizes = [len(test) for _, test in kf]\n\n        assert (np.max(sizes) - np.min(sizes)) <= 1\n        assert np.sum(sizes) == i\n\n\n@pytest.mark.parametrize(\"kfold\", [StratifiedKFold, StratifiedGroupKFold])\ndef test_stratifiedkfold_balance(kfold):\n    # Check that KFold returns folds with balanced sizes (only when\n    # stratification is possible)\n    # Repeat with shuffling turned off and on\n    X = np.ones(17)\n    y = [0] * 3 + [1] * 14\n    # ensure perfect stratification with StratifiedGroupKFold\n    groups = np.arange(len(y))\n\n    for shuffle in (True, False):\n        cv = kfold(3, shuffle=shuffle)\n        for i in range(11, 17):\n            skf = _split(cv, X[:i], y[:i], groups[:i])\n            sizes = [len(test) for _, test in skf]\n\n            assert (np.max(sizes) - np.min(sizes)) <= 1\n            assert np.sum(sizes) == i\n\n\ndef test_shuffle_kfold():\n    # Check the indices are shuffled properly\n    kf = KFold(3)\n    kf2 = KFold(3, shuffle=True, random_state=0)\n    kf3 = KFold(3, shuffle=True, random_state=1)\n\n    X = np.ones(300)\n\n    all_folds = np.zeros(300)\n    for (tr1, te1), (tr2, te2), (tr3, te3) in zip(\n        kf.split(X), kf2.split(X), kf3.split(X)\n    ):\n        for tr_a, tr_b in combinations((tr1, tr2, tr3), 2):\n            # Assert that there is no complete ove"}], "retrieved_count": 10, "cost_time": 0.35242486000061035}
{"question": "How does Scikit-learn support integration with popular deep learning frameworks like TensorFlow and PyTorch?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn supports integration with popular deep learning frameworks like TensorFlow and PyTorch through several mechanisms, though it doesn't directly implement deep learning capabilities. Here's how scikit-learn facilitates integration with these frameworks:\n\n1. **Array API Support for GPU Computing**:\n   - **PyTorch Tensor Support**: Direct support for PyTorch tensors as input data\n   - **CuPy Integration**: Support for CuPy arrays for GPU acceleration\n   - **Array API Standard**: Implements Array API specification for cross-framework compatibility\n   - **GPU Acceleration**: Enables GPU-based computation for compatible estimators\n   - **Automatic Dispatch**: Automatically dispatches operations to appropriate array libraries\n\n2. **Third-Party Wrapper Libraries**:\n   - **skorch**: Scikit-learn compatible neural network library that wraps PyTorch\n   - **scikeras**: Wrapper around Keras to interface with scikit-learn\n   - **TensorFlow Integration**: Support through Keras wrappers and ONNX export\n   - **PyTorch Integration**: Direct integration through skorch and Array API\n   - **API Compatibility**: Maintains scikit-learn's fit/predict interface\n\n3. **Model Export and Interoperability**:\n   - **ONNX Export**: Export scikit-learn models to ONNX format for TensorFlow/PyTorch\n   - **sklearn-onnx**: Serialization of pipelines to ONNX for framework interoperability\n   - **Model Conversion**: Convert between scikit-learn and deep learning model formats\n   - **Production Deployment**: Deploy scikit-learn models in deep learning environments\n   - **Cross-Framework Prediction**: Use scikit-learn models in TensorFlow/PyTorch pipelines\n\n4. **Pipeline Integration**:\n   - **Hybrid Pipelines**: Combine scikit-learn preprocessing with deep learning models\n   - **Feature Engineering**: Use scikit-learn transformers with deep learning frameworks\n   - **Ensemble Methods**: Combine scikit-learn and deep learning models\n   - **Cross-Validation**: Use scikit-learn CV with deep learning models\n   - **Hyperparameter Tuning**: Apply scikit-learn search methods to deep learning models\n\n5. **Data Preprocessing Integration**:\n   - **Feature Scaling**: Use scikit-learn scalers with deep learning frameworks\n   - **Categorical Encoding**: Apply scikit-learn encoders to deep learning data\n   - **Dimensionality Reduction**: Use scikit-learn methods for feature reduction\n   - **Data Validation**: Leverage scikit-learn's validation utilities\n   - **Pipeline Preprocessing**: Apply scikit-learn preprocessing in deep learning workflows\n\n6. **Evaluation and Metrics**:\n   - **Cross-Framework Evaluation**: Use scikit-learn metrics with deep learning models\n   - **Model Comparison**: Compare scikit-learn and deep learning model performance\n   - **Statistical Testing**: Apply scikit-learn's statistical testing capabilities\n   - **Visualization**: Use scikit-learn's plotting utilities for deep learning results\n   - **Performance Analysis**: Leverage scikit-learn's analysis tools\n\n7. **Experimental Features**:\n   - **Array API Dispatch**: Experimental support for automatic array library dispatch\n   - **GPU Support**: Limited GPU support through Array API compatible libraries\n   - **Memory Optimization**: Efficient memory usage across frameworks\n   - **Parallel Processing**: Support for parallel computation across frameworks\n   - **Performance Optimization**: Optimized performance for cross-framework workflows\n\n8. **Development and Testing**:\n   - **Testing Infrastructure**: Support for testing deep learning integrations\n   - **Continuous Integration**: CI/CD support for deep learning framework compatibility\n   - **Documentation**: Comprehensive documentation for integration patterns\n   - **Examples**: Code examples showing integration approaches\n   - **Best Practices**: Guidelines for effective integration\n\n9. **Limitations and Considerations**:\n   - **No Native Deep Learning**: Scikit-learn doesn't implement deep learning algorithms\n   - **GPU Limitations**: Limited GPU support compared to dedicated deep learning frameworks\n   - **Performance Trade-offs**: May have performance overhead in cross-framework workflows\n   - **Compatibility Issues**: Some advanced features may not be fully compatible\n   - **Maintenance Overhead**: Integration requires additional maintenance and testing\n\n10. **Best Practices for Integration**:\n    - **Use Wrapper Libraries**: Leverage skorch, scikeras for seamless integration\n    - **ONNX for Production**: Use ONNX for production deployment across frameworks\n    - **Pipeline Design**: Design pipelines that separate preprocessing from modeling\n    - **Performance Testing**: Test performance implications of cross-framework workflows\n    - **Version Compatibility**: Ensure compatible versions of all frameworks", "score": null, "retrieved_content": [{"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "instance_generator.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/_test_common", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "sklearn.kernel_approximation import (\n    Nystroem,\n    PolynomialCountSketch,\n    RBFSampler,\n    SkewedChi2Sampler,\n)\nfrom sklearn.linear_model import (\n    ARDRegression,\n    BayesianRidge,\n    ElasticNet,\n    ElasticNetCV,\n    GammaRegressor,\n    HuberRegressor,\n    LarsCV,\n    Lasso,\n    LassoCV,\n    LassoLars,\n    LassoLarsCV,\n    LassoLarsIC,\n    LinearRegression,\n    LogisticRegression,\n    LogisticRegressionCV,\n    MultiTaskElasticNet,\n    MultiTaskElasticNetCV,\n    MultiTaskLasso,\n    MultiTaskLassoCV,\n    OrthogonalMatchingPursuitCV,\n    PassiveAggressiveClassifier,\n    PassiveAggressiveRegressor,\n    Perceptron,\n    PoissonRegressor,\n    QuantileRegressor,\n    RANSACRegressor,\n    Ridge,\n    RidgeClassifier,\n    SGDClassifier,\n    SGDOneClassSVM,\n    SGDRegressor,\n    TheilSenRegressor,\n    TweedieRegressor,\n)\nfrom sklearn.manifold import (\n    MDS,\n    TSNE,\n    Isomap,\n    LocallyLinearEmbedding,\n    SpectralEmbedding,\n)\nfrom sklearn.mixture import BayesianGaussianMixture, GaussianMixture\nfrom sklearn.model_selection import (\n    FixedThresholdClassifier,\n    GridSearchCV,\n    HalvingGridSearchCV,\n    HalvingRandomSearchCV,\n    RandomizedSearchCV,\n    TunedThresholdClassifierCV,\n)\nfrom sklearn.multiclass import (\n    OneVsOneClassifier,\n    OneVsRestClassifier,\n    OutputCodeClassifier,\n)\nfrom sklearn.multioutput import (\n    ClassifierChain,\n    MultiOutputClassifier,\n    MultiOutputRegressor,\n    RegressorChain,\n)\nfrom sklearn.neighbors import (\n    KernelDensity,\n    KNeighborsClassifier,\n    KNeighborsRegressor,\n    KNeighborsTransformer,\n    NeighborhoodComponentsAnalysis,\n    RadiusNeighborsTransformer,\n)\nfrom sklearn.neural_network import BernoulliRBM, MLPClassifier, MLPRegressor\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.preprocessing import (\n    KBinsDiscretizer,\n    OneHotEncoder,\n    SplineTransformer,\n    StandardScaler,\n    TargetEncoder,\n)\nfrom sklearn.random_projection import GaussianRandomProjection, SparseRandomP"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "instance_generator.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/_test_common", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  MiniBatchSparsePCA,\n    SparseCoder,\n    SparsePCA,\n    TruncatedSVD,\n)\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.ensemble import (\n    AdaBoostClassifier,\n    AdaBoostRegressor,\n    BaggingClassifier,\n    BaggingRegressor,\n    ExtraTreesClassifier,\n    ExtraTreesRegressor,\n    GradientBoostingClassifier,\n    GradientBoostingRegressor,\n    HistGradientBoostingClassifier,\n    HistGradientBoostingRegressor,\n    IsolationForest,\n    RandomForestClassifier,\n    RandomForestRegressor,\n    RandomTreesEmbedding,\n    StackingClassifier,\n    StackingRegressor,\n    VotingClassifier,\n    VotingRegressor,\n)\nfrom sklearn.exceptions import SkipTestWarning\nfrom sklearn.experimental import enable_halving_search_cv  # noqa: F401\nfrom sklearn.feature_selection import (\n    RFE,\n    RFECV,\n    SelectFdr,\n    SelectFromModel,\n    SelectKBest,\n    SequentialFeatureSelector,\n)\nfrom sklearn.frozen import FrozenEstimator\nfrom sklearn.kernel_approximation import (\n    Nystroem,\n    PolynomialCountSketch,\n    RBFSampler,\n    SkewedChi2Sampler,\n)\nfrom sklearn.linear_model import (\n    ARDRegression,\n    BayesianRidge,\n    ElasticNet,\n    ElasticNetCV,\n    GammaRegressor,\n    HuberRegressor,\n    LarsCV,\n    Lasso,\n    LassoCV,\n    LassoLars,\n    LassoLarsCV,\n    LassoLarsIC,\n    LinearRegression,\n    LogisticRegression,\n    LogisticRegressionCV,\n    MultiTaskElasticNet,\n    MultiTaskElasticNetCV,\n    MultiTaskLasso,\n    MultiTaskLassoCV,\n    OrthogonalMatchingPursuitCV,\n    PassiveAggressiveClassifier,\n    PassiveAggressiveRegressor,\n    Perceptron,\n    PoissonRegressor,\n    QuantileRegressor,\n    RANSACRegressor,\n    Ridge,\n    RidgeClassifier,\n    SGDClassifier,\n    SGDOneClassSVM,\n    SGDRegressor,\n    TheilSenRegressor,\n    TweedieRegressor,\n)\nfrom sklearn.manifold import (\n    MDS,\n    TSNE,\n    Isomap,\n    LocallyLinearEmbedding,\n    SpectralEmbedding,\n)\nfrom sklearn.mixture import BayesianGaussianMixture"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "instance_generator.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/_test_common", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ", GaussianMixture\nfrom sklearn.model_selection import (\n    FixedThresholdClassifier,\n    GridSearchCV,\n    HalvingGridSearchCV,\n    HalvingRandomSearchCV,\n    RandomizedSearchCV,\n    TunedThresholdClassifierCV,\n)\nfrom sklearn.multiclass import (\n    OneVsOneClassifier,\n    OneVsRestClassifier,\n    OutputCodeClassifier,\n)\nfrom sklearn.multioutput import (\n    ClassifierChain,\n    MultiOutputClassifier,\n    MultiOutputRegressor,\n    RegressorChain,\n)\nfrom sklearn.neighbors import (\n    KernelDensity,\n    KNeighborsClassifier,\n    KNeighborsRegressor,\n    KNeighborsTransformer,\n    NeighborhoodComponentsAnalysis,\n    RadiusNeighborsTransformer,\n)\nfrom sklearn.neural_network import BernoulliRBM, MLPClassifier, MLPRegressor\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.preprocessing import (\n    KBinsDiscretizer,\n    OneHotEncoder,\n    SplineTransformer,\n    StandardScaler,\n    TargetEncoder,\n)\nfrom sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\nfrom sklearn.semi_supervised import (\n    LabelPropagation,\n    LabelSpreading,\n    SelfTrainingClassifier,\n)\nfrom sklearn.svm import SVC, SVR, LinearSVC, LinearSVR, NuSVC, NuSVR, OneClassSVM\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.utils import all_estimators\nfrom sklearn.utils._tags import get_tags\nfrom sklearn.utils._testing import SkipTest\nfrom sklearn.utils.fixes import _IS_32BIT, parse_version, sp_base_version\n\nCROSS_DECOMPOSITION = [\"PLSCanonical\", \"PLSRegression\", \"CCA\", \"PLSSVD\"]\n\n# The following dictionary is to indicate constructor arguments suitable for the test\n# suite, which uses very small datasets, and is intended to run rather quickly.\nINIT_PARAMS = {\n    AdaBoostClassifier: dict(n_estimators=5),\n    AdaBoostRegressor: dict(n_estimators=5),\n    AffinityPropagation: dict(max_iter=5),\n    AgglomerativeClustering: dict(n_clusters=2),\n    ARDRegression: dict(max_iter=5),\n    BaggingClassifier: dict(n_estimators=5),\n    B"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "instance_generator.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils/_test_common", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\n\nimport re\nimport warnings\nfrom contextlib import suppress\nfrom functools import partial\nfrom inspect import isfunction\n\nfrom sklearn import clone, config_context\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.cluster import (\n    HDBSCAN,\n    AffinityPropagation,\n    AgglomerativeClustering,\n    Birch,\n    BisectingKMeans,\n    FeatureAgglomeration,\n    KMeans,\n    MeanShift,\n    MiniBatchKMeans,\n    SpectralBiclustering,\n    SpectralClustering,\n    SpectralCoclustering,\n)\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.covariance import GraphicalLasso, GraphicalLassoCV\nfrom sklearn.cross_decomposition import CCA, PLSSVD, PLSCanonical, PLSRegression\nfrom sklearn.decomposition import (\n    NMF,\n    PCA,\n    DictionaryLearning,\n    FactorAnalysis,\n    FastICA,\n    IncrementalPCA,\n    KernelPCA,\n    LatentDirichletAllocation,\n    MiniBatchDictionaryLearning,\n    MiniBatchNMF,\n    MiniBatchSparsePCA,\n    SparseCoder,\n    SparsePCA,\n    TruncatedSVD,\n)\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.ensemble import (\n    AdaBoostClassifier,\n    AdaBoostRegressor,\n    BaggingClassifier,\n    BaggingRegressor,\n    ExtraTreesClassifier,\n    ExtraTreesRegressor,\n    GradientBoostingClassifier,\n    GradientBoostingRegressor,\n    HistGradientBoostingClassifier,\n    HistGradientBoostingRegressor,\n    IsolationForest,\n    RandomForestClassifier,\n    RandomForestRegressor,\n    RandomTreesEmbedding,\n    StackingClassifier,\n    StackingRegressor,\n    VotingClassifier,\n    VotingRegressor,\n)\nfrom sklearn.exceptions import SkipTestWarning\nfrom sklearn.experimental import enable_halving_search_cv  # noqa: F401\nfrom sklearn.feature_selection import (\n    RFE,\n    RFECV,\n    SelectFdr,\n    SelectFromModel,\n    SelectKBest,\n    SequentialFeatureSelector,\n)\nfrom sklearn.frozen import FrozenEstimator\nfrom "}, {"start_line": 3000, "end_line": 4836, "belongs_to": {"file_name": "plot_release_highlights_1_7_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "alidate`.\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import RocCurveDisplay\nfrom sklearn.model_selection import cross_validate\n\nX, y = make_classification(n_samples=150, random_state=0)\nclf = LogisticRegression(random_state=0)\ncv_results = cross_validate(clf, X, y, cv=5, return_estimator=True, return_indices=True)\n_ = RocCurveDisplay.from_cv_results(cv_results, X, y)\n\n# %%\n# Array API support\n# -----------------\n# Several functions have been updated to support array API compatible inputs since\n# version 1.6, especially metrics from the :mod:`sklearn.metrics` module.\n#\n# In addition, it is no longer required to install the `array-api-compat` package to use\n# the experimental array API support in scikit-learn.\n#\n# Please refer to the :ref:`array API support<array_api>` page for instructions to use\n# scikit-learn with array API compatible libraries such as PyTorch or CuPy.\n\n# %%\n# Improved API consistency of Multi-layer Perceptron\n# --------------------------------------------------\n# The :class:`neural_network.MLPRegressor` has a new parameter `loss` and now supports\n# the \"poisson\" loss in addition to the default \"squared_error\" loss.\n# Moreover, the :class:`neural_network.MLPClassifier` and\n# :class:`neural_network.MLPRegressor` estimators now support sample weights.\n# These improvements have been made to improve the consistency of these estimators\n# with regard to the other estimators in scikit-learn.\n\n# %%\n# Migration toward sparse arrays\n# ------------------------------\n# In order to prepare `SciPy migration from sparse matrices to sparse arrays <https://docs.scipy.org/doc/scipy/reference/sparse.migration_to_sparray.html>`_,\n# all scikit-learn estimators that accept sparse matrices as input now also accept\n# sparse arrays.\n"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_forest.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/ensemble/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ined_variance_score,\n    f1_score,\n    mean_poisson_deviance,\n    mean_squared_error,\n)\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree._classes import SPARSE_SPLITTERS\nfrom sklearn.utils._testing import (\n    _convert_container,\n    assert_allclose,\n    assert_almost_equal,\n    assert_array_almost_equal,\n    assert_array_equal,\n    ignore_warnings,\n    skip_if_no_parallel,\n)\nfrom sklearn.utils.fixes import COO_CONTAINERS, CSC_CONTAINERS, CSR_CONTAINERS\nfrom sklearn.utils.multiclass import type_of_target\nfrom sklearn.utils.parallel import Parallel\nfrom sklearn.utils.validation import check_random_state\n\n# toy sample\nX = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\ny = [-1, -1, -1, 1, 1, 1]\nT = [[-1, -1], [2, 2], [3, 2]]\ntrue_result = [-1, 1, 1]\n\n# Larger classification sample used for testing feature importances\nX_large, y_large = datasets.make_classification(\n    n_samples=500,\n    n_features=10,\n    n_informative=3,\n    n_redundant=0,\n    n_repeated=0,\n    shuffle=False,\n    random_state=0,\n)\n\n# also load the iris dataset\n# and randomly permute it\niris = datasets.load_iris()\nrng = check_random_state(0)\nperm = rng.permutation(iris.target.size)\niris.data = iris.data[perm]\niris.target = iris.target[perm]\n\n# Make regression dataset\nX_reg, y_reg = datasets.make_regression(n_samples=500, n_features=10, random_state=1)\n\n# also make a hastie_10_2 dataset\nhastie_X, hastie_y = datasets.make_hastie_10_2(n_samples=20, random_state=1)\nhastie_X = hastie_X.astype(np.float32)\n\n# Get the default backend in joblib to test parallelism and interaction with\n# different backends\nDEFAULT_JOBLIB_BACKEND = joblib.parallel.get_active_backend()[0].__class__\n\nFOREST_CLASSIFIERS = {\n    \"ExtraTreesClassifier\": ExtraTreesClassifier,\n    \"RandomForestClassifier\": RandomForestClassifier,\n}\n\nFOREST_REGRESSORS = {\n    \"ExtraTreesRegressor\": ExtraTreesRegressor,\n    \"RandomForestRegressor\": RandomFores"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "plot_release_highlights_1_6_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ray API support\n# -----------------\n#\n# Many more estimators and functions have been updated to support array API compatible\n# inputs since version 1.5, in particular the meta-estimators for hyperparameter tuning\n# from the :mod:`sklearn.model_selection` module and the metrics from the\n# :mod:`sklearn.metrics` module.\n#\n# Please refer to the :ref:`array API support<array_api>` page for instructions to use\n# scikit-learn with array API compatible libraries such as PyTorch or CuPy.\n\n# %%\n# Almost complete Metadata Routing support\n# ----------------------------------------\n#\n# Support for routing metadata has been added to all remaining estimators and\n# functions except AdaBoost. See :ref:`Metadata Routing User Guide <metadata_routing>`\n# for more details.\n\n# %%\n# Free-threaded CPython 3.13 support\n# ----------------------------------\n#\n# scikit-learn has preliminary support for free-threaded CPython, in particular\n# free-threaded wheels are available for all of our supported platforms.\n#\n# Free-threaded (also known as nogil) CPython 3.13 is an experimental version of\n# CPython 3.13 which aims at enabling efficient multi-threaded use cases by\n# removing the Global Interpreter Lock (GIL).\n#\n# For more details about free-threaded CPython see `py-free-threading doc <https://py-free-threading.github.io>`_,\n# in particular `how to install a free-threaded CPython <https://py-free-threading.github.io/installing_cpython/>`_\n# and `Ecosystem compatibility tracking <https://py-free-threading.github.io/tracking/>`_.\n#\n# Feel free to try free-threaded CPython on your use case and report any issues!\n\n# %%\n# Improvements to the developer API for third party libraries\n# -----------------------------------------------------------\n#\n# We have been working on improving the developer API for third party libraries.\n# This is still a work in progress, but a fair amount of work has been done in this\n# release. This release includes:\n#\n# - :func:`sklearn.utils.validation.validate_data` is in"}, {"start_line": 36000, "end_line": 38000, "belongs_to": {"file_name": "api_reference.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/doc", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "          \"KDTree\",\n                    \"KNeighborsClassifier\",\n                    \"KNeighborsRegressor\",\n                    \"KNeighborsTransformer\",\n                    \"KernelDensity\",\n                    \"LocalOutlierFactor\",\n                    \"NearestCentroid\",\n                    \"NearestNeighbors\",\n                    \"NeighborhoodComponentsAnalysis\",\n                    \"RadiusNeighborsClassifier\",\n                    \"RadiusNeighborsRegressor\",\n                    \"RadiusNeighborsTransformer\",\n                    \"kneighbors_graph\",\n                    \"radius_neighbors_graph\",\n                    \"sort_graph_by_row_values\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.neural_network\": {\n        \"short_summary\": \"Neural network models.\",\n        \"description\": _get_guide(\n            \"neural_networks_supervised\", \"neural_networks_unsupervised\"\n        ),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\"BernoulliRBM\", \"MLPClassifier\", \"MLPRegressor\"],\n            },\n        ],\n    },\n    \"sklearn.pipeline\": {\n        \"short_summary\": \"Pipeline.\",\n        \"description\": _get_guide(\"combining_estimators\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"FeatureUnion\",\n                    \"Pipeline\",\n                    \"make_pipeline\",\n                    \"make_union\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.preprocessing\": {\n        \"short_summary\": \"Preprocessing and normalization.\",\n        \"description\": _get_guide(\"preprocessing\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"Binarizer\",\n                    \"FunctionTransformer\",\n                    \"KBinsDiscretizer\",\n                    \"KernelCenterer\",\n                    \"LabelBinarizer\",\n                    \"LabelEncoder\",\n                    \"MaxAbsScaler"}, {"start_line": 37000, "end_line": 39000, "belongs_to": {"file_name": "api_reference.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/doc", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ernoulliRBM\", \"MLPClassifier\", \"MLPRegressor\"],\n            },\n        ],\n    },\n    \"sklearn.pipeline\": {\n        \"short_summary\": \"Pipeline.\",\n        \"description\": _get_guide(\"combining_estimators\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"FeatureUnion\",\n                    \"Pipeline\",\n                    \"make_pipeline\",\n                    \"make_union\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.preprocessing\": {\n        \"short_summary\": \"Preprocessing and normalization.\",\n        \"description\": _get_guide(\"preprocessing\"),\n        \"sections\": [\n            {\n                \"title\": None,\n                \"autosummary\": [\n                    \"Binarizer\",\n                    \"FunctionTransformer\",\n                    \"KBinsDiscretizer\",\n                    \"KernelCenterer\",\n                    \"LabelBinarizer\",\n                    \"LabelEncoder\",\n                    \"MaxAbsScaler\",\n                    \"MinMaxScaler\",\n                    \"MultiLabelBinarizer\",\n                    \"Normalizer\",\n                    \"OneHotEncoder\",\n                    \"OrdinalEncoder\",\n                    \"PolynomialFeatures\",\n                    \"PowerTransformer\",\n                    \"QuantileTransformer\",\n                    \"RobustScaler\",\n                    \"SplineTransformer\",\n                    \"StandardScaler\",\n                    \"TargetEncoder\",\n                    \"add_dummy_feature\",\n                    \"binarize\",\n                    \"label_binarize\",\n                    \"maxabs_scale\",\n                    \"minmax_scale\",\n                    \"normalize\",\n                    \"power_transform\",\n                    \"quantile_transform\",\n                    \"robust_scale\",\n                    \"scale\",\n                ],\n            },\n        ],\n    },\n    \"sklearn.random_projection\": {\n        \"short_summary\": \"Random projection.\",\n        \"description\": _get_guide(\"r"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "estimator_checks.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Various utilities to check the compatibility of estimators with scikit-learn API.\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\nfrom __future__ import annotations\n\nimport pickle\nimport re\nimport textwrap\nimport warnings\nfrom contextlib import nullcontext\nfrom copy import deepcopy\nfrom functools import partial, wraps\nfrom inspect import signature\nfrom numbers import Integral, Real\nfrom typing import Callable, Literal\n\nimport joblib\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.stats import rankdata\n\nfrom sklearn import config_context\nfrom sklearn.base import (\n    BaseEstimator,\n    BiclusterMixin,\n    ClassifierMixin,\n    ClassNamePrefixFeaturesOutMixin,\n    ClusterMixin,\n    DensityMixin,\n    MetaEstimatorMixin,\n    MultiOutputMixin,\n    OneToOneFeatureMixin,\n    OutlierMixin,\n    RegressorMixin,\n    TransformerMixin,\n    clone,\n    is_classifier,\n    is_outlier_detector,\n    is_regressor,\n)\nfrom sklearn.datasets import (\n    load_iris,\n    make_blobs,\n    make_classification,\n    make_multilabel_classification,\n    make_regression,\n)\nfrom sklearn.exceptions import (\n    DataConversionWarning,\n    EstimatorCheckFailedWarning,\n    NotFittedError,\n    SkipTestWarning,\n)\nfrom sklearn.linear_model._base import LinearClassifierMixin\nfrom sklearn.metrics import accuracy_score, adjusted_rand_score, f1_score\nfrom sklearn.metrics.pairwise import linear_kernel, pairwise_distances, rbf_kernel\nfrom sklearn.model_selection import LeaveOneGroupOut, ShuffleSplit, train_test_split\nfrom sklearn.model_selection._validation import _safe_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, scale\nfrom sklearn.utils import _safe_indexing, shuffle\nfrom sklearn.utils._array_api import (\n    _atol_for_type,\n    _convert_to_numpy,\n    get_namespace,\n    yield_namespace_device_dtype_combinations,\n)\nfrom sklearn.utils._array_api import device as array_device\nfrom sklearn.utils._missing import is_sca"}], "retrieved_count": 10, "cost_time": 0.3414492607116699}
{"question": "How does Scikit-learn facilitate integration with visualization libraries like Matplotlib and Seaborn for model evaluation?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn facilitates integration with visualization libraries like Matplotlib and Seaborn through a comprehensive visualization API that provides standardized plotting utilities for model evaluation. Here's how scikit-learn enables seamless integration with these visualization libraries:\n\n1. **Display Object API**:\n   - **Standardized Interface**: Provides Display classes for consistent visualization\n   - **from_estimator() Method**: Creates visualizations directly from fitted estimators\n   - **from_predictions() Method**: Creates visualizations from prediction results\n   - **Reusable Objects**: Display objects store computed values for efficient plotting\n   - **Matplotlib Integration**: All Display objects return matplotlib-compatible plots\n\n2. **Built-in Visualization Utilities**:\n   - **RocCurveDisplay**: ROC curve visualization for classification models\n   - **ConfusionMatrixDisplay**: Confusion matrix plotting with customization options\n   - **PredictionErrorDisplay**: Regression prediction error visualization\n   - **PartialDependenceDisplay**: Partial dependence plots for feature analysis\n   - **DecisionBoundaryDisplay**: Decision boundary visualization for classifiers\n\n3. **Matplotlib Integration**:\n   - **Axes Support**: All plotting functions accept matplotlib Axes objects\n   - **Figure Management**: Automatic figure creation or use of existing figures\n   - **Styling Options**: Full access to matplotlib styling and customization\n   - **Subplot Integration**: Easy integration with complex subplot layouts\n   - **Export Capabilities**: Standard matplotlib export formats (PNG, PDF, SVG)\n\n4. **Seaborn Compatibility**:\n   - **DataFrame Output**: Many functions return pandas DataFrames for seaborn plotting\n   - **Statistical Plots**: Integration with seaborn's statistical visualization functions\n   - **Heatmap Support**: Direct support for seaborn heatmaps with scikit-learn data\n   - **Color Palette Integration**: Compatible with seaborn's color palettes\n   - **Joint Plot Support**: Integration with seaborn's joint plotting capabilities\n\n5. **Model Evaluation Visualizations**:\n   - **Cross-Validation Results**: Plotting CV scores and performance metrics\n   - **Learning Curves**: Visualization of training and validation learning curves\n   - **Validation Curves**: Hyperparameter tuning visualization\n   - **Grid Search Results**: Statistical comparison of model performance\n   - **Prediction Analysis**: Actual vs predicted plots for regression models\n\n6. **Feature Analysis Visualizations**:\n   - **Feature Importance**: Bar plots and heatmaps for feature importance\n   - **Partial Dependence**: Individual and joint partial dependence plots\n   - **Permutation Importance**: Visualization of permutation-based importance\n   - **Feature Correlations**: Correlation matrix heatmaps\n   - **Dimensionality Reduction**: PCA and manifold learning visualizations\n\n7. **Classification Visualizations**:\n   - **ROC Curves**: Single and multi-class ROC curve plotting\n   - **Precision-Recall Curves**: Precision-recall visualization\n   - **Confusion Matrices**: Detailed confusion matrix plotting\n   - **Decision Boundaries**: 2D decision boundary visualization\n   - **Calibration Plots**: Model calibration assessment\n\n8. **Regression Visualizations**:\n   - **Prediction Error Plots**: Actual vs predicted scatter plots\n   - **Residual Analysis**: Residual vs predicted plots\n   - **Residual Distribution**: Histogram and Q-Q plots of residuals\n   - **Model Comparison**: Side-by-side model performance comparison\n   - **Feature Effects**: Individual feature effect visualization\n\n9. **Clustering Visualizations**:\n   - **Cluster Assignment**: Scatter plots with cluster colors\n   - **Silhouette Analysis**: Silhouette coefficient visualization\n   - **Elbow Method**: Inertia plots for optimal cluster number selection\n   - **Hierarchical Clustering**: Dendrogram visualization\n   - **Cluster Centers**: Visualization of cluster centroids\n\n10. **Advanced Integration Features**:\n    - **Custom Plotting**: Easy extension for custom visualization needs\n    - **Interactive Plots**: Support for interactive plotting libraries\n    - **Export Functions**: Built-in functions for saving plots in various formats\n    - **Style Consistency**: Consistent styling across all scikit-learn plots\n    - **Documentation Examples**: Comprehensive examples showing integration patterns", "score": null, "retrieved_content": [{"start_line": 23000, "end_line": 25000, "belongs_to": {"file_name": "_plot.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "s=train_scores, test_scores=test_scores, score_name=\"Score\"\n    ... )\n    >>> display.plot()\n    <...>\n    >>> plt.show()\n    \"\"\"\n\n    def __init__(\n        self, *, param_name, param_range, train_scores, test_scores, score_name=None\n    ):\n        self.param_name = param_name\n        self.param_range = param_range\n        self.train_scores = train_scores\n        self.test_scores = test_scores\n        self.score_name = score_name\n\n    def plot(\n        self,\n        ax=None,\n        *,\n        negate_score=False,\n        score_name=None,\n        score_type=\"both\",\n        std_display_style=\"fill_between\",\n        line_kw=None,\n        fill_between_kw=None,\n        errorbar_kw=None,\n    ):\n        \"\"\"Plot visualization.\n\n        Parameters\n        ----------\n        ax : matplotlib Axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        negate_score : bool, default=False\n            Whether or not to negate the scores obtained through\n            :func:`~sklearn.model_selection.validation_curve`. This is\n            particularly useful when using the error denoted by `neg_*` in\n            `scikit-learn`.\n\n        score_name : str, default=None\n            The name of the score used to decorate the y-axis of the plot. It will\n            override the name inferred from the `scoring` parameter. If `score` is\n            `None`, we use `\"Score\"` if `negate_score` is `False` and `\"Negative score\"`\n            otherwise. If `scoring` is a string or a callable, we infer the name. We\n            replace `_` by spaces and capitalize the first letter. We remove `neg_` and\n            replace it by `\"Negative\"` if `negate_score` is\n            `False` or just remove it otherwise.\n\n        score_type : {\"test\", \"train\", \"both\"}, default=\"both\"\n            The type of score to plot. Can be one of `\"test\"`, `\"train\"`, or\n            `\"both\"`.\n\n        std_display_style : {\"errorbar\", \"fill_between\"} or None, defaul"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "_plot.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n_scores, test_scores = learning_curve(\n    ...     tree, X, y)\n    >>> display = LearningCurveDisplay(train_sizes=train_sizes,\n    ...     train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\n    >>> display.plot()\n    <...>\n    >>> plt.show()\n    \"\"\"\n\n    def __init__(self, *, train_sizes, train_scores, test_scores, score_name=None):\n        self.train_sizes = train_sizes\n        self.train_scores = train_scores\n        self.test_scores = test_scores\n        self.score_name = score_name\n\n    def plot(\n        self,\n        ax=None,\n        *,\n        negate_score=False,\n        score_name=None,\n        score_type=\"both\",\n        std_display_style=\"fill_between\",\n        line_kw=None,\n        fill_between_kw=None,\n        errorbar_kw=None,\n    ):\n        \"\"\"Plot visualization.\n\n        Parameters\n        ----------\n        ax : matplotlib Axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        negate_score : bool, default=False\n            Whether or not to negate the scores obtained through\n            :func:`~sklearn.model_selection.learning_curve`. This is\n            particularly useful when using the error denoted by `neg_*` in\n            `scikit-learn`.\n\n        score_name : str, default=None\n            The name of the score used to decorate the y-axis of the plot. It will\n            override the name inferred from the `scoring` parameter. If `score` is\n            `None`, we use `\"Score\"` if `negate_score` is `False` and `\"Negative score\"`\n            otherwise. If `scoring` is a string or a callable, we infer the name. We\n            replace `_` by spaces and capitalize the first letter. We remove `neg_` and\n            replace it by `\"Negative\"` if `negate_score` is\n            `False` or just remove it otherwise.\n\n        score_type : {\"test\", \"train\", \"both\"}, default=\"both\"\n            The type of score to plot. Can be one of `\"test\"`, `\"train\"`, or\n            `\"both"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "decision_boundary.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/inspection/_plot", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "sklearn.inspection import DecisionBoundaryDisplay\n    >>> from sklearn.tree import DecisionTreeClassifier\n    >>> iris = load_iris()\n    >>> feature_1, feature_2 = np.meshgrid(\n    ...     np.linspace(iris.data[:, 0].min(), iris.data[:, 0].max()),\n    ...     np.linspace(iris.data[:, 1].min(), iris.data[:, 1].max())\n    ... )\n    >>> grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n    >>> tree = DecisionTreeClassifier().fit(iris.data[:, :2], iris.target)\n    >>> y_pred = np.reshape(tree.predict(grid), feature_1.shape)\n    >>> display = DecisionBoundaryDisplay(\n    ...     xx0=feature_1, xx1=feature_2, response=y_pred\n    ... )\n    >>> display.plot()\n    <...>\n    >>> display.ax_.scatter(\n    ...     iris.data[:, 0], iris.data[:, 1], c=iris.target, edgecolor=\"black\"\n    ... )\n    <...>\n    >>> plt.show()\n    \"\"\"\n\n    def __init__(\n        self, *, xx0, xx1, response, multiclass_colors=None, xlabel=None, ylabel=None\n    ):\n        self.xx0 = xx0\n        self.xx1 = xx1\n        self.response = response\n        self.multiclass_colors = multiclass_colors\n        self.xlabel = xlabel\n        self.ylabel = ylabel\n\n    def plot(self, plot_method=\"contourf\", ax=None, xlabel=None, ylabel=None, **kwargs):\n        \"\"\"Plot visualization.\n\n        Parameters\n        ----------\n        plot_method : {'contourf', 'contour', 'pcolormesh'}, default='contourf'\n            Plotting method to call when plotting the response. Please refer\n            to the following matplotlib documentation for details:\n            :func:`contourf <matplotlib.pyplot.contourf>`,\n            :func:`contour <matplotlib.pyplot.contour>`,\n            :func:`pcolormesh <matplotlib.pyplot.pcolormesh>`.\n\n        ax : Matplotlib axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        xlabel : str, default=None\n            Overwrite the x-axis label.\n\n        ylabel : str, default=None\n            Overwrite the y-axis label.\n\n        **kwa"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "_plot.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "is `None`.\n\n    lines_ : list of matplotlib Artist or None\n        When the `std_display_style` is `\"fill_between\"`, this is a list of\n        `matplotlib.lines.Line2D` objects corresponding to the mean train and\n        test scores. If another style is used, `line_` is `None`.\n\n    fill_between_ : list of matplotlib Artist or None\n        When the `std_display_style` is `\"fill_between\"`, this is a list of\n        `matplotlib.collections.PolyCollection` objects. If another style is\n        used, `fill_between_` is `None`.\n\n    See Also\n    --------\n    sklearn.model_selection.learning_curve : Compute the learning curve.\n\n    Examples\n    --------\n    >>> import matplotlib.pyplot as plt\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.model_selection import LearningCurveDisplay, learning_curve\n    >>> from sklearn.tree import DecisionTreeClassifier\n    >>> X, y = load_iris(return_X_y=True)\n    >>> tree = DecisionTreeClassifier(random_state=0)\n    >>> train_sizes, train_scores, test_scores = learning_curve(\n    ...     tree, X, y)\n    >>> display = LearningCurveDisplay(train_sizes=train_sizes,\n    ...     train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\n    >>> display.plot()\n    <...>\n    >>> plt.show()\n    \"\"\"\n\n    def __init__(self, *, train_sizes, train_scores, test_scores, score_name=None):\n        self.train_sizes = train_sizes\n        self.train_scores = train_scores\n        self.test_scores = test_scores\n        self.score_name = score_name\n\n    def plot(\n        self,\n        ax=None,\n        *,\n        negate_score=False,\n        score_name=None,\n        score_type=\"both\",\n        std_display_style=\"fill_between\",\n        line_kw=None,\n        fill_between_kw=None,\n        errorbar_kw=None,\n    ):\n        \"\"\"Plot visualization.\n\n        Parameters\n        ----------\n        ax : matplotlib Axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        negate_s"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "_plot.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport numpy as np\n\nfrom sklearn.model_selection._validation import learning_curve, validation_curve\nfrom sklearn.utils._optional_dependencies import check_matplotlib_support\nfrom sklearn.utils._plotting import _interval_max_min_ratio, _validate_score_name\n\n\nclass _BaseCurveDisplay:\n    def _plot_curve(\n        self,\n        x_data,\n        *,\n        ax=None,\n        negate_score=False,\n        score_name=None,\n        score_type=\"test\",\n        std_display_style=\"fill_between\",\n        line_kw=None,\n        fill_between_kw=None,\n        errorbar_kw=None,\n    ):\n        check_matplotlib_support(f\"{self.__class__.__name__}.plot\")\n\n        import matplotlib.pyplot as plt\n\n        if ax is None:\n            _, ax = plt.subplots()\n\n        if negate_score:\n            train_scores, test_scores = -self.train_scores, -self.test_scores\n        else:\n            train_scores, test_scores = self.train_scores, self.test_scores\n\n        if std_display_style not in (\"errorbar\", \"fill_between\", None):\n            raise ValueError(\n                f\"Unknown std_display_style: {std_display_style}. Should be one of\"\n                \" 'errorbar', 'fill_between', or None.\"\n            )\n\n        if score_type not in (\"test\", \"train\", \"both\"):\n            raise ValueError(\n                f\"Unknown score_type: {score_type}. Should be one of 'test', \"\n                \"'train', or 'both'.\"\n            )\n\n        if score_type == \"train\":\n            scores = {\"Train\": train_scores}\n        elif score_type == \"test\":\n            scores = {\"Test\": test_scores}\n        else:  # score_type == \"both\"\n            scores = {\"Train\": train_scores, \"Test\": test_scores}\n\n        if std_display_style in (\"fill_between\", None):\n            # plot the mean score\n            if line_kw is None:\n                line_kw = {}\n\n            self.lines_ = []\n            for line_label, score in scores.items():\n                "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "plot_release_highlights_0_22_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"\n========================================\nRelease Highlights for scikit-learn 0.22\n========================================\n\n.. currentmodule:: sklearn\n\nWe are pleased to announce the release of scikit-learn 0.22, which comes\nwith many bug fixes and new features! We detail below a few of the major\nfeatures of this release. For an exhaustive list of all the changes, please\nrefer to the :ref:`release notes <release_notes_0_22>`.\n\nTo install the latest version (with pip)::\n\n    pip install --upgrade scikit-learn\n\nor with conda::\n\n    conda install -c conda-forge scikit-learn\n\n\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\n# %%\n# New plotting API\n# ----------------\n#\n# A new plotting API is available for creating visualizations. This new API\n# allows for quickly adjusting the visuals of a plot without involving any\n# recomputation. It is also possible to add different plots to the same\n# figure. The following example illustrates `plot_roc_curve`,\n# but other plots utilities are supported like\n# `plot_partial_dependence`,\n# `plot_precision_recall_curve`, and\n# `plot_confusion_matrix`. Read more about this new API in the\n# :ref:`User Guide <visualizations>`.\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\n\n# from sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import RocCurveDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.utils.fixes import parse_version\n\nX, y = make_classification(random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nsvc = SVC(random_state=42)\nsvc.fit(X_train, y_train)\nrfc = RandomForestClassifier(random_state=42)\nrfc.fit(X_train, y_train)\n\n# plot_roc_curve has been removed in version 1.2. From 1.2, use RocCurveDisplay instead.\n# svc_disp = plot_roc_curve(svc, X_test, y_test)\n# rfc_disp = plot_roc_curve(rfc, X"}, {"start_line": 50000, "end_line": 52000, "belongs_to": {"file_name": "calibration.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "CalibrationDisplay\n    >>> X, y = make_classification(random_state=0)\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, random_state=0)\n    >>> clf = LogisticRegression(random_state=0)\n    >>> clf.fit(X_train, y_train)\n    LogisticRegression(random_state=0)\n    >>> y_prob = clf.predict_proba(X_test)[:, 1]\n    >>> prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n    >>> disp = CalibrationDisplay(prob_true, prob_pred, y_prob)\n    >>> disp.plot()\n    <...>\n    \"\"\"\n\n    def __init__(\n        self, prob_true, prob_pred, y_prob, *, estimator_name=None, pos_label=None\n    ):\n        self.prob_true = prob_true\n        self.prob_pred = prob_pred\n        self.y_prob = y_prob\n        self.estimator_name = estimator_name\n        self.pos_label = pos_label\n\n    def plot(self, *, ax=None, name=None, ref_line=True, **kwargs):\n        \"\"\"Plot visualization.\n\n        Extra keyword arguments will be passed to\n        :func:`matplotlib.pyplot.plot`.\n\n        Parameters\n        ----------\n        ax : Matplotlib Axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        name : str, default=None\n            Name for labeling curve. If `None`, use `estimator_name` if\n            not `None`, otherwise no labeling is shown.\n\n        ref_line : bool, default=True\n            If `True`, plots a reference line representing a perfectly\n            calibrated classifier.\n\n        **kwargs : dict\n            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`.\n\n        Returns\n        -------\n        display : :class:`~sklearn.calibration.CalibrationDisplay`\n            Object that stores computed values.\n        \"\"\"\n        self.ax_, self.figure_, name = self._validate_plot_params(ax=ax, name=name)\n\n        info_pos_label = (\n            f\"(Positive class: {self.pos_label})\" if self.pos_label is not None else \"\"\n        )\n\n        default_line_kwargs = {\"marker\": \"s\", \"line"}, {"start_line": 22000, "end_line": 24000, "belongs_to": {"file_name": "_plot.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ween\"`, this is a list of\n        `matplotlib.collections.PolyCollection` objects. If another style is\n        used, `fill_between_` is `None`.\n\n    See Also\n    --------\n    sklearn.model_selection.validation_curve : Compute the validation curve.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.model_selection import ValidationCurveDisplay, validation_curve\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X, y = make_classification(n_samples=1_000, random_state=0)\n    >>> logistic_regression = LogisticRegression()\n    >>> param_name, param_range = \"C\", np.logspace(-8, 3, 10)\n    >>> train_scores, test_scores = validation_curve(\n    ...     logistic_regression, X, y, param_name=param_name, param_range=param_range\n    ... )\n    >>> display = ValidationCurveDisplay(\n    ...     param_name=param_name, param_range=param_range,\n    ...     train_scores=train_scores, test_scores=test_scores, score_name=\"Score\"\n    ... )\n    >>> display.plot()\n    <...>\n    >>> plt.show()\n    \"\"\"\n\n    def __init__(\n        self, *, param_name, param_range, train_scores, test_scores, score_name=None\n    ):\n        self.param_name = param_name\n        self.param_range = param_range\n        self.train_scores = train_scores\n        self.test_scores = test_scores\n        self.score_name = score_name\n\n    def plot(\n        self,\n        ax=None,\n        *,\n        negate_score=False,\n        score_name=None,\n        score_type=\"both\",\n        std_display_style=\"fill_between\",\n        line_kw=None,\n        fill_between_kw=None,\n        errorbar_kw=None,\n    ):\n        \"\"\"Plot visualization.\n\n        Parameters\n        ----------\n        ax : matplotlib Axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        negate_score : bool, default=False\n            Whether or not to negate the scores obta"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_plot.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tions.PolyCollection)\n        assert fill.get_alpha() == 0.5\n\n    assert display.score_name == \"Score\"\n    assert display.ax_.get_xlabel() == \"Number of samples in the training set\"\n    assert display.ax_.get_ylabel() == \"Score\"\n\n    _, legend_labels = display.ax_.get_legend_handles_labels()\n    assert legend_labels == [\"Train\", \"Test\"]\n\n    train_sizes_abs, train_scores, test_scores = learning_curve(\n        estimator, X, y, train_sizes=train_sizes\n    )\n\n    assert_array_equal(display.train_sizes, train_sizes_abs)\n    assert_allclose(display.train_scores, train_scores)\n    assert_allclose(display.test_scores, test_scores)\n\n\ndef test_validation_curve_display_default_usage(pyplot, data):\n    \"\"\"Check the default usage of the ValidationCurveDisplay class.\"\"\"\n    X, y = data\n    estimator = DecisionTreeClassifier(random_state=0)\n\n    param_name, param_range = \"max_depth\", [1, 3, 5]\n    display = ValidationCurveDisplay.from_estimator(\n        estimator, X, y, param_name=param_name, param_range=param_range\n    )\n\n    import matplotlib as mpl\n\n    assert display.errorbar_ is None\n\n    assert isinstance(display.lines_, list)\n    for line in display.lines_:\n        assert isinstance(line, mpl.lines.Line2D)\n\n    assert isinstance(display.fill_between_, list)\n    for fill in display.fill_between_:\n        assert isinstance(fill, mpl.collections.PolyCollection)\n        assert fill.get_alpha() == 0.5\n\n    assert display.score_name == \"Score\"\n    assert display.ax_.get_xlabel() == f\"{param_name}\"\n    assert display.ax_.get_ylabel() == \"Score\"\n\n    _, legend_labels = display.ax_.get_legend_handles_labels()\n    assert legend_labels == [\"Train\", \"Test\"]\n\n    train_scores, test_scores = validation_curve(\n        estimator, X, y, param_name=param_name, param_range=param_range\n    )\n\n    assert_array_equal(display.param_range, param_range)\n    assert_allclose(display.train_scores, train_scores)\n    assert_allclose(display.test_scores, test_scores)\n\n\n@pytest.mark.parametrize(\n    \"Cur"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_plot.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/model_selection/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "import numpy as np\nimport pytest\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import (\n    LearningCurveDisplay,\n    ValidationCurveDisplay,\n    learning_curve,\n    validation_curve,\n)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.utils import shuffle\nfrom sklearn.utils._testing import assert_allclose, assert_array_equal\n\n\n@pytest.fixture\ndef data():\n    return shuffle(*load_iris(return_X_y=True), random_state=0)\n\n\n@pytest.mark.parametrize(\n    \"params, err_type, err_msg\",\n    [\n        ({\"std_display_style\": \"invalid\"}, ValueError, \"Unknown std_display_style:\"),\n        ({\"score_type\": \"invalid\"}, ValueError, \"Unknown score_type:\"),\n    ],\n)\n@pytest.mark.parametrize(\n    \"CurveDisplay, specific_params\",\n    [\n        (ValidationCurveDisplay, {\"param_name\": \"max_depth\", \"param_range\": [1, 3, 5]}),\n        (LearningCurveDisplay, {\"train_sizes\": [0.3, 0.6, 0.9]}),\n    ],\n)\ndef test_curve_display_parameters_validation(\n    pyplot, data, params, err_type, err_msg, CurveDisplay, specific_params\n):\n    \"\"\"Check that we raise a proper error when passing invalid parameters.\"\"\"\n    X, y = data\n    estimator = DecisionTreeClassifier(random_state=0)\n\n    with pytest.raises(err_type, match=err_msg):\n        CurveDisplay.from_estimator(estimator, X, y, **specific_params, **params)\n\n\ndef test_learning_curve_display_default_usage(pyplot, data):\n    \"\"\"Check the default usage of the LearningCurveDisplay class.\"\"\"\n    X, y = data\n    estimator = DecisionTreeClassifier(random_state=0)\n\n    train_sizes = [0.3, 0.6, 0.9]\n    display = LearningCurveDisplay.from_estimator(\n        estimator, X, y, train_sizes=train_sizes\n    )\n\n    import matplotlib as mpl\n\n    assert display.errorbar_ is None\n\n    assert isinstance(display.lines_, list)\n    for line in display.lines_:\n        assert isinstance(line, mpl.lines.Line2D)\n\n    assert isinstance(display.fill_between_, list)\n    for fill in display.fill_between_:\n        assert isinstance(fill, mpl.collec"}], "retrieved_count": 10, "cost_time": 0.4129903316497803}
{"question": "How does Scikit-learn support integration with database systems for large-scale data processing?", "answer": null, "relative_code_list": null, "ground_truth": "Scikit-learn supports integration with database systems for large-scale data processing through several strategies and mechanisms that enable efficient handling of data that exceeds memory capacity. Here's how scikit-learn facilitates database integration for large-scale processing:\n\n1. **Out-of-Core Learning Support**:\n   - **External Memory Learning**: Support for learning from data that cannot fit in RAM\n   - **Streaming Instances**: Ability to process data streams from databases, files, or networks\n   - **Incremental Algorithms**: Algorithms that can learn incrementally from data chunks\n   - **Memory Management**: Efficient memory usage for large datasets\n   - **Batch Processing**: Support for processing data in manageable batches\n\n2. **Incremental Learning Algorithms**:\n   - **SGD-based Models**: Stochastic Gradient Descent classifiers and regressors\n   - **MiniBatchKMeans**: Incremental clustering for large datasets\n   - **Incremental PCA**: Principal Component Analysis for streaming data\n   - **Partial_fit Support**: Many estimators support incremental fitting\n   - **Online Learning**: Real-time learning from streaming database data\n\n3. **Data Streaming Capabilities**:\n   - **Generator-based Loading**: Support for data generators that yield instances\n   - **Database Connectors**: Integration with various database systems through Python connectors\n   - **Chunked Processing**: Processing data in chunks to manage memory usage\n   - **Lazy Evaluation**: Deferred computation until data is actually needed\n   - **Memory Mapping**: Efficient disk-based data access for large datasets\n\n4. **Feature Extraction for Large Data**:\n   - **HashingVectorizer**: Stateless text vectorization for streaming data\n   - **FeatureHasher**: Hashing-based feature extraction for categorical data\n   - **Incremental Feature Selection**: Feature selection that works with streaming data\n   - **Sparse Matrix Support**: Efficient handling of sparse data from databases\n   - **Memory-Efficient Transformers**: Transformers optimized for large-scale data\n\n5. **Database Integration Patterns**:\n   - **SQL Query Integration**: Direct integration with SQL databases through pandas\n   - **NoSQL Support**: Support for NoSQL databases through appropriate connectors\n   - **Data Pipeline Integration**: Integration with data processing pipelines\n   - **ETL Process Support**: Support for Extract, Transform, Load processes\n   - **Real-time Processing**: Support for real-time data processing workflows\n\n6. **Scalability Strategies**:\n   - **Parallel Processing**: Support for parallel computation across multiple cores\n   - **Distributed Computing**: Integration with distributed computing frameworks\n   - **Memory Optimization**: Optimized memory usage for large datasets\n   - **Caching Mechanisms**: Efficient caching of intermediate results\n   - **Resource Management**: Intelligent resource allocation for large-scale processing\n\n7. **Performance Optimization**:\n   - **Batch Size Optimization**: Optimal batch sizes for different algorithms\n   - **Memory-Efficient Algorithms**: Algorithms designed for large-scale data\n   - **Sparse Data Handling**: Efficient handling of sparse data from databases\n   - **Compression Support**: Support for compressed data formats\n   - **I/O Optimization**: Optimized input/output operations for database access\n\n8. **Integration with Big Data Tools**:\n   - **Dask Integration**: Support for Dask DataFrames and arrays\n   - **Spark Integration**: Integration with Apache Spark through appropriate connectors\n   - **Hadoop Support**: Support for Hadoop-based data processing\n   - **Cloud Database Support**: Integration with cloud-based database systems\n   - **Data Warehouse Integration**: Support for data warehouse systems\n\n9. **Data Validation and Quality**:\n   - **Incremental Validation**: Validation that works with streaming data\n   - **Data Quality Checks**: Built-in data quality assessment tools\n   - **Schema Validation**: Validation of data schema and structure\n   - **Error Handling**: Robust error handling for database connection issues\n   - **Data Type Handling**: Automatic handling of different data types from databases\n\n10. **Best Practices and Limitations**:\n    - **Memory Management**: Careful memory management for large datasets\n    - **Batch Processing**: Use appropriate batch sizes for optimal performance\n    - **Connection Management**: Proper database connection management\n    - **Error Recovery**: Implement robust error recovery mechanisms\n    - **Performance Monitoring**: Monitor performance and resource usage", "score": null, "retrieved_content": [{"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "test_public_functions.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tch_openml\",\n    \"sklearn.datasets.fetch_species_distributions\",\n    \"sklearn.datasets.get_data_home\",\n    \"sklearn.datasets.load_breast_cancer\",\n    \"sklearn.datasets.load_diabetes\",\n    \"sklearn.datasets.load_digits\",\n    \"sklearn.datasets.load_files\",\n    \"sklearn.datasets.load_iris\",\n    \"sklearn.datasets.load_linnerud\",\n    \"sklearn.datasets.load_sample_image\",\n    \"sklearn.datasets.load_svmlight_file\",\n    \"sklearn.datasets.load_svmlight_files\",\n    \"sklearn.datasets.load_wine\",\n    \"sklearn.datasets.make_biclusters\",\n    \"sklearn.datasets.make_blobs\",\n    \"sklearn.datasets.make_checkerboard\",\n    \"sklearn.datasets.make_circles\",\n    \"sklearn.datasets.make_classification\",\n    \"sklearn.datasets.make_friedman1\",\n    \"sklearn.datasets.make_friedman2\",\n    \"sklearn.datasets.make_friedman3\",\n    \"sklearn.datasets.make_gaussian_quantiles\",\n    \"sklearn.datasets.make_hastie_10_2\",\n    \"sklearn.datasets.make_low_rank_matrix\",\n    \"sklearn.datasets.make_moons\",\n    \"sklearn.datasets.make_multilabel_classification\",\n    \"sklearn.datasets.make_regression\",\n    \"sklearn.datasets.make_s_curve\",\n    \"sklearn.datasets.make_sparse_coded_signal\",\n    \"sklearn.datasets.make_sparse_spd_matrix\",\n    \"sklearn.datasets.make_sparse_uncorrelated\",\n    \"sklearn.datasets.make_spd_matrix\",\n    \"sklearn.datasets.make_swiss_roll\",\n    \"sklearn.decomposition.sparse_encode\",\n    \"sklearn.feature_extraction.grid_to_graph\",\n    \"sklearn.feature_extraction.img_to_graph\",\n    \"sklearn.feature_extraction.image.extract_patches_2d\",\n    \"sklearn.feature_extraction.image.reconstruct_from_patches_2d\",\n    \"sklearn.feature_selection.chi2\",\n    \"sklearn.feature_selection.f_classif\",\n    \"sklearn.feature_selection.f_regression\",\n    \"sklearn.feature_selection.mutual_info_classif\",\n    \"sklearn.feature_selection.mutual_info_regression\",\n    \"sklearn.feature_selection.r_regression\",\n    \"sklearn.inspection.partial_dependence\",\n    \"sklearn.inspection.permutation_importance\",\n    \"sklearn.isotonic.check_"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/datasets", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Utilities to load popular datasets and artificial data generators.\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport textwrap\n\nfrom sklearn.datasets._base import (\n    clear_data_home,\n    fetch_file,\n    get_data_home,\n    load_breast_cancer,\n    load_diabetes,\n    load_digits,\n    load_files,\n    load_iris,\n    load_linnerud,\n    load_sample_image,\n    load_sample_images,\n    load_wine,\n)\nfrom sklearn.datasets._california_housing import fetch_california_housing\nfrom sklearn.datasets._covtype import fetch_covtype\nfrom sklearn.datasets._kddcup99 import fetch_kddcup99\nfrom sklearn.datasets._lfw import fetch_lfw_pairs, fetch_lfw_people\nfrom sklearn.datasets._olivetti_faces import fetch_olivetti_faces\nfrom sklearn.datasets._openml import fetch_openml\nfrom sklearn.datasets._rcv1 import fetch_rcv1\nfrom sklearn.datasets._samples_generator import (\n    make_biclusters,\n    make_blobs,\n    make_checkerboard,\n    make_circles,\n    make_classification,\n    make_friedman1,\n    make_friedman2,\n    make_friedman3,\n    make_gaussian_quantiles,\n    make_hastie_10_2,\n    make_low_rank_matrix,\n    make_moons,\n    make_multilabel_classification,\n    make_regression,\n    make_s_curve,\n    make_sparse_coded_signal,\n    make_sparse_spd_matrix,\n    make_sparse_uncorrelated,\n    make_spd_matrix,\n    make_swiss_roll,\n)\nfrom sklearn.datasets._species_distributions import fetch_species_distributions\nfrom sklearn.datasets._svmlight_format_io import (\n    dump_svmlight_file,\n    load_svmlight_file,\n    load_svmlight_files,\n)\nfrom sklearn.datasets._twenty_newsgroups import (\n    fetch_20newsgroups,\n    fetch_20newsgroups_vectorized,\n)\n\n__all__ = [\n    \"clear_data_home\",\n    \"dump_svmlight_file\",\n    \"fetch_20newsgroups\",\n    \"fetch_20newsgroups_vectorized\",\n    \"fetch_california_housing\",\n    \"fetch_covtype\",\n    \"fetch_file\",\n    \"fetch_kddcup99\",\n    \"fetch_lfw_pairs\",\n    \"fetch_lfw_people\",\n    \"fetch_olivetti_faces\",\n    \"fetch_openml\",\n    \"fetc"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "_base.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/datasets", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"\nBase IO code for all datasets\n\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport csv\nimport gzip\nimport hashlib\nimport os\nimport re\nimport shutil\nimport time\nimport unicodedata\nimport warnings\nfrom collections import namedtuple\nfrom importlib import resources\nfrom numbers import Integral\nfrom os import environ, listdir, makedirs\nfrom os.path import expanduser, isdir, join, splitext\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile\nfrom urllib.error import URLError\nfrom urllib.parse import urlparse\nfrom urllib.request import urlretrieve\n\nimport numpy as np\n\nfrom sklearn.preprocessing import scale\nfrom sklearn.utils import Bunch, check_random_state\nfrom sklearn.utils._optional_dependencies import check_pandas_support\nfrom sklearn.utils._param_validation import Interval, StrOptions, validate_params\n\nDATA_MODULE = \"sklearn.datasets.data\"\nDESCR_MODULE = \"sklearn.datasets.descr\"\nIMAGES_MODULE = \"sklearn.datasets.images\"\n\nRemoteFileMetadata = namedtuple(\"RemoteFileMetadata\", [\"filename\", \"url\", \"checksum\"])\n\n\n@validate_params(\n    {\n        \"data_home\": [str, os.PathLike, None],\n    },\n    prefer_skip_nested_validation=True,\n)\ndef get_data_home(data_home=None) -> str:\n    \"\"\"Return the path of the scikit-learn data directory.\n\n    This folder is used by some large dataset loaders to avoid downloading the\n    data several times.\n\n    By default the data directory is set to a folder named 'scikit_learn_data' in the\n    user home folder.\n\n    Alternatively, it can be set by the 'SCIKIT_LEARN_DATA' environment\n    variable or programmatically by giving an explicit folder path. The '~'\n    symbol is expanded to the user home folder.\n\n    If the folder does not already exist, it is automatically created.\n\n    Parameters\n    ----------\n    data_home : str or path-like, default=None\n        The path to scikit-learn data directory. If `None`, the default path\n        is `~/scikit_learn_data`.\n\n    Returns\n    -------\n    d"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "plot_release_highlights_1_2_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rmer(\n        transformers=[(\"cat\", OrdinalEncoder(), categorical_features)],\n        remainder=\"passthrough\",\n    ),\n    HistGradientBoostingRegressor(random_state=0),\n).fit(X, y)\n\n# %%\nfrom sklearn.inspection import PartialDependenceDisplay\n\nfig, ax = plt.subplots(figsize=(14, 4), constrained_layout=True)\n_ = PartialDependenceDisplay.from_estimator(\n    model,\n    X,\n    features=[\"age\", \"sex\", (\"pclass\", \"sex\")],\n    categorical_features=categorical_features,\n    ax=ax,\n)\n\n# %%\n# Faster parser in :func:`~datasets.fetch_openml`\n# -----------------------------------------------\n# :func:`~datasets.fetch_openml` now supports a new `\"pandas\"` parser that is\n# more memory and CPU efficient. In v1.4, the default will change to\n# `parser=\"auto\"` which will automatically use the `\"pandas\"` parser for dense\n# data and `\"liac-arff\"` for sparse data.\nX, y = fetch_openml(\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n)\nX.head()\n\n# %%\n# Experimental Array API support in :class:`~discriminant_analysis.LinearDiscriminantAnalysis`\n# --------------------------------------------------------------------------------------------\n# Experimental support for the `Array API <https://data-apis.org/array-api/latest/>`_\n# specification was added to :class:`~discriminant_analysis.LinearDiscriminantAnalysis`.\n# The estimator can now run on any Array API compliant libraries such as\n# `CuPy <https://docs.cupy.dev/en/stable/overview.html>`__, a GPU-accelerated array\n# library. For details, see the :ref:`User Guide <array_api>`.\n\n# %%\n# Improved efficiency of many estimators\n# --------------------------------------\n# In version 1.1 the efficiency of many estimators relying on the computation of\n# pairwise distances (essentially estimators related to clustering, manifold\n# learning and neighbors search algorithms) was greatly improved for float64\n# dense input. Efficiency improvement especially were a reduced memory footprint\n# and a much better scalability on multi-core"}, {"start_line": 0, "end_line": 424, "belongs_to": {"file_name": "_distributor_init.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Distributor init file\n\nDistributors: you can add custom code here to support particular distributions\nof scikit-learn.\n\nFor example, this is a good place to put any checks for hardware requirements.\n\nThe scikit-learn standard source distribution will not put code in this file,\nso you can safely replace this file with your own version.\n\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "datasets.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/asv_benchmarks/benchmarks", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "from pathlib import Path\n\nimport numpy as np\nimport scipy.sparse as sp\nfrom joblib import Memory\n\nfrom sklearn.datasets import (\n    fetch_20newsgroups,\n    fetch_olivetti_faces,\n    fetch_openml,\n    load_digits,\n    make_blobs,\n    make_classification,\n    make_regression,\n)\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MaxAbsScaler, StandardScaler\n\n# memory location for caching datasets\nM = Memory(location=str(Path(__file__).resolve().parent / \"cache\"))\n\n\n@M.cache\ndef _blobs_dataset(n_samples=500000, n_features=3, n_clusters=100, dtype=np.float32):\n    X, _ = make_blobs(\n        n_samples=n_samples, n_features=n_features, centers=n_clusters, random_state=0\n    )\n    X = X.astype(dtype, copy=False)\n\n    X, X_val = train_test_split(X, test_size=0.1, random_state=0)\n    return X, X_val, None, None\n\n\n@M.cache\ndef _20newsgroups_highdim_dataset(n_samples=None, ngrams=(1, 1), dtype=np.float32):\n    newsgroups = fetch_20newsgroups(random_state=0)\n    vectorizer = TfidfVectorizer(ngram_range=ngrams, dtype=dtype)\n    X = vectorizer.fit_transform(newsgroups.data[:n_samples])\n    y = newsgroups.target[:n_samples]\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n\n\n@M.cache\ndef _20newsgroups_lowdim_dataset(n_components=100, ngrams=(1, 1), dtype=np.float32):\n    newsgroups = fetch_20newsgroups()\n    vectorizer = TfidfVectorizer(ngram_range=ngrams)\n    X = vectorizer.fit_transform(newsgroups.data)\n    X = X.astype(dtype, copy=False)\n    svd = TruncatedSVD(n_components=n_components)\n    X = svd.fit_transform(X)\n    y = newsgroups.target\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n\n\n@M.cache\ndef _mnist_dataset(dtype=np.float32):\n    X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n  "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "plot_scalable_poly_kernels.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/kernel_approximation", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"\n======================================================\nScalable learning with polynomial kernel approximation\n======================================================\n\n.. currentmodule:: sklearn.kernel_approximation\n\nThis example illustrates the use of :class:`PolynomialCountSketch` to\nefficiently generate polynomial kernel feature-space approximations.\nThis is used to train linear classifiers that approximate the accuracy\nof kernelized ones.\n\nWe use the Covtype dataset [2], trying to reproduce the experiments on the\noriginal paper of Tensor Sketch [1], i.e. the algorithm implemented by\n:class:`PolynomialCountSketch`.\n\nFirst, we compute the accuracy of a linear classifier on the original\nfeatures. Then, we train linear classifiers on different numbers of\nfeatures (`n_components`) generated by :class:`PolynomialCountSketch`,\napproximating the accuracy of a kernelized classifier in a scalable manner.\n\n\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\n# %%\n# Preparing the data\n# ------------------\n#\n# Load the Covtype dataset, which contains 581,012 samples\n# with 54 features each, distributed among 6 classes. The goal of this dataset\n# is to predict forest cover type from cartographic variables only\n# (no remotely sensed data). After loading, we transform it into a binary\n# classification problem to match the version of the dataset in the\n# LIBSVM webpage [2], which was the one used in [1].\n\nfrom sklearn.datasets import fetch_covtype\n\nX, y = fetch_covtype(return_X_y=True)\n\ny[y != 2] = 0\ny[y == 2] = 1  # We will try to separate class 2 from the other 6 classes.\n\n# %%\n# Partitioning the data\n# ---------------------\n#\n# Here we select 5,000 samples for training and 10,000 for testing.\n# To actually reproduce the results in the original Tensor Sketch paper,\n# select 100,000 for training.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=5_000, test_size=10_0"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/sklearn/datasets", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n    make_friedman1,\n    make_friedman2,\n    make_friedman3,\n    make_gaussian_quantiles,\n    make_hastie_10_2,\n    make_low_rank_matrix,\n    make_moons,\n    make_multilabel_classification,\n    make_regression,\n    make_s_curve,\n    make_sparse_coded_signal,\n    make_sparse_spd_matrix,\n    make_sparse_uncorrelated,\n    make_spd_matrix,\n    make_swiss_roll,\n)\nfrom sklearn.datasets._species_distributions import fetch_species_distributions\nfrom sklearn.datasets._svmlight_format_io import (\n    dump_svmlight_file,\n    load_svmlight_file,\n    load_svmlight_files,\n)\nfrom sklearn.datasets._twenty_newsgroups import (\n    fetch_20newsgroups,\n    fetch_20newsgroups_vectorized,\n)\n\n__all__ = [\n    \"clear_data_home\",\n    \"dump_svmlight_file\",\n    \"fetch_20newsgroups\",\n    \"fetch_20newsgroups_vectorized\",\n    \"fetch_california_housing\",\n    \"fetch_covtype\",\n    \"fetch_file\",\n    \"fetch_kddcup99\",\n    \"fetch_lfw_pairs\",\n    \"fetch_lfw_people\",\n    \"fetch_olivetti_faces\",\n    \"fetch_openml\",\n    \"fetch_rcv1\",\n    \"fetch_species_distributions\",\n    \"get_data_home\",\n    \"load_breast_cancer\",\n    \"load_diabetes\",\n    \"load_digits\",\n    \"load_files\",\n    \"load_iris\",\n    \"load_linnerud\",\n    \"load_sample_image\",\n    \"load_sample_images\",\n    \"load_svmlight_file\",\n    \"load_svmlight_files\",\n    \"load_wine\",\n    \"make_biclusters\",\n    \"make_blobs\",\n    \"make_checkerboard\",\n    \"make_circles\",\n    \"make_classification\",\n    \"make_friedman1\",\n    \"make_friedman2\",\n    \"make_friedman3\",\n    \"make_gaussian_quantiles\",\n    \"make_hastie_10_2\",\n    \"make_low_rank_matrix\",\n    \"make_moons\",\n    \"make_multilabel_classification\",\n    \"make_regression\",\n    \"make_s_curve\",\n    \"make_sparse_coded_signal\",\n    \"make_sparse_spd_matrix\",\n    \"make_sparse_uncorrelated\",\n    \"make_spd_matrix\",\n    \"make_swiss_roll\",\n]\n\n\ndef __getattr__(name):\n    if name == \"load_boston\":\n        msg = textwrap.dedent(\n            \"\"\"\n            `load_boston` has been removed from scikit-learn since version 1.2.\n\n      "}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "plot_release_highlights_0_24_0.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/examples/release_highlights", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "------------------------------------------\n# The new :class:`~sklearn.kernel_approximation.PolynomialCountSketch`\n# approximates a polynomial expansion of a feature space when used with linear\n# models, but uses much less memory than\n# :class:`~sklearn.preprocessing.PolynomialFeatures`.\n\nfrom sklearn.datasets import fetch_covtype\nfrom sklearn.kernel_approximation import PolynomialCountSketch\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\nX, y = fetch_covtype(return_X_y=True)\npipe = make_pipeline(\n    MinMaxScaler(),\n    PolynomialCountSketch(degree=2, n_components=300),\n    LogisticRegression(max_iter=1000),\n)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=5000, test_size=10000, random_state=42\n)\npipe.fit(X_train, y_train).score(X_test, y_test)\n\n##############################################################################\n# For comparison, here is the score of a linear baseline for the same data:\n\nlinear_baseline = make_pipeline(MinMaxScaler(), LogisticRegression(max_iter=1000))\nlinear_baseline.fit(X_train, y_train).score(X_test, y_test)\n\n##############################################################################\n# Individual Conditional Expectation plots\n# ----------------------------------------\n# A new kind of partial dependence plot is available: the Individual\n# Conditional Expectation (ICE) plot. ICE plots visualize the dependence of the\n# prediction on a feature for each sample separately, with one line per sample.\n# See the :ref:`User Guide <individual_conditional>`\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.ensemble import RandomForestRegressor\n\n# from sklearn.inspection import plot_partial_dependence\nfrom sklearn.inspection import PartialDependenceDisplay\n\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\nfeatures = [\"MedInc\", \"AveOccup\", \"H"}, {"start_line": 0, "end_line": 49, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/scikit-learn/asv_benchmarks/benchmarks", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"Benchmark suite for scikit-learn using ASV\"\"\"\n"}], "retrieved_count": 10, "cost_time": 0.3995673656463623}
