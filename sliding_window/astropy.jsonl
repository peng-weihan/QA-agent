{"question": "What is the semantic relationship between the expiration threshold computed in the test initialization method and the early-exit optimization tested across multiple test methods?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 3000, "end_line": 4037, "belongs_to": {"file_name": "test_update_leap_seconds.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  [\"# File expires on 28 June 2010\"] + [str(item) for item in expired]\n            )\n        )\n        with pytest.warns(iers.IERSStaleWarning):\n            update_leap_seconds([\"erfa\", expired_file])\n\n    def test_init_thread_safety(self, monkeypatch):\n        # Set up expired ERFA leap seconds.\n        expired = self.erfa_ls[self.erfa_ls[\"year\"] < 2017]\n        expired.update_erfa_leap_seconds(initialize_erfa=\"empty\")\n        # Force re-initialization, even if another test already did it\n        monkeypatch.setattr(\n            astropy.time.core,\n            \"_LEAP_SECONDS_CHECK\",\n            astropy.time.core._LeapSecondsCheck.NOT_STARTED,\n        )\n        workers = 4\n        with ThreadPoolExecutor(max_workers=workers) as executor:\n            futures = [\n                executor.submit(lambda: str(Time(\"2019-01-01 00:00:00.000\").tai))\n                for i in range(workers)\n            ]\n            results = [future.result() for future in futures]\n            assert results == [\"2019-01-01 00:00:37.000\"] * workers\n"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_update_leap_seconds.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "s.expires > self.good_enough\n\n    def test_auto_update_bad_file(self):\n        with pytest.warns(AstropyWarning, match=\"FileNotFound\"):\n            update_leap_seconds([\"nonsense\"])\n\n    def test_auto_update_corrupt_file(self, tmp_path):\n        bad_file = tmp_path / \"no_expiration\"\n        lines = Path(iers.IERS_LEAP_SECOND_FILE).read_text().splitlines()\n        bad_file.write_text(\n            \"\\n\".join(line for line in lines if not line.startswith(\"#\"))\n        )\n\n        with pytest.warns(AstropyWarning, match=\"ValueError.*did not find expiration\"):\n            update_leap_seconds([bad_file])\n\n    def test_auto_update_expired_file(self, tmp_path):\n        # Set up expired ERFA leap seconds.\n        expired = self.erfa_ls[self.erfa_ls[\"year\"] < 2017]\n        expired.update_erfa_leap_seconds(initialize_erfa=\"empty\")\n        # Create similarly expired file.\n        expired_file = str(tmp_path / \"expired.dat\")\n        Path(expired_file).write_text(\n            \"\\n\".join(\n                [\"# File expires on 28 June 2010\"] + [str(item) for item in expired]\n            )\n        )\n        with pytest.warns(iers.IERSStaleWarning):\n            update_leap_seconds([\"erfa\", expired_file])\n\n    def test_init_thread_safety(self, monkeypatch):\n        # Set up expired ERFA leap seconds.\n        expired = self.erfa_ls[self.erfa_ls[\"year\"] < 2017]\n        expired.update_erfa_leap_seconds(initialize_erfa=\"empty\")\n        # Force re-initialization, even if another test already did it\n        monkeypatch.setattr(\n            astropy.time.core,\n            \"_LEAP_SECONDS_CHECK\",\n            astropy.time.core._LeapSecondsCheck.NOT_STARTED,\n        )\n        workers = 4\n        with ThreadPoolExecutor(max_workers=workers) as executor:\n            futures = [\n                executor.submit(lambda: str(Time(\"2019-01-01 00:00:00.000\").tai))\n                for i in range(workers)\n            ]\n            results = [future.result() for future in futures]\n            assert results == ["}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "cale=\"tai\")\n        assert ls2.meta[\"data_url\"] == str(fake_url)\n\n    def test_fake_expired_file(self, tmp_path):\n        self.remove_auto_open_files(\n            \"erfa\", \"iers_leap_second_auto_url\", \"ietf_leap_second_auto_url\"\n        )\n        fake_file = make_fake_file(\"28 June 2010\", tmp_path)\n        with iers.conf.set_temp(\"system_leap_second_file\", fake_file):\n            # If we try this directly, the built-in file will be found.\n            ls = iers.LeapSeconds.open()\n            assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n            # But if we remove the built-in one, the expired one will be\n            # used and we get a warning that it is stale.\n            self.remove_auto_open_files(iers.IERS_LEAP_SECOND_FILE)\n            with pytest.warns(iers.IERSStaleWarning):\n                ls2 = iers.LeapSeconds.open()\n            assert ls2.meta[\"data_url\"] == fake_file\n            assert ls2.expires == Time(\"2010-06-28\", scale=\"tai\")\n\n    @pytest.mark.skipif(\n        not os.path.isfile(SYSTEM_FILE), reason=f\"system does not have {SYSTEM_FILE}\"\n    )\n    def test_system_file_used_if_not_expired(self, tmp_path):\n        # We skip the test if the system file is on a CI and is expired -\n        # we should not depend on CI keeping it up to date, but if it is,\n        # we should check that it is used if possible.\n        if iers.LeapSeconds.open(SYSTEM_FILE).expires <= self.good_enough:\n            pytest.skip(\"System leap second file is expired.\")\n\n        self.remove_auto_open_files(\"erfa\")\n        with iers.conf.set_temp(\"system_leap_second_file\", SYSTEM_FILE):\n            ls = iers.LeapSeconds.open()\n            assert ls.expires > self.good_enough\n            assert ls.meta[\"data_url\"] in (iers.IERS_LEAP_SECOND_FILE, SYSTEM_FILE)\n\n            # Also check with a \"built-in\" file that is expired\n            fake_file = make_fake_file(\"28 June 2017\", tmp_path)\n            iers.LeapSeconds._auto_open_files[0] = fake_file\n            ls2 = iers.Le"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     not os.path.isfile(SYSTEM_FILE), reason=f\"system does not have {SYSTEM_FILE}\"\n    )\n    def test_system_file_used_if_not_expired(self, tmp_path):\n        # We skip the test if the system file is on a CI and is expired -\n        # we should not depend on CI keeping it up to date, but if it is,\n        # we should check that it is used if possible.\n        if iers.LeapSeconds.open(SYSTEM_FILE).expires <= self.good_enough:\n            pytest.skip(\"System leap second file is expired.\")\n\n        self.remove_auto_open_files(\"erfa\")\n        with iers.conf.set_temp(\"system_leap_second_file\", SYSTEM_FILE):\n            ls = iers.LeapSeconds.open()\n            assert ls.expires > self.good_enough\n            assert ls.meta[\"data_url\"] in (iers.IERS_LEAP_SECOND_FILE, SYSTEM_FILE)\n\n            # Also check with a \"built-in\" file that is expired\n            fake_file = make_fake_file(\"28 June 2017\", tmp_path)\n            iers.LeapSeconds._auto_open_files[0] = fake_file\n            ls2 = iers.LeapSeconds.open()\n            assert ls2.expires > Time.now()\n            assert ls2.meta[\"data_url\"] == SYSTEM_FILE\n\n    @pytest.mark.remote_data\n    def test_auto_open_urls_always_good_enough(self):\n        # Avoid using the erfa, built-in and system files, as they might\n        # be good enough already.\n        try:\n            # Need auto_download so that IERS_B won't be loaded and\n            # cause tests to fail.\n            iers.conf.auto_download = True\n\n            self.remove_auto_open_files(\n                \"erfa\", iers.IERS_LEAP_SECOND_FILE, \"system_leap_second_file\"\n            )\n            ls = iers.LeapSeconds.open()\n            assert ls.expires > self.good_enough\n            assert ls.meta[\"data_url\"].startswith(\"http\")\n        finally:\n            # This setting is to be consistent with astropy/conftest.py\n            iers.conf.auto_download = False\n\n\nclass ERFALeapSecondsSafe:\n    \"\"\"Base class for tests that change the ERFA leap-second tables.\n\n    It ensures the o"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "str(fake_url)\n\n    def test_fake_expired_file(self, tmp_path):\n        fake_file1 = make_fake_file(\"28 June 2010\", tmp_path)\n        fake_file2 = make_fake_file(\"27 June 2012\", tmp_path)\n        # Between these and the built-in one, the built-in file is best.\n        ls = iers.LeapSeconds.auto_open(\n            [fake_file1, fake_file2, iers.IERS_LEAP_SECOND_FILE]\n        )\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n        # But if we remove the built-in one, the least expired one will be\n        # used and we get a warning that it is stale.\n        with pytest.warns(iers.IERSStaleWarning):\n            ls2 = iers.LeapSeconds.auto_open([fake_file1, fake_file2])\n        assert ls2.meta[\"data_url\"] == fake_file2\n        assert ls2.expires == Time(\"2012-06-27\", scale=\"tai\")\n\n        # Use the fake files to make sure auto_max_age is safe.\n        # Should have no warning in either example.\n        with iers.conf.set_temp(\"auto_max_age\", None):\n            ls3 = iers.LeapSeconds.auto_open([fake_file1, iers.IERS_LEAP_SECOND_FILE])\n        assert ls3.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n        with iers.conf.set_temp(\"auto_max_age\", None):\n            ls4 = iers.LeapSeconds.auto_open([fake_file1, fake_file2])\n        assert ls4.meta[\"data_url\"] == fake_file2\n\n\n@pytest.mark.remote_data\nclass TestRemoteURLs:\n    def setup_class(cls):\n        # Need auto_download so that IERS_B won't be loaded and cause tests to\n        # fail.\n        iers.conf.auto_download = True\n\n    def teardown_class(cls):\n        # This setting is to be consistent with astropy/conftest.py\n        iers.conf.auto_download = False\n\n    # In these tests, the results may be cached.\n    # This is fine - no need to download again.\n    def test_iers_url(self):\n        ls = iers.LeapSeconds.auto_open([iers.IERS_LEAP_SECOND_URL])\n        assert ls.expires > Time.now()\n\n    def test_ietf_url(self):\n        ls = iers.LeapSeconds.auto_open([iers.IETF_LEAP_SECOND_URL])\n        assert"}, {"start_line": 45000, "end_line": 47000, "belongs_to": {"file_name": "iers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "list = []\n        # Go through all entries, and return the first one that\n        # is not expired, or the most up to date one.\n        for f, allow_cache in trials:\n            if not allow_cache:\n                clear_download_cache(f)\n\n            try:\n                trial = cls.open(f, cache=True)\n            except Exception as exc:\n                err_list.append(exc)\n                continue\n\n            if self is None or trial.expires > self.expires:\n                self = trial\n                self.meta[\"data_url\"] = str(f)\n                if self.expires > good_enough:\n                    break\n\n        if self is None:\n            raise ValueError(\n                \"none of the files could be read. The \"\n                f\"following errors were raised:\\n {err_list}\"\n            )\n\n        if self.expires < self._today() and conf.auto_max_age is not None:\n            warn(\"leap-second file is expired.\", IERSStaleWarning)\n\n        return self\n\n    @property\n    def expires(self):\n        \"\"\"The limit of validity of the table.\"\"\"\n        return self._expires\n\n    @classmethod\n    def _read_leap_seconds(cls, file, **kwargs):\n        \"\"\"Read a file, identifying expiration by matching 'File expires'.\"\"\"\n        expires = None\n        # Find expiration date.\n        with get_readable_fileobj(file) as fh:\n            lines = fh.readlines()\n            for line in lines:\n                match = cls._re_expires.match(line)\n                if match:\n                    day, month, year = match.groups()[0].split()\n                    month_nb = MONTH_ABBR.index(month[:3]) + 1\n                    expires = Time(\n                        f\"{year}-{month_nb:02d}-{day}\", scale=\"tai\", out_subfmt=\"date\"\n                    )\n                    break\n            else:\n                raise ValueError(f\"did not find expiration date in {file}\")\n\n        self = cls.read(lines, format=\"ascii.no_header\", **kwargs)\n        self._expires = expires\n        return self\n\n    @classme"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "OND_FILE]\n\n    @pytest.mark.filterwarnings(iers.IERSStaleWarning)\n    def test_fake_future_file(self, tmp_path):\n        fake_file = make_fake_file(\"28 June 2345\", tmp_path)\n        # Try as system file for auto_open, setting auto_max_age such\n        # that any ERFA or system files are guaranteed to be expired,\n        # while the fake file is guaranteed to be OK.\n        with iers.conf.set_temp(\"auto_max_age\", -100000):\n            ls = iers.LeapSeconds.auto_open(\n                [\"erfa\", iers.IERS_LEAP_SECOND_FILE, fake_file]\n            )\n            assert ls.expires == Time(\"2345-06-28\", scale=\"tai\")\n            assert ls.meta[\"data_url\"] == str(fake_file)\n            # And as URL\n            fake_url = \"file:\" + urllib.request.pathname2url(fake_file)\n            ls2 = iers.LeapSeconds.auto_open(\n                [\"erfa\", iers.IERS_LEAP_SECOND_FILE, fake_url]\n            )\n            assert ls2.expires == Time(\"2345-06-28\", scale=\"tai\")\n            assert ls2.meta[\"data_url\"] == str(fake_url)\n\n    def test_fake_expired_file(self, tmp_path):\n        fake_file1 = make_fake_file(\"28 June 2010\", tmp_path)\n        fake_file2 = make_fake_file(\"27 June 2012\", tmp_path)\n        # Between these and the built-in one, the built-in file is best.\n        ls = iers.LeapSeconds.auto_open(\n            [fake_file1, fake_file2, iers.IERS_LEAP_SECOND_FILE]\n        )\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n        # But if we remove the built-in one, the least expired one will be\n        # used and we get a warning that it is stale.\n        with pytest.warns(iers.IERSStaleWarning):\n            ls2 = iers.LeapSeconds.auto_open([fake_file1, fake_file2])\n        assert ls2.meta[\"data_url\"] == fake_file2\n        assert ls2.expires == Time(\"2012-06-27\", scale=\"tai\")\n\n        # Use the fake files to make sure auto_max_age is safe.\n        # Should have no warning in either example.\n        with iers.conf.set_temp(\"auto_max_age\", None):\n            ls3 = iers.L"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " iers.LeapSeconds.open()\n        assert ls.meta[\"data_url\"] == \"erfa\"\n\n    def test_builtin_found(self):\n        # Set huge maximum age such that built-in file is always OK.\n        # If we remove 'erfa', it should thus be found.\n        self.remove_auto_open_files(\"erfa\")\n        with iers.conf.set_temp(\"auto_max_age\", 100000):\n            ls = iers.LeapSeconds.open()\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n    # The test below is marked remote_data only to ensure it runs\n    # as an allowed-fail job on CI: i.e., we will notice it (eventually)\n    # but will not be misled in thinking that a PR is bad.\n    @pytest.mark.remote_data\n    def test_builtin_not_expired(self):\n        # TODO: would be nice to have automatic PRs for this!\n        ls = iers.LeapSeconds.open(iers.IERS_LEAP_SECOND_FILE)\n        assert ls.expires > self.good_enough, (\n            \"The leap second file built in to astropy is expired. Fix with:\\n\"\n            \"cd astropy/utils/iers/data/; . update_builtin_iers.sh\\n\"\n            \"and commit as a PR (for details, see release procedure).\"\n        )\n\n    def test_fake_future_file(self, tmp_path):\n        fake_file = make_fake_file(\"28 June 2345\", tmp_path)\n        # Try as system file for auto_open, setting auto_max_age such\n        # that any ERFA or system files are guaranteed to be expired.\n        with (\n            iers.conf.set_temp(\"auto_max_age\", -100000),\n            iers.conf.set_temp(\"system_leap_second_file\", fake_file),\n        ):\n            ls = iers.LeapSeconds.open()\n        assert ls.expires == Time(\"2345-06-28\", scale=\"tai\")\n        assert ls.meta[\"data_url\"] == str(fake_file)\n        # And as URL\n        fake_url = \"file:\" + urllib.request.pathname2url(fake_file)\n        with (\n            iers.conf.set_temp(\"auto_max_age\", -100000),\n            iers.conf.set_temp(\"iers_leap_second_auto_url\", fake_url),\n        ):\n            ls2 = iers.LeapSeconds.open()\n        assert ls2.expires == Time(\"2345-06-28\", s"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "econds.dat\")\n    with open(fake_file, \"w\") as fh:\n        fh.write(\n            \"\\n\".join([f\"#  File expires on {expiration}\"] + str(ls).split(\"\\n\")[2:-1])\n        )\n        return fake_file\n\n\ndef test_fake_file(tmp_path):\n    fake_file = make_fake_file(\"28 June 2345\", tmp_path)\n    fake = iers.LeapSeconds.from_iers_leap_seconds(fake_file)\n    assert fake.expires == Time(\"2345-06-28\", scale=\"tai\")\n\n\nclass TestAutoOpenExplicitLists:\n    # For this set of tests, leap-seconds are allowed to be expired\n    # except as explicitly tested.\n    @pytest.mark.filterwarnings(iers.IERSStaleWarning)\n    def test_auto_open_simple(self):\n        ls = iers.LeapSeconds.auto_open([iers.IERS_LEAP_SECOND_FILE])\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n    @pytest.mark.filterwarnings(iers.IERSStaleWarning)\n    def test_auto_open_erfa(self):\n        ls = iers.LeapSeconds.auto_open([\"erfa\", iers.IERS_LEAP_SECOND_FILE])\n        assert ls.meta[\"data_url\"] in [\"erfa\", iers.IERS_LEAP_SECOND_FILE]\n\n    @pytest.mark.filterwarnings(iers.IERSStaleWarning)\n    def test_fake_future_file(self, tmp_path):\n        fake_file = make_fake_file(\"28 June 2345\", tmp_path)\n        # Try as system file for auto_open, setting auto_max_age such\n        # that any ERFA or system files are guaranteed to be expired,\n        # while the fake file is guaranteed to be OK.\n        with iers.conf.set_temp(\"auto_max_age\", -100000):\n            ls = iers.LeapSeconds.auto_open(\n                [\"erfa\", iers.IERS_LEAP_SECOND_FILE, fake_file]\n            )\n            assert ls.expires == Time(\"2345-06-28\", scale=\"tai\")\n            assert ls.meta[\"data_url\"] == str(fake_file)\n            # And as URL\n            fake_url = \"file:\" + urllib.request.pathname2url(fake_file)\n            ls2 = iers.LeapSeconds.auto_open(\n                [\"erfa\", iers.IERS_LEAP_SECOND_FILE, fake_url]\n            )\n            assert ls2.expires == Time(\"2345-06-28\", scale=\"tai\")\n            assert ls2.meta[\"data_url\"] == "}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "   assert ls[\"mjd\"][-1] == 57754\n        assert ls[\"tai_utc\"][-1] == 37\n        self.verify_day_month_year(ls)\n\n    @pytest.mark.parametrize(\n        \"file\",\n        (LEAP_SECOND_LIST, \"file:\" + urllib.request.pathname2url(LEAP_SECOND_LIST)),\n    )\n    def test_open_leap_seconds_list(self, file):\n        ls = iers.LeapSeconds.from_leap_seconds_list(file)\n        ls2 = iers.LeapSeconds.open(file)\n        assert np.all(ls == ls2)\n\n    @pytest.mark.skipif(\n        not os.path.isfile(SYSTEM_FILE), reason=f\"system does not have {SYSTEM_FILE}\"\n    )\n    def test_open_system_file(self):\n        ls = iers.LeapSeconds.open(SYSTEM_FILE)\n        expired = ls.expires < Time.now()\n        if expired:\n            pytest.skip(\"System leap second file is expired.\")\n        assert not expired\n\n\ndef make_fake_file(expiration, tmp_path):\n    \"\"\"copy the built-in IERS file but set a different expiration date.\"\"\"\n    ls = iers.LeapSeconds.from_iers_leap_seconds()\n    fake_file = str(tmp_path / \"fake_leap_seconds.dat\")\n    with open(fake_file, \"w\") as fh:\n        fh.write(\n            \"\\n\".join([f\"#  File expires on {expiration}\"] + str(ls).split(\"\\n\")[2:-1])\n        )\n        return fake_file\n\n\ndef test_fake_file(tmp_path):\n    fake_file = make_fake_file(\"28 June 2345\", tmp_path)\n    fake = iers.LeapSeconds.from_iers_leap_seconds(fake_file)\n    assert fake.expires == Time(\"2345-06-28\", scale=\"tai\")\n\n\nclass TestAutoOpenExplicitLists:\n    # For this set of tests, leap-seconds are allowed to be expired\n    # except as explicitly tested.\n    @pytest.mark.filterwarnings(iers.IERSStaleWarning)\n    def test_auto_open_simple(self):\n        ls = iers.LeapSeconds.auto_open([iers.IERS_LEAP_SECOND_FILE])\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n    @pytest.mark.filterwarnings(iers.IERSStaleWarning)\n    def test_auto_open_erfa(self):\n        ls = iers.LeapSeconds.auto_open([\"erfa\", iers.IERS_LEAP_SECOND_FILE])\n        assert ls.meta[\"data_url\"] in [\"erfa\", iers.IERS_LEAP_SEC"}], "retrieved_count": 10, "cost_time": 1.0202116966247559}
{"question": "What is the dependency relationship between the class that manages macro definitions and performs token expansion in the ANSI-C style preprocessor and the data container class that stores macro information including name, token sequence, and variadic flag, in handling macros that accept a variable number of arguments during replacement of macro references with their expanded token sequences?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "cpp.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/ply", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "--------------------------------------------------\n# Macro object\n#\n# This object holds information about preprocessor macros\n#\n#    .name      - Macro name (string)\n#    .value     - Macro value (a list of tokens)\n#    .arglist   - List of argument names\n#    .variadic  - Boolean indicating whether or not variadic macro\n#    .vararg    - Name of the variadic parameter\n#\n# When a macro is created, the macro replacement token sequence is\n# pre-scanned and used to create patch lists that are later used\n# during macro expansion\n# ------------------------------------------------------------------\n\nclass Macro(object):\n    def __init__(self,name,value,arglist=None,variadic=False):\n        self.name = name\n        self.value = value\n        self.arglist = arglist\n        self.variadic = variadic\n        if variadic:\n            self.vararg = arglist[-1]\n        self.source = None\n\n# ------------------------------------------------------------------\n# Preprocessor object\n#\n# Object representing a preprocessor.  Contains macro definitions,\n# include directories, and other information\n# ------------------------------------------------------------------\n\nclass Preprocessor(object):\n    def __init__(self,lexer=None):\n        if lexer is None:\n            lexer = lex.lexer\n        self.lexer = lexer\n        self.macros = { }\n        self.path = []\n        self.temp_path = []\n\n        # Probe the lexer for selected tokens\n        self.lexprobe()\n\n        tm = time.localtime()\n        self.define(\"__DATE__ \\\"%s\\\"\" % time.strftime(\"%b %d %Y\",tm))\n        self.define(\"__TIME__ \\\"%s\\\"\" % time.strftime(\"%H:%M:%S\",tm))\n        self.parser = None\n\n    # -----------------------------------------------------------------------------\n    # tokenize()\n    #\n    # Utility function. Given a string of text, tokenize into a list of tokens\n    # -----------------------------------------------------------------------------\n\n    def tokenize(self,text):\n        tokens = []\n        self.lexer.input"}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "cpp.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/ply", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "en(macro.value)) and (macro.value[i+1].type == self.t_ID) and \\\n                        (macro.value[i+1].value == macro.vararg):\n                    macro.var_comma_patch.append(i-1)\n            i += 1\n        macro.patch.sort(key=lambda x: x[2],reverse=True)\n\n    # ----------------------------------------------------------------------\n    # macro_expand_args()\n    #\n    # Given a Macro and list of arguments (each a token list), this method\n    # returns an expanded version of a macro.  The return value is a token sequence\n    # representing the replacement macro tokens\n    # ----------------------------------------------------------------------\n\n    def macro_expand_args(self,macro,args):\n        # Make a copy of the macro token sequence\n        rep = [copy.copy(_x) for _x in macro.value]\n\n        # Make string expansion patches.  These do not alter the length of the replacement sequence\n\n        str_expansion = {}\n        for argnum, i in macro.str_patch:\n            if argnum not in str_expansion:\n                str_expansion[argnum] = ('\"%s\"' % \"\".join([x.value for x in args[argnum]])).replace(\"\\\\\",\"\\\\\\\\\")\n            rep[i] = copy.copy(rep[i])\n            rep[i].value = str_expansion[argnum]\n\n        # Make the variadic macro comma patch.  If the variadic macro argument is empty, we get rid\n        comma_patch = False\n        if macro.variadic and not args[-1]:\n            for i in macro.var_comma_patch:\n                rep[i] = None\n                comma_patch = True\n\n        # Make all other patches.   The order of these matters.  It is assumed that the patch list\n        # has been sorted in reverse order of patch location since replacements will cause the\n        # size of the replacement sequence to expand from the patch point.\n\n        expanded = { }\n        for ptype, argnum, i in macro.patch:\n            # Concatenation.   Argument is left unexpanded\n            if ptype == 'c':\n                rep[i:i+1] = args[argnum]\n            # Normal expansion."}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "cpp.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/ply", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " == '#':\n                    macro.value[i] = copy.copy(macro.value[i])\n                    macro.value[i].type = self.t_STRING\n                    del macro.value[i-1]\n                    macro.str_patch.append((argnum,i-1))\n                    continue\n                # Concatenation\n                elif (i > 0 and macro.value[i-1].value == '##'):\n                    macro.patch.append(('c',argnum,i-1))\n                    del macro.value[i-1]\n                    i -= 1\n                    continue\n                elif ((i+1) < len(macro.value) and macro.value[i+1].value == '##'):\n                    macro.patch.append(('c',argnum,i))\n                    del macro.value[i + 1]\n                    continue\n                # Standard expansion\n                else:\n                    macro.patch.append(('e',argnum,i))\n            elif macro.value[i].value == '##':\n                if macro.variadic and (i > 0) and (macro.value[i-1].value == ',') and \\\n                        ((i+1) < len(macro.value)) and (macro.value[i+1].type == self.t_ID) and \\\n                        (macro.value[i+1].value == macro.vararg):\n                    macro.var_comma_patch.append(i-1)\n            i += 1\n        macro.patch.sort(key=lambda x: x[2],reverse=True)\n\n    # ----------------------------------------------------------------------\n    # macro_expand_args()\n    #\n    # Given a Macro and list of arguments (each a token list), this method\n    # returns an expanded version of a macro.  The return value is a token sequence\n    # representing the replacement macro tokens\n    # ----------------------------------------------------------------------\n\n    def macro_expand_args(self,macro,args):\n        # Make a copy of the macro token sequence\n        rep = [copy.copy(_x) for _x in macro.value]\n\n        # Make string expansion patches.  These do not alter the length of the replacement sequence\n\n        str_expansion = {}\n        for argnum, i in macro.str_patch:\n            if argnum not i"}, {"start_line": 31000, "end_line": 33000, "belongs_to": {"file_name": "cpp.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/ply", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "                 # of macro expansion\n                        if a[0].value[-3:] == '...':\n                            a[0].value = a[0].value[:-3]\n                        continue\n                    if len(a) > 1 or a[0].type != self.t_ID:\n                        print(\"Invalid macro argument\")\n                        break\n                else:\n                    mvalue = self.tokenstrip(linetok[1+tokcount:])\n                    i = 0\n                    while i < len(mvalue):\n                        if i+1 < len(mvalue):\n                            if mvalue[i].type in self.t_WS and mvalue[i+1].value == '##':\n                                del mvalue[i]\n                                continue\n                            elif mvalue[i].value == '##' and mvalue[i+1].type in self.t_WS:\n                                del mvalue[i+1]\n                        i += 1\n                    m = Macro(name.value,mvalue,[x[0].value for x in args],variadic)\n                    self.macro_prescan(m)\n                    self.macros[name.value] = m\n            else:\n                print(\"Bad macro definition\")\n        except LookupError:\n            print(\"Bad macro definition\")\n\n    # ----------------------------------------------------------------------\n    # undef()\n    #\n    # Undefine a macro\n    # ----------------------------------------------------------------------\n\n    def undef(self,tokens):\n        id = tokens[0].value\n        try:\n            del self.macros[id]\n        except LookupError:\n            pass\n\n    # ----------------------------------------------------------------------\n    # parse()\n    #\n    # Parse input text.\n    # ----------------------------------------------------------------------\n    def parse(self,input,source=None,ignore={}):\n        self.ignore = ignore\n        self.parser = self.parsegen(input,source)\n\n    # ----------------------------------------------------------------------\n    # token()\n    #\n    # Method to return individual tokens"}, {"start_line": 15000, "end_line": 17000, "belongs_to": {"file_name": "cpp.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/ply", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n str_expansion:\n                str_expansion[argnum] = ('\"%s\"' % \"\".join([x.value for x in args[argnum]])).replace(\"\\\\\",\"\\\\\\\\\")\n            rep[i] = copy.copy(rep[i])\n            rep[i].value = str_expansion[argnum]\n\n        # Make the variadic macro comma patch.  If the variadic macro argument is empty, we get rid\n        comma_patch = False\n        if macro.variadic and not args[-1]:\n            for i in macro.var_comma_patch:\n                rep[i] = None\n                comma_patch = True\n\n        # Make all other patches.   The order of these matters.  It is assumed that the patch list\n        # has been sorted in reverse order of patch location since replacements will cause the\n        # size of the replacement sequence to expand from the patch point.\n\n        expanded = { }\n        for ptype, argnum, i in macro.patch:\n            # Concatenation.   Argument is left unexpanded\n            if ptype == 'c':\n                rep[i:i+1] = args[argnum]\n            # Normal expansion.  Argument is macro expanded first\n            elif ptype == 'e':\n                if argnum not in expanded:\n                    expanded[argnum] = self.expand_macros(args[argnum])\n                rep[i:i+1] = expanded[argnum]\n\n        # Get rid of removed comma if necessary\n        if comma_patch:\n            rep = [_i for _i in rep if _i]\n\n        return rep\n\n\n    # ----------------------------------------------------------------------\n    # expand_macros()\n    #\n    # Given a list of tokens, this function performs macro expansion.\n    # The expanded argument is a dictionary that contains macros already\n    # expanded.  This is used to prevent infinite recursion.\n    # ----------------------------------------------------------------------\n\n    def expand_macros(self,tokens,expanded=None):\n        if expanded is None:\n            expanded = {}\n        i = 0\n        while i < len(tokens):\n            t = tokens[i]\n            if t.type == self.t_ID:\n                if t.value in self.m"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "cpp.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/ply", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " macro arguments\")\n        return 0, [],[]\n\n    # ----------------------------------------------------------------------\n    # macro_prescan()\n    #\n    # Examine the macro value (token sequence) and identify patch points\n    # This is used to speed up macro expansion later on---we'll know\n    # right away where to apply patches to the value to form the expansion\n    # ----------------------------------------------------------------------\n\n    def macro_prescan(self,macro):\n        macro.patch     = []             # Standard macro arguments\n        macro.str_patch = []             # String conversion expansion\n        macro.var_comma_patch = []       # Variadic macro comma patch\n        i = 0\n        while i < len(macro.value):\n            if macro.value[i].type == self.t_ID and macro.value[i].value in macro.arglist:\n                argnum = macro.arglist.index(macro.value[i].value)\n                # Conversion of argument to a string\n                if i > 0 and macro.value[i-1].value == '#':\n                    macro.value[i] = copy.copy(macro.value[i])\n                    macro.value[i].type = self.t_STRING\n                    del macro.value[i-1]\n                    macro.str_patch.append((argnum,i-1))\n                    continue\n                # Concatenation\n                elif (i > 0 and macro.value[i-1].value == '##'):\n                    macro.patch.append(('c',argnum,i-1))\n                    del macro.value[i-1]\n                    i -= 1\n                    continue\n                elif ((i+1) < len(macro.value) and macro.value[i+1].value == '##'):\n                    macro.patch.append(('c',argnum,i))\n                    del macro.value[i + 1]\n                    continue\n                # Standard expansion\n                else:\n                    macro.patch.append(('e',argnum,i))\n            elif macro.value[i].value == '##':\n                if macro.variadic and (i > 0) and (macro.value[i-1].value == ',') and \\\n                        ((i+1) < l"}, {"start_line": 29000, "end_line": 31000, "belongs_to": {"file_name": "cpp.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/ply", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "            yield tok\n                if dname:\n                    del self.temp_path[0]\n                break\n            except IOError:\n                pass\n        else:\n            print(\"Couldn't find '%s'\" % filename)\n\n    # ----------------------------------------------------------------------\n    # define()\n    #\n    # Define a new macro\n    # ----------------------------------------------------------------------\n\n    def define(self,tokens):\n        if isinstance(tokens,STRING_TYPES):\n            tokens = self.tokenize(tokens)\n\n        linetok = tokens\n        try:\n            name = linetok[0]\n            if len(linetok) > 1:\n                mtype = linetok[1]\n            else:\n                mtype = None\n            if not mtype:\n                m = Macro(name.value,[])\n                self.macros[name.value] = m\n            elif mtype.type in self.t_WS:\n                # A normal macro\n                m = Macro(name.value,self.tokenstrip(linetok[2:]))\n                self.macros[name.value] = m\n            elif mtype.value == '(':\n                # A macro with arguments\n                tokcount, args, positions = self.collect_args(linetok[1:])\n                variadic = False\n                for a in args:\n                    if variadic:\n                        print(\"No more arguments may follow a variadic argument\")\n                        break\n                    astr = \"\".join([str(_i.value) for _i in a])\n                    if astr == \"...\":\n                        variadic = True\n                        a[0].type = self.t_ID\n                        a[0].value = '__VA_ARGS__'\n                        variadic = True\n                        del a[1:]\n                        continue\n                    elif astr[-3:] == \"...\" and a[0].type == self.t_ID:\n                        variadic = True\n                        del a[1:]\n                        # If, for some reason, \".\" is part of the identifier, strip off the name for the purposes\n       "}, {"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "cpp.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/ply", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  Argument is macro expanded first\n            elif ptype == 'e':\n                if argnum not in expanded:\n                    expanded[argnum] = self.expand_macros(args[argnum])\n                rep[i:i+1] = expanded[argnum]\n\n        # Get rid of removed comma if necessary\n        if comma_patch:\n            rep = [_i for _i in rep if _i]\n\n        return rep\n\n\n    # ----------------------------------------------------------------------\n    # expand_macros()\n    #\n    # Given a list of tokens, this function performs macro expansion.\n    # The expanded argument is a dictionary that contains macros already\n    # expanded.  This is used to prevent infinite recursion.\n    # ----------------------------------------------------------------------\n\n    def expand_macros(self,tokens,expanded=None):\n        if expanded is None:\n            expanded = {}\n        i = 0\n        while i < len(tokens):\n            t = tokens[i]\n            if t.type == self.t_ID:\n                if t.value in self.macros and t.value not in expanded:\n                    # Yes, we found a macro match\n                    expanded[t.value] = True\n\n                    m = self.macros[t.value]\n                    if not m.arglist:\n                        # A simple macro\n                        ex = self.expand_macros([copy.copy(_x) for _x in m.value],expanded)\n                        for e in ex:\n                            e.lineno = t.lineno\n                        tokens[i:i+1] = ex\n                        i += len(ex)\n                    else:\n                        # A macro with arguments\n                        j = i + 1\n                        while j < len(tokens) and tokens[j].type in self.t_WS:\n                            j += 1\n                        if j < len(tokens) and tokens[j].value == '(':\n                            tokcount,args,positions = self.collect_args(tokens[j:])\n                            if not m.variadic and len(args) !=  len(m.arglist):\n                              "}, {"start_line": 30000, "end_line": 32000, "belongs_to": {"file_name": "cpp.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/ply", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "f.macros[name.value] = m\n            elif mtype.value == '(':\n                # A macro with arguments\n                tokcount, args, positions = self.collect_args(linetok[1:])\n                variadic = False\n                for a in args:\n                    if variadic:\n                        print(\"No more arguments may follow a variadic argument\")\n                        break\n                    astr = \"\".join([str(_i.value) for _i in a])\n                    if astr == \"...\":\n                        variadic = True\n                        a[0].type = self.t_ID\n                        a[0].value = '__VA_ARGS__'\n                        variadic = True\n                        del a[1:]\n                        continue\n                    elif astr[-3:] == \"...\" and a[0].type == self.t_ID:\n                        variadic = True\n                        del a[1:]\n                        # If, for some reason, \".\" is part of the identifier, strip off the name for the purposes\n                        # of macro expansion\n                        if a[0].value[-3:] == '...':\n                            a[0].value = a[0].value[:-3]\n                        continue\n                    if len(a) > 1 or a[0].type != self.t_ID:\n                        print(\"Invalid macro argument\")\n                        break\n                else:\n                    mvalue = self.tokenstrip(linetok[1+tokcount:])\n                    i = 0\n                    while i < len(mvalue):\n                        if i+1 < len(mvalue):\n                            if mvalue[i].type in self.t_WS and mvalue[i+1].value == '##':\n                                del mvalue[i]\n                                continue\n                            elif mvalue[i].value == '##' and mvalue[i+1].type in self.t_WS:\n                                del mvalue[i+1]\n                        i += 1\n                    m = Macro(name.value,mvalue,[x[0].value for x in args],variadic)\n                    self.macro_pres"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "cpp.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/ply", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ent\ndef t_CPP_COMMENT2(t):\n    r'(//.*?(\\n|$))'\n    # replace with '/n'\n    t.type = 'CPP_WS'; t.value = '\\n'\n    return t\n\ndef t_error(t):\n    t.type = t.value[0]\n    t.value = t.value[0]\n    t.lexer.skip(1)\n    return t\n\nimport re\nimport copy\nimport time\nimport os.path\n\n# -----------------------------------------------------------------------------\n# trigraph()\n#\n# Given an input string, this function replaces all trigraph sequences.\n# The following mapping is used:\n#\n#     ??=    #\n#     ??/    \\\n#     ??'    ^\n#     ??(    [\n#     ??)    ]\n#     ??!    |\n#     ??<    {\n#     ??>    }\n#     ??-    ~\n# -----------------------------------------------------------------------------\n\n_trigraph_pat = re.compile(r'''\\?\\?[=/\\'\\(\\)\\!<>\\-]''')\n_trigraph_rep = {\n    '=':'#',\n    '/':'\\\\',\n    \"'\":'^',\n    '(':'[',\n    ')':']',\n    '!':'|',\n    '<':'{',\n    '>':'}',\n    '-':'~'\n}\n\ndef trigraph(input):\n    return _trigraph_pat.sub(lambda g: _trigraph_rep[g.group()[-1]],input)\n\n# ------------------------------------------------------------------\n# Macro object\n#\n# This object holds information about preprocessor macros\n#\n#    .name      - Macro name (string)\n#    .value     - Macro value (a list of tokens)\n#    .arglist   - List of argument names\n#    .variadic  - Boolean indicating whether or not variadic macro\n#    .vararg    - Name of the variadic parameter\n#\n# When a macro is created, the macro replacement token sequence is\n# pre-scanned and used to create patch lists that are later used\n# during macro expansion\n# ------------------------------------------------------------------\n\nclass Macro(object):\n    def __init__(self,name,value,arglist=None,variadic=False):\n        self.name = name\n        self.value = value\n        self.arglist = arglist\n        self.variadic = variadic\n        if variadic:\n            self.vararg = arglist[-1]\n        self.source = None\n\n# ------------------------------------------------------------------\n# Preprocessor object\n#\n# Object representi"}], "retrieved_count": 10, "cost_time": 1.0274558067321777}
{"question": "How does the method that processes argument values in the custom argparse action class used for command-line options that accept either comma-separated lists or file references in the FITS file comparison script prevent unauthorized access to files outside the intended directory when processing file path arguments that begin with the '@' character?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "fitsdiff.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/scripts", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "gument on the\ncommand-line unless the --exact option is specified.  The FITSDIFF_SETTINGS\nenvironment variable exists to make it easier to change the\nbehavior of fitsdiff on a global level, such as in a set of regression tests.\n\"\"\".strip(),\n    width=80,\n)\n\n\nclass StoreListAction(argparse.Action):\n    def __init__(self, option_strings, dest, nargs=None, **kwargs):\n        if nargs is not None:\n            raise ValueError(\"nargs not allowed\")\n        super().__init__(option_strings, dest, nargs, **kwargs)\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        setattr(namespace, self.dest, [])\n        # Accept either a comma-separated list or a filename (starting with @)\n        # containing a value on each line\n        if values and values[0] == \"@\":\n            value = values[1:]\n            if not os.path.exists(value):\n                log.warning(f\"{self.dest} argument {value} does not exist\")\n                return\n            try:\n                values = [v.strip() for v in open(value)]\n                setattr(namespace, self.dest, values)\n            except OSError as exc:\n                log.warning(\n                    f\"reading {value} for {self.dest} failed: {exc}; \"\n                    \"ignoring this argument\"\n                )\n                del exc\n        else:\n            setattr(namespace, self.dest, [v.strip() for v in values.split(\",\")])\n\n\ndef handle_options(argv=None):\n    parser = argparse.ArgumentParser(\n        description=DESCRIPTION,\n        epilog=EPILOG,\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    # TODO: pass color and suggest_on_error as kwargs when PYTHON_LT_14 is dropped\n    parser.color = True\n    parser.suggest_on_error = True\n\n    parser.add_argument(\n        \"--version\", action=\"version\", version=f\"%(prog)s {__version__}\"\n    )\n\n    parser.add_argument(\n        \"fits_files\", metavar=\"file\", nargs=\"+\", help=\".fits files to process.\"\n    )\n\n    parser.add_argument(\n        \"-q\",\n "}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "fitsdiff.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/scripts", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "[v.strip() for v in open(value)]\n                setattr(namespace, self.dest, values)\n            except OSError as exc:\n                log.warning(\n                    f\"reading {value} for {self.dest} failed: {exc}; \"\n                    \"ignoring this argument\"\n                )\n                del exc\n        else:\n            setattr(namespace, self.dest, [v.strip() for v in values.split(\",\")])\n\n\ndef handle_options(argv=None):\n    parser = argparse.ArgumentParser(\n        description=DESCRIPTION,\n        epilog=EPILOG,\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    # TODO: pass color and suggest_on_error as kwargs when PYTHON_LT_14 is dropped\n    parser.color = True\n    parser.suggest_on_error = True\n\n    parser.add_argument(\n        \"--version\", action=\"version\", version=f\"%(prog)s {__version__}\"\n    )\n\n    parser.add_argument(\n        \"fits_files\", metavar=\"file\", nargs=\"+\", help=\".fits files to process.\"\n    )\n\n    parser.add_argument(\n        \"-q\",\n        \"--quiet\",\n        action=\"store_true\",\n        help=\"Produce no output and just return a status code.\",\n    )\n\n    parser.add_argument(\n        \"-n\",\n        \"--num-diffs\",\n        type=int,\n        default=10,\n        dest=\"numdiffs\",\n        metavar=\"INTEGER\",\n        help=(\n            \"Max number of data differences (image pixel or table element) \"\n            \"to report per extension (default %(default)s).\"\n        ),\n    )\n\n    parser.add_argument(\n        \"-r\",\n        \"--rtol\",\n        \"--relative-tolerance\",\n        type=float,\n        default=None,\n        dest=\"rtol\",\n        metavar=\"NUMBER\",\n        help=(\n            \"The relative tolerance for comparison of two numbers, \"\n            \"specifically two floating point numbers.  This applies to data \"\n            \"in both images and tables, and to floating point keyword values \"\n            \"in headers (default %(default)s).\"\n        ),\n    )\n\n    parser.add_argument(\n        \"-a\",\n        \"--atol\",\n        \"--absol"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "fitsdiff.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/scripts", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rt of the Astropy package. See\nhttps://docs.astropy.org/en/latest/io/fits/usage/scripts.html#fitsdiff\nfor further documentation.\n\"\"\".strip()\n\n\nEPILOG = fill(\n    \"\"\"\nIf the two files are identical within the specified conditions, it will report\n\"No difference is found.\" If the value(s) of -c and -k takes the form\n'@filename', list is in the text file 'filename', and each line in that text\nfile contains one keyword.\n\nExample\n-------\n\n    fitsdiff -k filename,filtnam1 -n 5 -r 1.e-6 test1.fits test2\n\nThis command will compare files test1.fits and test2.fits, report maximum of 5\ndifferent pixels values per extension, only report data values larger than\n1.e-6 relative to each other, and will neglect the different values of keywords\nFILENAME and FILTNAM1 (or their very existence).\n\nfitsdiff command-line arguments can also be set using the environment variable\nFITSDIFF_SETTINGS.  If the FITSDIFF_SETTINGS environment variable is present,\neach argument present will override the corresponding argument on the\ncommand-line unless the --exact option is specified.  The FITSDIFF_SETTINGS\nenvironment variable exists to make it easier to change the\nbehavior of fitsdiff on a global level, such as in a set of regression tests.\n\"\"\".strip(),\n    width=80,\n)\n\n\nclass StoreListAction(argparse.Action):\n    def __init__(self, option_strings, dest, nargs=None, **kwargs):\n        if nargs is not None:\n            raise ValueError(\"nargs not allowed\")\n        super().__init__(option_strings, dest, nargs, **kwargs)\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        setattr(namespace, self.dest, [])\n        # Accept either a comma-separated list or a filename (starting with @)\n        # containing a value on each line\n        if values and values[0] == \"@\":\n            value = values[1:]\n            if not os.path.exists(value):\n                log.warning(f\"{self.dest} argument {value} does not exist\")\n                return\n            try:\n                values = "}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "fitsdiff.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/scripts", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "None]\n\n    for i, path in enumerate(paths):\n        if glob.has_magic(path):\n            files = [os.path.split(f) for f in glob.glob(path)]\n            if not files:\n                log.error(\"Wildcard pattern %r did not match any files.\", path)\n                sys.exit(2)\n\n            dirs, files = list(zip(*files))\n            if len(set(dirs)) > 1:\n                log.error(\"Wildcard pattern %r should match only one directory.\", path)\n                sys.exit(2)\n\n            dirnames[i] = set(dirs).pop()\n            filelists[i] = sorted(files)\n        elif os.path.isdir(path):\n            dirnames[i] = path\n            filelists[i] = [\n                f\n                for f in sorted(os.listdir(path))\n                if os.path.isfile(os.path.join(path, f))\n            ]\n        elif os.path.isfile(path):\n            dirnames[i] = os.path.dirname(path)\n            filelists[i] = [os.path.basename(path)]\n        else:\n            log.error(\n                \"%r is not an existing file, directory, or wildcard \"\n                \"pattern; see `fitsdiff --help` for more usage help.\",\n                path,\n            )\n            sys.exit(2)\n\n        dirnames[i] = os.path.abspath(dirnames[i])\n\n    filematch = set(filelists[0]) & set(filelists[1])\n\n    for a, b in [(0, 1), (1, 0)]:\n        if len(filelists[a]) > len(filematch) and not os.path.isdir(paths[a]):\n            for extra in sorted(set(filelists[a]) - filematch):\n                log.warning(\"%r has no match in %r\", extra, dirnames[b])\n\n    return [\n        (os.path.join(dirnames[0], f), os.path.join(dirnames[1], f)) for f in filematch\n    ]\n\n\ndef main(args=None):\n    args = args or sys.argv[1:]\n\n    if \"FITSDIFF_SETTINGS\" in os.environ:\n        args = os.environ[\"FITSDIFF_SETTINGS\"].split() + args\n\n    opts = handle_options(args)\n\n    if opts.rtol is None:\n        opts.rtol = 0.0\n    if opts.atol is None:\n        opts.atol = 0.0\n\n    if opts.exact_comparisons:\n        # override the options so that each is "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "fitsdiff.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/scripts", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport argparse\nimport glob\nimport logging\nimport os\nimport sys\n\nfrom astropy import __version__\nfrom astropy.io import fits\nfrom astropy.io.fits.util import fill\n\nlog = logging.getLogger(\"fitsdiff\")\n\n\nDESCRIPTION = \"\"\"\nCompare two FITS image files and report the differences in header keywords and\ndata.\n\n    fitsdiff [options] filename1 filename2\n\nwhere filename1 filename2 are the two files to be compared.  They may also be\nwild cards, in such cases, they must be enclosed by double or single quotes, or\nthey may be directory names.  If both are directory names, all files in each of\nthe directories will be included; if only one is a directory name, then the\ndirectory name will be prefixed to the file name(s) specified by the other\nargument.  for example::\n\n    fitsdiff \"*.fits\" \"/machine/data1\"\n\nwill compare all FITS files in the current directory to the corresponding files\nin the directory /machine/data1.\n\nThis script is part of the Astropy package. See\nhttps://docs.astropy.org/en/latest/io/fits/usage/scripts.html#fitsdiff\nfor further documentation.\n\"\"\".strip()\n\n\nEPILOG = fill(\n    \"\"\"\nIf the two files are identical within the specified conditions, it will report\n\"No difference is found.\" If the value(s) of -c and -k takes the form\n'@filename', list is in the text file 'filename', and each line in that text\nfile contains one keyword.\n\nExample\n-------\n\n    fitsdiff -k filename,filtnam1 -n 5 -r 1.e-6 test1.fits test2\n\nThis command will compare files test1.fits and test2.fits, report maximum of 5\ndifferent pixels values per extension, only report data values larger than\n1.e-6 relative to each other, and will neglect the different values of keywords\nFILENAME and FILTNAM1 (or their very existence).\n\nfitsdiff command-line arguments can also be set using the environment variable\nFITSDIFF_SETTINGS.  If the FITSDIFF_SETTINGS environment variable is present,\neach argument present will override the corresponding ar"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "conftest.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see PYFITS.rst\n\nimport os\nimport pathlib\nimport shutil\nimport stat\nimport tempfile\nimport time\n\nimport pytest\n\nfrom astropy.io import fits\n\n\n@pytest.fixture(\n    params=[False, \"str\", \"pathlib\"], ids=[\"\", \"home_is_data\", \"home_is_data, pathlib\"]\n)\ndef home_is_data(request, monkeypatch):\n    \"\"\"\n    Pytest fixture to run a test case both with and without tilde paths.\n\n    In the tilde-path case, calls like self.data('filename.fits') will\n    produce '~/filename.fits', and environment variables will be temporarily\n    modified so that '~' resolves to the data directory.\n    \"\"\"\n    # This checks the value specified in the fixture annotation\n    if request.param:\n        # `request.instance` refers to the test case that's using this fixture.\n        request.instance.monkeypatch = monkeypatch\n        request.instance.set_home_as_data()\n\n        request.instance.set_paths_via_pathlib(request.param == \"pathlib\")\n\n\n@pytest.fixture(\n    params=[False, \"str\", \"pathlib\"], ids=[\"\", \"home_is_data\", \"home_is_data, pathlib\"]\n)\ndef home_is_temp(request, monkeypatch):\n    \"\"\"\n    Pytest fixture to run a test case both with and without tilde paths.\n\n    In the tilde-path case, calls like self.temp('filename.fits') will\n    produce '~/filename.fits', and environment variables will be temporarily\n    modified so that '~' resolves to the temp directory. These files will also\n    be tracked so that, after the test case, we can verify no files were written\n    to a literal tilde path.\n    \"\"\"\n    # This checks the value specified in the fixture annotation\n    if request.param:\n        # `request.instance` refers to the test case that's using this fixture.\n        request.instance.monkeypatch = monkeypatch\n        request.instance.set_home_as_temp()\n\n        request.instance.set_paths_via_pathlib(request.param == \"pathlib\")\n\n\nclass FitsTestCase:\n    def setup_method(self):\n        self.data_dir = os.path.join(os.path.dirname(__file__), \"data"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "fitsdiff.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/scripts", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ile, directory, or wildcard \"\n                \"pattern; see `fitsdiff --help` for more usage help.\",\n                path,\n            )\n            sys.exit(2)\n\n        dirnames[i] = os.path.abspath(dirnames[i])\n\n    filematch = set(filelists[0]) & set(filelists[1])\n\n    for a, b in [(0, 1), (1, 0)]:\n        if len(filelists[a]) > len(filematch) and not os.path.isdir(paths[a]):\n            for extra in sorted(set(filelists[a]) - filematch):\n                log.warning(\"%r has no match in %r\", extra, dirnames[b])\n\n    return [\n        (os.path.join(dirnames[0], f), os.path.join(dirnames[1], f)) for f in filematch\n    ]\n\n\ndef main(args=None):\n    args = args or sys.argv[1:]\n\n    if \"FITSDIFF_SETTINGS\" in os.environ:\n        args = os.environ[\"FITSDIFF_SETTINGS\"].split() + args\n\n    opts = handle_options(args)\n\n    if opts.rtol is None:\n        opts.rtol = 0.0\n    if opts.atol is None:\n        opts.atol = 0.0\n\n    if opts.exact_comparisons:\n        # override the options so that each is the most restrictive\n        opts.ignore_keywords = []\n        opts.ignore_comments = []\n        opts.ignore_fields = []\n        opts.rtol = 0.0\n        opts.atol = 0.0\n        opts.ignore_blanks = False\n        opts.ignore_blank_cards = False\n\n    if not opts.quiet:\n        setup_logging(opts.output_file)\n    files = match_files(opts.fits_files)\n\n    close_file = False\n    if opts.quiet:\n        out_file = None\n    elif opts.output_file:\n        out_file = open(opts.output_file, \"w\")\n        close_file = True\n    else:\n        out_file = sys.stdout\n\n    identical = []\n    try:\n        for a, b in files:\n            # TODO: pass in any additional arguments here too\n            diff = fits.diff.FITSDiff(\n                a,\n                b,\n                ignore_hdus=opts.ignore_hdus,\n                ignore_keywords=opts.ignore_keywords,\n                ignore_comments=opts.ignore_comments,\n                ignore_fields=opts.ignore_fields,\n                numdiffs=opts.numdiffs,\n     "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "file.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "only without mmap and\n# with mmap still have to same behavior with regard to updating the array.  To\n# get a truly readonly mmap use denywrite\n# the name 'denywrite' comes from a deprecated flag to mmap() on Linux--it\n# should be clarified that 'denywrite' mode is not directly analogous to the\n# use of that flag; it was just taken, for lack of anything better, as a name\n# that means something like \"read only\" but isn't readonly.\nMEMMAP_MODES = {\n    \"readonly\": mmap.ACCESS_COPY,\n    \"copyonwrite\": mmap.ACCESS_COPY,\n    \"update\": mmap.ACCESS_WRITE,\n    \"append\": mmap.ACCESS_COPY,\n    \"denywrite\": mmap.ACCESS_READ,\n}\n\n# TODO: Eventually raise a warning, and maybe even later disable the use of\n# 'copyonwrite' and 'denywrite' modes unless memmap=True.  For now, however,\n# that would generate too many warnings for too many users.  If nothing else,\n# wait until the new logging system is in place.\n\nGZIP_MAGIC = b\"\\x1f\\x8b\\x08\"\nPKZIP_MAGIC = b\"\\x50\\x4b\\x03\\x04\"\nBZIP2_MAGIC = b\"\\x42\\x5a\"\nLZMA_MAGIC = b\"\\xfd7zXZ\\x00\"\nLZW_MAGIC = b\"\\x1f\\x9d\"\n\n\ndef _is_bz2file(fileobj):\n    if HAS_BZ2:\n        return isinstance(fileobj, bz2.BZ2File)\n    else:\n        return False\n\n\ndef _is_lzmafile(fileobj):\n    if HAS_LZMA:\n        return isinstance(fileobj, lzma.LZMAFile)\n    else:\n        return False\n\n\ndef _is_lzwfile(fileobj):\n    if HAS_UNCOMPRESSPY:\n        return isinstance(fileobj, uncompresspy.LZWFile)\n    else:\n        return False\n\n\ndef _normalize_fits_mode(mode):\n    if mode is not None and mode not in IO_FITS_MODES:\n        if TEXT_RE.match(mode):\n            raise ValueError(\n                f\"Text mode '{mode}' not supported: files must be opened in binary mode\"\n            )\n        new_mode = FILE_MODES.get(mode)\n        if new_mode not in IO_FITS_MODES:\n            raise ValueError(f\"Mode '{mode}' not recognized\")\n        mode = new_mode\n    return mode\n\n\nclass _File:\n    \"\"\"\n    Represents a FITS file on disk (or in some other file-like object).\n    \"\"\"\n\n    def __init__(\n  "}, {"start_line": 25000, "end_line": 27000, "belongs_to": {"file_name": "util.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ray\n    while hasattr(base, \"base\") and base.base is not None:\n        if isinstance(base.base, mmap.mmap):\n            return base.base\n        base = base.base\n\n\n@contextmanager\ndef _free_space_check(hdulist, dirname=None):\n    try:\n        yield\n    except OSError as exc:\n        error_message = \"\"\n        if not isinstance(hdulist, list):\n            hdulist = [hdulist]\n        if dirname is None:\n            dirname = os.path.dirname(hdulist._file.name)\n        if os.path.isdir(dirname):\n            free_space = data.get_free_space_in_dir(dirname)\n            hdulist_size = sum(hdu.size for hdu in hdulist)\n            if free_space < hdulist_size:\n                error_message = (\n                    f\"Not enough space on disk: requested {hdulist_size}, \"\n                    f\"available {free_space}. \"\n                )\n\n        for hdu in hdulist:\n            hdu._close()\n\n        raise OSError(error_message + str(exc))\n\n\ndef _extract_number(value, default):\n    \"\"\"\n    Attempts to extract an integer number from the given value. If the\n    extraction fails, the value of the 'default' argument is returned.\n    \"\"\"\n    try:\n        # The _str_to_num method converts the value to string/float\n        # so we need to perform one additional conversion to int on top\n        return int(_str_to_num(value))\n    except (TypeError, ValueError):\n        return default\n\n\ndef get_testdata_filepath(filename):\n    \"\"\"\n    Return a string representing the path to the file requested from the\n    io.fits test data set.\n\n    .. versionadded:: 2.0.3\n\n    Parameters\n    ----------\n    filename : str\n        The filename of the test data file.\n\n    Returns\n    -------\n    filepath : str\n        The path to the requested file.\n    \"\"\"\n    return data.get_pkg_data_filename(f\"io/fits/tests/data/{filename}\", \"astropy\")\n\n\ndef _rstrip_inplace(array):\n    \"\"\"\n    Performs an in-place rstrip operation on string arrays. This is necessary\n    since the built-in `np.char.rstrip` in Numpy does n"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "fitsdiff.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/scripts", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n        ),\n    )\n\n    group = parser.add_argument_group(\"Table Comparison Options\")\n\n    group.add_argument(\n        \"-f\",\n        \"--ignore-fields\",\n        action=StoreListAction,\n        default=[],\n        dest=\"ignore_fields\",\n        metavar=\"COLUMNS\",\n        help=(\n            \"Comma-separated list of fields (i.e. columns) not to be \"\n            'compared.  All columns may be excluded using \"*\" as with '\n            \"--ignore-keywords.\"\n        ),\n    )\n\n    options = parser.parse_args(argv)\n\n    # Determine which filenames to compare\n    if len(options.fits_files) != 2:\n        parser.error(\n            \"\\nfitsdiff requires two arguments; see `fitsdiff --help` for more details.\"\n        )\n\n    return options\n\n\ndef setup_logging(outfile=None):\n    log.setLevel(logging.INFO)\n    error_handler = logging.StreamHandler(sys.stderr)\n    error_handler.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n    error_handler.setLevel(logging.WARNING)\n    log.addHandler(error_handler)\n\n    if outfile is not None:\n        output_handler = logging.FileHandler(outfile)\n    else:\n        output_handler = logging.StreamHandler()\n\n        class LevelFilter(logging.Filter):\n            \"\"\"Log only messages matching the specified level.\"\"\"\n\n            def __init__(self, name=\"\", level=logging.NOTSET):\n                logging.Filter.__init__(self, name)\n                self.level = level\n\n            def filter(self, rec):\n                return rec.levelno == self.level\n\n        # File output logs all messages, but stdout logs only INFO messages\n        # (since errors are already logged to stderr)\n        output_handler.addFilter(LevelFilter(level=logging.INFO))\n\n    output_handler.setFormatter(logging.Formatter(\"%(message)s\"))\n    log.addHandler(output_handler)\n\n\ndef match_files(paths):\n    if os.path.isfile(paths[0]) and os.path.isfile(paths[1]):\n        # shortcut if both paths are files\n        return [paths]\n\n    dirnames = [None, None]\n    filelists = [None, "}], "retrieved_count": 10, "cost_time": 1.0158839225769043}
{"question": "What architectural trade-offs does the base metadata container class for masked arrays introduce by delegating the mechanism for choosing serialization strategies to a runtime context manager rather than an inheritance-based approach?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "elf.unmasked.copy()\n        unmasked[self.mask] = fill_value\n        return unmasked\n\n\nclass MaskedInfoBase:\n    mask_val = np.ma.masked\n\n    def __init__(self, bound=False):\n        super().__init__(bound)\n\n        # If bound to a data object instance then create the dict of attributes\n        # which stores the info attribute values.\n        if bound:\n            # Specify how to serialize this object depending on context.\n            self.serialize_method = {\n                \"fits\": \"null_value\",\n                \"ecsv\": \"null_value\",\n                \"hdf5\": \"data_mask\",\n                \"parquet\": \"data_mask\",\n                None: \"null_value\",\n            }\n\n\nclass MaskedNDArrayInfo(MaskedInfoBase, ParentDtypeInfo):\n    \"\"\"\n    Container for meta information like name, description, format.\n    \"\"\"\n\n    # Add `serialize_method` attribute to the attrs that MaskedNDArrayInfo knows\n    # about.  This allows customization of the way that MaskedColumn objects\n    # get written to file depending on format.  The default is to use whatever\n    # the writer would normally do, which in the case of FITS or ECSV is to use\n    # a NULL value within the data itself.  If serialize_method is 'data_mask'\n    # then the mask is explicitly written out as a separate column if there\n    # are any masked values.  This is the same as for MaskedColumn.\n    attr_names = ParentDtypeInfo.attr_names | {\"serialize_method\"}\n\n    # When `serialize_method` is 'data_mask', and data and mask are being written\n    # as separate columns, use column names <name> and <name>.mask (instead\n    # of default encoding as <name>.data and <name>.mask).\n    _represent_as_dict_primary_data = \"data\"\n\n    def _represent_as_dict(self):\n        out = super()._represent_as_dict()\n\n        masked_array = self._parent\n\n        # If the serialize method for this context (e.g. 'fits' or 'ecsv') is\n        # 'data_mask', that means to serialize using an explicit mask column.\n        method = self.serialize_method[self."}, {"start_line": 52000, "end_line": 54000, "belongs_to": {"file_name": "column.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ect instance then create the dict of attributes\n        # which stores the info attribute values.\n        if bound:\n            # Specify how to serialize this object depending on context.\n            self.serialize_method = {\n                \"fits\": \"null_value\",\n                \"ecsv\": \"null_value\",\n                \"hdf5\": \"data_mask\",\n                \"parquet\": \"data_mask\",\n                None: \"null_value\",\n            }\n\n    def _represent_as_dict(self):\n        out = super()._represent_as_dict()\n        # If we are a structured masked column, then our parent class,\n        # ColumnInfo, will already have set up a dict with masked parts,\n        # which will be serialized later, so no further work needed here.\n        if self._parent.dtype.names is not None:\n            return out\n\n        col = self._parent\n\n        # If the serialize method for this context (e.g. 'fits' or 'ecsv') is\n        # 'data_mask', that means to serialize using an explicit mask column.\n        method = self.serialize_method[self._serialize_context]\n\n        if method == \"data_mask\":\n            # Note: a driver here is a performance issue in #8443 where repr() of a\n            # np.ma.MaskedArray value is up to 10 times slower than repr of a normal array\n            # value.  So regardless of whether there are masked elements it is useful to\n            # explicitly define this as a serialized column and use col.data.data (ndarray)\n            # instead of letting it fall through to the \"standard\" serialization machinery.\n            out[\"data\"] = col.data.data\n\n            if np.any(col.mask):\n                # Only if there are actually masked elements do we add the ``mask`` column\n                out[\"mask\"] = col.mask\n\n        elif method == \"null_value\":\n            pass\n\n        else:\n            raise ValueError(\n                'serialize method must be either \"data_mask\" or \"null_value\"'\n            )\n\n        return out\n\n\nclass MaskedColumn(Column, _MaskedColumnGetitemShim,"}, {"start_line": 15000, "end_line": 17000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "pending on format.  The default is to use whatever\n    # the writer would normally do, which in the case of FITS or ECSV is to use\n    # a NULL value within the data itself.  If serialize_method is 'data_mask'\n    # then the mask is explicitly written out as a separate column if there\n    # are any masked values.  This is the same as for MaskedColumn.\n    attr_names = ParentDtypeInfo.attr_names | {\"serialize_method\"}\n\n    # When `serialize_method` is 'data_mask', and data and mask are being written\n    # as separate columns, use column names <name> and <name>.mask (instead\n    # of default encoding as <name>.data and <name>.mask).\n    _represent_as_dict_primary_data = \"data\"\n\n    def _represent_as_dict(self):\n        out = super()._represent_as_dict()\n\n        masked_array = self._parent\n\n        # If the serialize method for this context (e.g. 'fits' or 'ecsv') is\n        # 'data_mask', that means to serialize using an explicit mask column.\n        method = self.serialize_method[self._serialize_context]\n\n        if method == \"data_mask\":\n            out[\"data\"] = masked_array.unmasked\n\n            if np.any(masked_array.mask):\n                # Only if there are actually masked elements do we add the ``mask`` column\n                out[\"mask\"] = masked_array.mask\n\n        elif method == \"null_value\":\n            out[\"data\"] = np.ma.MaskedArray(\n                masked_array.unmasked, mask=masked_array.mask\n            )\n\n        else:\n            raise ValueError(\n                'serialize method must be either \"data_mask\" or \"null_value\"'\n            )\n\n        return out\n\n    def _construct_from_dict(self, map):\n        # Override usual handling, since MaskedNDArray takes shape and buffer\n        # as input, which is less useful here.\n        # The map can contain either a MaskedColumn or a Column and a mask.\n        # Extract the mask for the former case.\n        map.setdefault(\"mask\", getattr(map[\"data\"], \"mask\", False))\n        return self._parent_cls.from_unm"}, {"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_serialize_context]\n\n        if method == \"data_mask\":\n            out[\"data\"] = masked_array.unmasked\n\n            if np.any(masked_array.mask):\n                # Only if there are actually masked elements do we add the ``mask`` column\n                out[\"mask\"] = masked_array.mask\n\n        elif method == \"null_value\":\n            out[\"data\"] = np.ma.MaskedArray(\n                masked_array.unmasked, mask=masked_array.mask\n            )\n\n        else:\n            raise ValueError(\n                'serialize method must be either \"data_mask\" or \"null_value\"'\n            )\n\n        return out\n\n    def _construct_from_dict(self, map):\n        # Override usual handling, since MaskedNDArray takes shape and buffer\n        # as input, which is less useful here.\n        # The map can contain either a MaskedColumn or a Column and a mask.\n        # Extract the mask for the former case.\n        map.setdefault(\"mask\", getattr(map[\"data\"], \"mask\", False))\n        return self._parent_cls.from_unmasked(**map)\n\n\nclass MaskedArraySubclassInfo(MaskedInfoBase):\n    \"\"\"Mixin class to create a subclasses such as MaskedQuantityInfo.\"\"\"\n\n    # This is used below in __init_subclass__, which also inserts a\n    # 'serialize_method' attribute in attr_names.\n\n    def _represent_as_dict(self):\n        # Use the data_cls as the class name for serialization,\n        # so that we do not have to store all possible masked classes\n        # in astropy.table.serialize.__construct_mixin_classes.\n        out = super()._represent_as_dict()\n        data_cls = self._parent._data_cls\n        out.setdefault(\"__class__\", data_cls.__module__ + \".\" + data_cls.__name__)\n        return out\n\n\ndef _comparison_method(op):\n    \"\"\"\n    Create a comparison operator for MaskedNDArray.\n\n    Needed since for string dtypes the base operators bypass __array_ufunc__\n    and hence return unmasked results.\n    \"\"\"\n\n    def _compare(self, other):\n        other_data, other_mask = get_data_and_mask(other)\n        result = geta"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ked(self):\n        \"\"\"Whether or not the instance uses masked values.\"\"\"\n\n    @property\n    @abc.abstractmethod\n    def mask(self):\n        \"\"\"The mask.\"\"\"\n\n    @property\n    def unmasked(self):\n        \"\"\"Get an instance without the mask.\n\n        Note that while one gets a new instance, the underlying data will be shared.\n\n        See Also\n        --------\n        filled : get a copy of the underlying data, with masked values filled in.\n        \"\"\"\n        return self._apply(lambda x: getattr(x, \"unmasked\", x))\n\n    def filled(self, fill_value):\n        \"\"\"Get a copy of the underlying data, with masked values filled in.\n\n        Parameters\n        ----------\n        fill_value : object\n            Value to replace masked values with.\n\n        Returns\n        -------\n        filled : instance\n            Copy of ``self`` with masked items replaced by ``fill_value``.\n\n        See Also\n        --------\n        unmasked : get an instance without the mask.\n        \"\"\"\n        unmasked = self.unmasked.copy()\n        unmasked[self.mask] = fill_value\n        return unmasked\n\n\nclass MaskedInfoBase:\n    mask_val = np.ma.masked\n\n    def __init__(self, bound=False):\n        super().__init__(bound)\n\n        # If bound to a data object instance then create the dict of attributes\n        # which stores the info attribute values.\n        if bound:\n            # Specify how to serialize this object depending on context.\n            self.serialize_method = {\n                \"fits\": \"null_value\",\n                \"ecsv\": \"null_value\",\n                \"hdf5\": \"data_mask\",\n                \"parquet\": \"data_mask\",\n                None: \"null_value\",\n            }\n\n\nclass MaskedNDArrayInfo(MaskedInfoBase, ParentDtypeInfo):\n    \"\"\"\n    Container for meta information like name, description, format.\n    \"\"\"\n\n    # Add `serialize_method` attribute to the attrs that MaskedNDArrayInfo knows\n    # about.  This allows customization of the way that MaskedColumn objects\n    # get written to file de"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n out\n\n\nclass TimeInfoBase(MixinInfo):\n    \"\"\"\n    Container for meta information like name, description, format.  This is\n    required when the object is used as a mixin column within a table, but can\n    be used as a general way to store meta information.\n\n    This base class is common between TimeInfo and TimeDeltaInfo.\n    \"\"\"\n\n    attr_names = MixinInfo.attr_names | {\"serialize_method\"}\n    _supports_indexing = True\n\n    # The usual tuple of attributes needed for serialization is replaced\n    # by a property, since Time can be serialized different ways.\n    _represent_as_dict_extra_attrs = (\n        \"format\",\n        \"scale\",\n        \"precision\",\n        \"in_subfmt\",\n        \"out_subfmt\",\n        \"location\",\n        \"_delta_ut1_utc\",\n        \"_delta_tdb_tt\",\n    )\n\n    # When serializing, write out the `value` attribute using the column name.\n    _represent_as_dict_primary_data = \"value\"\n\n    mask_val = np.ma.masked\n\n    @property\n    def _represent_as_dict_attrs(self):\n        method = self.serialize_method[self._serialize_context]\n\n        if method == \"formatted_value\":\n            out = (\"value\",)\n        elif method == \"jd1_jd2\":\n            out = (\"jd1\", \"jd2\")\n        else:\n            raise ValueError(\"serialize method must be 'formatted_value' or 'jd1_jd2'\")\n\n        return out + self._represent_as_dict_extra_attrs\n\n    def __init__(self, bound=False):\n        super().__init__(bound)\n\n        # If bound to a data object instance then create the dict of attributes\n        # which stores the info attribute values.\n        if bound:\n            # Specify how to serialize this object depending on context.\n            # If ``True`` for a context, then use formatted ``value`` attribute\n            # (e.g. the ISO time string).  If ``False`` then use float jd1 and jd2.\n            self.serialize_method = {\n                \"fits\": \"jd1_jd2\",\n                \"ecsv\": \"formatted_value\",\n                \"hdf5\": \"jd1_jd2\",\n                \"yaml\": \"jd1_jd2\",\n       "}, {"start_line": 51000, "end_line": 53000, "belongs_to": {"file_name": "column.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " attribute.\n    \"\"\"\n\n    # Add `serialize_method` attribute to the attrs that MaskedColumnInfo knows\n    # about.  This allows customization of the way that MaskedColumn objects\n    # get written to file depending on format.  The default is to use whatever\n    # the writer would normally do, which in the case of FITS or ECSV is to use\n    # a NULL value within the data itself.  If serialize_method is 'data_mask'\n    # then the mask is explicitly written out as a separate column if there\n    # are any masked values.  See also code below.\n    attr_names = ColumnInfo.attr_names | {\"serialize_method\"}\n\n    # When `serialize_method` is 'data_mask', and data and mask are being written\n    # as separate columns, use column names <name> and <name>.mask (instead\n    # of default encoding as <name>.data and <name>.mask).\n    _represent_as_dict_primary_data = \"data\"\n\n    mask_val = np.ma.masked\n\n    def __init__(self, bound=False):\n        super().__init__(bound)\n\n        # If bound to a data object instance then create the dict of attributes\n        # which stores the info attribute values.\n        if bound:\n            # Specify how to serialize this object depending on context.\n            self.serialize_method = {\n                \"fits\": \"null_value\",\n                \"ecsv\": \"null_value\",\n                \"hdf5\": \"data_mask\",\n                \"parquet\": \"data_mask\",\n                None: \"null_value\",\n            }\n\n    def _represent_as_dict(self):\n        out = super()._represent_as_dict()\n        # If we are a structured masked column, then our parent class,\n        # ColumnInfo, will already have set up a dict with masked parts,\n        # which will be serialized later, so no further work needed here.\n        if self._parent.dtype.names is not None:\n            return out\n\n        col = self._parent\n\n        # If the serialize method for this context (e.g. 'fits' or 'ecsv') is\n        # 'data_mask', that means to serialize using an explicit mask column.\n        method = "}, {"start_line": 50000, "end_line": 52000, "belongs_to": {"file_name": "column.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "vidually.\n            data = np.insert(self, obj, None, axis=axis)\n            data[obj] = values\n        else:\n            self_for_insert = _expand_string_array_for_values(self, values)\n            data = np.insert(self_for_insert, obj, values, axis=axis)\n\n        out = data.view(self.__class__)\n        out.__array_finalize__(self)\n        return out\n\n    # We do this to make the methods show up in the API docs\n    name = BaseColumn.name\n    unit = BaseColumn.unit\n    copy = BaseColumn.copy\n    more = BaseColumn.more\n    pprint = BaseColumn.pprint\n    pformat = BaseColumn.pformat\n    convert_unit_to = BaseColumn.convert_unit_to\n    quantity = BaseColumn.quantity\n    to = BaseColumn.to\n\n\nclass MaskedColumnInfo(ColumnInfo):\n    \"\"\"\n    Container for meta information like name, description, format.\n\n    This is required when the object is used as a mixin column within a table,\n    but can be used as a general way to store meta information.  In this case\n    it just adds the ``mask_val`` attribute.\n    \"\"\"\n\n    # Add `serialize_method` attribute to the attrs that MaskedColumnInfo knows\n    # about.  This allows customization of the way that MaskedColumn objects\n    # get written to file depending on format.  The default is to use whatever\n    # the writer would normally do, which in the case of FITS or ECSV is to use\n    # a NULL value within the data itself.  If serialize_method is 'data_mask'\n    # then the mask is explicitly written out as a separate column if there\n    # are any masked values.  See also code below.\n    attr_names = ColumnInfo.attr_names | {\"serialize_method\"}\n\n    # When `serialize_method` is 'data_mask', and data and mask are being written\n    # as separate columns, use column names <name> and <name>.mask (instead\n    # of default encoding as <name>.data and <name>.mask).\n    _represent_as_dict_primary_data = \"data\"\n\n    mask_val = np.ma.masked\n\n    def __init__(self, bound=False):\n        super().__init__(bound)\n\n        # If bound to a data obj"}, {"start_line": 53000, "end_line": 55000, "belongs_to": {"file_name": "column.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "self.serialize_method[self._serialize_context]\n\n        if method == \"data_mask\":\n            # Note: a driver here is a performance issue in #8443 where repr() of a\n            # np.ma.MaskedArray value is up to 10 times slower than repr of a normal array\n            # value.  So regardless of whether there are masked elements it is useful to\n            # explicitly define this as a serialized column and use col.data.data (ndarray)\n            # instead of letting it fall through to the \"standard\" serialization machinery.\n            out[\"data\"] = col.data.data\n\n            if np.any(col.mask):\n                # Only if there are actually masked elements do we add the ``mask`` column\n                out[\"mask\"] = col.mask\n\n        elif method == \"null_value\":\n            pass\n\n        else:\n            raise ValueError(\n                'serialize method must be either \"data_mask\" or \"null_value\"'\n            )\n\n        return out\n\n\nclass MaskedColumn(Column, _MaskedColumnGetitemShim, ma.MaskedArray):\n    \"\"\"Define a masked data column for use in a Table object.\n\n    Parameters\n    ----------\n    data : list, ndarray, or None\n        Column data values\n    name : str\n        Column name and key for reference within Table\n    mask : list, ndarray or None\n        Boolean mask for which True indicates missing or invalid data\n    fill_value : float, int, str, or None\n        Value used when filling masked column elements\n    dtype : `~numpy.dtype`-like\n        Data type for column\n    shape : tuple or ()\n        Dimensions of a single row element in the column data\n    length : int or 0\n        Number of row elements in column data\n    description : str or None\n        Full description of column\n    unit : str or None\n        Physical unit\n    format : str, None, or callable\n        Format string for outputting column values.  This can be an\n        \"old-style\" (``format % value``) or \"new-style\" (`str.format`)\n        format specification string or a function or any c"}, {"start_line": 17000, "end_line": 19000, "belongs_to": {"file_name": "data_info.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "arnings.filterwarnings(\"ignore\", **ignore_kwargs)\n                info.update(option_(dat))\n\n        if hasattr(dat, \"mask\"):\n            n_bad = np.count_nonzero(dat.mask)\n        else:\n            try:\n                n_bad = np.count_nonzero(np.isinf(dat) | np.isnan(dat))\n            except Exception:\n                n_bad = 0\n        info[\"n_bad\"] = n_bad\n\n        try:\n            info[\"length\"] = len(dat)\n        except (TypeError, IndexError):\n            pass\n\n        if out is None:\n            return info\n\n        for key, val in info.items():\n            if val != \"\":\n                out.write(f\"{key} = {val}\" + os.linesep)\n\n    def __repr__(self):\n        if self._parent is None:\n            return super().__repr__()\n\n        out = StringIO()\n        self.__call__(out=out)\n        return out.getvalue()\n\n\nclass BaseColumnInfo(DataInfo):\n    \"\"\"Base info class for anything that can be a column in an astropy Table.\n\n    There are at least two classes that inherit from this:\n\n      ColumnInfo: for native astropy Column / MaskedColumn objects\n      MixinInfo: for mixin column objects\n\n    Note that this class is defined here so that mixins can use it\n    without importing the table package.\n    \"\"\"\n\n    attr_names = DataInfo.attr_names | {\"parent_table\", \"indices\"}\n    _attrs_no_copy = {\"parent_table\", \"indices\"}\n\n    # Context for serialization.  This can be set temporarily via\n    # ``serialize_context_as(context)`` context manager to allow downstream\n    # code to understand the context in which a column is being serialized.\n    # Typical values are 'fits', 'hdf5', 'parquet', 'ecsv', 'yaml'.  Objects\n    # like Time or SkyCoord will have different default serialization\n    # representations depending on context.\n    _serialize_context = None\n    __slots__ = [\"_copy_indices\", \"_format_funcs\"]\n\n    @property\n    def parent_table(self):\n        value = self._attrs.get(\"parent_table\")\n        if callable(value):\n            value = value()\n        return value\n"}], "retrieved_count": 10, "cost_time": 1.0456840991973877}
{"question": "What architectural pattern does the header initialization process use to prevent modifications when creating new header data unit instances from existing headers in the FITS file I/O module?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 70000, "end_line": 72000, "belongs_to": {"file_name": "header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e HDU creation process (NAXIS, XTENSION, ...).\n\n    \"\"\"\n\n    def __init__(self, cards):\n        # dict of (keywords, card images)\n        self._raw_cards = cards\n        self._keys = list(cards.keys())\n        # dict of (keyword, Card object) storing the parsed cards\n        self._cards = {}\n        # the _BasicHeaderCards object allows to access Card objects from\n        # keyword indices\n        self.cards = _BasicHeaderCards(self)\n\n        self._modified = False\n\n    def __getitem__(self, key):\n        if isinstance(key, numbers.Integral):\n            key = self._keys[key]\n\n        try:\n            return self._cards[key].value\n        except KeyError:\n            # parse the Card and store it\n            cardstr = self._raw_cards[key]\n            self._cards[key] = card = Card.fromstring(cardstr)\n            return card.value\n\n    def __len__(self):\n        return len(self._raw_cards)\n\n    def __iter__(self):\n        return iter(self._raw_cards)\n\n    def index(self, keyword):\n        return self._keys.index(keyword)\n\n    @property\n    def data_size(self):\n        \"\"\"\n        Return the size (in bytes) of the data portion following the `Header`.\n        \"\"\"\n        return _hdr_data_size(self)\n\n    @property\n    def data_size_padded(self):\n        \"\"\"\n        Return the size (in bytes) of the data portion following the `Header`\n        including padding.\n        \"\"\"\n        size = self.data_size\n        return size + _pad_length(size)\n\n    @classmethod\n    def fromfile(cls, fileobj):\n        \"\"\"The main method to parse a FITS header from a file. The parsing is\n        done with the parse_header function implemented in Cython.\n        \"\"\"\n        close_file = False\n        if isinstance(fileobj, str):\n            fileobj = open(fileobj, \"rb\")\n            close_file = True\n\n        try:\n            header_str, cards = parse_header(fileobj)\n            _check_padding(header_str, BLOCK_SIZE, False)\n            return header_str, cls(cards)\n        finally:\n           "}, {"start_line": 28000, "end_line": 30000, "belongs_to": {"file_name": "header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "y` on a `Header` will call this\n            method.\n\n        Parameters\n        ----------\n        strip : bool, optional\n            If `True`, strip any headers that are specific to one of the\n            standard HDU types, so that this header can be used in a different\n            HDU.\n\n        Returns\n        -------\n        `Header`\n            A new :class:`Header` instance.\n        \"\"\"\n        tmp = self.__class__(copy.copy(card) for card in self._cards)\n        if strip:\n            tmp.strip()\n        return tmp\n\n    def __copy__(self):\n        return self.copy()\n\n    def __deepcopy__(self, *args, **kwargs):\n        return self.copy()\n\n    @classmethod\n    def fromkeys(cls, iterable, value=None):\n        \"\"\"\n        Similar to :meth:`dict.fromkeys`--creates a new `Header` from an\n        iterable of keywords and an optional default value.\n\n        This method is not likely to be particularly useful for creating real\n        world FITS headers, but it is useful for testing.\n\n        Parameters\n        ----------\n        iterable\n            Any iterable that returns strings representing FITS keywords.\n\n        value : optional\n            A default value to assign to each keyword; must be a valid type for\n            FITS keywords.\n\n        Returns\n        -------\n        `Header`\n            A new `Header` instance.\n        \"\"\"\n        d = cls()\n        if not isinstance(value, tuple):\n            value = (value,)\n        for key in iterable:\n            d.append((key,) + value)\n        return d\n\n    def get(self, key, default=None):\n        \"\"\"\n        Similar to :meth:`dict.get`--returns the value associated with keyword\n        in the header, or a default value if the keyword is not found.\n\n        Parameters\n        ----------\n        key : str\n            A keyword that may or may not be in the header.\n\n        default : optional\n            A default value to return if the keyword is not found in the\n            header.\n\n        Returns\n        -----"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/hdu", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n    def __init__(self, data=None, header=None, *args, **kwargs):\n        if header is None:\n            header = Header()\n        self._header = header\n        self._header_str = None\n        self._file = None\n        self._buffer = None\n        self._header_offset = None\n        self._data_offset = None\n        self._data_size = None\n\n        # This internal variable is used to track whether the data attribute\n        # still points to the same data array as when the HDU was originally\n        # created (this does not track whether the data is actually the same\n        # content-wise)\n        self._data_replaced = False\n        self._data_needs_rescale = False\n        self._new = True\n        self._output_checksum = False\n\n        if \"DATASUM\" in self._header and \"CHECKSUM\" not in self._header:\n            self._output_checksum = \"datasum\"\n        elif \"CHECKSUM\" in self._header:\n            self._output_checksum = True\n\n    def __init_subclass__(cls, **kwargs):\n        # Add the same data.deleter to all HDUs with a data property.\n        # It's unfortunate, but there's otherwise no straightforward way\n        # that a property can inherit setters/deleters of the property of the\n        # same name on base classes.\n        data_prop = cls.__dict__.get(\"data\", None)\n        if isinstance(data_prop, (lazyproperty, property)) and data_prop.fdel is None:\n            # Don't do anything if the class has already explicitly\n            # set the deleter for its data property\n            def data(self):\n                # The deleter\n                if self._file is not None and self._data_loaded:\n                    # sys.getrefcount is CPython specific and not on PyPy.\n                    has_getrefcount = hasattr(sys, \"getrefcount\")\n                    if has_getrefcount:\n                        data_refcount = sys.getrefcount(self.data)\n\n                    # Manually delete *now* so that FITS_rec.__del__\n                    # cleanup can happen if applicable\n         "}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/hdu", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "           continue\n            except Exception as exc:\n                warnings.warn(\n                    \"An exception occurred matching an HDU header to the \"\n                    f\"appropriate HDU type: {exc}\",\n                    AstropyUserWarning,\n                )\n                warnings.warn(\n                    \"The HDU will be treated as corrupted.\", AstropyUserWarning\n                )\n                klass = _CorruptedHDU\n                del exc\n                break\n\n    return klass\n\n\n# TODO: Come up with a better __repr__ for HDUs (and for HDULists, for that\n# matter)\nclass _BaseHDU:\n    \"\"\"Base class for all HDU (header data unit) classes.\"\"\"\n\n    _hdu_registry = set()\n\n    # This HDU type is part of the FITS standard\n    _standard = True\n\n    # Byte to use for padding out blocks\n    _padding_byte = \"\\x00\"\n\n    _default_name = \"\"\n\n    # _header uses a descriptor to delay the loading of the fits.Header object\n    # until it is necessary.\n    _header = _DelayedHeader()\n\n    def __init__(self, data=None, header=None, *args, **kwargs):\n        if header is None:\n            header = Header()\n        self._header = header\n        self._header_str = None\n        self._file = None\n        self._buffer = None\n        self._header_offset = None\n        self._data_offset = None\n        self._data_size = None\n\n        # This internal variable is used to track whether the data attribute\n        # still points to the same data array as when the HDU was originally\n        # created (this does not track whether the data is actually the same\n        # content-wise)\n        self._data_replaced = False\n        self._data_needs_rescale = False\n        self._new = True\n        self._output_checksum = False\n\n        if \"DATASUM\" in self._header and \"CHECKSUM\" not in self._header:\n            self._output_checksum = \"datasum\"\n        elif \"CHECKSUM\" in self._header:\n            self._output_checksum = True\n\n    def __init_subclass__(cls, **kwargs):\n        # Add the sam"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_image.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "geHDU(header=hdr, name=\"FOO\")\n        assert hdu.name == \"FOO\"\n        assert hdu.header[\"EXTNAME\"] == \"FOO\"\n\n    def test_constructor_ver_arg(self):\n        def assert_ver_is(hdu, reference_ver):\n            __tracebackhide__ = True\n            assert hdu.ver == reference_ver\n            assert hdu.header[\"EXTVER\"] == reference_ver\n\n        hdu = fits.ImageHDU()\n        assert hdu.ver == 1  # defaults to 1\n        assert \"EXTVER\" not in hdu.header\n\n        hdu.ver = 1\n        assert_ver_is(hdu, 1)\n\n        # Passing name to constructor\n        hdu = fits.ImageHDU(ver=2)\n        assert_ver_is(hdu, 2)\n\n        # And overriding a header with a different extver\n        hdr = fits.Header()\n        hdr[\"EXTVER\"] = 3\n        hdu = fits.ImageHDU(header=hdr, ver=4)\n        assert_ver_is(hdu, 4)\n\n        # The header card is not overridden if ver is None or not passed in\n        hdr = fits.Header()\n        hdr[\"EXTVER\"] = 5\n        hdu = fits.ImageHDU(header=hdr, ver=None)\n        assert_ver_is(hdu, 5)\n        hdu = fits.ImageHDU(header=hdr)\n        assert_ver_is(hdu, 5)\n\n    def test_constructor_copies_header(self):\n        \"\"\"\n        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/153\n\n        Ensure that a header from one HDU is copied when used to initialize new\n        HDU.\n        \"\"\"\n\n        ifd = fits.HDUList(fits.PrimaryHDU())\n        phdr = ifd[0].header\n        phdr[\"FILENAME\"] = \"labq01i3q_rawtag.fits\"\n\n        primary_hdu = fits.PrimaryHDU(header=phdr)\n        ofd = fits.HDUList(primary_hdu)\n        ofd[0].header[\"FILENAME\"] = \"labq01i3q_flt.fits\"\n\n        # Original header should be unchanged\n        assert phdr[\"FILENAME\"] == \"labq01i3q_rawtag.fits\"\n\n    def test_open(self):\n        # The function \"open\" reads a FITS file into an HDUList object.  There\n        # are three modes to open: \"readonly\" (the default), \"append\", and\n        # \"update\".\n\n        # Open a file read-only (the default mode), the content of the FITS\n        # file are "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see PYFITS.rst\n\nimport collections\nimport copy\nimport warnings\nfrom io import BytesIO, StringIO\n\nimport numpy as np\nimport pytest\n\nfrom astropy.io import fits\nfrom astropy.io.fits.card import _pad\nfrom astropy.io.fits.header import _pad_length\nfrom astropy.io.fits.scripts import fitsheader\nfrom astropy.io.fits.util import encode_ascii\nfrom astropy.io.fits.verify import VerifyError, VerifyWarning\nfrom astropy.utils.exceptions import AstropyUserWarning\nfrom astropy.utils.misc import _NOT_OVERWRITING_MSG_MATCH\n\nfrom .conftest import FitsTestCase\n\n\ndef test_shallow_copy():\n    \"\"\"Make sure that operations on a shallow copy do not alter the original.\n    #4990.\"\"\"\n    original_header = fits.Header([(\"a\", 1), (\"b\", 1)])\n    copied_header = copy.copy(original_header)\n\n    # Modifying the original dict should not alter the copy\n    original_header[\"c\"] = 100\n    assert \"c\" not in copied_header\n\n    # and changing the copy should not change the original.\n    copied_header[\"a\"] = 0\n    assert original_header[\"a\"] == 1\n\n\ndef test_init_with_header():\n    \"\"\"Make sure that creating a Header from another Header makes a copy if\n    copy is True.\"\"\"\n\n    original_header = fits.Header([(\"a\", 10)])\n    new_header = fits.Header(original_header, copy=True)\n    original_header[\"a\"] = 20\n    assert new_header[\"a\"] == 10\n\n    new_header[\"a\"] = 0\n    assert original_header[\"a\"] == 20\n\n\ndef test_init_with_dict():\n    dict1 = {\"a\": 11, \"b\": 12, \"c\": 13, \"d\": 14, \"e\": 15}\n    h1 = fits.Header(dict1)\n    for i, expected in dict1.items():\n        assert h1[i] == expected\n\n\ndef test_init_with_ordereddict():\n    # Create a list of tuples. Each tuple consisting of a letter and the number\n    list1 = [(i, j) for j, i in enumerate(\"abcdefghijklmnopqrstuvwxyz\")]\n    # Create an ordered dictionary and a header from this dictionary\n    dict1 = collections.OrderedDict(list1)\n    h1 = fits.Header(dict1)\n    # Check that the order is preserved of the initial"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_image.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "(hdu, 5)\n        hdu = fits.ImageHDU(header=hdr)\n        assert_ver_is(hdu, 5)\n\n    def test_constructor_copies_header(self):\n        \"\"\"\n        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/153\n\n        Ensure that a header from one HDU is copied when used to initialize new\n        HDU.\n        \"\"\"\n\n        ifd = fits.HDUList(fits.PrimaryHDU())\n        phdr = ifd[0].header\n        phdr[\"FILENAME\"] = \"labq01i3q_rawtag.fits\"\n\n        primary_hdu = fits.PrimaryHDU(header=phdr)\n        ofd = fits.HDUList(primary_hdu)\n        ofd[0].header[\"FILENAME\"] = \"labq01i3q_flt.fits\"\n\n        # Original header should be unchanged\n        assert phdr[\"FILENAME\"] == \"labq01i3q_rawtag.fits\"\n\n    def test_open(self):\n        # The function \"open\" reads a FITS file into an HDUList object.  There\n        # are three modes to open: \"readonly\" (the default), \"append\", and\n        # \"update\".\n\n        # Open a file read-only (the default mode), the content of the FITS\n        # file are read into memory.\n        r = fits.open(self.data(\"test0.fits\"))  # readonly\n\n        # data parts are latent instantiation, so if we close the HDUList\n        # without touching data, data can not be accessed.\n        r.close()\n\n        with pytest.raises(IndexError) as exc_info:\n            r[1].data[:2, :2]\n\n        # Check that the exception message is the enhanced version, not the\n        # default message from list.__getitem__\n        assert str(exc_info.value) == (\n            \"HDU not found, possibly because the index \"\n            \"is out of range, or because the file was \"\n            \"closed before all HDUs were read\"\n        )\n\n    def test_open_2(self):\n        r = fits.open(self.data(\"test0.fits\"))\n\n        info = [(0, \"PRIMARY\", 1, \"PrimaryHDU\", 138, (), \"\", \"\")] + [\n            (x, \"SCI\", x, \"ImageHDU\", 61, (40, 40), \"int16\", \"\") for x in range(1, 5)\n        ]\n\n        try:\n            assert r.info(output=False) == info\n        finally:\n            r.close()\n\n    def t"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " keyword, in the case where there\n    are duplicate keywords.\n\n    For example::\n\n        >>> header['NAXIS']\n        0\n        >>> header[('FOO', 1)]  # Return the value of the second FOO keyword\n        'foo'\n\n    The header may also be indexed by card number::\n\n        >>> header[0]  # Return the value of the first card in the header\n        'T'\n\n    Commentary keywords such as HISTORY and COMMENT are special cases: When\n    indexing the Header object with either 'HISTORY' or 'COMMENT' a list of all\n    the HISTORY/COMMENT values is returned::\n\n        >>> header['HISTORY']\n        This is the first history entry in this header.\n        This is the second history entry in this header.\n        ...\n\n    See the Astropy documentation for more details on working with headers.\n\n    Notes\n    -----\n    Although FITS keywords must be exclusively upper case, retrieving an item\n    in a `Header` object is case insensitive.\n    \"\"\"\n\n    def __init__(self, cards=[], copy=False):\n        \"\"\"\n        Construct a `Header` from an iterable and/or text file.\n\n        Parameters\n        ----------\n        cards : list of `Card`, optional\n            The cards to initialize the header with. Also allowed are other\n            `Header` (or `dict`-like) objects.\n\n            .. versionchanged:: 1.2\n                Allowed ``cards`` to be a `dict`-like object.\n\n        copy : bool, optional\n\n            If ``True`` copies the ``cards`` if they were another `Header`\n            instance.\n            Default is ``False``.\n\n            .. versionadded:: 1.3\n        \"\"\"\n        self.clear()\n\n        if isinstance(cards, Header):\n            if copy:\n                cards = cards.copy()\n            cards = cards.cards\n        elif isinstance(cards, dict):\n            cards = cards.items()\n\n        for card in cards:\n            self.append(card, end=True)\n\n        self._modified = False\n\n    def __len__(self):\n        return len(self._cards)\n\n    def __iter__(self):\n        for card in sel"}, {"start_line": 68000, "end_line": 70000, "belongs_to": {"file_name": "header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " obj._header_str is not None:\n                hdr = Header.fromstring(obj._header_str)\n                obj._header_str = None\n            else:\n                raise AttributeError(\n                    f\"'{type(obj).__name__}' object has no attribute '_header'\"\n                )\n\n            obj.__dict__[\"_header\"] = hdr\n            return hdr\n\n    def __set__(self, obj, val):\n        obj.__dict__[\"_header\"] = val\n\n    def __delete__(self, obj):\n        del obj.__dict__[\"_header\"]\n\n\nclass _BasicHeaderCards:\n    \"\"\"\n    This class allows to access cards with the _BasicHeader.cards attribute.\n\n    This is needed because during the HDU class detection, some HDUs uses\n    the .cards interface.  Cards cannot be modified here as the _BasicHeader\n    object will be deleted once the HDU object is created.\n\n    \"\"\"\n\n    def __init__(self, header):\n        self.header = header\n\n    def __getitem__(self, key):\n        # .cards is a list of cards, so key here is an integer.\n        # get the keyword name from its index.\n        key = self.header._keys[key]\n        # then we get the card from the _BasicHeader._cards list, or parse it\n        # if needed.\n        try:\n            return self.header._cards[key]\n        except KeyError:\n            cardstr = self.header._raw_cards[key]\n            card = Card.fromstring(cardstr)\n            self.header._cards[key] = card\n            return card\n\n\nclass _BasicHeader(collections.abc.Mapping):\n    \"\"\"This class provides a fast header parsing, without all the additional\n    features of the Header class. Here only standard keywords are parsed, no\n    support for CONTINUE, HIERARCH, COMMENT, HISTORY, or rvkc.\n\n    The raw card images are stored and parsed only if needed. The idea is that\n    to create the HDU objects, only a small subset of standard cards is needed.\n    Once a card is parsed, which is deferred to the Card class, the Card object\n    is kept in a cache. This is useful because a small subset of cards is used\n    a lot in th"}, {"start_line": 17000, "end_line": 19000, "belongs_to": {"file_name": "table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/hdu", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "header and data are copied.\n        \"\"\"\n        # touch the data, so it's defined (in the case of reading from a\n        # FITS file)\n        return self.__class__(data=self.data.copy(), header=self._header.copy())\n\n    def _prewriteto(self, inplace=False):\n        if self._has_data:\n            self.data._scale_back(update_heap_pointers=not self._manages_own_heap)\n            # check TFIELDS and NAXIS2\n            self._header[\"TFIELDS\"] = len(self.data._coldefs)\n            self._header[\"NAXIS2\"] = self.data.shape[0]\n\n            # calculate PCOUNT, for variable length tables\n            tbsize = self._header[\"NAXIS1\"] * self._header[\"NAXIS2\"]\n            heapstart = self._header.get(\"THEAP\", tbsize)\n            self.data._tbsize = tbsize\n            self.data._gap = heapstart - tbsize\n            pcount = self.data._heapsize + self.data._gap\n            if pcount > 0:\n                self._header[\"PCOUNT\"] = pcount\n\n            # update the other T****n keywords\n            self._populate_table_keywords()\n\n            # update TFORM for variable length columns\n            for idx in range(self.data._nfields):\n                format = self.data._coldefs._recformats[idx]\n                if isinstance(format, _FormatP):\n                    if self.data._load_variable_length_data:\n                        _max = self.data.field(idx).max\n                    else:\n                        _max = self.data.field(idx)[:, 0].max()\n                    # May be either _FormatP or _FormatQ\n                    format_cls = format.__class__\n                    format = format_cls(format.dtype, repeat=format.repeat, max=_max)\n                    self._header[\"TFORM\" + str(idx + 1)] = format.tform\n        return super()._prewriteto(inplace)\n\n    def _verify(self, option=\"warn\"):\n        \"\"\"\n        _TableBaseHDU verify method.\n        \"\"\"\n        errs = super()._verify(option=option)\n        if len(self._header) > 1:\n            if not (\n                isinstance(self._header[0],"}], "retrieved_count": 10, "cost_time": 1.0543243885040283}
{"question": "What architectural design does the template-style string interpolation class employ to separate identifying template variables from retrieving values from configuration sections?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "configobj.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/configobj", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tion(InterpolationEngine):\n    \"\"\"Behaves like string.Template.\"\"\"\n    _cookie = '$'\n    _delimiter = '$'\n    _KEYCRE = re.compile(r\"\"\"\n        \\$(?:\n          (?P<escaped>\\$)              |   # Two $ signs\n          (?P<named>[_a-z][_a-z0-9]*)  |   # $name format\n          {(?P<braced>[^}]*)}              # ${name} format\n        )\n        \"\"\", re.IGNORECASE | re.VERBOSE)\n\n    def _parse_match(self, match):\n        # Valid name (in or out of braces): fetch value from section\n        key = match.group('named') or match.group('braced')\n        if key is not None:\n            value, section = self._fetch(key)\n            return key, value, section\n        # Escaped delimiter (e.g., $$): return single delimiter\n        if match.group('escaped') is not None:\n            # Return None for key and section to indicate it's time to stop\n            return None, self._delimiter, None\n        # Anything else: ignore completely, just return it unchanged\n        return None, match.group(), None\n\n\ninterpolation_engines = {\n    'configparser': ConfigParserInterpolation,\n    'template': TemplateInterpolation,\n}\n\n\ndef __newobj__(cls, *args):\n    # Hack for pickle\n    return cls.__new__(cls, *args)\n\nclass Section(dict):\n    \"\"\"\n    A dictionary-like object that represents a section in a config file.\n\n    It does string interpolation if the 'interpolation' attribute\n    of the 'main' object is set to True.\n\n    Interpolation is tried first from this object, then from the 'DEFAULT'\n    section of this object, next from the parent and its 'DEFAULT' section,\n    and so on until the main object is reached.\n\n    A Section will behave like an ordered dictionary - following the\n    order of the ``scalars`` and ``sections`` attributes.\n    You can use this to change the order of members.\n\n    Iteration follows the order: scalars, then sections.\n    \"\"\"\n\n\n    def __setstate__(self, state):\n        dict.update(self, state[0])\n        self.__dict__.update(state[1])\n\n    def __reduce__(self):\n  "}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "configobj.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/configobj", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n        while True:\n            # try the current section first\n            val = current_section.get(key)\n            if val is not None and not isinstance(val, Section):\n                break\n            # try \"DEFAULT\" next\n            val = current_section.get('DEFAULT', {}).get(key)\n            if val is not None and not isinstance(val, Section):\n                break\n            # move up to parent and try again\n            # top-level's parent is itself\n            if current_section.parent is current_section:\n                # reached top level, time to give up\n                break\n            current_section = current_section.parent\n\n        # restore interpolation to previous value before returning\n        self.section.main.interpolation = save_interp\n        if val is None:\n            raise MissingInterpolationOption(key)\n        return val, current_section\n\n\n    def _parse_match(self, match):\n        \"\"\"Implementation-dependent helper function.\n\n        Will be passed a match object corresponding to the interpolation\n        key we just found (e.g., \"%(foo)s\" or \"$foo\"). Should look up that\n        key in the appropriate config file section (using the ``_fetch()``\n        helper function) and return a 3-tuple: (key, value, section)\n\n        ``key`` is the name of the key we're looking for\n        ``value`` is the value found for that key\n        ``section`` is a reference to the section where it was found\n\n        ``key`` and ``section`` should be None if no further\n        interpolation should be performed on the resulting value\n        (e.g., if we interpolated \"$$\" and returned \"$\").\n        \"\"\"\n        raise NotImplementedError()\n\n\n\nclass ConfigParserInterpolation(InterpolationEngine):\n    \"\"\"Behaves like ConfigParser.\"\"\"\n    _cookie = '%'\n    _KEYCRE = re.compile(r\"%\\(([^)]*)\\)s\")\n\n    def _parse_match(self, match):\n        key = match.group(1)\n        value, section = self._fetch(key)\n        return key, value, section\n\n\n\nclass TemplateInterpola"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "configobj.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/configobj", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "match object corresponding to the interpolation\n        key we just found (e.g., \"%(foo)s\" or \"$foo\"). Should look up that\n        key in the appropriate config file section (using the ``_fetch()``\n        helper function) and return a 3-tuple: (key, value, section)\n\n        ``key`` is the name of the key we're looking for\n        ``value`` is the value found for that key\n        ``section`` is a reference to the section where it was found\n\n        ``key`` and ``section`` should be None if no further\n        interpolation should be performed on the resulting value\n        (e.g., if we interpolated \"$$\" and returned \"$\").\n        \"\"\"\n        raise NotImplementedError()\n\n\n\nclass ConfigParserInterpolation(InterpolationEngine):\n    \"\"\"Behaves like ConfigParser.\"\"\"\n    _cookie = '%'\n    _KEYCRE = re.compile(r\"%\\(([^)]*)\\)s\")\n\n    def _parse_match(self, match):\n        key = match.group(1)\n        value, section = self._fetch(key)\n        return key, value, section\n\n\n\nclass TemplateInterpolation(InterpolationEngine):\n    \"\"\"Behaves like string.Template.\"\"\"\n    _cookie = '$'\n    _delimiter = '$'\n    _KEYCRE = re.compile(r\"\"\"\n        \\$(?:\n          (?P<escaped>\\$)              |   # Two $ signs\n          (?P<named>[_a-z][_a-z0-9]*)  |   # $name format\n          {(?P<braced>[^}]*)}              # ${name} format\n        )\n        \"\"\", re.IGNORECASE | re.VERBOSE)\n\n    def _parse_match(self, match):\n        # Valid name (in or out of braces): fetch value from section\n        key = match.group('named') or match.group('braced')\n        if key is not None:\n            value, section = self._fetch(key)\n            return key, value, section\n        # Escaped delimiter (e.g., $$): return single delimiter\n        if match.group('escaped') is not None:\n            # Return None for key and section to indicate it's time to stop\n            return None, self._delimiter, None\n        # Anything else: ignore completely, just return it unchanged\n        return None, match.group(), None\n\n\n"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "configobj.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/configobj", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "interpolation_engines = {\n    'configparser': ConfigParserInterpolation,\n    'template': TemplateInterpolation,\n}\n\n\ndef __newobj__(cls, *args):\n    # Hack for pickle\n    return cls.__new__(cls, *args)\n\nclass Section(dict):\n    \"\"\"\n    A dictionary-like object that represents a section in a config file.\n\n    It does string interpolation if the 'interpolation' attribute\n    of the 'main' object is set to True.\n\n    Interpolation is tried first from this object, then from the 'DEFAULT'\n    section of this object, next from the parent and its 'DEFAULT' section,\n    and so on until the main object is reached.\n\n    A Section will behave like an ordered dictionary - following the\n    order of the ``scalars`` and ``sections`` attributes.\n    You can use this to change the order of members.\n\n    Iteration follows the order: scalars, then sections.\n    \"\"\"\n\n\n    def __setstate__(self, state):\n        dict.update(self, state[0])\n        self.__dict__.update(state[1])\n\n    def __reduce__(self):\n        state = (dict(self), self.__dict__)\n        return (__newobj__, (self.__class__,), state)\n\n\n    def __init__(self, parent, depth, main, indict=None, name=None):\n        \"\"\"\n        * parent is the section above\n        * depth is the depth level of this section\n        * main is the main ConfigObj\n        * indict is a dictionary to initialise the section with\n        \"\"\"\n        if indict is None:\n            indict = {}\n        dict.__init__(self)\n        # used for nesting level *and* interpolation\n        self.parent = parent\n        # used for the interpolation attribute\n        self.main = main\n        # level of nesting depth of this Section\n        self.depth = depth\n        # purely for information\n        self.name = name\n        #\n        self._initialise()\n        # we do this explicitly so that __setitem__ is used properly\n        # (rather than just passing to ``dict.__init__``)\n        for entry, value in indict.items():\n            self[entry] = value\n\n\n    def _i"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "configobj.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/configobj", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "            # Place a marker on our backtrail so we won't come back here again\n            backtrail[(key, section.name)] = 1\n\n            # Now start the actual work\n            match = self._KEYCRE.search(value)\n            while match:\n                # The actual parsing of the match is implementation-dependent,\n                # so delegate to our helper function\n                k, v, s = self._parse_match(match)\n                if k is None:\n                    # That's the signal that no further interpolation is needed\n                    replacement = v\n                else:\n                    # Further interpolation may be needed to obtain final value\n                    replacement = recursive_interpolate(k, v, s, backtrail)\n                # Replace the matched string with its final value\n                start, end = match.span()\n                value = ''.join((value[:start], replacement, value[end:]))\n                new_search_start = start + len(replacement)\n                # Pick up the next interpolation key, if any, for next time\n                # through the while loop\n                match = self._KEYCRE.search(value, new_search_start)\n\n            # Now safe to come back here again; remove marker from backtrail\n            del backtrail[(key, section.name)]\n\n            return value\n\n        # Back in interpolate(), all we have to do is kick off the recursive\n        # function with appropriate starting values\n        value = recursive_interpolate(key, value, self.section, {})\n        return value\n\n\n    def _fetch(self, key):\n        \"\"\"Helper function to fetch values from owning section.\n\n        Returns a 2-tuple: the value, and the section where it was found.\n        \"\"\"\n        # switch off interpolation before we try and fetch anything !\n        save_interp = self.section.main.interpolation\n        self.section.main.interpolation = False\n\n        # Start at section that \"owns\" this InterpolationEngine\n        current_section = self.section"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "configobj.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/configobj", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     # Pick up the next interpolation key, if any, for next time\n                # through the while loop\n                match = self._KEYCRE.search(value, new_search_start)\n\n            # Now safe to come back here again; remove marker from backtrail\n            del backtrail[(key, section.name)]\n\n            return value\n\n        # Back in interpolate(), all we have to do is kick off the recursive\n        # function with appropriate starting values\n        value = recursive_interpolate(key, value, self.section, {})\n        return value\n\n\n    def _fetch(self, key):\n        \"\"\"Helper function to fetch values from owning section.\n\n        Returns a 2-tuple: the value, and the section where it was found.\n        \"\"\"\n        # switch off interpolation before we try and fetch anything !\n        save_interp = self.section.main.interpolation\n        self.section.main.interpolation = False\n\n        # Start at section that \"owns\" this InterpolationEngine\n        current_section = self.section\n        while True:\n            # try the current section first\n            val = current_section.get(key)\n            if val is not None and not isinstance(val, Section):\n                break\n            # try \"DEFAULT\" next\n            val = current_section.get('DEFAULT', {}).get(key)\n            if val is not None and not isinstance(val, Section):\n                break\n            # move up to parent and try again\n            # top-level's parent is itself\n            if current_section.parent is current_section:\n                # reached top level, time to give up\n                break\n            current_section = current_section.parent\n\n        # restore interpolation to previous value before returning\n        self.section.main.interpolation = save_interp\n        if val is None:\n            raise MissingInterpolationOption(key)\n        return val, current_section\n\n\n    def _parse_match(self, match):\n        \"\"\"Implementation-dependent helper function.\n\n        Will be passed a "}, {"start_line": 15000, "end_line": 17000, "belongs_to": {"file_name": "configobj.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/configobj", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "nitialise(self):\n        # the sequence of scalar values in this Section\n        self.scalars = []\n        # the sequence of sections in this Section\n        self.sections = []\n        # for comments :-)\n        self.comments = {}\n        self.inline_comments = {}\n        # the configspec\n        self.configspec = None\n        # for defaults\n        self.defaults = []\n        self.default_values = {}\n        self.extra_values = []\n        self._created = False\n\n\n    def _interpolate(self, key, value):\n        try:\n            # do we already have an interpolation engine?\n            engine = self._interpolation_engine\n        except AttributeError:\n            # not yet: first time running _interpolate(), so pick the engine\n            name = self.main.interpolation\n            if name == True:  # note that \"if name:\" would be incorrect here\n                # backwards-compatibility: interpolation=True means use default\n                name = DEFAULT_INTERPOLATION\n            name = name.lower()  # so that \"Template\", \"template\", etc. all work\n            class_ = interpolation_engines.get(name, None)\n            if class_ is None:\n                # invalid value for self.main.interpolation\n                self.main.interpolation = False\n                return value\n            else:\n                # save reference to engine so we don't have to do this again\n                engine = self._interpolation_engine = class_(self)\n        # let the engine do the actual work\n        return engine.interpolate(key, value)\n\n\n    def __getitem__(self, key):\n        \"\"\"Fetch the item and do string interpolation.\"\"\"\n        val = dict.__getitem__(self, key)\n        if self.main.interpolation:\n            if isinstance(val, str):\n                return self._interpolate(key, val)\n            if isinstance(val, list):\n                def _check(entry):\n                    if isinstance(entry, str):\n                        return self._interpolate(key, entry)\n                    re"}, {"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "configobj.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/configobj", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "me.lower()  # so that \"Template\", \"template\", etc. all work\n            class_ = interpolation_engines.get(name, None)\n            if class_ is None:\n                # invalid value for self.main.interpolation\n                self.main.interpolation = False\n                return value\n            else:\n                # save reference to engine so we don't have to do this again\n                engine = self._interpolation_engine = class_(self)\n        # let the engine do the actual work\n        return engine.interpolate(key, value)\n\n\n    def __getitem__(self, key):\n        \"\"\"Fetch the item and do string interpolation.\"\"\"\n        val = dict.__getitem__(self, key)\n        if self.main.interpolation:\n            if isinstance(val, str):\n                return self._interpolate(key, val)\n            if isinstance(val, list):\n                def _check(entry):\n                    if isinstance(entry, str):\n                        return self._interpolate(key, entry)\n                    return entry\n                new = [_check(entry) for entry in val]\n                if new != val:\n                    return new\n        return val\n\n\n    def __setitem__(self, key, value, unrepr=False):\n        \"\"\"\n        Correctly set a value.\n\n        Making dictionary values Section instances.\n        (We have to special case 'Section' instances - which are also dicts)\n\n        Keys must be strings.\n        Values need only be strings (or lists of strings) if\n        ``main.stringify`` is set.\n\n        ``unrepr`` must be set when setting a value to a dictionary, without\n        creating a new sub-section.\n        \"\"\"\n        if not isinstance(key, str):\n            raise ValueError('The key \"%s\" is not a string.' % key)\n\n        # add the comment\n        if key not in self.comments:\n            self.comments[key] = []\n            self.inline_comments[key] = ''\n        # remove the entry from defaults\n        if key in self.defaults:\n            self.defaults.remove(key)\n        #\n  "}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "configobj.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/configobj", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tion errors.\"\"\"\n\n\nclass InterpolationLoopError(InterpolationError):\n    \"\"\"Maximum interpolation depth exceeded in string interpolation.\"\"\"\n\n    def __init__(self, option):\n        InterpolationError.__init__(\n            self,\n            'interpolation loop detected in value \"%s\".' % option)\n\n\nclass RepeatSectionError(ConfigObjError):\n    \"\"\"\n    This error indicates additional sections in a section with a\n    ``__many__`` (repeated) section.\n    \"\"\"\n\n\nclass MissingInterpolationOption(InterpolationError):\n    \"\"\"A value specified for interpolation was missing.\"\"\"\n    def __init__(self, option):\n        msg = 'missing option \"%s\" in interpolation.' % option\n        InterpolationError.__init__(self, msg)\n\n\nclass UnreprError(ConfigObjError):\n    \"\"\"An error parsing in unrepr mode.\"\"\"\n\n\n\nclass InterpolationEngine(object):\n    \"\"\"\n    A helper class to help perform string interpolation.\n\n    This class is an abstract base class; its descendants perform\n    the actual work.\n    \"\"\"\n\n    # compiled regexp to use in self.interpolate()\n    _KEYCRE = re.compile(r\"%\\(([^)]*)\\)s\")\n    _cookie = '%'\n\n    def __init__(self, section):\n        # the Section instance that \"owns\" this engine\n        self.section = section\n\n\n    def interpolate(self, key, value):\n        # short-cut\n        if not self._cookie in value:\n            return value\n\n        def recursive_interpolate(key, value, section, backtrail):\n            \"\"\"The function that does the actual work.\n\n            ``value``: the string we're trying to interpolate.\n            ``section``: the section in which that string was found\n            ``backtrail``: a dict to keep track of where we've been,\n            to detect and prevent infinite recursion loops\n\n            This is similar to a depth-first-search algorithm.\n            \"\"\"\n            # Have we been here already?\n            if (key, section.name) in backtrail:\n                # Yes - infinite loop detected\n                raise InterpolationLoopError(key)\n"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "configobj.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/configobj", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "compiled regexp to use in self.interpolate()\n    _KEYCRE = re.compile(r\"%\\(([^)]*)\\)s\")\n    _cookie = '%'\n\n    def __init__(self, section):\n        # the Section instance that \"owns\" this engine\n        self.section = section\n\n\n    def interpolate(self, key, value):\n        # short-cut\n        if not self._cookie in value:\n            return value\n\n        def recursive_interpolate(key, value, section, backtrail):\n            \"\"\"The function that does the actual work.\n\n            ``value``: the string we're trying to interpolate.\n            ``section``: the section in which that string was found\n            ``backtrail``: a dict to keep track of where we've been,\n            to detect and prevent infinite recursion loops\n\n            This is similar to a depth-first-search algorithm.\n            \"\"\"\n            # Have we been here already?\n            if (key, section.name) in backtrail:\n                # Yes - infinite loop detected\n                raise InterpolationLoopError(key)\n            # Place a marker on our backtrail so we won't come back here again\n            backtrail[(key, section.name)] = 1\n\n            # Now start the actual work\n            match = self._KEYCRE.search(value)\n            while match:\n                # The actual parsing of the match is implementation-dependent,\n                # so delegate to our helper function\n                k, v, s = self._parse_match(match)\n                if k is None:\n                    # That's the signal that no further interpolation is needed\n                    replacement = v\n                else:\n                    # Further interpolation may be needed to obtain final value\n                    replacement = recursive_interpolate(k, v, s, backtrail)\n                # Replace the matched string with its final value\n                start, end = match.span()\n                value = ''.join((value[:start], replacement, value[end:]))\n                new_search_start = start + len(replacement)\n           "}], "retrieved_count": 10, "cost_time": 1.0650062561035156}
{"question": "How does the base context manager class in astropy.config.paths that temporarily overrides configuration and cache directory paths ensure atomicity and exception safety when overriding its class-level temporary path attribute during context manager entry?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "paths.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " not in effect\n        (this is useful for testing).  In this case the ``delete`` argument is\n        always ignored.\n\n    delete : bool, optional\n        If True, cleans up the temporary directory after exiting the temp\n        context (default: False).\n    \"\"\"\n\n    _directory_type = \"config\"\n    _directory_env_var = \"XDG_CONFIG_HOME\"\n\n    def __enter__(self) -> str:\n        # Special case for the config case, where we need to reset all the\n        # cached config objects.  We do keep the cache, since some of it\n        # may have been set programmatically rather than be stored in the\n        # config file (e.g., iers.conf.auto_download=False for our tests).\n        from .configuration import _cfgobjs\n\n        self._cfgobjs_copy = _cfgobjs.copy()\n        _cfgobjs.clear()\n        return super().__enter__()\n\n    def __exit__(\n        self,\n        type: type[BaseException] | None,\n        value: BaseException | None,\n        tb: TracebackType | None,\n    ) -> None:\n        from .configuration import _cfgobjs\n\n        _cfgobjs.clear()\n        _cfgobjs.update(self._cfgobjs_copy)\n        del self._cfgobjs_copy\n        super().__exit__(type, value, tb)\n\n\nclass set_temp_cache(_SetTempPath):\n    \"\"\"\n    Context manager to set a temporary path for the Astropy download cache,\n    primarily for use with testing (though there may be other applications\n    for setting a different cache directory, for example to switch to a cache\n    dedicated to large files).\n\n    If the path set by this context manager does not already exist it will be\n    created, if possible.\n\n    This may also be used as a decorator on a function to set the cache path\n    just within that function.\n\n    Parameters\n    ----------\n    path : str\n        The directory (which must exist) in which to find the Astropy cache\n        files, or create them if they do not already exist.  If None, this\n        restores the cache path to the user's default cache path as returned\n        by `get_cache_dir` as though thi"}, {"start_line": 8000, "end_line": 9358, "belongs_to": {"file_name": "paths.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ration import _cfgobjs\n\n        _cfgobjs.clear()\n        _cfgobjs.update(self._cfgobjs_copy)\n        del self._cfgobjs_copy\n        super().__exit__(type, value, tb)\n\n\nclass set_temp_cache(_SetTempPath):\n    \"\"\"\n    Context manager to set a temporary path for the Astropy download cache,\n    primarily for use with testing (though there may be other applications\n    for setting a different cache directory, for example to switch to a cache\n    dedicated to large files).\n\n    If the path set by this context manager does not already exist it will be\n    created, if possible.\n\n    This may also be used as a decorator on a function to set the cache path\n    just within that function.\n\n    Parameters\n    ----------\n    path : str\n        The directory (which must exist) in which to find the Astropy cache\n        files, or create them if they do not already exist.  If None, this\n        restores the cache path to the user's default cache path as returned\n        by `get_cache_dir` as though this context manager were not in effect\n        (this is useful for testing).  In this case the ``delete`` argument is\n        always ignored.\n\n    delete : bool, optional\n        If True, cleans up the temporary directory after exiting the temp\n        context (default: False).\n    \"\"\"\n\n    _directory_type = \"cache\"\n    _directory_env_var = \"XDG_CACHE_HOME\"\n"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "paths.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " maindir.mkdir(parents=True, exist_ok=True)\n            if (\n                not sys.platform.startswith(\"win\")\n                and linkto is not None\n                and not linkto.exists()\n            ):\n                os.symlink(maindir, linkto)\n\n        return maindir.resolve()\n\n\nclass set_temp_config(_SetTempPath):\n    \"\"\"\n    Context manager to set a temporary path for the Astropy config, primarily\n    for use with testing.\n\n    If the path set by this context manager does not already exist it will be\n    created, if possible.\n\n    This may also be used as a decorator on a function to set the config path\n    just within that function.\n\n    Parameters\n    ----------\n    path : str, optional\n        The directory (which must exist) in which to find the Astropy config\n        files, or create them if they do not already exist.  If None, this\n        restores the config path to the user's default config path as returned\n        by `get_config_dir` as though this context manager were not in effect\n        (this is useful for testing).  In this case the ``delete`` argument is\n        always ignored.\n\n    delete : bool, optional\n        If True, cleans up the temporary directory after exiting the temp\n        context (default: False).\n    \"\"\"\n\n    _directory_type = \"config\"\n    _directory_env_var = \"XDG_CONFIG_HOME\"\n\n    def __enter__(self) -> str:\n        # Special case for the config case, where we need to reset all the\n        # cached config objects.  We do keep the cache, since some of it\n        # may have been set programmatically rather than be stored in the\n        # config file (e.g., iers.conf.auto_download=False for our tests).\n        from .configuration import _cfgobjs\n\n        self._cfgobjs_copy = _cfgobjs.copy()\n        _cfgobjs.clear()\n        return super().__enter__()\n\n    def __exit__(\n        self,\n        type: type[BaseException] | None,\n        value: BaseException | None,\n        tb: TracebackType | None,\n    ) -> None:\n        from .configu"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "paths.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ootname).is_symlink()\n        ):\n            if xchpth.exists():\n                return xchpth.resolve()\n\n            # symlink will be set to this if the directory is created\n            linkto = xchpth\n        else:\n            linkto = None\n\n        return cls._find_or_create_root_dir(linkto, rootname)\n\n    @classmethod\n    def _find_or_create_root_dir(\n        cls,\n        linkto: Path | None,\n        pkgname: str = \"astropy\",\n    ) -> Path:\n        innerdir = Path.home() / f\".{pkgname}\"\n        maindir = innerdir / cls._directory_type\n\n        if maindir.is_file():\n            raise OSError(\n                f\"Intended {pkgname} {cls._directory_type} directory {maindir} is actually a file.\"\n            )\n        if not maindir.is_dir():\n            # first create .astropy dir if needed\n            if innerdir.is_file():\n                raise OSError(\n                    f\"Intended {pkgname} {cls._directory_type} directory {maindir} is actually a file.\"\n                )\n            maindir.mkdir(parents=True, exist_ok=True)\n            if (\n                not sys.platform.startswith(\"win\")\n                and linkto is not None\n                and not linkto.exists()\n            ):\n                os.symlink(maindir, linkto)\n\n        return maindir.resolve()\n\n\nclass set_temp_config(_SetTempPath):\n    \"\"\"\n    Context manager to set a temporary path for the Astropy config, primarily\n    for use with testing.\n\n    If the path set by this context manager does not already exist it will be\n    created, if possible.\n\n    This may also be used as a decorator on a function to set the config path\n    just within that function.\n\n    Parameters\n    ----------\n    path : str, optional\n        The directory (which must exist) in which to find the Astropy config\n        files, or create them if they do not already exist.  If None, this\n        restores the config path to the user's default config path as returned\n        by `get_config_dir` as though this context manager were"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_configs.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/config/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "   temp_astropy_config = temp_config_dir / \"astropy\"\n\n    # Test decorator mode\n    @paths.set_temp_config(temp_config_dir)\n    def test_func():\n        assert paths.get_config_dir(rootname=\"astropy\") == str(temp_astropy_config)\n        assert paths.get_config_dir_path(rootname=\"astropy\") == temp_astropy_config\n\n        # Test temporary restoration of original default\n        with paths.set_temp_config() as d:\n            assert d == orig_config_dir == paths.get_config_dir(rootname=\"astropy\")\n\n    test_func()\n\n    # Test context manager mode (with cleanup)\n    with paths.set_temp_config(temp_config_dir, delete=True):\n        assert paths.get_config_dir(rootname=\"astropy\") == str(temp_astropy_config)\n        assert paths.get_config_dir_path(rootname=\"astropy\") == temp_astropy_config\n\n    assert not temp_config_dir.exists()\n    # Check that we have returned to our old configuration.\n    assert configuration._cfgobjs == OLD_CONFIG\n\n\ndef test_set_temp_cache(tmp_path, monkeypatch):\n    monkeypatch.setattr(paths.set_temp_cache, \"_temp_path\", None)\n\n    orig_cache_dir = paths.get_cache_dir(rootname=\"astropy\")\n    (temp_cache_dir := tmp_path / \"cache\").mkdir()\n    temp_astropy_cache = temp_cache_dir / \"astropy\"\n\n    # Test decorator mode\n    @paths.set_temp_cache(temp_cache_dir)\n    def test_func():\n        assert paths.get_cache_dir(rootname=\"astropy\") == str(temp_astropy_cache)\n\n        # Test temporary restoration of original default\n        with paths.set_temp_cache() as d:\n            assert d == orig_cache_dir == paths.get_cache_dir(rootname=\"astropy\")\n\n    test_func()\n\n    # Test context manager mode (with cleanup)\n    with paths.set_temp_cache(temp_cache_dir, delete=True):\n        assert paths.get_cache_dir(rootname=\"astropy\") == str(temp_astropy_cache)\n\n    assert not temp_cache_dir.exists()\n\n\ndef test_set_temp_cache_resets_on_exception(tmp_path):\n    \"\"\"Test for regression of  bug #9704\"\"\"\n    t = paths.get_cache_dir()\n    (a := tmp_path / \"a\").write_text(\"not a g"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "paths.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "-----\n    get_cache_dir_path : same as this function, except that the return value is a pathlib.Path\n\n    \"\"\"\n    )\n\n\nclass _SetTempPath:\n    _temp_path: Path | None = None\n\n    # This base class serves as a deduplication layer for its only two intended\n    # children (set_temp_cache and set_temp_config)\n    _directory_type: Literal[\"cache\", \"config\"]\n    _directory_env_var: Literal[\"XDG_CACHE_HOME\", \"XDG_CONFIG_HOME\"]\n\n    def __init__(\n        self, path: os.PathLike[str] | str | None = None, delete: bool = False\n    ) -> None:\n        if path is not None:\n            path = Path(path).resolve()\n\n        self._path = path\n        self._delete = delete\n        self._prev_path = self.__class__._temp_path\n\n    def __enter__(self) -> str:\n        self.__class__._temp_path = self._path\n        try:\n            return str(self.__class__._get_dir_path(rootname=\"astropy\"))\n        except Exception:\n            self.__class__._temp_path = self._prev_path\n            raise\n\n    def __exit__(\n        self,\n        type: type[BaseException] | None,\n        value: BaseException | None,\n        tb: TracebackType | None,\n    ) -> None:\n        self.__class__._temp_path = self._prev_path\n\n        if self._delete and self._path is not None:\n            shutil.rmtree(self._path)\n\n    def __call__(self, func: Callable[P, object]) -> Callable[P, None]:\n        \"\"\"Implements use as a decorator.\"\"\"\n\n        @wraps(func)\n        def wrapper(*args: P.args, **kwargs: P.kwargs) -> None:\n            with self:\n                func(*args, **kwargs)\n\n        return wrapper\n\n    @classmethod\n    def _get_dir_path(cls, rootname: str) -> Path:\n        if (xch := cls._temp_path) is not None:\n            path = xch / rootname\n            if not path.is_file():\n                path.mkdir(exist_ok=True)\n            return path.resolve()\n\n        if (\n            (dir_ := os.getenv(cls._directory_env_var)) is not None\n            and (xch := Path(dir_)).exists()\n            and not (xchpth := xch / r"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "paths.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/config", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "cache directory name and creates the directory if it\n    doesn't exist.\n\n    This directory is typically ``$HOME/.astropy/cache``, but if the\n    XDG_CACHE_HOME environment variable is set and the\n    ``$XDG_CACHE_HOME/astropy`` directory exists, it will be that directory.\n    If neither exists, the former will be created and symlinked to the latter.\n\n    Parameters\n    ----------\n    rootname : str\n        Name of the root cache directory. For example, if\n        ``rootname = 'pkgname'``, the cache directory will be\n        ``<cache>/.pkgname/``.\n\n    Returns\n    -------\n    cachedir : Path\n        The absolute path to the cache directory.\n\n    \"\"\"\n    return set_temp_cache._get_dir_path(rootname)\n\n\ndef get_cache_dir(rootname: str = \"astropy\") -> str:\n    return str(get_cache_dir_path(rootname))\n\n\nif get_cache_dir_path.__doc__ is not None:\n    # guard against PYTHONOPTIMIZE mode\n    get_cache_dir.__doc__ = cleandoc(\n        get_cache_dir_path.__doc__\n        + \"\"\"\n    See Also\n    --------\n    get_cache_dir_path : same as this function, except that the return value is a pathlib.Path\n\n    \"\"\"\n    )\n\n\nclass _SetTempPath:\n    _temp_path: Path | None = None\n\n    # This base class serves as a deduplication layer for its only two intended\n    # children (set_temp_cache and set_temp_config)\n    _directory_type: Literal[\"cache\", \"config\"]\n    _directory_env_var: Literal[\"XDG_CACHE_HOME\", \"XDG_CONFIG_HOME\"]\n\n    def __init__(\n        self, path: os.PathLike[str] | str | None = None, delete: bool = False\n    ) -> None:\n        if path is not None:\n            path = Path(path).resolve()\n\n        self._path = path\n        self._delete = delete\n        self._prev_path = self.__class__._temp_path\n\n    def __enter__(self) -> str:\n        self.__class__._temp_path = self._path\n        try:\n            return str(self.__class__._get_dir_path(rootname=\"astropy\"))\n        except Exception:\n            self.__class__._temp_path = self._prev_path\n            raise\n\n    def __exit__(\n "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_configs.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/config/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "assert \"testpkg\" in paths.get_cache_dir(rootname=\"testpkg\")\n\n\n@pytest.mark.parametrize(\n    \"environment_variable,func\",\n    [\n        # Regression test for #17514 - XDG_CACHE_HOME had no effect\n        pytest.param(\"XDG_CACHE_HOME\", paths.get_cache_dir_path, id=\"cache\"),\n        pytest.param(\"XDG_CONFIG_HOME\", paths.get_config_dir_path, id=\"config\"),\n    ],\n)\ndef test_xdg_variables(monkeypatch, tmp_path, environment_variable, func):\n    config_dir = tmp_path / \"astropy\"\n    config_dir.mkdir()\n    monkeypatch.setenv(environment_variable, str(tmp_path))\n    assert func() == config_dir\n\n\ndef test_set_temp_config(tmp_path, monkeypatch):\n    # Check that we start in an understood state.\n    assert configuration._cfgobjs == OLD_CONFIG\n    # Temporarily remove any temporary overrides of the configuration dir.\n    monkeypatch.setattr(paths.set_temp_config, \"_temp_path\", None)\n\n    orig_config_dir = paths.get_config_dir(rootname=\"astropy\")\n    (temp_config_dir := tmp_path / \"config\").mkdir()\n    temp_astropy_config = temp_config_dir / \"astropy\"\n\n    # Test decorator mode\n    @paths.set_temp_config(temp_config_dir)\n    def test_func():\n        assert paths.get_config_dir(rootname=\"astropy\") == str(temp_astropy_config)\n        assert paths.get_config_dir_path(rootname=\"astropy\") == temp_astropy_config\n\n        # Test temporary restoration of original default\n        with paths.set_temp_config() as d:\n            assert d == orig_config_dir == paths.get_config_dir(rootname=\"astropy\")\n\n    test_func()\n\n    # Test context manager mode (with cleanup)\n    with paths.set_temp_config(temp_config_dir, delete=True):\n        assert paths.get_config_dir(rootname=\"astropy\") == str(temp_astropy_config)\n        assert paths.get_config_dir_path(rootname=\"astropy\") == temp_astropy_config\n\n    assert not temp_config_dir.exists()\n    # Check that we have returned to our old configuration.\n    assert configuration._cfgobjs == OLD_CONFIG\n\n\ndef test_set_temp_cache(tmp_path, monkeypatch):\n    monk"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_configs.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/config/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "eypatch.setattr(paths.set_temp_cache, \"_temp_path\", None)\n\n    orig_cache_dir = paths.get_cache_dir(rootname=\"astropy\")\n    (temp_cache_dir := tmp_path / \"cache\").mkdir()\n    temp_astropy_cache = temp_cache_dir / \"astropy\"\n\n    # Test decorator mode\n    @paths.set_temp_cache(temp_cache_dir)\n    def test_func():\n        assert paths.get_cache_dir(rootname=\"astropy\") == str(temp_astropy_cache)\n\n        # Test temporary restoration of original default\n        with paths.set_temp_cache() as d:\n            assert d == orig_cache_dir == paths.get_cache_dir(rootname=\"astropy\")\n\n    test_func()\n\n    # Test context manager mode (with cleanup)\n    with paths.set_temp_cache(temp_cache_dir, delete=True):\n        assert paths.get_cache_dir(rootname=\"astropy\") == str(temp_astropy_cache)\n\n    assert not temp_cache_dir.exists()\n\n\ndef test_set_temp_cache_resets_on_exception(tmp_path):\n    \"\"\"Test for regression of  bug #9704\"\"\"\n    t = paths.get_cache_dir()\n    (a := tmp_path / \"a\").write_text(\"not a good cache\\n\")\n    with pytest.raises(OSError), paths.set_temp_cache(a):\n        pass\n    assert t == paths.get_cache_dir()\n\n\ndef test_config_file():\n    from astropy.config.configuration import get_config, reload_config\n\n    apycfg = get_config(\"astropy\")\n    assert apycfg.filename.endswith(\"astropy.cfg\")\n\n    cfgsec = get_config(\"astropy.config\")\n    assert cfgsec.depth == 1\n    assert cfgsec.name == \"config\"\n    assert cfgsec.parent.filename.endswith(\"astropy.cfg\")\n\n    # try with a different package name, still inside astropy config dir:\n    testcfg = get_config(\"testpkg\", rootname=\"astropy\")\n    parts = os.path.normpath(testcfg.filename).split(os.sep)\n    assert \".astropy\" in parts or \"astropy\" in parts\n    assert parts[-1] == \"testpkg.cfg\"\n    configuration._cfgobjs[\"testpkg\"] = None  # HACK\n\n    # try with a different package name, no specified root name (should\n    #   default to astropy):\n    testcfg = get_config(\"testpkg\")\n    parts = os.path.normpath(testcfg.filename).split(o"}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "test_configs.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/config/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n    # astropy dir could not be accessed\n    @classmethod\n    def osraiser(cls, linkto, pkgname=None):\n        raise OSError\n\n    monkeypatch.setattr(paths._SetTempPath, \"_find_or_create_root_dir\", osraiser)\n\n    # also have to make sure the stored configuration objects are cleared\n    monkeypatch.setattr(configuration, \"_cfgobjs\", {})\n\n    # make sure the config dir search fails\n    with pytest.raises(OSError):\n        paths.get_config_dir(rootname=\"astropy\")\n\n    with pytest.raises(OSError):\n        paths.get_config_dir_path(rootname=\"astropy\")\n\n    # now run the basic tests, and make sure the warning about no astropy\n    # is present\n    test_configitem()\n\n\ndef test_configitem_setters():\n    from astropy.config.configuration import ConfigItem, ConfigNamespace\n\n    class Conf(ConfigNamespace):\n        tstnm12 = ConfigItem(42, \"this is another Description\")\n\n    conf = Conf()\n\n    assert conf.tstnm12 == 42\n    with conf.set_temp(\"tstnm12\", 45):\n        assert conf.tstnm12 == 45\n    assert conf.tstnm12 == 42\n\n    conf.tstnm12 = 43\n    assert conf.tstnm12 == 43\n\n    with conf.set_temp(\"tstnm12\", 46):\n        assert conf.tstnm12 == 46\n\n    # Make sure it is reset even with Exception\n    try:\n        with conf.set_temp(\"tstnm12\", 47):\n            raise Exception\n    except Exception:\n        pass\n\n    assert conf.tstnm12 == 43\n\n\ndef test_empty_config_file():\n    from astropy.config.configuration import is_unedited_config_file\n\n    def get_content(fn):\n        with open(get_pkg_data_filename(fn), encoding=\"latin-1\") as fd:\n            return fd.read()\n\n    content = get_content(\"data/empty.cfg\")\n    assert is_unedited_config_file(content)\n\n    content = get_content(\"data/not_empty.cfg\")\n    assert not is_unedited_config_file(content)\n\n\nclass TestAliasRead:\n    def setup_class(self):\n        configuration._override_config_file = get_pkg_data_filename(\"data/alias.cfg\")\n\n    def test_alias_read(self):\n        from astropy.utils.data import conf\n\n        with pytest.warns(\n"}], "retrieved_count": 10, "cost_time": 1.0757312774658203}
{"question": "What attributes must be implemented by the object type tested in the test class that verifies compatibility with numpy's shape, size, and dimensionality inspection functions to satisfy the dependencies of those functions?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_quantity_non_ufuncs.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "p(BasicTestSetup):\n    def check(self, func, *args, **kwargs):\n        o = func(self.q, *args, **kwargs)\n        expected = func(self.q.value, *args, **kwargs) * self.q.unit\n        assert o.shape == expected.shape\n        assert np.all(o == expected)\n\n\nclass NoUnitTestSetup(BasicTestSetup):\n    def check(self, func, *args, **kwargs):\n        out = func(self.q, *args, **kwargs)\n        expected = func(self.q.value, *args, *kwargs)\n        assert type(out) is type(expected)\n        if isinstance(expected, tuple):\n            assert all(np.all(o == x) for o, x in zip(out, expected))\n        else:\n            assert np.all(out == expected)\n\n\nclass TestShapeInformation(BasicTestSetup):\n    def test_shape(self):\n        assert np.shape(self.q) == (3, 3)\n\n    def test_size(self):\n        assert np.size(self.q) == 9\n\n    def test_ndim(self):\n        assert np.ndim(self.q) == 2\n\n\nclass TestShapeManipulation(InvariantUnitTestSetup):\n    # Note: do not parametrize the below, since test names are used\n    # to check coverage.\n    def test_reshape(self):\n        self.check(np.reshape, (9, 1))\n\n    def test_ravel(self):\n        self.check(np.ravel)\n\n    def test_moveaxis(self):\n        self.check(np.moveaxis, 0, 1)\n\n    def test_rollaxis(self):\n        self.check(np.rollaxis, 0, 2)\n\n    def test_swapaxes(self):\n        self.check(np.swapaxes, 0, 1)\n\n    def test_transpose(self):\n        self.check(np.transpose)\n\n    def test_atleast_1d(self):\n        q = 1.0 * u.m\n        o, so = np.atleast_1d(q, self.q)\n        assert o.shape == (1,)\n        assert o == q\n        expected = np.atleast_1d(self.q.value) * u.m\n        assert np.all(so == expected)\n\n    def test_atleast_2d(self):\n        q = 1.0 * u.m\n        o, so = np.atleast_2d(q, self.q)\n        assert o.shape == (1, 1)\n        assert o == q\n        expected = np.atleast_2d(self.q.value) * u.m\n        assert np.all(so == expected)\n\n    def test_atleast_3d(self):\n        q = 1.0 * u.m\n        o, so = np.atleast_3d(q, self.q)\n   "}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "compat.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/nddata", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " np.ma.nomask:\n            return None\n        else:\n            return self._mask\n\n    @mask.setter\n    def mask(self, value):\n        # Check that value is not either type of null mask.\n        if (value is not None) and (value is not np.ma.nomask):\n            mask = np.asarray(value, dtype=np.bool_)\n            if mask.shape != self.data.shape:\n                raise ValueError(\n                    f\"dimensions of mask {mask.shape} and data {self.data.shape} do not match\"\n                )\n            else:\n                self._mask = mask\n        else:\n            # internal representation should be one numpy understands\n            self._mask = np.ma.nomask\n\n    @property\n    def shape(self):\n        \"\"\"\n        shape tuple of this object's data.\n        \"\"\"\n        return self.data.shape\n\n    @property\n    def size(self):\n        \"\"\"\n        integer size of this object's data.\n        \"\"\"\n        return self.data.size\n\n    @property\n    def dtype(self):\n        \"\"\"\n        `numpy.dtype` of this object's data.\n        \"\"\"\n        return self.data.dtype\n\n    @property\n    def ndim(self):\n        \"\"\"\n        integer dimensions of this object's data.\n        \"\"\"\n        return self.data.ndim\n\n    @property\n    def flags(self):\n        return self._flags\n\n    @flags.setter\n    def flags(self, value):\n        if value is not None:\n            if isinstance(value, FlagCollection):\n                if value.shape != self.shape:\n                    raise ValueError(\"dimensions of FlagCollection does not match data\")\n                else:\n                    self._flags = value\n            else:\n                flags = np.asarray(value)\n                if flags.shape != self.shape:\n                    raise ValueError(\"dimensions of flags do not match data\")\n                else:\n                    self._flags = flags\n        else:\n            self._flags = value\n\n    def __array__(self, dtype=None, copy=COPY_IF_NEEDED):\n        \"\"\"\n        This allows code that reques"}, {"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "test_methods.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "5)\n        assert self.t2.location.strides == (0, 0, 24)\n        # But for reshape(50), location would need to be copied, so this\n        # should fail.\n        oldshape = self.t2.shape\n        with pytest.raises(AttributeError):\n            self.t2.shape = (50,)\n        # check no shape was changed.\n        assert self.t2.jd1.shape == oldshape\n        assert self.t2.jd2.shape == oldshape\n        assert self.t2.location.shape == oldshape\n\n\n@pytest.mark.parametrize(\"use_mask\", (\"masked\", \"not_masked\"))\nclass TestShapeFunctions(ShapeSetup):\n    @needs_array_function\n    def test_broadcast(self, use_mask):\n        \"\"\"Test as supported numpy function.\"\"\"\n        self.create_data(use_mask)\n\n        t0_broadcast = np.broadcast_to(self.t0, shape=(3, 10, 5))\n        assert t0_broadcast.shape == (3, 10, 5)\n        assert np.all(t0_broadcast.jd1 == self.t0.jd1)\n        assert np.may_share_memory(t0_broadcast.jd1, self.t0.jd1)\n        assert t0_broadcast.location is None\n        t1_broadcast = np.broadcast_to(self.t1, shape=(3, 10, 5))\n        assert t1_broadcast.shape == (3, 10, 5)\n        assert np.all(t1_broadcast.jd1 == self.t1.jd1)\n        assert np.may_share_memory(t1_broadcast.jd1, self.t1.jd1)\n        assert t1_broadcast.location is self.t1.location\n        t2_broadcast = np.broadcast_to(self.t2, shape=(3, 10, 5))\n        assert t2_broadcast.shape == (3, 10, 5)\n        assert np.all(t2_broadcast.jd1 == self.t2.jd1)\n        assert np.may_share_memory(t2_broadcast.jd1, self.t2.jd1)\n        assert t2_broadcast.location.shape == t2_broadcast.shape\n        assert np.may_share_memory(t2_broadcast.location, self.t2.location)\n\n    @needs_array_function\n    def test_atleast_1d(self, use_mask):\n        self.create_data(use_mask)\n\n        t00 = self.t0.ravel()[0]\n        assert t00.ndim == 0\n        t00_1d = np.atleast_1d(t00)\n        assert t00_1d.ndim == 1\n        assert_time_all_equal(t00[np.newaxis], t00_1d)\n        # Actual jd1 will not share memory, as cast to scalar.\n     "}, {"start_line": 15000, "end_line": 17000, "belongs_to": {"file_name": "test_methods.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_reshape_t.shape = (10, 5)  # Cannot be done without copy.\n        # check no shape was changed.\n        assert t0_reshape_t.shape == t0_reshape.T.shape\n        assert t0_reshape_t.jd1.shape == t0_reshape.T.shape\n        assert t0_reshape_t.jd2.shape == t0_reshape.T.shape\n        t1_reshape = self.t1.copy()\n        t1_reshape.shape = (2, 5, 5)\n        assert t1_reshape.shape == (2, 5, 5)\n        assert np.all(t1_reshape.jd1 == self.t1.jd1.reshape(2, 5, 5))\n        # location is a single element, so its shape should not change.\n        assert t1_reshape.location.shape == ()\n        # For reshape(5, 2, 5), the location array can remain the same.\n        # Note that we need to work directly on self.t2 here, since any\n        # copy would cause location to have the full shape.\n        self.t2.shape = (5, 2, 5)\n        assert self.t2.shape == (5, 2, 5)\n        assert self.t2.jd1.shape == (5, 2, 5)\n        assert self.t2.jd2.shape == (5, 2, 5)\n        assert self.t2.location.shape == (5, 2, 5)\n        assert self.t2.location.strides == (0, 0, 24)\n        # But for reshape(50), location would need to be copied, so this\n        # should fail.\n        oldshape = self.t2.shape\n        with pytest.raises(AttributeError):\n            self.t2.shape = (50,)\n        # check no shape was changed.\n        assert self.t2.jd1.shape == oldshape\n        assert self.t2.jd2.shape == oldshape\n        assert self.t2.location.shape == oldshape\n\n\n@pytest.mark.parametrize(\"use_mask\", (\"masked\", \"not_masked\"))\nclass TestShapeFunctions(ShapeSetup):\n    @needs_array_function\n    def test_broadcast(self, use_mask):\n        \"\"\"Test as supported numpy function.\"\"\"\n        self.create_data(use_mask)\n\n        t0_broadcast = np.broadcast_to(self.t0, shape=(3, 10, 5))\n        assert t0_broadcast.shape == (3, 10, 5)\n        assert np.all(t0_broadcast.jd1 == self.t0.jd1)\n        assert np.may_share_memory(t0_broadcast.jd1, self.t0.jd1)\n        assert t0_broadcast.location is None\n        t1_broadcast = np"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "shapes.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "perly with the classes that use this, such as Time and\n    # BaseRepresentation, i.e., look at their ``_apply`` methods and add\n    # relevant tests.  This is particularly important for methods that imply\n    # copies rather than views of data (see the special-case treatment of\n    # 'flatten' in Time).\n\n    @property\n    @abc.abstractmethod\n    def shape(self) -> tuple[int, ...]:\n        \"\"\"The shape of the underlying data.\"\"\"\n\n    @abc.abstractmethod\n    def _apply(method, *args, **kwargs):\n        \"\"\"Create a new instance, with ``method`` applied to underlying data.\n\n        The method is any of the shape-changing methods for `~numpy.ndarray`\n        (``reshape``, ``swapaxes``, etc.), as well as those picking particular\n        elements (``__getitem__``, ``take``, etc.). It will be applied to the\n        underlying arrays (e.g., ``jd1`` and ``jd2`` in `~astropy.time.Time`),\n        with the results used to create a new instance.\n\n        Parameters\n        ----------\n        method : str\n            Method to be applied to the instance's internal data arrays.\n        args : tuple\n            Any positional arguments for ``method``.\n        kwargs : dict\n            Any keyword arguments for ``method``.\n\n        \"\"\"\n\n    @property\n    def ndim(self) -> int:\n        \"\"\"The number of dimensions of the instance and underlying arrays.\"\"\"\n        return len(self.shape)\n\n    @property\n    def size(self) -> int:\n        \"\"\"The size of the object, as calculated from its shape.\"\"\"\n        return prod(self.shape)\n\n    @property\n    def isscalar(self) -> bool:\n        return self.shape == ()\n\n    def __len__(self) -> int:\n        if self.isscalar:\n            raise TypeError(f\"Scalar {self.__class__.__name__!r} object has no len()\")\n        return self.shape[0]\n\n    def __bool__(self) -> bool:\n        \"\"\"Any instance should evaluate to True, except when it is empty.\"\"\"\n        return self.size > 0\n\n    def __getitem__(self, item):\n        try:\n            return self._apply("}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "test_representation_methods.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ".d_distance.shape == (2, 3, 7)\n\n        # this works with the broadcasting.\n        self.s1.shape = (2, 3, 7)\n        assert self.s1.shape == (2, 3, 7)\n        assert self.s1.lon.shape == (2, 3, 7)\n        assert self.s1.lat.shape == (2, 3, 7)\n        assert self.s1.distance.shape == (2, 3, 7)\n        assert self.s1.distance.strides == (0, 0, 0)\n\n        # but this one does not.\n        oldshape = self.s1.shape\n        with pytest.raises(ValueError):\n            self.s1.shape = (1,)\n        with pytest.raises(AttributeError):\n            self.s1.shape = (42,)\n        assert self.s1.shape == oldshape\n        assert self.s1.lon.shape == oldshape\n        assert self.s1.lat.shape == oldshape\n        assert self.s1.distance.shape == oldshape\n\n        # Finally, a more complicated one that checks that things get reset\n        # properly if it is not the first component that fails.\n        s2 = SphericalRepresentation(\n            self.s1.lon.copy(), self.s1.lat, self.s1.distance, copy=False\n        )\n        assert 0 not in s2.lon.strides\n        assert 0 in s2.lat.strides\n        with pytest.raises(AttributeError):\n            s2.shape = (42,)\n        assert s2.shape == oldshape\n        assert s2.lon.shape == oldshape\n        assert s2.lat.shape == oldshape\n        assert s2.distance.shape == oldshape\n        assert 0 not in s2.lon.strides\n        assert 0 in s2.lat.strides\n\n\nclass TestShapeFunctions(ShapeSetup):\n    @needs_array_function\n    def test_broadcast_to(self):\n        s0_broadcast = np.broadcast_to(self.s0, (3, 6, 7))\n        s0_diff = s0_broadcast.differentials[\"s\"]\n        assert type(s0_broadcast) is type(self.s0)\n        assert s0_broadcast.shape == (3, 6, 7)\n        assert s0_diff.shape == s0_broadcast.shape\n        assert np.all(s0_broadcast.lon == self.s0.lon)\n        assert np.all(s0_broadcast.lat == self.s0.lat)\n        assert np.all(s0_broadcast.distance == self.s0.distance)\n        assert np.may_share_memory(s0_broadcast.lon, self.s0.lon)\n        as"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "compat.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/nddata", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "f._uncertainty = value\n\n    # Override unit so that we can add a setter.\n    @property\n    def unit(self):\n        return self._unit\n\n    @unit.setter\n    def unit(self, value):\n        from . import conf\n\n        try:\n            if self._unit is not None and conf.warn_setting_unit_directly:\n                log.info(\n                    \"Setting the unit directly changes the unit without \"\n                    \"updating the data or uncertainty. Use the \"\n                    \".convert_unit_to() method to change the unit and \"\n                    \"scale values appropriately.\"\n                )\n        except AttributeError:\n            # raised if self._unit has not been set yet, in which case the\n            # warning is irrelevant\n            pass\n\n        if value is None:\n            self._unit = None\n        else:\n            self._unit = Unit(value)\n\n    # Implement mask in a way that converts nicely to a numpy masked array\n    @property\n    def mask(self):\n        if self._mask is np.ma.nomask:\n            return None\n        else:\n            return self._mask\n\n    @mask.setter\n    def mask(self, value):\n        # Check that value is not either type of null mask.\n        if (value is not None) and (value is not np.ma.nomask):\n            mask = np.asarray(value, dtype=np.bool_)\n            if mask.shape != self.data.shape:\n                raise ValueError(\n                    f\"dimensions of mask {mask.shape} and data {self.data.shape} do not match\"\n                )\n            else:\n                self._mask = mask\n        else:\n            # internal representation should be one numpy understands\n            self._mask = np.ma.nomask\n\n    @property\n    def shape(self):\n        \"\"\"\n        shape tuple of this object's data.\n        \"\"\"\n        return self.data.shape\n\n    @property\n    def size(self):\n        \"\"\"\n        integer size of this object's data.\n        \"\"\"\n        return self.data.size\n\n    @property\n    def dtype(self):\n        \"\"\"\n        `nump"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "test_representation_methods.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "s0_broadcast.lon == self.s0.lon)\n        assert np.all(s0_broadcast.lat == self.s0.lat)\n        assert np.all(s0_broadcast.distance == self.s0.distance)\n        assert np.may_share_memory(s0_broadcast.lon, self.s0.lon)\n        assert np.may_share_memory(s0_broadcast.lat, self.s0.lat)\n        assert np.may_share_memory(s0_broadcast.distance, self.s0.distance)\n\n\nclass TestSetShape(ShapeSetup):\n    def test_shape_setting(self):\n        # Shape-setting should be on the object itself, since copying removes\n        # zero-strides due to broadcasting.  Hence, this should be the only\n        # test in this class.\n        self.s0.shape = (2, 3, 7)\n        assert self.s0.shape == (2, 3, 7)\n        assert self.s0.lon.shape == (2, 3, 7)\n        assert self.s0.lat.shape == (2, 3, 7)\n        assert self.s0.distance.shape == (2, 3, 7)\n        assert self.diff.shape == (2, 3, 7)\n        assert self.diff.d_lon.shape == (2, 3, 7)\n        assert self.diff.d_lat.shape == (2, 3, 7)\n        assert self.diff.d_distance.shape == (2, 3, 7)\n\n        # this works with the broadcasting.\n        self.s1.shape = (2, 3, 7)\n        assert self.s1.shape == (2, 3, 7)\n        assert self.s1.lon.shape == (2, 3, 7)\n        assert self.s1.lat.shape == (2, 3, 7)\n        assert self.s1.distance.shape == (2, 3, 7)\n        assert self.s1.distance.strides == (0, 0, 0)\n\n        # but this one does not.\n        oldshape = self.s1.shape\n        with pytest.raises(ValueError):\n            self.s1.shape = (1,)\n        with pytest.raises(AttributeError):\n            self.s1.shape = (42,)\n        assert self.s1.shape == oldshape\n        assert self.s1.lon.shape == oldshape\n        assert self.s1.lat.shape == oldshape\n        assert self.s1.distance.shape == oldshape\n\n        # Finally, a more complicated one that checks that things get reset\n        # properly if it is not the first component that fails.\n        s2 = SphericalRepresentation(\n            self.s1.lon.copy(), self.s1.lat, self.s1.distance, copy=False\n"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_function_helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ected = func(self.a, *args, **kwargs)\n        assert_array_equal(o, expected)\n\n\nclass InvariantMaskTestSetup(MaskedArraySetup):\n    def check(self, func, *args, **kwargs):\n        o = func(self.ma, *args, **kwargs)\n        expected = func(self.a, *args, **kwargs)\n        assert_array_equal(o.unmasked, expected)\n        assert_array_equal(o.mask, self.mask_a)\n\n\nclass TestShapeInformation(BasicTestSetup):\n    def test_shape(self):\n        assert np.shape(self.ma) == (2, 3)\n\n    def test_size(self):\n        assert np.size(self.ma) == 6\n\n    def test_ndim(self):\n        assert np.ndim(self.ma) == 2\n\n\nclass TestShapeManipulation(BasicTestSetup):\n    # Note: do not parametrize the below, since test names are used\n    # to check coverage.\n    def test_reshape(self):\n        self.check(np.reshape, (6, 1))\n\n    def test_ravel(self):\n        self.check(np.ravel)\n\n    def test_moveaxis(self):\n        self.check(np.moveaxis, 0, 1)\n\n    def test_rollaxis(self):\n        self.check(np.rollaxis, 0, 2)\n\n    def test_swapaxes(self):\n        self.check(np.swapaxes, 0, 1)\n\n    def test_transpose(self):\n        self.check(np.transpose)\n\n    if not NUMPY_LT_2_0:\n\n        def test_matrix_transpose(self):\n            self.check(np.matrix_transpose)\n\n    def test_atleast_1d(self):\n        self.check(np.atleast_1d)\n        o, so = np.atleast_1d(self.mb[0], self.mc[0])\n        assert o.shape == o.mask.shape == so.shape == so.mask.shape == (1,)\n\n    def test_atleast_2d(self):\n        self.check(np.atleast_2d)\n        o, so = np.atleast_2d(self.mb[0], self.mc[0])\n        assert o.shape == o.mask.shape == so.shape == so.mask.shape == (1, 1)\n\n    def test_atleast_3d(self):\n        self.check(np.atleast_3d)\n        o, so = np.atleast_3d(self.mb[0], self.mc[0])\n        assert o.shape == o.mask.shape == so.shape == so.mask.shape == (1, 1, 1)\n\n    def test_expand_dims(self):\n        self.check(np.expand_dims, 1)\n\n    def test_squeeze(self):\n        o = np.squeeze(self.mc)\n        assert o.shape == o."}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "test_representation_methods.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "        )\n        assert 0 not in s2.lon.strides\n        assert 0 in s2.lat.strides\n        with pytest.raises(AttributeError):\n            s2.shape = (42,)\n        assert s2.shape == oldshape\n        assert s2.lon.shape == oldshape\n        assert s2.lat.shape == oldshape\n        assert s2.distance.shape == oldshape\n        assert 0 not in s2.lon.strides\n        assert 0 in s2.lat.strides\n\n\nclass TestShapeFunctions(ShapeSetup):\n    @needs_array_function\n    def test_broadcast_to(self):\n        s0_broadcast = np.broadcast_to(self.s0, (3, 6, 7))\n        s0_diff = s0_broadcast.differentials[\"s\"]\n        assert type(s0_broadcast) is type(self.s0)\n        assert s0_broadcast.shape == (3, 6, 7)\n        assert s0_diff.shape == s0_broadcast.shape\n        assert np.all(s0_broadcast.lon == self.s0.lon)\n        assert np.all(s0_broadcast.lat == self.s0.lat)\n        assert np.all(s0_broadcast.distance == self.s0.distance)\n        assert np.may_share_memory(s0_broadcast.lon, self.s0.lon)\n        assert np.may_share_memory(s0_broadcast.lat, self.s0.lat)\n        assert np.may_share_memory(s0_broadcast.distance, self.s0.distance)\n\n        s1_broadcast = np.broadcast_to(self.s1, shape=(3, 6, 7))\n        s1_diff = s1_broadcast.differentials[\"s\"]\n        assert s1_broadcast.shape == (3, 6, 7)\n        assert s1_diff.shape == s1_broadcast.shape\n        assert np.all(s1_broadcast.lat == self.s1.lat)\n        assert np.all(s1_broadcast.lon == self.s1.lon)\n        assert np.all(s1_broadcast.distance == self.s1.distance)\n        assert s1_broadcast.distance.shape == (3, 6, 7)\n        assert np.may_share_memory(s1_broadcast.lat, self.s1.lat)\n        assert np.may_share_memory(s1_broadcast.lon, self.s1.lon)\n        assert np.may_share_memory(s1_broadcast.distance, self.s1.distance)\n\n        # A final test that \"may_share_memory\" equals \"does_share_memory\"\n        # Do this on a copy, to keep self.s0 unchanged.\n        sc = self.s0.copy()\n        assert not np.may_share_memory(sc.lon, self.s0.l"}], "retrieved_count": 10, "cost_time": 1.0565309524536133}
{"question": "How does the cosmology trait mixin that provides Hubble parameter functionality leverage the functools caching decorator to optimize repeated access to dimensionless, time, and distance properties while maintaining unit consistency?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 0, "end_line": 1615, "belongs_to": {"file_name": "hubble.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/traits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"Hubble parameter trait.\n\nThis is private API. See `~astropy.cosmology.traits` for public API.\n\"\"\"\n\n__all__ = [\"HubbleParameter\"]\n\nfrom collections.abc import Callable\nfrom functools import cached_property\nfrom typing import Any\n\nimport numpy as np\nfrom numpy.typing import ArrayLike, NDArray\n\nimport astropy.units as u\nfrom astropy import constants as const\nfrom astropy.cosmology._src.utils import deprecated_keywords\nfrom astropy.units import Quantity\n\n\nclass HubbleParameter:\n    \"\"\"The object has attributes and methods for the Hubble parameter.\"\"\"\n\n    H0: Quantity\n    \"\"\"Hubble Parameter at redshift 0.\"\"\"\n\n    efunc: Callable[[Any], NDArray[Any]]\n\n    @deprecated_keywords(\"z\", since=\"7.0\")\n    def H(self, z: Quantity | ArrayLike) -> Quantity:\n        \"\"\"Hubble parameter at redshift ``z``.\n\n        Parameters\n        ----------\n        z : Quantity-like ['redshift'], array-like\n            Input redshift.\n\n        Returns\n        -------\n        H : Quantity ['frequency']\n            Hubble parameter at each input redshift.\n        \"\"\"\n        return self.H0 * self.efunc(z)\n\n    @cached_property\n    def h(self) -> np.floating[Any]:\n        \"\"\"Dimensionless Hubble constant: h = H_0 / 100 [km/sec/Mpc].\"\"\"\n        return self.H0.to_value(\"km/(s Mpc)\") / 100.0\n\n    @cached_property\n    def hubble_time(self) -> u.Quantity:\n        \"\"\"Hubble time.\"\"\"\n        return (1 / self.H0).to(u.Gyr)\n\n    @cached_property\n    def hubble_distance(self) -> u.Quantity:\n        \"\"\"Hubble distance.\"\"\"\n        return (const.c / self.H0).to(u.Mpc)\n"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "test_base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "1 / cosmo.H0) << u.Gyr)\n\n    def test_hubble_distance(self, cosmo_cls, cosmo):\n        \"\"\"Test ``cached_property`` ``hubble_distance``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.hubble_distance, cached_property)\n\n        # on the instance\n        assert cosmo.hubble_distance == (const.c / cosmo.H0).to(u.Mpc)\n\n    def test_critical_density0(self, cosmo_cls, cosmo):\n        \"\"\"Test ``cached_property`` ``critical_density0``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.critical_density0, cached_property)\n\n        # on the instance\n        assert cosmo.critical_density0.unit == u.g / u.cm**3\n        assert u.allclose(  # sanity check\n            cosmo.critical_density0, 3 * cosmo.H0**2 / (8 * np.pi * const.G)\n        )\n\n    def test_Ogamma0(self, cosmo_cls, cosmo):\n        \"\"\"Test ``cached_property`` ``Ogamma0``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.Ogamma0, cached_property)\n\n        # on the instance\n        # Ogamma cor \\propto T^4/rhocrit\n        expect = _a_B_c2 * cosmo.Tcmb0.value**4 / cosmo.critical_density0.value\n        assert np.allclose(cosmo.Ogamma0, expect)\n        # check absolute equality to 0 if Tcmb0 is 0\n        if cosmo.Tcmb0 == 0:\n            assert cosmo.Ogamma0 == 0\n\n    def test_Onu0(self, cosmo_cls, cosmo):\n        \"\"\"Test ``cached_property`` ``Onu0``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.Onu0, cached_property)\n\n        # on the instance\n        # neutrino temperature <= photon temperature since the neutrinos\n        # decouple first.\n        if cosmo.has_massive_nu:  # Tcmb0 > 0 & has massive\n            # check the expected formula\n            assert cosmo.Onu0 == cosmo.Ogamma0 * cosmo.nu_relative_density(0)\n            # a sanity check on on the ratio of neutrinos to photons\n            # technically it could be 1, but not for any of the tested cases.\n            assert cosmo.nu_relative_density(0) <= 1\n        elif cosmo.Tcmb0 == 0:\n            assert cosmo."}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "test_base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ", 0.7137658555036082 * cosmo.Tcmb0, rtol=1e-5)\n\n    def test_has_massive_nu(self, cosmo_cls, cosmo):\n        \"\"\"Test property ``has_massive_nu``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.has_massive_nu, property)\n        assert cosmo_cls.has_massive_nu.fset is None  # immutable\n\n        # on the instance\n        if cosmo.Tnu0 == 0:\n            assert cosmo.has_massive_nu is False\n        else:\n            assert cosmo.has_massive_nu is cosmo._massivenu\n\n    def test_h(self, cosmo_cls, cosmo):\n        \"\"\"Test ``cached_property`` ``h``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.h, cached_property)\n\n        # on the instance\n        assert np.allclose(cosmo.h, cosmo.H0.value / 100.0)\n\n    def test_hubble_time(self, cosmo_cls, cosmo):\n        \"\"\"Test ``cached_property`` ``hubble_time``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.hubble_time, cached_property)\n\n        # on the instance\n        assert u.allclose(cosmo.hubble_time, (1 / cosmo.H0) << u.Gyr)\n\n    def test_hubble_distance(self, cosmo_cls, cosmo):\n        \"\"\"Test ``cached_property`` ``hubble_distance``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.hubble_distance, cached_property)\n\n        # on the instance\n        assert cosmo.hubble_distance == (const.c / cosmo.H0).to(u.Mpc)\n\n    def test_critical_density0(self, cosmo_cls, cosmo):\n        \"\"\"Test ``cached_property`` ``critical_density0``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.critical_density0, cached_property)\n\n        # on the instance\n        assert cosmo.critical_density0.unit == u.g / u.cm**3\n        assert u.allclose(  # sanity check\n            cosmo.critical_density0, 3 * cosmo.H0**2 / (8 * np.pi * const.G)\n        )\n\n    def test_Ogamma0(self, cosmo_cls, cosmo):\n        \"\"\"Test ``cached_property`` ``Ogamma0``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.Ogamma0, cached_property)\n\n        # on the instance\n        # Ogamma cor \\propt"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "w0cdm.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ift z:\n\n    >>> z = 0.5\n    >>> dc = cosmo.comoving_distance(z)\n\n    To get an equivalent cosmology, but of type `astropy.cosmology.wCDM`,\n    use :attr:`astropy.cosmology.FlatFLRWMixin.nonflat`.\n\n    >>> print(cosmo.nonflat)\n    wCDM(H0=70.0 km / (Mpc s), Om0=0.3, Ode0=0.7, ...\n    \"\"\"\n\n    def __post_init__(self):\n        super().__post_init__()\n\n        # Please see :ref:`astropy-cosmology-fast-integrals` for discussion\n        # about what is being done here.\n        if self.Tcmb0.value == 0:\n            inv_efunc_scalar = scalar_inv_efuncs.fwcdm_inv_efunc_norel\n            inv_efunc_scalar_args = (self.Om0, self.Ode0, self.w0)\n        elif not self._massivenu:\n            inv_efunc_scalar = scalar_inv_efuncs.fwcdm_inv_efunc_nomnu\n            inv_efunc_scalar_args = (\n                self.Om0,\n                self.Ode0,\n                self.Ogamma0 + self.Onu0,\n                self.w0,\n            )\n        else:\n            inv_efunc_scalar = scalar_inv_efuncs.fwcdm_inv_efunc\n            inv_efunc_scalar_args = (\n                self.Om0,\n                self.Ode0,\n                self.Ogamma0,\n                self._neff_per_nu,\n                self._nmasslessnu,\n                self._nu_y_list,\n                self.w0,\n            )\n        object.__setattr__(self, \"_inv_efunc_scalar\", inv_efunc_scalar)\n        object.__setattr__(self, \"_inv_efunc_scalar_args\", inv_efunc_scalar_args)\n\n    @deprecated_keywords(\"z\", since=\"7.0\")\n    def efunc(self, z):\n        \"\"\"Function used to calculate H(z), the Hubble parameter.\n\n        Parameters\n        ----------\n        z : Quantity-like ['redshift'], array-like\n            Input redshift.\n\n            .. versionchanged:: 7.0\n                Passing z as a keyword argument is deprecated.\n\n        Returns\n        -------\n        E : ndarray or float\n            The redshift scaling of the Hubble constant.\n            Returns `float` if the input is scalar.\n            Defined such that :math:`H(z) = H_0 E(z)`.\n        \""}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "test_base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "cosmo.Om0 - cosmo.Ob0)\n\n    def test_Ok0(self, cosmo_cls, cosmo):\n        \"\"\"Test ``cached_property`` ``Ok0``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.Ok0, cached_property)\n\n        # on the instance\n        assert np.allclose(\n            cosmo.Ok0, 1.0 - (cosmo.Om0 + cosmo.Ode0 + cosmo.Ogamma0 + cosmo.Onu0)\n        )\n\n    def test_is_flat(self, cosmo_cls, cosmo):\n        \"\"\"Test property ``is_flat``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.is_flat, property)\n        assert cosmo_cls.is_flat.fset is None  # immutable\n\n        # on the instance\n        assert isinstance(cosmo.is_flat, bool)\n        assert cosmo.is_flat is bool((cosmo.Ok0 == 0.0) and (cosmo.Otot0 == 1.0))\n\n    def test_Tnu0(self, cosmo_cls, cosmo):\n        \"\"\"Test ``cached_property`` ``Tnu0``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.Tnu0, cached_property)\n\n        # on the instance\n        assert cosmo.Tnu0.unit == u.K\n        assert u.allclose(cosmo.Tnu0, 0.7137658555036082 * cosmo.Tcmb0, rtol=1e-5)\n\n    def test_has_massive_nu(self, cosmo_cls, cosmo):\n        \"\"\"Test property ``has_massive_nu``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.has_massive_nu, property)\n        assert cosmo_cls.has_massive_nu.fset is None  # immutable\n\n        # on the instance\n        if cosmo.Tnu0 == 0:\n            assert cosmo.has_massive_nu is False\n        else:\n            assert cosmo.has_massive_nu is cosmo._massivenu\n\n    def test_h(self, cosmo_cls, cosmo):\n        \"\"\"Test ``cached_property`` ``h``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.h, cached_property)\n\n        # on the instance\n        assert np.allclose(cosmo.h, cosmo.H0.value / 100.0)\n\n    def test_hubble_time(self, cosmo_cls, cosmo):\n        \"\"\"Test ``cached_property`` ``hubble_time``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.hubble_time, cached_property)\n\n        # on the instance\n        assert u.allclose(cosmo.hubble_time, ("}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_parameters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n\"\"\"Parameter test mixin classes.\"\"\"\n\nimport copy\nfrom inspect import BoundArguments\n\nimport numpy as np\nimport pytest\n\nimport astropy.units as u\nfrom astropy.cosmology import Cosmology, FlatFLRWMixin, Parameter\nfrom astropy.cosmology._src.parameter import MISSING\nfrom astropy.cosmology._src.tests.test_core import ParameterTestMixin\nfrom astropy.tests.helper import assert_quantity_allclose\n\n\nclass ParameterH0TestMixin(ParameterTestMixin):\n    \"\"\"Tests for `astropy.cosmology.Parameter` H0 on a Cosmology.\n\n    H0 is a descriptor, which are tested by mixin, here with ``TestFLRW``.\n    These tests expect dicts ``_cls_args`` and ``cls_kwargs`` which give the\n    args and kwargs for the cosmology class, respectively. See ``TestFLRW``.\n    \"\"\"\n\n    def test_H0(self, cosmo_cls: type[Cosmology], cosmo: Cosmology):\n        \"\"\"Test Parameter ``H0``.\"\"\"\n        unit = u.Unit(\"km/(s Mpc)\")\n\n        # on the class\n        H0 = cosmo_cls.parameters[\"H0\"]\n        assert isinstance(H0, Parameter)\n        assert \"Hubble constant\" in H0.__doc__\n        assert H0.unit == unit\n        assert H0.default is MISSING\n\n        # validation\n        assert H0.validate(cosmo, 1) == 1 * unit\n        assert H0.validate(cosmo, 10 * unit) == 10 * unit\n        with pytest.raises(ValueError, match=\"H0 is a non-scalar quantity\"):\n            H0.validate(cosmo, [1, 2])\n\n        # on the instance\n        assert cosmo.H0 is cosmo.__dict__[\"H0\"]\n        assert cosmo.H0 == self._cls_args[\"H0\"]\n        assert isinstance(cosmo.H0, u.Quantity) and cosmo.H0.unit == unit\n\n    def test_init_H0(self, cosmo_cls: type[Cosmology], ba: BoundArguments):\n        \"\"\"Test initialization for values of ``H0``.\"\"\"\n        # test that it works with units\n        cosmo = cosmo_cls(*ba.args, **ba.kwargs)\n        assert cosmo.H0 == ba.arguments[\"H0\"]\n\n        # also without units\n        ba.arguments[\"H0\"] = ba.arguments[\"H0\"].value  # strip units\n        cosmo = c"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "hese adapted from Hogg 1999, astro-ph/9905116\n# and Linder 2003, PRL 90, 91301\n\n__all__ = [\"Cosmology\", \"CosmologyError\", \"FlatCosmologyMixin\"]\n\n\n##############################################################################\n# Parameters\n\n# typing\n# NOTE: private b/c RTD error\n_FlatCosmoT = TypeVar(\"_FlatCosmoT\", bound=\"FlatCosmologyMixin\")\n\n\n# dataclass transformation\ndef _with_signature(cls: type[\"Cosmology\"]) -> type[\"Cosmology\"]:\n    \"\"\"Decorator to precompute the class' signature.\n\n    This provides around a 20x speedup for future calls of ``inspect.signature(cls)``.\n    `Cosmology` has a lot of I/O methods that use the signature, so this is a\n    significant speedup for those methods.\n\n    Note that CPython does not promise that this precomputation is a stable feature.\n    If it is removed, the worst that will happen is that the signature will be\n    computed on the fly, the speedup will be lost, and this decorator can be\n    deprecated.\n    \"\"\"\n    cls.__signature__ = None  # clear the signature cache\n    cls.__signature__ = inspect.signature(cls)  # add the new signature to the class\n    return cls\n\n\ndef dataclass_decorator(cls):\n    \"\"\"Decorator for the dataclass transform.\n\n    Returns\n    -------\n    cls : type\n        The `cls` transformed into a frozen `~dataclasses.dataclass`.\n        The ``__eq__`` method is custom (``eq=False``).\n        The signature is precomputed and added to the class.\n    \"\"\"\n    return _with_signature(dataclass(frozen=True, repr=True, eq=False, init=True)(cls))\n\n\n##############################################################################\n\n\nclass CosmologyError(Exception):\n    pass\n\n\n# TODO: replace with `field(converter=lambda x: None if x is None else str(x))` when\n#       the `converter` argument is available in `field` (py3.13, maybe?).\n#       See https://peps.python.org/pep-0712/\n@dataclass(frozen=True, slots=True)\nclass _NameField:\n    default: str | None = None\n\n    def __get__(self, instance: Union[\"Cosmology\", None]"}, {"start_line": 21000, "end_line": 23000, "belongs_to": {"file_name": "lambdacdm.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "le constant.\n            Returns `float` if the input is scalar.\n            Defined such that :math:`H(z) = H_0 E(z)`.\n        \"\"\"\n        # We override this because it takes a particularly simple\n        # form for a cosmological constant\n        Or = self.Ogamma0 + (\n            self.Onu0\n            if not self._massivenu\n            else self.Ogamma0 * self.nu_relative_density(z)\n        )\n        zp1 = aszarr(z) + 1.0  # (converts z [unit] -> z [dimensionless])\n\n        return np.sqrt(zp1**2 * ((Or * zp1 + self.Om0) * zp1 + self.Ok0) + self.Ode0)\n\n    @deprecated_keywords(\"z\", since=\"7.0\")\n    def inv_efunc(self, z):\n        r\"\"\"Function used to calculate :math:`\\frac{1}{H_z}`.\n\n        Parameters\n        ----------\n        z : Quantity-like ['redshift'], array-like\n            Input redshift.\n\n            .. versionchanged:: 7.0\n                Passing z as a keyword argument is deprecated.\n\n        Returns\n        -------\n        E : ndarray or float\n            The inverse redshift scaling of the Hubble constant.\n            Returns `float` if the input is scalar.\n            Defined such that :math:`H_z = H_0 / E`.\n        \"\"\"\n        Or = self.Ogamma0 + (\n            self.Onu0\n            if not self._massivenu\n            else self.Ogamma0 * self.nu_relative_density(z)\n        )\n        zp1 = aszarr(z) + 1.0  # (converts z [unit] -> z [dimensionless])\n\n        return (zp1**2 * ((Or * zp1 + self.Om0) * zp1 + self.Ok0) + self.Ode0) ** (-0.5)\n\n\n@dataclass_decorator\nclass FlatLambdaCDM(FlatFLRWMixin, LambdaCDM):\n    \"\"\"FLRW cosmology with a cosmological constant and no curvature.\n\n    This has no additional attributes beyond those of FLRW.\n\n    Parameters\n    ----------\n    H0 : float or scalar quantity-like ['frequency']\n        Hubble constant at z = 0. If a float, must be in [km/sec/Mpc].\n\n    Om0 : float\n        Omega matter: density of non-relativistic matter in units of the\n        critical density at z=0.\n\n    Tcmb0 : float or scalar quantity-like ['t"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "w0cdm.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n        ----------\n        z : Quantity-like ['redshift'], array-like\n            Input redshift.\n\n            .. versionchanged:: 7.0\n                Passing z as a keyword argument is deprecated.\n\n        Returns\n        -------\n        E : ndarray or float\n            The redshift scaling of the Hubble constant.\n            Returns `float` if the input is scalar.\n            Defined such that :math:`H(z) = H_0 E(z)`.\n        \"\"\"\n        Or = self.Ogamma0 + (\n            self.Onu0\n            if not self._massivenu\n            else self.Ogamma0 * self.nu_relative_density(z)\n        )\n        zp1 = aszarr(z) + 1.0  # (converts z [unit] -> z [dimensionless])\n\n        return sqrt(\n            zp1**2 * ((Or * zp1 + self.Om0) * zp1 + self.Ok0)\n            + self.Ode0 * zp1 ** (3.0 * (1.0 + self.w0))\n        )\n\n    @deprecated_keywords(\"z\", since=\"7.0\")\n    def inv_efunc(self, z):\n        r\"\"\"Function used to calculate :math:`\\frac{1}{H_z}`.\n\n        Parameters\n        ----------\n        z : Quantity-like ['redshift'], array-like\n            Input redshift.\n\n            .. versionchanged:: 7.0\n                Passing z as a keyword argument is deprecated.\n\n        Returns\n        -------\n        E : ndarray or float\n            The inverse redshift scaling of the Hubble constant.\n            Returns `float` if the input is scalar.\n            Defined such that :math:`H_z = H_0 / E`.\n        \"\"\"\n        Or = self.Ogamma0 + (\n            self.Onu0\n            if not self._massivenu\n            else self.Ogamma0 * self.nu_relative_density(z)\n        )\n        zp1 = aszarr(z) + 1.0  # (converts z [unit] -> z [dimensionless])\n\n        return (\n            zp1**2 * ((Or * zp1 + self.Om0) * zp1 + self.Ok0)\n            + self.Ode0 * zp1 ** (3.0 * (1.0 + self.w0))\n        ) ** (-0.5)\n\n\n@dataclass_decorator\nclass FlatwCDM(FlatFLRWMixin, wCDM):\n    \"\"\"FLRW cosmology with a constant dark energy EoS and no spatial curvature.\n\n    This has one additional attribute beyond those of FL"}, {"start_line": 22000, "end_line": 24000, "belongs_to": {"file_name": "lambdacdm.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "shift scaling of the Hubble constant.\n            Returns `float` if the input is scalar.\n            Defined such that :math:`H_z = H_0 / E`.\n        \"\"\"\n        Or = self.Ogamma0 + (\n            self.Onu0\n            if not self._massivenu\n            else self.Ogamma0 * self.nu_relative_density(z)\n        )\n        zp1 = aszarr(z) + 1.0  # (converts z [unit] -> z [dimensionless])\n\n        return (zp1**2 * ((Or * zp1 + self.Om0) * zp1 + self.Ok0) + self.Ode0) ** (-0.5)\n\n\n@dataclass_decorator\nclass FlatLambdaCDM(FlatFLRWMixin, LambdaCDM):\n    \"\"\"FLRW cosmology with a cosmological constant and no curvature.\n\n    This has no additional attributes beyond those of FLRW.\n\n    Parameters\n    ----------\n    H0 : float or scalar quantity-like ['frequency']\n        Hubble constant at z = 0. If a float, must be in [km/sec/Mpc].\n\n    Om0 : float\n        Omega matter: density of non-relativistic matter in units of the\n        critical density at z=0.\n\n    Tcmb0 : float or scalar quantity-like ['temperature'], optional\n        Temperature of the CMB z=0. If a float, must be in [K]. Default: 0 [K].\n        Setting this to zero will turn off both photons and neutrinos\n        (even massive ones).\n\n    Neff : float, optional\n        Effective number of Neutrino species. Default 3.04.\n\n    m_nu : quantity-like ['energy', 'mass'] or array-like, optional\n        Mass of each neutrino species in [eV] (mass-energy equivalency enabled).\n        If this is a scalar Quantity, then all neutrino species are assumed to\n        have that mass. Otherwise, the mass of each species. The actual number\n        of neutrino species (and hence the number of elements of m_nu if it is\n        not scalar) must be the floor of Neff. Typically this means you should\n        provide three neutrino masses unless you are considering something like\n        a sterile neutrino.\n\n    Ob0 : float or None, optional\n        Omega baryons: density of baryonic matter in units of the critical\n        density at z=0.  I"}], "retrieved_count": 10, "cost_time": 1.0647847652435303}
{"question": "How does clearing the cache property in the output subformat setter propagate through the base time class hierarchy via the instance tracking dictionary to invalidate cached format and scale conversions?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 37000, "end_line": 39000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "elf.format:\n                subfmt = self.out_subfmt\n            else:\n                subfmt = self.FORMATS[format]._get_allowed_subfmt(self.out_subfmt)\n\n        cache = self.cache[\"format\"]\n        key = format, subfmt, conf.masked_array_type\n        value = cache.get(key)\n        if value is None:\n            if format == self.format:\n                tm = self\n            else:\n                tm = self.replicate(format=format)\n\n            # Some TimeFormat subclasses may not be able to handle being passes\n            # on a out_subfmt. This includes some core classes like\n            # TimeBesselianEpochString that do not have any allowed subfmts. But\n            # those do deal with `self.out_subfmt` internally, so if subfmt is\n            # the same, we do not pass it on.\n            kwargs = {}\n            if subfmt is not None and subfmt != tm.out_subfmt:\n                kwargs[\"out_subfmt\"] = subfmt\n            try:\n                value = tm._time.to_value(parent=tm, **kwargs)\n            except TypeError as exc:\n                # Try validating subfmt, e.g. for formats like 'jyear_str' that\n                # do not implement out_subfmt in to_value() (because there are\n                # no allowed subformats).  If subfmt is not valid this gives the\n                # same exception as would have occurred if the call to\n                # `to_value()` had succeeded.\n                tm._time._select_subfmts(subfmt)\n\n                # Subfmt was valid, so fall back to the original exception to see\n                # if it was lack of support for out_subfmt as a call arg.\n                if \"unexpected keyword argument 'out_subfmt'\" in str(exc):\n                    raise ValueError(\n                        f\"to_value() method for format {format!r} does not \"\n                        \"support passing a 'subfmt' argument\"\n                    ) from None\n                else:\n                    # Some unforeseen exception so raise.\n                    raise\n\n      "}, {"start_line": 64000, "end_line": 66000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "wargs),\n            val2=np.sum(np.ma.getdata(self.jd2), **kwargs),\n            divisor=divisor,\n        )\n\n        result = type(self)(\n            val=jd1,\n            val2=jd2,\n            format=\"jd\",\n            scale=self.scale,\n            copy=COPY_IF_NEEDED,\n        )\n        result.format = self.format\n        return result\n\n    @lazyproperty\n    def _id_cache(self):\n        \"\"\"Cache of all instances that share underlying data.\n\n        Helps to ensure all cached data can be deleted if the\n        underlying data is changed.\n        \"\"\"\n        return WeakValueDictionary({id(self): self})\n\n    @_id_cache.setter\n    def _id_cache(self, _id_cache):\n        _id_cache[id(self)] = self\n        # lazyproperty will do the actual storing of the result.\n\n    @lazyproperty\n    def cache(self):\n        \"\"\"\n        Return the cache associated with this instance.\n        \"\"\"\n        return defaultdict(dict)\n\n    @cache.deleter\n    def cache(self):\n        for instance in self._id_cache.values():\n            instance.cache.clear()\n\n    def __getattr__(self, attr):\n        \"\"\"\n        Get dynamic attributes to output format or do timescale conversion.\n        \"\"\"\n        if attr in self.SCALES and self.scale is not None:\n            cache = self.cache[\"scale\"]\n            if attr not in cache:\n                if attr == self.scale:\n                    tm = self\n                else:\n                    tm = self.replicate()\n                    tm._set_scale(attr)\n                    if tm.shape:\n                        # Prevent future modification of cached array-like object\n                        tm.writeable = False\n                cache[attr] = tm\n            return cache[attr]\n\n        elif attr in self.FORMATS:\n            return self.to_value(attr, subfmt=None)\n\n        elif attr in TIME_SCALES:  # allowed ones done above (self.SCALES)\n            if self.scale is None:\n                raise ScaleValueError(\n                    \"Cannot convert TimeDelta with \"\n  "}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "formats.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "t subformat for inputting string times\n    out_subfmt : str\n        Select subformat for outputting string times\n    from_jd : bool\n        If true then val1, val2 are jd1, jd2\n    \"\"\"\n\n    _default_scale = \"utc\"  # As of astropy 0.4\n    _default_precision = 3\n    _min_precision = 0\n    _max_precision = 9\n\n    subfmts = ()\n    _registry = TIME_FORMATS\n\n    # Check that numeric inputs are finite (not nan or inf). This is overridden in\n    # subclasses in which nan and inf are valid inputs.\n    _check_finite = True\n\n    def __init__(\n        self, val1, val2, scale, precision, in_subfmt, out_subfmt, from_jd=False\n    ):\n        self.scale = scale  # validation of scale done later with _check_scale\n        self.precision = precision\n        self.in_subfmt = in_subfmt\n        self.out_subfmt = out_subfmt\n\n        self._jd1, self._jd2 = None, None\n\n        if from_jd:\n            self.jd1 = val1\n            self.jd2 = val2\n        else:\n            val1, val2 = self._check_val_type(val1, val2)\n            self.set_jds(val1, val2)\n\n    def __init_subclass__(cls, **kwargs):\n        # Register time formats that define a name, but leave out astropy_time since\n        # it is not a user-accessible format and is only used for initialization into\n        # a different format.\n        if \"name\" in cls.__dict__ and cls.name != \"astropy_time\":\n            # FIXME: check here that we're not introducing a collision with\n            # an existing method or attribute; problem is it could be either\n            # astropy.time.Time or astropy.time.TimeDelta, and at the point\n            # where this is run neither of those classes have necessarily been\n            # constructed yet.\n            if \"value\" in cls.__dict__ and not hasattr(cls.value, \"fget\"):\n                raise ValueError(\"If defined, 'value' must be a property\")\n\n            cls._registry[cls.name] = cls\n\n        # If this class defines its own subfmts, preprocess the definitions.\n        if \"subfmts\" in cls.__dict__:\n "}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "formats.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "l2)\n            self.set_jds(val1, val2)\n\n    def __init_subclass__(cls, **kwargs):\n        # Register time formats that define a name, but leave out astropy_time since\n        # it is not a user-accessible format and is only used for initialization into\n        # a different format.\n        if \"name\" in cls.__dict__ and cls.name != \"astropy_time\":\n            # FIXME: check here that we're not introducing a collision with\n            # an existing method or attribute; problem is it could be either\n            # astropy.time.Time or astropy.time.TimeDelta, and at the point\n            # where this is run neither of those classes have necessarily been\n            # constructed yet.\n            if \"value\" in cls.__dict__ and not hasattr(cls.value, \"fget\"):\n                raise ValueError(\"If defined, 'value' must be a property\")\n\n            cls._registry[cls.name] = cls\n\n        # If this class defines its own subfmts, preprocess the definitions.\n        if \"subfmts\" in cls.__dict__:\n            cls.subfmts = _regexify_subfmts(cls.subfmts)\n\n        return super().__init_subclass__(**kwargs)\n\n    @classmethod\n    def _get_allowed_subfmt(cls, subfmt):\n        \"\"\"Get an allowed subfmt for this class, either the input ``subfmt``\n        if this is valid or '*' as a default.  This method gets used in situations\n        where the format of an existing Time object is changing and so the\n        out_ or in_subfmt may need to be coerced to the default '*' if that\n        ``subfmt`` is no longer valid.\n        \"\"\"\n        try:\n            cls._select_subfmts(subfmt)\n        except ValueError:\n            subfmt = \"*\"\n        return subfmt\n\n    @property\n    def in_subfmt(self):\n        return self._in_subfmt\n\n    @in_subfmt.setter\n    def in_subfmt(self, subfmt):\n        # Validate subfmt value for this class, raises ValueError if not.\n        self._select_subfmts(subfmt)\n        self._in_subfmt = subfmt\n\n    @property\n    def out_subfmt(self):\n        return self._out_subfmt"}, {"start_line": 29000, "end_line": 31000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ided through Time methods.  These values may be supplied by\n            # the user or computed based on available approximations.  The\n            # get_delta_ methods are available for only one combination of\n            # sys1, sys2 though the property applies for both xform directions.\n            args = [jd1, jd2]\n            for sys12 in ((sys1, sys2), (sys2, sys1)):\n                dt_method = \"_get_delta_{}_{}\".format(*sys12)\n                try:\n                    get_dt = getattr(self, dt_method)\n                except AttributeError:\n                    pass\n                else:\n                    args.append(get_dt(jd1, jd2))\n                    break\n\n            conv_func = getattr(erfa, sys1 + sys2)\n            jd1, jd2 = conv_func(*args)\n\n        jd1, jd2 = day_frac(jd1, jd2)\n\n        self._time = self.FORMATS[self.format](\n            jd1,\n            jd2,\n            scale,\n            self.precision,\n            self.in_subfmt,\n            self.out_subfmt,\n            from_jd=True,\n        )\n\n    @property\n    def precision(self):\n        \"\"\"\n        Decimal precision when outputting seconds as floating point (int\n        value between 0 and 9 inclusive).\n        \"\"\"\n        return self._time.precision\n\n    @precision.setter\n    def precision(self, val):\n        del self.cache\n        self._time.precision = val\n\n    @property\n    def in_subfmt(self):\n        \"\"\"\n        Unix wildcard pattern to select subformats for parsing string input\n        times.\n        \"\"\"\n        return self._time.in_subfmt\n\n    @in_subfmt.setter\n    def in_subfmt(self, val):\n        self._time.in_subfmt = val\n        del self.cache\n\n    @property\n    def out_subfmt(self):\n        \"\"\"\n        Unix wildcard pattern to select subformats for outputting times.\n        \"\"\"\n        return self._time.out_subfmt\n\n    @out_subfmt.setter\n    def out_subfmt(self, val):\n        # Setting the out_subfmt property here does validation of ``val``\n        self._time.out_subfmt = val\n    "}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "formats.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " subfmt_in = subfmt_in.replace(strptime_code, regex)\n\n            if \"%\" not in subfmt_in:\n                subfmt_tuple = (\n                    subfmt_tuple[0],\n                    re.compile(subfmt_in + \"$\"),\n                    subfmt_tuple[2],\n                )\n        new_subfmts.append(subfmt_tuple)\n\n    return tuple(new_subfmts)\n\n\nclass TimeFormat:\n    \"\"\"\n    Base class for time representations.\n\n    Parameters\n    ----------\n    val1 : numpy ndarray, list, number, str, or bytes\n        Values to initialize the time or times.  Bytes are decoded as ascii.\n        Quantities with time units are allowed for formats where the\n        interpretation is unambiguous.\n    val2 : numpy ndarray, list, or number; optional\n        Value(s) to initialize the time or times.  Only used for numerical\n        input, to help preserve precision.\n    scale : str\n        Time scale of input value(s)\n    precision : int\n        Precision for seconds as floating point\n    in_subfmt : str\n        Select subformat for inputting string times\n    out_subfmt : str\n        Select subformat for outputting string times\n    from_jd : bool\n        If true then val1, val2 are jd1, jd2\n    \"\"\"\n\n    _default_scale = \"utc\"  # As of astropy 0.4\n    _default_precision = 3\n    _min_precision = 0\n    _max_precision = 9\n\n    subfmts = ()\n    _registry = TIME_FORMATS\n\n    # Check that numeric inputs are finite (not nan or inf). This is overridden in\n    # subclasses in which nan and inf are valid inputs.\n    _check_finite = True\n\n    def __init__(\n        self, val1, val2, scale, precision, in_subfmt, out_subfmt, from_jd=False\n    ):\n        self.scale = scale  # validation of scale done later with _check_scale\n        self.precision = precision\n        self.in_subfmt = in_subfmt\n        self.out_subfmt = out_subfmt\n\n        self._jd1, self._jd2 = None, None\n\n        if from_jd:\n            self.jd1 = val1\n            self.jd2 = val2\n        else:\n            val1, val2 = self._check_val_type(val1, va"}, {"start_line": 30000, "end_line": 32000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    from_jd=True,\n        )\n\n    @property\n    def precision(self):\n        \"\"\"\n        Decimal precision when outputting seconds as floating point (int\n        value between 0 and 9 inclusive).\n        \"\"\"\n        return self._time.precision\n\n    @precision.setter\n    def precision(self, val):\n        del self.cache\n        self._time.precision = val\n\n    @property\n    def in_subfmt(self):\n        \"\"\"\n        Unix wildcard pattern to select subformats for parsing string input\n        times.\n        \"\"\"\n        return self._time.in_subfmt\n\n    @in_subfmt.setter\n    def in_subfmt(self, val):\n        self._time.in_subfmt = val\n        del self.cache\n\n    @property\n    def out_subfmt(self):\n        \"\"\"\n        Unix wildcard pattern to select subformats for outputting times.\n        \"\"\"\n        return self._time.out_subfmt\n\n    @out_subfmt.setter\n    def out_subfmt(self, val):\n        # Setting the out_subfmt property here does validation of ``val``\n        self._time.out_subfmt = val\n        del self.cache\n\n    @property\n    def shape(self):\n        \"\"\"The shape of the time instances.\n\n        Like `~numpy.ndarray.shape`, can be set to a new shape by assigning a\n        tuple.  Note that if different instances share some but not all\n        underlying data, setting the shape of one instance can make the other\n        instance unusable.  Hence, it is strongly recommended to get new,\n        reshaped instances with the ``reshape`` method.\n\n        Raises\n        ------\n        ValueError\n            If the new shape has the wrong total number of elements.\n        AttributeError\n            If the shape of the ``jd1``, ``jd2``, ``location``,\n            ``delta_ut1_utc``, or ``delta_tdb_tt`` attributes cannot be changed\n            without the arrays being copied.  For these cases, use the\n            `Time.reshape` method (which copies any arrays that cannot be\n            reshaped in-place).\n        \"\"\"\n        return self._time.jd1.shape\n\n    @shape.setter\n    def shap"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "formats.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "           cls.subfmts = _regexify_subfmts(cls.subfmts)\n\n        return super().__init_subclass__(**kwargs)\n\n    @classmethod\n    def _get_allowed_subfmt(cls, subfmt):\n        \"\"\"Get an allowed subfmt for this class, either the input ``subfmt``\n        if this is valid or '*' as a default.  This method gets used in situations\n        where the format of an existing Time object is changing and so the\n        out_ or in_subfmt may need to be coerced to the default '*' if that\n        ``subfmt`` is no longer valid.\n        \"\"\"\n        try:\n            cls._select_subfmts(subfmt)\n        except ValueError:\n            subfmt = \"*\"\n        return subfmt\n\n    @property\n    def in_subfmt(self):\n        return self._in_subfmt\n\n    @in_subfmt.setter\n    def in_subfmt(self, subfmt):\n        # Validate subfmt value for this class, raises ValueError if not.\n        self._select_subfmts(subfmt)\n        self._in_subfmt = subfmt\n\n    @property\n    def out_subfmt(self):\n        return self._out_subfmt\n\n    @out_subfmt.setter\n    def out_subfmt(self, subfmt):\n        # Validate subfmt value for this class, raises ValueError if not.\n        self._select_subfmts(subfmt)\n        self._out_subfmt = subfmt\n\n    @property\n    def jd1(self):\n        return self._jd1\n\n    @jd1.setter\n    def jd1(self, jd1):\n        self._jd1 = _validate_jd_for_storage(jd1)\n        if self._jd2 is not None:\n            self._jd1, self._jd2 = _broadcast_writeable(self._jd1, self._jd2)\n\n    @property\n    def jd2(self):\n        return self._jd2\n\n    @jd2.setter\n    def jd2(self, jd2):\n        self._jd2 = _validate_jd_for_storage(jd2)\n        if self._jd1 is not None:\n            self._jd1, self._jd2 = _broadcast_writeable(self._jd1, self._jd2)\n\n    @classmethod\n    @functools.cache\n    def fill_value(cls, subfmt):\n        \"\"\"\n        Return a value corresponding to J2000 (2000-01-01 12:00:00) in this format.\n\n        This is used as a fill value for masked arrays to ensure that any ERFA\n        operations on th"}, {"start_line": 22000, "end_line": 24000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "= cls._fill_masked_values(oval, oval2, mask, in_subfmt)\n                return cls(val, val2, scale, precision, in_subfmt, out_subfmt)\n            except UnitConversionError:\n                raise\n            except (ValueError, TypeError) as err:\n                # If ``format`` specified then there is only one possibility, so raise\n                # immediately and include the upstream exception message to make it\n                # easier for user to see what is wrong.\n                if len(formats) == 1:\n                    raise ValueError(\n                        f\"Input values did not match the format class {format}:\"\n                        + os.linesep\n                        + f\"{err.__class__.__name__}: {err}\"\n                    ) from err\n                else:\n                    problems[name] = err\n\n        message = (\n            \"Input values did not match any of the formats where the format \"\n            \"keyword is optional:\\n\"\n        ) + \"\\n\".join(f\"- '{name}': {err}\" for name, err in problems.items())\n        raise ValueError(message)\n\n    @property\n    def writeable(self):\n        return self._time.jd1.flags.writeable & self._time.jd2.flags.writeable\n\n    @writeable.setter\n    def writeable(self, value):\n        self._time.jd1.flags.writeable = value\n        self._time.jd2.flags.writeable = value\n\n    @property\n    def format(self):\n        \"\"\"\n        Get or set time format.\n\n        The format defines the way times are represented when accessed via the\n        ``.value`` attribute.  By default it is the same as the format used for\n        initializing the `Time` instance, but it can be set to any other value\n        that could be used for initialization.  These can be listed with::\n\n          >>> list(Time.FORMATS)\n          ['jd', 'mjd', 'decimalyear', 'unix', 'unix_tai', 'cxcsec', 'gps', 'plot_date',\n           'stardate', 'datetime', 'ymdhms', 'iso', 'isot', 'yday', 'datetime64',\n           'fits', 'byear', 'jyear', 'byear_str', 'jyear_str"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "formats.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "          \"\"\"\n            return val.view(np.ndarray) if isinstance(val, np.ndarray) else val\n\n        return asarray_or_scalar(val1), asarray_or_scalar(val2)\n\n    def _check_scale(self, scale):\n        \"\"\"\n        Return a validated scale value.\n\n        If there is a class attribute 'scale' then that defines the default /\n        required time scale for this format.  In this case if a scale value was\n        provided that needs to match the class default, otherwise return\n        the class default.\n\n        Otherwise just make sure that scale is in the allowed list of\n        scales.  Provide a different error message if `None` (no value) was\n        supplied.\n        \"\"\"\n        if scale is None:\n            scale = self._default_scale\n\n        if scale not in TIME_SCALES:\n            raise ScaleValueError(\n                f\"Scale value '{scale}' not in allowed values {TIME_SCALES}\"\n            )\n\n        return scale\n\n    def set_jds(self, val1, val2):\n        \"\"\"\n        Set internal jd1 and jd2 from val1 and val2.  Must be provided\n        by derived classes.\n        \"\"\"\n        raise NotImplementedError\n\n    def to_value(self, parent=None, out_subfmt=None):\n        \"\"\"\n        Return time representation from internal jd1 and jd2 in specified\n        ``out_subfmt``.\n\n        This is the base method that ignores ``parent`` and uses the ``value``\n        property to compute the output. This is done by temporarily setting\n        ``self.out_subfmt`` and calling ``self.value``. This is required for\n        legacy Format subclasses prior to astropy 4.0  New code should instead\n        implement the value functionality in ``to_value()`` and then make the\n        ``value`` property be a simple call to ``self.to_value()``.\n\n        Parameters\n        ----------\n        parent : object\n            Parent `~astropy.time.Time` object associated with this\n            `~astropy.time.TimeFormat` object\n        out_subfmt : str or None\n            Output subformt (use existi"}], "retrieved_count": 10, "cost_time": 1.0821270942687988}
{"question": "What semantic constraints does the unit mapping specification field in the representation-to-frame attribute mapping tuple impose on astronomical coordinate system conversions when set to the null value versus the automatic unit inference string?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "test_frames.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e right units\n    assert i2.dec.unit == i4.dec.unit\n    # unless no/explicitly given units\n    assert i2.dec.unit != fi.dec.unit\n    assert i2.ra.unit != fi.ra.unit\n    assert fi.ra.unit == u.hourangle\n\n\ndef test_representation_info():\n    class NewICRS1(ICRS):\n        frame_specific_representation_info = {\n            r.SphericalRepresentation: [\n                RepresentationMapping(\"lon\", \"rara\", u.hourangle),\n                RepresentationMapping(\"lat\", \"decdec\", u.degree),\n                RepresentationMapping(\"distance\", \"distance\", u.kpc),\n            ]\n        }\n\n    i1 = NewICRS1(\n        rara=10 * u.degree,\n        decdec=-12 * u.deg,\n        distance=1000 * u.pc,\n        pm_rara_cosdecdec=100 * u.mas / u.yr,\n        pm_decdec=17 * u.mas / u.yr,\n        radial_velocity=10 * u.km / u.s,\n    )\n    assert allclose(i1.rara, 10 * u.deg)\n    assert i1.rara.unit == u.hourangle\n    assert allclose(i1.decdec, -12 * u.deg)\n    assert allclose(i1.distance, 1000 * u.pc)\n    assert i1.distance.unit == u.kpc\n    assert allclose(i1.pm_rara_cosdecdec, 100 * u.mas / u.yr)\n    assert allclose(i1.pm_decdec, 17 * u.mas / u.yr)\n\n    # this should auto-set the names of UnitSpherical:\n    i1.set_representation_cls(\n        r.UnitSphericalRepresentation, s=r.UnitSphericalCosLatDifferential\n    )\n    assert allclose(i1.rara, 10 * u.deg)\n    assert allclose(i1.decdec, -12 * u.deg)\n    assert allclose(i1.pm_rara_cosdecdec, 100 * u.mas / u.yr)\n    assert allclose(i1.pm_decdec, 17 * u.mas / u.yr)\n\n    # For backwards compatibility, we also support the string name in the\n    # representation info dictionary:\n    class NewICRS2(ICRS):\n        frame_specific_representation_info = {\n            \"spherical\": [\n                RepresentationMapping(\"lon\", \"ang1\", u.hourangle),\n                RepresentationMapping(\"lat\", \"ang2\", u.degree),\n                RepresentationMapping(\"distance\", \"howfar\", u.kpc),\n            ]\n        }\n\n    i2 = NewICRS2(ang1=10 * u.degree, ang2=-12 * u.deg, howf"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "baseframe.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "of the string aliases {list(r.DIFFERENTIAL_CLASSES)}\"\n            )\n        repr_classes[name] = differential_type\n    return repr_classes\n\n\nclass RepresentationMapping(NamedTuple):\n    \"\"\"\n    This :class:`~typing.NamedTuple` is used with the\n    ``frame_specific_representation_info`` attribute to tell frames what\n    attribute names (and default units) to use for a particular representation.\n    ``reprname`` and ``framename`` should be strings, while ``defaultunit`` can\n    be either an astropy unit, the string ``'recommended'`` (which is degrees\n    for Angles, nothing otherwise), or None (to indicate that no unit mapping\n    should be done).\n    \"\"\"\n\n    reprname: str\n    framename: str\n    defaultunit: str | Unit = \"recommended\"\n\n\nclass CoordinateFrameInfo(MixinInfo):\n    \"\"\"\n    Container for meta information like name, description, format.  This is\n    required when the object is used as a mixin column within a table, but can\n    be used as a general way to store meta information.\n    \"\"\"\n\n    attrs_from_parent = {\"unit\"}  # Unit is read-only\n    _supports_indexing = False\n    mask_val = np.ma.masked\n\n    @staticmethod\n    def default_format(val):\n        repr_data = val.info._repr_data\n        formats = [\"{0.\" + compname + \".value:}\" for compname in repr_data.components]\n        return \",\".join(formats).format(repr_data)\n\n    @property\n    def unit(self):\n        repr_data = self._repr_data\n        return (\n            \",\".join(\n                str(getattr(repr_data, comp).unit) or \"None\"\n                for comp in repr_data.components\n            )\n            if repr_data is not None\n            else None\n        )\n\n    @property\n    def _repr_data(self):\n        coord = self._parent\n        if coord is None or not coord.has_data:\n            return None\n\n        if issubclass(\n            coord.representation_type, r.SphericalRepresentation\n        ) and isinstance(coord.data, r.UnitSphericalRepresentation):\n            repr_data = coord.represent_as(coo"}, {"start_line": 33000, "end_line": 35000, "belongs_to": {"file_name": "baseframe.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "sical type'\n                            f' \"{current_diff_unit.physical_type}\" instead of the'\n                            f' expected \"{(expected_unit).physical_type}\".'\n                        )\n\n            representation_data = representation_data.with_differentials(\n                {\"s\": differential_data}\n            )\n\n        return representation_data\n\n    @classmethod\n    def _infer_repr_info(cls, repr_info):\n        # Unless overridden via `frame_specific_representation_info`, velocity\n        # name defaults are (see also docstring for BaseCoordinateFrame):\n        #   * ``pm_{lon}_cos{lat}``, ``pm_{lat}`` for\n        #     `SphericalCosLatDifferential` proper motion components\n        #   * ``pm_{lon}``, ``pm_{lat}`` for `SphericalDifferential` proper\n        #     motion components\n        #   * ``radial_velocity`` for any `d_distance` component\n        #   * ``v_{x,y,z}`` for `CartesianDifferential` velocity components\n        # where `{lon}` and `{lat}` are the frame names of the angular\n        # components.\n        if repr_info is None:\n            repr_info = {}\n\n        # the tuple() call below is necessary because if it is not there,\n        # the iteration proceeds in a difficult-to-predict manner in the\n        # case that one of the class objects hash is such that it gets\n        # revisited by the iteration.  The tuple() call prevents this by\n        # making the items iterated over fixed regardless of how the dict\n        # changes\n        for cls_or_name in tuple(repr_info.keys()):\n            if isinstance(cls_or_name, str):\n                # TODO: this provides a layer of backwards compatibility in\n                # case the key is a string, but now we want explicit classes.\n                repr_info[_get_repr_cls(cls_or_name)] = repr_info.pop(cls_or_name)\n\n        # The default spherical names are 'lon' and 'lat'\n        sph_repr = repr_info.setdefault(\n            r.SphericalRepresentation,\n            [RepresentationMapping(\"lon\", \"lo"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "test_frames.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "r because of the preferred units defined in the\n    # frame RepresentationMapping\n    assert (\n        repr(i) == \"<ICRS Coordinate: (ra, dec) in deg\\n\"\n        \"    (1., 2.)\\n\"\n        \" (pm_ra_cosdec, pm_dec) in mas / yr\\n\"\n        \"    (1., 2.)>\"\n    )\n\n\ndef test_converting_units():\n    # this is a regular expression that with split (see below) removes what's\n    # the decimal point  to fix rounding problems\n    rexrepr = re.compile(r\"(.*?=\\d\\.).*?( .*?=\\d\\.).*?( .*)\")\n\n    # Use values that aren't subject to rounding down to X.9999...\n    i2 = ICRS(ra=2.0 * u.deg, dec=2.0 * u.deg)\n    i2_many = ICRS(ra=[2.0, 4.0] * u.deg, dec=[2.0, -8.1] * u.deg)\n\n    # converting from FK5 to ICRS and back changes the *internal* representation,\n    # but it should still come out in the preferred form\n\n    i4 = i2.transform_to(FK5()).transform_to(ICRS())\n    i4_many = i2_many.transform_to(FK5()).transform_to(ICRS())\n\n    ri2 = \"\".join(rexrepr.split(repr(i2)))\n    ri4 = \"\".join(rexrepr.split(repr(i4)))\n    assert ri2 == ri4\n    assert i2.data.lon.unit != i4.data.lon.unit  # Internal repr changed\n\n    ri2_many = \"\".join(rexrepr.split(repr(i2_many)))\n    ri4_many = \"\".join(rexrepr.split(repr(i4_many)))\n\n    assert ri2_many == ri4_many\n    assert i2_many.data.lon.unit != i4_many.data.lon.unit  # Internal repr changed\n\n    # but that *shouldn't* hold if we turn off units for the representation\n    class FakeICRS(ICRS):\n        frame_specific_representation_info = {\n            \"spherical\": [\n                RepresentationMapping(\"lon\", \"ra\", u.hourangle),\n                RepresentationMapping(\"lat\", \"dec\", None),\n                RepresentationMapping(\"distance\", \"distance\"),\n            ]  # should fall back to default of None unit\n        }\n\n    fi = FakeICRS(i4.data)\n    ri2 = \"\".join(rexrepr.split(repr(i2)))\n    rfi = \"\".join(rexrepr.split(repr(fi)))\n    rfi = re.sub(\"FakeICRS\", \"ICRS\", rfi)  # Force frame name to match\n    assert ri2 != rfi\n\n    # the attributes should also get th"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "mappings.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    >>> poly = Polynomial1D(1, c0=1, c1=2)\n    >>> model = UnitsMapping(((u.m, None),), input_units_allow_dimensionless=True) | poly\n    >>> model = model | UnitsMapping(((None, u.s),))\n    >>> model(u.Quantity(10, u.m))  # doctest: +FLOAT_CMP\n    <Quantity 21. s>\n    >>> model(10)  # doctest: +FLOAT_CMP\n    <Quantity 21. s>\n    \"\"\"\n\n    def __init__(\n        self,\n        mapping,\n        input_units_equivalencies=None,\n        input_units_allow_dimensionless=False,\n        name=None,\n        meta=None,\n    ):\n        self._mapping = mapping\n\n        none_mapping_count = len([m for m in mapping if m[-1] is None])\n        if none_mapping_count > 0 and none_mapping_count != len(mapping):\n            raise ValueError(\"If one return unit is None, then all must be None\")\n\n        # These attributes are read and handled by Model\n        self._input_units_strict = True\n        self.input_units_equivalencies = input_units_equivalencies\n        self._input_units_allow_dimensionless = input_units_allow_dimensionless\n\n        super().__init__(name=name, meta=meta)\n\n        # Can't invoke this until after super().__init__, since\n        # we need self.inputs and self.outputs to be populated.\n        self._rebuild_units()\n\n    def _rebuild_units(self):\n        self._input_units = {\n            input_name: input_unit\n            for input_name, (input_unit, _) in zip(self.inputs, self.mapping)\n        }\n\n    @property\n    def n_inputs(self):\n        return len(self._mapping)\n\n    @property\n    def n_outputs(self):\n        return len(self._mapping)\n\n    @property\n    def inputs(self):\n        return super().inputs\n\n    @inputs.setter\n    def inputs(self, value):\n        super(UnitsMapping, self.__class__).inputs.fset(self, value)\n        self._rebuild_units()\n\n    @property\n    def outputs(self):\n        return super().outputs\n\n    @outputs.setter\n    def outputs(self, value):\n        super(UnitsMapping, self.__class__).outputs.fset(self, value)\n        self._rebuild_units()\n\n   "}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "mappings.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "endly name associated with this model instance\n        (particularly useful for identifying the individual components of a\n        compound model).\n    meta : dict-like, optional\n        Free-form metadata to associate with this model.\n\n    Examples\n    --------\n    Wrapping a unitless model to require and convert units:\n\n    >>> from astropy.modeling.models import Polynomial1D, UnitsMapping\n    >>> from astropy import units as u\n    >>> poly = Polynomial1D(1, c0=1, c1=2)\n    >>> model = UnitsMapping(((u.m, None),)) | poly\n    >>> model = model | UnitsMapping(((None, u.s),))\n    >>> model(u.Quantity(10, u.m))  # doctest: +FLOAT_CMP\n    <Quantity 21. s>\n    >>> model(u.Quantity(1000, u.cm)) # doctest: +FLOAT_CMP\n    <Quantity 21. s>\n    >>> model(u.Quantity(10, u.cm)) # doctest: +FLOAT_CMP\n    <Quantity 1.2 s>\n\n    Wrapping a unitless model but still permitting unitless input:\n\n    >>> from astropy.modeling.models import Polynomial1D, UnitsMapping\n    >>> from astropy import units as u\n    >>> poly = Polynomial1D(1, c0=1, c1=2)\n    >>> model = UnitsMapping(((u.m, None),), input_units_allow_dimensionless=True) | poly\n    >>> model = model | UnitsMapping(((None, u.s),))\n    >>> model(u.Quantity(10, u.m))  # doctest: +FLOAT_CMP\n    <Quantity 21. s>\n    >>> model(10)  # doctest: +FLOAT_CMP\n    <Quantity 21. s>\n    \"\"\"\n\n    def __init__(\n        self,\n        mapping,\n        input_units_equivalencies=None,\n        input_units_allow_dimensionless=False,\n        name=None,\n        meta=None,\n    ):\n        self._mapping = mapping\n\n        none_mapping_count = len([m for m in mapping if m[-1] is None])\n        if none_mapping_count > 0 and none_mapping_count != len(mapping):\n            raise ValueError(\"If one return unit is None, then all must be None\")\n\n        # These attributes are read and handled by Model\n        self._input_units_strict = True\n        self.input_units_equivalencies = input_units_equivalencies\n        self._input_units_allow_dimensionless = input_uni"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "test_frames.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "))\n    assert ri2 == ri4\n    assert i2.data.lon.unit != i4.data.lon.unit  # Internal repr changed\n\n    ri2_many = \"\".join(rexrepr.split(repr(i2_many)))\n    ri4_many = \"\".join(rexrepr.split(repr(i4_many)))\n\n    assert ri2_many == ri4_many\n    assert i2_many.data.lon.unit != i4_many.data.lon.unit  # Internal repr changed\n\n    # but that *shouldn't* hold if we turn off units for the representation\n    class FakeICRS(ICRS):\n        frame_specific_representation_info = {\n            \"spherical\": [\n                RepresentationMapping(\"lon\", \"ra\", u.hourangle),\n                RepresentationMapping(\"lat\", \"dec\", None),\n                RepresentationMapping(\"distance\", \"distance\"),\n            ]  # should fall back to default of None unit\n        }\n\n    fi = FakeICRS(i4.data)\n    ri2 = \"\".join(rexrepr.split(repr(i2)))\n    rfi = \"\".join(rexrepr.split(repr(fi)))\n    rfi = re.sub(\"FakeICRS\", \"ICRS\", rfi)  # Force frame name to match\n    assert ri2 != rfi\n\n    # the attributes should also get the right units\n    assert i2.dec.unit == i4.dec.unit\n    # unless no/explicitly given units\n    assert i2.dec.unit != fi.dec.unit\n    assert i2.ra.unit != fi.ra.unit\n    assert fi.ra.unit == u.hourangle\n\n\ndef test_representation_info():\n    class NewICRS1(ICRS):\n        frame_specific_representation_info = {\n            r.SphericalRepresentation: [\n                RepresentationMapping(\"lon\", \"rara\", u.hourangle),\n                RepresentationMapping(\"lat\", \"decdec\", u.degree),\n                RepresentationMapping(\"distance\", \"distance\", u.kpc),\n            ]\n        }\n\n    i1 = NewICRS1(\n        rara=10 * u.degree,\n        decdec=-12 * u.deg,\n        distance=1000 * u.pc,\n        pm_rara_cosdecdec=100 * u.mas / u.yr,\n        pm_decdec=17 * u.mas / u.yr,\n        radial_velocity=10 * u.km / u.s,\n    )\n    assert allclose(i1.rara, 10 * u.deg)\n    assert i1.rara.unit == u.hourangle\n    assert allclose(i1.decdec, -12 * u.deg)\n    assert allclose(i1.distance, 1000 * u.pc)\n    assert i1.dis"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "test_frames.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tance.unit == u.kpc\n    assert allclose(i1.pm_rara_cosdecdec, 100 * u.mas / u.yr)\n    assert allclose(i1.pm_decdec, 17 * u.mas / u.yr)\n\n    # this should auto-set the names of UnitSpherical:\n    i1.set_representation_cls(\n        r.UnitSphericalRepresentation, s=r.UnitSphericalCosLatDifferential\n    )\n    assert allclose(i1.rara, 10 * u.deg)\n    assert allclose(i1.decdec, -12 * u.deg)\n    assert allclose(i1.pm_rara_cosdecdec, 100 * u.mas / u.yr)\n    assert allclose(i1.pm_decdec, 17 * u.mas / u.yr)\n\n    # For backwards compatibility, we also support the string name in the\n    # representation info dictionary:\n    class NewICRS2(ICRS):\n        frame_specific_representation_info = {\n            \"spherical\": [\n                RepresentationMapping(\"lon\", \"ang1\", u.hourangle),\n                RepresentationMapping(\"lat\", \"ang2\", u.degree),\n                RepresentationMapping(\"distance\", \"howfar\", u.kpc),\n            ]\n        }\n\n    i2 = NewICRS2(ang1=10 * u.degree, ang2=-12 * u.deg, howfar=1000 * u.pc)\n    assert allclose(i2.ang1, 10 * u.deg)\n    assert i2.ang1.unit == u.hourangle\n    assert allclose(i2.ang2, -12 * u.deg)\n    assert allclose(i2.howfar, 1000 * u.pc)\n    assert i2.howfar.unit == u.kpc\n\n    # Test that the differential kwargs get overridden\n    class NewICRS3(ICRS):\n        frame_specific_representation_info = {\n            r.SphericalCosLatDifferential: [\n                RepresentationMapping(\"d_lon_coslat\", \"pm_ang1\", u.hourangle / u.year),\n                RepresentationMapping(\"d_lat\", \"pm_ang2\"),\n                RepresentationMapping(\"d_distance\", \"vlos\", u.kpc / u.Myr),\n            ]\n        }\n\n    i3 = NewICRS3(\n        lon=10 * u.degree,\n        lat=-12 * u.deg,\n        distance=1000 * u.pc,\n        pm_ang1=1 * u.mas / u.yr,\n        pm_ang2=2 * u.mas / u.yr,\n        vlos=100 * u.km / u.s,\n    )\n    assert allclose(i3.pm_ang1, 1 * u.mas / u.yr)\n    assert i3.pm_ang1.unit == u.hourangle / u.year\n    assert allclose(i3.pm_ang2, 2 * u.mas / u.yr)\n    "}, {"start_line": 9000, "end_line": 10841, "belongs_to": {"file_name": "mappings.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ts_allow_dimensionless\n\n        super().__init__(name=name, meta=meta)\n\n        # Can't invoke this until after super().__init__, since\n        # we need self.inputs and self.outputs to be populated.\n        self._rebuild_units()\n\n    def _rebuild_units(self):\n        self._input_units = {\n            input_name: input_unit\n            for input_name, (input_unit, _) in zip(self.inputs, self.mapping)\n        }\n\n    @property\n    def n_inputs(self):\n        return len(self._mapping)\n\n    @property\n    def n_outputs(self):\n        return len(self._mapping)\n\n    @property\n    def inputs(self):\n        return super().inputs\n\n    @inputs.setter\n    def inputs(self, value):\n        super(UnitsMapping, self.__class__).inputs.fset(self, value)\n        self._rebuild_units()\n\n    @property\n    def outputs(self):\n        return super().outputs\n\n    @outputs.setter\n    def outputs(self, value):\n        super(UnitsMapping, self.__class__).outputs.fset(self, value)\n        self._rebuild_units()\n\n    @property\n    def input_units(self):\n        return self._input_units\n\n    @property\n    def mapping(self):\n        return self._mapping\n\n    def evaluate(self, *args):\n        result = []\n        for arg, (_, return_unit) in zip(args, self.mapping):\n            if isinstance(arg, Quantity):\n                value = arg.value\n            else:\n                value = arg\n            if return_unit is None:\n                result.append(value)\n            else:\n                result.append(Quantity(value, return_unit, subok=True))\n\n        if self.n_outputs == 1:\n            return result[0]\n        else:\n            return tuple(result)\n\n    def __repr__(self):\n        if self.name is None:\n            return f\"<UnitsMapping({self.mapping})>\"\n        else:\n            return f\"<UnitsMapping({self.mapping}, name={self.name!r})>\"\n"}, {"start_line": 58000, "end_line": 60000, "belongs_to": {"file_name": "baseframe.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      for comp, new_attr_unit in zip(diff.components, new_attrs[\"units\"]):\n                    # Some special-casing to treat a situation where the\n                    # input data has a UnitSphericalDifferential or a\n                    # RadialDifferential. It is re-represented to the\n                    # frame's differential class (which might be, e.g., a\n                    # dimensional Differential), so we don't want to try to\n                    # convert the empty component units\n                    if (\n                        isinstance(\n                            data_diff,\n                            (\n                                r.UnitSphericalDifferential,\n                                r.UnitSphericalCosLatDifferential,\n                                r.RadialDifferential,\n                            ),\n                        )\n                        and comp not in data_diff.__class__.attr_classes\n                    ):\n                        continue\n\n                    # Try to convert to requested units. Since that might\n                    # not be possible (e.g., for a coordinate with proper\n                    # motion but without distance, one cannot convert to a\n                    # cartesian differential in km/s), we allow the unit\n                    # conversion to fail.  See gh-7028 for discussion.\n                    if new_attr_unit and hasattr(diff, comp):\n                        try:\n                            diffkwargs[comp] = diffkwargs[comp].to(new_attr_unit)\n                        except Exception:\n                            pass\n\n                diff = diff.__class__(copy=False, **diffkwargs)\n\n                # Here we have to bypass using with_differentials() because\n                # it has a validation check. But because\n                # .representation_type and .differential_type don't point to\n                # the original classes, if the input differential is a\n                # RadialDifferential, it usua"}], "retrieved_count": 10, "cost_time": 1.0982770919799805}
{"question": "Why does the test class that verifies mask assignment behavior in the masked array utilities module check memory sharing between the mask property and the input mask array after assignment instead of only verifying value equality?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 15000, "end_line": 17000, "belongs_to": {"file_name": "test_masked.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tem] = True\n        assert_array_equal(ma.mask, expected)\n        ma.mask[item] = False\n        assert_array_equal(ma.mask, np.zeros(ma.shape, bool))\n        # Mask propagation\n        mask = np.zeros(self.a.shape, bool)\n        ma = Masked(self.a, mask)\n        ma.mask[item] = True\n        assert np.may_share_memory(ma.mask, mask)\n        assert_array_equal(ma.mask, mask)\n\n    @pytest.mark.parametrize(\"item\", [\"a\"] + VARIOUS_ITEMS)\n    def test_part_mask_setting_structured(self, item):\n        ma = Masked(self.sa)\n        ma.mask[item] = True\n        expected = np.zeros(ma.shape, self.mask_sdt)\n        expected[item] = True\n        assert_array_equal(ma.mask, expected)\n        ma.mask[item] = False\n        assert_array_equal(ma.mask, np.zeros(ma.shape, self.mask_sdt))\n        # Mask propagation\n        mask = np.zeros(self.sa.shape, self.mask_sdt)\n        ma = Masked(self.sa, mask)\n        ma.mask[item] = True\n        assert np.may_share_memory(ma.mask, mask)\n        assert_array_equal(ma.mask, mask)\n\n\n# Following are tests where we trust the initializer works.\n\n\nclass MaskedArraySetup(ArraySetup):\n    @classmethod\n    def setup_class(cls):\n        super().setup_class()\n        cls.ma = Masked(cls.a, mask=cls.mask_a)\n        cls.mb = Masked(cls.b, mask=cls.mask_b)\n        cls.mc = Masked(cls.c, mask=cls.mask_c)\n        cls.msa = Masked(cls.sa, mask=cls.mask_sa)\n        cls.msb = Masked(cls.sb, mask=cls.mask_sb)\n        cls.msc = Masked(cls.sc, mask=cls.mask_sc)\n\n\nclass TestViewing(MaskedArraySetup):\n    def test_viewing_as_new_type(self):\n        ma2 = self.ma.view(type(self.ma))\n        assert_masked_equal(ma2, self.ma)\n\n        ma3 = self.ma.view()\n        assert_masked_equal(ma3, self.ma)\n\n    def test_viewing_as_new_dtype(self):\n        # Not very meaningful, but possible...\n        ma2 = self.ma.view(\"c8\")\n        assert_array_equal(ma2.unmasked, self.a.view(\"c8\"))\n        assert_array_equal(ma2.mask, self.mask_a)\n\n    def test_viewing_as_new_structured_dtype("}, {"start_line": 50000, "end_line": 52000, "belongs_to": {"file_name": "test_function_helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     assert_array_equal(o.unmasked, x)\n            assert_array_equal(o.mask, m)\n\n\nclass TestMemoryFunctions(MaskedArraySetup):\n    def test_shares_memory(self):\n        assert np.shares_memory(self.ma, self.ma.unmasked)\n        assert not np.shares_memory(self.ma, self.ma.mask)\n\n    def test_may_share_memory(self):\n        assert np.may_share_memory(self.ma, self.ma.unmasked)\n        assert not np.may_share_memory(self.ma, self.ma.mask)\n\n\nclass TestDatetimeFunctions:\n    # Could in principle support np.is_busday, np.busday_count, np.busday_offset.\n    @classmethod\n    def setup_class(cls):\n        cls.a = np.array([\"2020-12-31\", \"2021-01-01\", \"2021-01-02\"], dtype=\"M\")\n        cls.mask_a = np.array([False, True, False])\n        cls.ma = Masked(cls.a, mask=cls.mask_a)\n        cls.b = np.array([[\"2021-01-07\"], [\"2021-01-31\"]], dtype=\"M\")\n        cls.mask_b = np.array([[False], [True]])\n        cls.mb = Masked(cls.b, mask=cls.mask_b)\n\n    def test_datetime_as_string(self):\n        out = np.datetime_as_string(self.ma)\n        expected = np.datetime_as_string(self.a)\n        assert_array_equal(out.unmasked, expected)\n        assert_array_equal(out.mask, self.mask_a)\n\n\n@pytest.mark.filterwarnings(\"ignore:all-nan\")\nclass TestNaNFunctions:\n    def setup_class(self):\n        self.a = np.array(\n            [\n                [np.nan, np.nan, 3.0],\n                [4.0, 5.0, 6.0],\n            ]\n        )\n        self.mask_a = np.array(\n            [\n                [True, False, False],\n                [False, True, False],\n            ]\n        )\n        self.b = np.arange(1, 7).reshape(2, 3)\n        self.mask_b = self.mask_a\n        self.ma = Masked(self.a, mask=self.mask_a)\n        self.mb = Masked(self.b, mask=self.mask_b)\n\n    def check(self, function, exact_fill_value=None, masked_result=True, **kwargs):\n        result = function(self.ma, **kwargs)\n        expected_data = function(self.ma.filled(np.nan), **kwargs)\n        expected_mask = np.isnan(expected_data)\n        if"}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "test_masked.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e_memory(ma.mask, self.mask_a)\n\n    def test_whole_mask_setting_structured(self):\n        ma = Masked(self.sa)\n        assert ma.mask.shape == ma.shape\n        assert not ma.mask[\"a\"].any() and not ma.mask[\"b\"].any()\n        ma.mask = True\n        assert ma.mask.shape == ma.shape\n        assert ma.mask[\"a\"].all() and ma.mask[\"b\"].all()\n        ma.mask = [[True], [False]]\n        assert ma.mask.shape == ma.shape\n        assert_array_equal(\n            ma.mask,\n            np.array([[(True, True)] * 2, [(False, False)] * 2], dtype=self.mask_sdt),\n        )\n        ma.mask = self.mask_sa\n        assert ma.mask.shape == ma.shape\n        assert_array_equal(ma.mask, self.mask_sa)\n        assert ma.mask is not self.mask_sa\n        assert np.may_share_memory(ma.mask, self.mask_sa)\n\n    @pytest.mark.parametrize(\"item\", VARIOUS_ITEMS)\n    def test_part_mask_setting(self, item):\n        ma = Masked(self.a)\n        ma.mask[item] = True\n        expected = np.zeros(ma.shape, bool)\n        expected[item] = True\n        assert_array_equal(ma.mask, expected)\n        ma.mask[item] = False\n        assert_array_equal(ma.mask, np.zeros(ma.shape, bool))\n        # Mask propagation\n        mask = np.zeros(self.a.shape, bool)\n        ma = Masked(self.a, mask)\n        ma.mask[item] = True\n        assert np.may_share_memory(ma.mask, mask)\n        assert_array_equal(ma.mask, mask)\n\n    @pytest.mark.parametrize(\"item\", [\"a\"] + VARIOUS_ITEMS)\n    def test_part_mask_setting_structured(self, item):\n        ma = Masked(self.sa)\n        ma.mask[item] = True\n        expected = np.zeros(ma.shape, self.mask_sdt)\n        expected[item] = True\n        assert_array_equal(ma.mask, expected)\n        ma.mask[item] = False\n        assert_array_equal(ma.mask, np.zeros(ma.shape, self.mask_sdt))\n        # Mask propagation\n        mask = np.zeros(self.sa.shape, self.mask_sdt)\n        ma = Masked(self.sa, mask)\n        ma.mask[item] = True\n        assert np.may_share_memory(ma.mask, mask)\n        assert_array_equa"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "test_masked.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ray_equal(mq.mask, [True, False, False])\n\n    def test_initialization_with_list_of_masked_quantity_arrays(self):\n        ma = Masked(self.a, self.mask_a)\n        mq = self.MQ([ma, ma << u.km])\n        assert isinstance(mq, self.MQ)\n        assert_array_equal(mq.unmasked, u.Quantity([self.a, self.a << u.km]))\n        assert_array_equal(mq.mask, np.array([self.mask_a, self.mask_a]))\n\n\nclass TestMaskSetting(ArraySetup):\n    def test_whole_mask_setting_simple(self):\n        ma = Masked(self.a)\n        assert ma.mask.shape == ma.shape\n        assert not ma.mask.any()\n        ma.mask = True\n        assert ma.mask.shape == ma.shape\n        assert ma.mask.all()\n        ma.mask = [[True], [False]]\n        assert ma.mask.shape == ma.shape\n        assert_array_equal(ma.mask, np.array([[True] * 3, [False] * 3]))\n        ma.mask = self.mask_a\n        assert ma.mask.shape == ma.shape\n        assert_array_equal(ma.mask, self.mask_a)\n        assert ma.mask is not self.mask_a\n        assert np.may_share_memory(ma.mask, self.mask_a)\n\n    def test_whole_mask_setting_structured(self):\n        ma = Masked(self.sa)\n        assert ma.mask.shape == ma.shape\n        assert not ma.mask[\"a\"].any() and not ma.mask[\"b\"].any()\n        ma.mask = True\n        assert ma.mask.shape == ma.shape\n        assert ma.mask[\"a\"].all() and ma.mask[\"b\"].all()\n        ma.mask = [[True], [False]]\n        assert ma.mask.shape == ma.shape\n        assert_array_equal(\n            ma.mask,\n            np.array([[(True, True)] * 2, [(False, False)] * 2], dtype=self.mask_sdt),\n        )\n        ma.mask = self.mask_sa\n        assert ma.mask.shape == ma.shape\n        assert_array_equal(ma.mask, self.mask_sa)\n        assert ma.mask is not self.mask_sa\n        assert np.may_share_memory(ma.mask, self.mask_sa)\n\n    @pytest.mark.parametrize(\"item\", VARIOUS_ITEMS)\n    def test_part_mask_setting(self, item):\n        ma = Masked(self.a)\n        ma.mask[item] = True\n        expected = np.zeros(ma.shape, bool)\n        expected[i"}, {"start_line": 18000, "end_line": 20000, "belongs_to": {"file_name": "test_masked.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ".array([1, 2], dtype=np.intp)).view(Masked)\n\n\nclass TestMaskedArrayCopyFilled(MaskedArraySetup):\n    def test_copy(self):\n        ma_copy = self.ma.copy()\n        assert type(ma_copy) is type(self.ma)\n        assert_array_equal(ma_copy.unmasked, self.ma.unmasked)\n        assert_array_equal(ma_copy.mask, self.ma.mask)\n        assert not np.may_share_memory(ma_copy.unmasked, self.ma.unmasked)\n        assert not np.may_share_memory(ma_copy.mask, self.ma.mask)\n\n    @pytest.mark.parametrize(\"fill_value\", (0, 1))\n    def test_filled(self, fill_value):\n        fill_value = fill_value * getattr(self.a, \"unit\", 1)\n        expected = self.a.copy()\n        expected[self.ma.mask] = fill_value\n        result = self.ma.filled(fill_value)\n        assert_array_equal(expected, result)\n\n    def test_filled_no_fill_value(self):\n        with pytest.raises(TypeError, match=\"missing 1 required\"):\n            self.ma.filled()\n\n    @pytest.mark.parametrize(\"fill_value\", [(0, 1), (-1, -1)])\n    def test_filled_structured(self, fill_value):\n        fill_value = np.array(fill_value, dtype=self.sdt)\n        if hasattr(self.sa, \"unit\"):\n            fill_value = fill_value << self.sa.unit\n        expected = self.sa.copy()\n        expected[\"a\"][self.msa.mask[\"a\"]] = fill_value[\"a\"]\n        expected[\"b\"][self.msa.mask[\"b\"]] = fill_value[\"b\"]\n        result = self.msa.filled(fill_value)\n        assert_array_equal(expected, result)\n\n    def test_flat(self):\n        ma_copy = self.ma.copy()\n        ma_flat = ma_copy.flat\n        # Check that single item keeps class and mask\n        ma_flat1 = ma_flat[1]\n        assert ma_flat1.unmasked == self.a.flat[1]\n        assert ma_flat1.mask == self.mask_a.flat[1]\n        # As well as getting items via iteration.\n        assert all(\n            (ma.unmasked == a and ma.mask == m)\n            for (ma, a, m) in zip(self.ma.flat, self.a.flat, self.mask_a.flat)\n        )\n\n        # check that flat works like a view of the real array\n        ma_flat[1] = self.b[1]\n"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_masked.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ls.c, u.deg)\n        # Note: Longitude does not work on structured arrays, so\n        # leaving it as regular array (which just reruns some tests).\n\n\nclass TestMaskedArrayInitialization(ArraySetup):\n    def test_simple(self):\n        ma = Masked(self.a, mask=self.mask_a)\n        assert isinstance(ma, np.ndarray)\n        assert isinstance(ma, type(self.a))\n        assert isinstance(ma, Masked)\n        assert_array_equal(ma.unmasked, self.a)\n        assert_array_equal(ma.mask, self.mask_a)\n        assert ma.mask is not self.mask_a\n        assert np.may_share_memory(ma.mask, self.mask_a)\n\n    def test_structured(self):\n        ma = Masked(self.sa, mask=self.mask_sa)\n        assert isinstance(ma, np.ndarray)\n        assert isinstance(ma, type(self.sa))\n        assert isinstance(ma, Masked)\n        assert_array_equal(ma.unmasked, self.sa)\n        assert_array_equal(ma.mask, self.mask_sa)\n        assert ma.mask is not self.mask_sa\n        assert np.may_share_memory(ma.mask, self.mask_sa)\n\n    def test_masked_input(self):\n        ma = Masked(self.a, mask=self.mask_a)\n        mab = Masked(ma, mask=self.mask_b)\n        assert isinstance(mab, np.ndarray)\n        assert isinstance(mab, type(self.sa))\n        assert isinstance(mab, Masked)\n        assert_array_equal(mab.unmasked, self.a)\n        assert_array_equal(mab.mask, self.mask_a | self.mask_b)\n\n\ndef test_masked_ndarray_init():\n    # Note: as a straight ndarray subclass, MaskedNDArray passes on\n    # the arguments relevant for np.ndarray, not np.array.\n    a_in = np.arange(3, dtype=int)\n    m_in = np.array([True, False, False])\n    buff = a_in.tobytes()\n    # Check we're doing things correctly using regular ndarray.\n    a = np.ndarray(shape=(3,), dtype=int, buffer=buff)\n    assert_array_equal(a, a_in)\n    # Check with and without mask.\n    ma = MaskedNDArray((3,), dtype=int, mask=m_in, buffer=buff)\n    assert_array_equal(ma.unmasked, a_in)\n    assert_array_equal(ma.mask, m_in)\n    ma = MaskedNDArray((3,), dtype=int, buffe"}, {"start_line": 17000, "end_line": 19000, "belongs_to": {"file_name": "test_function_helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_array_equal(ma.unmasked, expected)\n        assert_array_equal(ma.mask, expected_mask)\n        np.copyto(ma, np.ma.masked, where=~mask)\n        np.copyto(expected_mask, True, where=~mask)\n        assert_array_equal(ma.unmasked, expected)\n        assert_array_equal(ma.mask, expected_mask)\n        np.copyto(ma, np.ma.nomask, where=mask)\n        np.copyto(expected_mask, False, where=mask)\n        assert_array_equal(ma.unmasked, expected)\n        assert_array_equal(ma.mask, expected_mask)\n\n        with pytest.raises(TypeError):\n            np.copyto(self.a.flatten(), values, where=mask)\n\n    @pytest.mark.parametrize(\"value\", [0.25, np.ma.masked])\n    def test_fill_diagonal(self, value):\n        ma = self.ma[:2, :2].copy()\n        np.fill_diagonal(ma, value)\n        expected = ma.copy()\n        expected[np.diag_indices_from(expected)] = value\n        assert_array_equal(ma.unmasked, expected.unmasked)\n        assert_array_equal(ma.mask, expected.mask)\n\n\nclass TestRepeat(BasicTestSetup):\n    def test_tile(self):\n        self.check(np.tile, 2)\n\n    def test_repeat(self):\n        self.check(np.repeat, 2)\n\n    def test_resize(self):\n        self.check(np.resize, (4, 4))\n\n\nclass TestConcatenate(MaskedArraySetup):\n    # More tests at TestMaskedArrayConcatenation in test_functions.\n    def check(self, func, *args, **kwargs):\n        ma_list = kwargs.pop(\"ma_list\", [self.ma, self.ma])\n        a_list = [Masked(ma).unmasked for ma in ma_list]\n        m_list = [Masked(ma).mask for ma in ma_list]\n        o = func(ma_list, *args, **kwargs)\n        expected = func(a_list, *args, **kwargs)\n        expected_mask = func(m_list, *args, **kwargs)\n        assert_array_equal(o.unmasked, expected)\n        assert_array_equal(o.mask, expected_mask)\n\n    def test_concatenate(self):\n        self.check(np.concatenate)\n        self.check(np.concatenate, axis=1)\n        self.check(np.concatenate, ma_list=[self.a, self.ma])\n        self.check(np.concatenate, dtype=\"f4\")\n\n        out = Masked(np.empty("}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "test_masked.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "lf.m))\n        assert isinstance(mms, Masked)\n        assert isinstance(mms, self.MyArray)\n        assert_array_equal(mms.unmasked, self.a)\n        assert_array_equal(mms.mask, self.m)\n\n    def test_viewing(self):\n        mms = Masked(self.a, mask=self.m)\n        mms2 = mms.view()\n        assert type(mms2) is mms.__class__\n        assert_masked_equal(mms2, mms)\n\n        ma = mms.view(np.ndarray)\n        assert type(ma) is MaskedNDArray\n        assert_array_equal(ma.unmasked, self.a.view(np.ndarray))\n        assert_array_equal(ma.mask, self.m)\n\n    def test_viewing_independent_shape(self):\n        mms = Masked(self.a, mask=self.m)\n        mms2 = mms.view()\n        mms2.shape = mms2.shape[::-1]\n        assert mms2.shape == mms.shape[::-1]\n        assert mms2.mask.shape == mms.shape[::-1]\n        # This should not affect the original array!\n        assert mms.shape == self.a.shape\n        assert mms.mask.shape == self.a.shape\n\n\nclass TestMaskedQuantityInitialization(TestMaskedArrayInitialization, QuantitySetup):\n    @classmethod\n    def setup_class(cls):\n        super().setup_class()\n        # Ensure we have used MaskedQuantity before - just in case a single test gets\n        # called; see gh-15316.\n        cls.MQ = Masked(Quantity)\n\n    def test_masked_quantity_getting(self):\n        # First check setup_class (or previous use) defined a cache entry.\n        mcls = Masked._masked_classes[type(self.a)]\n        # Next check this is what one gets now.\n        MQ = Masked(Quantity)\n        assert MQ is mcls\n        assert issubclass(MQ, Quantity)\n        assert issubclass(MQ, Masked)\n        # And also what one gets implicitly.\n        mq = Masked([1.0, 2.0] * u.m)\n        assert isinstance(mq, MQ)\n\n    def test_masked_quantity_class_init(self):\n        # This is not a very careful test.\n        mq = self.MQ([1.0, 2.0], mask=[True, False], unit=u.s)\n        assert mq.unit == u.s\n        assert np.all(mq.value.unmasked == [1.0, 2.0])\n        assert np.all(mq.value.mask == ["}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_masked.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      ([-1.0, -2.0], [[-1, -2], [-3, -4]]),\n            ],\n            dtype=cls.scdt,\n        )\n        cls.mask_scdt = np.dtype([(\"sa\", \"2?\"), (\"sb\", \"?\", (2, 2))])\n        cls.mask_sc = np.array(\n            [\n                ([True, False], [[False, False], [True, True]]),\n                ([False, True], [[True, False], [False, True]]),\n            ],\n            dtype=cls.mask_scdt,\n        )\n\n\nclass QuantitySetup(ArraySetup):\n    _data_cls = Quantity\n\n    @classmethod\n    def setup_class(cls):\n        super().setup_class()\n        cls.a = Quantity(cls.a, u.m)\n        cls.b = Quantity(cls.b, u.cm)\n        cls.c = Quantity(cls.c, u.km)\n        cls.sa = Quantity(cls.sa, u.m, dtype=cls.sdt)\n        cls.sb = Quantity(cls.sb, u.cm, dtype=cls.sdt)\n\n\nclass LongitudeSetup(ArraySetup):\n    _data_cls = Longitude\n\n    @classmethod\n    def setup_class(cls):\n        super().setup_class()\n        cls.a = Longitude(cls.a, u.deg)\n        cls.b = Longitude(cls.b, u.deg)\n        cls.c = Longitude(cls.c, u.deg)\n        # Note: Longitude does not work on structured arrays, so\n        # leaving it as regular array (which just reruns some tests).\n\n\nclass TestMaskedArrayInitialization(ArraySetup):\n    def test_simple(self):\n        ma = Masked(self.a, mask=self.mask_a)\n        assert isinstance(ma, np.ndarray)\n        assert isinstance(ma, type(self.a))\n        assert isinstance(ma, Masked)\n        assert_array_equal(ma.unmasked, self.a)\n        assert_array_equal(ma.mask, self.mask_a)\n        assert ma.mask is not self.mask_a\n        assert np.may_share_memory(ma.mask, self.mask_a)\n\n    def test_structured(self):\n        ma = Masked(self.sa, mask=self.mask_sa)\n        assert isinstance(ma, np.ndarray)\n        assert isinstance(ma, type(self.sa))\n        assert isinstance(ma, Masked)\n        assert_array_equal(ma.unmasked, self.sa)\n        assert_array_equal(ma.mask, self.mask_sa)\n        assert ma.mask is not self.mask_sa\n        assert np.may_share_memory(ma.mask, self.mask_sa)\n\n  "}, {"start_line": 49000, "end_line": 51000, "belongs_to": {"file_name": "test_function_helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "self):\n        self.check(np.result_type)\n\n    def test_can_cast(self):\n        self.check(np.can_cast, self.a.dtype)\n        self.check(np.can_cast, \"f4\")\n\n    def test_min_scalar_type(self):\n        out = np.min_scalar_type(self.ma[0, 0])\n        expected = np.min_scalar_type(self.a[0, 0])\n        assert out == expected\n\n    def test_iscomplexobj(self):\n        self.check(np.iscomplexobj)\n\n    def test_isrealobj(self):\n        self.check(np.isrealobj)\n\n\nclass TestMeshGrid(MaskedArraySetup):\n    def test_meshgrid(self):\n        a = np.arange(1.0, 4.0)\n        mask_a = np.array([True, False, False])\n        ma = Masked(a, mask=mask_a)\n        b = np.array([2.5, 10.0, 3.0, 4.0])\n        mask_b = np.array([False, True, False, True])\n        mb = Masked(b, mask=mask_b)\n        oa, ob = np.meshgrid(ma, mb)\n        xa, xb = np.broadcast_arrays(a, b[:, np.newaxis])\n        ma, mb = np.broadcast_arrays(mask_a, mask_b[:, np.newaxis])\n        for o, x, m in ((oa, xa, ma), (ob, xb, mb)):\n            assert_array_equal(o.unmasked, x)\n            assert_array_equal(o.mask, m)\n\n\nclass TestMemoryFunctions(MaskedArraySetup):\n    def test_shares_memory(self):\n        assert np.shares_memory(self.ma, self.ma.unmasked)\n        assert not np.shares_memory(self.ma, self.ma.mask)\n\n    def test_may_share_memory(self):\n        assert np.may_share_memory(self.ma, self.ma.unmasked)\n        assert not np.may_share_memory(self.ma, self.ma.mask)\n\n\nclass TestDatetimeFunctions:\n    # Could in principle support np.is_busday, np.busday_count, np.busday_offset.\n    @classmethod\n    def setup_class(cls):\n        cls.a = np.array([\"2020-12-31\", \"2021-01-01\", \"2021-01-02\"], dtype=\"M\")\n        cls.mask_a = np.array([False, True, False])\n        cls.ma = Masked(cls.a, mask=cls.mask_a)\n        cls.b = np.array([[\"2021-01-07\"], [\"2021-01-31\"]], dtype=\"M\")\n        cls.mask_b = np.array([[False], [True]])\n        cls.mb = Masked(cls.b, mask=cls.mask_b)\n\n    def test_datetime_as_string(self):\n        out = n"}], "retrieved_count": 10, "cost_time": 1.0713353157043457}
{"question": "Why is the test function in the console utilities test module that verifies terminal color output without assertions implemented as a smoke test rather than a comprehensive unit test?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_console.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rn True\n\n\ndef test_fake_tty():\n    # First test without a specified encoding; we should be able to write\n    # arbitrary unicode strings\n    f1 = FakeTTY()\n    assert f1.isatty()\n    f1.write(\"\")\n    assert f1.getvalue() == \"\"\n\n    # Now test an ASCII-only TTY--it should raise a UnicodeEncodeError when\n    # trying to write a string containing non-ASCII characters\n    f2 = FakeTTY(\"ascii\")\n    assert f2.isatty()\n    assert f2.__class__.__name__ == \"AsciiFakeTTY\"\n    assert pytest.raises(UnicodeEncodeError, f2.write, \"\")\n    assert f2.getvalue() == \"\"\n\n\n@pytest.mark.skipif(sys.platform.startswith(\"win\"), reason=\"Cannot test on Windows\")\ndef test_color_text():\n    assert console._color_text(\"foo\", \"green\") == \"\\033[0;32mfoo\\033[0m\"\n\n\ndef test_color_print():\n    # This stuff is hard to test, at least smoke test it\n    console.color_print(\"foo\", \"green\")\n\n    console.color_print(\"foo\", \"green\", \"bar\", \"red\")\n\n\ndef test_color_print2():\n    # Test that this automatically detects that io.StringIO is\n    # not a tty\n    stream = io.StringIO()\n    console.color_print(\"foo\", \"green\", file=stream)\n    assert stream.getvalue() == \"foo\\n\"\n\n    stream = io.StringIO()\n    console.color_print(\"foo\", \"green\", \"bar\", \"red\", \"baz\", file=stream)\n    assert stream.getvalue() == \"foobarbaz\\n\"\n\n\n@pytest.mark.skipif(sys.platform.startswith(\"win\"), reason=\"Cannot test on Windows\")\ndef test_color_print3():\n    # Test that this thinks the FakeTTY is a tty and applies colors.\n\n    stream = FakeTTY()\n    console.color_print(\"foo\", \"green\", file=stream)\n    assert stream.getvalue() == \"\\x1b[0;32mfoo\\x1b[0m\\n\"\n\n    stream = FakeTTY()\n    console.color_print(\"foo\", \"green\", \"bar\", \"red\", \"baz\", file=stream)\n    assert stream.getvalue() == \"\\x1b[0;32mfoo\\x1b[0m\\x1b[0;31mbar\\x1b[0mbaz\\n\"\n\n\ndef test_color_print_unicode():\n    console.color_print(\"berbr\", \"red\")\n\n\ndef test_color_print_invalid_color():\n    console.color_print(\"foo\", \"unknown\")\n\n\ndef test_spinner_non_unicode_console():\n    \"\"\"Regre"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_console.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n\nimport io\nimport sys\n\nimport pytest\n\nfrom astropy import units as u\nfrom astropy.utils import console\n\nfrom . import test_progress_bar_func\n\n\nclass FakeTTY(io.StringIO):\n    \"\"\"IOStream that fakes a TTY; provide an encoding to emulate an output\n    stream with a specific encoding.\n    \"\"\"\n\n    def __new__(cls, encoding=None):\n        # Return a new subclass of FakeTTY with the requested encoding\n        if encoding is None:\n            return super().__new__(cls)\n\n        cls = type(encoding.title() + cls.__name__, (cls,), {\"encoding\": encoding})\n\n        return cls.__new__(cls)\n\n    def __init__(self, encoding=None):\n        super().__init__()\n\n    def write(self, s):\n        if isinstance(s, bytes):\n            # Just allow this case to work\n            s = s.decode(\"latin-1\")\n        elif self.encoding is not None:\n            s.encode(self.encoding)\n\n        return super().write(s)\n\n    def isatty(self):\n        return True\n\n\ndef test_fake_tty():\n    # First test without a specified encoding; we should be able to write\n    # arbitrary unicode strings\n    f1 = FakeTTY()\n    assert f1.isatty()\n    f1.write(\"\")\n    assert f1.getvalue() == \"\"\n\n    # Now test an ASCII-only TTY--it should raise a UnicodeEncodeError when\n    # trying to write a string containing non-ASCII characters\n    f2 = FakeTTY(\"ascii\")\n    assert f2.isatty()\n    assert f2.__class__.__name__ == \"AsciiFakeTTY\"\n    assert pytest.raises(UnicodeEncodeError, f2.write, \"\")\n    assert f2.getvalue() == \"\"\n\n\n@pytest.mark.skipif(sys.platform.startswith(\"win\"), reason=\"Cannot test on Windows\")\ndef test_color_text():\n    assert console._color_text(\"foo\", \"green\") == \"\\033[0;32mfoo\\033[0m\"\n\n\ndef test_color_print():\n    # This stuff is hard to test, at least smoke test it\n    console.color_print(\"foo\", \"green\")\n\n    console.color_print(\"foo\", \"green\", \"bar\", \"red\")\n\n\ndef test_color_print2():\n    # Test that this automatically detects that io.S"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_console.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tringIO is\n    # not a tty\n    stream = io.StringIO()\n    console.color_print(\"foo\", \"green\", file=stream)\n    assert stream.getvalue() == \"foo\\n\"\n\n    stream = io.StringIO()\n    console.color_print(\"foo\", \"green\", \"bar\", \"red\", \"baz\", file=stream)\n    assert stream.getvalue() == \"foobarbaz\\n\"\n\n\n@pytest.mark.skipif(sys.platform.startswith(\"win\"), reason=\"Cannot test on Windows\")\ndef test_color_print3():\n    # Test that this thinks the FakeTTY is a tty and applies colors.\n\n    stream = FakeTTY()\n    console.color_print(\"foo\", \"green\", file=stream)\n    assert stream.getvalue() == \"\\x1b[0;32mfoo\\x1b[0m\\n\"\n\n    stream = FakeTTY()\n    console.color_print(\"foo\", \"green\", \"bar\", \"red\", \"baz\", file=stream)\n    assert stream.getvalue() == \"\\x1b[0;32mfoo\\x1b[0m\\x1b[0;31mbar\\x1b[0mbaz\\n\"\n\n\ndef test_color_print_unicode():\n    console.color_print(\"berbr\", \"red\")\n\n\ndef test_color_print_invalid_color():\n    console.color_print(\"foo\", \"unknown\")\n\n\ndef test_spinner_non_unicode_console():\n    \"\"\"Regression test for #1760\n\n    Ensures that the spinner can fall go into fallback mode when using the\n    unicode spinner on a terminal whose default encoding cannot encode the\n    unicode characters.\n    \"\"\"\n\n    stream = FakeTTY(\"ascii\")\n    chars = console.Spinner._default_unicode_chars\n\n    with console.Spinner(\"Reticulating splines\", file=stream, chars=chars) as s:\n        next(s)\n\n\ndef test_progress_bar():\n    # This stuff is hard to test, at least smoke test it\n    with console.ProgressBar(50) as bar:\n        for _ in range(50):\n            bar.update()\n\n\ndef test_progress_bar2():\n    for _ in console.ProgressBar(range(50)):\n        pass\n\n\ndef test_progress_bar3():\n    def do_nothing(*args, **kwargs):\n        pass\n\n    console.ProgressBar.map(do_nothing, range(50))\n\n\ndef test_zero_progress_bar():\n    with console.ProgressBar(0) as bar:\n        pass\n\n\ndef test_progress_bar_as_generator():\n    sum = 0\n    for x in console.ProgressBar(range(50)):\n        sum += x\n    assert sum == 1225\n"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "console.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tgrey,\n        default, darkgrey, lightred, lightgreen, yellow, lightblue,\n        lightmagenta, lightcyan, white, or '' (the empty string).\n    \"\"\"\n    color_mapping = {\n        \"black\": \"0;30\",\n        \"red\": \"0;31\",\n        \"green\": \"0;32\",\n        \"brown\": \"0;33\",\n        \"blue\": \"0;34\",\n        \"magenta\": \"0;35\",\n        \"cyan\": \"0;36\",\n        \"lightgrey\": \"0;37\",\n        \"default\": \"0;39\",\n        \"darkgrey\": \"1;30\",\n        \"lightred\": \"1;31\",\n        \"lightgreen\": \"1;32\",\n        \"yellow\": \"1;33\",\n        \"lightblue\": \"1;34\",\n        \"lightmagenta\": \"1;35\",\n        \"lightcyan\": \"1;36\",\n        \"white\": \"1;37\",\n    }\n\n    if sys.platform == \"win32\" and _IPython.OutStream is None:\n        # On Windows do not colorize text unless in IPython\n        return text\n\n    color_code = color_mapping.get(color, \"0;39\")\n    return f\"\\033[{color_code}m{text}\\033[0m\"\n\n\ndef _write_with_fallback(s, write, fileobj):\n    \"\"\"Write the supplied string with the given write function like\n    ``write(s)``, but use a writer for the locale's preferred encoding in case\n    of a UnicodeEncodeError.  Failing that attempt to write with 'utf-8' or\n    'latin-1'.\n    \"\"\"\n    try:\n        write(s)\n        return write\n    except UnicodeEncodeError:\n        # Let's try the next approach...\n        pass\n\n    enc = locale.getpreferredencoding()\n    try:\n        Writer = codecs.getwriter(enc)\n    except LookupError:\n        Writer = codecs.getwriter(\"utf-8\")\n\n    f = Writer(fileobj)\n    write = f.write\n\n    try:\n        write(s)\n        return write\n    except UnicodeEncodeError:\n        Writer = codecs.getwriter(\"latin-1\")\n        f = Writer(fileobj)\n        write = f.write\n\n    # If this doesn't work let the exception bubble up; I'm out of ideas\n    write(s)\n    return write\n\n\ndef color_print(*args, end=\"\\n\", **kwargs):\n    \"\"\"\n    Prints colors and styles to the terminal uses ANSI escape\n    sequences.\n\n    ::\n\n       color_print('This is the color ', 'default', 'GREEN', 'green')\n\n    Param"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_console.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ssion test for #1760\n\n    Ensures that the spinner can fall go into fallback mode when using the\n    unicode spinner on a terminal whose default encoding cannot encode the\n    unicode characters.\n    \"\"\"\n\n    stream = FakeTTY(\"ascii\")\n    chars = console.Spinner._default_unicode_chars\n\n    with console.Spinner(\"Reticulating splines\", file=stream, chars=chars) as s:\n        next(s)\n\n\ndef test_progress_bar():\n    # This stuff is hard to test, at least smoke test it\n    with console.ProgressBar(50) as bar:\n        for _ in range(50):\n            bar.update()\n\n\ndef test_progress_bar2():\n    for _ in console.ProgressBar(range(50)):\n        pass\n\n\ndef test_progress_bar3():\n    def do_nothing(*args, **kwargs):\n        pass\n\n    console.ProgressBar.map(do_nothing, range(50))\n\n\ndef test_zero_progress_bar():\n    with console.ProgressBar(0) as bar:\n        pass\n\n\ndef test_progress_bar_as_generator():\n    sum = 0\n    for x in console.ProgressBar(range(50)):\n        sum += x\n    assert sum == 1225\n\n    sum = 0\n    for x in console.ProgressBar(50):\n        sum += x\n    assert sum == 1225\n\n\ndef test_progress_bar_map():\n    items = list(range(100))\n    result = console.ProgressBar.map(\n        test_progress_bar_func.func, items, step=10, multiprocess=True\n    )\n    assert items == result\n\n    result1 = console.ProgressBar.map(\n        test_progress_bar_func.func, items, step=10, multiprocess=2\n    )\n\n    assert items == result1\n\n\n@pytest.mark.parametrize(\n    (\"seconds\", \"string\"),\n    [\n        (864088, \" 1w 3d\"),\n        (187213, \" 2d 4h\"),\n        (3905, \" 1h 5m\"),\n        (64, \" 1m 4s\"),\n        (15, \"   15s\"),\n        (2, \"    2s\"),\n    ],\n)\ndef test_human_time(seconds, string):\n    human_time = console.human_time(seconds)\n    assert human_time == string\n\n\n@pytest.mark.parametrize(\n    (\"size\", \"string\"),\n    [\n        (8640882, \"8.6M\"),\n        (187213, \"187k\"),\n        (3905, \"3.9k\"),\n        (64, \" 64 \"),\n        (2, \"  2 \"),\n        (10 * u.GB, \" 10G\"),\n    ],\n)\ndef test_h"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "console.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "xception:\n        try:\n            # see if POSIX standard variables will work\n            return (int(os.environ.get(\"LINES\")), int(os.environ.get(\"COLUMNS\")))\n        except TypeError:\n            # fall back on configuration variables, or if not\n            # set, (25, 80)\n            lines = conf.max_lines\n            width = conf.max_width\n            if lines is None:\n                lines = 25\n            if width is None:\n                width = 80\n            return lines, width\n\n\ndef _color_text(text, color):\n    \"\"\"Returns a string wrapped in ANSI color codes for coloring the text in a terminal.\n\n    ::\n\n        colored_text = color_text('Here is a message', 'blue')\n\n    This won't actually effect the text until it is printed to the\n    terminal.\n\n    Parameters\n    ----------\n    text : str\n        The string to return, bounded by the color codes.\n    color : str\n        An ANSI terminal color name. Must be one of:\n        black, red, green, brown, blue, magenta, cyan, lightgrey,\n        default, darkgrey, lightred, lightgreen, yellow, lightblue,\n        lightmagenta, lightcyan, white, or '' (the empty string).\n    \"\"\"\n    color_mapping = {\n        \"black\": \"0;30\",\n        \"red\": \"0;31\",\n        \"green\": \"0;32\",\n        \"brown\": \"0;33\",\n        \"blue\": \"0;34\",\n        \"magenta\": \"0;35\",\n        \"cyan\": \"0;36\",\n        \"lightgrey\": \"0;37\",\n        \"default\": \"0;39\",\n        \"darkgrey\": \"1;30\",\n        \"lightred\": \"1;31\",\n        \"lightgreen\": \"1;32\",\n        \"yellow\": \"1;33\",\n        \"lightblue\": \"1;34\",\n        \"lightmagenta\": \"1;35\",\n        \"lightcyan\": \"1;36\",\n        \"white\": \"1;37\",\n    }\n\n    if sys.platform == \"win32\" and _IPython.OutStream is None:\n        # On Windows do not colorize text unless in IPython\n        return text\n\n    color_code = color_mapping.get(color, \"0;39\")\n    return f\"\\033[{color_code}m{text}\\033[0m\"\n\n\ndef _write_with_fallback(s, write, fileobj):\n    \"\"\"Write the supplied string with the given write function like\n    ``write"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "console.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "eters\n    ----------\n    positional args : str\n        The positional arguments come in pairs (*msg*, *color*), where\n        *msg* is the string to display and *color* is the color to\n        display it in.\n\n        *color* is an ANSI terminal color name.  Must be one of:\n        black, red, green, brown, blue, magenta, cyan, lightgrey,\n        default, darkgrey, lightred, lightgreen, yellow, lightblue,\n        lightmagenta, lightcyan, white, or '' (the empty string).\n\n    file : :term:`file-like (writeable)`, optional\n        Where to write to.  Defaults to `sys.stdout`.  If file is not\n        a tty (as determined by calling its `isatty` member, if one\n        exists), no coloring will be included.\n\n    end : str, optional\n        The ending of the message.  Defaults to ``\\\\n``.  The end will\n        be printed after resetting any color or font state.\n    \"\"\"\n    file = kwargs.get(\"file\", sys.stdout)\n\n    write = file.write\n    if isatty(file) and conf.use_color:\n        for i in range(0, len(args), 2):\n            msg = args[i]\n            if i + 1 == len(args):\n                color = \"\"\n            else:\n                color = args[i + 1]\n\n            if color:\n                msg = _color_text(msg, color)\n\n            # Some file objects support writing unicode sensibly on some Python\n            # versions; if this fails try creating a writer using the locale's\n            # preferred encoding. If that fails too give up.\n\n            write = _write_with_fallback(msg, write, file)\n\n        write(end)\n    else:\n        for i in range(0, len(args), 2):\n            msg = args[i]\n            write(msg)\n        write(end)\n\n\ndef human_time(seconds):\n    \"\"\"\n    Returns a human-friendly time string that is always exactly 6\n    characters long.\n\n    Depending on the number of seconds given, can be one of::\n\n        1w 3d\n        2d 4h\n        1h 5m\n        1m 4s\n          15s\n\n    Will be in color if console coloring is turned on.\n\n    Parameters\n    ----------\n    "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "console.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "file, \"isatty\"):\n        return file.isatty()\n\n    if _IPython.OutStream is None or (not isinstance(file, _IPython.OutStream)):\n        return False\n\n    return getattr(file, \"name\", None) == \"stdout\"\n\n\n@deprecated(\"6.1\", alternative=\"shutil.get_terminal_size\")\ndef terminal_size(file=None):\n    \"\"\"\n    Returns a tuple (height, width) containing the height and width of\n    the terminal.\n\n    This function will look for the width in height in multiple areas\n    before falling back on the width and height in astropy's\n    configuration.\n    \"\"\"\n    if file is None:\n        file = sys.stdout\n\n    try:\n        s = struct.pack(\"HHHH\", 0, 0, 0, 0)\n        x = fcntl.ioctl(file, termios.TIOCGWINSZ, s)\n        (lines, width, xpixels, ypixels) = struct.unpack(\"HHHH\", x)\n        if lines > 12:\n            lines -= 6\n        if width > 10:\n            width -= 1\n        if lines <= 0 or width <= 0:\n            raise Exception(\"unable to get terminal size\")\n        return (lines, width)\n    except Exception:\n        try:\n            # see if POSIX standard variables will work\n            return (int(os.environ.get(\"LINES\")), int(os.environ.get(\"COLUMNS\")))\n        except TypeError:\n            # fall back on configuration variables, or if not\n            # set, (25, 80)\n            lines = conf.max_lines\n            width = conf.max_width\n            if lines is None:\n                lines = 25\n            if width is None:\n                width = 80\n            return lines, width\n\n\ndef _color_text(text, color):\n    \"\"\"Returns a string wrapped in ANSI color codes for coloring the text in a terminal.\n\n    ::\n\n        colored_text = color_text('Here is a message', 'blue')\n\n    This won't actually effect the text until it is printed to the\n    terminal.\n\n    Parameters\n    ----------\n    text : str\n        The string to return, bounded by the color codes.\n    color : str\n        An ANSI terminal color name. Must be one of:\n        black, red, green, brown, blue, magenta, cyan, ligh"}, {"start_line": 4000, "end_line": 5283, "belongs_to": {"file_name": "test_console.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n    sum = 0\n    for x in console.ProgressBar(50):\n        sum += x\n    assert sum == 1225\n\n\ndef test_progress_bar_map():\n    items = list(range(100))\n    result = console.ProgressBar.map(\n        test_progress_bar_func.func, items, step=10, multiprocess=True\n    )\n    assert items == result\n\n    result1 = console.ProgressBar.map(\n        test_progress_bar_func.func, items, step=10, multiprocess=2\n    )\n\n    assert items == result1\n\n\n@pytest.mark.parametrize(\n    (\"seconds\", \"string\"),\n    [\n        (864088, \" 1w 3d\"),\n        (187213, \" 2d 4h\"),\n        (3905, \" 1h 5m\"),\n        (64, \" 1m 4s\"),\n        (15, \"   15s\"),\n        (2, \"    2s\"),\n    ],\n)\ndef test_human_time(seconds, string):\n    human_time = console.human_time(seconds)\n    assert human_time == string\n\n\n@pytest.mark.parametrize(\n    (\"size\", \"string\"),\n    [\n        (8640882, \"8.6M\"),\n        (187213, \"187k\"),\n        (3905, \"3.9k\"),\n        (64, \" 64 \"),\n        (2, \"  2 \"),\n        (10 * u.GB, \" 10G\"),\n    ],\n)\ndef test_human_file_size(size, string):\n    human_time = console.human_file_size(size)\n    assert human_time == string\n\n\n@pytest.mark.parametrize(\"size\", (50 * u.km, 100 * u.g))\ndef test_bad_human_file_size(size):\n    assert pytest.raises(u.UnitConversionError, console.human_file_size, size)\n"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "test_pprint.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ript&gt;alert(&quot;gotcha&quot;);&lt;/script&gt;</td></tr>\",\n        \"<tr><td>2</td></tr>\",\n        \"<tr><td>3</td></tr>\",\n        \"</table></div>\",\n    ]\n\n\n@pytest.mark.usefixtures(\"table_type\")\nclass TestPprint:\n    def _setup(self, table_type):\n        self.tb = table_type(BIG_WIDE_ARR)\n        self.tb[\"col0\"].format = \"e\"\n        self.tb[\"col1\"].format = \".6f\"\n\n        self.tb[\"col0\"].unit = \"km**2\"\n        self.tb[\"col19\"].unit = \"kg s m**-2\"\n        self.ts = table_type(SMALL_ARR)\n\n    def test_empty_table(self, table_type):\n        t = table_type()\n        lines = t.pformat()\n        assert lines == [\"<No columns>\"]\n        c = repr(t)\n        masked = \"masked=True \" if t.masked else \"\"\n        assert c.splitlines() == [\n            f\"<{table_type.__name__} {masked}length=0>\",\n            \"<No columns>\",\n        ]\n\n    def test_format0(self, table_type):\n        \"\"\"Try getting screen size but fail to defaults because testing doesn't\n        have access to screen (fcntl.ioctl fails).\n        \"\"\"\n        self._setup(table_type)\n        arr = np.arange(4000, dtype=np.float64).reshape(100, 40)\n        with conf.set_temp(\"max_width\", None), conf.set_temp(\"max_lines\", None):\n            lines = table_type(arr).pformat(max_lines=None, max_width=None)\n        width, nlines = get_terminal_size()\n        assert len(lines) == nlines\n        for line in lines[:-1]:  # skip last \"Length = .. rows\" line\n            assert width - 10 < len(line) <= width\n\n    def test_format1(self, table_type):\n        \"\"\"Basic test of formatting, unit header row included\"\"\"\n        self._setup(table_type)\n        lines = self.tb.pformat(max_lines=8, max_width=40)\n        assert lines == [\n            \"    col0         col1    ...   col19  \",\n            \"    km2                  ... kg s / m2\",\n            \"------------ ----------- ... ---------\",\n            \"0.000000e+00    1.000000 ...      19.0\",\n            \"         ...         ... ...       ...\",\n            \"1.960000e+03 1961.0000"}], "retrieved_count": 10, "cost_time": 1.0778002738952637}
{"question": "How does the unit validation decorator that validates function argument and return value units in the astropy units module handle return type annotation when explicitly set to None in the decorated function signature?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "decorators.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n\n    # check if unit-like\n    try:\n        unit = Unit(target)\n    except (TypeError, ValueError):\n        try:\n            ptype = get_physical_type(target)\n        except (TypeError, ValueError, KeyError):  # KeyError for Enum\n            if isinstance(target, str):\n                raise ValueError(f\"invalid unit or physical type {target!r}.\") from None\n        else:\n            return ptype\n    else:\n        return unit\n\n    # could be a type hint\n    origin = T.get_origin(target)\n    if origin is T.Union:\n        return [_parse_annotation(t) for t in T.get_args(target)]\n    elif origin is not T.Annotated:  # can't be Quantity[]\n        return False\n\n    # parse type hint\n    cls, *annotations = T.get_args(target)\n    if not issubclass(cls, Quantity) or not annotations:\n        return False\n\n    # get unit from type hint\n    unit, *rest = annotations\n    if not isinstance(unit, (UnitBase, PhysicalType)):\n        return False\n\n    return unit\n\n\nclass QuantityInput:\n    @classmethod\n    def as_decorator(cls, func=None, **kwargs):\n        r\"\"\"\n        A decorator for validating the units of arguments to functions.\n\n        Unit specifications can be provided as keyword arguments to the\n        decorator, or by using function annotation syntax. Arguments to the\n        decorator take precedence over any function annotations present.\n\n        A `~astropy.units.UnitsError` will be raised if the unit attribute of\n        the argument is not equivalent to the unit specified to the decorator or\n        in the annotation. If the argument has no unit attribute, i.e. it is not\n        a Quantity object, a `ValueError` will be raised unless the argument is\n        an annotation. This is to allow non Quantity annotations to pass\n        through.\n\n        Where an equivalency is specified in the decorator, the function will be\n        executed with that equivalency in force.\n\n        Notes\n        -----\n        The checking of arguments inside variable arguments to a function "}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "decorators.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    def as_decorator(cls, func=None, **kwargs):\n        r\"\"\"\n        A decorator for validating the units of arguments to functions.\n\n        Unit specifications can be provided as keyword arguments to the\n        decorator, or by using function annotation syntax. Arguments to the\n        decorator take precedence over any function annotations present.\n\n        A `~astropy.units.UnitsError` will be raised if the unit attribute of\n        the argument is not equivalent to the unit specified to the decorator or\n        in the annotation. If the argument has no unit attribute, i.e. it is not\n        a Quantity object, a `ValueError` will be raised unless the argument is\n        an annotation. This is to allow non Quantity annotations to pass\n        through.\n\n        Where an equivalency is specified in the decorator, the function will be\n        executed with that equivalency in force.\n\n        Notes\n        -----\n        The checking of arguments inside variable arguments to a function is not\n        supported (i.e. \\*arg or \\**kwargs).\n\n        The original function is accessible by the attributed ``__wrapped__``.\n        See :func:`functools.wraps` for details.\n\n        Examples\n        --------\n        .. code-block:: python\n\n            import astropy.units as u\n            @u.quantity_input(myangle=u.arcsec)\n            def myfunction(myangle):\n                return myangle**2\n\n\n        .. code-block:: python\n\n            import astropy.units as u\n            @u.quantity_input\n            def myfunction(myangle: u.arcsec):\n                return myangle**2\n\n        Or using a unit-aware Quantity annotation.\n\n        .. code-block:: python\n\n            @u.quantity_input\n            def myfunction(myangle: u.Quantity[u.arcsec]):\n                return myangle**2\n\n        Also you can specify a return value annotation, which will\n        cause the function to always return a `~astropy.units.Quantity` in that\n        unit.\n\n        .. code-block:: python\n\n         "}, {"start_line": 8000, "end_line": 9880, "belongs_to": {"file_name": "test_quantity_annotations.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ble to '{str(solary_unit)}'.\"\n        ),\n    ):\n        solarx, solary = myfunc_args(1 * u.arcsec, solary=100 * u.km)\n\n\n@pytest.mark.parametrize(\n    \"solarx_unit,solary_unit\", [(u.arcsec, u.deg), (\"angle\", \"angle\")]\n)\ndef test_kwarg_not_quantity3(solarx_unit, solary_unit):\n    @u.quantity_input\n    def myfunc_args(solarx: solarx_unit, solary: solary_unit = 10 * u.deg):\n        return solarx, solary\n\n    with pytest.raises(\n        TypeError,\n        match=(\n            \"Argument 'solary' to function 'myfunc_args' has no 'unit' attribute. \"\n            \"You should pass in an astropy Quantity instead.\"\n        ),\n    ):\n        solarx, solary = myfunc_args(1 * u.arcsec, solary=100)\n\n\n@pytest.mark.parametrize(\n    \"solarx_unit,solary_unit\", [(u.arcsec, u.deg), (\"angle\", \"angle\")]\n)\ndef test_kwarg_default3(solarx_unit, solary_unit):\n    @u.quantity_input\n    def myfunc_args(solarx: solarx_unit, solary: solary_unit = 10 * u.deg):\n        return solarx, solary\n\n    solarx, solary = myfunc_args(1 * u.arcsec)\n\n\ndef test_return_annotation():\n    @u.quantity_input\n    def myfunc_args(solarx: u.arcsec) -> u.deg:\n        return solarx\n\n    solarx = myfunc_args(1 * u.arcsec)\n    assert solarx.unit is u.deg\n\n\ndef test_return_annotation_none():\n    @u.quantity_input\n    def myfunc_args(solarx: u.arcsec) -> None:\n        pass\n\n    solarx = myfunc_args(1 * u.arcsec)\n    assert solarx is None\n\n\ndef test_return_annotation_notUnit():\n    @u.quantity_input\n    def myfunc_args(solarx: u.arcsec) -> int:\n        return 0\n\n    solarx = myfunc_args(1 * u.arcsec)\n    assert solarx == 0\n\n\ndef test_enum_annotation():\n    # Regression test for gh-9932\n    from enum import Enum, auto\n\n    class BasicEnum(Enum):\n        AnOption = auto()\n\n    @u.quantity_input\n    def myfunc_args(a: BasicEnum, b: u.arcsec) -> None:\n        pass\n\n    myfunc_args(BasicEnum.AnOption, 1 * u.arcsec)\n"}, {"start_line": 11000, "end_line": 12517, "belongs_to": {"file_name": "decorators.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "try if we don't have\n                # equivalencies to add. (If we're wrapping a short function,\n                # the time spent duplicating the registry is quite noticeable.)\n                equiv_context = contextlib.nullcontext()\n            # Call the original function with any equivalencies in force.\n            with equiv_context:\n                return_ = wrapped_function(*func_args, **func_kwargs)\n\n            # Return\n            ra = wrapped_signature.return_annotation\n            valid_empty = (inspect.Signature.empty, None, NoneType, T.NoReturn)\n            if ra not in valid_empty:\n                target = (\n                    ra\n                    if T.get_origin(ra) not in (T.Annotated, T.Union)\n                    else _parse_annotation(ra)\n                )\n                if isinstance(target, str) or not isinstance(target, Sequence):\n                    target = [target]\n                valid_targets = [\n                    t for t in target if isinstance(t, (str, UnitBase, PhysicalType))\n                ]\n                _validate_arg_value(\n                    \"return\",\n                    wrapped_function.__name__,\n                    return_,\n                    valid_targets,\n                    self.equivalencies,\n                    self.strict_dimensionless,\n                )\n                if len(valid_targets) > 0:\n                    return_ <<= valid_targets[0]\n            return return_\n\n        return wrapper\n\n\nquantity_input = QuantityInput.as_decorator\n"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "test_quantity_decorator.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ysical_type():\n    @u.quantity_input(x=\"angle\", y=\"africanswallow\")\n    def myfunc_args(x, y=10 * u.deg):\n        return x, y\n\n    with pytest.raises(\n        ValueError, match=\"Invalid unit or physical type 'africanswallow'.\"\n    ):\n        x, y = myfunc_args(1 * u.arcsec, y=100 * u.deg)\n\n\ndef test_default_value_check():\n    x_target = u.deg\n    x_unit = u.arcsec\n\n    with pytest.raises(TypeError):\n\n        @u.quantity_input(x=x_target)\n        def myfunc_args(x=1.0):\n            return x\n\n        x = myfunc_args()\n\n    x = myfunc_args(1 * x_unit)\n    assert isinstance(x, u.Quantity)\n    assert x.unit == x_unit\n\n\ndef test_str_unit_typo():\n    @u.quantity_input\n    def myfunc_args(x: \"kilograam\"):  # noqa: F821\n        return x\n\n    with pytest.raises(ValueError):\n        result = myfunc_args(u.kg)\n\n\nclass TestTypeAnnotations:\n    @pytest.mark.parametrize(\n        \"annot\",\n        [u.m, u.Quantity[u.m], u.Quantity[u.m, \"more\"]],\n    )  # Note: parametrization is done even if test class is skipped\n    def test_single_annotation_unit(self, annot):\n        \"\"\"Try a variety of valid annotations.\"\"\"\n\n        @u.quantity_input\n        def myfunc_args(x: annot, y: str):\n            return x, y\n\n        i_q, i_str = 2 * u.m, \"cool string\"\n        o_q, o_str = myfunc_args(i_q, i_str)\n        assert o_q == i_q\n        assert o_str == i_str\n\n\ndef test_args_None():\n    x_target = u.deg\n    x_unit = u.arcsec\n    y_target = u.km\n    y_unit = u.kpc\n\n    @u.quantity_input(x=[x_target, None], y=[None, y_target])\n    def myfunc_args(x, y):\n        return x, y\n\n    x, y = myfunc_args(1 * x_unit, None)\n    assert isinstance(x, u.Quantity)\n    assert x.unit == x_unit\n    assert y is None\n\n    x, y = myfunc_args(None, 1 * y_unit)\n    assert isinstance(y, u.Quantity)\n    assert y.unit == y_unit\n    assert x is None\n\n\ndef test_args_None_kwarg():\n    x_target = u.deg\n    x_unit = u.arcsec\n    y_target = u.km\n\n    @u.quantity_input(x=x_target, y=y_target)\n    def myfunc_args(x, y=None):\n    "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_quantity_annotations.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\nimport pytest\n\nfrom astropy import units as u\nfrom astropy.units import Quantity\n\n\ndef test_ignore_generic_type_annotations():\n    \"\"\"Test annotations that are not unit related are ignored.\n\n    This test passes if the function works.\n    \"\"\"\n\n    # one unit, one not (should be ignored)\n    @u.quantity_input\n    def func(x: u.m, y: str):\n        return x, y\n\n    i_q, i_str = 2 * u.m, \"cool string\"\n    o_q, o_str = func(i_q, i_str)  # if this doesn't fail, it worked.\n    assert i_q == o_q\n    assert i_str == o_str\n\n\nclass TestQuantityUnitAnnotations:\n    \"\"\"Test Quantity[Unit] type annotation.\"\"\"\n\n    def test_simple_annotation(self):\n        @u.quantity_input\n        def func(x: Quantity[u.m], y: str):\n            return x, y\n\n        i_q, i_str = 2 * u.m, \"cool string\"\n        o_q, o_str = func(i_q, i_str)\n        assert i_q == o_q\n        assert i_str == o_str\n\n        # checks the input on the 1st arg\n        with pytest.raises(u.UnitsError):\n            func(1 * u.s, i_str)\n\n        # but not the second\n        o_q, o_str = func(i_q, {\"not\": \"a string\"})\n        assert i_q == o_q\n        assert i_str != o_str\n\n    def test_multiple_annotation(self):\n        @u.quantity_input\n        def multi_func(a: Quantity[u.km]) -> Quantity[u.m]:\n            return a\n\n        i_q = 2 * u.km\n        o_q = multi_func(i_q)\n        assert o_q == i_q\n        assert o_q.unit == u.m\n\n    def test_optional_and_annotated(self):\n        @u.quantity_input\n        def opt_func(x: Quantity[u.m] | None = None) -> Quantity[u.km]:\n            if x is None:\n                return 1 * u.km\n            return x\n\n        i_q = 250 * u.m\n        o_q = opt_func(i_q)\n        assert o_q.unit == u.km\n        assert o_q == i_q\n\n        i_q = None\n        o_q = opt_func(i_q)\n        assert o_q == 1 * u.km\n\n    def test_union_and_annotated(self):\n        #  Union and Annotated\n        @u.quantity_input\n        def union_func(x: Quantity[u"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "decorators.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "target unit\n                if arg is None and param.default is None:\n                    continue\n\n                # Here, we check whether multiple target unit/physical type's\n                #   were specified in the decorator/annotation, or whether a\n                #   single string (unit or physical type) or a Unit object was\n                #   specified\n                if isinstance(targets, str) or not isinstance(targets, Sequence):\n                    valid_targets = [targets]\n\n                # Check for None in the supplied list of allowed units and, if\n                #   present and the passed value is also None, ignore.\n                elif None in targets or NoneType in targets:\n                    if arg is None:\n                        continue\n                    else:\n                        valid_targets = [t for t in targets if t is not None]\n\n                else:\n                    valid_targets = targets\n\n                # If we're dealing with an annotation, skip all the targets that\n                #    are not strings or subclasses of Unit. This is to allow\n                #    non unit related annotations to pass through\n                if is_annotation:\n                    valid_targets = [\n                        t\n                        for t in valid_targets\n                        if isinstance(t, (str, UnitBase, PhysicalType))\n                    ]\n\n                # Now we loop over the allowed units/physical types and validate\n                #   the value of the argument:\n                _validate_arg_value(\n                    param.name,\n                    wrapped_function.__name__,\n                    arg,\n                    valid_targets,\n                    self.equivalencies,\n                    self.strict_dimensionless,\n                )\n\n            if self.equivalencies:\n                equiv_context = add_enabled_equivalencies(self.equivalencies)\n            else:\n                # Avoid creating a duplicate regis"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_quantity_annotations.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ".m] | (Quantity[u.s] | None)):\n            if x is None:\n                return None\n            else:\n                return 2 * x\n\n        i_q = 1 * u.m\n        o_q = union_func(i_q)\n        assert o_q == 2 * i_q\n\n        i_q = 1 * u.s\n        o_q = union_func(i_q)\n        assert o_q == 2 * i_q\n\n        i_q = None\n        o_q = union_func(i_q)\n        assert o_q is None\n\n    def test_not_unit_or_ptype(self):\n        with pytest.raises(TypeError, match=\"unit annotation is not\"):\n            Quantity[\"definitely not a unit\"]\n\n\n@pytest.mark.parametrize(\n    \"solarx_unit,solary_unit\", [(u.arcsec, u.arcsec), (\"angle\", \"angle\")]\n)\ndef test_args3(solarx_unit, solary_unit):\n    @u.quantity_input\n    def myfunc_args(solarx: solarx_unit, solary: solary_unit):\n        return solarx, solary\n\n    solarx, solary = myfunc_args(1 * u.arcsec, 1 * u.arcsec)\n\n    assert isinstance(solarx, Quantity)\n    assert isinstance(solary, Quantity)\n\n    assert solarx.unit == u.arcsec\n    assert solary.unit == u.arcsec\n\n\n@pytest.mark.parametrize(\n    \"solarx_unit,solary_unit\", [(u.arcsec, u.arcsec), (\"angle\", \"angle\")]\n)\ndef test_args_noconvert3(solarx_unit, solary_unit):\n    @u.quantity_input()\n    def myfunc_args(solarx: solarx_unit, solary: solary_unit):\n        return solarx, solary\n\n    solarx, solary = myfunc_args(1 * u.deg, 1 * u.arcmin)\n\n    assert isinstance(solarx, Quantity)\n    assert isinstance(solary, Quantity)\n\n    assert solarx.unit == u.deg\n    assert solary.unit == u.arcmin\n\n\n@pytest.mark.parametrize(\"solarx_unit\", [u.arcsec, \"angle\"])\ndef test_args_nonquantity3(solarx_unit):\n    @u.quantity_input\n    def myfunc_args(solarx: solarx_unit, solary):\n        return solarx, solary\n\n    solarx, solary = myfunc_args(1 * u.arcsec, 100)\n\n    assert isinstance(solarx, Quantity)\n    assert isinstance(solary, int)\n\n    assert solarx.unit == u.arcsec\n\n\n@pytest.mark.parametrize(\n    \"solarx_unit,solary_unit\", [(u.arcsec, u.eV), (\"angle\", \"energy\")]\n)\ndef test_arg_equivalencies3(solarx_unit, "}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "decorators.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "skip all the targets that\n                #    are not strings or subclasses of Unit. This is to allow\n                #    non unit related annotations to pass through\n                if is_annotation:\n                    valid_targets = [\n                        t\n                        for t in valid_targets\n                        if isinstance(t, (str, UnitBase, PhysicalType))\n                    ]\n\n                # Now we loop over the allowed units/physical types and validate\n                #   the value of the argument:\n                _validate_arg_value(\n                    param.name,\n                    wrapped_function.__name__,\n                    arg,\n                    valid_targets,\n                    self.equivalencies,\n                    self.strict_dimensionless,\n                )\n\n            if self.equivalencies:\n                equiv_context = add_enabled_equivalencies(self.equivalencies)\n            else:\n                # Avoid creating a duplicate registry if we don't have\n                # equivalencies to add. (If we're wrapping a short function,\n                # the time spent duplicating the registry is quite noticeable.)\n                equiv_context = contextlib.nullcontext()\n            # Call the original function with any equivalencies in force.\n            with equiv_context:\n                return_ = wrapped_function(*func_args, **func_kwargs)\n\n            # Return\n            ra = wrapped_signature.return_annotation\n            valid_empty = (inspect.Signature.empty, None, NoneType, T.NoReturn)\n            if ra not in valid_empty:\n                target = (\n                    ra\n                    if T.get_origin(ra) not in (T.Annotated, T.Union)\n                    else _parse_annotation(ra)\n                )\n                if isinstance(target, str) or not isinstance(target, Sequence):\n                    target = [target]\n                valid_targets = [\n                    t for t in target if isinstance(t, (str"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "decorators.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ype {target!r}.\") from None\n\n        allowed_units.append(unit)\n\n    return allowed_units\n\n\ndef _validate_arg_value(\n    param_name, func_name, arg, targets, equivalencies, strict_dimensionless=False\n):\n    \"\"\"\n    Validates the object passed in to the wrapped function, ``arg``, with target\n    unit or physical type, ``target``.\n    \"\"\"\n    if len(targets) == 0:\n        return\n\n    allowed_units = _get_allowed_units(targets)\n\n    # If dimensionless is an allowed unit and the argument is unit-less,\n    #   allow numbers or numpy arrays with numeric dtypes\n    if (\n        not strict_dimensionless\n        and not hasattr(arg, \"unit\")\n        and dimensionless_unscaled in allowed_units\n    ):\n        if isinstance(arg, Number):\n            return\n\n        elif isinstance(arg, np.ndarray) and np.issubdtype(arg.dtype, np.number):\n            return\n\n    for allowed_unit in allowed_units:\n        try:\n            if arg.unit.is_equivalent(allowed_unit, equivalencies=equivalencies):\n                break\n\n        except AttributeError:  # Either there is no .unit or no .is_equivalent\n            if hasattr(arg, \"unit\"):\n                error_msg = \"a 'unit' attribute without an 'is_equivalent' method\"\n            else:\n                error_msg = \"no 'unit' attribute\"\n\n            raise TypeError(\n                f\"Argument '{param_name}' to function '{func_name}'\"\n                f\" has {error_msg}. You should pass in an astropy \"\n                \"Quantity instead.\"\n            )\n\n    else:\n        error_msg = (\n            f\"Argument '{param_name}' to function '{func_name}' must \"\n            \"be in units convertible to\"\n        )\n        if len(targets) > 1:\n            targ_names = \", \".join([f\"'{targ}'\" for targ in targets])\n            raise UnitsError(f\"{error_msg} one of: {targ_names}.\")\n        else:\n            raise UnitsError(f\"{error_msg} '{targets[0]}'.\")\n\n\ndef _parse_annotation(target):\n    if target in (None, NoneType, inspect._empty):\n        return target"}], "retrieved_count": 10, "cost_time": 1.1133239269256592}
{"question": "Why does the time delta format class that represents time intervals as human-readable strings with quantity components use two separate Julian Date components (an integer part and a fractional part) instead of a single floating-point representation?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 80000, "end_line": 82000, "belongs_to": {"file_name": "formats.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " be datetime.timedelta objects\"\n            )\n        if val2 is not None:\n            raise ValueError(\n                f\"{self.name} objects do not accept a val2 but you provided {val2}\"\n            )\n        return val1, None\n\n    def set_jds(self, val1, val2):\n        self._check_scale(self._scale)  # Validate scale.\n        iterator = np.nditer(\n            [val1, None, None],\n            flags=[\"refs_ok\", \"zerosize_ok\"],\n            op_dtypes=[None, np.double, np.double],\n        )\n\n        day = datetime.timedelta(days=1)\n        for val, jd1, jd2 in iterator:\n            jd1[...], other = divmod(val.item(), day)\n            jd2[...] = other / day\n\n        self.jd1, self.jd2 = day_frac(iterator.operands[-2], iterator.operands[-1])\n\n    @property\n    def value(self):\n        iterator = np.nditer(\n            [self.jd1, self.jd2, None],\n            flags=[\"refs_ok\", \"zerosize_ok\"],\n            op_dtypes=[None, None, object],\n        )\n\n        for jd1, jd2, out in iterator:\n            jd1_, jd2_ = day_frac(jd1, jd2)\n            out[...] = datetime.timedelta(days=jd1_, microseconds=jd2_ * 86400 * 1e6)\n\n        return iterator.operands[-1]\n\n\nclass TimeDeltaQuantityString(TimeDeltaFormat, TimeUnique):\n    \"\"\"Time delta as a string with one or more Quantity components.\n\n    This format provides a human-readable multi-scale string representation of a time\n    delta. It is convenient for applications like a configuration file or a command line\n    option.\n\n    The format is specified as follows:\n\n    - The string is a sequence of one or more components.\n    - Each component is a number followed by an astropy unit of time.\n    - For input, whitespace within the string is allowed but optional.\n    - For output, there is a single space between components.\n    - The allowed components are listed below.\n    - The order (yr, d, hr, min, s) is fixed but individual components are optional.\n\n    The allowed input component units are shown below:\n\n    - \"yr\": years (365.25 da"}, {"start_line": 79000, "end_line": 81000, "belongs_to": {"file_name": "formats.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " day_frac(val1, val2, divisor=1.0 / self.unit)\n\n    def to_value(self, **kwargs):\n        # Note that 1/unit is always exactly representable, so the\n        # following multiplications are exact.\n        factor = 1.0 / self.unit\n        jd1 = self.jd1 * factor\n        jd2 = self.jd2 * factor\n        return super().to_value(jd1=jd1, jd2=jd2, **kwargs)\n\n    value = property(to_value)\n\n\nclass TimeDeltaSec(TimeDeltaNumeric):\n    \"\"\"Time delta in SI seconds.\"\"\"\n\n    name = \"sec\"\n    unit = 1.0 / erfa.DAYSEC  # for quantity input\n\n\nclass TimeDeltaJD(TimeDeltaNumeric, TimeUnique):\n    \"\"\"Time delta in Julian days (86400 SI seconds).\"\"\"\n\n    name = \"jd\"\n    unit = 1.0\n\n\nclass TimeDeltaDatetime(TimeDeltaFormat, TimeUnique):\n    \"\"\"Time delta in datetime.timedelta.\"\"\"\n\n    name = \"datetime\"\n\n    def _check_val_type(self, val1, val2):\n        if not all(isinstance(val, datetime.timedelta) for val in val1.flat):\n            raise TypeError(\n                f\"Input values for {self.name} class must be datetime.timedelta objects\"\n            )\n        if val2 is not None:\n            raise ValueError(\n                f\"{self.name} objects do not accept a val2 but you provided {val2}\"\n            )\n        return val1, None\n\n    def set_jds(self, val1, val2):\n        self._check_scale(self._scale)  # Validate scale.\n        iterator = np.nditer(\n            [val1, None, None],\n            flags=[\"refs_ok\", \"zerosize_ok\"],\n            op_dtypes=[None, np.double, np.double],\n        )\n\n        day = datetime.timedelta(days=1)\n        for val, jd1, jd2 in iterator:\n            jd1[...], other = divmod(val.item(), day)\n            jd2[...] = other / day\n\n        self.jd1, self.jd2 = day_frac(iterator.operands[-2], iterator.operands[-1])\n\n    @property\n    def value(self):\n        iterator = np.nditer(\n            [self.jd1, self.jd2, None],\n            flags=[\"refs_ok\", \"zerosize_ok\"],\n            op_dtypes=[None, None, object],\n        )\n\n        for jd1, jd2, out in iterator:\n      "}, {"start_line": 87000, "end_line": 89000, "belongs_to": {"file_name": "formats.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    else:\n                value = (jd * u.day).to_value(subfmt)\n                value = np.round(value, self.precision)\n                comps = [f\"{value}{subfmt}\"]\n\n            out[...] = sign + \" \".join(comps)\n\n        return iterator.operands[-1]\n\n    def get_multi_comps(self, jd1, jd2):\n        jd, remainder = two_sum(jd1, jd2)\n        days = int(np.floor(jd))\n        jd -= days\n        jd += remainder\n\n        hours = int(np.floor(jd * 24.0))\n        jd -= hours / 24.0\n        mins = int(np.floor(jd * 1440.0))\n        jd -= mins / 1440.0\n        secs = np.round(jd * 86400.0, self.precision)\n\n        comp_vals = [days, hours, mins, secs]\n        if secs >= 60.0:\n            self.fix_comp_vals_overflow(comp_vals)\n\n        comps = [\n            f\"{comp_val}{name}\"\n            for comp_val, name in zip(comp_vals, (\"d\", \"hr\", \"min\", \"s\"))\n            if comp_val != 0\n        ]\n        if not comps:\n            comps = [\"0.0s\"]\n\n        return comps\n\n    @staticmethod\n    def fix_comp_vals_overflow(comp_vals):\n        comp_maxes = (None, 24, 60, 60.0)\n        for ii in [3, 2, 1]:\n            comp_val = comp_vals[ii]\n            comp_max = comp_maxes[ii]\n            if comp_val >= comp_max:\n                comp_vals[ii] -= comp_max\n                comp_vals[ii - 1] += 1\n\n    @property\n    def value(self):\n        return self.to_value()\n\n\ndef _validate_jd_for_storage(jd):\n    if isinstance(jd, (float, int)):\n        return np.array(jd, dtype=float)\n    if isinstance(jd, np.generic) and (\n        (jd.dtype.kind == \"f\" and jd.dtype.itemsize <= 8) or jd.dtype.kind in \"iu\"\n    ):\n        return np.array(jd, dtype=float)\n    elif isinstance(jd, np.ndarray) and jd.dtype.kind == \"f\" and jd.dtype.itemsize == 8:\n        return jd\n    else:\n        raise TypeError(\n            \"JD values must be arrays (possibly zero-dimensional) \"\n            f\"of floats but we got {jd!r} of type {type(jd)}\"\n        )\n\n\ndef _broadcast_writeable(jd1, jd2):\n    if jd1.shape == jd2.shape:\n        "}, {"start_line": 81000, "end_line": 83000, "belongs_to": {"file_name": "formats.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      jd1_, jd2_ = day_frac(jd1, jd2)\n            out[...] = datetime.timedelta(days=jd1_, microseconds=jd2_ * 86400 * 1e6)\n\n        return iterator.operands[-1]\n\n\nclass TimeDeltaQuantityString(TimeDeltaFormat, TimeUnique):\n    \"\"\"Time delta as a string with one or more Quantity components.\n\n    This format provides a human-readable multi-scale string representation of a time\n    delta. It is convenient for applications like a configuration file or a command line\n    option.\n\n    The format is specified as follows:\n\n    - The string is a sequence of one or more components.\n    - Each component is a number followed by an astropy unit of time.\n    - For input, whitespace within the string is allowed but optional.\n    - For output, there is a single space between components.\n    - The allowed components are listed below.\n    - The order (yr, d, hr, min, s) is fixed but individual components are optional.\n\n    The allowed input component units are shown below:\n\n    - \"yr\": years (365.25 days)\n    - \"d\": days (24 hours)\n    - \"hr\": hours (60 minutes)\n    - \"min\": minutes (60 seconds)\n    - \"s\": seconds\n\n    .. Note:: These definitions correspond to physical units of time and are NOT\n       calendar date intervals. Thus adding \"1yr\" to \"2000-01-01 00:00:00\" will give\n       \"2000-12-31 06:00:00\" instead of \"2001-01-01 00:00:00\".\n\n    The ``out_subfmt`` attribute specifies the components to be included in the string\n    output.  The default is ``\"multi\"`` which represents the time delta as\n    ``\"<days>d <hours>hr <minutes>min <seconds>s\"``, where only non-zero components are\n    included.\n\n    - \"multi\": multiple components, e.g. \"2d 3hr 15min 5.6s\"\n    - \"yr\": years\n    - \"d\": days\n    - \"hr\": hours\n    - \"min\": minutes\n    - \"s\": seconds\n\n    Examples\n    --------\n    >>> from astropy.time import Time, TimeDelta\n    >>> import astropy.units as u\n\n    >>> print(TimeDelta(\"1yr\"))\n    365d 6hr\n\n    >>> print(Time(\"2000-01-01\") + TimeDelta(\"1yr\"))\n    2000-12-31 06:00:00.00"}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "formats.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_class__.value.fget is not self.__class__.to_value:\n            return self.value\n\n        if jd1 is None:\n            jd1 = self.jd1\n        if jd2 is None:\n            jd2 = self.jd2\n        if out_subfmt is None:\n            out_subfmt = self.out_subfmt\n        subfmt = self._select_subfmts(out_subfmt)[0]\n        kwargs = {}\n        if subfmt[0] in (\"str\", \"bytes\"):\n            unit = getattr(self, \"unit\", 1)\n            digits = int(np.ceil(np.log10(unit / np.finfo(float).eps)))\n            # TODO: allow a way to override the format.\n            kwargs[\"fmt\"] = f\".{digits}f\"\n        return subfmt[3](jd1, jd2, **kwargs)\n\n    value = property(to_value)\n\n\nclass TimeJD(TimeNumeric):\n    \"\"\"\n    Julian Date time format.\n\n    This represents the number of days since the beginning of\n    the Julian Period.\n    For example, 2451544.5 in JD is midnight on January 1, 2000.\n    \"\"\"\n\n    name = \"jd\"\n\n    def set_jds(self, val1, val2):\n        self._check_scale(self._scale)  # Validate scale.\n        self.jd1, self.jd2 = day_frac(val1, val2)\n\n\nclass TimeMJD(TimeNumeric):\n    \"\"\"\n    Modified Julian Date time format.\n\n    This represents the number of days since midnight on November 17, 1858.\n    For example, 51544.0 in MJD is midnight on January 1, 2000.\n    \"\"\"\n\n    name = \"mjd\"\n\n    def set_jds(self, val1, val2):\n        self._check_scale(self._scale)  # Validate scale.\n        jd1, jd2 = day_frac(val1, val2)\n        jd1 += erfa.DJM0  # erfa.DJM0=2400000.5 (from erfam.h).\n        self.jd1, self.jd2 = day_frac(jd1, jd2)\n\n    def to_value(self, **kwargs):\n        jd1 = self.jd1 - erfa.DJM0  # This cannot lose precision.\n        jd2 = self.jd2\n        return super().to_value(jd1=jd1, jd2=jd2, **kwargs)\n\n    value = property(to_value)\n\n\ndef _check_val_type_not_quantity(format_name, val1, val2):\n    # If val2 is a Quantity, the super() call that follows this check\n    # will raise a TypeError.\n    if hasattr(val1, \"to\") and getattr(val1, \"unit\", None) is not None:\n        raise"}, {"start_line": 78000, "end_line": 80000, "belongs_to": {"file_name": "formats.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    \"\"\"Julian Epoch year as string value(s) like 'J2000.0'.\"\"\"\n\n    name = \"jyear_str\"\n    epoch_to_jd = \"epj2jd\"\n    jd_to_epoch = \"epj\"\n    epoch_prefix = \"J\"\n\n\nclass TimeDeltaFormat(TimeFormat):\n    \"\"\"Base class for time delta representations.\"\"\"\n\n    _registry = TIME_DELTA_FORMATS\n    _default_precision = 3\n    # Somewhat arbitrary values that are effectively no limit for precision.\n    _min_precision = -99\n    _max_precision = 99\n\n    def _check_scale(self, scale):\n        \"\"\"\n        Check that the scale is in the allowed list of scales, or is `None`.\n        \"\"\"\n        if scale is not None and scale not in TIME_DELTA_SCALES:\n            raise ScaleValueError(\n                f\"Scale value '{scale}' not in allowed values {TIME_DELTA_SCALES}\"\n            )\n\n        return scale\n\n\nclass TimeDeltaNumeric(TimeDeltaFormat, TimeNumeric):\n    _check_finite = False\n\n    def set_jds(self, val1, val2):\n        self._check_scale(self._scale)  # Validate scale.\n        self.jd1, self.jd2 = day_frac(val1, val2, divisor=1.0 / self.unit)\n\n    def to_value(self, **kwargs):\n        # Note that 1/unit is always exactly representable, so the\n        # following multiplications are exact.\n        factor = 1.0 / self.unit\n        jd1 = self.jd1 * factor\n        jd2 = self.jd2 * factor\n        return super().to_value(jd1=jd1, jd2=jd2, **kwargs)\n\n    value = property(to_value)\n\n\nclass TimeDeltaSec(TimeDeltaNumeric):\n    \"\"\"Time delta in SI seconds.\"\"\"\n\n    name = \"sec\"\n    unit = 1.0 / erfa.DAYSEC  # for quantity input\n\n\nclass TimeDeltaJD(TimeDeltaNumeric, TimeUnique):\n    \"\"\"Time delta in Julian days (86400 SI seconds).\"\"\"\n\n    name = \"jd\"\n    unit = 1.0\n\n\nclass TimeDeltaDatetime(TimeDeltaFormat, TimeUnique):\n    \"\"\"Time delta in datetime.timedelta.\"\"\"\n\n    name = \"datetime\"\n\n    def _check_val_type(self, val1, val2):\n        if not all(isinstance(val, datetime.timedelta) for val in val1.flat):\n            raise TypeError(\n                f\"Input values for {self.name} class must"}, {"start_line": 24000, "end_line": 26000, "belongs_to": {"file_name": "formats.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " = np.zeros_like(y_frac)\n\n        # Possible enhancement: use np.unique to only compute start, stop\n        # for unique values of iy_start.\n        scale = self.scale.upper().encode(\"ascii\")\n        jd1_start, jd2_start = erfa.dtf2d(scale, iy_start, imon, iday, ihr, imin, isec)\n        jd1_end, jd2_end = erfa.dtf2d(scale, iy_start + 1, imon, iday, ihr, imin, isec)\n\n        t_start = Time(jd1_start, jd2_start, scale=self.scale, format=\"jd\")\n        t_end = Time(jd1_end, jd2_end, scale=self.scale, format=\"jd\")\n        t_frac = t_start + (t_end - t_start) * y_frac\n\n        self.jd1, self.jd2 = day_frac(t_frac.jd1, t_frac.jd2)\n\n    def to_value(self, **kwargs):\n        scale = self.scale.upper().encode(\"ascii\")\n        # precision=0\n        iy_start, ims, ids, ihmsfs = erfa.d2dtf(scale, 0, self.jd1, self.jd2)\n        imon = np.ones_like(iy_start)\n        iday = np.ones_like(iy_start)\n        ihr = np.zeros_like(iy_start)\n        imin = np.zeros_like(iy_start)\n        isec = np.zeros_like(self.jd1)\n\n        # Possible enhancement: use np.unique to only compute start, stop\n        # for unique values of iy_start.\n        scale = self.scale.upper().encode(\"ascii\")\n        jd1_start, jd2_start = erfa.dtf2d(scale, iy_start, imon, iday, ihr, imin, isec)\n        jd1_end, jd2_end = erfa.dtf2d(scale, iy_start + 1, imon, iday, ihr, imin, isec)\n        # Trying to be precise, but more than float64 not useful.\n        dt = (self.jd1 - jd1_start) + (self.jd2 - jd2_start)\n        dt_end = (jd1_end - jd1_start) + (jd2_end - jd2_start)\n        decimalyear = iy_start + dt / dt_end\n\n        return super().to_value(jd1=decimalyear, jd2=np.float64(0.0), **kwargs)\n\n    value = property(to_value)\n\n\nclass TimeFromEpoch(TimeNumeric):\n    \"\"\"\n    Base class for times that represent the interval from a particular\n    epoch as a numerical multiple of a unit time interval (e.g. seconds\n    or days).\n    \"\"\"\n\n    @classproperty(lazy=True)\n    def _epoch(cls):\n        # Ideally we would use `def ep"}, {"start_line": 86000, "end_line": 88000, "belongs_to": {"file_name": "formats.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "in[...],\n                sec[...],\n            ) = self.parse_string(val)\n\n        yrs, days, hrs, mins, secs = iterator.operands[1:]\n\n        jd1 = yrs * 365.25 + days  # Exact in the case that yrs and days are integer\n        jd2 = hrs / 24.0 + mins / 1440.0 + secs / 86400.0  # Inexact\n        self.jd1, self.jd2 = day_frac(jd1, jd2)\n\n    def to_value(self, parent=None, out_subfmt=None):\n        out_subfmt = out_subfmt or self.out_subfmt\n        subfmt = self._get_allowed_subfmt(out_subfmt)\n\n        iterator = np.nditer(\n            [self.jd1, self.jd2, None],\n            flags=[\"refs_ok\", \"zerosize_ok\"],\n            op_dtypes=[None, None, object],\n        )\n\n        for jd1, jd2, out in iterator:\n            jd = jd1 + jd2\n            if jd < 0:\n                jd1, jd2, jd = -jd1, -jd2, -jd  # Flip all signs\n                sign = \"-\"\n            else:\n                sign = \"\"\n\n            if subfmt in [\"*\", \"multi\"]:\n                comps = self.get_multi_comps(jd1, jd2)\n\n            else:\n                value = (jd * u.day).to_value(subfmt)\n                value = np.round(value, self.precision)\n                comps = [f\"{value}{subfmt}\"]\n\n            out[...] = sign + \" \".join(comps)\n\n        return iterator.operands[-1]\n\n    def get_multi_comps(self, jd1, jd2):\n        jd, remainder = two_sum(jd1, jd2)\n        days = int(np.floor(jd))\n        jd -= days\n        jd += remainder\n\n        hours = int(np.floor(jd * 24.0))\n        jd -= hours / 24.0\n        mins = int(np.floor(jd * 1440.0))\n        jd -= mins / 1440.0\n        secs = np.round(jd * 86400.0, self.precision)\n\n        comp_vals = [days, hours, mins, secs]\n        if secs >= 60.0:\n            self.fix_comp_vals_overflow(comp_vals)\n\n        comps = [\n            f\"{comp_val}{name}\"\n            for comp_val, name in zip(comp_vals, (\"d\", \"hr\", \"min\", \"s\"))\n            if comp_val != 0\n        ]\n        if not comps:\n            comps = [\"0.0s\"]\n\n        return comps\n\n    @staticmethod\n    def fix_comp_v"}, {"start_line": 82000, "end_line": 84000, "belongs_to": {"file_name": "formats.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ys)\n    - \"d\": days (24 hours)\n    - \"hr\": hours (60 minutes)\n    - \"min\": minutes (60 seconds)\n    - \"s\": seconds\n\n    .. Note:: These definitions correspond to physical units of time and are NOT\n       calendar date intervals. Thus adding \"1yr\" to \"2000-01-01 00:00:00\" will give\n       \"2000-12-31 06:00:00\" instead of \"2001-01-01 00:00:00\".\n\n    The ``out_subfmt`` attribute specifies the components to be included in the string\n    output.  The default is ``\"multi\"`` which represents the time delta as\n    ``\"<days>d <hours>hr <minutes>min <seconds>s\"``, where only non-zero components are\n    included.\n\n    - \"multi\": multiple components, e.g. \"2d 3hr 15min 5.6s\"\n    - \"yr\": years\n    - \"d\": days\n    - \"hr\": hours\n    - \"min\": minutes\n    - \"s\": seconds\n\n    Examples\n    --------\n    >>> from astropy.time import Time, TimeDelta\n    >>> import astropy.units as u\n\n    >>> print(TimeDelta(\"1yr\"))\n    365d 6hr\n\n    >>> print(Time(\"2000-01-01\") + TimeDelta(\"1yr\"))\n    2000-12-31 06:00:00.000\n    >>> print(TimeDelta(\"+3.6d\"))\n    3d 14hr 24min\n    >>> print(TimeDelta(\"-3.6d\"))\n    -3d 14hr 24min\n    >>> print(TimeDelta(\"1yr 3.6d\", out_subfmt=\"d\"))\n    368.85d\n\n    >>> td = TimeDelta(40 * u.hr)\n    >>> print(td.to_value(format=\"quantity_str\"))\n    1d 16hr\n    >>> print(td.to_value(format=\"quantity_str\", subfmt=\"d\"))\n    1.667d\n    >>> td.precision = 9\n    >>> print(td.to_value(format=\"quantity_str\", subfmt=\"d\"))\n    1.666666667d\n    \"\"\"\n\n    name = \"quantity_str\"\n\n    subfmts = (\n        (\"multi\", None, None),\n        (\"yr\", None, None),\n        (\"d\", None, None),\n        (\"hr\", None, None),\n        (\"min\", None, None),\n        (\"s\", None, None),\n    )\n\n    # Regex to parse \"1.02yr 2.2d 3.12hr 4.322min 5.6s\" where each element is optional\n    # but the order is fixed. Each element is a float with optional exponent. Each\n    # element is named.\n    re_float = r\"(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?\"\n    re_ydhms = re.compile(\n        rf\"\"\"^ \\s*\n        (?P<sign>[-+])? \\s*  # O"}, {"start_line": 22000, "end_line": 24000, "belongs_to": {"file_name": "formats.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " ValueError(\n            f\"cannot use Quantities for {format_name!r} format, as the unit of year \"\n            \"is defined as 365.25 days, while the length of year is variable \"\n            \"in this format. Use float instead.\"\n        )\n\n\nclass TimeDecimalYear(TimeNumeric):\n    \"\"\"\n    Time as a decimal year, with integer values corresponding to midnight of the first\n    day of each year.\n\n    The fractional part represents the exact fraction of the year, considering the\n    precise number of days in the year (365 or 366). The following example shows\n    essentially how the decimal year is computed::\n\n      >>> from astropy.time import Time\n      >>> tm = Time(\"2024-04-05T12:34:00\")\n      >>> tm0 = Time(\"2024-01-01T00:00:00\")\n      >>> tm1 = Time(\"2025-01-01T00:00:00\")\n      >>> print(2024 + (tm.jd - tm0.jd) / (tm1.jd - tm0.jd))  # doctest: +FLOAT_CMP\n      2024.2609934729812\n      >>> print(tm.decimalyear)  # doctest: +FLOAT_CMP\n      2024.2609934729812\n\n    Since for this format the length of the year varies between 365 and 366 days, it is\n    not possible to use Quantity input, in which a year is always 365.25 days.\n\n    This format is convenient for low-precision applications or for plotting data.\n    \"\"\"\n\n    name = \"decimalyear\"\n\n    def _check_val_type(self, val1, val2):\n        _check_val_type_not_quantity(self.name, val1, val2)\n        # if val2 is a Quantity, super() will raise a TypeError.\n        return super()._check_val_type(val1, val2)\n\n    def set_jds(self, val1, val2):\n        self._check_scale(self._scale)  # Validate scale.\n\n        sum12, err12 = two_sum(val1, val2)\n        iy_start = np.trunc(sum12).astype(int)\n        extra, y_frac = two_sum(sum12, -iy_start)\n        y_frac += extra + err12\n\n        val = (val1 + val2).astype(np.double)\n        iy_start = np.trunc(val).astype(int)\n\n        imon = np.ones_like(iy_start)\n        iday = np.ones_like(iy_start)\n        ihr = np.zeros_like(iy_start)\n        imin = np.zeros_like(iy_start)\n        isec"}], "retrieved_count": 10, "cost_time": 1.0946681499481201}
{"question": "How does the converter class for single-precision complex VOTable datatypes use its numpy dtype format string attribute to convert complex values to bytes for binary format serialization?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 32000, "end_line": 34000, "belongs_to": {"file_name": "converters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "(float(value.real))\n        imag = self._output_format.format(float(value.imag))\n        if self._output_format[2] == \"s\":\n            real = real.removesuffix(\".0\")\n            imag = imag.removesuffix(\".0\")\n        return real + \" \" + imag\n\n\nclass FloatComplex(Complex):\n    \"\"\"\n    Handle floatComplex datatype.  Pair of single-precision IEEE\n    floating-point numbers.\n    \"\"\"\n\n    format = \"c8\"\n\n\nclass DoubleComplex(Complex):\n    \"\"\"\n    Handle doubleComplex datatype.  Pair of double-precision IEEE\n    floating-point numbers.\n    \"\"\"\n\n    format = \"c16\"\n\n\nclass BitArray(NumericArray):\n    \"\"\"\n    Handles an array of bits.\n    \"\"\"\n\n    vararray_type = ArrayVarArray\n\n    def __init__(self, field, base, arraysize, config=None, pos=None):\n        NumericArray.__init__(self, field, base, arraysize, config, pos)\n\n        self._bytes = ((self._items - 1) // 8) + 1\n\n    @staticmethod\n    def _splitter_pedantic(value, config=None, pos=None):\n        return list(re.sub(r\"\\s\", \"\", value))\n\n    @staticmethod\n    def _splitter_lax(value, config=None, pos=None):\n        if \",\" in value:\n            vo_warn(W01, (), config, pos)\n        return list(re.sub(r\"\\s|,\", \"\", value))\n\n    def output(self, value, mask):\n        if np.any(mask):\n            vo_warn(W39)\n        value = np.asarray(value)\n        mapping = {False: \"0\", True: \"1\"}\n        return \"\".join(mapping[x] for x in value.flat)\n\n    def binparse(self, read):\n        data = read(self._bytes)\n        result = bitarray_to_bool(data, self._items)\n        result = result.reshape(self._arraysize)\n        result_mask = np.zeros(self._arraysize, dtype=\"b1\")\n        return result, result_mask\n\n    def binoutput(self, value, mask):\n        if np.any(mask):\n            vo_warn(W39)\n\n        return bool_to_bitarray(value)\n\n\nclass Bit(Converter):\n    \"\"\"\n    Handles the bit datatype.\n    \"\"\"\n\n    format = \"b1\"\n    array_type = BitArray\n    vararray_type = ScalarVarArray\n    default = False\n    binary_one = b\"\\x08\"\n    binary_zero"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "converters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "= []\n    for v in value:\n        if v:\n            byte |= 1 << bit_no\n        if bit_no == 0:\n            bytes.append(byte)\n            bit_no = 7\n            byte = 0\n        else:\n            bit_no -= 1\n    if bit_no != 7:\n        bytes.append(byte)\n\n    return struct.pack(f\"{len(bytes)}B\", *bytes)\n\n\nclass Converter:\n    \"\"\"\n    The base class for all converters.  Each subclass handles\n    converting a specific VOTABLE data type to/from the TABLEDATA_ and\n    BINARY_ on-disk representations.\n\n    Parameters\n    ----------\n    field : `~astropy.io.votable.tree.Field`\n        object describing the datatype\n\n    config : dict\n        The parser configuration dictionary\n\n    pos : tuple\n        The position in the XML file where the FIELD object was\n        found.  Used for error messages.\n\n    \"\"\"\n\n    def __init__(self, field, config=None, pos=None):\n        pass\n\n    @staticmethod\n    def _parse_length(read):\n        return struct.unpack(\">I\", read(4))[0]\n\n    @staticmethod\n    def _write_length(length):\n        return struct.pack(\">I\", int(length))\n\n    def supports_empty_values(self, config):\n        \"\"\"\n        Returns True when the field can be completely empty.\n        \"\"\"\n        return config.get(\"version_1_3_or_later\")\n\n    def parse(self, value, config=None, pos=None):\n        \"\"\"\n        Convert the string *value* from the TABLEDATA_ format into an\n        object with the correct native in-memory datatype and mask flag.\n\n        Parameters\n        ----------\n        value : str\n            value in TABLEDATA format\n\n        Returns\n        -------\n        native : tuple\n            A two-element tuple of: value, mask.\n            The value as a Numpy array or scalar, and *mask* is True\n            if the value is missing.\n        \"\"\"\n        raise NotImplementedError(\"This datatype must implement a 'parse' method.\")\n\n    def parse_scalar(self, value, config=None, pos=None):\n        \"\"\"\n        Parse a single scalar of the underlying type of the convert"}, {"start_line": 37000, "end_line": 39000, "belongs_to": {"file_name": "converters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ue, False),\n        ord(\"t\"): (True, False),\n        ord(\"1\"): (True, False),\n        ord(\"F\"): (False, False),\n        ord(\"f\"): (False, False),\n        ord(\"0\"): (False, False),\n        ord(\"\\0\"): (False, True),\n        ord(\" \"): (False, True),\n        ord(\"?\"): (False, True),\n    }\n\n    def binparse_value(self, value):\n        try:\n            return self._binparse_mapping[value]\n        except KeyError:\n            vo_raise(E05, (value,))\n\n    def binoutput(self, value, mask):\n        if mask:\n            return self.binary_question_mark\n        if value:\n            return self.binary_true\n        return self.binary_false\n\n\nconverter_mapping = {\n    \"double\": Double,\n    \"float\": Float,\n    \"bit\": Bit,\n    \"boolean\": Boolean,\n    \"unsignedByte\": UnsignedByte,\n    \"short\": Short,\n    \"int\": Int,\n    \"long\": Long,\n    \"floatComplex\": FloatComplex,\n    \"doubleComplex\": DoubleComplex,\n    \"char\": Char,\n    \"unicodeChar\": UnicodeChar,\n}\n\n\ndef get_converter(field, config=None, pos=None):\n    \"\"\"\n    Get an appropriate converter instance for a given field.\n\n    Parameters\n    ----------\n    field : astropy.io.votable.tree.Field\n\n    config : dict, optional\n        Parser configuration dictionary\n\n    pos : tuple\n        Position in the input XML file.  Used for error messages.\n\n    Returns\n    -------\n    converter : astropy.io.votable.converters.Converter\n    \"\"\"\n    if config is None:\n        config = {}\n\n    if field.datatype not in converter_mapping:\n        vo_raise(E06, (field.datatype, field.ID), config)\n\n    cls = converter_mapping[field.datatype]\n    converter = cls(field, config, pos)\n\n    arraysize = field.arraysize\n\n    # With numeric datatypes, special things need to happen for\n    # arrays.\n    if field.datatype not in (\"char\", \"unicodeChar\") and arraysize is not None:\n        if arraysize[-1] == \"*\":\n            arraysize = arraysize[:-1]\n            last_x = arraysize.rfind(\"x\")\n            if last_x == -1:\n                arraysize = \"\"\n            el"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "converters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " The number of bits to read.  The least significant bits in the\n        data bytes beyond length will be ignored.\n\n    Returns\n    -------\n    array : numpy bool array\n    \"\"\"\n    results = []\n    for byte in data:\n        for bit_no in range(7, -1, -1):\n            bit = byte & (1 << bit_no)\n            bit = bit != 0\n            results.append(bit)\n            if len(results) == length:\n                break\n        if len(results) == length:\n            break\n\n    return np.array(results, dtype=\"b1\")\n\n\ndef bool_to_bitarray(value):\n    \"\"\"\n    Converts a numpy boolean array to a bit array (a string of bits in\n    a bytes object).\n\n    Parameters\n    ----------\n    value : numpy bool array\n\n    Returns\n    -------\n    bit_array : bytes\n        The first value in the input array will be the most\n        significant bit in the result.  The length will be `floor((N +\n        7) / 8)` where `N` is the length of `value`.\n    \"\"\"\n    value = value.flat\n    bit_no = 7\n    byte = 0\n    bytes = []\n    for v in value:\n        if v:\n            byte |= 1 << bit_no\n        if bit_no == 0:\n            bytes.append(byte)\n            bit_no = 7\n            byte = 0\n        else:\n            bit_no -= 1\n    if bit_no != 7:\n        bytes.append(byte)\n\n    return struct.pack(f\"{len(bytes)}B\", *bytes)\n\n\nclass Converter:\n    \"\"\"\n    The base class for all converters.  Each subclass handles\n    converting a specific VOTABLE data type to/from the TABLEDATA_ and\n    BINARY_ on-disk representations.\n\n    Parameters\n    ----------\n    field : `~astropy.io.votable.tree.Field`\n        object describing the datatype\n\n    config : dict\n        The parser configuration dictionary\n\n    pos : tuple\n        The position in the XML file where the FIELD object was\n        found.  Used for error messages.\n\n    \"\"\"\n\n    def __init__(self, field, config=None, pos=None):\n        pass\n\n    @staticmethod\n    def _parse_length(read):\n        return struct.unpack(\">I\", read(4))[0]\n\n    @staticmethod\n    def"}, {"start_line": 39000, "end_line": 41000, "belongs_to": {"file_name": "converters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "se:\n                arraysize = arraysize[:last_x]\n            fixed = False\n        else:\n            fixed = True\n\n        if arraysize != \"\":\n            arraysize = [int(x) for x in arraysize.split(\"x\")]\n            arraysize.reverse()\n        else:\n            arraysize = []\n\n        if arraysize != []:\n            converter = converter.array_type(field, converter, arraysize, config)\n\n        if not fixed:\n            converter = converter.vararray_type(field, converter, arraysize, config)\n\n    return converter\n\n\nnumpy_dtype_to_field_mapping = {\n    np.float64().dtype.num: \"double\",\n    np.float32().dtype.num: \"float\",\n    np.bool_().dtype.num: \"bit\",\n    np.uint8().dtype.num: \"unsignedByte\",\n    np.int16().dtype.num: \"short\",\n    np.int32().dtype.num: \"int\",\n    np.int64().dtype.num: \"long\",\n    np.complex64().dtype.num: \"floatComplex\",\n    np.complex128().dtype.num: \"doubleComplex\",\n    np.str_().dtype.num: \"unicodeChar\",\n    np.bytes_().dtype.num: \"char\",\n}\n\n\ndef _all_matching_dtype(column):\n    first_dtype = False\n    first_shape = ()\n    for x in column:\n        if not isinstance(x, np.ndarray) or len(x) == 0:\n            continue\n\n        if first_dtype is False:\n            first_dtype = x.dtype\n            first_shape = x.shape[1:]\n        elif first_dtype != x.dtype:\n            return False, ()\n        elif first_shape != x.shape[1:]:\n            first_shape = ()\n    return first_dtype, first_shape\n\n\ndef numpy_to_votable_dtype(dtype, shape):\n    \"\"\"\n    Converts a numpy dtype and shape to a dictionary of attributes for\n    a VOTable FIELD element and correspond to that type.\n\n    Parameters\n    ----------\n    dtype : Numpy dtype instance\n\n    shape : tuple\n\n    Returns\n    -------\n    attributes : dict\n        A dict containing 'datatype' and 'arraysize' keys that can be\n        set on a VOTable FIELD element.\n    \"\"\"\n    if dtype.num not in numpy_dtype_to_field_mapping:\n        raise TypeError(f\"{dtype!r} can not be represented in VOTable\")\n\n    if d"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "converters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      format.\n\n        Parameters\n        ----------\n        value\n            The value, the native type corresponding to this converter\n\n        mask : bool\n            If `True`, will return the string representation of a\n            masked value.\n\n        Returns\n        -------\n        bytes : bytes\n            The binary representation of the value, suitable for\n            serialization in the BINARY_ format.\n        \"\"\"\n        raise NotImplementedError(\"This datatype must implement a 'binoutput' method.\")\n\n\nclass Char(Converter):\n    \"\"\"\n    Handles the char datatype. (7-bit unsigned characters).\n\n    Missing values are not handled for string or unicode types.\n    \"\"\"\n\n    default = _empty_bytes\n\n    def __init__(self, field, config=None, pos=None):\n        if config is None:\n            config = {}\n\n        Converter.__init__(self, field, config, pos)\n\n        self.field_name = field.name\n\n        if field.arraysize is None:\n            vo_warn(W47, (), config, pos)\n            field.arraysize = \"1\"\n\n        if field.arraysize == \"*\":\n            self.format = \"O\"\n            self.binparse = self._binparse_var\n            self.binoutput = self._binoutput_var\n            self.arraysize = \"*\"\n        else:\n            # Check if this is a bounded variable-length field\n            is_variable = field.arraysize.endswith(\"*\")\n            numeric_part = field.arraysize.removesuffix(\"*\")\n            try:\n                self.arraysize = int(numeric_part)\n            except ValueError:\n                vo_raise(E01, (numeric_part, \"char\", field.ID), config)\n\n            self.format = f\"U{self.arraysize:d}\"\n\n            # For bounded variable-length fields use the variable methods\n            if is_variable:\n                self.binparse = self._binparse_var\n                self.binoutput = self._binoutput_var\n            else:\n                self.binparse = self._binparse_fixed\n                self.binoutput = self._binoutput_fixed\n\n            self._struct_format"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "converters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"\nThis module handles the conversion of various VOTABLE datatypes\nto/from TABLEDATA_ and BINARY_ formats.\n\"\"\"\n\n# STDLIB\nimport re\nimport struct\nimport sys\nfrom math import prod\n\n# THIRD-PARTY\nimport numpy as np\nfrom numpy import ma\n\n# ASTROPY\nfrom astropy.utils.xml.writer import xml_escape_cdata\n\n# LOCAL\nfrom .exceptions import (\n    E01,\n    E02,\n    E03,\n    E04,\n    E05,\n    E06,\n    E24,\n    W01,\n    W30,\n    W31,\n    W39,\n    W46,\n    W47,\n    W49,\n    W51,\n    W55,\n    vo_raise,\n    vo_warn,\n    warn_or_raise,\n)\n\n__all__ = [\"Converter\", \"get_converter\", \"table_column_to_votable_datatype\"]\n\n\npedantic_array_splitter = re.compile(r\" +\")\narray_splitter = re.compile(r\"\\s+|(?:\\s*,\\s*)\")\n\"\"\"\nA regex to handle splitting values on either whitespace or commas.\n\nSPEC: Usage of commas is not actually allowed by the spec, but many\nfiles in the wild use them.\n\"\"\"\n\n_zero_int = b\"\\0\\0\\0\\0\"\n_empty_bytes = b\"\"\n_zero_byte = b\"\\0\"\n\n\nif sys.byteorder == \"little\":\n\n    def _ensure_bigendian(x):\n        if x.dtype.byteorder != \">\":\n            return x.byteswap()\n        return x\n\nelse:\n\n    def _ensure_bigendian(x):\n        if x.dtype.byteorder == \"<\":\n            return x.byteswap()\n        return x\n\n\ndef _make_masked_array(data, mask):\n    \"\"\"\n    Masked arrays of zero length that also have a mask of zero length\n    cause problems in Numpy (at least in 1.6.2).  This function\n    creates a masked array from data and a mask, unless it is zero\n    length.\n    \"\"\"\n    # np.ma doesn't like setting mask to []\n    if len(data):\n        return ma.array(np.array(data), mask=np.array(mask, dtype=\"bool\"))\n    else:\n        return ma.array(np.array(data))\n\n\ndef bitarray_to_bool(data, length):\n    \"\"\"\n    Converts a bit array (a string of bits in a bytes object) to a\n    boolean Numpy array.\n\n    Parameters\n    ----------\n    data : bytes\n        The bit array.  The most significant byte is read first.\n\n    length : int\n       "}, {"start_line": 53000, "end_line": 55000, "belongs_to": {"file_name": "column.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ol\"):\n                    return np.where(array == np.False_, ord(\"F\"), ord(\"T\"))\n                else:\n                    return np.where(array == 0, ord(\"F\"), ord(\"T\"))\n            elif \"X\" in format:\n                return _convert_array(array, np.dtype(\"uint8\"))\n            else:\n                # Preserve byte order of the original array for now; see #77\n                numpy_format = array.dtype.byteorder + format.recformat\n\n                # Handle arrays passed in as unsigned ints as pseudo-unsigned\n                # int arrays; blatantly tacked in here for now--we need columns\n                # to have explicit knowledge of whether they treated as\n                # pseudo-unsigned\n                bzeros = {\n                    2: np.uint16(2**15),\n                    4: np.uint32(2**31),\n                    8: np.uint64(2**63),\n                }\n                if (\n                    array.dtype.kind == \"u\"\n                    and array.dtype.itemsize in bzeros\n                    and self.bscale in (1, None, \"\")\n                    and self.bzero == bzeros[array.dtype.itemsize]\n                ):\n                    # Basically the array is uint, has scale == 1.0, and the\n                    # bzero is the appropriate value for a pseudo-unsigned\n                    # integer of the input dtype, then go ahead and assume that\n                    # uint is assumed\n                    numpy_format = numpy_format.replace(\"i\", \"u\")\n                    self._pseudo_unsigned_ints = True\n\n                # The .base here means we're dropping the shape information,\n                # which is only used to format recarray fields, and is not\n                # useful for converting input arrays to the correct data type\n                dtype = np.dtype(numpy_format).base\n\n                return _convert_array(array, dtype)\n\n\nclass ColDefs(NotifierMixin):\n    \"\"\"\n    Column definitions class.\n\n    It has attributes corresponding to the `Column` attributes\n    (e.g. `Co"}, {"start_line": 52000, "end_line": 54000, "belongs_to": {"file_name": "column.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "               shape = dims[:-1] if \"A\" in format else dims\n                shape = (len(array),) + shape\n                array = array.reshape(shape)\n\n            if \"P\" in format or \"Q\" in format:\n                return array\n            elif \"A\" in format:\n                if array.dtype.char in \"SU\":\n                    if dims:\n                        # The 'last' dimension (first in the order given\n                        # in the TDIMn keyword itself) is the number of\n                        # characters in each string\n                        fsize = dims[-1]\n                    else:\n                        fsize = np.dtype(format.recformat).itemsize\n                    return chararray.array(array, itemsize=fsize, copy=False)\n                else:\n                    return _convert_array(array, np.dtype(format.recformat))\n            elif \"L\" in format:\n                # boolean needs to be scaled back to storage values ('T', 'F')\n                if array.dtype == np.dtype(\"bool\"):\n                    return np.where(array == np.False_, ord(\"F\"), ord(\"T\"))\n                else:\n                    return np.where(array == 0, ord(\"F\"), ord(\"T\"))\n            elif \"X\" in format:\n                return _convert_array(array, np.dtype(\"uint8\"))\n            else:\n                # Preserve byte order of the original array for now; see #77\n                numpy_format = array.dtype.byteorder + format.recformat\n\n                # Handle arrays passed in as unsigned ints as pseudo-unsigned\n                # int arrays; blatantly tacked in here for now--we need columns\n                # to have explicit knowledge of whether they treated as\n                # pseudo-unsigned\n                bzeros = {\n                    2: np.uint16(2**15),\n                    4: np.uint32(2**31),\n                    8: np.uint64(2**63),\n                }\n                if (\n                    array.dtype.kind == \"u\"\n                    and array.dtype.itemsize in bzeros\n            "}, {"start_line": 19000, "end_line": 21000, "belongs_to": {"file_name": "converters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  else:\n            if len(parts) == self._items:\n                pass\n            elif len(parts) > self._items:\n                parts = parts[: self._items]\n            else:\n                parts = parts + ([self._base.default] * (self._items - len(parts)))\n            return self.parse_parts(parts, config, pos)\n\n    def parse_parts(self, parts, config=None, pos=None):\n        base_parse = self._base.parse\n        result = []\n        result_mask = []\n        for x in parts:\n            value, mask = base_parse(x, config, pos)\n            result.append(value)\n            result_mask.append(mask)\n        result = np.array(result, dtype=self._base.format).reshape(self._arraysize)\n        result_mask = np.array(result_mask, dtype=\"bool\").reshape(self._arraysize)\n        return result, result_mask\n\n    def output(self, value, mask):\n        base_output = self._base.output\n        value = np.asarray(value)\n        mask = np.asarray(mask)\n        if mask.size <= 1:\n            func = np.broadcast\n        else:  # When mask is already array but value is scalar, this prevents broadcast\n            func = zip\n        return \" \".join(base_output(x, m) for x, m in func(value.flat, mask.flat))\n\n    def binparse(self, read):\n        result = np.frombuffer(read(self._memsize), dtype=self._bigendian_format)[0]\n        result_mask = self._base.is_null(result)\n        return result, result_mask\n\n    def binoutput(self, value, mask):\n        filtered = self._base.filter_array(value, mask)\n        filtered = _ensure_bigendian(filtered)\n        return filtered.tobytes()\n\n\nclass Numeric(Converter):\n    \"\"\"\n    The base class for all numeric data types.\n    \"\"\"\n\n    array_type = NumericArray\n    vararray_type = ScalarVarArray\n    null = None\n\n    def __init__(self, field, config=None, pos=None):\n        Converter.__init__(self, field, config, pos)\n\n        self._memsize = np.dtype(self.format).itemsize\n        self._bigendian_format = \">\" + self.format\n        if field.values.null is n"}], "retrieved_count": 10, "cost_time": 1.1275010108947754}
{"question": "How does the mixin class that adds input/output methods to N-dimensional data containers ensure extensibility of the centralized file format registry without breaking existing subclasses of the base data container when new format handlers are registered?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "ndio.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/nddata/mixins", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n# This module implements the I/O mixin to the NDData class.\n\n\nfrom astropy.io import registry\n\n__all__ = [\"NDIOMixin\"]\n__doctest_skip__ = [\"NDDataRead\", \"NDDataWrite\"]\n\n\nclass NDDataRead(registry.UnifiedReadWrite):\n    \"\"\"Read and parse gridded N-dimensional data and return as an NDData-derived\n    object.\n\n    This function provides the NDDataBase interface to the astropy unified I/O\n    layer.  This allows easily reading a file in the supported data formats,\n    for example::\n\n      >>> from astropy.nddata import CCDData\n      >>> dat = CCDData.read('image.fits')\n\n    Get help on the available readers for ``CCDData`` using the``help()`` method::\n\n      >>> CCDData.read.help()  # Get help reading CCDData and list supported formats\n      >>> CCDData.read.help('fits')  # Get detailed help on CCDData FITS reader\n      >>> CCDData.read.list_formats()  # Print list of available formats\n\n    For more information see:\n\n    - https://docs.astropy.org/en/stable/nddata\n    - https://docs.astropy.org/en/stable/io/unified.html\n\n    Parameters\n    ----------\n    *args : tuple, optional\n        Positional arguments passed through to data reader. If supplied the\n        first argument is the input filename.\n    format : str, optional\n        File format specifier.\n    cache : bool, optional\n        Caching behavior if file is a URL.\n    **kwargs : dict, optional\n        Keyword arguments passed through to data reader.\n\n    Returns\n    -------\n    out : `NDData` subclass\n        NDData-basd object corresponding to file contents\n\n    Notes\n    -----\n    \"\"\"\n\n    def __init__(self, instance, cls):\n        super().__init__(instance, cls, \"read\", registry=None)\n        # uses default global registry\n\n    def __call__(self, *args, **kwargs):\n        return self.registry.read(self._cls, *args, **kwargs)\n\n\nclass NDDataWrite(registry.UnifiedReadWrite):\n    \"\"\"Write this CCDData object out in the specified format.\n\n    This fu"}, {"start_line": 2000, "end_line": 3689, "belongs_to": {"file_name": "ndio.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/nddata/mixins", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "nction provides the NDData interface to the astropy unified I/O\n    layer.  This allows easily writing a file in many supported data formats\n    using syntax such as::\n\n      >>> from astropy.nddata import CCDData\n      >>> dat = CCDData(np.zeros((12, 12)), unit='adu')  # 12x12 image of zeros\n      >>> dat.write('zeros.fits')\n\n    Get help on the available writers for ``CCDData`` using the``help()`` method::\n\n      >>> CCDData.write.help()  # Get help writing CCDData and list supported formats\n      >>> CCDData.write.help('fits')  # Get detailed help on CCDData FITS writer\n      >>> CCDData.write.list_formats()  # Print list of available formats\n\n    For more information see:\n\n    - https://docs.astropy.org/en/stable/nddata\n    - https://docs.astropy.org/en/stable/io/unified.html\n\n    Parameters\n    ----------\n    *args : tuple, optional\n        Positional arguments passed through to data writer. If supplied the\n        first argument is the output filename.\n    format : str, optional\n        File format specifier.\n    **kwargs : dict, optional\n        Keyword arguments passed through to data writer.\n\n    Notes\n    -----\n    \"\"\"\n\n    def __init__(self, instance, cls):\n        super().__init__(instance, cls, \"write\", registry=None)\n        # uses default global registry\n\n    def __call__(self, *args, **kwargs):\n        self.registry.write(self._instance, *args, **kwargs)\n\n\nclass NDIOMixin:\n    \"\"\"\n    Mixin class to connect NDData to the astropy input/output registry.\n\n    This mixin adds two methods to its subclasses, ``read`` and ``write``.\n    \"\"\"\n\n    read = registry.UnifiedReadWriteMethod(NDDataRead)\n    write = registry.UnifiedReadWriteMethod(NDDataWrite)\n"}, {"start_line": 0, "end_line": 379, "belongs_to": {"file_name": "__init__.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/registry", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"\nUnified I/O Registry.\n\"\"\"\n\nfrom . import base, compat, core, interface\nfrom .base import *\nfrom .compat import *\nfrom .compat import _identifiers, _readers, _writers  # for backwards compat\nfrom .core import *\nfrom .interface import *\n\n__all__ = core.__all__ + interface.__all__ + compat.__all__ + base.__all__\n"}, {"start_line": 0, "end_line": 1896, "belongs_to": {"file_name": "compat.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/registry", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\nimport functools\n\nfrom .core import UnifiedIORegistry\n\n__all__ = [  # noqa: F822\n    \"delay_doc_updates\",\n    \"get_formats\",\n    \"get_reader\",\n    \"get_writer\",\n    \"identify_format\",\n    \"read\",\n    \"register_identifier\",\n    \"register_reader\",\n    \"register_writer\",\n    \"unregister_identifier\",\n    \"unregister_reader\",\n    \"unregister_writer\",\n    \"write\",\n]\n\n# make a default global-state registry  (not publicly scoped, but often accessed)\n# this is for backward compatibility when ``io.registry`` was a file.\ndefault_registry = UnifiedIORegistry()\n# also need to expose the enclosed registries\n_identifiers = default_registry._identifiers\n_readers = default_registry._readers\n_writers = default_registry._writers\n\n\ndef _make_io_func(method_name):\n    \"\"\"Makes a function for a method on UnifiedIORegistry.\n\n    .. todo::\n\n        Make kwarg \"registry\" not hidden.\n\n    Returns\n    -------\n    wrapper : callable\n        Signature matches method on UnifiedIORegistry.\n        Accepts (hidden) kwarg \"registry\". default is ``default_registry``.\n    \"\"\"\n\n    @functools.wraps(getattr(default_registry, method_name))\n    def wrapper(*args, registry=None, **kwargs):\n        # written this way in case ever controlled by ScienceState\n        if registry is None:\n            registry = default_registry\n        # get and call bound method from registry instance\n        return getattr(registry, method_name)(*args, **kwargs)\n\n    return wrapper\n\n\n# =============================================================================\n# JIT function creation and lookup (PEP 562)\n\n\ndef __dir__():\n    dir_out = list(globals())\n    return sorted(dir_out + __all__)\n\n\ndef __getattr__(method: str):\n    if method in __all__:\n        return _make_io_func(method)\n\n    raise AttributeError(f\"module {__name__!r} has no attribute {method!r}\")\n"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/registry", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "         except OSError:\n                            raise\n                        except Exception:\n                            fileobj = None\n                        else:\n                            args = [fileobj] + list(args[1:])\n                    elif hasattr(args[0], \"read\"):\n                        path = None\n                        fileobj = args[0]\n\n                format = self._get_valid_format(\n                    \"read\", cls, path, fileobj, args, kwargs\n                )\n\n                # We need to keep track the original path in case it uses a\n                # relative path to the parquet binary\n                if format == \"votable\":\n                    kwargs.update({\"filename\": path})\n\n            reader = self.get_reader(format, cls)\n            data = reader(*args, **kwargs)\n\n            if not isinstance(data, cls):\n                # User has read with a subclass where only the parent class is\n                # registered.  This returns the parent class, so try coercing\n                # to desired subclass.\n                try:\n                    data = cls(data, copy=False)\n                except Exception:\n                    raise TypeError(\n                        f\"could not convert reader output to {cls.__name__} class.\"\n                    )\n        finally:\n            if ctx is not None:\n                ctx.__exit__(*sys.exc_info())\n\n        return data\n\n\n# -----------------------------------------------------------------------------\n\n\nclass UnifiedOutputRegistry(_UnifiedIORegistryBase):\n    \"\"\"Write-only Registry.\n\n    .. versionadded:: 5.0\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._writers = {}\n        self._registries[\"write\"] = {\"attr\": \"_writers\", \"column\": \"Write\"}\n        self._registries_order = (\"write\", \"identify\")\n\n    # =========================================================================\n    # Write Methods\n\n    def register_writer(\n        self, data_format, data_class, function,"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_registries.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/registry/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "read and write.\n    Note that the read/write are the compatibility methods, which allow for the\n    kwarg ``registry``. This allows us to not subclass ``EmptyData`` for each\n    of the types of registry (read-only, ...) and use this class everywhere.\n    \"\"\"\n\n    read = classmethod(io_registry.read)\n    write = io_registry.write\n\n\nclass OtherEmptyData:\n    \"\"\"A different class with different I/O\"\"\"\n\n    read = classmethod(io_registry.read)\n    write = io_registry.write\n\n\ndef empty_reader(*args, **kwargs):\n    return EmptyData()\n\n\ndef empty_writer(table, *args, **kwargs):\n    return \"status: success\"\n\n\ndef empty_identifier(*args, **kwargs):\n    return True\n\n\n@pytest.fixture\ndef fmtcls1():\n    return (\"test1\", EmptyData)\n\n\n@pytest.fixture\ndef fmtcls2():\n    return (\"test2\", EmptyData)\n\n\n@pytest.fixture(params=[\"test1\", \"test2\"])\ndef fmtcls(request):\n    return (request.param, EmptyData)\n\n\n@pytest.fixture\ndef original():\n    ORIGINAL = {}\n    ORIGINAL[\"readers\"] = deepcopy(default_registry._readers)\n    ORIGINAL[\"writers\"] = deepcopy(default_registry._writers)\n    ORIGINAL[\"identifiers\"] = deepcopy(default_registry._identifiers)\n    return ORIGINAL\n\n\n###############################################################################\n\n\ndef test_fmcls1_fmtcls2(fmtcls1, fmtcls2):\n    \"\"\"Just check a fact that we rely on in other tests.\"\"\"\n    assert fmtcls1[1] is fmtcls2[1]\n\n\ndef test_IORegistryError():\n    with pytest.raises(IORegistryError, match=\"just checking\"):\n        raise IORegistryError(\"just checking\")\n\n\nclass TestUnifiedIORegistryBase:\n    \"\"\"Test :class:`astropy.io.registry.UnifiedIORegistryBase`.\"\"\"\n\n    def setup_class(self):\n        \"\"\"Setup class. This is called 1st by pytest.\"\"\"\n        self._cls = UnifiedIORegistryBaseSubClass\n\n    @pytest.fixture\n    def registry(self):\n        \"\"\"I/O registry. Cleaned before and after each function.\"\"\"\n        registry = self._cls()\n\n        HAS_READERS = hasattr(registry, \"_readers\")\n        HAS_WRITERS = hasattr(registry"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/registry", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\nimport contextlib\nimport re\nimport warnings\nfrom operator import itemgetter\n\nimport numpy as np\n\n__all__ = [\"IORegistryError\"]\n\n\nclass IORegistryError(Exception):\n    \"\"\"Custom error for registry clashes.\"\"\"\n\n\n# -----------------------------------------------------------------------------\n\n\nclass _UnifiedIORegistryBase:\n    \"\"\"Base class for registries in Astropy's Unified IO.\n\n    This base class provides identification functions and miscellaneous\n    utilities. For an example how to build a registry subclass we suggest\n    :class:`~astropy.io.registry.UnifiedInputRegistry`, which enables\n    read-only registries. These higher-level subclasses will probably serve\n    better as a baseclass, for instance\n    :class:`~astropy.io.registry.UnifiedIORegistry` subclasses both\n    :class:`~astropy.io.registry.UnifiedInputRegistry` and\n    :class:`~astropy.io.registry.UnifiedOutputRegistry` to enable both\n    reading from and writing to files.\n\n    .. versionadded:: 5.0\n\n    \"\"\"\n\n    def __init__(self):\n        # registry of identifier functions\n        self._identifiers = {}\n\n        # what this class can do: e.g. 'read' &/or 'write'\n        self._registries = {}\n        self._registries[\"identify\"] = {\n            \"attr\": \"_identifiers\",\n            \"column\": \"Auto-identify\",\n        }\n        self._registries_order = (\"identify\",)  # match keys in `_registries`\n\n        # If multiple formats are added to one class the update of the docs is quite\n        # expensive. Classes for which the doc update is temporarily delayed are added\n        # to this set.\n        self._delayed_docs_classes = set()\n\n    @property\n    def available_registries(self):\n        \"\"\"Available registries.\n\n        Returns\n        -------\n        ``dict_keys``\n        \"\"\"\n        return self._registries.keys()\n\n    def get_formats(self, data_class=None, filter_on=None):\n        \"\"\"\n        Get the list of registered formats as a `~astr"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/registry", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ting to files.\n\n    .. versionadded:: 5.0\n\n    \"\"\"\n\n    def __init__(self):\n        # registry of identifier functions\n        self._identifiers = {}\n\n        # what this class can do: e.g. 'read' &/or 'write'\n        self._registries = {}\n        self._registries[\"identify\"] = {\n            \"attr\": \"_identifiers\",\n            \"column\": \"Auto-identify\",\n        }\n        self._registries_order = (\"identify\",)  # match keys in `_registries`\n\n        # If multiple formats are added to one class the update of the docs is quite\n        # expensive. Classes for which the doc update is temporarily delayed are added\n        # to this set.\n        self._delayed_docs_classes = set()\n\n    @property\n    def available_registries(self):\n        \"\"\"Available registries.\n\n        Returns\n        -------\n        ``dict_keys``\n        \"\"\"\n        return self._registries.keys()\n\n    def get_formats(self, data_class=None, filter_on=None):\n        \"\"\"\n        Get the list of registered formats as a `~astropy.table.Table`.\n\n        Parameters\n        ----------\n        data_class : class or None, optional\n            Filter readers/writer to match data class (default = all classes).\n        filter_on : str or None, optional\n            Which registry to show. E.g. \"identify\"\n            If None search for both.  Default is None.\n\n        Returns\n        -------\n        format_table : :class:`~astropy.table.Table`\n            Table of available I/O formats.\n\n        Raises\n        ------\n        ValueError\n            If ``filter_on`` is not None nor a registry name.\n        \"\"\"\n        from astropy.table import Table\n\n        # set up the column names\n        colnames = (\n            \"Data class\",\n            \"Format\",\n            *[self._registries[k][\"column\"] for k in self._registries_order],\n            \"Deprecated\",\n        )\n        i_dataclass = colnames.index(\"Data class\")\n        i_format = colnames.index(\"Format\")\n        i_regstart = colnames.index(\n            self._registri"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "ndio.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/nddata/mixins", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ps://docs.astropy.org/en/stable/nddata\n    - https://docs.astropy.org/en/stable/io/unified.html\n\n    Parameters\n    ----------\n    *args : tuple, optional\n        Positional arguments passed through to data reader. If supplied the\n        first argument is the input filename.\n    format : str, optional\n        File format specifier.\n    cache : bool, optional\n        Caching behavior if file is a URL.\n    **kwargs : dict, optional\n        Keyword arguments passed through to data reader.\n\n    Returns\n    -------\n    out : `NDData` subclass\n        NDData-basd object corresponding to file contents\n\n    Notes\n    -----\n    \"\"\"\n\n    def __init__(self, instance, cls):\n        super().__init__(instance, cls, \"read\", registry=None)\n        # uses default global registry\n\n    def __call__(self, *args, **kwargs):\n        return self.registry.read(self._cls, *args, **kwargs)\n\n\nclass NDDataWrite(registry.UnifiedReadWrite):\n    \"\"\"Write this CCDData object out in the specified format.\n\n    This function provides the NDData interface to the astropy unified I/O\n    layer.  This allows easily writing a file in many supported data formats\n    using syntax such as::\n\n      >>> from astropy.nddata import CCDData\n      >>> dat = CCDData(np.zeros((12, 12)), unit='adu')  # 12x12 image of zeros\n      >>> dat.write('zeros.fits')\n\n    Get help on the available writers for ``CCDData`` using the``help()`` method::\n\n      >>> CCDData.write.help()  # Get help writing CCDData and list supported formats\n      >>> CCDData.write.help('fits')  # Get detailed help on CCDData FITS writer\n      >>> CCDData.write.list_formats()  # Print list of available formats\n\n    For more information see:\n\n    - https://docs.astropy.org/en/stable/nddata\n    - https://docs.astropy.org/en/stable/io/unified.html\n\n    Parameters\n    ----------\n    *args : tuple, optional\n        Positional arguments passed through to data writer. If supplied the\n        first argument is the output filename.\n    format : str, optional\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "registry.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table/mixins", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# This module handles the definition of mixin 'handlers' which are functions\n# that given an arbitrary object (e.g. a dask array) will return an object that\n# can be used as a mixin column. This is useful because it means that users can\n# then add objects to tables that are not formally mixin columns and where\n# adding an info attribute is beyond our control.\n\n__all__ = [\"MixinRegistryError\", \"get_mixin_handler\", \"register_mixin_handler\"]\n\n# The internal dictionary of handlers maps fully qualified names of classes\n# to a function that can take an object and return a mixin-compatible object.\n_handlers = {}\n\n\nclass MixinRegistryError(Exception):\n    pass\n\n\ndef register_mixin_handler(fully_qualified_name, handler, force=False):\n    \"\"\"\n    Register a mixin column 'handler'.\n\n    A mixin column handler is a function that given an arbitrary Python object,\n    will return an object with the .info attribute that can then be used as a\n    mixin column (this can be e.g. a copy of the object with a new attribute,\n    a subclass instance, or a wrapper class - this is left up to the handler).\n\n    The handler will be used on classes that have an exactly matching fully\n    qualified name.\n\n    Parameters\n    ----------\n    fully_qualified_name : str\n        The fully qualified name of the class that the handler can operate on,\n        such as e.g. ``dask.array.core.Array``.\n    handler : func\n        The handler function.\n    force : bool, optional\n        Whether to overwrite any previous handler if there is already one for\n        the same fully qualified name.\n    \"\"\"\n    if fully_qualified_name not in _handlers or force:\n        _handlers[fully_qualified_name] = handler\n    else:\n        raise MixinRegistryError(\n            f\"Handler for class {fully_qualified_name} is already defined\"\n        )\n\n\ndef get_mixin_handler(obj):\n    \"\"\"\n    Given an arbitrary object, return the matching mixin handler (if any).\n\n    Parameters\n    ----------\n    obj : object or str\n        The o"}], "retrieved_count": 10, "cost_time": 1.1352849006652832}
{"question": "What does verifying different values for the metadata field that records which source was selected from the priority list indicate about the source selection algorithm in the test class that validates automatic loading of leap second tables with different priority configurations?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " iers.LeapSeconds.open()\n        assert ls.meta[\"data_url\"] == \"erfa\"\n\n    def test_builtin_found(self):\n        # Set huge maximum age such that built-in file is always OK.\n        # If we remove 'erfa', it should thus be found.\n        self.remove_auto_open_files(\"erfa\")\n        with iers.conf.set_temp(\"auto_max_age\", 100000):\n            ls = iers.LeapSeconds.open()\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n    # The test below is marked remote_data only to ensure it runs\n    # as an allowed-fail job on CI: i.e., we will notice it (eventually)\n    # but will not be misled in thinking that a PR is bad.\n    @pytest.mark.remote_data\n    def test_builtin_not_expired(self):\n        # TODO: would be nice to have automatic PRs for this!\n        ls = iers.LeapSeconds.open(iers.IERS_LEAP_SECOND_FILE)\n        assert ls.expires > self.good_enough, (\n            \"The leap second file built in to astropy is expired. Fix with:\\n\"\n            \"cd astropy/utils/iers/data/; . update_builtin_iers.sh\\n\"\n            \"and commit as a PR (for details, see release procedure).\"\n        )\n\n    def test_fake_future_file(self, tmp_path):\n        fake_file = make_fake_file(\"28 June 2345\", tmp_path)\n        # Try as system file for auto_open, setting auto_max_age such\n        # that any ERFA or system files are guaranteed to be expired.\n        with (\n            iers.conf.set_temp(\"auto_max_age\", -100000),\n            iers.conf.set_temp(\"system_leap_second_file\", fake_file),\n        ):\n            ls = iers.LeapSeconds.open()\n        assert ls.expires == Time(\"2345-06-28\", scale=\"tai\")\n        assert ls.meta[\"data_url\"] == str(fake_file)\n        # And as URL\n        fake_url = \"file:\" + urllib.request.pathname2url(fake_file)\n        with (\n            iers.conf.set_temp(\"auto_max_age\", -100000),\n            iers.conf.set_temp(\"iers_leap_second_auto_url\", fake_url),\n        ):\n            ls2 = iers.LeapSeconds.open()\n        assert ls2.expires == Time(\"2345-06-28\", s"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "eapSeconds.auto_open([fake_file1, iers.IERS_LEAP_SECOND_FILE])\n        assert ls3.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n        with iers.conf.set_temp(\"auto_max_age\", None):\n            ls4 = iers.LeapSeconds.auto_open([fake_file1, fake_file2])\n        assert ls4.meta[\"data_url\"] == fake_file2\n\n\n@pytest.mark.remote_data\nclass TestRemoteURLs:\n    def setup_class(cls):\n        # Need auto_download so that IERS_B won't be loaded and cause tests to\n        # fail.\n        iers.conf.auto_download = True\n\n    def teardown_class(cls):\n        # This setting is to be consistent with astropy/conftest.py\n        iers.conf.auto_download = False\n\n    # In these tests, the results may be cached.\n    # This is fine - no need to download again.\n    def test_iers_url(self):\n        ls = iers.LeapSeconds.auto_open([iers.IERS_LEAP_SECOND_URL])\n        assert ls.expires > Time.now()\n\n    def test_ietf_url(self):\n        ls = iers.LeapSeconds.auto_open([iers.IETF_LEAP_SECOND_URL])\n        assert ls.expires > Time.now()\n\n\nclass TestDefaultAutoOpen:\n    \"\"\"Test auto_open with different _auto_open_files.\"\"\"\n\n    def setup_method(self):\n        # Identical to what is used in LeapSeconds.auto_open().\n        self.good_enough = iers.LeapSeconds._today() + TimeDelta(\n            180 - iers._none_to_float(iers.conf.auto_max_age), format=\"jd\"\n        )\n        self._auto_open_files = iers.LeapSeconds._auto_open_files.copy()\n\n    def teardown_method(self):\n        iers.LeapSeconds._auto_open_files = self._auto_open_files\n\n    def remove_auto_open_files(self, *files):\n        \"\"\"Remove some files from the auto-opener.\n\n        The default set is restored in teardown.\n        \"\"\"\n        for f in files:\n            iers.LeapSeconds._auto_open_files.remove(f)\n\n    def test_erfa_found(self):\n        # Set huge maximum age such that whatever ERFA has is OK.\n        # Since it is checked first, it should thus be found.\n        with iers.conf.set_temp(\"auto_max_age\", 100000):\n            ls ="}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "cale=\"tai\")\n        assert ls2.meta[\"data_url\"] == str(fake_url)\n\n    def test_fake_expired_file(self, tmp_path):\n        self.remove_auto_open_files(\n            \"erfa\", \"iers_leap_second_auto_url\", \"ietf_leap_second_auto_url\"\n        )\n        fake_file = make_fake_file(\"28 June 2010\", tmp_path)\n        with iers.conf.set_temp(\"system_leap_second_file\", fake_file):\n            # If we try this directly, the built-in file will be found.\n            ls = iers.LeapSeconds.open()\n            assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n            # But if we remove the built-in one, the expired one will be\n            # used and we get a warning that it is stale.\n            self.remove_auto_open_files(iers.IERS_LEAP_SECOND_FILE)\n            with pytest.warns(iers.IERSStaleWarning):\n                ls2 = iers.LeapSeconds.open()\n            assert ls2.meta[\"data_url\"] == fake_file\n            assert ls2.expires == Time(\"2010-06-28\", scale=\"tai\")\n\n    @pytest.mark.skipif(\n        not os.path.isfile(SYSTEM_FILE), reason=f\"system does not have {SYSTEM_FILE}\"\n    )\n    def test_system_file_used_if_not_expired(self, tmp_path):\n        # We skip the test if the system file is on a CI and is expired -\n        # we should not depend on CI keeping it up to date, but if it is,\n        # we should check that it is used if possible.\n        if iers.LeapSeconds.open(SYSTEM_FILE).expires <= self.good_enough:\n            pytest.skip(\"System leap second file is expired.\")\n\n        self.remove_auto_open_files(\"erfa\")\n        with iers.conf.set_temp(\"system_leap_second_file\", SYSTEM_FILE):\n            ls = iers.LeapSeconds.open()\n            assert ls.expires > self.good_enough\n            assert ls.meta[\"data_url\"] in (iers.IERS_LEAP_SECOND_FILE, SYSTEM_FILE)\n\n            # Also check with a \"built-in\" file that is expired\n            fake_file = make_fake_file(\"28 June 2017\", tmp_path)\n            iers.LeapSeconds._auto_open_files[0] = fake_file\n            ls2 = iers.Le"}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "apSeconds.open()\n            assert ls2.expires > Time.now()\n            assert ls2.meta[\"data_url\"] == SYSTEM_FILE\n\n    @pytest.mark.remote_data\n    def test_auto_open_urls_always_good_enough(self):\n        # Avoid using the erfa, built-in and system files, as they might\n        # be good enough already.\n        try:\n            # Need auto_download so that IERS_B won't be loaded and\n            # cause tests to fail.\n            iers.conf.auto_download = True\n\n            self.remove_auto_open_files(\n                \"erfa\", iers.IERS_LEAP_SECOND_FILE, \"system_leap_second_file\"\n            )\n            ls = iers.LeapSeconds.open()\n            assert ls.expires > self.good_enough\n            assert ls.meta[\"data_url\"].startswith(\"http\")\n        finally:\n            # This setting is to be consistent with astropy/conftest.py\n            iers.conf.auto_download = False\n\n\nclass ERFALeapSecondsSafe:\n    \"\"\"Base class for tests that change the ERFA leap-second tables.\n\n    It ensures the original state is restored.\n    \"\"\"\n\n    def setup_method(self):\n        # Keep current leap-second table and expiration.\n        self.erfa_ls = self._erfa_ls = erfa.leap_seconds.get()\n        self.erfa_expires = self._expires = erfa.leap_seconds._expires\n\n    def teardown_method(self):\n        # Restore leap-second table and expiration.\n        erfa.leap_seconds.set(self.erfa_ls)\n        erfa.leap_seconds._expires = self._expires\n\n\nclass TestFromERFA(ERFALeapSecondsSafe):\n    def test_get_erfa_ls(self):\n        ls = iers.LeapSeconds.from_erfa()\n        assert ls.colnames == [\"year\", \"month\", \"tai_utc\"]\n        assert isinstance(ls.expires, Time)\n        assert ls.expires == self.erfa_expires\n        ls_array = np.array(ls[\"year\", \"month\", \"tai_utc\"])\n        assert np.all(ls_array == self.erfa_ls)\n\n    def test_get_built_in_erfa_ls(self):\n        ls = iers.LeapSeconds.from_erfa(built_in=True)\n        assert ls.colnames == [\"year\", \"month\", \"tai_utc\"]\n        assert isinstance(ls.expire"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "econds.dat\")\n    with open(fake_file, \"w\") as fh:\n        fh.write(\n            \"\\n\".join([f\"#  File expires on {expiration}\"] + str(ls).split(\"\\n\")[2:-1])\n        )\n        return fake_file\n\n\ndef test_fake_file(tmp_path):\n    fake_file = make_fake_file(\"28 June 2345\", tmp_path)\n    fake = iers.LeapSeconds.from_iers_leap_seconds(fake_file)\n    assert fake.expires == Time(\"2345-06-28\", scale=\"tai\")\n\n\nclass TestAutoOpenExplicitLists:\n    # For this set of tests, leap-seconds are allowed to be expired\n    # except as explicitly tested.\n    @pytest.mark.filterwarnings(iers.IERSStaleWarning)\n    def test_auto_open_simple(self):\n        ls = iers.LeapSeconds.auto_open([iers.IERS_LEAP_SECOND_FILE])\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n    @pytest.mark.filterwarnings(iers.IERSStaleWarning)\n    def test_auto_open_erfa(self):\n        ls = iers.LeapSeconds.auto_open([\"erfa\", iers.IERS_LEAP_SECOND_FILE])\n        assert ls.meta[\"data_url\"] in [\"erfa\", iers.IERS_LEAP_SECOND_FILE]\n\n    @pytest.mark.filterwarnings(iers.IERSStaleWarning)\n    def test_fake_future_file(self, tmp_path):\n        fake_file = make_fake_file(\"28 June 2345\", tmp_path)\n        # Try as system file for auto_open, setting auto_max_age such\n        # that any ERFA or system files are guaranteed to be expired,\n        # while the fake file is guaranteed to be OK.\n        with iers.conf.set_temp(\"auto_max_age\", -100000):\n            ls = iers.LeapSeconds.auto_open(\n                [\"erfa\", iers.IERS_LEAP_SECOND_FILE, fake_file]\n            )\n            assert ls.expires == Time(\"2345-06-28\", scale=\"tai\")\n            assert ls.meta[\"data_url\"] == str(fake_file)\n            # And as URL\n            fake_url = \"file:\" + urllib.request.pathname2url(fake_file)\n            ls2 = iers.LeapSeconds.auto_open(\n                [\"erfa\", iers.IERS_LEAP_SECOND_FILE, fake_url]\n            )\n            assert ls2.expires == Time(\"2345-06-28\", scale=\"tai\")\n            assert ls2.meta[\"data_url\"] == "}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " ls.expires > Time.now()\n\n\nclass TestDefaultAutoOpen:\n    \"\"\"Test auto_open with different _auto_open_files.\"\"\"\n\n    def setup_method(self):\n        # Identical to what is used in LeapSeconds.auto_open().\n        self.good_enough = iers.LeapSeconds._today() + TimeDelta(\n            180 - iers._none_to_float(iers.conf.auto_max_age), format=\"jd\"\n        )\n        self._auto_open_files = iers.LeapSeconds._auto_open_files.copy()\n\n    def teardown_method(self):\n        iers.LeapSeconds._auto_open_files = self._auto_open_files\n\n    def remove_auto_open_files(self, *files):\n        \"\"\"Remove some files from the auto-opener.\n\n        The default set is restored in teardown.\n        \"\"\"\n        for f in files:\n            iers.LeapSeconds._auto_open_files.remove(f)\n\n    def test_erfa_found(self):\n        # Set huge maximum age such that whatever ERFA has is OK.\n        # Since it is checked first, it should thus be found.\n        with iers.conf.set_temp(\"auto_max_age\", 100000):\n            ls = iers.LeapSeconds.open()\n        assert ls.meta[\"data_url\"] == \"erfa\"\n\n    def test_builtin_found(self):\n        # Set huge maximum age such that built-in file is always OK.\n        # If we remove 'erfa', it should thus be found.\n        self.remove_auto_open_files(\"erfa\")\n        with iers.conf.set_temp(\"auto_max_age\", 100000):\n            ls = iers.LeapSeconds.open()\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n    # The test below is marked remote_data only to ensure it runs\n    # as an allowed-fail job on CI: i.e., we will notice it (eventually)\n    # but will not be misled in thinking that a PR is bad.\n    @pytest.mark.remote_data\n    def test_builtin_not_expired(self):\n        # TODO: would be nice to have automatic PRs for this!\n        ls = iers.LeapSeconds.open(iers.IERS_LEAP_SECOND_FILE)\n        assert ls.expires > self.good_enough, (\n            \"The leap second file built in to astropy is expired. Fix with:\\n\"\n            \"cd astropy/utils/iers/data/; ."}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     not os.path.isfile(SYSTEM_FILE), reason=f\"system does not have {SYSTEM_FILE}\"\n    )\n    def test_system_file_used_if_not_expired(self, tmp_path):\n        # We skip the test if the system file is on a CI and is expired -\n        # we should not depend on CI keeping it up to date, but if it is,\n        # we should check that it is used if possible.\n        if iers.LeapSeconds.open(SYSTEM_FILE).expires <= self.good_enough:\n            pytest.skip(\"System leap second file is expired.\")\n\n        self.remove_auto_open_files(\"erfa\")\n        with iers.conf.set_temp(\"system_leap_second_file\", SYSTEM_FILE):\n            ls = iers.LeapSeconds.open()\n            assert ls.expires > self.good_enough\n            assert ls.meta[\"data_url\"] in (iers.IERS_LEAP_SECOND_FILE, SYSTEM_FILE)\n\n            # Also check with a \"built-in\" file that is expired\n            fake_file = make_fake_file(\"28 June 2017\", tmp_path)\n            iers.LeapSeconds._auto_open_files[0] = fake_file\n            ls2 = iers.LeapSeconds.open()\n            assert ls2.expires > Time.now()\n            assert ls2.meta[\"data_url\"] == SYSTEM_FILE\n\n    @pytest.mark.remote_data\n    def test_auto_open_urls_always_good_enough(self):\n        # Avoid using the erfa, built-in and system files, as they might\n        # be good enough already.\n        try:\n            # Need auto_download so that IERS_B won't be loaded and\n            # cause tests to fail.\n            iers.conf.auto_download = True\n\n            self.remove_auto_open_files(\n                \"erfa\", iers.IERS_LEAP_SECOND_FILE, \"system_leap_second_file\"\n            )\n            ls = iers.LeapSeconds.open()\n            assert ls.expires > self.good_enough\n            assert ls.meta[\"data_url\"].startswith(\"http\")\n        finally:\n            # This setting is to be consistent with astropy/conftest.py\n            iers.conf.auto_download = False\n\n\nclass ERFALeapSecondsSafe:\n    \"\"\"Base class for tests that change the ERFA leap-second tables.\n\n    It ensures the o"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "str(fake_url)\n\n    def test_fake_expired_file(self, tmp_path):\n        fake_file1 = make_fake_file(\"28 June 2010\", tmp_path)\n        fake_file2 = make_fake_file(\"27 June 2012\", tmp_path)\n        # Between these and the built-in one, the built-in file is best.\n        ls = iers.LeapSeconds.auto_open(\n            [fake_file1, fake_file2, iers.IERS_LEAP_SECOND_FILE]\n        )\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n        # But if we remove the built-in one, the least expired one will be\n        # used and we get a warning that it is stale.\n        with pytest.warns(iers.IERSStaleWarning):\n            ls2 = iers.LeapSeconds.auto_open([fake_file1, fake_file2])\n        assert ls2.meta[\"data_url\"] == fake_file2\n        assert ls2.expires == Time(\"2012-06-27\", scale=\"tai\")\n\n        # Use the fake files to make sure auto_max_age is safe.\n        # Should have no warning in either example.\n        with iers.conf.set_temp(\"auto_max_age\", None):\n            ls3 = iers.LeapSeconds.auto_open([fake_file1, iers.IERS_LEAP_SECOND_FILE])\n        assert ls3.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n        with iers.conf.set_temp(\"auto_max_age\", None):\n            ls4 = iers.LeapSeconds.auto_open([fake_file1, fake_file2])\n        assert ls4.meta[\"data_url\"] == fake_file2\n\n\n@pytest.mark.remote_data\nclass TestRemoteURLs:\n    def setup_class(cls):\n        # Need auto_download so that IERS_B won't be loaded and cause tests to\n        # fail.\n        iers.conf.auto_download = True\n\n    def teardown_class(cls):\n        # This setting is to be consistent with astropy/conftest.py\n        iers.conf.auto_download = False\n\n    # In these tests, the results may be cached.\n    # This is fine - no need to download again.\n    def test_iers_url(self):\n        ls = iers.LeapSeconds.auto_open([iers.IERS_LEAP_SECOND_URL])\n        assert ls.expires > Time.now()\n\n    def test_ietf_url(self):\n        ls = iers.LeapSeconds.auto_open([iers.IETF_LEAP_SECOND_URL])\n        assert"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_update_leap_seconds.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "s.expires > self.good_enough\n\n    def test_auto_update_bad_file(self):\n        with pytest.warns(AstropyWarning, match=\"FileNotFound\"):\n            update_leap_seconds([\"nonsense\"])\n\n    def test_auto_update_corrupt_file(self, tmp_path):\n        bad_file = tmp_path / \"no_expiration\"\n        lines = Path(iers.IERS_LEAP_SECOND_FILE).read_text().splitlines()\n        bad_file.write_text(\n            \"\\n\".join(line for line in lines if not line.startswith(\"#\"))\n        )\n\n        with pytest.warns(AstropyWarning, match=\"ValueError.*did not find expiration\"):\n            update_leap_seconds([bad_file])\n\n    def test_auto_update_expired_file(self, tmp_path):\n        # Set up expired ERFA leap seconds.\n        expired = self.erfa_ls[self.erfa_ls[\"year\"] < 2017]\n        expired.update_erfa_leap_seconds(initialize_erfa=\"empty\")\n        # Create similarly expired file.\n        expired_file = str(tmp_path / \"expired.dat\")\n        Path(expired_file).write_text(\n            \"\\n\".join(\n                [\"# File expires on 28 June 2010\"] + [str(item) for item in expired]\n            )\n        )\n        with pytest.warns(iers.IERSStaleWarning):\n            update_leap_seconds([\"erfa\", expired_file])\n\n    def test_init_thread_safety(self, monkeypatch):\n        # Set up expired ERFA leap seconds.\n        expired = self.erfa_ls[self.erfa_ls[\"year\"] < 2017]\n        expired.update_erfa_leap_seconds(initialize_erfa=\"empty\")\n        # Force re-initialization, even if another test already did it\n        monkeypatch.setattr(\n            astropy.time.core,\n            \"_LEAP_SECONDS_CHECK\",\n            astropy.time.core._LeapSecondsCheck.NOT_STARTED,\n        )\n        workers = 4\n        with ThreadPoolExecutor(max_workers=workers) as executor:\n            futures = [\n                executor.submit(lambda: str(Time(\"2019-01-01 00:00:00.000\").tai))\n                for i in range(workers)\n            ]\n            results = [future.result() for future in futures]\n            assert results == ["}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "OND_FILE]\n\n    @pytest.mark.filterwarnings(iers.IERSStaleWarning)\n    def test_fake_future_file(self, tmp_path):\n        fake_file = make_fake_file(\"28 June 2345\", tmp_path)\n        # Try as system file for auto_open, setting auto_max_age such\n        # that any ERFA or system files are guaranteed to be expired,\n        # while the fake file is guaranteed to be OK.\n        with iers.conf.set_temp(\"auto_max_age\", -100000):\n            ls = iers.LeapSeconds.auto_open(\n                [\"erfa\", iers.IERS_LEAP_SECOND_FILE, fake_file]\n            )\n            assert ls.expires == Time(\"2345-06-28\", scale=\"tai\")\n            assert ls.meta[\"data_url\"] == str(fake_file)\n            # And as URL\n            fake_url = \"file:\" + urllib.request.pathname2url(fake_file)\n            ls2 = iers.LeapSeconds.auto_open(\n                [\"erfa\", iers.IERS_LEAP_SECOND_FILE, fake_url]\n            )\n            assert ls2.expires == Time(\"2345-06-28\", scale=\"tai\")\n            assert ls2.meta[\"data_url\"] == str(fake_url)\n\n    def test_fake_expired_file(self, tmp_path):\n        fake_file1 = make_fake_file(\"28 June 2010\", tmp_path)\n        fake_file2 = make_fake_file(\"27 June 2012\", tmp_path)\n        # Between these and the built-in one, the built-in file is best.\n        ls = iers.LeapSeconds.auto_open(\n            [fake_file1, fake_file2, iers.IERS_LEAP_SECOND_FILE]\n        )\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n        # But if we remove the built-in one, the least expired one will be\n        # used and we get a warning that it is stale.\n        with pytest.warns(iers.IERSStaleWarning):\n            ls2 = iers.LeapSeconds.auto_open([fake_file1, fake_file2])\n        assert ls2.meta[\"data_url\"] == fake_file2\n        assert ls2.expires == Time(\"2012-06-27\", scale=\"tai\")\n\n        # Use the fake files to make sure auto_max_age is safe.\n        # Should have no warning in either example.\n        with iers.conf.set_temp(\"auto_max_age\", None):\n            ls3 = iers.L"}], "retrieved_count": 10, "cost_time": 1.158785104751587}
{"question": "How does the standard deviation uncertainty class's property that references the associated n-dimensional data container instance handle access when there is no associated parent object?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "nduncertainty.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/nddata", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "parent_lost_message = (\n            \"the associated NDData object was deleted and cannot be accessed \"\n            \"anymore. You can prevent the NDData object from being deleted by \"\n            \"assigning it to a variable. If this happened after unpickling \"\n            \"make sure you pickle the parent not the uncertainty directly.\"\n        )\n        try:\n            parent = self._parent_nddata\n        except AttributeError:\n            raise MissingDataAssociationException(no_parent_message)\n        else:\n            if parent is None:\n                raise MissingDataAssociationException(no_parent_message)\n            else:\n                # The NDData is saved as weak reference so we must call it\n                # to get the object the reference points to. However because\n                # we have a weak reference here it's possible that the parent\n                # was deleted because its reference count dropped to zero.\n                if isinstance(self._parent_nddata, weakref.ref):\n                    resolved_parent = self._parent_nddata()\n                    if resolved_parent is None:\n                        log.info(parent_lost_message)\n                    return resolved_parent\n                else:\n                    log.info(\"parent_nddata should be a weakref to an NDData object.\")\n                    return self._parent_nddata\n\n    @parent_nddata.setter\n    def parent_nddata(self, value):\n        if value is not None and not isinstance(value, weakref.ref):\n            # Save a weak reference on the uncertainty that points to this\n            # instance of NDData. Direct references should NOT be used:\n            # https://github.com/astropy/astropy/pull/4799#discussion_r61236832\n            value = weakref.ref(value)\n        # Set _parent_nddata here and access below with the property because value\n        # is a weakref\n        self._parent_nddata = value\n        # set uncertainty unit to that of the parent if it was not already set, unless initia"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "nduncertainty.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/nddata", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ref):\n                    resolved_parent = self._parent_nddata()\n                    if resolved_parent is None:\n                        log.info(parent_lost_message)\n                    return resolved_parent\n                else:\n                    log.info(\"parent_nddata should be a weakref to an NDData object.\")\n                    return self._parent_nddata\n\n    @parent_nddata.setter\n    def parent_nddata(self, value):\n        if value is not None and not isinstance(value, weakref.ref):\n            # Save a weak reference on the uncertainty that points to this\n            # instance of NDData. Direct references should NOT be used:\n            # https://github.com/astropy/astropy/pull/4799#discussion_r61236832\n            value = weakref.ref(value)\n        # Set _parent_nddata here and access below with the property because value\n        # is a weakref\n        self._parent_nddata = value\n        # set uncertainty unit to that of the parent if it was not already set, unless initializing\n        # with empty parent (Value=None)\n        if value is not None:\n            parent_unit = self.parent_nddata.unit\n            # this will get the unit for masked quantity input:\n            parent_data_unit = getattr(self.parent_nddata.data, \"unit\", None)\n            if parent_unit is None and parent_data_unit is None:\n                self.unit = None\n            elif self.unit is None and parent_unit is not None:\n                # Set the uncertainty's unit to the appropriate value\n                self.unit = self._data_unit_to_uncertainty_unit(parent_unit)\n            elif parent_data_unit is not None:\n                # if the parent_nddata object has a unit, use it:\n                self.unit = self._data_unit_to_uncertainty_unit(parent_data_unit)\n            else:\n                # Check that units of uncertainty are compatible with those of\n                # the parent. If they are, no need to change units of the\n                # uncertainty or the data. If they are "}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "nduncertainty.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/nddata", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "            self._data_unit_to_uncertainty_unit(parent_unit).to(value)\n                except UnitConversionError:\n                    raise UnitConversionError(\n                        f\"Unit {value} is incompatible with unit {parent_unit} of \"\n                        \"parent nddata\"\n                    )\n\n            self._unit = Unit(value)\n        else:\n            self._unit = value\n\n    @property\n    def quantity(self):\n        \"\"\"\n        This uncertainty as an `~astropy.units.Quantity` object.\n        \"\"\"\n        return Quantity(self.array, self.unit, copy=False, dtype=self.array.dtype)\n\n    @property\n    def parent_nddata(self):\n        \"\"\"`NDData` : reference to `NDData` instance with this uncertainty.\n\n        In case the reference is not set uncertainty propagation will not be\n        possible since propagation might need the uncertain data besides the\n        uncertainty.\n        \"\"\"\n        no_parent_message = \"uncertainty is not associated with an NDData object\"\n        parent_lost_message = (\n            \"the associated NDData object was deleted and cannot be accessed \"\n            \"anymore. You can prevent the NDData object from being deleted by \"\n            \"assigning it to a variable. If this happened after unpickling \"\n            \"make sure you pickle the parent not the uncertainty directly.\"\n        )\n        try:\n            parent = self._parent_nddata\n        except AttributeError:\n            raise MissingDataAssociationException(no_parent_message)\n        else:\n            if parent is None:\n                raise MissingDataAssociationException(no_parent_message)\n            else:\n                # The NDData is saved as weak reference so we must call it\n                # to get the object the reference points to. However because\n                # we have a weak reference here it's possible that the parent\n                # was deleted because its reference count dropped to zero.\n                if isinstance(self._parent_nddata, weakref."}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "nduncertainty.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/nddata", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " return False\n\n    @property\n    def array(self):\n        \"\"\"`numpy.ndarray` : the uncertainty's value.\"\"\"\n        return self._array\n\n    @array.setter\n    def array(self, value):\n        if isinstance(value, (list, np.ndarray)):\n            value = np.asarray(value)\n        self._array = value\n\n    @property\n    def unit(self):\n        \"\"\"`~astropy.units.Unit` : The unit of the uncertainty, if any.\"\"\"\n        return self._unit\n\n    @unit.setter\n    def unit(self, value):\n        \"\"\"\n        The unit should be set to a value consistent with the parent NDData\n        unit and the uncertainty type.\n        \"\"\"\n        if value is not None:\n            # Check the hidden attribute below, not the property. The property\n            # raises an exception if there is no parent_nddata.\n            if self._parent_nddata is not None:\n                parent_unit = self.parent_nddata.unit\n                try:\n                    # Check for consistency with the unit of the parent_nddata\n                    self._data_unit_to_uncertainty_unit(parent_unit).to(value)\n                except UnitConversionError:\n                    raise UnitConversionError(\n                        f\"Unit {value} is incompatible with unit {parent_unit} of \"\n                        \"parent nddata\"\n                    )\n\n            self._unit = Unit(value)\n        else:\n            self._unit = value\n\n    @property\n    def quantity(self):\n        \"\"\"\n        This uncertainty as an `~astropy.units.Quantity` object.\n        \"\"\"\n        return Quantity(self.array, self.unit, copy=False, dtype=self.array.dtype)\n\n    @property\n    def parent_nddata(self):\n        \"\"\"`NDData` : reference to `NDData` instance with this uncertainty.\n\n        In case the reference is not set uncertainty propagation will not be\n        possible since propagation might need the uncertain data besides the\n        uncertainty.\n        \"\"\"\n        no_parent_message = \"uncertainty is not associated with an NDData object\"\n        "}, {"start_line": 15000, "end_line": 16581, "belongs_to": {"file_name": "nddata.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/nddata", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ve an attribute ``uncertainty_type`` that defines what kind of\n        uncertainty is stored, such as ``'std'`` for standard deviation or\n        ``'var'`` for variance. A metaclass defining such an interface is\n        `~astropy.nddata.NDUncertainty` but isn't mandatory.\n        \"\"\"\n        return self._uncertainty\n\n    @uncertainty.setter\n    def uncertainty(self, value):\n        if value is not None:\n            # There is one requirements on the uncertainty: That\n            # it has an attribute 'uncertainty_type'.\n            # If it does not match this requirement convert it to an unknown\n            # uncertainty.\n            if not hasattr(value, \"uncertainty_type\"):\n                log.info(\"uncertainty should have attribute uncertainty_type.\")\n                value = UnknownUncertainty(value, copy=False)\n\n            # If it is a subclass of NDUncertainty we must set the\n            # parent_nddata attribute. (#4152)\n            if isinstance(value, NDUncertainty):\n                # In case the uncertainty already has a parent create a new\n                # instance because we need to assume that we don't want to\n                # steal the uncertainty from another NDData object\n                if value._parent_nddata is not None:\n                    value = value.__class__(value, copy=False)\n                # Then link it to this NDData instance (internally this needs\n                # to be saved as weakref but that's done by NDUncertainty\n                # setter).\n                value.parent_nddata = self\n        self._uncertainty = value\n"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "ccddata.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/nddata", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_handle_wcs_slicing_error(err, item)\n\n    @property\n    def data(self):\n        return self._data\n\n    @data.setter\n    def data(self, value):\n        self._data = value\n\n    @property\n    def wcs(self):\n        return self._wcs\n\n    @wcs.setter\n    def wcs(self, value):\n        if value is not None and not isinstance(value, WCS):\n            raise TypeError(\"the wcs must be a WCS instance.\")\n        self._wcs = value\n\n    @property\n    def unit(self):\n        return self._unit\n\n    @unit.setter\n    def unit(self, value):\n        self._unit = u.Unit(value)\n\n    @property\n    def psf(self):\n        return self._psf\n\n    @psf.setter\n    def psf(self, value):\n        if value is not None and not isinstance(value, np.ndarray):\n            raise TypeError(\"The psf must be a numpy array.\")\n        self._psf = value\n\n    @property\n    def header(self):\n        return self._meta\n\n    @header.setter\n    def header(self, value):\n        self.meta = value\n\n    @property\n    def uncertainty(self):\n        return self._uncertainty\n\n    @uncertainty.setter\n    def uncertainty(self, value):\n        if value is not None:\n            if isinstance(value, NDUncertainty):\n                if getattr(value, \"_parent_nddata\", None) is not None:\n                    value = value.__class__(value, copy=False)\n                self._uncertainty = value\n            elif isinstance(value, np.ndarray):\n                if value.shape != self.shape:\n                    raise ValueError(\"uncertainty must have same shape as data.\")\n                self._uncertainty = StdDevUncertainty(value)\n                log.info(\n                    \"array provided for uncertainty; assuming it is a \"\n                    \"StdDevUncertainty.\"\n                )\n            else:\n                raise TypeError(\n                    \"uncertainty must be an instance of a \"\n                    \"NDUncertainty object or a numpy array.\"\n                )\n            self._uncertainty.parent_nddata = self\n        else:\n   "}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "nduncertainty.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/nddata", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "not, let the user know.\n                unit_from_data = self._data_unit_to_uncertainty_unit(parent_unit)\n                try:\n                    unit_from_data.to(self.unit)\n                except UnitConversionError:\n                    raise UnitConversionError(\n                        f\"Unit {self.unit} of uncertainty \"\n                        f\"incompatible with unit {parent_unit} of \"\n                        \"data\"\n                    )\n\n    @abstractmethod\n    def _data_unit_to_uncertainty_unit(self, value):\n        \"\"\"\n        Subclasses must override this property. It should take in a data unit\n        and return the correct unit for the uncertainty given the uncertainty\n        type.\n        \"\"\"\n        return None\n\n    def __repr__(self):\n        prefix = self.__class__.__name__ + \"(\"\n        try:\n            body = np.array2string(self.array, separator=\", \", prefix=prefix)\n        except AttributeError:\n            # In case it wasn't possible to use array2string\n            body = str(self.array)\n        return f\"{prefix}{body})\"\n\n    def __getstate__(self):\n        # Because of the weak reference the class wouldn't be picklable.\n        try:\n            return self._array, self._unit, self.parent_nddata\n        except MissingDataAssociationException:\n            # In case there's no parent\n            return self._array, self._unit, None\n\n    def __setstate__(self, state):\n        if len(state) != 3:\n            raise TypeError(\"The state should contain 3 items.\")\n        self._array = state[0]\n        self._unit = state[1]\n\n        parent = state[2]\n        if parent is not None:\n            parent = weakref.ref(parent)\n        self._parent_nddata = parent\n\n    def __getitem__(self, item):\n        \"\"\"Normal slicing on the array, keep the unit and return a reference.\"\"\"\n        return self.__class__(self.array[item], unit=self.unit, copy=False)\n\n    def propagate(self, operation, other_nddata, result_data, correlation, axis=None):\n        \"\"\"Calculate"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "nduncertainty.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/nddata", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "lizing\n        # with empty parent (Value=None)\n        if value is not None:\n            parent_unit = self.parent_nddata.unit\n            # this will get the unit for masked quantity input:\n            parent_data_unit = getattr(self.parent_nddata.data, \"unit\", None)\n            if parent_unit is None and parent_data_unit is None:\n                self.unit = None\n            elif self.unit is None and parent_unit is not None:\n                # Set the uncertainty's unit to the appropriate value\n                self.unit = self._data_unit_to_uncertainty_unit(parent_unit)\n            elif parent_data_unit is not None:\n                # if the parent_nddata object has a unit, use it:\n                self.unit = self._data_unit_to_uncertainty_unit(parent_data_unit)\n            else:\n                # Check that units of uncertainty are compatible with those of\n                # the parent. If they are, no need to change units of the\n                # uncertainty or the data. If they are not, let the user know.\n                unit_from_data = self._data_unit_to_uncertainty_unit(parent_unit)\n                try:\n                    unit_from_data.to(self.unit)\n                except UnitConversionError:\n                    raise UnitConversionError(\n                        f\"Unit {self.unit} of uncertainty \"\n                        f\"incompatible with unit {parent_unit} of \"\n                        \"data\"\n                    )\n\n    @abstractmethod\n    def _data_unit_to_uncertainty_unit(self, value):\n        \"\"\"\n        Subclasses must override this property. It should take in a data unit\n        and return the correct unit for the uncertainty given the uncertainty\n        type.\n        \"\"\"\n        return None\n\n    def __repr__(self):\n        prefix = self.__class__.__name__ + \"(\"\n        try:\n            body = np.array2string(self.array, separator=\", \", prefix=prefix)\n        except AttributeError:\n            # In case it wasn't possible to use array2string\n         "}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "nddata.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/nddata", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"\"\"\n        return self._unit\n\n    @property\n    def wcs(self):\n        \"\"\"\n        any type : A world coordinate system (WCS) for the dataset, if any.\n        \"\"\"\n        return self._wcs\n\n    @wcs.setter\n    def wcs(self, wcs):\n        if self._wcs is not None and wcs is not None:\n            raise ValueError(\n                \"You can only set the wcs attribute with a WCS if no WCS is present.\"\n            )\n\n        if wcs is None or isinstance(wcs, BaseHighLevelWCS):\n            self._wcs = wcs\n        elif isinstance(wcs, BaseLowLevelWCS):\n            self._wcs = HighLevelWCSWrapper(wcs)\n        else:\n            raise TypeError(\n                \"The wcs argument must implement either the high or low level WCS API.\"\n            )\n\n    @property\n    def psf(self):\n        return self._psf\n\n    @psf.setter\n    def psf(self, value):\n        self._psf = value\n\n    @property\n    def uncertainty(self):\n        \"\"\"\n        any type : Uncertainty in the dataset, if any.\n\n        Should have an attribute ``uncertainty_type`` that defines what kind of\n        uncertainty is stored, such as ``'std'`` for standard deviation or\n        ``'var'`` for variance. A metaclass defining such an interface is\n        `~astropy.nddata.NDUncertainty` but isn't mandatory.\n        \"\"\"\n        return self._uncertainty\n\n    @uncertainty.setter\n    def uncertainty(self, value):\n        if value is not None:\n            # There is one requirements on the uncertainty: That\n            # it has an attribute 'uncertainty_type'.\n            # If it does not match this requirement convert it to an unknown\n            # uncertainty.\n            if not hasattr(value, \"uncertainty_type\"):\n                log.info(\"uncertainty should have attribute uncertainty_type.\")\n                value = UnknownUncertainty(value, copy=False)\n\n            # If it is a subclass of NDUncertainty we must set the\n            # parent_nddata attribute. (#4152)\n            if isinstance(value, NDUncertainty):\n         "}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "data_info.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n a temporary object.\n\nIt looks like you have done something like ``col[3:5].info`` or\n``col.quantity.info``, i.e.  you accessed ``info`` from a temporary slice\nobject that only exists momentarily.  This has failed because the reference to\nthat temporary object is now lost.  Instead force a permanent reference (e.g.\n``c = col[3:5]`` followed by ``c.info``).\"\"\"\n            )\n\n        return parent\n\n    def __get__(self, instance, owner_cls):\n        if instance is None:\n            # This is an unbound descriptor on the class\n            self._parent_cls = owner_cls\n            return self\n\n        info = instance.__dict__.get(\"info\")\n        if info is None:\n            info = instance.__dict__[\"info\"] = self.__class__(bound=True)\n        # We set _parent_ref on every call, since if one makes copies of\n        # instances, 'info' will be copied as well, which will lose the\n        # reference.\n        info._parent_ref = weakref.ref(instance)\n        return info\n\n    def __set__(self, instance, value):\n        if instance is None:\n            # This is an unbound descriptor on the class\n            raise ValueError(\"cannot set unbound descriptor\")\n\n        if isinstance(value, DataInfo):\n            info = instance.__dict__[\"info\"] = self.__class__(bound=True)\n            attr_names = info.attr_names\n            if value.__class__ is self.__class__:\n                # For same class, attributes are guaranteed to be stored in\n                # _attrs, so speed matters up by not accessing defaults.\n                # Doing this before difference in for loop helps speed.\n                attr_names = attr_names & set(value._attrs)  # NOT in-place!\n            else:\n                # For different classes, copy over the attributes in common.\n                attr_names = attr_names & (value.attr_names - value._attrs_no_copy)\n\n            for attr in attr_names - info.attrs_from_parent - info._attrs_no_copy:\n                info._attrs[attr] = deepcopy(getattr(value, attr))\n\n"}], "retrieved_count": 10, "cost_time": 1.1419789791107178}
{"question": "Why does the test mixin class for verifying cosmology read/write operations through HTML table format exist to ensure the integrity of converting cosmology instances to HTML tables when default values defined in cosmology class parameters must compensate for columns absent from the HTML table during reading?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "test_html.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "at=\"ascii.html\")\n        converted_tbl = cosmo.to_format(\"astropy.table\")\n\n        # asserts each column name has been reverted\n        # cosmology name is still stored in first slot\n        for column_name in converted_tbl.colnames[1:]:\n            assert column_name in _FORMAT_TABLE.keys()\n\n    @pytest.mark.skipif(not HAS_BS4, reason=\"requires beautifulsoup4\")\n    @pytest.mark.parametrize(\"latex_names\", [True, False])\n    def test_readwrite_html_subclass_partial_info(\n        self, cosmo_cls, cosmo, read, write, latex_names, tmp_path, add_cu\n    ):\n        \"\"\"\n        Test writing from an instance and reading from that class.\n        This works with missing information.\n        \"\"\"\n        fp = tmp_path / \"test_read_html_subclass_partial_info.html\"\n\n        # test write\n        write(fp, format=\"ascii.html\", latex_names=latex_names)\n\n        # partial information\n        tbl = QTable.read(fp)\n\n        # tbl.meta.pop(\"cosmology\", None) # metadata not implemented\n        cname = \"$$T_{0}$$\" if latex_names else \"Tcmb0\"\n        del tbl[cname]  # format is not converted to original units\n        tbl.write(fp, overwrite=True)\n\n        # read with the same class that wrote fills in the missing info with\n        # the default value\n        got = cosmo_cls.read(fp, format=\"ascii.html\")\n        got2 = read(fp, format=\"ascii.html\", cosmology=cosmo_cls)\n        got3 = read(fp, format=\"ascii.html\", cosmology=cosmo_cls.__qualname__)\n\n        assert (got == got2) and (got2 == got3)  # internal consistency\n\n        # not equal, because Tcmb0 is changed, which also changes m_nu\n        assert got != cosmo\n        assert got.Tcmb0 == cosmo_cls.parameters[\"Tcmb0\"].default\n        assert got.clone(name=cosmo.name, Tcmb0=cosmo.Tcmb0, m_nu=cosmo.m_nu) == cosmo\n        # but the metadata is the same\n        # assert got.meta == cosmo.meta # metadata read not implemented\n\n    @pytest.mark.skipif(not HAS_BS4, reason=\"requires beautifulsoup4\")\n    def test_readwrite_html_mutlirow(self, cos"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "test_html.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "0}$$\" if latex_names else \"Tcmb0\"\n        del tbl[cname]  # format is not converted to original units\n        tbl.write(fp, overwrite=True)\n\n        # read with the same class that wrote fills in the missing info with\n        # the default value\n        got = cosmo_cls.read(fp, format=\"ascii.html\")\n        got2 = read(fp, format=\"ascii.html\", cosmology=cosmo_cls)\n        got3 = read(fp, format=\"ascii.html\", cosmology=cosmo_cls.__qualname__)\n\n        assert (got == got2) and (got2 == got3)  # internal consistency\n\n        # not equal, because Tcmb0 is changed, which also changes m_nu\n        assert got != cosmo\n        assert got.Tcmb0 == cosmo_cls.parameters[\"Tcmb0\"].default\n        assert got.clone(name=cosmo.name, Tcmb0=cosmo.Tcmb0, m_nu=cosmo.m_nu) == cosmo\n        # but the metadata is the same\n        # assert got.meta == cosmo.meta # metadata read not implemented\n\n    @pytest.mark.skipif(not HAS_BS4, reason=\"requires beautifulsoup4\")\n    def test_readwrite_html_mutlirow(self, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test if table has multiple rows.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_mutlirow.html\"\n\n        # Make\n        cosmo1 = cosmo.clone(name=\"row 0\")\n        cosmo2 = cosmo.clone(name=\"row 2\")\n        table = vstack(\n            [c.to_format(\"astropy.table\") for c in (cosmo1, cosmo, cosmo2)],\n            metadata_conflicts=\"silent\",\n        )\n\n        cosmo_cls = type(cosmo)\n        assert cosmo is not None\n\n        for n, col in zip(table.colnames, table.itercols()):\n            if n not in cosmo_cls.parameters:\n                continue\n            param = cosmo_cls.parameters[n]\n            if param.unit in (None, u.one):\n                continue\n            # Replace column with unitless version\n            table.replace_column(n, (col << param.unit).value, copy=False)\n\n        table.write(fp, format=\"ascii.html\")\n\n        # ------------\n        # From Table\n\n        # it will error on a multi-row table\n        with pytest.raises(V"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_html.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\nimport pytest\n\nimport astropy.units as u\nfrom astropy.cosmology._src.io.builtin.html import (\n    _FORMAT_TABLE,\n    read_html_table,\n    write_html_table,\n)\nfrom astropy.table import QTable, Table, vstack\nfrom astropy.utils.compat.optional_deps import HAS_BS4\n\nfrom .base import ReadWriteDirectTestBase, ReadWriteTestMixinBase\n\n###############################################################################\n\n\nclass ReadWriteHTMLTestMixin(ReadWriteTestMixinBase):\n    \"\"\"\n    Tests for a Cosmology[Read/Write] with ``format=\"ascii.html\"``.\n    This class will not be directly called by :mod:`pytest` since its name does\n    not begin with ``Test``. To activate the contained tests this class must\n    be inherited in a subclass. Subclasses must define a :func:`pytest.fixture`\n    ``cosmo`` that returns/yields an instance of a |Cosmology|.\n    See ``TestCosmology`` for an example.\n    \"\"\"\n\n    @pytest.mark.skipif(not HAS_BS4, reason=\"requires beautifulsoup4\")\n    def test_to_html_table_bad_index(self, read, write, tmp_path):\n        \"\"\"Test if argument ``index`` is incorrect\"\"\"\n        fp = tmp_path / \"test_to_html_table_bad_index.html\"\n\n        write(fp, format=\"ascii.html\")\n\n        # single-row table and has a non-0/None index\n        with pytest.raises(IndexError, match=\"index 2 out of range\"):\n            read(fp, index=2, format=\"ascii.html\")\n\n        # string index where doesn't match\n        with pytest.raises(KeyError, match=\"No matches found for key\"):\n            read(fp, index=\"row 0\", format=\"ascii.html\")\n\n    # -----------------------\n\n    @pytest.mark.skipif(not HAS_BS4, reason=\"requires beautifulsoup4\")\n    def test_to_html_table_failed_cls(self, write, tmp_path):\n        \"\"\"Test failed table type.\"\"\"\n        fp = tmp_path / \"test_to_html_table_failed_cls.html\"\n\n        with pytest.raises(TypeError, match=\"'cls' must be\"):\n            write(fp, format=\"ascii.html\", cls=list)\n\n    @pytest.mark.par"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_html.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ametrize(\"tbl_cls\", [QTable, Table])\n    @pytest.mark.skipif(not HAS_BS4, reason=\"requires beautifulsoup4\")\n    def test_to_html_table_cls(self, write, tbl_cls, tmp_path):\n        fp = tmp_path / \"test_to_html_table_cls.html\"\n        write(fp, format=\"ascii.html\", cls=tbl_cls)\n\n    # -----------------------\n\n    @pytest.mark.skipif(not HAS_BS4, reason=\"requires beautifulsoup4\")\n    def test_readwrite_html_table_instance(\n        self, cosmo_cls, cosmo, read, write, tmp_path, add_cu\n    ):\n        \"\"\"Test cosmology -> ascii.html -> cosmology.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_table_instance.html\"\n\n        # ------------\n        # To Table\n\n        write(fp, format=\"ascii.html\")\n\n        # some checks on the saved file\n        tbl = QTable.read(fp)\n        # assert tbl.meta[\"cosmology\"] == cosmo_cls.__qualname__  # metadata read not implemented\n        assert tbl[\"name\"] == cosmo.name\n\n        # ------------\n        # From Table\n\n        tbl[\"mismatching\"] = \"will error\"\n        tbl.write(fp, format=\"ascii.html\", overwrite=True)\n\n        # tests are different if the last argument is a **kwarg\n        if cosmo._init_has_kwargs:\n            got = read(fp, format=\"ascii.html\")\n\n            assert got.__class__ is cosmo_cls\n            assert got.name == cosmo.name\n            # assert \"mismatching\" not in got.meta # metadata read not implemented\n\n            return  # don't continue testing\n\n        # read with mismatching parameters errors\n        with pytest.raises(TypeError, match=\"there are unused parameters\"):\n            read(fp, format=\"ascii.html\")\n\n        # unless mismatched are moved to meta\n        got = read(fp, format=\"ascii.html\", move_to_meta=True)\n        assert got == cosmo\n        # assert got.meta[\"mismatching\"] == \"will error\" # metadata read not implemented\n\n        # it won't error if everything matches up\n        tbl.remove_column(\"mismatching\")\n        tbl.write(fp, format=\"ascii.html\", overwrite=True)\n        got = read(fp, format="}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "test_html.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "alueError, match=\"need to select a specific row\"):\n            read(fp, format=\"ascii.html\")\n\n        # unless the index argument is provided\n        got = cosmo_cls.read(fp, index=1, format=\"ascii.html\")\n        # got = read(fp, index=1, format=\"ascii.html\")\n        assert got == cosmo\n\n        # the index can be a string\n        got = cosmo_cls.read(fp, index=cosmo.name, format=\"ascii.html\")\n        assert got == cosmo\n\n        # it's better if the table already has an index\n        # this will be identical to the previous ``got``\n        table.add_index(\"name\")\n        got2 = cosmo_cls.read(fp, index=cosmo.name, format=\"ascii.html\")\n        assert got2 == cosmo\n\n\nclass TestReadWriteHTML(ReadWriteDirectTestBase, ReadWriteHTMLTestMixin):\n    \"\"\"\n    Directly test ``read/write_html``.\n    These are not public API and are discouraged from use, in favor of\n    ``Cosmology.read/write(..., format=\"ascii.html\")``, but should be\n    tested regardless b/c they are used internally.\n    \"\"\"\n\n    def setup_class(self):\n        self.functions = {\"read\": read_html_table, \"write\": write_html_table}\n\n    @pytest.mark.skipif(not HAS_BS4, reason=\"requires beautifulsoup4\")\n    def test_rename_direct_html_table_columns(self, read, write, tmp_path):\n        \"\"\"Tests renaming columns\"\"\"\n\n        fp = tmp_path / \"test_rename_html_table_columns.html\"\n\n        write(fp, format=\"ascii.html\", latex_names=True)\n\n        tbl = QTable.read(fp)\n\n        # asserts each column name has not been reverted yet\n        for column_name in tbl.colnames[2:]:\n            # for now, Cosmology as metadata and name is stored in first 2 slots\n            assert column_name in _FORMAT_TABLE.values()\n\n        cosmo = read(fp, format=\"ascii.html\")\n        converted_tbl = cosmo.to_format(\"astropy.table\")\n\n        # asserts each column name has been reverted\n        for column_name in converted_tbl.colnames[1:]:\n            # for now now, metadata is still stored in first slot\n            assert column_name in _F"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_connect.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "to/from formats. Unfortunately this is NOT\n# automatic since the output format class is not stored on the registry.\n#                 (format, data type)\ntofrom_formats = [\n    (\"mapping\", dict),\n    (\"yaml\", str),\n    (\"astropy.cosmology\", Cosmology),\n    (\"astropy.row\", Row),\n    (\"astropy.table\", QTable),\n]\n\n\n###############################################################################\n\n\nclass ReadWriteTestMixin(\n    test_ecsv.ReadWriteECSVTestMixin,\n    test_html.ReadWriteHTMLTestMixin,\n    test_json.ReadWriteJSONTestMixin,\n    test_latex.WriteLATEXTestMixin,\n):\n    \"\"\"\n    Tests for a CosmologyRead/Write on a |Cosmology|.\n    This class will not be directly called by :mod:`pytest` since its name does\n    not begin with ``Test``. To activate the contained tests this class must\n    be inherited in a subclass. Subclasses must define a :func:`pytest.fixture`\n    ``cosmo`` that returns/yields an instance of a |Cosmology|.\n    See ``TestReadWriteCosmology`` or ``TestCosmology`` for examples.\n    \"\"\"\n\n    @pytest.mark.parametrize(\"format, metaio, has_deps\", readwrite_formats)\n    def test_readwrite_complete_info(self, cosmo, tmp_path, format, metaio, has_deps):\n        \"\"\"\n        Test writing from an instance and reading from the base class.\n        This requires full information.\n        The round-tripped metadata can be in a different order, so the\n        OrderedDict must be converted to a dict before testing equality.\n        \"\"\"\n        if not has_deps:\n            pytest.skip(\"missing a dependency\")\n        if (format, Cosmology) not in readwrite_registry._readers:\n            pytest.xfail(f\"no read method is registered for format {format!r}\")\n\n        fname = tmp_path / f\"{cosmo.name}.{format}\"\n        cosmo.write(fname, format=format)\n\n        # Also test kwarg \"overwrite\"\n        assert fname.is_file()\n        with pytest.raises(IOError):\n            cosmo.write(fname, format=format, overwrite=False)\n\n        assert fname.exists()  # overwrite file existin"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_connect.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\nimport inspect\nimport sys\n\nimport pytest\n\nfrom astropy import cosmology\nfrom astropy.cosmology import Cosmology, w0wzCDM\nfrom astropy.cosmology._src.tests.io import (\n    test_cosmology,\n    test_ecsv,\n    test_html,\n    test_json,\n    test_latex,\n    test_mapping,\n    test_model,\n    test_row,\n    test_table,\n    test_yaml,\n)\nfrom astropy.cosmology.io import readwrite_registry\nfrom astropy.table import QTable, Row\nfrom astropy.utils.compat.optional_deps import HAS_BS4\n\n###############################################################################\n# SETUP\n\ncosmo_instances = cosmology.realizations.available\n\n# Collect the registered read/write formats.\n#   (format, supports_metadata, has_all_required_dependencies)\nreadwrite_formats = [\n    (\"ascii.ecsv\", True, True),\n    (\"ascii.html\", False, HAS_BS4),\n    (\"ascii.latex\", False, True),\n    (\"json\", True, True),\n    (\"latex\", False, True),\n]\n\n\n# Collect all the registered to/from formats. Unfortunately this is NOT\n# automatic since the output format class is not stored on the registry.\n#                 (format, data type)\ntofrom_formats = [\n    (\"mapping\", dict),\n    (\"yaml\", str),\n    (\"astropy.cosmology\", Cosmology),\n    (\"astropy.row\", Row),\n    (\"astropy.table\", QTable),\n]\n\n\n###############################################################################\n\n\nclass ReadWriteTestMixin(\n    test_ecsv.ReadWriteECSVTestMixin,\n    test_html.ReadWriteHTMLTestMixin,\n    test_json.ReadWriteJSONTestMixin,\n    test_latex.WriteLATEXTestMixin,\n):\n    \"\"\"\n    Tests for a CosmologyRead/Write on a |Cosmology|.\n    This class will not be directly called by :mod:`pytest` since its name does\n    not begin with ``Test``. To activate the contained tests this class must\n    be inherited in a subclass. Subclasses must define a :func:`pytest.fixture`\n    ``cosmo`` that returns/yields an instance of a |Cosmology|.\n    See ``TestReadWriteCosmology`` or ``TestCosmology`` for ex"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "test_html.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "mo, read, write, tmp_path, add_cu):\n        \"\"\"Test if table has multiple rows.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_mutlirow.html\"\n\n        # Make\n        cosmo1 = cosmo.clone(name=\"row 0\")\n        cosmo2 = cosmo.clone(name=\"row 2\")\n        table = vstack(\n            [c.to_format(\"astropy.table\") for c in (cosmo1, cosmo, cosmo2)],\n            metadata_conflicts=\"silent\",\n        )\n\n        cosmo_cls = type(cosmo)\n        assert cosmo is not None\n\n        for n, col in zip(table.colnames, table.itercols()):\n            if n not in cosmo_cls.parameters:\n                continue\n            param = cosmo_cls.parameters[n]\n            if param.unit in (None, u.one):\n                continue\n            # Replace column with unitless version\n            table.replace_column(n, (col << param.unit).value, copy=False)\n\n        table.write(fp, format=\"ascii.html\")\n\n        # ------------\n        # From Table\n\n        # it will error on a multi-row table\n        with pytest.raises(ValueError, match=\"need to select a specific row\"):\n            read(fp, format=\"ascii.html\")\n\n        # unless the index argument is provided\n        got = cosmo_cls.read(fp, index=1, format=\"ascii.html\")\n        # got = read(fp, index=1, format=\"ascii.html\")\n        assert got == cosmo\n\n        # the index can be a string\n        got = cosmo_cls.read(fp, index=cosmo.name, format=\"ascii.html\")\n        assert got == cosmo\n\n        # it's better if the table already has an index\n        # this will be identical to the previous ``got``\n        table.add_index(\"name\")\n        got2 = cosmo_cls.read(fp, index=cosmo.name, format=\"ascii.html\")\n        assert got2 == cosmo\n\n\nclass TestReadWriteHTML(ReadWriteDirectTestBase, ReadWriteHTMLTestMixin):\n    \"\"\"\n    Directly test ``read/write_html``.\n    These are not public API and are discouraged from use, in favor of\n    ``Cosmology.read/write(..., format=\"ascii.html\")``, but should be\n    tested regardless b/c they are used internally.\n    \"\"\"\n\n  "}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "html.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/io/builtin", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " values, filling in\n        any non-mandatory arguments missing in 'table'.\n    latex_names : bool, optional keyword-only\n        Whether the |Table| (might) have latex column names for the parameters that need\n        to be mapped to the correct parameter name -- e.g. $$H_{0}$$ to 'H0'. This is\n        `True` by default, but can be turned off (set to `False`) if there is a known\n        name conflict (e.g. both an 'H0' and '$$H_{0}$$' column) as this will raise an\n        error. In this case, the correct name ('H0') is preferred.\n    **kwargs : Any\n        Passed to ``QTable.read``. ``format`` is set to 'ascii.html', regardless of\n        input.\n\n    Returns\n    -------\n    |Cosmology| subclass instance\n\n    Raises\n    ------\n    ValueError\n        If the keyword argument 'format' is given and is not \"ascii.html\".\n    \"\"\"\n    # Check that the format is 'ascii.html' (or not specified)\n    format = kwargs.pop(\"format\", \"ascii.html\")\n    if format != \"ascii.html\":\n        raise ValueError(f\"format must be 'ascii.html', not {format}\")\n\n    # Reading is handled by `QTable`.\n    with u.add_enabled_units(cu):  # (cosmology units not turned on by default)\n        table = QTable.read(filename, format=\"ascii.html\", **kwargs)\n\n    # Need to map the table's column names to Cosmology inputs (parameter\n    # names).\n    # TODO! move the `latex_names` into `from_table`\n    if latex_names:\n        table_columns = set(table.colnames)\n        for name, latex in _FORMAT_TABLE.items():\n            if latex in table_columns:\n                table.rename_column(latex, name)\n\n    # Build the cosmology from table, using the private backend.\n    return from_table(\n        table, index=index, move_to_meta=move_to_meta, cosmology=cosmology, rename=None\n    )\n\n\ndef write_html_table(\n    cosmology: Cosmology,\n    file: PathLike | WriteableFileLike[_TableT],\n    *,\n    overwrite: bool = False,\n    cls: type[_TableT] = QTable,\n    latex_names: bool = False,\n    **kwargs: Any,\n) -> None:\n    r\"\"\""}, {"start_line": 11000, "end_line": 12394, "belongs_to": {"file_name": "html.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/io/builtin", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "        param = cosmo_cls.parameters.get(name)\n        if not isinstance(param, Parameter) or param.unit in (None, u.one):\n            continue\n        # Replace column with unitless version\n        table.replace_column(name, (col << param.unit).value, copy=False)\n\n    if latex_names:\n        new_names = [_FORMAT_TABLE.get(k, k) for k in cosmology.parameters]\n        table.rename_columns(tuple(cosmology.parameters), new_names)\n\n    # Write HTML, using table I/O\n    table.write(file, overwrite=overwrite, format=\"ascii.html\", **kwargs)\n\n\ndef html_identify(\n    origin: object, filepath: object, *args: object, **kwargs: object\n) -> bool:\n    \"\"\"Identify if an object uses the HTML Table format.\n\n    Parameters\n    ----------\n    origin : object\n        Not used.\n    filepath : object\n        From where to read the Cosmology.\n    *args : object\n        Not used.\n    **kwargs : object\n        Not used.\n\n    Returns\n    -------\n    bool\n        If the filepath is a string ending with '.html'.\n    \"\"\"\n    return isinstance(filepath, str) and filepath.endswith(\".html\")\n\n\n# ===================================================================\n# Register\n\nreadwrite_registry.register_reader(\"ascii.html\", Cosmology, read_html_table)\nreadwrite_registry.register_writer(\"ascii.html\", Cosmology, write_html_table)\nreadwrite_registry.register_identifier(\"ascii.html\", Cosmology, html_identify)\n"}], "retrieved_count": 10, "cost_time": 1.1235592365264893}
{"question": "Why does the base class for non-corrupted header data units that enable checksum verification exist in the FITS header data unit hierarchy?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 31000, "end_line": 33000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/hdu", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "not corrupted.\n    \"\"\"\n\n    def __init__(self, data=None, header=None, name=None, ver=None, **kwargs):\n        super().__init__(data=data, header=header)\n\n        if header is not None and not isinstance(header, (Header, _BasicHeader)):\n            # TODO: Instead maybe try initializing a new Header object from\n            # whatever is passed in as the header--there are various types\n            # of objects that could work for this...\n            raise ValueError(\"header must be a Header object\")\n\n        # NOTE:  private data members _checksum and _datasum are used by the\n        # utility script \"fitscheck\" to detect missing checksums.\n        self._checksum = None\n        self._checksum_valid = None\n        self._datasum = None\n        self._datasum_valid = None\n\n        if name is not None:\n            self.name = name\n        if ver is not None:\n            self.ver = ver\n\n    @classmethod\n    def match_header(cls, header):\n        \"\"\"\n        Matches any HDU that is not recognized as having either the SIMPLE or\n        XTENSION keyword in its header's first card, but is nonetheless not\n        corrupted.\n\n        TODO: Maybe it would make more sense to use _NonstandardHDU in this\n        case?  Not sure...\n        \"\"\"\n        return first(header.keys()) not in (\"SIMPLE\", \"XTENSION\")\n\n    @property\n    def size(self):\n        \"\"\"\n        Size (in bytes) of the data portion of the HDU.\n        \"\"\"\n        return self._header.data_size\n\n    def filebytes(self):\n        \"\"\"\n        Calculates and returns the number of bytes that this HDU will write to\n        a file.\n        \"\"\"\n        f = _File()\n        # TODO: Fix this once new HDU writing API is settled on\n        return self._writeheader(f)[1] + self._writedata(f)[1]\n\n    def fileinfo(self):\n        \"\"\"\n        Returns a dictionary detailing information about the locations\n        of this HDU within any associated file.  The values are only\n        valid after a read or write of the associated file with no"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/hdu", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "           continue\n            except Exception as exc:\n                warnings.warn(\n                    \"An exception occurred matching an HDU header to the \"\n                    f\"appropriate HDU type: {exc}\",\n                    AstropyUserWarning,\n                )\n                warnings.warn(\n                    \"The HDU will be treated as corrupted.\", AstropyUserWarning\n                )\n                klass = _CorruptedHDU\n                del exc\n                break\n\n    return klass\n\n\n# TODO: Come up with a better __repr__ for HDUs (and for HDULists, for that\n# matter)\nclass _BaseHDU:\n    \"\"\"Base class for all HDU (header data unit) classes.\"\"\"\n\n    _hdu_registry = set()\n\n    # This HDU type is part of the FITS standard\n    _standard = True\n\n    # Byte to use for padding out blocks\n    _padding_byte = \"\\x00\"\n\n    _default_name = \"\"\n\n    # _header uses a descriptor to delay the loading of the fits.Header object\n    # until it is necessary.\n    _header = _DelayedHeader()\n\n    def __init__(self, data=None, header=None, *args, **kwargs):\n        if header is None:\n            header = Header()\n        self._header = header\n        self._header_str = None\n        self._file = None\n        self._buffer = None\n        self._header_offset = None\n        self._data_offset = None\n        self._data_size = None\n\n        # This internal variable is used to track whether the data attribute\n        # still points to the same data array as when the HDU was originally\n        # created (this does not track whether the data is actually the same\n        # content-wise)\n        self._data_replaced = False\n        self._data_needs_rescale = False\n        self._new = True\n        self._output_checksum = False\n\n        if \"DATASUM\" in self._header and \"CHECKSUM\" not in self._header:\n            self._output_checksum = \"datasum\"\n        elif \"CHECKSUM\" in self._header:\n            self._output_checksum = True\n\n    def __init_subclass__(cls, **kwargs):\n        # Add the sam"}, {"start_line": 27000, "end_line": 29000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/hdu", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " have\n# been using this directly:\n_AllHDU = _BaseHDU\n\n# For convenience...\n# TODO: register_hdu could be made into a class decorator which would be pretty\n# cool, but only once 2.6 support is dropped.\nregister_hdu = _BaseHDU.register_hdu\nunregister_hdu = _BaseHDU.unregister_hdu\n\n\nclass _CorruptedHDU(_BaseHDU):\n    \"\"\"\n    A Corrupted HDU class.\n\n    This class is used when one or more mandatory `Card`s are\n    corrupted (unparsable), such as the ``BITPIX``, ``NAXIS``, or\n    ``END`` cards.  A corrupted HDU usually means that the data size\n    cannot be calculated or the ``END`` card is not found.  In the case\n    of a missing ``END`` card, the `Header` may also contain the binary\n    data\n\n    .. note::\n       In future, it may be possible to decipher where the last block\n       of the `Header` ends, but this task may be difficult when the\n       extension is a `TableHDU` containing ASCII data.\n    \"\"\"\n\n    @property\n    def size(self):\n        \"\"\"\n        Returns the size (in bytes) of the HDU's data part.\n        \"\"\"\n        # Note: On compressed files this might report a negative size; but the\n        # file is corrupt anyways so I'm not too worried about it.\n        if self._buffer is not None:\n            return len(self._buffer) - self._data_offset\n\n        return self._file.size - self._data_offset\n\n    def _summary(self):\n        return (self.name, self.ver, \"CorruptedHDU\")\n\n    def verify(self):\n        pass\n\n\nclass _NonstandardHDU(_BaseHDU, _Verify):\n    \"\"\"\n    A Non-standard HDU class.\n\n    This class is used for a Primary HDU when the ``SIMPLE`` Card has\n    a value of `False`.  A non-standard HDU comes from a file that\n    resembles a FITS file but departs from the standards in some\n    significant way.  One example would be files where the numbers are\n    in the DEC VAX internal storage format rather than the standard\n    FITS most significant byte first.  The header for this HDU should\n    be valid.  The data for this HDU is read from the file as a b"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/hdu", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n    def __init__(self, data=None, header=None, *args, **kwargs):\n        if header is None:\n            header = Header()\n        self._header = header\n        self._header_str = None\n        self._file = None\n        self._buffer = None\n        self._header_offset = None\n        self._data_offset = None\n        self._data_size = None\n\n        # This internal variable is used to track whether the data attribute\n        # still points to the same data array as when the HDU was originally\n        # created (this does not track whether the data is actually the same\n        # content-wise)\n        self._data_replaced = False\n        self._data_needs_rescale = False\n        self._new = True\n        self._output_checksum = False\n\n        if \"DATASUM\" in self._header and \"CHECKSUM\" not in self._header:\n            self._output_checksum = \"datasum\"\n        elif \"CHECKSUM\" in self._header:\n            self._output_checksum = True\n\n    def __init_subclass__(cls, **kwargs):\n        # Add the same data.deleter to all HDUs with a data property.\n        # It's unfortunate, but there's otherwise no straightforward way\n        # that a property can inherit setters/deleters of the property of the\n        # same name on base classes.\n        data_prop = cls.__dict__.get(\"data\", None)\n        if isinstance(data_prop, (lazyproperty, property)) and data_prop.fdel is None:\n            # Don't do anything if the class has already explicitly\n            # set the deleter for its data property\n            def data(self):\n                # The deleter\n                if self._file is not None and self._data_loaded:\n                    # sys.getrefcount is CPython specific and not on PyPy.\n                    has_getrefcount = hasattr(sys, \"getrefcount\")\n                    if has_getrefcount:\n                        data_refcount = sys.getrefcount(self.data)\n\n                    # Manually delete *now* so that FITS_rec.__del__\n                    # cleanup can happen if applicable\n         "}, {"start_line": 30000, "end_line": 32000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/hdu", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  offset = 0\n        size = 0\n\n        fileobj.flush()\n        try:\n            offset = fileobj.tell()\n        except OSError:\n            offset = 0\n\n        if self.data is not None:\n            fileobj.write(self.data)\n            # flush, to make sure the content is written\n            fileobj.flush()\n            size = len(self.data)\n\n        # return both the location and the size of the data area\n        return offset, size\n\n    def _summary(self):\n        return (self.name, self.ver, \"NonstandardHDU\", len(self._header))\n\n    @lazyproperty\n    def data(self):\n        \"\"\"\n        Return the file data.\n        \"\"\"\n        return self._get_raw_data(self.size, \"ubyte\", self._data_offset)\n\n    def _verify(self, option=\"warn\"):\n        errs = _ErrList([], unit=\"Card\")\n\n        # verify each card\n        for card in self._header.cards:\n            errs.append(card._verify(option))\n\n        return errs\n\n\nclass _ValidHDU(_BaseHDU, _Verify):\n    \"\"\"\n    Base class for all HDUs which are not corrupted.\n    \"\"\"\n\n    def __init__(self, data=None, header=None, name=None, ver=None, **kwargs):\n        super().__init__(data=data, header=header)\n\n        if header is not None and not isinstance(header, (Header, _BasicHeader)):\n            # TODO: Instead maybe try initializing a new Header object from\n            # whatever is passed in as the header--there are various types\n            # of objects that could work for this...\n            raise ValueError(\"header must be a Header object\")\n\n        # NOTE:  private data members _checksum and _datasum are used by the\n        # utility script \"fitscheck\" to detect missing checksums.\n        self._checksum = None\n        self._checksum_valid = None\n        self._datasum = None\n        self._datasum_valid = None\n\n        if name is not None:\n            self.name = name\n        if ver is not None:\n            self.ver = ver\n\n    @classmethod\n    def match_header(cls, header):\n        \"\"\"\n        Matches any HDU that is not recogni"}, {"start_line": 26000, "end_line": 28000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/hdu", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ap = None\n                else:\n                    array_mmap = _get_array_mmap(self.data)\n\n                if array_mmap is not None:\n                    array_mmap.flush()\n                else:\n                    self._file.seek(self._data_offset)\n                    datloc, datsize = self._writedata(fileobj)\n        elif copy:\n            datsize = self._writedata_direct_copy(fileobj)\n\n        self._header_offset = hdrloc\n        self._data_offset = datloc\n        self._data_size = datsize\n        self._data_replaced = False\n\n    def _close(self, closed=True):\n        # If the data was mmap'd, close the underlying mmap (this will\n        # prevent any future access to the .data attribute if there are\n        # not other references to it; if there are other references then\n        # it is up to the user to clean those up\n        if closed and self._data_loaded and _get_array_mmap(self.data) is not None:\n            del self.data\n\n\n# For backwards-compatibility, though nobody should have\n# been using this directly:\n_AllHDU = _BaseHDU\n\n# For convenience...\n# TODO: register_hdu could be made into a class decorator which would be pretty\n# cool, but only once 2.6 support is dropped.\nregister_hdu = _BaseHDU.register_hdu\nunregister_hdu = _BaseHDU.unregister_hdu\n\n\nclass _CorruptedHDU(_BaseHDU):\n    \"\"\"\n    A Corrupted HDU class.\n\n    This class is used when one or more mandatory `Card`s are\n    corrupted (unparsable), such as the ``BITPIX``, ``NAXIS``, or\n    ``END`` cards.  A corrupted HDU usually means that the data size\n    cannot be calculated or the ``END`` card is not found.  In the case\n    of a missing ``END`` card, the `Header` may also contain the binary\n    data\n\n    .. note::\n       In future, it may be possible to decipher where the last block\n       of the `Header` ends, but this task may be difficult when the\n       extension is a `TableHDU` containing ASCII data.\n    \"\"\"\n\n    @property\n    def size(self):\n        \"\"\"\n        Returns the size (in bytes) o"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/hdu", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     end card the end of the header may be ambiguous and resulted in a\n            corrupt HDU.  In this case the assumption is that the first 2880\n            block that does not begin with valid FITS header data is the\n            beginning of the data.\n\n        **kwargs : optional\n            May consist of additional keyword arguments specific to an HDU\n            type--these correspond to keywords recognized by the constructors of\n            different HDU classes such as `PrimaryHDU`, `ImageHDU`, or\n            `BinTableHDU`.  Any unrecognized keyword arguments are simply\n            ignored.\n        \"\"\"\n        return cls._readfrom_internal(\n            data, checksum=checksum, ignore_missing_end=ignore_missing_end, **kwargs\n        )\n\n    @classmethod\n    def readfrom(cls, fileobj, checksum=False, ignore_missing_end=False, **kwargs):\n        \"\"\"\n        Read the HDU from a file.  Normally an HDU should be opened with\n        :func:`open` which reads the entire HDU list in a FITS file.  But this\n        method is still provided for symmetry with :func:`writeto`.\n\n        Parameters\n        ----------\n        fileobj : file-like\n            Input FITS file.  The file's seek pointer is assumed to be at the\n            beginning of the HDU.\n\n        checksum : bool\n            If `True`, verifies that both ``DATASUM`` and ``CHECKSUM`` card\n            values (when present in the HDU header) match the header and data\n            of all HDU's in the file.\n\n        ignore_missing_end : bool\n            Do not issue an exception when opening a file that is missing an\n            ``END`` card in the last header.\n        \"\"\"\n        # TODO: Figure out a way to make it possible for the _File\n        # constructor to be a noop if the argument is already a _File\n        if not isinstance(fileobj, _File):\n            fileobj = _File(fileobj)\n\n        hdu = cls._readfrom_internal(\n            fileobj, checksum=checksum, ignore_missing_end=ignore_missing_end, **kwargs\n    "}, {"start_line": 32000, "end_line": 34000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/hdu", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "zed as having either the SIMPLE or\n        XTENSION keyword in its header's first card, but is nonetheless not\n        corrupted.\n\n        TODO: Maybe it would make more sense to use _NonstandardHDU in this\n        case?  Not sure...\n        \"\"\"\n        return first(header.keys()) not in (\"SIMPLE\", \"XTENSION\")\n\n    @property\n    def size(self):\n        \"\"\"\n        Size (in bytes) of the data portion of the HDU.\n        \"\"\"\n        return self._header.data_size\n\n    def filebytes(self):\n        \"\"\"\n        Calculates and returns the number of bytes that this HDU will write to\n        a file.\n        \"\"\"\n        f = _File()\n        # TODO: Fix this once new HDU writing API is settled on\n        return self._writeheader(f)[1] + self._writedata(f)[1]\n\n    def fileinfo(self):\n        \"\"\"\n        Returns a dictionary detailing information about the locations\n        of this HDU within any associated file.  The values are only\n        valid after a read or write of the associated file with no\n        intervening changes to the `HDUList`.\n\n        Returns\n        -------\n        dict or None\n            The dictionary details information about the locations of\n            this HDU within an associated file.  Returns `None` when\n            the HDU is not associated with a file.\n\n            Dictionary contents:\n\n            ========== ================================================\n            Key        Value\n            ========== ================================================\n            file       File object associated with the HDU\n            filemode   Mode in which the file was opened (readonly, copyonwrite,\n                       update, append, ostream)\n            hdrLoc     Starting byte location of header in file\n            datLoc     Starting byte location of data block in file\n            datSpan    Data size including padding\n            ========== ================================================\n        \"\"\"\n        if hasattr(self, \"_file\") and self._fi"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/hdu", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e.  Abstract types may choose to simply return False\n    or raise NotImplementedError to be skipped.\n\n    If any unexpected exceptions are raised while evaluating\n    match_header(), the type is taken to be _CorruptedHDU.\n\n    Used primarily by _BaseHDU._readfrom_internal and _BaseHDU._from_data to\n    find an appropriate HDU class to use based on values in the header.\n    \"\"\"\n    klass = cls  # By default, if no subclasses are defined\n    if header:\n        for c in reversed(list(itersubclasses(cls))):\n            try:\n                # HDU classes built into astropy.io.fits are always considered,\n                # but extension HDUs must be explicitly registered\n                if not (\n                    c.__module__.startswith(\"astropy.io.fits.\")\n                    or c in cls._hdu_registry\n                ):\n                    continue\n                if c.match_header(header):\n                    klass = c\n                    break\n            except NotImplementedError:\n                continue\n            except Exception as exc:\n                warnings.warn(\n                    \"An exception occurred matching an HDU header to the \"\n                    f\"appropriate HDU type: {exc}\",\n                    AstropyUserWarning,\n                )\n                warnings.warn(\n                    \"The HDU will be treated as corrupted.\", AstropyUserWarning\n                )\n                klass = _CorruptedHDU\n                del exc\n                break\n\n    return klass\n\n\n# TODO: Come up with a better __repr__ for HDUs (and for HDULists, for that\n# matter)\nclass _BaseHDU:\n    \"\"\"Base class for all HDU (header data unit) classes.\"\"\"\n\n    _hdu_registry = set()\n\n    # This HDU type is part of the FITS standard\n    _standard = True\n\n    # Byte to use for padding out blocks\n    _padding_byte = \"\\x00\"\n\n    _default_name = \"\"\n\n    # _header uses a descriptor to delay the loading of the fits.Header object\n    # until it is necessary.\n    _header = _DelayedHeader()\n"}, {"start_line": 55000, "end_line": 57000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/hdu", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "nHDU(_ValidHDU):\n    \"\"\"\n    An extension HDU class.\n\n    This class is the base class for the `TableHDU`, `ImageHDU`, and\n    `BinTableHDU` classes.\n    \"\"\"\n\n    _extension = \"\"\n\n    @classmethod\n    def match_header(cls, header):\n        \"\"\"\n        This class should never be instantiated directly.  Either a standard\n        extension HDU type should be used for a specific extension, or\n        NonstandardExtHDU should be used.\n        \"\"\"\n        raise NotImplementedError\n\n    def writeto(self, name, output_verify=\"exception\", overwrite=False, checksum=False):\n        \"\"\"\n        Works similarly to the normal writeto(), but prepends a default\n        `PrimaryHDU` are required by extension HDUs (which cannot stand on\n        their own).\n        \"\"\"\n        from .hdulist import HDUList\n        from .image import PrimaryHDU\n\n        hdulist = HDUList([PrimaryHDU(), self])\n        hdulist.writeto(name, output_verify, overwrite=overwrite, checksum=checksum)\n\n    def _verify(self, option=\"warn\"):\n        errs = super()._verify(option=option)\n\n        # Verify location and value of mandatory keywords.\n        naxis = self._header.get(\"NAXIS\", 0)\n        self.req_cards(\n            \"PCOUNT\", naxis + 3, lambda v: (_is_int(v) and v >= 0), 0, option, errs\n        )\n        self.req_cards(\n            \"GCOUNT\", naxis + 4, lambda v: (_is_int(v) and v == 1), 1, option, errs\n        )\n\n        return errs\n\n\nclass NonstandardExtHDU(ExtensionHDU):\n    \"\"\"\n    A Non-standard Extension HDU class.\n\n    This class is used for an Extension HDU when the ``XTENSION``\n    `Card` has a non-standard value.  In this case, Astropy can figure\n    out how big the data is but not what it is.  The data for this HDU\n    is read from the file as a byte stream that begins at the first\n    byte after the header ``END`` card and continues until the\n    beginning of the next header or the end of the file.\n    \"\"\"\n\n    _standard = False\n\n    @classmethod\n    def match_header(cls, header):\n        \"\"\"\n "}], "retrieved_count": 10, "cost_time": 1.1361215114593506}
{"question": "How does the test class that validates table initialization from heterogeneous column sources enforce separation between column name and type resolution logic and parent table reference assignment when creating tables from mixed column inputs?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 19000, "end_line": 21000, "belongs_to": {"file_name": "test_table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n\n        # Now test with an input column from another table\n        t2 = table_types.Table()\n        t2.add_column(t[\"a\"], name=\"c\")\n        assert t2.colnames == [\"c\"]\n        # Check that we did not change the name of the input column\n        assert t.colnames == [\"b\", \"a\"]\n\n        # Check that we can give a name if none was present\n        col = table_types.Column([1, 2, 3])\n        t.add_column(col, name=\"c\")\n        assert t.colnames == [\"b\", \"a\", \"c\"]\n\n    def test_default_name(self, table_types):\n        t = table_types.Table()\n        col = table_types.Column([1, 2, 3])\n        t.add_column(col)\n        assert t.colnames == [\"col0\"]\n\n    def test_setting_column_name_to_with_invalid_type(self, table_types):\n        t = table_types.Table()\n        t[\"a\"] = [1, 2]\n        with pytest.raises(\n            TypeError, match=r\"Expected a str value, got None with type NoneType\"\n        ):\n            t[\"a\"].name = None\n        assert t[\"a\"].name == \"a\"\n\n\n@pytest.mark.usefixtures(\"table_types\")\nclass TestInitFromTable(SetupData):\n    def test_from_table_cols(self, table_types):\n        \"\"\"Ensure that using cols from an existing table gives\n        a clean copy.\n        \"\"\"\n        self._setup(table_types)\n        t = self.t\n        cols = t.columns\n        # Construct Table with cols via Table._new_from_cols\n        t2a = table_types.Table([cols[\"a\"], cols[\"b\"], self.c])\n\n        # Construct with add_column\n        t2b = table_types.Table()\n        t2b.add_column(cols[\"a\"])\n        t2b.add_column(cols[\"b\"])\n        t2b.add_column(self.c)\n\n        t[\"a\"][1] = 20\n        t[\"b\"][1] = 21\n        for t2 in [t2a, t2b]:\n            t2[\"a\"][2] = 10\n            t2[\"b\"][2] = 11\n            t2[\"c\"][2] = 12\n            t2.columns[\"a\"].meta[\"aa\"][3] = 10\n            assert np.all(t[\"a\"] == np.array([1, 20, 3]))\n            assert np.all(t[\"b\"] == np.array([4, 21, 6]))\n            assert np.all(t2[\"a\"] == np.array([1, 2, 10]))\n            assert np.all(t2[\"b\"] == np.array([4, 5, "}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "test_init_table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ", table_type):\n        self._setup(table_type)\n        t = table_type(self.data)\n        assert t.colnames == [\"x\", \"col1\", \"col2\"]\n        assert all(t[name].name == name for name in t.colnames)\n\n    def test_partial_names_dtype(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data, names=[\"b\", None, \"c\"], dtype=[\"f4\", None, \"f8\"])\n        assert t.colnames == [\"b\", \"col1\", \"c\"]\n        assert t[\"b\"].dtype.type == np.float32\n        assert t[\"col1\"].dtype.type == np.int32\n        assert t[\"c\"].dtype.type == np.float64\n        assert all(t[name].name == name for name in t.colnames)\n\n    def test_ref(self, table_type):\n        \"\"\"Test that initializing from a list of columns can be done by reference\"\"\"\n        self._setup(table_type)\n        t = table_type(self.data, copy=False)\n        t[\"x\"][0] = 100\n        assert self.data[0][0] == 100\n\n\n@pytest.mark.usefixtures(\"table_type\")\nclass TestInitFromNdarrayStruct(BaseInitFromDictLike):\n    def _setup(self, table_type):\n        self.data = np.array(\n            [(1, 2, 3), (3, 4, 5)], dtype=[(\"x\", \"i8\"), (\"y\", \"i4\"), (\"z\", \"i8\")]\n        )\n\n    def test_ndarray_ref(self, table_type):\n        \"\"\"Init with ndarray and copy=False and show that table uses reference\n        to input ndarray\"\"\"\n        self._setup(table_type)\n        t = table_type(self.data, copy=False)\n\n        t[\"x\"][1] = 0  # Column-wise assignment\n        t[0][\"y\"] = 0  # Row-wise assignment\n        assert self.data[\"x\"][1] == 0\n        assert self.data[\"y\"][0] == 0\n        assert np.all(np.array(t) == self.data)\n        assert all(t[name].name == name for name in t.colnames)\n\n    def test_partial_names_dtype(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data, names=[\"e\", None, \"d\"], dtype=[\"f4\", None, \"f8\"])\n        assert t.colnames == [\"e\", \"y\", \"d\"]\n        assert t[\"e\"].dtype.type == np.float32\n        assert t[\"y\"].dtype.type == np.int32\n        assert t[\"d\"].dtype.type == np.float64\n  "}, {"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "test_init_table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " assert t[\"e\"].dtype.type == np.float32\n        assert t[\"y\"].dtype.type == np.int64\n        assert t[\"d\"].dtype.type == np.int64\n        assert all(t[name].name == name for name in t.colnames)\n\n    def test_partial_names_ref(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data, names=[\"e\", None, \"d\"], copy=False)\n        assert t.colnames == [\"e\", \"y\", \"d\"]\n        assert t[\"e\"].dtype.type == np.int64\n        assert t[\"y\"].dtype.type == np.int64\n        assert t[\"d\"].dtype.type == np.float64\n        assert all(t[name].name == name for name in t.colnames)\n\n    def test_init_from_columns(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data)\n        t2 = table_type(t.columns[\"z\", \"x\", \"y\"])\n        assert t2.colnames == [\"z\", \"x\", \"y\"]\n        assert t2.dtype.names == (\"z\", \"x\", \"y\")\n\n    def test_init_from_columns_slice(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data)\n        t2 = table_type(t.columns[0:2])\n        assert t2.colnames == [\"x\", \"y\"]\n        assert t2.dtype.names == (\"x\", \"y\")\n\n    def test_init_from_columns_mix(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data)\n        t2 = table_type([t.columns[0], t.columns[\"z\"]])\n        assert t2.colnames == [\"x\", \"z\"]\n        assert t2.dtype.names == (\"x\", \"z\")\n\n\n@pytest.mark.usefixtures(\"table_type\")\nclass TestInitFromNone:\n    # Note table_table.TestEmptyData tests initializing a completely empty\n    # table and adding data.\n\n    def test_data_none_with_cols(self, table_type):\n        \"\"\"\n        Test different ways of initing an empty table\n        \"\"\"\n        np_t = np.empty(0, dtype=[(\"a\", \"f4\", (2,)), (\"b\", \"i4\")])\n        for kwargs in (\n            {\"names\": (\"a\", \"b\")},\n            {\"names\": (\"a\", \"b\"), \"dtype\": ((\"f4\", (2,)), \"i4\")},\n            {\"dtype\": [(\"a\", \"f4\", (2,)), (\"b\", \"i4\")]},\n            {\"dtype\": np_t.dtype},\n        ):\n            t = table_type(**kwargs)\n   "}, {"start_line": 17000, "end_line": 19000, "belongs_to": {"file_name": "test_init_table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e(t.columns[0:2])\n        assert t2.colnames == [\"x\", \"y\"]\n        assert t2.dtype.names == (\"x\", \"y\")\n\n    def test_init_from_columns_mix(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data)\n        t2 = table_type([t.columns[0], t.columns[\"z\"]])\n        assert t2.colnames == [\"x\", \"z\"]\n        assert t2.dtype.names == (\"x\", \"z\")\n\n\n@pytest.mark.usefixtures(\"table_type\")\nclass TestInitFromNone:\n    # Note table_table.TestEmptyData tests initializing a completely empty\n    # table and adding data.\n\n    def test_data_none_with_cols(self, table_type):\n        \"\"\"\n        Test different ways of initing an empty table\n        \"\"\"\n        np_t = np.empty(0, dtype=[(\"a\", \"f4\", (2,)), (\"b\", \"i4\")])\n        for kwargs in (\n            {\"names\": (\"a\", \"b\")},\n            {\"names\": (\"a\", \"b\"), \"dtype\": ((\"f4\", (2,)), \"i4\")},\n            {\"dtype\": [(\"a\", \"f4\", (2,)), (\"b\", \"i4\")]},\n            {\"dtype\": np_t.dtype},\n        ):\n            t = table_type(**kwargs)\n            assert t.colnames == [\"a\", \"b\"]\n            assert len(t[\"a\"]) == 0\n            assert len(t[\"b\"]) == 0\n            if \"dtype\" in kwargs:\n                assert t[\"a\"].dtype.type == np.float32\n                assert t[\"b\"].dtype.type == np.int32\n                assert t[\"a\"].shape[1:] == (2,)\n\n\n@pytest.mark.usefixtures(\"table_types\")\nclass TestInitFromRows:\n    def test_init_with_rows(self, table_type):\n        for rows in ([[1, \"a\"], [2, \"b\"]], [(1, \"a\"), (2, \"b\")], ((1, \"a\"), (2, \"b\"))):\n            t = table_type(rows=rows, names=(\"a\", \"b\"))\n            assert np.all(t[\"a\"] == [1, 2])\n            assert np.all(t[\"b\"] == [\"a\", \"b\"])\n            assert t.colnames == [\"a\", \"b\"]\n            assert t[\"a\"].dtype.kind == \"i\"\n            assert t[\"b\"].dtype.kind in (\"S\", \"U\")\n            # Regression test for\n            # https://github.com/astropy/astropy/issues/3052\n            assert t[\"b\"].dtype.str.endswith(\"1\")\n\n        rows = np.arange(6).reshape(2, 3)\n        t = table_typ"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "test_init_table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ble_type):\n        self.data = np.array(\n            [(1, 2, 3), (3, 4, 5)], dtype=[(\"x\", \"i8\"), (\"y\", \"i4\"), (\"z\", \"i8\")]\n        )\n\n    def test_ndarray_ref(self, table_type):\n        \"\"\"Init with ndarray and copy=False and show that table uses reference\n        to input ndarray\"\"\"\n        self._setup(table_type)\n        t = table_type(self.data, copy=False)\n\n        t[\"x\"][1] = 0  # Column-wise assignment\n        t[0][\"y\"] = 0  # Row-wise assignment\n        assert self.data[\"x\"][1] == 0\n        assert self.data[\"y\"][0] == 0\n        assert np.all(np.array(t) == self.data)\n        assert all(t[name].name == name for name in t.colnames)\n\n    def test_partial_names_dtype(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data, names=[\"e\", None, \"d\"], dtype=[\"f4\", None, \"f8\"])\n        assert t.colnames == [\"e\", \"y\", \"d\"]\n        assert t[\"e\"].dtype.type == np.float32\n        assert t[\"y\"].dtype.type == np.int32\n        assert t[\"d\"].dtype.type == np.float64\n        assert all(t[name].name == name for name in t.colnames)\n\n    def test_partial_names_ref(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data, names=[\"e\", None, \"d\"], copy=False)\n        assert t.colnames == [\"e\", \"y\", \"d\"]\n        assert t[\"e\"].dtype.type == np.int64\n        assert t[\"y\"].dtype.type == np.int32\n        assert t[\"d\"].dtype.type == np.int64\n        assert all(t[name].name == name for name in t.colnames)\n\n\n@pytest.mark.usefixtures(\"table_type\")\nclass TestInitFromDict(BaseInitFromDictLike):\n    def _setup(self, table_type):\n        self.data = {\n            \"a\": Column([1, 3], name=\"x\"),\n            \"b\": [2, 4],\n            \"c\": np.array([3, 5], dtype=\"i8\"),\n        }\n\n\n@pytest.mark.usefixtures(\"table_type\")\nclass TestInitFromMapping(BaseInitFromDictLike):\n    def _setup(self, table_type):\n        self.data = UserDict(\n            [\n                (\"a\", Column([1, 3], name=\"x\")),\n                (\"b\", [2, 4]),\n                (\"c\", np."}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_init_table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ame] is col\n\n\n# pytest.mark.usefixtures('table_type')\nclass BaseInitFrom:\n    def _setup(self, table_type):\n        pass\n\n    def test_basic_init(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data, names=(\"a\", \"b\", \"c\"))\n        assert t.colnames == [\"a\", \"b\", \"c\"]\n        assert np.all(t[\"a\"] == np.array([1, 3]))\n        assert np.all(t[\"b\"] == np.array([2, 4]))\n        assert np.all(t[\"c\"] == np.array([3, 5]))\n        assert all(t[name].name == name for name in t.colnames)\n\n    def test_set_dtype(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data, names=(\"a\", \"b\", \"c\"), dtype=(\"i4\", \"f4\", \"f8\"))\n        assert t.colnames == [\"a\", \"b\", \"c\"]\n        assert np.all(t[\"a\"] == np.array([1, 3], dtype=\"i4\"))\n        assert np.all(t[\"b\"] == np.array([2, 4], dtype=\"f4\"))\n        assert np.all(t[\"c\"] == np.array([3, 5], dtype=\"f8\"))\n        assert t[\"a\"].dtype.type == np.int32\n        assert t[\"b\"].dtype.type == np.float32\n        assert t[\"c\"].dtype.type == np.float64\n        assert all(t[name].name == name for name in t.colnames)\n\n    def test_names_dtype_mismatch(self, table_type):\n        self._setup(table_type)\n        with pytest.raises(ValueError):\n            table_type(self.data, names=(\"a\",), dtype=(\"i4\", \"f4\", \"i4\"))\n\n    def test_names_cols_mismatch(self, table_type):\n        self._setup(table_type)\n        with pytest.raises(ValueError):\n            table_type(self.data, names=(\"a\",), dtype=\"i4\")\n\n\n@pytest.mark.usefixtures(\"table_type\")\nclass BaseInitFromListLike(BaseInitFrom):\n    def test_names_cols_mismatch(self, table_type):\n        self._setup(table_type)\n        with pytest.raises(ValueError):\n            table_type(self.data, names=[\"a\"], dtype=[int])\n\n    def test_names_copy_false(self, table_type):\n        self._setup(table_type)\n        with pytest.raises(ValueError):\n            table_type(self.data, names=[\"a\"], dtype=[int], copy=False)\n\n\n@pytest.mark.usefixtures(\"table_type\")\nclass Ba"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "test_table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " assert t[\"a\"].shape == (0,)\n        t[\"a\"] = 1.5\n        assert len(t) == 0\n        assert isinstance(t[\"a\"], Column)\n        assert t[\"a\"].shape == (0,)\n\n    def test_add_via_setitem_and_slice(self, table_types):\n        \"\"\"Test related to #3023 where a MaskedColumn is created with name=None\n        and then gets changed to name='a'.  After PR #2790 this test fails\n        without the #3023 fix.\"\"\"\n        t = table_types.Table()\n        t[\"a\"] = table_types.Column([1, 2, 3])\n        t2 = t[:]\n        assert t2.colnames == t.colnames\n\n\n@pytest.mark.usefixtures(\"table_types\")\nclass TestNewFromColumns:\n    def test_simple(self, table_types):\n        cols = [\n            table_types.Column(name=\"a\", data=[1, 2, 3]),\n            table_types.Column(name=\"b\", data=[4, 5, 6], dtype=np.float32),\n        ]\n        t = table_types.Table(cols)\n        assert np.all(t[\"a\"].data == np.array([1, 2, 3]))\n        assert np.all(t[\"b\"].data == np.array([4, 5, 6], dtype=np.float32))\n        assert type(t[\"b\"][1]) is np.float32\n\n    def test_from_np_array(self, table_types):\n        cols = [\n            table_types.Column(\n                name=\"a\", data=np.array([1, 2, 3], dtype=np.int64), dtype=np.float64\n            ),\n            table_types.Column(name=\"b\", data=np.array([4, 5, 6], dtype=np.float32)),\n        ]\n        t = table_types.Table(cols)\n        assert np.all(t[\"a\"] == np.array([1, 2, 3], dtype=np.float64))\n        assert np.all(t[\"b\"] == np.array([4, 5, 6], dtype=np.float32))\n        assert type(t[\"a\"][1]) is np.float64\n        assert type(t[\"b\"][1]) is np.float32\n\n    def test_size_mismatch(self, table_types):\n        cols = [\n            table_types.Column(name=\"a\", data=[1, 2, 3]),\n            table_types.Column(name=\"b\", data=[4, 5, 6, 7]),\n        ]\n        with pytest.raises(ValueError):\n            table_types.Table(cols)\n\n    def test_name_none(self, table_types):\n        \"\"\"Column with name=None can init a table whether or not names are supplied\"\"\"\n        c = "}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "test_init_table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "seInitFromDictLike(BaseInitFrom):\n    pass\n\n\n@pytest.mark.usefixtures(\"table_type\")\nclass TestInitFromNdarrayHomo(BaseInitFromListLike):\n    def setup_method(self, method):\n        self.data = np.array([(1, 2, 3), (3, 4, 5)], dtype=\"i4\")\n\n    def test_default_names(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data)\n        assert t.colnames == [\"col0\", \"col1\", \"col2\"]\n\n    def test_ndarray_ref(self, table_type):\n        \"\"\"Init with ndarray and copy=False and show that this is a reference\n        to input ndarray\"\"\"\n        self._setup(table_type)\n        t = table_type(self.data, copy=False)\n        t[\"col1\"][1] = 0\n        assert t.as_array()[\"col1\"][1] == 0\n        assert t[\"col1\"][1] == 0\n        assert self.data[1][1] == 0\n\n    def test_partial_names_dtype(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data, names=[\"a\", None, \"c\"], dtype=[None, None, \"f8\"])\n        assert t.colnames == [\"a\", \"col1\", \"c\"]\n        assert t[\"a\"].dtype.type == np.int32\n        assert t[\"col1\"].dtype.type == np.int32\n        assert t[\"c\"].dtype.type == np.float64\n        assert all(t[name].name == name for name in t.colnames)\n\n    def test_partial_names_ref(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data, names=[\"a\", None, \"c\"])\n        assert t.colnames == [\"a\", \"col1\", \"c\"]\n        assert t[\"a\"].dtype.type == np.int32\n        assert t[\"col1\"].dtype.type == np.int32\n        assert t[\"c\"].dtype.type == np.int32\n        assert all(t[name].name == name for name in t.colnames)\n\n\n@pytest.mark.usefixtures(\"table_type\")\nclass TestInitFromListOfLists(BaseInitFromListLike):\n    def setup_method(self, table_type):\n        self._setup(table_type)\n        self.data = [\n            (np.int32(1), np.int32(3)),\n            Column(name=\"col1\", data=[2, 4], dtype=np.int32),\n            np.array([3, 5], dtype=np.int32),\n        ]\n\n    def test_default_names(self, table_type):\n        self._setup(tab"}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "test_table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e_types\")\nclass TestInitFromTable(SetupData):\n    def test_from_table_cols(self, table_types):\n        \"\"\"Ensure that using cols from an existing table gives\n        a clean copy.\n        \"\"\"\n        self._setup(table_types)\n        t = self.t\n        cols = t.columns\n        # Construct Table with cols via Table._new_from_cols\n        t2a = table_types.Table([cols[\"a\"], cols[\"b\"], self.c])\n\n        # Construct with add_column\n        t2b = table_types.Table()\n        t2b.add_column(cols[\"a\"])\n        t2b.add_column(cols[\"b\"])\n        t2b.add_column(self.c)\n\n        t[\"a\"][1] = 20\n        t[\"b\"][1] = 21\n        for t2 in [t2a, t2b]:\n            t2[\"a\"][2] = 10\n            t2[\"b\"][2] = 11\n            t2[\"c\"][2] = 12\n            t2.columns[\"a\"].meta[\"aa\"][3] = 10\n            assert np.all(t[\"a\"] == np.array([1, 20, 3]))\n            assert np.all(t[\"b\"] == np.array([4, 21, 6]))\n            assert np.all(t2[\"a\"] == np.array([1, 2, 10]))\n            assert np.all(t2[\"b\"] == np.array([4, 5, 11]))\n            assert np.all(t2[\"c\"] == np.array([7, 8, 12]))\n            assert t2[\"a\"].name == \"a\"\n            assert t2.columns[\"a\"].meta[\"aa\"][3] == 10\n            assert t.columns[\"a\"].meta[\"aa\"][3] == 3\n\n\n@pytest.mark.usefixtures(\"table_types\")\nclass TestAddColumns(SetupData):\n    def test_add_columns1(self, table_types):\n        self._setup(table_types)\n        t = table_types.Table()\n        t.add_columns([self.a, self.b, self.c])\n        assert t.colnames == [\"a\", \"b\", \"c\"]\n\n    def test_add_columns2(self, table_types):\n        self._setup(table_types)\n        t = table_types.Table([self.a, self.b])\n        t.add_columns([self.c, self.d])\n        assert t.colnames == [\"a\", \"b\", \"c\", \"d\"]\n        assert np.all(t[\"c\"] == np.array([7, 8, 9]))\n\n    def test_add_columns3(self, table_types):\n        self._setup(table_types)\n        t = table_types.Table([self.a, self.b])\n        t.add_columns([self.c, self.d], indexes=[1, 0])\n        assert t.colnames == [\"d\", \"a\", \"c\", \"b\"]\n\n   "}, {"start_line": 55000, "end_line": 57000, "belongs_to": {"file_name": "table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "olumn):\n                col_cls = self.MaskedColumn\n            elif isinstance(col, Quantity) and not isinstance(col, Masked):\n                col_cls = Masked(col_cls)\n        else:\n            if isinstance(col, MaskedColumn):\n                if not isinstance(col, self.MaskedColumn):\n                    col_cls = self.MaskedColumn\n            elif isinstance(col, Column) and not isinstance(col, self.Column):\n                col_cls = self.Column\n\n        return col_cls\n\n    def _convert_col_for_table(self, col):\n        \"\"\"\n        Make sure that all Column objects have correct base class for this type of\n        Table.  For a base Table this most commonly means setting to\n        MaskedColumn if the table is masked.  Table subclasses like QTable\n        override this method.\n        \"\"\"\n        if isinstance(col, Column) and not isinstance(col, self.ColumnClass):\n            col_cls = self._get_col_cls_for_table(col)\n            if col_cls is not col.__class__:\n                col = col_cls(col, copy=COPY_IF_NEEDED)\n\n        return col\n\n    def _init_from_cols(self, cols):\n        \"\"\"Initialize table from a list of Column or mixin objects.\"\"\"\n        lengths = {len(col) for col in cols}\n        if len(lengths) > 1:\n            raise ValueError(f\"Inconsistent data column lengths: {lengths}\")\n\n        # Make sure that all Column-based objects have correct class.  For\n        # plain Table this is self.ColumnClass, but for instance QTable will\n        # convert columns with units to a Quantity mixin.\n        newcols = [self._convert_col_for_table(col) for col in cols]\n        self._make_table_from_cols(self, newcols)\n\n        # Deduplicate indices.  It may happen that after pickling or when\n        # initing from an existing table that column indices which had been\n        # references to a single index object got *copied* into an independent\n        # object.  This results in duplicates which will cause downstream problems.\n        index_dict = {}\n        for col"}], "retrieved_count": 10, "cost_time": 1.1476619243621826}
{"question": "How should the validation methods that orchestrate unit serialization and deserialization testing be refactored to separate the format-specific string conversion operations from the unit decomposition and scale comparison operations in the pytest test classes?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "test_format.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tsWarning, match=\"deprecated\") as w:\n                a = Unit(s, format=self.format_)\n            assert len(w) == 1\n        else:\n            a = Unit(s, format=self.format_)  # No warning\n\n        assert_allclose(a.decompose().scale, unit.decompose().scale, rtol=1e-9)\n\n    def check_roundtrip_decompose(self, unit):\n        ud = unit.decompose()\n        s = ud.to_string(self.format_)\n        assert \"  \" not in s\n        a = Unit(s, format=self.format_)\n        assert_allclose(a.decompose().scale, ud.scale, rtol=1e-5)\n\n\nclass TestRoundtripGeneric(RoundtripBase):\n    format_ = u_format.Generic\n\n    @pytest.mark.parametrize(\n        \"unit\",\n        [\n            unit\n            for unit in u.__dict__.values()\n            if (isinstance(unit, UnitBase) and not isinstance(unit, PrefixUnit))\n        ],\n        ids=str,\n    )\n    def test_roundtrip(self, unit):\n        self.check_roundtrip(unit)\n        self.check_roundtrip(unit, output_format=\"unicode\")\n        self.check_roundtrip_decompose(unit)\n\n\nclass TestRoundtripVOUnit(RoundtripBase):\n    format_ = u_format.VOUnit\n\n    @pytest.mark.parametrize(\n        \"unit\",\n        [u for u in u_format.VOUnit._units.values() if not isinstance(u, PrefixUnit)],\n        ids=str,\n    )\n    def test_roundtrip(self, unit):\n        self.check_roundtrip(unit)\n        if unit not in (u.mag, u.dB):\n            self.check_roundtrip_decompose(unit)\n\n\nclass TestRoundtripFITS(RoundtripBase):\n    format_ = u_format.FITS\n\n    @pytest.mark.parametrize(\n        \"unit\",\n        [u for u in u_format.FITS._units.values() if not isinstance(u, PrefixUnit)],\n        ids=str,\n    )\n    def test_roundtrip(self, unit):\n        self.check_roundtrip(unit)\n\n\nclass TestRoundtripCDS(RoundtripBase):\n    format_ = u_format.CDS\n\n    @pytest.mark.parametrize(\n        \"unit\",\n        [u for u in u_format.CDS._units.values() if not isinstance(u, PrefixUnit)],\n        ids=str,\n    )\n    def test_roundtrip(self, unit):\n        self.check_roundtrip(unit)\n        if un"}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "test_format.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "it.to_string(self.format_)\n                a = Unit(s, format=self.format_)\n            assert_allclose(a.decompose().scale, unit.decompose().scale, rtol=1e-9)\n        else:\n            self.check_roundtrip(unit)\n        if str(unit) in (\"mag\", \"byte\", \"Crab\"):\n            # Skip mag and byte, which decompose into dex and bit, resp.,\n            # both of which are unknown to OGIP, as well as Crab, which does\n            # not decompose, and thus gives a deprecated unit warning.\n            return\n\n        power_of_ten = np.log10(unit.decompose().scale)\n        if abs(power_of_ten - round(power_of_ten)) > 1e-3:\n            ctx = pytest.warns(UnitsWarning, match=\"power of 10\")\n        elif str(unit) == \"0.001 Crab\":\n            ctx = pytest.warns(UnitsWarning, match=\"deprecated\")\n        else:\n            ctx = nullcontext()\n        with ctx:\n            self.check_roundtrip_decompose(unit)\n\n\n@pytest.mark.parametrize(\n    \"unit_formatter_class,n_units\",\n    [(u_format.FITS, 765), (u_format.VOUnit, 1303), (u_format.CDS, 3326)],\n)\ndef test_units_available(unit_formatter_class, n_units):\n    assert len(unit_formatter_class._units) == n_units\n\n\ndef test_cds_non_ascii_unit():\n    \"\"\"Regression test for #5350.  This failed with a decoding error as\n    as could not be represented in ascii.\"\"\"\n    with cds.enable():\n        u.radian.find_equivalent_units(include_prefix_units=True)\n\n\ndef test_latex():\n    fluxunit = u.erg / (u.cm**2 * u.s)\n    assert fluxunit.to_string(\"latex\") == r\"$\\mathrm{\\frac{erg}{s\\,cm^{2}}}$\"\n\n\ndef test_new_style_latex():\n    fluxunit = u.erg / (u.cm**2 * u.s)\n    assert f\"{fluxunit:latex}\" == r\"$\\mathrm{\\frac{erg}{s\\,cm^{2}}}$\"\n\n\ndef test_latex_scale():\n    fluxunit = u.Unit(1.0e-24 * u.erg / (u.cm**2 * u.s * u.Hz))\n    latex = r\"$\\mathrm{1 \\times 10^{-24}\\,\\frac{erg}{Hz\\,s\\,cm^{2}}}$\"\n    assert fluxunit.to_string(\"latex\") == latex\n\n\ndef test_latex_inline_scale():\n    fluxunit = u.Unit(1.0e-24 * u.erg / (u.cm**2 * u.s * u.Hz))\n    latex_inline = r\"$"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "test_format.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ", id=\"int_scale_power\"),\n    ],\n)\ndef test_ogip_negative_exponent_parenthesis(string, unit, power):\n    # Regression test for #16788 - negative powers require parenthesis\n    with pytest.warns(\n        UnitParserWarning,\n        match=(\n            r\"^negative exponents must be enclosed in parenthesis\\. \"\n            rf\"Expected '\\*\\*\\({power}\\)' instead of '\\*\\*{power}'\\.$\"\n        ),\n    ):\n        assert u_format.OGIP.parse(string) == unit\n\n\ndef test_ogip_ohm():\n    # Regression test for #17200 - OGIP converted u.ohm to 'V / A'\n    assert u_format.OGIP.to_string(u.ohm) == \"ohm\"\n\n\nclass RoundtripBase:\n    def check_roundtrip(self, unit, output_format=None):\n        if output_format is None:\n            output_format = self.format_.name\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")  # Same warning shows up multiple times\n            s = unit.to_string(output_format)\n\n        if s in self.format_._deprecated_units:\n            with pytest.warns(UnitsWarning, match=\"deprecated\") as w:\n                a = Unit(s, format=self.format_)\n            assert len(w) == 1\n        else:\n            a = Unit(s, format=self.format_)  # No warning\n\n        assert_allclose(a.decompose().scale, unit.decompose().scale, rtol=1e-9)\n\n    def check_roundtrip_decompose(self, unit):\n        ud = unit.decompose()\n        s = ud.to_string(self.format_)\n        assert \"  \" not in s\n        a = Unit(s, format=self.format_)\n        assert_allclose(a.decompose().scale, ud.scale, rtol=1e-5)\n\n\nclass TestRoundtripGeneric(RoundtripBase):\n    format_ = u_format.Generic\n\n    @pytest.mark.parametrize(\n        \"unit\",\n        [\n            unit\n            for unit in u.__dict__.values()\n            if (isinstance(unit, UnitBase) and not isinstance(unit, PrefixUnit))\n        ],\n        ids=str,\n    )\n    def test_roundtrip(self, unit):\n        self.check_roundtrip(unit)\n        self.check_roundtrip(unit, output_format=\"unicode\")\n        self.check_roundtrip_decompo"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "test_format.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "it == u.mag:\n            # Skip mag: decomposes into dex, which is unknown to CDS.\n            return\n\n        self.check_roundtrip_decompose(unit)\n\n    @pytest.mark.parametrize(\n        \"unit\", [u.dex(unit) for unit in (u.cm / u.s**2, u.K, u.Lsun)], ids=str\n    )\n    def test_roundtrip_dex(self, unit):\n        string = unit.to_string(format=\"cds\")\n        recovered = u.Unit(string, format=\"cds\")\n        assert recovered == unit\n\n\nclass TestRoundtripOGIP(RoundtripBase):\n    format_ = u_format.OGIP\n\n    @pytest.mark.parametrize(\n        \"unit\",\n        [\n            unit\n            for unit in u_format.OGIP._units.values()\n            if (isinstance(unit, UnitBase) and not isinstance(unit, PrefixUnit))\n        ],\n        ids=str,\n    )\n    def test_roundtrip(self, unit):\n        if str(unit) == \"0.001 Crab\":\n            # Special-case mCrab, which the default check does not recognize\n            # as a deprecated unit.\n            with pytest.warns(UnitsWarning):\n                s = unit.to_string(self.format_)\n                a = Unit(s, format=self.format_)\n            assert_allclose(a.decompose().scale, unit.decompose().scale, rtol=1e-9)\n        else:\n            self.check_roundtrip(unit)\n        if str(unit) in (\"mag\", \"byte\", \"Crab\"):\n            # Skip mag and byte, which decompose into dex and bit, resp.,\n            # both of which are unknown to OGIP, as well as Crab, which does\n            # not decompose, and thus gives a deprecated unit warning.\n            return\n\n        power_of_ten = np.log10(unit.decompose().scale)\n        if abs(power_of_ten - round(power_of_ten)) > 1e-3:\n            ctx = pytest.warns(UnitsWarning, match=\"power of 10\")\n        elif str(unit) == \"0.001 Crab\":\n            ctx = pytest.warns(UnitsWarning, match=\"deprecated\")\n        else:\n            ctx = nullcontext()\n        with ctx:\n            self.check_roundtrip_decompose(unit)\n\n\n@pytest.mark.parametrize(\n    \"unit_formatter_class,n_units\",\n    [(u_format.FITS, 765), (u_for"}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "test_format.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "BAR_One\", u.erg / u.Hz)\n    assert myunit.to_string(\"fits\") == \"erg Hz-1\"\n    myunit2 = myunit * u.bit**3\n    assert myunit2.to_string(\"fits\") == \"bit3 erg Hz-1\"\n\n\ndef test_flatten_impossible():\n    myunit = u.def_unit(\"FOOBAR_Two\")\n    with u.add_enabled_units(myunit), pytest.raises(ValueError):\n        myunit.to_string(\"fits\")\n\n\ndef test_console_out():\n    \"\"\"\n    Issue #436.\n    \"\"\"\n    u.Jy.decompose().to_string(\"console\")\n\n\n@pytest.mark.parametrize(\n    \"test_pair\",\n    list_format_string_pairs(\n        (\"generic\", \"10\"),\n        (\"console\", \"10\"),\n        (\"unicode\", \"10\"),\n        (\"cds\", \"10\"),\n        (\"latex\", r\"$\\mathrm{10}$\"),\n    ),\n    ids=lambda x: x.format,\n)\ndef test_scale_only(test_pair: FormatStringPair):\n    assert u.Unit(10).to_string(test_pair.format) == test_pair.string\n\n\ndef test_flexible_float():\n    assert u.min._represents.to_string(\"latex\") == r\"$\\mathrm{60\\,s}$\"\n\n\ndef test_fits_to_string_function_error():\n    \"\"\"Test function raises TypeError on bad input.\n\n    This instead of returning None, see gh-11825.\n    \"\"\"\n\n    with pytest.raises(TypeError, match=\"unit argument must be\"):\n        u_format.FITS.to_string(None)\n\n\ndef test_fraction_repr():\n    area = u.cm**2.0\n    assert \".\" not in area.to_string(\"latex\")\n\n    fractional = u.cm**2.5\n    assert \"5/2\" in fractional.to_string(\"latex\")\n\n    assert fractional.to_string(\"unicode\") == \"cm\"\n\n\ndef test_scale_effectively_unity():\n    \"\"\"Scale just off unity at machine precision level is OK.\n    Ensures #748 does not recur\n    \"\"\"\n    a = (3.0 * u.N).cgs\n    assert is_effectively_unity(a.unit.scale)\n    assert len(a.__repr__().split()) == 3\n\n\ndef test_percent():\n    \"\"\"Test that the % unit is properly recognized.  Since % is a special\n    symbol, this goes slightly beyond the round-tripping tested above.\"\"\"\n    assert u.Unit(\"%\") == u.percent == u.Unit(0.01)\n\n    assert u.Unit(\"%\", format=\"cds\") == u.Unit(0.01)\n    assert u.Unit(0.01).to_string(\"cds\") == \"%\"\n\n    with pytest.raises(ValueErr"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "test_format.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "se(unit)\n\n\nclass TestRoundtripVOUnit(RoundtripBase):\n    format_ = u_format.VOUnit\n\n    @pytest.mark.parametrize(\n        \"unit\",\n        [u for u in u_format.VOUnit._units.values() if not isinstance(u, PrefixUnit)],\n        ids=str,\n    )\n    def test_roundtrip(self, unit):\n        self.check_roundtrip(unit)\n        if unit not in (u.mag, u.dB):\n            self.check_roundtrip_decompose(unit)\n\n\nclass TestRoundtripFITS(RoundtripBase):\n    format_ = u_format.FITS\n\n    @pytest.mark.parametrize(\n        \"unit\",\n        [u for u in u_format.FITS._units.values() if not isinstance(u, PrefixUnit)],\n        ids=str,\n    )\n    def test_roundtrip(self, unit):\n        self.check_roundtrip(unit)\n\n\nclass TestRoundtripCDS(RoundtripBase):\n    format_ = u_format.CDS\n\n    @pytest.mark.parametrize(\n        \"unit\",\n        [u for u in u_format.CDS._units.values() if not isinstance(u, PrefixUnit)],\n        ids=str,\n    )\n    def test_roundtrip(self, unit):\n        self.check_roundtrip(unit)\n        if unit == u.mag:\n            # Skip mag: decomposes into dex, which is unknown to CDS.\n            return\n\n        self.check_roundtrip_decompose(unit)\n\n    @pytest.mark.parametrize(\n        \"unit\", [u.dex(unit) for unit in (u.cm / u.s**2, u.K, u.Lsun)], ids=str\n    )\n    def test_roundtrip_dex(self, unit):\n        string = unit.to_string(format=\"cds\")\n        recovered = u.Unit(string, format=\"cds\")\n        assert recovered == unit\n\n\nclass TestRoundtripOGIP(RoundtripBase):\n    format_ = u_format.OGIP\n\n    @pytest.mark.parametrize(\n        \"unit\",\n        [\n            unit\n            for unit in u_format.OGIP._units.values()\n            if (isinstance(unit, UnitBase) and not isinstance(unit, PrefixUnit))\n        ],\n        ids=str,\n    )\n    def test_roundtrip(self, unit):\n        if str(unit) == \"0.001 Crab\":\n            # Special-case mCrab, which the default check does not recognize\n            # as a deprecated unit.\n            with pytest.warns(UnitsWarning):\n                s = un"}, {"start_line": 27000, "end_line": 29000, "belongs_to": {"file_name": "test_format.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  ],\n)\ndef test_vounit_power(unit, vounit):\n    x = u.Unit(unit)\n    assert x.to_string(format=\"vounit\") == vounit\n\n\ndef test_vounit_custom():\n    x = u.Unit(\"'foo' m\", format=\"vounit\")\n    x_vounit = x.to_string(\"vounit\")\n    assert x_vounit == \"'foo'.m\"\n    x_string = x.to_string()\n    assert x_string == \"foo m\"\n\n    x = u.Unit(\"m'foo' m\", format=\"vounit\")\n    assert x.bases[1]._represents.scale == 0.001\n    x_vounit = x.to_string(\"vounit\")\n    assert x_vounit == \"m.m'foo'\"\n    x_string = x.to_string()\n    assert x_string == \"m mfoo\"\n\n\ndef test_vounit_implicit_custom():\n    # Yikes, this becomes \"femto-urlong\"...  But at least there's a warning.\n    with pytest.warns(UnitsWarning) as w:\n        x = u.Unit(\"furlong/week\", format=\"vounit\", parse_strict=\"warn\")\n    assert x.bases[0]._represents.scale == 1e-15\n    assert x.bases[0]._represents.bases[0].name == \"urlong\"\n    assert len(w) == 2\n    assert \"furlong\" in str(w[0].message)\n    assert \"week\" in str(w[1].message)\n\n\n@pytest.mark.parametrize(\n    \"scale, number, string\",\n    [\n        (\"10+2\", 100, \"10**2\"),\n        (\"10(+2)\", 100, \"10**2\"),\n        (\"10**+2\", 100, \"10**2\"),\n        (\"10**(+2)\", 100, \"10**2\"),\n        (\"10^+2\", 100, \"10**2\"),\n        (\"10^(+2)\", 100, \"10**2\"),\n        (\"10**2\", 100, \"10**2\"),\n        (\"10**(2)\", 100, \"10**2\"),\n        (\"10^2\", 100, \"10**2\"),\n        (\"10^(2)\", 100, \"10**2\"),\n        (\"10-20\", 10 ** (-20), \"10**-20\"),\n        (\"10(-20)\", 10 ** (-20), \"10**-20\"),\n        (\"10**-20\", 10 ** (-20), \"10**-20\"),\n        (\"10**(-20)\", 10 ** (-20), \"10**-20\"),\n        (\"10^-20\", 10 ** (-20), \"10**-20\"),\n        (\"10^(-20)\", 10 ** (-20), \"10**-20\"),\n    ],\n)\ndef test_fits_scale_factor(scale, number, string):\n    x = u.Unit(scale + \" erg/(s cm**2 Angstrom)\", format=\"fits\")\n    assert x == number * (u.erg / u.s / u.cm**2 / u.Angstrom)\n    assert x.to_string(format=\"fits\") == string + \" erg Angstrom-1 s-1 cm-2\"\n\n    x = u.Unit(scale + \"*erg/(s cm**2 Angstrom)\", format=\"fits\")\n    assert x =="}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_format.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"\nRegression tests for the units.format package\n\"\"\"\n\nimport re\nimport warnings\nfrom collections.abc import Iterable\nfrom contextlib import nullcontext\nfrom fractions import Fraction\nfrom typing import NamedTuple\n\nimport numpy as np\nimport pytest\nfrom numpy.testing import assert_allclose\n\nfrom astropy import units as u\nfrom astropy.constants import si\nfrom astropy.units import (\n    PrefixUnit,\n    Unit,\n    UnitBase,\n    UnitParserWarning,\n    UnitsWarning,\n    cds,\n    dex,\n)\nfrom astropy.units import format as u_format\nfrom astropy.units.utils import is_effectively_unity\nfrom astropy.utils.exceptions import AstropyDeprecationWarning\n\n\nclass FormatStringPair(NamedTuple):\n    format: str\n    string: str\n\n\nclass StringUnitPair(NamedTuple):\n    string: str\n    unit: UnitBase\n\n\ndef list_format_string_pairs(*test_cases: tuple[str, str]) -> list[FormatStringPair]:\n    return [FormatStringPair(format, string) for format, string in test_cases]\n\n\ndef list_string_unit_pairs(\n    *test_cases: tuple[Iterable[str], UnitBase],\n) -> list[StringUnitPair]:\n    return [\n        StringUnitPair(string, unit)\n        for strings, unit in test_cases\n        for string in strings\n    ]\n\n\n@pytest.mark.parametrize(\n    \"test_pair\",\n    list_string_unit_pairs(\n        ([\"m s\", \"m*s\", \"m.s\"], u.m * u.s),\n        ([\"m/s\", \"m*s**-1\", \"m /s\", \"m / s\", \"m/ s\"], u.m / u.s),\n        ([\"m**2\", \"m2\", \"m**(2)\", \"m**+2\", \"m+2\", \"m^(+2)\"], u.m**2),\n        ([\"m**-3\", \"m-3\", \"m^(-3)\", \"/m3\"], u.m**-3),\n        ([\"m**(1.5)\", \"m(3/2)\", \"m**(3/2)\", \"m^(3/2)\"], u.m**1.5),\n        ([\"2.54 cm\"], u.Unit(u.cm * 2.54)),\n        ([\"10+8m\"], u.Unit(u.m * 1e8)),\n        # This is the VOUnits documentation, but doesn't seem to follow the\n        # unity grammar ([\"3.45 10**(-4)Jy\"], 3.45 * 1e-4 * u.Jy)\n        ([\"sqrt(m)\"], u.m**0.5),\n        ([\"dB(mW)\", \"dB (mW)\"], u.DecibelUnit(u.mW)),\n        ([\"mag\"], u.mag),\n        ([\"mag(ct/s)\"], u.MagUnit(u.ct"}, {"start_line": 33000, "end_line": 35000, "belongs_to": {"file_name": "test_format.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\"{string}/s\") == unit / u.s\n    assert u.Unit(f\"m {string}\") == u.m * unit\n    assert u.Unit(f\"{string} {string}\") == unit**2\n    # Not obvious that \"2\" should be \"deg**2\", but not easy to reject,\n    # and \"R\" should work.  But don't run on examples with a space or that\n    # already end in a number.\n    if re.match(r\"^\\S*[^\\d]$\", string):\n        assert u.Unit(f\"{string}2\") == unit**2\n    assert u.Unit(f\"{string}/{string}\") == u.dimensionless_unscaled\n    # Finally, check round-trip\n    assert u.Unit(unit.to_string(\"unicode\")) == unit\n\n\n@pytest.mark.parametrize(\n    \"string\",\n    [\n        \"g\\N{MICRO SIGN}\",\n        \"g\\N{MINUS SIGN}\",\n        \"m\\N{SUPERSCRIPT MINUS}1\",\n        \"m+\\N{SUPERSCRIPT ONE}\",\n        \"m\\N{MINUS SIGN}\\N{SUPERSCRIPT ONE}\",\n        \"k\\N{ANGSTROM SIGN}\",\n    ],\n)\ndef test_unicode_failures(string):\n    with pytest.raises(ValueError):\n        u.Unit(string)\n\n\n@pytest.mark.parametrize(\"format_\", (\"unicode\", \"latex\", \"latex_inline\"))\ndef test_parse_error_message_for_output_only_format(format_):\n    with pytest.raises(NotImplementedError, match=\"not parse\"):\n        u.Unit(\"m\", format=format_)\n\n\n@pytest.mark.parametrize(\n    \"parser,error_type,err_msg_start\",\n    [\n        pytest.param(\"foo\", ValueError, \"Unknown format 'foo'\", id=\"ValueError\"),\n        pytest.param(\n            {}, TypeError, \"Expected a formatter name, not {}\", id=\"TypeError\"\n        ),\n    ],\n)\ndef test_unknown_parser(parser, error_type, err_msg_start):\n    with pytest.raises(\n        error_type,\n        match=(\n            f\"^{err_msg_start}\\\\.\\nValid parser names are: \"\n            \"'cds', 'generic', 'fits', 'ogip', 'vounit'$\"\n        ),\n    ):\n        u.Unit(\"m\", format=parser)\n\n\n@pytest.mark.parametrize(\n    \"formatter,error_type,err_msg_start\",\n    [\n        pytest.param(\"abc\", ValueError, \"Unknown format 'abc'\", id=\"ValueError\"),\n        pytest.param(\n            float,\n            TypeError,\n            \"Expected a formatter name, not <class 'float'>\",\n     "}, {"start_line": 22000, "end_line": 24000, "belongs_to": {"file_name": "test_format.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "or):\n        u.Unit(\"%\", format=\"fits\")\n\n    with pytest.raises(ValueError):\n        u.Unit(\"%\", format=\"vounit\")\n\n\ndef test_scaled_dimensionless():\n    \"\"\"Test that scaled dimensionless units are properly recognized in generic\n    and CDS, but not in fits and vounit.\"\"\"\n    assert u.Unit(\"0.1\") == u.Unit(0.1) == 0.1 * u.dimensionless_unscaled\n    assert u.Unit(\"1.e-4\") == u.Unit(1.0e-4)\n\n    assert u.Unit(\"10-4\", format=\"cds\") == u.Unit(1.0e-4)\n    assert u.Unit(\"10+8\").to_string(\"cds\") == \"10+8\"\n\n    with pytest.raises(ValueError):\n        u.Unit(0.15).to_string(\"fits\")\n\n    assert u.Unit(0.1).to_string(\"fits\") == \"10**-1\"\n\n    with pytest.raises(ValueError):\n        u.Unit(0.1).to_string(\"vounit\")\n\n\ndef test_deprecated_did_you_mean_units():\n    with pytest.raises(ValueError) as exc_info:\n        u.Unit(\"ANGSTROM\", format=\"fits\")\n    assert \"Did you mean Angstrom or angstrom?\" in str(exc_info.value)\n\n    with pytest.raises(ValueError) as exc_info:\n        u.Unit(\"crab\", format=\"ogip\")\n    assert \"Crab (deprecated)\" in str(exc_info.value)\n    assert \"mCrab (deprecated)\" in str(exc_info.value)\n\n    with pytest.raises(\n        ValueError,\n        match=(\n            r\"Did you mean 0\\.1nm, Angstrom \\(deprecated\\) or angstrom \\(deprecated\\)\\?\"\n        ),\n    ):\n        u.Unit(\"ANGSTROM\", format=\"vounit\")\n\n    with pytest.warns(UnitsWarning, match=r\".* 0\\.1nm\\.\") as w:\n        u.Unit(\"angstrom\", format=\"vounit\")\n    assert len(w) == 1\n\n\n@pytest.mark.parametrize(\"string\", [\"mag(ct/s)\", \"dB(mW)\", \"dex(cm s**-2)\"])\ndef test_fits_function(string):\n    # Function units cannot be written, so ensure they're not parsed either.\n    with pytest.raises(ValueError):\n        u_format.FITS().parse(string)\n\n\n@pytest.mark.parametrize(\"string\", [\"mag(ct/s)\", \"dB(mW)\", \"dex(cm s**-2)\"])\ndef test_vounit_function(string):\n    # Function units cannot be written, so ensure they're not parsed either.\n    with pytest.raises(ValueError), warnings.catch_warnings():\n        # ct, dex also raise w"}], "retrieved_count": 10, "cost_time": 1.180361032485962}
{"question": "How does the method that appends multiple keyword-value cards to a FITS header handle inserting cards with commentary keywords differently from standard keyword-value cards when the parameter that prevents duplicate keywords is enabled, to ensure the header maintains a length of 5 cards in the test that verifies this behavior?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 58000, "end_line": 60000, "belongs_to": {"file_name": "test_header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "elf):\n        \"\"\"\n        Test appending a new card with just the keyword, and no value or\n        comment given.\n        \"\"\"\n\n        header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n        header.append(\"E\")\n        assert len(header) == 3\n        assert list(header)[-1] == \"E\"\n        assert header[-1] is None\n        assert header.comments[\"E\"] == \"\"\n\n        # Try appending a blank--normally this can be accomplished with just\n        # header.append(), but header.append('') should also work (and is maybe\n        # a little more clear)\n        header.append(\"\")\n        assert len(header) == 4\n\n        assert list(header)[-1] == \"\"\n        assert header[\"\"] == \"\"\n        assert header.comments[\"\"] == \"\"\n\n    def test_header_insert_use_blanks(self):\n        header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n\n        # Append a couple blanks\n        header.append()\n        header.append()\n\n        # Insert a new card; should use up one of the blanks\n        header.insert(1, (\"E\", \"F\"))\n        assert len(header) == 4\n        assert header[1] == \"F\"\n        assert header[-1] == \"\"\n        assert header[-2] == \"D\"\n\n        # Insert a new card without using blanks\n        header.insert(1, (\"G\", \"H\"), useblanks=False)\n        assert len(header) == 5\n        assert header[1] == \"H\"\n        assert header[-1] == \"\"\n\n    def test_header_insert_before_keyword(self):\n        \"\"\"\n        Test that a keyword name or tuple can be used to insert new keywords.\n\n        Also tests the ``after`` keyword argument.\n\n        Regression test for https://github.com/spacetelescope/PyFITS/issues/12\n        \"\"\"\n\n        header = fits.Header(\n            [(\"NAXIS1\", 10), (\"COMMENT\", \"Comment 1\"), (\"COMMENT\", \"Comment 3\")]\n        )\n\n        header.insert(\"NAXIS1\", (\"NAXIS\", 2, \"Number of axes\"))\n        assert list(header.keys())[0] == \"NAXIS\"\n        assert header[0] == 2\n        assert header.comments[0] == \"Number of axes\"\n\n        header.insert(\"NAXIS1\", (\"NAXIS2\", 20), after=True)\n        a"}, {"start_line": 40000, "end_line": 42000, "belongs_to": {"file_name": "header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "d = Card.normalize_keyword(card.keyword)\n        self._keyword_indices[keyword].append(idx)\n        if card.field_specifier is not None:\n            self._rvkc_indices[card.rawkeyword].append(idx)\n\n        if not end:\n            # If the appended card was a commentary card, and it was appended\n            # before existing cards with the same keyword, the indices for\n            # cards with that keyword may have changed\n            if not bottom and card.keyword in _commentary_keywords:\n                self._keyword_indices[keyword].sort()\n\n            # Finally, if useblanks, delete a blank cards from the end\n            if useblanks and self._countblanks():\n                # Don't do this unless there is at least one blanks at the end\n                # of the header; we need to convert the card to its string\n                # image to see how long it is.  In the vast majority of cases\n                # this will just be 80 (Card.length) but it may be longer for\n                # CONTINUE cards\n                self._useblanks(len(str(card)) // Card.length)\n\n        self._modified = True\n\n    def extend(\n        self,\n        cards,\n        strip=True,\n        unique=False,\n        update=False,\n        update_first=False,\n        useblanks=True,\n        bottom=False,\n        end=False,\n    ):\n        \"\"\"\n        Appends multiple keyword+value cards to the end of the header, similar\n        to `list.extend`.\n\n        Parameters\n        ----------\n        cards : iterable\n            An iterable of (keyword, value, [comment]) tuples; see\n            `Header.append`.\n\n        strip : bool, optional\n            Remove any keywords that have meaning only to specific types of\n            HDUs, so that only more general keywords are added from extension\n            Header or Card list (default: `True`).\n\n        unique : bool, optional\n            If `True`, ensures that no duplicate keywords are appended;\n            keywords already in this header are simply discarded"}, {"start_line": 57000, "end_line": 59000, "belongs_to": {"file_name": "test_header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    header[\"HISTORY\"] = \"b\"\n        assert header.count(\"HISTORY\") == 2\n        pytest.raises(KeyError, header.count, \"G\")\n\n    def test_header_append_use_blanks(self):\n        \"\"\"\n        Tests that blank cards can be appended, and that future appends will\n        use blank cards when available (unless useblanks=False)\n        \"\"\"\n\n        header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n\n        # Append a couple blanks\n        header.append()\n        header.append()\n        assert len(header) == 4\n        assert header[-1] == \"\"\n        assert header[-2] == \"\"\n\n        # New card should fill the first blank by default\n        header.append((\"E\", \"F\"))\n        assert len(header) == 4\n        assert header[-2] == \"F\"\n        assert header[-1] == \"\"\n\n        # This card should not use up a blank spot\n        header.append((\"G\", \"H\"), useblanks=False)\n        assert len(header) == 5\n        assert header[-1] == \"\"\n        assert header[-2] == \"H\"\n\n    def test_header_append_keyword_only(self):\n        \"\"\"\n        Test appending a new card with just the keyword, and no value or\n        comment given.\n        \"\"\"\n\n        header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n        header.append(\"E\")\n        assert len(header) == 3\n        assert list(header)[-1] == \"E\"\n        assert header[-1] is None\n        assert header.comments[\"E\"] == \"\"\n\n        # Try appending a blank--normally this can be accomplished with just\n        # header.append(), but header.append('') should also work (and is maybe\n        # a little more clear)\n        header.append(\"\")\n        assert len(header) == 4\n\n        assert list(header)[-1] == \"\"\n        assert header[\"\"] == \"\"\n        assert header.comments[\"\"] == \"\"\n\n    def test_header_insert_use_blanks(self):\n        header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n\n        # Append a couple blanks\n        header.append()\n        header.append()\n\n        # Insert a new card; should use up one of the blanks\n        header.insert(1, (\"E\", \"F\"))\n   "}, {"start_line": 56000, "end_line": 58000, "belongs_to": {"file_name": "test_header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e we want.\n                assert commentary_card not in hdu.header\n                hdu2 = fits.ImageHDU()\n                hdu2.header[commentary_card] = \"My text\"\n                hdu.header.extend(hdu2.header, update=is_update)\n                assert len(hdu.header) == 5\n                assert hdu.header[commentary_card][0] == \"My text\"\n\n    def test_header_extend_exact(self):\n        \"\"\"\n        Test that extending an empty header with the contents of an existing\n        header can exactly duplicate that header, given strip=False and\n        end=True.\n        \"\"\"\n\n        header = fits.getheader(self.data(\"test0.fits\"))\n        header2 = fits.Header()\n        header2.extend(header, strip=False, end=True)\n        assert header == header2\n\n    def test_header_count(self):\n        header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\"), (\"E\", \"F\")])\n        assert header.count(\"A\") == 1\n        assert header.count(\"C\") == 1\n        assert header.count(\"E\") == 1\n        header[\"HISTORY\"] = \"a\"\n        header[\"HISTORY\"] = \"b\"\n        assert header.count(\"HISTORY\") == 2\n        pytest.raises(KeyError, header.count, \"G\")\n\n    def test_header_append_use_blanks(self):\n        \"\"\"\n        Tests that blank cards can be appended, and that future appends will\n        use blank cards when available (unless useblanks=False)\n        \"\"\"\n\n        header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n\n        # Append a couple blanks\n        header.append()\n        header.append()\n        assert len(header) == 4\n        assert header[-1] == \"\"\n        assert header[-2] == \"\"\n\n        # New card should fill the first blank by default\n        header.append((\"E\", \"F\"))\n        assert len(header) == 4\n        assert header[-2] == \"F\"\n        assert header[-1] == \"\"\n\n        # This card should not use up a blank spot\n        header.append((\"G\", \"H\"), useblanks=False)\n        assert len(header) == 5\n        assert header[-1] == \"\"\n        assert header[-2] == \"H\"\n\n    def test_header_append_keyword_only(s"}, {"start_line": 41000, "end_line": 43000, "belongs_to": {"file_name": "header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "NTINUE cards\n                self._useblanks(len(str(card)) // Card.length)\n\n        self._modified = True\n\n    def extend(\n        self,\n        cards,\n        strip=True,\n        unique=False,\n        update=False,\n        update_first=False,\n        useblanks=True,\n        bottom=False,\n        end=False,\n    ):\n        \"\"\"\n        Appends multiple keyword+value cards to the end of the header, similar\n        to `list.extend`.\n\n        Parameters\n        ----------\n        cards : iterable\n            An iterable of (keyword, value, [comment]) tuples; see\n            `Header.append`.\n\n        strip : bool, optional\n            Remove any keywords that have meaning only to specific types of\n            HDUs, so that only more general keywords are added from extension\n            Header or Card list (default: `True`).\n\n        unique : bool, optional\n            If `True`, ensures that no duplicate keywords are appended;\n            keywords already in this header are simply discarded.  The\n            exception is commentary keywords (COMMENT, HISTORY, etc.): they are\n            only treated as duplicates if their values match.\n\n        update : bool, optional\n            If `True`, update the current header with the values and comments\n            from duplicate keywords in the input header.  This supersedes the\n            ``unique`` argument.  Commentary keywords are treated the same as\n            if ``unique=True``.\n\n        update_first : bool, optional\n            If the first keyword in the header is 'SIMPLE', and the first\n            keyword in the input header is 'XTENSION', the 'SIMPLE' keyword is\n            replaced by the 'XTENSION' keyword.  Likewise if the first keyword\n            in the header is 'XTENSION' and the first keyword in the input\n            header is 'SIMPLE', the 'XTENSION' keyword is replaced by the\n            'SIMPLE' keyword.  This behavior is otherwise dumb as to whether or\n            not the resulting header is a valid prim"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "test_header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ments[\"FOO\"] = \"QUX\"\n        hdul.close()\n\n        hdul = fits.open(self.temp(\"test.fits\"))\n        assert hdul[0].header.comments[\"FOO\"] == \"QUX\"\n\n        hdul[0].header.add_comment(0, after=\"FOO\")\n        assert str(hdul[0].header.cards[-1]).strip() == \"COMMENT 0\"\n        hdul.close()\n\n    def test_commentary_cards(self):\n        # commentary cards\n        val = \"A commentary card's value has no quotes around it.\"\n        c = fits.Card(\"HISTORY\", val)\n        assert str(c) == _pad(\"HISTORY \" + val)\n        val = \"A commentary card has no comment.\"\n        c = fits.Card(\"COMMENT\", val, \"comment\")\n        assert str(c) == _pad(\"COMMENT \" + val)\n\n    def test_commentary_card_created_by_fromstring(self):\n        # commentary card created by fromstring()\n        c = fits.Card.fromstring(\n            \"COMMENT card has no comments. \"\n            \"/ text after slash is still part of the value.\"\n        )\n        assert (\n            c.value == \"card has no comments. \"\n            \"/ text after slash is still part of the value.\"\n        )\n        assert c.comment == \"\"\n\n    def test_commentary_card_will_not_parse_numerical_value(self):\n        # commentary card will not parse the numerical value\n        c = fits.Card.fromstring(\"HISTORY  (1, 2)\")\n        assert str(c) == _pad(\"HISTORY  (1, 2)\")\n\n    def test_equal_sign_after_column8(self):\n        # equal sign after column 8 of a commentary card will be part of the\n        # string value\n        c = fits.Card.fromstring(\"HISTORY =   (1, 2)\")\n        assert str(c) == _pad(\"HISTORY =   (1, 2)\")\n\n    def test_blank_keyword(self):\n        c = fits.Card(\"\", \"       / EXPOSURE INFORMATION\")\n        assert str(c) == _pad(\"               / EXPOSURE INFORMATION\")\n        c = fits.Card.fromstring(str(c))\n        assert c.keyword == \"\"\n        assert c.value == \"       / EXPOSURE INFORMATION\"\n\n    def test_specify_undefined_value(self):\n        # this is how to specify an undefined value\n        c = fits.Card(\"undef\", fits.card.UNDEF"}, {"start_line": 37000, "end_line": 39000, "belongs_to": {"file_name": "header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e) and (1 < len(card) <= 3):\n                    self._update(Card(*card))\n                else:\n                    raise ValueError(\n                        f\"Header update sequence item #{idx} is invalid; \"\n                        \"the item must either be a 2-tuple containing \"\n                        \"a keyword and value, or a 3-tuple containing \"\n                        \"a keyword, value, and comment string.\"\n                    )\n        if kwargs:\n            self.update(kwargs)\n\n    def append(self, card=None, useblanks=True, bottom=False, end=False):\n        \"\"\"\n        Appends a new keyword+value card to the end of the Header, similar\n        to `list.append`.\n\n        By default if the last cards in the Header have commentary keywords,\n        this will append the new keyword before the commentary (unless the new\n        keyword is also commentary).\n\n        Also differs from `list.append` in that it can be called with no\n        arguments: In this case a blank card is appended to the end of the\n        Header.  In the case all the keyword arguments are ignored.\n\n        Parameters\n        ----------\n        card : str, tuple\n            A keyword or a (keyword, value, [comment]) tuple representing a\n            single header card; the comment is optional in which case a\n            2-tuple may be used\n\n        useblanks : bool, optional\n            If there are blank cards at the end of the Header, replace the\n            first blank card so that the total number of cards in the Header\n            does not increase.  Otherwise preserve the number of blank cards.\n\n        bottom : bool, optional\n            If True, instead of appending after the last non-commentary card,\n            append after the last non-blank card.\n\n        end : bool, optional\n            If True, ignore the useblanks and bottom options, and append at the\n            very end of the Header.\n\n        \"\"\"\n        if isinstance(card, str):\n            card = Card(card)\n        elif is"}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "test_compressed.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/hdu/compressed/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n        with fits.open(self.data(\"comp.fits\")) as hdul:\n            imghdr = hdul[1].header\n\n            imghdr.append(\"TFIELDS\")\n\n            imghdr.append((\"FOO\", \"bar\", \"qux\"), end=True)\n            assert \"FOO\" in imghdr\n            assert imghdr[-1] == \"bar\"\n\n            imghdr.append((\"CHECKSUM\", \"abcd1234\"))\n            assert \"CHECKSUM\" in imghdr\n            assert imghdr[\"CHECKSUM\"] == \"abcd1234\"\n\n            with pytest.warns(\n                VerifyWarning, match=\"Keyword 'TFIELDS' is reserved\"\n            ) as w:\n                hdul.writeto(tmp_path / \"updated.fits\")\n\n        with fits.open(\n            tmp_path / \"updated.fits\", disable_image_compression=True\n        ) as hdulc:\n            tblhdr = hdulc[1].header\n\n            assert \"FOO\" in tblhdr\n            assert tblhdr[\"FOO\"] == \"bar\"\n\n            assert \"CHECKSUM\" not in tblhdr\n            assert \"ZHECKSUM\" in tblhdr\n            assert tblhdr[\"ZHECKSUM\"] == \"abcd1234\"\n\n    def test_compression_header_append2(self):\n        \"\"\"\n        Regression test for issue https://github.com/astropy/astropy/issues/5827\n        \"\"\"\n        with fits.open(self.data(\"comp.fits\")) as hdul:\n            header = hdul[1].header\n            while len(header) < 1000:\n                header.append()  # pad with grow room\n\n            # Append stats to header:\n            header.append((\"Q1_OSAVG\", 1, \"[adu] quadrant 1 overscan mean\"))\n            header.append((\"Q1_OSSTD\", 1, \"[adu] quadrant 1 overscan stddev\"))\n            header.append((\"Q1_OSMED\", 1, \"[adu] quadrant 1 overscan median\"))\n\n    def test_compression_header_insert(self, tmp_path):\n        with fits.open(self.data(\"comp.fits\")) as hdul:\n            imghdr = hdul[1].header\n\n            # First try inserting a restricted keyword\n            imghdr.insert(1000, \"TFIELDS\")\n\n            # First try keyword-relative insert\n            imghdr.insert(\"TELESCOP\", (\"OBSERVER\", \"Phil Plait\"))\n            assert \"OBSERVER\" in imghdr\n            assert imghdr.index("}, {"start_line": 36000, "end_line": 38000, "belongs_to": {"file_name": "header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "< len(v) <= 2:\n                card = Card(*((k,) + v))\n            else:\n                raise ValueError(\n                    f\"Header update value for key {k!r} is invalid; the \"\n                    \"value must be either a scalar, a 1-tuple \"\n                    \"containing the scalar value, or a 2-tuple \"\n                    \"containing the value and a comment string.\"\n                )\n            self._update(card)\n\n        if other is None:\n            pass\n        elif isinstance(other, Header):\n            for card in other.cards:\n                self._update(card)\n        elif hasattr(other, \"items\"):\n            for k, v in other.items():\n                update_from_dict(k, v)\n        elif hasattr(other, \"keys\"):\n            for k in other.keys():\n                update_from_dict(k, other[k])\n        else:\n            for idx, card in enumerate(other):\n                if isinstance(card, Card):\n                    self._update(card)\n                elif isinstance(card, tuple) and (1 < len(card) <= 3):\n                    self._update(Card(*card))\n                else:\n                    raise ValueError(\n                        f\"Header update sequence item #{idx} is invalid; \"\n                        \"the item must either be a 2-tuple containing \"\n                        \"a keyword and value, or a 3-tuple containing \"\n                        \"a keyword, value, and comment string.\"\n                    )\n        if kwargs:\n            self.update(kwargs)\n\n    def append(self, card=None, useblanks=True, bottom=False, end=False):\n        \"\"\"\n        Appends a new keyword+value card to the end of the Header, similar\n        to `list.append`.\n\n        By default if the last cards in the Header have commentary keywords,\n        this will append the new keyword before the commentary (unless the new\n        keyword is also commentary).\n\n        Also differs from `list.append` in that it can be called with no\n        arguments: In this case a blank card is appen"}, {"start_line": 66000, "end_line": 68000, "belongs_to": {"file_name": "header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    return True\n\n        return False\n\n    def _splitcommentary(self, keyword, value):\n        \"\"\"\n        Given a commentary keyword and value, returns a list of the one or more\n        cards needed to represent the full value.  This is primarily used to\n        create the multiple commentary cards needed to represent a long value\n        that won't fit into a single commentary card.\n        \"\"\"\n        # The maximum value in each card can be the maximum card length minus\n        # the maximum key length (which can include spaces if they key length\n        # less than 8\n        maxlen = Card.length - KEYWORD_LENGTH\n        valuestr = str(value)\n\n        if len(valuestr) <= maxlen:\n            # The value can fit in a single card\n            cards = [Card(keyword, value)]\n        else:\n            # The value must be split across multiple consecutive commentary\n            # cards\n            idx = 0\n            cards = []\n            while idx < len(valuestr):\n                cards.append(Card(keyword, valuestr[idx : idx + maxlen]))\n                idx += maxlen\n        return cards\n\n    def _add_commentary(self, key, value, before=None, after=None):\n        \"\"\"\n        Add a commentary card.\n\n        If ``before`` and ``after`` are `None`, add to the last occurrence\n        of cards of the same name (except blank card).  If there is no\n        card (or blank card), append at the end.\n        \"\"\"\n        if before is not None or after is not None:\n            self._relativeinsert((key, value), before=before, after=after)\n        else:\n            self[key] = value\n\n\ncollections.abc.MutableSequence.register(Header)\ncollections.abc.MutableMapping.register(Header)\n\n\nclass _DelayedHeader:\n    \"\"\"\n    Descriptor used to create the Header object from the header string that\n    was stored in HDU._header_str when parsing the file.\n    \"\"\"\n\n    def __get__(self, obj, owner=None):\n        try:\n            return obj.__dict__[\"_header\"]\n        except KeyError:\n            if"}], "retrieved_count": 10, "cost_time": 1.1890630722045898}
{"question": "What design mechanisms in the numpy.ndarray subclass that represents numbers with associated physical units in the astropy.units module enforce conversion prevention boundaries between quantities without physical dimensions and Python's built-in scalar type conversion mechanisms?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 62000, "end_line": 64000, "belongs_to": {"file_name": "quantity.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ts and hasattr(new_unit, \"scale\"):\n            new_value = self.value * new_unit.scale\n            new_unit = new_unit / new_unit.scale\n            return self._new_view(new_value, new_unit)\n        else:\n            return self._new_view(self.copy(), new_unit)\n\n    # These functions need to be overridden to take into account the units\n    # Array conversion\n    # https://numpy.org/doc/stable/reference/arrays.ndarray.html#array-conversion\n\n    def item(self, *args):\n        \"\"\"Copy an element of an array to a scalar Quantity and return it.\n\n        Like :meth:`~numpy.ndarray.item` except that it always\n        returns a `Quantity`, not a Python scalar.\n\n        \"\"\"\n        return self._new_view(super().item(*args))\n\n    def tolist(self):\n        raise NotImplementedError(\n            \"cannot make a list of Quantities. Get list of values with\"\n            \" q.value.tolist().\"\n        )\n\n    def _to_own_unit(self, value, check_precision=True, *, unit=None):\n        \"\"\"Convert value to one's own unit (or that given).\n\n        Here, non-quantities are treated as dimensionless, and care is taken\n        for values of 0, infinity or nan, which are allowed to have any unit.\n\n        Parameters\n        ----------\n        value : anything convertible to `~astropy.units.Quantity`\n            The value to be converted to the requested unit.\n        check_precision : bool\n            Whether to forbid conversion of float to integer if that changes\n            the input number.  Default: `True`.\n        unit : `~astropy.units.Unit` or None\n            The unit to convert to.  By default, the unit of ``self``.\n\n        Returns\n        -------\n        value : number or `~numpy.ndarray`\n            In the requested units.\n\n        \"\"\"\n        if unit is None:\n            unit = self.unit\n        try:\n            _value = value.to_value(unit)\n        except AttributeError:\n            # We're not a Quantity.\n            # First remove two special cases (with a fast test):\n          "}, {"start_line": 19000, "end_line": 21000, "belongs_to": {"file_name": "quantity.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ") from exc\n\n                if unit is None:\n                    unit = value_unit\n                elif unit is not value_unit:\n                    copy = COPY_IF_NEEDED  # copy will be made in conversion at end\n\n        value = np.array(\n            value, dtype=dtype, copy=copy, order=order, subok=True, ndmin=ndmin\n        )\n\n        # For no-user-input unit, make sure the constructed unit matches the\n        # structure of the data.\n        if using_default_unit and value.dtype.names is not None:\n            unit = value_unit = _structured_unit_like_dtype(value_unit, value.dtype)\n\n        # check that array contains numbers or long int objects\n        if value.dtype.kind in \"OSU\" and not (\n            value.dtype.kind == \"O\" and isinstance(value.item(0), numbers.Number)\n        ):\n            raise TypeError(\"The value must be a valid Python or Numpy numeric type.\")\n\n        # by default, cast any integer, boolean, etc., to float\n        if float_default and value.dtype.kind in \"iuO\":\n            value = value.astype(float)\n\n        # if we allow subclasses, allow a class from the unit.\n        if subok:\n            qcls = getattr(unit, \"_quantity_class\", cls)\n            if issubclass(qcls, cls):\n                cls = qcls\n\n        value = value.view(cls)\n        value._set_unit(value_unit)\n        if unit is value_unit:\n            return value\n        else:\n            # here we had non-Quantity input that had a \"unit\" attribute\n            # with a unit different from the desired one.  So, convert.\n            return value.to(unit)\n\n    def __array_finalize__(self, obj):\n        # Check whether super().__array_finalize should be called\n        # (sadly, ndarray.__array_finalize__ is None; we cannot be sure\n        # what is above us).\n        super_array_finalize = super().__array_finalize__\n        if super_array_finalize is not None:\n            super_array_finalize(obj)\n\n        # If we're a new object or viewing an ndarray, nothing has to be done.\n       "}, {"start_line": 44000, "end_line": 46000, "belongs_to": {"file_name": "quantity.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    try:\n            other = Unit(other, parse_strict=\"silent\")\n        except UnitTypeError:\n            return NotImplemented\n\n        return self.__class__(self, other, copy=False, subok=True)\n\n    def __ilshift__(self, other):\n        try:\n            other = Unit(other, parse_strict=\"silent\")\n        except UnitTypeError:\n            return NotImplemented  # try other.__rlshift__(self)\n\n        try:\n            factor = self.unit._to(other)\n        except UnitConversionError:  # incompatible, or requires an Equivalency\n            return NotImplemented\n        except AttributeError:  # StructuredUnit does not have `_to`\n            # In principle, in-place might be possible.\n            return NotImplemented\n\n        view = self.view(np.ndarray)\n        try:\n            view *= factor  # operates on view\n        except TypeError:\n            # The error is `numpy.core._exceptions._UFuncOutputCastingError`,\n            # which inherits from `TypeError`.\n            return NotImplemented\n\n        self._set_unit(other)\n        return self\n\n    def __rlshift__(self, other):\n        if not self.isscalar:\n            return NotImplemented\n        return Unit(self).__rlshift__(other)\n\n    # Give warning for other >> self, since probably other << self was meant.\n    def __rrshift__(self, other):\n        warnings.warn(\n            \">> is not implemented. Did you mean to convert \"\n            \"something to this quantity as a unit using '<<'?\",\n            AstropyWarning,\n        )\n        return NotImplemented\n\n    # Also define __rshift__ and __irshift__ so we override default ndarray\n    # behaviour, but instead of emitting a warning here, let it be done by\n    # other (which likely is a unit if this was a mistake).\n    def __rshift__(self, other):\n        return NotImplemented\n\n    def __irshift__(self, other):\n        return NotImplemented\n\n    # Arithmetic operations\n    def __mul__(self, other):\n        if isinstance(other, (UnitBase, str)):\n            try:\n      "}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "quantity.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\":\n            value = value.astype(float)\n\n        # if we allow subclasses, allow a class from the unit.\n        if subok:\n            qcls = getattr(unit, \"_quantity_class\", cls)\n            if issubclass(qcls, cls):\n                cls = qcls\n\n        value = value.view(cls)\n        value._set_unit(value_unit)\n        if unit is value_unit:\n            return value\n        else:\n            # here we had non-Quantity input that had a \"unit\" attribute\n            # with a unit different from the desired one.  So, convert.\n            return value.to(unit)\n\n    def __array_finalize__(self, obj):\n        # Check whether super().__array_finalize should be called\n        # (sadly, ndarray.__array_finalize__ is None; we cannot be sure\n        # what is above us).\n        super_array_finalize = super().__array_finalize__\n        if super_array_finalize is not None:\n            super_array_finalize(obj)\n\n        # If we're a new object or viewing an ndarray, nothing has to be done.\n        if obj is None or obj.__class__ is np.ndarray:\n            return\n\n        # Copy over the unit and possibly info.  Note that the only way the\n        # unit can already be set is if one enters via _new_view(), where the\n        # unit is often different from that of self, and where propagation of\n        # info is not always desirable.\n        if self._unit is None:\n            unit = getattr(obj, \"_unit\", None)\n            if unit is not None:\n                self._set_unit(unit)\n\n            # Copy info if the original had `info` defined.  Because of the way the\n            # DataInfo works, `'info' in obj.__dict__` is False until the\n            # `info` attribute is accessed or set.\n            if \"info\" in obj.__dict__:\n                self.info = obj.info\n\n    def __array_wrap__(self, obj, context=None, return_scalar=False):\n        if context is None:\n            # Methods like .squeeze() created a new `ndarray` and then call\n            # __array_wrap__ to turn the array into"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "                 # special case: OK if unitless number is zero, inf, nan\n                    converters.append(False)\n                else:\n                    raise UnitConversionError(\n                        f\"Can only apply '{f.__name__}' function to quantities with \"\n                        \"compatible dimensions\"\n                    )\n            else:\n                converters.append(converter)\n\n    return converters, result_unit\n\n\n# list of ufuncs:\n# https://numpy.org/doc/stable/reference/ufuncs.html#available-ufuncs\n\nUNSUPPORTED_UFUNCS |= {\n    np.bitwise_and,\n    np.bitwise_or,\n    np.bitwise_xor,\n    np.invert,\n    np.left_shift,\n    np.right_shift,\n    np.logical_and,\n    np.logical_or,\n    np.logical_xor,\n    np.logical_not,\n    np.isnat,\n    np.gcd,\n    np.lcm,\n}\n\nif not NUMPY_LT_2_0:\n    # string utilities - make no sense for Quantity.\n    UNSUPPORTED_UFUNCS |= {\n        np.bitwise_count,\n        np._core.umath.count,\n        np._core.umath.isalpha,\n        np._core.umath.isdigit,\n        np._core.umath.isspace,\n        np._core.umath.isnumeric,\n        np._core.umath.isdecimal,\n        np._core.umath.isalnum,\n        np._core.umath.istitle,\n        np._core.umath.islower,\n        np._core.umath.isupper,\n        np._core.umath.index,\n        np._core.umath.rindex,\n        np._core.umath.startswith,\n        np._core.umath.endswith,\n        np._core.umath.find,\n        np._core.umath.rfind,\n        np._core.umath.str_len,\n        np._core.umath._strip_chars,\n        np._core.umath._lstrip_chars,\n        np._core.umath._rstrip_chars,\n        np._core.umath._strip_whitespace,\n        np._core.umath._lstrip_whitespace,\n        np._core.umath._rstrip_whitespace,\n        np._core.umath._replace,\n        np._core.umath._expandtabs,\n        np._core.umath._expandtabs_length,\n    }\nif not NUMPY_LT_2_1:\n    UNSUPPORTED_UFUNCS |= {\n        np._core.umath._ljust,\n        np._core.umath._rjust,\n        np._core.umath._center,\n        np._core.umath._zfill,\n       "}, {"start_line": 21000, "end_line": 23000, "belongs_to": {"file_name": "test_quantity.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "m\n\n        # when one is a unit, Quantity does not know what to do,\n        # but unit is fine with it, so it still works\n        unit = u.cm**3\n        q = 1.0 * unit\n        assert q.__eq__(unit) is NotImplemented\n        assert unit.__eq__(q) is True\n        assert q == unit\n        q = 1000.0 * u.mm**3\n        assert q == unit\n\n        # mismatched types should never work\n        assert not 1.0 * u.cm == 1.0\n        assert 1.0 * u.cm != 1.0\n\n        for quantity in (1.0 * u.cm, 1.0 * u.dimensionless_unscaled):\n            with pytest.raises(ValueError, match=\"ambiguous\"):\n                bool(quantity)\n\n    def test_numeric_converters(self):\n        # float, int, long, and __index__ should only work for single\n        # quantities, of appropriate type, and only if they are dimensionless.\n        # for index, this should be unscaled as well\n        # (Check on __index__ is also a regression test for #1557)\n\n        # quantities with units should never convert, or be usable as an index\n        q1 = u.Quantity(1, u.m)\n\n        converter_err_msg = (\n            \"only dimensionless scalar quantities can be converted to Python scalars\"\n        )\n        index_err_msg = (\n            \"only integer dimensionless scalar quantities \"\n            \"can be converted to a Python index\"\n        )\n        with pytest.raises(TypeError) as exc:\n            float(q1)\n        assert exc.value.args[0] == converter_err_msg\n\n        with pytest.raises(TypeError) as exc:\n            int(q1)\n        assert exc.value.args[0] == converter_err_msg\n\n        # We used to test `q1 * ['a', 'b', 'c'] here, but that that worked\n        # at all was a really odd confluence of bugs.  Since it doesn't work\n        # in numpy >=1.10 any more, just go directly for `__index__` (which\n        # makes the test more similar to the `int`, `long`, etc., tests).\n        with pytest.raises(TypeError) as exc:\n            q1.__index__()\n        assert exc.value.args[0] == index_err_msg\n\n        # dimensionless"}, {"start_line": 41000, "end_line": 43000, "belongs_to": {"file_name": "quantity.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "umpy.isscalar` in that\n            `numpy.isscalar` returns False for a zero-dimensional array\n            (e.g. ``np.array(1)``), while this is True for quantities,\n            since quantities cannot represent true numpy scalars.\n        \"\"\"\n        return not self.shape\n\n    # This flag controls whether convenience conversion members, such\n    # as `q.m` equivalent to `q.to_value(u.m)` are available.  This is\n    # not turned on on Quantity itself, but is on some subclasses of\n    # Quantity, such as `astropy.coordinates.Angle`.\n    _include_easy_conversion_members = False\n\n    def __dir__(self):\n        \"\"\"\n        Quantities are able to directly convert to other units that\n        have the same physical type.  This function is implemented in\n        order to make autocompletion still work correctly in IPython.\n        \"\"\"\n        if not self._include_easy_conversion_members:\n            return super().__dir__()\n\n        dir_values = set(super().__dir__())\n        equivalencies = Unit._normalize_equivalencies(self.equivalencies)\n        for equivalent in self.unit._get_units_with_same_physical_type(equivalencies):\n            dir_values.update(equivalent.names)\n        return sorted(dir_values)\n\n    def __getattr__(self, attr):\n        \"\"\"\n        Quantities are able to directly convert to other units that\n        have the same physical type.\n        \"\"\"\n        if not self._include_easy_conversion_members:\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no '{attr}' member\"\n            )\n\n        def get_virtual_unit_attribute():\n            registry = get_current_unit_registry().registry\n            to_unit = registry.get(attr, None)\n            if to_unit is None:\n                return None\n\n            try:\n                return self.unit.to(\n                    to_unit, self.value, equivalencies=self.equivalencies\n                )\n            except UnitsError:\n                return None\n\n        value = get_vir"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "converters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " unit, i.e., when it\n            # is all zero, infinity or NaN.  In that case, the non-quantity\n            # can just have the unit of the quantity\n            # (this allows, e.g., `q > 0.` independent of unit)\n            try:\n                # Don't fold this loop in the test above: this rare case\n                # should not make the common case slower.\n                for i, converter in enumerate(converters):\n                    if converter is not False:\n                        continue\n                    if can_have_arbitrary_unit(args[i]):\n                        converters[i] = None\n                    else:\n                        raise UnitConversionError(\n                            f\"Can only apply '{function.__name__}' function to \"\n                            \"dimensionless quantities when other argument is not \"\n                            \"a quantity (unless the latter is all zero/infinity/nan).\"\n                        )\n            except TypeError:\n                # _can_have_arbitrary_unit failed: arg could not be compared\n                # with zero or checked to be finite. Then, ufunc will fail too.\n                raise TypeError(\n                    \"Unsupported operand type(s) for ufunc {}: '{}'\".format(\n                        function.__name__,\n                        \",\".join([arg.__class__.__name__ for arg in args]),\n                    )\n                )\n\n        # In the case of np.power and np.float_power, the unit itself needs to\n        # be modified by an amount that depends on one of the input values,\n        # so we need to treat this as a special case.\n        # TODO: find a better way to deal with this.\n        if result_unit is False:\n            if units[0] is None or units[0] == dimensionless_unscaled:\n                result_unit = dimensionless_unscaled\n            else:\n                if units[1] is None:\n                    p = args[1]\n                else:\n                    p = args[1].to(dimensionless_unscaled)"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "quantity.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "`` argument is explicitly specified, integer\n    or (non-Quantity) object inputs are converted to `float` by default.\n    \"\"\"\n\n    # Need to set a class-level default for _equivalencies, or\n    # Constants can not initialize properly\n    _equivalencies = []\n\n    # Default unit for initialization; can be overridden by subclasses,\n    # possibly to `None` to indicate there is no default unit.\n    _default_unit = dimensionless_unscaled\n\n    # Ensures views have an undefined unit.\n    _unit = None\n\n    __array_priority__ = 10000\n\n    def __class_getitem__(cls, unit_shape_dtype):\n        \"\"\"Quantity Type Hints.\n\n        Unit-aware type hints are ``Annotated`` objects that encode the class,\n        the unit, and possibly shape and dtype information, depending on the\n        python and :mod:`numpy` versions.\n\n        Schematically, ``Annotated[cls[shape, dtype], unit]``\n\n        As a classmethod, the type is the class, ie ``Quantity``\n        produces an ``Annotated[Quantity, ...]`` while a subclass\n        like :class:`~astropy.coordinates.Angle` returns\n        ``Annotated[Angle, ...]``.\n\n        Parameters\n        ----------\n        unit_shape_dtype : :class:`~astropy.units.UnitBase`, str, `~astropy.units.PhysicalType`, or tuple\n            Unit specification, can be the physical type (ie str or class).\n            If tuple, then the first element is the unit specification\n            and all other elements are for `numpy.ndarray` type annotations.\n            Whether they are included depends on the python and :mod:`numpy`\n            versions.\n\n        Returns\n        -------\n        `typing.Annotated`, `astropy.units.Unit`, or `astropy.units.PhysicalType`\n            Return type in this preference order:\n            * `typing.Annotated`\n            * `astropy.units.Unit` or `astropy.units.PhysicalType`\n\n        Raises\n        ------\n        TypeError\n            If the unit/physical_type annotation is not Unit-like or\n            PhysicalType-like.\n\n        Examples\n"}, {"start_line": 64000, "end_line": 66000, "belongs_to": {"file_name": "quantity.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  # 1) Maybe masked printing? MaskedArray with quantities does not\n            # work very well, but no reason to break even repr and str.\n            # 2) np.ma.masked? useful if we're a MaskedQuantity.\n            if value is np.ma.masked or (\n                value is np.ma.masked_print_option and self.dtype.kind == \"O\"\n            ):\n                return value\n            # Now, let's try a more general conversion.\n            # Plain arrays will be converted to dimensionless in the process,\n            # but anything with a unit attribute will use that.\n            try:\n                as_quantity = Quantity(value)\n                _value = as_quantity.to_value(unit)\n            except UnitsError:\n                # last chance: if this was not something with a unit\n                # and is all 0, inf, or nan, we treat it as arbitrary unit.\n                if not hasattr(value, \"unit\") and can_have_arbitrary_unit(\n                    as_quantity.value\n                ):\n                    _value = as_quantity.value\n                else:\n                    raise\n\n        if self.dtype.kind == \"i\" and check_precision:\n            # If, e.g., we are casting float to int, we want to fail if\n            # precision is lost, but let things pass if it works.\n            _value = np.array(_value, copy=COPY_IF_NEEDED, subok=True)\n            if not np.can_cast(_value.dtype, self.dtype):\n                self_dtype_array = np.array(_value, self.dtype, subok=True)\n                if not np.all((self_dtype_array == _value) | np.isnan(_value)):\n                    raise TypeError(\n                        \"cannot convert value type to array type without precision loss\"\n                    )\n\n        # Setting names to ensure things like equality work (note that\n        # above will have failed already if units did not match).\n        # TODO: is this the best place to do this?\n        if _value.dtype.names is not None:\n            _value = _value.astype(self.dtype, copy=False"}], "retrieved_count": 10, "cost_time": 1.216728687286377}
{"question": "Why does the regression test that verifies compatibility with one-dimensional labeled array structures from the pandas library validate the interaction between these structures and the input conversion process in the convolution function that transforms various array-like inputs into numpy arrays for processing?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 39000, "end_line": 41000, "belongs_to": {"file_name": "test_convolve.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/convolution/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ", 3, 2, 1])\n\n    assert_array_almost_equal(\n        np.convolve(y, x, \"same\"), convolve(y, x, normalize_kernel=False)\n    )\n    assert_array_almost_equal(\n        np.convolve(y, x, \"same\"), convolve_fft(y, x, normalize_kernel=False)\n    )\n\n\n@pytest.mark.skipif(not HAS_SCIPY, reason=\"Requires scipy\")\ndef test_astropy_convolution_against_scipy():\n    from scipy.signal import fftconvolve\n\n    x = np.array([1, 2, 3])\n    y = np.array([5, 4, 3, 2, 1])\n\n    assert_array_almost_equal(\n        fftconvolve(y, x, \"same\"), convolve(y, x, normalize_kernel=False)\n    )\n    assert_array_almost_equal(\n        fftconvolve(y, x, \"same\"), convolve_fft(y, x, normalize_kernel=False)\n    )\n\n\n@pytest.mark.skipif(not HAS_PANDAS, reason=\"Requires pandas\")\ndef test_regression_6099():\n    import pandas as pd\n\n    wave = np.array(np.linspace(5000, 5100, 10))\n    boxcar = 3\n    nonseries_result = convolve(wave, np.ones((boxcar,)) / boxcar)\n\n    wave_series = pd.Series(wave)\n    series_result = convolve(wave_series, np.ones((boxcar,)) / boxcar)\n\n    assert_array_almost_equal(nonseries_result, series_result)\n\n\ndef test_invalid_array_convolve():\n    kernel = np.ones(3) / 3.0\n\n    with pytest.raises(TypeError):\n        convolve(\"glork\", kernel)\n\n\n@pytest.mark.parametrize(\"boundary\", BOUNDARY_OPTIONS)\ndef test_non_square_kernel_asymmetric(boundary):\n    # Regression test for a bug that occurred when using non-square kernels in\n    # 2D when using boundary=None\n    kernel = np.array([[1, 2, 3, 2, 1], [0, 1, 2, 1, 0], [0, 0, 0, 0, 0]])\n    image = np.zeros((13, 13))\n    image[6, 6] = 1\n    result = convolve(image, kernel, normalize_kernel=False, boundary=boundary)\n    assert_allclose(result[5:8, 4:9], kernel)\n\n\n@pytest.mark.parametrize(\"boundary\", BOUNDARY_OPTIONS)\n@pytest.mark.parametrize(\"normalize_kernel\", NORMALIZE_OPTIONS)\ndef test_uninterpolated_nan_regions(boundary, normalize_kernel):\n    # Issue #8086\n    # Test NaN interpolation of contiguous NaN regions with kernels of size\n    # identical "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_convolve.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/convolution/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_options + convolve_fft_options\n\n\nclass TestConvolve1D:\n    def test_list(self):\n        \"\"\"\n        Test that convolve works correctly when inputs are lists\n        \"\"\"\n\n        x = [1, 4, 5, 6, 5, 7, 8]\n        y = [0.2, 0.6, 0.2]\n        z = convolve(x, y, boundary=None)\n        assert_array_almost_equal_nulp(\n            z, np.array([0.0, 3.6, 5.0, 5.6, 5.6, 6.8, 0.0]), 10\n        )\n\n    def test_tuple(self):\n        \"\"\"\n        Test that convolve works correctly when inputs are tuples\n        \"\"\"\n\n        x = (1, 4, 5, 6, 5, 7, 8)\n        y = (0.2, 0.6, 0.2)\n        z = convolve(x, y, boundary=None)\n        assert_array_almost_equal_nulp(\n            z, np.array([0.0, 3.6, 5.0, 5.6, 5.6, 6.8, 0.0]), 10\n        )\n\n    @pytest.mark.parametrize(\"boundary\", BOUNDARY_OPTIONS)\n    @pytest.mark.parametrize(\"nan_treatment\", NANHANDLING_OPTIONS)\n    @pytest.mark.parametrize(\"normalize_kernel\", NORMALIZE_OPTIONS)\n    @pytest.mark.parametrize(\"preserve_nan\", PRESERVE_NAN_OPTIONS)\n    @pytest.mark.parametrize(\"dtype\", VALID_DTYPES)\n    def test_quantity(\n        self, boundary, nan_treatment, normalize_kernel, preserve_nan, dtype\n    ):\n        \"\"\"\n        Test that convolve works correctly when input array is a Quantity\n        \"\"\"\n\n        x = np.array([1, 4, 5, 6, 5, 7, 8], dtype=dtype) * u.ph\n        y = np.array([0.2, 0.6, 0.2], dtype=dtype)\n        z = convolve(\n            x,\n            y,\n            boundary=boundary,\n            nan_treatment=nan_treatment,\n            normalize_kernel=normalize_kernel,\n            preserve_nan=preserve_nan,\n        )\n\n        assert x.unit == z.unit\n\n    @pytest.mark.parametrize(\"boundary\", BOUNDARY_OPTIONS)\n    @pytest.mark.parametrize(\"nan_treatment\", NANHANDLING_OPTIONS)\n    @pytest.mark.parametrize(\"normalize_kernel\", NORMALIZE_OPTIONS)\n    @pytest.mark.parametrize(\"preserve_nan\", PRESERVE_NAN_OPTIONS)\n    @pytest.mark.parametrize(\"dtype\", VALID_DTYPES)\n    def test_input_unmodified(\n        self, boundary, nan_treatment, n"}, {"start_line": 38000, "end_line": 40000, "belongs_to": {"file_name": "test_convolve.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/convolution/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "lize_kernel=False)\n\n    if boundary == \"fill\":\n        assert_array_almost_equal_nulp(z, np.array([6.0, 10.0, 2.0], dtype=\"float\"), 10)\n    elif boundary is None:\n        assert_array_almost_equal_nulp(z, np.array([0.0, 10.0, 0.0], dtype=\"float\"), 10)\n    elif boundary == \"extend\":\n        assert_array_almost_equal_nulp(\n            z, np.array([15.0, 10.0, 3.0], dtype=\"float\"), 10\n        )\n    elif boundary == \"wrap\":\n        assert_array_almost_equal_nulp(z, np.array([9.0, 10.0, 5.0], dtype=\"float\"), 10)\n\n\n@pytest.mark.parametrize(\"ndims\", (1, 2, 3))\ndef test_convolution_consistency(ndims):\n    np.random.seed(0)\n    array = np.random.randn(*([3] * ndims))\n    np.random.seed(0)\n    kernel = np.random.rand(*([3] * ndims))\n\n    conv_f = convolve_fft(array, kernel, boundary=\"fill\")\n    conv_d = convolve(array, kernel, boundary=\"fill\")\n\n    assert_array_almost_equal_nulp(conv_f, conv_d, 30)\n\n\ndef test_astropy_convolution_against_numpy():\n    x = np.array([1, 2, 3])\n    y = np.array([5, 4, 3, 2, 1])\n\n    assert_array_almost_equal(\n        np.convolve(y, x, \"same\"), convolve(y, x, normalize_kernel=False)\n    )\n    assert_array_almost_equal(\n        np.convolve(y, x, \"same\"), convolve_fft(y, x, normalize_kernel=False)\n    )\n\n\n@pytest.mark.skipif(not HAS_SCIPY, reason=\"Requires scipy\")\ndef test_astropy_convolution_against_scipy():\n    from scipy.signal import fftconvolve\n\n    x = np.array([1, 2, 3])\n    y = np.array([5, 4, 3, 2, 1])\n\n    assert_array_almost_equal(\n        fftconvolve(y, x, \"same\"), convolve(y, x, normalize_kernel=False)\n    )\n    assert_array_almost_equal(\n        fftconvolve(y, x, \"same\"), convolve_fft(y, x, normalize_kernel=False)\n    )\n\n\n@pytest.mark.skipif(not HAS_PANDAS, reason=\"Requires pandas\")\ndef test_regression_6099():\n    import pandas as pd\n\n    wave = np.array(np.linspace(5000, 5100, 10))\n    boxcar = 3\n    nonseries_result = convolve(wave, np.ones((boxcar,)) / boxcar)\n\n    wave_series = pd.Series(wave)\n    series_result = convolve(wave_serie"}, {"start_line": 34000, "end_line": 36000, "belongs_to": {"file_name": "test_convolve_fft.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/convolution/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "est_input_unmodified(\n    boundary, nan_treatment, normalize_kernel, preserve_nan, dtype\n):\n    \"\"\"\n    Test that convolve_fft works correctly when inputs are lists\n    \"\"\"\n\n    array = [1.0, 4.0, 5.0, 6.0, 5.0, 7.0, 8.0]\n    kernel = [0.2, 0.6, 0.2]\n    x = np.array(array, dtype=dtype)\n    y = np.array(kernel, dtype=dtype)\n\n    # Make pseudoimmutable\n    x.flags.writeable = False\n    y.flags.writeable = False\n\n    with expected_boundary_warning(boundary=boundary):\n        convolve_fft(\n            x,\n            y,\n            boundary=boundary,\n            nan_treatment=nan_treatment,\n            normalize_kernel=normalize_kernel,\n            preserve_nan=preserve_nan,\n        )\n\n    assert np.all(np.array(array, dtype=dtype) == x)\n    assert np.all(np.array(kernel, dtype=dtype) == y)\n\n\n@pytest.mark.parametrize(\"boundary\", BOUNDARY_OPTIONS)\n@pytest.mark.parametrize(\"nan_treatment\", NANTREATMENT_OPTIONS)\n@pytest.mark.parametrize(\"normalize_kernel\", NORMALIZE_OPTIONS)\n@pytest.mark.parametrize(\"preserve_nan\", PRESERVE_NAN_OPTIONS)\n@pytest.mark.parametrize(\"dtype\", VALID_DTYPES)\ndef test_input_unmodified_with_nan(\n    boundary, nan_treatment, normalize_kernel, preserve_nan, dtype\n):\n    \"\"\"\n    Test that convolve_fft doesn't modify the input data\n    \"\"\"\n\n    array = [1.0, 4.0, 5.0, np.nan, 5.0, 7.0, 8.0]\n    kernel = [0.2, 0.6, 0.2]\n    x = np.array(array, dtype=dtype)\n    y = np.array(kernel, dtype=dtype)\n\n    # Make pseudoimmutable\n    x.flags.writeable = False\n    y.flags.writeable = False\n\n    # make copies for post call comparison\n    x_copy = x.copy()\n    y_copy = y.copy()\n\n    with expected_boundary_warning(boundary=boundary):\n        convolve_fft(\n            x,\n            y,\n            boundary=boundary,\n            nan_treatment=nan_treatment,\n            normalize_kernel=normalize_kernel,\n            preserve_nan=preserve_nan,\n        )\n\n    # ( NaN == NaN ) = False\n    # Only compare non NaN values for canonical equivalence\n    # and then check NaN expl"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_convolve.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/convolution/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\nfrom contextlib import nullcontext\n\nimport numpy as np\nimport numpy.ma as ma\nimport pytest\nfrom numpy.testing import (\n    assert_allclose,\n    assert_array_almost_equal,\n    assert_array_almost_equal_nulp,\n)\n\nfrom astropy import units as u\nfrom astropy.convolution.convolve import convolve, convolve_fft\nfrom astropy.convolution.kernels import Gaussian2DKernel\nfrom astropy.utils.compat.optional_deps import HAS_PANDAS, HAS_SCIPY\nfrom astropy.utils.exceptions import AstropyUserWarning\n\nVALID_DTYPES = (\">f4\", \"<f4\", \">f8\", \"<f8\")\n\nBOUNDARY_OPTIONS = [None, \"fill\", \"wrap\", \"extend\"]\nNANHANDLING_OPTIONS = [\"interpolate\", \"fill\"]\nNORMALIZE_OPTIONS = [True, False]\nPRESERVE_NAN_OPTIONS = [True, False]\n\nconvolve_options = []\nfor boundary_option in BOUNDARY_OPTIONS:\n    convolve_options.append((convolve, boundary_option))\n\nconvolve_fft_options = [(convolve_fft, \"wrap\"), (convolve_fft, \"fill\")]\n\nBOUNDARIES_AND_CONVOLUTIONS = convolve_options + convolve_fft_options\n\n\nclass TestConvolve1D:\n    def test_list(self):\n        \"\"\"\n        Test that convolve works correctly when inputs are lists\n        \"\"\"\n\n        x = [1, 4, 5, 6, 5, 7, 8]\n        y = [0.2, 0.6, 0.2]\n        z = convolve(x, y, boundary=None)\n        assert_array_almost_equal_nulp(\n            z, np.array([0.0, 3.6, 5.0, 5.6, 5.6, 6.8, 0.0]), 10\n        )\n\n    def test_tuple(self):\n        \"\"\"\n        Test that convolve works correctly when inputs are tuples\n        \"\"\"\n\n        x = (1, 4, 5, 6, 5, 7, 8)\n        y = (0.2, 0.6, 0.2)\n        z = convolve(x, y, boundary=None)\n        assert_array_almost_equal_nulp(\n            z, np.array([0.0, 3.6, 5.0, 5.6, 5.6, 6.8, 0.0]), 10\n        )\n\n    @pytest.mark.parametrize(\"boundary\", BOUNDARY_OPTIONS)\n    @pytest.mark.parametrize(\"nan_treatment\", NANHANDLING_OPTIONS)\n    @pytest.mark.parametrize(\"normalize_kernel\", NORMALIZE_OPTIONS)\n    @pytest.mark.parametrize(\"preserve_nan\", PRESERVE_NAN_OPTIONS)\n    @pytest"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_convolve.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/convolution/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ormalize_kernel, preserve_nan, dtype\n    ):\n        \"\"\"\n        Test that convolve works correctly when inputs are lists\n        \"\"\"\n\n        array = [1.0, 4.0, 5.0, 6.0, 5.0, 7.0, 8.0]\n        kernel = [0.2, 0.6, 0.2]\n        x = np.array(array, dtype=dtype)\n        y = np.array(kernel, dtype=dtype)\n\n        # Make pseudoimmutable\n        x.flags.writeable = False\n        y.flags.writeable = False\n\n        convolve(\n            x,\n            y,\n            boundary=boundary,\n            nan_treatment=nan_treatment,\n            normalize_kernel=normalize_kernel,\n            preserve_nan=preserve_nan,\n        )\n\n        assert np.all(np.array(array, dtype=dtype) == x)\n        assert np.all(np.array(kernel, dtype=dtype) == y)\n\n    @pytest.mark.parametrize(\"boundary\", BOUNDARY_OPTIONS)\n    @pytest.mark.parametrize(\"nan_treatment\", NANHANDLING_OPTIONS)\n    @pytest.mark.parametrize(\"normalize_kernel\", NORMALIZE_OPTIONS)\n    @pytest.mark.parametrize(\"preserve_nan\", PRESERVE_NAN_OPTIONS)\n    @pytest.mark.parametrize(\"dtype\", VALID_DTYPES)\n    def test_input_unmodified_with_nan(\n        self, boundary, nan_treatment, normalize_kernel, preserve_nan, dtype\n    ):\n        \"\"\"\n        Test that convolve doesn't modify the input data\n        \"\"\"\n\n        array = [1.0, 4.0, 5.0, np.nan, 5.0, 7.0, 8.0]\n        kernel = [0.2, 0.6, 0.2]\n        x = np.array(array, dtype=dtype)\n        y = np.array(kernel, dtype=dtype)\n\n        # Make pseudoimmutable\n        x.flags.writeable = False\n        y.flags.writeable = False\n\n        # make copies for post call comparison\n        x_copy = x.copy()\n        y_copy = y.copy()\n\n        convolve(\n            x,\n            y,\n            boundary=boundary,\n            nan_treatment=nan_treatment,\n            normalize_kernel=normalize_kernel,\n            preserve_nan=preserve_nan,\n        )\n\n        # ( NaN == NaN ) = False\n        # Only compare non NaN values for canonical equivalence\n        # and then check NaN explicitly with np.isnan()\n   "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_convolve.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/convolution/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ".mark.parametrize(\"dtype\", VALID_DTYPES)\n    def test_quantity(\n        self, boundary, nan_treatment, normalize_kernel, preserve_nan, dtype\n    ):\n        \"\"\"\n        Test that convolve works correctly when input array is a Quantity\n        \"\"\"\n\n        x = np.array([1, 4, 5, 6, 5, 7, 8], dtype=dtype) * u.ph\n        y = np.array([0.2, 0.6, 0.2], dtype=dtype)\n        z = convolve(\n            x,\n            y,\n            boundary=boundary,\n            nan_treatment=nan_treatment,\n            normalize_kernel=normalize_kernel,\n            preserve_nan=preserve_nan,\n        )\n\n        assert x.unit == z.unit\n\n    @pytest.mark.parametrize(\"boundary\", BOUNDARY_OPTIONS)\n    @pytest.mark.parametrize(\"nan_treatment\", NANHANDLING_OPTIONS)\n    @pytest.mark.parametrize(\"normalize_kernel\", NORMALIZE_OPTIONS)\n    @pytest.mark.parametrize(\"preserve_nan\", PRESERVE_NAN_OPTIONS)\n    @pytest.mark.parametrize(\"dtype\", VALID_DTYPES)\n    def test_input_unmodified(\n        self, boundary, nan_treatment, normalize_kernel, preserve_nan, dtype\n    ):\n        \"\"\"\n        Test that convolve works correctly when inputs are lists\n        \"\"\"\n\n        array = [1.0, 4.0, 5.0, 6.0, 5.0, 7.0, 8.0]\n        kernel = [0.2, 0.6, 0.2]\n        x = np.array(array, dtype=dtype)\n        y = np.array(kernel, dtype=dtype)\n\n        # Make pseudoimmutable\n        x.flags.writeable = False\n        y.flags.writeable = False\n\n        convolve(\n            x,\n            y,\n            boundary=boundary,\n            nan_treatment=nan_treatment,\n            normalize_kernel=normalize_kernel,\n            preserve_nan=preserve_nan,\n        )\n\n        assert np.all(np.array(array, dtype=dtype) == x)\n        assert np.all(np.array(kernel, dtype=dtype) == y)\n\n    @pytest.mark.parametrize(\"boundary\", BOUNDARY_OPTIONS)\n    @pytest.mark.parametrize(\"nan_treatment\", NANHANDLING_OPTIONS)\n    @pytest.mark.parametrize(\"normalize_kernel\", NORMALIZE_OPTIONS)\n    @pytest.mark.parametrize(\"preserve_nan\", PRESERVE_NAN_OPTIONS)\n   "}, {"start_line": 1000, "end_line": 2624, "belongs_to": {"file_name": "test_convolution.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  assert results.shape == (1,)\n\n    x = np.arange(-1, 1, 0.1)\n    results = model(x)\n    assert results.shape == x.shape\n\n\n@pytest.mark.skipif(not HAS_SCIPY, reason=\"requires scipy\")\ndef test_input_shape_2d():\n    m1 = Const2D()\n    m2 = Const2D()\n\n    model = convolve_models_fft(m1, m2, ((-1, 1), (-1, 1)), 0.01)\n\n    results = model(0, 0)\n    assert results.shape == (1,)\n\n    x = np.arange(-1, 1, 0.1)\n    results = model(x, 0)\n    assert results.shape == x.shape\n    results = model(0, x)\n    assert results.shape == x.shape\n\n    grid = np.meshgrid(x, x)\n    results = model(*grid)\n    assert results.shape == grid[0].shape\n    assert results.shape == grid[1].shape\n\n\n@pytest.mark.skipif(not HAS_SCIPY, reason=\"requires scipy\")\ndef test__convolution_inputs():\n    m1 = Const2D()\n    m2 = Const2D()\n\n    model = convolve_models_fft(m1, m2, ((-1, 1), (-1, 1)), 0.01)\n\n    x = np.arange(-1, 1, 0.1)\n    y = np.arange(-2, 2, 0.1)\n    grid0 = np.meshgrid(x, x)\n    grid1 = np.meshgrid(y, y)\n\n    # scalar inputs\n    assert (np.array([1]), (1,)) == model._convolution_inputs(1)\n\n    # Multiple inputs\n    assert np.all(\n        model._convolution_inputs(*grid0)[0]\n        == np.reshape([grid0[0], grid0[1]], (2, -1)).T\n    )\n    assert model._convolution_inputs(*grid0)[1] == grid0[0].shape\n    assert np.all(\n        model._convolution_inputs(*grid1)[0]\n        == np.reshape([grid1[0], grid1[1]], (2, -1)).T\n    )\n    assert model._convolution_inputs(*grid1)[1] == grid1[0].shape\n\n    # Error\n    with pytest.raises(ValueError, match=r\"Values have differing shapes\"):\n        model._convolution_inputs(grid0[0], grid1[1])\n"}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "test_convolve.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/convolution/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " convolve(x, y, preserve_nan=preserve_nan)\n\n        if preserve_nan:\n            assert np.isnan(z[2])\n            z[2] = 8\n\n        assert_array_almost_equal_nulp(z, (8 / 3.0, 4, 8, 12, 8), 10)\n\n    @pytest.mark.parametrize(\n        \"array, exc_type, match\",\n        [\n            (0, ValueError, \"cannot convolve 0-dimensional arrays\"),\n            (\n                [[1]],\n                ValueError,\n                r\"array and kernel have differing number of dimensions\\.\",\n            ),\n            ([], ValueError, \"cannot convolve empty array\"),\n            (\n                np.ones((1, 1, 1, 1)),\n                NotImplementedError,\n                \"convolve only supports 1, 2, and 3-dimensional arrays at this time\",\n            ),\n        ],\n    )\n    def test_exceptions(self, array, exc_type, match):\n        kernel = [1]\n        with pytest.raises(exc_type, match=match):\n            convolve(array, kernel)\n\n\nclass TestConvolve2D:\n    def test_list(self):\n        \"\"\"\n        Test that convolve works correctly when inputs are lists\n        \"\"\"\n        x = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]\n\n        z = convolve(x, x, boundary=\"fill\", fill_value=1, normalize_kernel=True)\n        assert_array_almost_equal_nulp(z, x, 10)\n        z = convolve(x, x, boundary=\"fill\", fill_value=1, normalize_kernel=False)\n        assert_array_almost_equal_nulp(z, np.array(x, float) * 9, 10)\n\n    @pytest.mark.parametrize(\"dtype_array\", VALID_DTYPES)\n    @pytest.mark.parametrize(\"dtype_kernel\", VALID_DTYPES)\n    def test_dtype(self, dtype_array, dtype_kernel):\n        \"\"\"\n        Test that 32- and 64-bit floats are correctly handled\n        \"\"\"\n\n        x = np.array(\n            [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], dtype=dtype_array\n        )\n\n        y = np.array(\n            [[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]], dtype=dtype_kernel\n        )\n\n        z = convolve(x, y)\n\n        assert x.dtype == z.dtype\n\n    @pytest.mark.parametrize(\"boundary\", BOUNDARY_OPT"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "test_convolve.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/convolution/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "1, -1, 8, -1, -1, -1, -1],\n            mask=[1, 0, 0, 0, 0, 0, 0, 0, 0],\n            fill_value=0.0,\n        )\n\n        z = convolve(x, y, boundary=boundary, normalize_kernel=normalize_kernel)\n\n        # boundary, normalize_kernel == False\n        rslt = {\n            (None): [0.0, 0.0, 0.0, 0.0, 9.0, 0.0, 0.0, 0.0, 0.0],\n            \"fill\": [-1.0, 3.0, 6.0, 8.0, 9.0, 10.0, 21.0, 33.0, 46.0],\n            \"wrap\": [-31.0, -21.0, -11.0, -1.0, 9.0, 10.0, 20.0, 30.0, 40.0],\n            \"extend\": [-5.0, 0.0, 4.0, 7.0, 9.0, 10.0, 12.0, 15.0, 19.0],\n        }[boundary]\n\n        assert_array_almost_equal_nulp(z, np.array(rslt, dtype=\">f8\"), 10)\n\n    @pytest.mark.parametrize(\"preserve_nan\", PRESERVE_NAN_OPTIONS)\n    def test_int_masked_array(self, preserve_nan):\n        \"\"\"\n        Test that convolve works correctly with integer masked arrays.\n        \"\"\"\n\n        x = ma.array([3, 5, 7, 11, 13], mask=[0, 0, 1, 0, 0], fill_value=0.0)\n        y = np.array([1.0, 1.0, 1.0], dtype=\">f8\")\n\n        z = convolve(x, y, preserve_nan=preserve_nan)\n\n        if preserve_nan:\n            assert np.isnan(z[2])\n            z[2] = 8\n\n        assert_array_almost_equal_nulp(z, (8 / 3.0, 4, 8, 12, 8), 10)\n\n    @pytest.mark.parametrize(\n        \"array, exc_type, match\",\n        [\n            (0, ValueError, \"cannot convolve 0-dimensional arrays\"),\n            (\n                [[1]],\n                ValueError,\n                r\"array and kernel have differing number of dimensions\\.\",\n            ),\n            ([], ValueError, \"cannot convolve empty array\"),\n            (\n                np.ones((1, 1, 1, 1)),\n                NotImplementedError,\n                \"convolve only supports 1, 2, and 3-dimensional arrays at this time\",\n            ),\n        ],\n    )\n    def test_exceptions(self, array, exc_type, match):\n        kernel = [1]\n        with pytest.raises(exc_type, match=match):\n            convolve(array, kernel)\n\n\nclass TestConvolve2D:\n    def test_list(self):\n        \"\"\"\n        Test "}], "retrieved_count": 10, "cost_time": 1.1753296852111816}
{"question": "Why does the function that validates unified content descriptor strings enforce stricter standard compliance when the configuration flag for VOTable version 1.2 or later is enabled?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 4000, "end_line": 5757, "belongs_to": {"file_name": "ucd.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ary(word):\n                        raise ValueError(\n                            f\"Secondary word '{word}' is not valid as a primary word\"\n                        )\n                    else:\n                        raise ValueError(f\"Unknown word '{word}'\")\n            else:\n                if not _ucd_singleton.is_secondary(word):\n                    if _ucd_singleton.is_primary(word):\n                        raise ValueError(\n                            f\"Primary word '{word}' is not valid as a secondary word\"\n                        )\n                    else:\n                        raise ValueError(f\"Unknown word '{word}'\")\n\n        try:\n            normalized_word = _ucd_singleton.normalize_capitalization(word)\n        except KeyError:\n            normalized_word = word\n        words.append((ns, normalized_word))\n\n    return words\n\n\ndef check_ucd(ucd, check_controlled_vocabulary=False, has_colon=False):\n    \"\"\"\n    Returns False if *ucd* is not a valid `unified content descriptor`_.\n\n    Parameters\n    ----------\n    ucd : str\n        The UCD string\n\n    check_controlled_vocabulary : bool, optional\n        If `True`, then each word in the UCD will be verified against\n        the UCD1+ controlled vocabulary, (as required by the VOTable\n        specification version 1.2), otherwise not.\n\n    has_colon : bool, optional\n        If `True`, the UCD may contain a colon (as defined in earlier\n        versions of the standard).\n\n    Returns\n    -------\n    valid : bool\n    \"\"\"\n    if ucd is None:\n        return True\n\n    try:\n        parse_ucd(\n            ucd,\n            check_controlled_vocabulary=check_controlled_vocabulary,\n            has_colon=has_colon,\n        )\n    except ValueError:\n        return False\n    return True\n"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "tree.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " the field this year was found in (used for error\n        message)\n\n    config, pos : optional\n        Information about the source of the value\n    \"\"\"\n    if string is not None and not isinstance(string, str):\n        warn_or_raise(W08, W08, attr_name, config, pos)\n        return False\n    return True\n\n\ndef resolve_id(ID, id, config=None, pos=None):\n    if ID is None and id is not None:\n        warn_or_raise(W09, W09, (), config, pos)\n        return id\n    return ID\n\n\ndef check_ucd(ucd, config=None, pos=None):\n    \"\"\"\n    Warns or raises a\n    `~astropy.io.votable.exceptions.VOTableSpecError` if *ucd* is not\n    a valid `unified content descriptor`_ string as defined by the\n    VOTABLE standard.\n\n    Parameters\n    ----------\n    ucd : str\n        A UCD string.\n\n    config, pos : optional\n        Information about the source of the value\n    \"\"\"\n    if config is None:\n        config = {}\n    if config.get(\"version_1_1_or_later\"):\n        try:\n            ucd_mod.parse_ucd(\n                ucd,\n                check_controlled_vocabulary=config.get(\"version_1_2_or_later\", False),\n                has_colon=config.get(\"version_1_2_or_later\", False),\n            )\n        except ValueError as e:\n            # This weird construction is for Python 3 compatibility\n            if config.get(\"verify\", \"ignore\") == \"exception\":\n                vo_raise(W06, (ucd, str(e)), config, pos)\n            elif config.get(\"verify\", \"ignore\") == \"warn\":\n                vo_warn(W06, (ucd, str(e)), config, pos)\n                return False\n            else:\n                return False\n    return True\n\n\n######################################################################\n# PROPERTY MIXINS\nclass _IDProperty:\n    @property\n    def ID(self):\n        \"\"\"\n        The XML ID_ of the element.  May be `None` or a string\n        conforming to XML ID_ syntax.\n        \"\"\"\n        return self._ID\n\n    @ID.setter\n    def ID(self, ID):\n        xmlutil.check_id(ID, \"ID\", self._config, self._pos)\n  "}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "ucd.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " = re.search(r\"[^A-Za-z0-9_.;\\-]\", ucd)\n    if m is not None:\n        raise ValueError(f\"UCD has invalid character '{m.group(0)}' in '{ucd}'\")\n\n    word_component_re = r\"[A-Za-z0-9][A-Za-z0-9\\-_]*\"\n    word_re = rf\"{word_component_re}(\\.{word_component_re})*\"\n\n    parts = ucd.split(\";\")\n    words = []\n    for i, word in enumerate(parts):\n        colon_count = word.count(\":\")\n        if colon_count == 1:\n            ns, word = word.split(\":\", 1)\n            if not re.match(word_component_re, ns):\n                raise ValueError(f\"Invalid namespace '{ns}'\")\n            ns = ns.lower()\n        elif colon_count > 1:\n            raise ValueError(f\"Too many colons in '{word}'\")\n        else:\n            ns = \"ivoa\"\n\n        if not re.match(word_re, word):\n            raise ValueError(f\"Invalid word '{word}'\")\n\n        if ns == \"ivoa\" and check_controlled_vocabulary:\n            if i == 0:\n                if not _ucd_singleton.is_primary(word):\n                    if _ucd_singleton.is_secondary(word):\n                        raise ValueError(\n                            f\"Secondary word '{word}' is not valid as a primary word\"\n                        )\n                    else:\n                        raise ValueError(f\"Unknown word '{word}'\")\n            else:\n                if not _ucd_singleton.is_secondary(word):\n                    if _ucd_singleton.is_primary(word):\n                        raise ValueError(\n                            f\"Primary word '{word}' is not valid as a secondary word\"\n                        )\n                    else:\n                        raise ValueError(f\"Unknown word '{word}'\")\n\n        try:\n            normalized_word = _ucd_singleton.normalize_capitalization(word)\n        except KeyError:\n            normalized_word = word\n        words.append((ns, normalized_word))\n\n    return words\n\n\ndef check_ucd(ucd, check_controlled_vocabulary=False, has_colon=False):\n    \"\"\"\n    Returns False if *ucd* is not a valid `unified content descriptor"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "exceptions.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "le-1.2.html#sec:link>`__\n    \"\"\"\n\n    message_template = \"content-type '{}' must be a valid MIME content type\"\n    default_args = (\"x\",)\n\n\nclass W05(VOTableSpecWarning):\n    \"\"\"\n    The attribute must be a valid URI as defined in `RFC 2396\n    <https://www.ietf.org/rfc/rfc2396.txt>`_.\n    \"\"\"\n\n    message_template = \"'{}' is not a valid URI\"\n    default_args = (\"x\",)\n\n\nclass W06(VOTableSpecWarning):\n    \"\"\"\n    This warning is emitted when a ``ucd`` attribute does not match\n    the syntax of a `unified content descriptor\n    <https://vizier.unistra.fr/doc/UCD.htx>`__.\n\n    If the VOTable version is 1.2 or later, the UCD will also be\n    checked to ensure it conforms to the controlled vocabulary defined\n    by UCD1+.\n\n    **References**: `1.1\n    <http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#sec:ucd>`__,\n    `1.2\n    <http://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html#sec:ucd>`__\n    \"\"\"\n\n    message_template = \"Invalid UCD '{}': {}\"\n    default_args = (\"x\", \"explanation\")\n\n\nclass W07(VOTableSpecWarning):\n    \"\"\"Invalid astroYear.\n\n    As astro year field is a Besselian or Julian year matching the\n    regular expression::\n\n        ^[JB]?[0-9]+([.][0-9]*)?$\n\n    Defined in this XML Schema snippet::\n\n        <xs:simpleType  name=\"astroYear\">\n          <xs:restriction base=\"xs:token\">\n            <xs:pattern  value=\"[JB]?[0-9]+([.][0-9]*)?\"/>\n          </xs:restriction>\n        </xs:simpleType>\n    \"\"\"\n\n    message_template = \"Invalid astroYear in {}: '{}'\"\n    default_args = (\"x\", \"y\")\n\n\nclass W08(VOTableSpecWarning):\n    \"\"\"\n    To avoid local-dependent number parsing differences, ``astropy.io.votable``\n    may require a string or unicode string where a numeric type may\n    make more sense.\n    \"\"\"\n\n    message_template = \"'{}' must be a str or bytes object\"\n\n    default_args = (\"x\",)\n\n\nclass W09(VOTableSpecWarning):\n    \"\"\"\n    The VOTable specification uses the attribute name ``ID`` (with\n    uppercase letters) to spe"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "tree.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "#####\n# ATTRIBUTE CHECKERS\ndef check_astroyear(year, field, config=None, pos=None):\n    \"\"\"\n    Raises a `~astropy.io.votable.exceptions.VOTableSpecError` if\n    *year* is not a valid astronomical year as defined by the VOTABLE\n    standard.\n\n    Parameters\n    ----------\n    year : str\n        An astronomical year string\n\n    field : str\n        The name of the field this year was found in (used for error\n        message)\n\n    config, pos : optional\n        Information about the source of the value\n    \"\"\"\n    if year is not None and re.match(r\"^[JB]?[0-9]+([.][0-9]*)?$\", year) is None:\n        warn_or_raise(W07, W07, (field, year), config, pos)\n        return False\n    return True\n\n\ndef check_string(string, attr_name, config=None, pos=None):\n    \"\"\"\n    Raises a `~astropy.io.votable.exceptions.VOTableSpecError` if\n    *string* is not a string or Unicode string.\n\n    Parameters\n    ----------\n    string : str\n        An astronomical year string\n\n    attr_name : str\n        The name of the field this year was found in (used for error\n        message)\n\n    config, pos : optional\n        Information about the source of the value\n    \"\"\"\n    if string is not None and not isinstance(string, str):\n        warn_or_raise(W08, W08, attr_name, config, pos)\n        return False\n    return True\n\n\ndef resolve_id(ID, id, config=None, pos=None):\n    if ID is None and id is not None:\n        warn_or_raise(W09, W09, (), config, pos)\n        return id\n    return ID\n\n\ndef check_ucd(ucd, config=None, pos=None):\n    \"\"\"\n    Warns or raises a\n    `~astropy.io.votable.exceptions.VOTableSpecError` if *ucd* is not\n    a valid `unified content descriptor`_ string as defined by the\n    VOTABLE standard.\n\n    Parameters\n    ----------\n    ucd : str\n        A UCD string.\n\n    config, pos : optional\n        Information about the source of the value\n    \"\"\"\n    if config is None:\n        config = {}\n    if config.get(\"version_1_1_or_later\"):\n        try:\n            ucd_mod.parse_ucd(\n          "}, {"start_line": 0, "end_line": 1939, "belongs_to": {"file_name": "test_ucd.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\nimport pytest\n\nfrom astropy.io.votable import ucd\n\n\ndef test_none():\n    assert ucd.check_ucd(None)\n\n\nexamples = {\n    \"phys.temperature\": [(\"ivoa\", \"phys.temperature\")],\n    \"pos.eq.ra;meta.main\": [(\"ivoa\", \"pos.eq.ra\"), (\"ivoa\", \"meta.main\")],\n    \"meta.id;src\": [(\"ivoa\", \"meta.id\"), (\"ivoa\", \"src\")],\n    \"phot.flux;em.radio;arith.ratio\": [\n        (\"ivoa\", \"phot.flux\"),\n        (\"ivoa\", \"em.radio\"),\n        (\"ivoa\", \"arith.ratio\"),\n    ],\n    \"PHot.Flux;EM.Radio;ivoa:arith.Ratio\": [\n        (\"ivoa\", \"phot.flux\"),\n        (\"ivoa\", \"em.radio\"),\n        (\"ivoa\", \"arith.ratio\"),\n    ],\n    \"pos.galactic.lat\": [(\"ivoa\", \"pos.galactic.lat\")],\n    \"meta.code;phot.mag\": [(\"ivoa\", \"meta.code\"), (\"ivoa\", \"phot.mag\")],\n    \"stat.error;phot.mag\": [(\"ivoa\", \"stat.error\"), (\"ivoa\", \"phot.mag\")],\n    \"phys.temperature;instr;stat.max\": [\n        (\"ivoa\", \"phys.temperature\"),\n        (\"ivoa\", \"instr\"),\n        (\"ivoa\", \"stat.max\"),\n    ],\n    \"stat.error;phot.mag;em.opt.V\": [\n        (\"ivoa\", \"stat.error\"),\n        (\"ivoa\", \"phot.mag\"),\n        (\"ivoa\", \"em.opt.V\"),\n    ],\n    \"phot.color;em.opt.B;em.opt.V\": [\n        (\"ivoa\", \"phot.color\"),\n        (\"ivoa\", \"em.opt.B\"),\n        (\"ivoa\", \"em.opt.V\"),\n    ],\n    \"stat.error;phot.color;em.opt.B;em.opt.V\": [\n        (\"ivoa\", \"stat.error\"),\n        (\"ivoa\", \"phot.color\"),\n        (\"ivoa\", \"em.opt.B\"),\n        (\"ivoa\", \"em.opt.V\"),\n    ],\n}\n\n\ndef test_check():\n    for s, p in examples.items():\n        assert ucd.parse_ucd(s, True, True) == p\n        assert ucd.check_ucd(s, True, True)\n\n\ndef test_too_many_colons():\n    with pytest.raises(ValueError):\n        ucd.parse_ucd(\"ivoa:stsci:phot\", True, True)\n\n\ndef test_invalid_namespace():\n    with pytest.raises(ValueError):\n        ucd.parse_ucd(\"_ivoa:phot.mag\", True, True)\n\n\ndef test_invalid_word():\n    with pytest.raises(ValueError):\n        ucd.parse_ucd(\"-pho\")\n"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "exceptions.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "s not indicate that the input file is invalid\n        with respect to the VOTable specification, only that the\n        column names in the record array may not match exactly the\n        ``name`` attributes specified in the file.\n\n    **References**: `1.1\n    <http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#sec:name>`__,\n    `1.2\n    <http://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html#sec:name>`__\n    \"\"\"\n\n    message_template = \"Implicitly generating an ID from a name '{}' -> '{}'\"\n    default_args = (\"x\", \"y\")\n\n\nclass W04(VOTableSpecWarning):\n    \"\"\"\n    The ``content-type`` attribute must use MIME content-type syntax as\n    defined in `RFC 2046 <https://tools.ietf.org/html/rfc2046>`__.\n\n    The current check for validity is somewhat over-permissive.\n\n    **References**: `1.1\n    <http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#sec:link>`__,\n    `1.2\n    <http://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html#sec:link>`__\n    \"\"\"\n\n    message_template = \"content-type '{}' must be a valid MIME content type\"\n    default_args = (\"x\",)\n\n\nclass W05(VOTableSpecWarning):\n    \"\"\"\n    The attribute must be a valid URI as defined in `RFC 2396\n    <https://www.ietf.org/rfc/rfc2396.txt>`_.\n    \"\"\"\n\n    message_template = \"'{}' is not a valid URI\"\n    default_args = (\"x\",)\n\n\nclass W06(VOTableSpecWarning):\n    \"\"\"\n    This warning is emitted when a ``ucd`` attribute does not match\n    the syntax of a `unified content descriptor\n    <https://vizier.unistra.fr/doc/UCD.htx>`__.\n\n    If the VOTable version is 1.2 or later, the UCD will also be\n    checked to ensure it conforms to the controlled vocabulary defined\n    by UCD1+.\n\n    **References**: `1.1\n    <http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#sec:ucd>`__,\n    `1.2\n    <http://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html#sec:ucd>`__\n    \"\"\"\n\n    message_template = \"Invalid UCD '{}': {}\"\n    "}, {"start_line": 33000, "end_line": 35000, "belongs_to": {"file_name": "exceptions.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ng::\n\n        query, hints, doc, location\n\n    And in VOTable 1.3, additionally::\n\n        type\n\n    **References**: `1.1\n    <http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#ToC54>`__,\n    `1.2\n    <http://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html#ToC58>`__\n    `1.3\n    <http://www.ivoa.net/documents/VOTable/20130315/PR-VOTable-1.3-20130315.html#sec:link>`__\n    \"\"\"\n\n    message_template = \"content-role attribute '{}' invalid\"\n    default_args = (\"x\",)\n\n\nclass W46(VOTableSpecWarning):\n    \"\"\"\n    The given char or unicode string is too long for the specified\n    field length.\n    \"\"\"\n\n    message_template = \"{} value is too long for specified length of {}\"\n    default_args = (\"char or unicode\", \"x\")\n\n\nclass W47(VOTableSpecWarning):\n    \"\"\"\n    If no arraysize is specified on a char field, the default of '1'\n    is implied, but this is rarely what is intended.\n    \"\"\"\n\n    message_template = \"Missing arraysize indicates length 1\"\n\n\nclass W48(VOTableSpecWarning):\n    \"\"\"\n    The attribute is not defined in the specification.\n    \"\"\"\n\n    message_template = \"Unknown attribute '{}' on {}\"\n    default_args = (\"attribute\", \"element\")\n\n\nclass W49(VOTableSpecWarning):\n    \"\"\"\n    Prior to VOTable 1.3, the empty cell was illegal for integer\n    fields.\n\n    If a \\\"null\\\" value was specified for the cell, it will be used\n    for the value, otherwise, 0 will be used.\n    \"\"\"\n\n    message_template = \"Empty cell illegal for integer fields.\"\n\n\nclass W50(VOTableSpecWarning):\n    \"\"\"\n    Invalid unit string as defined in the `Units in the VO, Version 1.0\n    <https://www.ivoa.net/documents/VOUnits/20140523/index.html>`_\n    (VOTable version >= 1.4) or `Standards for Astronomical Catalogues,\n    Version 2.0 <https://cdsarc.cds.unistra.fr/doc/catstd-3.2.htx>`_\n    (version < 1.4).\n\n    Consider passing an explicit ``unit_format`` parameter if the units\n    in this file conform to another specification.\n    \"\"\"\n\n    message_template = \""}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_converter.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\nimport io\n\n# THIRD-PARTY\nimport numpy as np\nimport pytest\nfrom numpy.testing import assert_array_equal\n\n# LOCAL\nfrom astropy.io.votable import converters, exceptions, tree\nfrom astropy.io.votable.table import parse_single_table\nfrom astropy.utils.data import get_pkg_data_filename\n\n\ndef test_invalid_arraysize():\n    with pytest.raises(exceptions.E13):\n        field = tree.Field(None, name=\"broken\", datatype=\"char\", arraysize=\"foo\")\n        converters.get_converter(field)\n\n\ndef test_oversize_char():\n    config = {\"verify\": \"exception\"}\n    with pytest.warns(exceptions.W47) as w:\n        field = tree.Field(None, name=\"c\", datatype=\"char\", config=config)\n        c = converters.get_converter(field, config=config)\n    assert len(w) == 1\n\n    with pytest.warns(exceptions.W46) as w:\n        c.parse(\"XXX\")\n    assert len(w) == 1\n\n\ndef test_char_mask():\n    config = {\"verify\": \"exception\"}\n    field = tree.Field(None, name=\"c\", arraysize=\"1\", datatype=\"char\", config=config)\n    c = converters.get_converter(field, config=config)\n    assert c.output(\"Foo\", True) == \"\"\n\n\ndef test_oversize_unicode():\n    config = {\"verify\": \"exception\"}\n    with pytest.warns(exceptions.W46) as w:\n        field = tree.Field(\n            None, name=\"c2\", datatype=\"unicodeChar\", arraysize=\"1\", config=config\n        )\n        c = converters.get_converter(field, config=config)\n        c.parse(\"XXX\")\n    assert len(w) == 1\n\n\ndef test_bounded_variable_size_unicode():\n    # regression test for #18075\n    field = tree.Field(None, name=\"unicode\", datatype=\"unicodeChar\", arraysize=\"20*\")\n    unicode_converter = converters.get_converter(field)\n    assert unicode_converter.parse(\"XXX\") == (\"XXX\", False)\n\n\ndef test_unicode_mask():\n    config = {\"verify\": \"exception\"}\n    field = tree.Field(\n        None, name=\"c\", arraysize=\"1\", datatype=\"unicodeChar\", config=config\n    )\n    c = converters.get_converter(field, config=config)\n    assert c.output("}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "xmlutil.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "xml id.\n\n    This is rather simplistic at the moment, since it just replaces\n    non-valid characters with underscores.\n    \"\"\"\n    if ID is None:\n        return None\n    corrected = xml_check.fix_id(ID)\n    if corrected != ID:\n        vo_warn(W03, (ID, corrected), config, pos)\n    return corrected\n\n\n_token_regex = r\"(?![\\r\\l\\t ])[^\\r\\l\\t]*(?![\\r\\l\\t ])\"\n\n\ndef check_token(token, attr_name, config=None, pos=None):\n    \"\"\"\n    Raises a `ValueError` if *token* is not a valid XML token.\n\n    As defined by XML Schema Part 2.\n    \"\"\"\n    return token is None or xml_check.check_token(token)\n\n\ndef check_mime_content_type(content_type, config=None, pos=None):\n    \"\"\"\n    Raises a `~astropy.io.votable.exceptions.VOTableSpecError` if\n    *content_type* is not a valid MIME content type.\n\n    As defined by RFC 2045 (syntactically, at least).\n    \"\"\"\n    if content_type is not None and not xml_check.check_mime_content_type(content_type):\n        warn_or_raise(W04, W04, content_type, config, pos)\n        return False\n    return True\n\n\ndef check_anyuri(uri, config=None, pos=None):\n    \"\"\"\n    Raises a `~astropy.io.votable.exceptions.VOTableSpecError` if\n    *uri* is not a valid URI.\n\n    As defined in RFC 2396.\n    \"\"\"\n    if uri is not None and not xml_check.check_anyuri(uri):\n        warn_or_raise(W05, W05, uri, config, pos)\n        return False\n    return True\n\n\ndef validate_schema(filename, version=\"1.1\"):\n    \"\"\"\n    Validates the given file against the appropriate VOTable schema.\n\n    Parameters\n    ----------\n    filename : str\n        The path to the XML file to validate\n\n    version : str, optional\n        The VOTABLE version to check, which must be a string \\\"1.0\\\",\n        \\\"1.1\\\", \\\"1.2\\\" or \\\"1.3\\\".  If it is not one of these,\n        version \\\"1.1\\\" is assumed.\n\n        For version \\\"1.0\\\", it is checked against a DTD, since that\n        version did not have an XML Schema.\n\n    Returns\n    -------\n    returncode, stdout, stderr : int, str, str\n        Returns the retu"}], "retrieved_count": 10, "cost_time": 1.1798133850097656}
{"question": "Why does the serialization state preparation method use the mixin-safe copying function conditionally only for columns that are not instances of the base column class?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "serialize.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ed columns the ``mixin_cols`` dict is updated with required\n    attributes and information to subsequently reconstruct the table.\n\n    Table mixin columns are always serialized and get represented by one\n    or more data columns.  In earlier versions of the code *only* mixin\n    columns were serialized, hence the use within this code of \"mixin\"\n    to imply serialization.  Starting with version 3.1, the non-mixin\n    ``MaskedColumn`` can also be serialized.\n    \"\"\"\n    obj_attrs = col.info._represent_as_dict()\n\n    # If serialization is not required (see function docstring above)\n    # or explicitly specified as excluded, then treat as a normal column.\n    if not obj_attrs or col.__class__ in exclude_classes:\n        new_cols.append(col)\n        return\n\n    # Subtlety here is handling mixin info attributes.  The basic list of such\n    # attributes is: 'name', 'unit', 'dtype', 'format', 'description', 'meta'.\n    # - name: handled directly [DON'T store]\n    # - unit: DON'T store if this is a parent attribute\n    # - dtype: captured in plain Column if relevant [DON'T store]\n    # - format: possibly irrelevant but settable post-object creation [DO store]\n    # - description: DO store\n    # - meta: DO store\n    info = {}\n    for attr, nontrivial in (\n        (\"unit\", lambda x: x is not None and x != \"\"),\n        (\"format\", lambda x: x is not None),\n        (\"description\", lambda x: x is not None),\n        (\"meta\", lambda x: x),\n    ):\n        col_attr = getattr(col.info, attr)\n        if nontrivial(col_attr):\n            info[attr] = col_attr\n\n    # Find column attributes that have the same length as the column itself.\n    # These will be stored in the table as new columns (aka \"data attributes\").\n    # Examples include SkyCoord.ra (what is typically considered the data and is\n    # always an array) and Skycoord.obs_time (which can be a scalar or an\n    # array).\n    data_attrs = [\n        key\n        for key, value in obj_attrs.items()\n        if getattr(value, \"shape\""}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "serialize.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "   # plain Column objects (in new_cols) + metadata (in mixin_cols).\n    for col in tbl.itercols():\n        _represent_mixin_as_column(\n            col, col.info.name, new_cols, mixin_cols, exclude_classes=exclude_classes\n        )\n\n    # If no metadata was created then just return the original table.\n    if mixin_cols:\n        meta = deepcopy(tbl.meta)\n        meta[\"__serialized_columns__\"] = mixin_cols\n        out = Table(new_cols, meta=meta, copy=False)\n    else:\n        out = tbl\n\n    for col in out.itercols():\n        if not isinstance(col, Column) and col.__class__ not in exclude_classes:\n            # This catches columns for which info has not been set up right and\n            # therefore were not converted. See the corresponding test in\n            # test_mixin.py for an example.\n            raise TypeError(\n                \"failed to represent column \"\n                f\"{col.info.name!r} ({col.__class__.__name__}) as one \"\n                \"or more Column subclasses. This looks like a mixin class \"\n                \"that does not have the correct _represent_as_dict() method \"\n                \"in the class `info` attribute.\"\n            )\n\n    return out\n\n\ndef _construct_mixin_from_obj_attrs_and_info(obj_attrs, info):\n    # If this is a supported class then import the class and run\n    # the _construct_from_col method.  Prevent accidentally running\n    # untrusted code by only importing known astropy classes.\n    cls_full_name = obj_attrs.pop(\"__class__\", None)\n    if cls_full_name is None:\n        # We're dealing with a SerializedColumn holding columns, stored in\n        # obj_attrs. For this case, info holds the name (and nothing else).\n        mixin = SerializedColumn(obj_attrs)\n        mixin.info.name = info[\"name\"]\n        return mixin\n\n    # We translate locally created skyoffset frames and treat all\n    # built-in frames as known.\n    if cls_full_name.startswith(\"abc.SkyOffset\"):\n        cls_full_name = \"astropy.coordinates.SkyOffsetFrame\"\n    elif (\n  "}, {"start_line": 1000, "end_line": 2171, "belongs_to": {"file_name": "ndarray_mixin.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " \"__dict__\", ()):\n            self.info = obj.info\n        return self\n\n    def __array_finalize__(self, obj):\n        if obj is None:\n            return\n\n        if callable(super().__array_finalize__):\n            super().__array_finalize__(obj)\n\n        # Self was created from template (e.g. obj[slice] or (obj * 2))\n        # or viewcast e.g. obj.view(Column).  In either case we want to\n        # init Column attributes for self from obj if possible.\n        if \"info\" in getattr(obj, \"__dict__\", ()):\n            self.info = obj.info\n\n    def __reduce__(self):\n        # patch to pickle NdArrayMixin objects (ndarray subclasses), see\n        # http://www.mail-archive.com/numpy-discussion@scipy.org/msg02446.html\n\n        object_state = list(super().__reduce__())\n        object_state[2] = (object_state[2], self.__dict__)\n        return tuple(object_state)\n\n    def __setstate__(self, state):\n        # patch to unpickle NdarrayMixin objects (ndarray subclasses), see\n        # http://www.mail-archive.com/numpy-discussion@scipy.org/msg02446.html\n\n        nd_state, own_state = state\n        super().__setstate__(nd_state)\n        self.__dict__.update(own_state)\n"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "column.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "mn\n        Input column\n    copy_indices : bool\n        Copy the column ``indices`` attribute\n\n    Returns\n    -------\n    col : Copy of input column\n    \"\"\"\n    if isinstance(col, BaseColumn):\n        return col.copy()\n\n    newcol = col.copy() if hasattr(col, \"copy\") else deepcopy(col)\n    # If the column has info defined, we copy it and adjust any indices\n    # to point to the copied column.  By guarding with the if statement,\n    # we avoid side effects (of creating the default info instance).\n    if \"info\" in col.__dict__:\n        newcol.info = col.info\n        if copy_indices and col.info.indices:\n            newcol.info.indices = deepcopy(col.info.indices)\n            for index in newcol.info.indices:\n                index.replace_col(col, newcol)\n\n    return newcol\n\n\nclass FalseArray(np.ndarray):\n    \"\"\"\n    Boolean mask array that is always False.\n\n    This is used to create a stub ``mask`` property which is a boolean array of\n    ``False`` used by default for mixin columns and corresponding to the mixin\n    column data shape.  The ``mask`` looks like a normal numpy array but an\n    exception will be raised if ``True`` is assigned to any element.  The\n    consequences of the limitation are most obvious in the high-level table\n    operations.\n\n    Parameters\n    ----------\n    shape : tuple\n        Data shape\n    \"\"\"\n\n    def __new__(cls, shape):\n        obj = np.zeros(shape, dtype=bool).view(cls)\n        return obj\n\n    def __setitem__(self, item, val):\n        val = np.asarray(val)\n        if np.any(val):\n            raise ValueError(\n                f\"Cannot set any element of {type(self).__name__} class to True\"\n            )\n\n\ndef _expand_string_array_for_values(arr, values):\n    \"\"\"\n    For string-dtype return a version of ``arr`` that is wide enough for ``values``.\n    If ``arr`` is not string-dtype or does not need expansion then return ``arr``.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Input array\n    values : scalar or array-like\n "}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "serialize.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "inates.angles.Longitude\",\n    \"astropy.coordinates.angles.Angle\",\n)\n\n\nclass SerializedColumnInfo(MixinInfo):\n    \"\"\"\n    Minimal info to allow SerializedColumn to be recognized as a mixin Column.\n\n    Used to help create a dict of columns in ColumnInfo for structured data.\n    \"\"\"\n\n    def _represent_as_dict(self):\n        # SerializedColumn is already a `dict`, so we can return it directly.\n        return self._parent\n\n\nclass SerializedColumn(dict):\n    \"\"\"Subclass of dict used to serialize  mixin columns.\n\n    It is used in the representation to contain the name and possible\n    other info for a mixin column or attribute (either primary data or an\n    array-like attribute) that is serialized as a column in the table.\n\n    \"\"\"\n\n    info = SerializedColumnInfo()\n\n    @property\n    def shape(self):\n        \"\"\"Minimal shape implementation to allow use as a mixin column.\n\n        Returns the shape of the first item that has a shape at all,\n        or ``()`` if none of the values has a shape attribute.\n        \"\"\"\n        return next(\n            (value.shape for value in self.values() if hasattr(value, \"shape\")), ()\n        )\n\n\ndef _represent_mixin_as_column(col, name, new_cols, mixin_cols, exclude_classes=()):\n    \"\"\"Carry out processing needed to serialize ``col`` in an output table\n    consisting purely of plain ``Column`` or ``MaskedColumn`` columns.  This\n    relies on the object determine if any transformation is required and may\n    depend on the ``serialize_method`` and ``serialize_context`` context\n    variables.  For instance a ``MaskedColumn`` may be stored directly to\n    FITS, but can also be serialized as separate data and mask columns.\n\n    This function builds up a list of plain columns in the ``new_cols`` arg (which\n    is passed as a persistent list).  This includes both plain columns from the\n    original table and plain columns that represent data from serialized columns\n    (e.g. ``jd1`` and ``jd2`` arrays from a ``Time`` column).\n\n    For serializ"}, {"start_line": 15000, "end_line": 17000, "belongs_to": {"file_name": "serialize.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      cls_full_name not in __construct_mixin_classes\n        and not cls_full_name.startswith(\"astropy.coordinates.builtin_frames\")\n    ):\n        raise ValueError(f\"unsupported class for construct {cls_full_name}\")\n\n    mod_name, _, cls_name = cls_full_name.rpartition(\".\")\n    module = import_module(mod_name)\n    cls = getattr(module, cls_name)\n    for attr, value in info.items():\n        if attr in cls.info.attrs_from_parent:\n            obj_attrs[attr] = value\n    mixin = cls.info._construct_from_dict(obj_attrs)\n    for attr, value in info.items():\n        if attr not in obj_attrs:\n            setattr(mixin.info, attr, value)\n    return mixin\n\n\nclass _TableLite(OrderedDict):\n    \"\"\"\n    Minimal table-like object for _construct_mixin_from_columns.  This allows\n    manipulating the object like a Table but without the actual overhead\n    for a full Table.\n\n    More pressing, there is an issue with constructing MaskedColumn, where the\n    encoded Column components (data, mask) are turned into a MaskedColumn.\n    When this happens in a real table then all other columns are immediately\n    Masked and a warning is issued. This is not desirable.\n    \"\"\"\n\n    def add_column(self, col, index=0):\n        colnames = self.colnames\n        self[col.info.name] = col\n        for ii, name in enumerate(colnames):\n            if ii >= index:\n                self.move_to_end(name)\n\n    @property\n    def colnames(self):\n        return list(self.keys())\n\n    def itercols(self):\n        return self.values()\n\n\ndef _construct_mixin_from_columns(new_name, obj_attrs, out):\n    data_attrs_map = {}\n    for name, val in obj_attrs.items():\n        if isinstance(val, SerializedColumn):\n            # A SerializedColumn can just link to a serialized column using a name\n            # (e.g., time.jd1), or itself be a mixin (e.g., coord.obstime).  Note\n            # that in principle a mixin could have include a column called 'name',\n            # hence we check whether the value is actually a strin"}, {"start_line": 23000, "end_line": 25000, "belongs_to": {"file_name": "column.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " internal numpy array instead of using a\n            reference.  Default is True.\n\n        Returns\n        -------\n        col : Column or MaskedColumn\n            Copy of the current column (same type as original)\n        \"\"\"\n        if data is None:\n            data = self.data\n            if copy_data:\n                data = data.copy(order)\n\n        out = data.view(self.__class__)\n        out.__array_finalize__(self)\n\n        # If there is meta on the original column then deepcopy (since \"copy\" of column\n        # implies complete independence from original).  __array_finalize__ will have already\n        # made a light copy.  I'm not sure how to avoid that initial light copy.\n        if self.meta is not None:\n            out.meta = self.meta  # MetaData descriptor does a deepcopy here\n\n        # for MaskedColumn, MaskedArray.__array_finalize__ also copies mask\n        # from self, which is not the idea here, so undo\n        if isinstance(self, MaskedColumn):\n            out._mask = data._mask\n\n        self._copy_groups(out)\n\n        return out\n\n    def __setstate__(self, state):\n        \"\"\"\n        Restore the internal state of the Column/MaskedColumn for pickling\n        purposes.  This requires that the last element of ``state`` is a\n        5-tuple that has Column-specific state values.\n        \"\"\"\n        # Get the Column attributes\n        names = (\"_name\", \"_unit\", \"_format\", \"description\", \"meta\", \"indices\")\n        attrs = dict(zip(names, state[-1]))\n\n        state = state[:-1]\n\n        # Using super().__setstate__(state) gives\n        # \"TypeError 'int' object is not iterable\", raised in\n        # astropy.table._column_mixins._ColumnGetitemShim.__setstate_cython__()\n        # Previously, it seems to have given an infinite recursion.\n        # Hence, manually call the right super class to actually set up\n        # the array object.\n        super_class = ma.MaskedArray if isinstance(self, ma.MaskedArray) else np.ndarray\n        super_class.__setstate__(se"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "serialize.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ", ())[:1] == col.shape[:1]\n    ]\n\n    for data_attr in data_attrs:\n        data = obj_attrs[data_attr]\n\n        # New column name combines the old name and attribute\n        # (e.g. skycoord.ra, skycoord.dec).unless it is the primary data\n        # attribute for the column (e.g. value for Quantity or data for\n        # MaskedColumn).  For primary data, we attempt to store any info on\n        # the format, etc., on the column, but not for ancillary data (e.g.,\n        # no sense to use a float format for a mask).\n        is_primary = data_attr == col.info._represent_as_dict_primary_data\n        if is_primary:\n            new_name = name\n            new_info = info\n        else:\n            new_name = name + \".\" + data_attr\n            new_info = {}\n\n        if not has_info_class(data, MixinInfo):\n            col_cls = (\n                MaskedColumn\n                if (\n                    hasattr(data, \"mask\")\n                    and np.any(data.mask != np.zeros((), data.mask.dtype))\n                )\n                else Column\n            )\n            data = col_cls(data, name=new_name, **new_info)\n            if is_primary:\n                # Don't store info in the __serialized_columns__ dict for this column\n                # since this is redundant with info stored on the new column.\n                info = {}\n\n        # Recurse. If this is anything that needs further serialization (i.e.,\n        # a Mixin column, a structured Column, a MaskedColumn for which mask is\n        # stored, etc.), it will define obj_attrs[new_name]. Otherwise, it will\n        # just add to new_cols and all we have to do is to link to the new name.\n        _represent_mixin_as_column(data, new_name, new_cols, obj_attrs)\n        obj_attrs[data_attr] = SerializedColumn(\n            obj_attrs.pop(new_name, {\"name\": new_name})\n        )\n\n    # Strip out from info any attributes defined by the parent,\n    # and store whatever remains.\n    for attr in col.info.attrs_from_parent:\n        if att"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "serialize.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "              )\n                else Column\n            )\n            data = col_cls(data, name=new_name, **new_info)\n            if is_primary:\n                # Don't store info in the __serialized_columns__ dict for this column\n                # since this is redundant with info stored on the new column.\n                info = {}\n\n        # Recurse. If this is anything that needs further serialization (i.e.,\n        # a Mixin column, a structured Column, a MaskedColumn for which mask is\n        # stored, etc.), it will define obj_attrs[new_name]. Otherwise, it will\n        # just add to new_cols and all we have to do is to link to the new name.\n        _represent_mixin_as_column(data, new_name, new_cols, obj_attrs)\n        obj_attrs[data_attr] = SerializedColumn(\n            obj_attrs.pop(new_name, {\"name\": new_name})\n        )\n\n    # Strip out from info any attributes defined by the parent,\n    # and store whatever remains.\n    for attr in col.info.attrs_from_parent:\n        if attr in info:\n            del info[attr]\n    if info:\n        obj_attrs[\"__info__\"] = info\n\n    # Store the fully qualified class name\n    if not isinstance(col, SerializedColumn):\n        obj_attrs.setdefault(\"__class__\", col.__module__ + \".\" + col.__class__.__name__)\n\n    mixin_cols[name] = obj_attrs\n\n\ndef represent_mixins_as_columns(tbl, exclude_classes=()):\n    \"\"\"Represent input Table ``tbl`` using only `~astropy.table.Column`\n    or  `~astropy.table.MaskedColumn` objects.\n\n    This function represents any mixin columns like `~astropy.time.Time` in\n    ``tbl`` to one or more plain ``~astropy.table.Column`` objects and returns\n    a new Table.  A single mixin column may be split into multiple column\n    components as needed for fully representing the column.  This includes the\n    possibility of recursive splitting, as shown in the example below.  The\n    new column names are formed as ``<column_name>.<component>``, e.g.\n    ``sc.ra`` for a `~astropy.coordinates.SkyCoord` column named"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "column.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "o_copy = BaseColumnInfo._attrs_no_copy | {\"groups\"}\n    attrs_from_parent = attr_names\n    _supports_indexing = True\n    # For structured columns, data is used to store a dict of columns.\n    # Store entries in that dict as name.key instead of name.data.key.\n    _represent_as_dict_primary_data = \"data\"\n\n    def _represent_as_dict(self):\n        result = super()._represent_as_dict()\n        names = self._parent.dtype.names\n        # For a regular column, we are done, but for a structured\n        # column, we use a SerializedColumns to store the pieces.\n        if names is None:\n            return result\n\n        from .serialize import SerializedColumn\n\n        data = SerializedColumn()\n        # If this column has a StructuredUnit, we split it and store\n        # it on the corresponding part. Otherwise, we just store it\n        # as an attribute below.  All other attributes we remove from\n        # the parts, so that we do not store them multiple times.\n        # (Note that attributes are not linked to the parent, so it\n        # is safe to reset them.)\n        # TODO: deal with (some of) this in Column.__getitem__?\n        # Alternatively: should we store info on the first part?\n        # TODO: special-case format somehow? Can we have good formats\n        # for structured columns?\n        unit = self.unit\n        if isinstance(unit, StructuredUnit) and len(unit) == len(names):\n            units = unit.values()\n            unit = None  # No need to store as an attribute as well.\n        else:\n            units = [None] * len(names)\n        for name, part_unit in zip(names, units):\n            part = self._parent.__class__(self._parent[name])\n            part.unit = part_unit\n            part.description = None\n            part.meta = {}\n            part.format = None\n            data[name] = part\n\n        # Create the attributes required to reconstruct the column.\n        result[\"data\"] = data\n        # Store the shape if needed. Just like scalar data, a structured d"}], "retrieved_count": 10, "cost_time": 1.1889357566833496}
{"question": "Why does the boolean caching control parameter in the wrapper class initialization method for convolution models create a trade-off between memory consumption and computational overhead when the discretization step size for integration approximation limits is dynamically increased during runtime?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "convolution.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "che=True):\n        super().__init__(operator, model, kernel)\n\n        self.bounding_box = bounding_box\n        self._resolution = resolution\n\n        self._cache_convolution = cache\n        self._kwargs = None\n        self._convolution = None\n\n    def clear_cache(self):\n        \"\"\"\n        Clears the cached convolution.\n        \"\"\"\n        self._kwargs = None\n        self._convolution = None\n\n    def _get_convolution(self, **kwargs):\n        if (self._convolution is None) or (self._kwargs != kwargs):\n            domain = self.bounding_box.domain(self._resolution)\n            mesh = np.meshgrid(*domain)\n            data = super().__call__(*mesh, **kwargs)\n\n            from scipy.interpolate import RegularGridInterpolator\n\n            convolution = RegularGridInterpolator(domain, data)\n\n            if self._cache_convolution:\n                self._kwargs = kwargs\n                self._convolution = convolution\n\n        else:\n            convolution = self._convolution\n\n        return convolution\n\n    @staticmethod\n    def _convolution_inputs(*args):\n        not_scalar = np.where([not np.isscalar(arg) for arg in args])[0]\n\n        if len(not_scalar) == 0:\n            return np.array(args), (1,)\n        else:\n            output_shape = args[not_scalar[0]].shape\n            if not all(args[index].shape == output_shape for index in not_scalar):\n                raise ValueError(\"Values have differing shapes\")\n\n            inputs = []\n            for arg in args:\n                if np.isscalar(arg):\n                    inputs.append(np.full(output_shape, arg))\n                else:\n                    inputs.append(arg)\n\n            return np.reshape(inputs, (len(inputs), -1)).T, output_shape\n\n    @staticmethod\n    def _convolution_outputs(outputs, output_shape):\n        return outputs.reshape(output_shape)\n\n    def __call__(self, *args, **kw):\n        inputs, output_shape = self._convolution_inputs(*args)\n        convolution = self._get_convolution(**kw)\n        outputs = "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "convolution.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "l convolution binary operator implemented in\n    astropy.convolution under `~astropy.convolution.convolve_fft`. In this\n    `~astropy.convolution.convolve_fft` it is assumed that the inputs ``array``\n    and ``kernel`` span a sufficient portion of the support of the functions of\n    the convolution. Consequently, the ``Compound`` created by the\n    `~astropy.convolution.convolve_models` function makes the assumption that\n    one should pass an input array that sufficiently spans this space. This means\n    that slightly different input arrays to this model will result in different\n    outputs, even on points of intersection between these arrays.\n\n    This issue is solved by requiring a ``bounding_box`` together with a\n    resolution so that one can pre-calculate the entire domain and then\n    (by default) cache the convolution values. The function then just\n    interpolates the results from this cache.\n    \"\"\"\n\n    def __init__(self, operator, model, kernel, bounding_box, resolution, cache=True):\n        super().__init__(operator, model, kernel)\n\n        self.bounding_box = bounding_box\n        self._resolution = resolution\n\n        self._cache_convolution = cache\n        self._kwargs = None\n        self._convolution = None\n\n    def clear_cache(self):\n        \"\"\"\n        Clears the cached convolution.\n        \"\"\"\n        self._kwargs = None\n        self._convolution = None\n\n    def _get_convolution(self, **kwargs):\n        if (self._convolution is None) or (self._kwargs != kwargs):\n            domain = self.bounding_box.domain(self._resolution)\n            mesh = np.meshgrid(*domain)\n            data = super().__call__(*mesh, **kwargs)\n\n            from scipy.interpolate import RegularGridInterpolator\n\n            convolution = RegularGridInterpolator(domain, data)\n\n            if self._cache_convolution:\n                self._kwargs = kwargs\n                self._convolution = convolution\n\n        else:\n            convolution = self._convolution\n\n        return con"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "convolution.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n\"\"\"Convolution Model.\"\"\"\n\n# pylint: disable=line-too-long, too-many-lines, too-many-arguments, invalid-name\nimport numpy as np\n\nfrom .core import CompoundModel\n\n\nclass Convolution(CompoundModel):\n    \"\"\"\n    Wrapper class for a convolution model.\n\n    Parameters\n    ----------\n    operator: tuple\n        The SPECIAL_OPERATORS entry for the convolution being used.\n    model : Model\n        The model for the convolution.\n    kernel: Model\n        The kernel model for the convolution.\n    bounding_box : tuple\n        A bounding box to define the limits of the integration\n        approximation for the convolution.\n    resolution : float\n        The resolution for the approximation of the convolution.\n    cache : bool, optional\n        Allow convolution computation to be cached for reuse. This is\n        enabled by default.\n\n    Notes\n    -----\n    This is wrapper is necessary to handle the limitations of the\n    pseudospectral convolution binary operator implemented in\n    astropy.convolution under `~astropy.convolution.convolve_fft`. In this\n    `~astropy.convolution.convolve_fft` it is assumed that the inputs ``array``\n    and ``kernel`` span a sufficient portion of the support of the functions of\n    the convolution. Consequently, the ``Compound`` created by the\n    `~astropy.convolution.convolve_models` function makes the assumption that\n    one should pass an input array that sufficiently spans this space. This means\n    that slightly different input arrays to this model will result in different\n    outputs, even on points of intersection between these arrays.\n\n    This issue is solved by requiring a ``bounding_box`` together with a\n    resolution so that one can pre-calculate the entire domain and then\n    (by default) cache the convolution values. The function then just\n    interpolates the results from this cache.\n    \"\"\"\n\n    def __init__(self, operator, model, kernel, bounding_box, resolution, ca"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/convolution", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "       * 'integrate'\n                Discretize model by integrating the\n                model over the bin.\n    width : number\n        Width of the filter kernel.\n    factor : number, optional\n        Factor of oversampling. Default factor = 10.\n    \"\"\"\n\n    def __init__(self, model=None, x_size=None, y_size=None, array=None, **kwargs):\n        # Initialize from model\n        if self._model:\n            if array is not None:\n                # Reject \"array\" keyword for kernel models, to avoid them not being\n                # populated as expected.\n                raise TypeError(\"Array argument not allowed for kernel models.\")\n            if x_size is None:\n                x_size = self._default_size\n            elif x_size != int(x_size):\n                raise TypeError(\"x_size should be an integer\")\n\n            if y_size is None:\n                y_size = x_size\n            elif y_size != int(y_size):\n                raise TypeError(\"y_size should be an integer\")\n\n            # Set ranges where to evaluate the model\n\n            if x_size % 2 == 0:  # even kernel\n                x_range = (-(int(x_size)) // 2 + 0.5, (int(x_size)) // 2 + 0.5)\n            else:  # odd kernel\n                x_range = (-(int(x_size) - 1) // 2, (int(x_size) - 1) // 2 + 1)\n\n            if y_size % 2 == 0:  # even kernel\n                y_range = (-(int(y_size)) // 2 + 0.5, (int(y_size)) // 2 + 0.5)\n            else:  # odd kernel\n                y_range = (-(int(y_size) - 1) // 2, (int(y_size) - 1) // 2 + 1)\n\n            array = discretize_model(self._model, x_range, y_range, **kwargs)\n\n        # Initialize from array\n        elif array is None:\n            raise TypeError(\"Must specify either array or model.\")\n\n        super().__init__(array)\n\n\ndef kernel_arithmetics(kernel, value, operation):\n    \"\"\"\n    Add, subtract or multiply two kernels.\n\n    Parameters\n    ----------\n    kernel : `astropy.convolution.Kernel`\n        Kernel instance.\n    value : `astropy.convolution.Kernel`, fl"}, {"start_line": 43000, "end_line": 44314, "belongs_to": {"file_name": "convolve.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/convolution", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "Error(f\"Mode {mode} is not supported.\")\n\n    return CompoundModel(operator, model, kernel)\n\n\ndef convolve_models_fft(model, kernel, bounding_box, resolution, cache=True, **kwargs):\n    \"\"\"\n    Convolve two models using `~astropy.convolution.convolve_fft`.\n\n    Parameters\n    ----------\n    model : `~astropy.modeling.core.Model`\n        Functional model\n    kernel : `~astropy.modeling.core.Model`\n        Convolution kernel\n    bounding_box : tuple\n        The bounding box which encompasses enough of the support of both\n        the ``model`` and ``kernel`` so that an accurate convolution can be\n        computed.\n    resolution : float\n        The resolution that one wishes to approximate the convolution\n        integral at.\n    cache : optional, bool\n        Default value True. Allow for the storage of the convolution\n        computation for later reuse.\n    **kwargs : dict\n        Keyword arguments to be passed either to `~astropy.convolution.convolve`\n        or `~astropy.convolution.convolve_fft` depending on ``mode``.\n\n    Returns\n    -------\n    default : `~astropy.modeling.core.CompoundModel`\n        Convolved model\n    \"\"\"\n    operator = SPECIAL_OPERATORS.add(\"convolve_fft\", partial(convolve_fft, **kwargs))\n\n    return Convolution(operator, model, kernel, bounding_box, resolution, cache)\n"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "utils.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/convolution", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     x_range[1] - 0.5 * (1 + 1 / factor),\n        num=int((x_range[1] - x_range[0]) * factor),\n    )\n\n    values = model(x)\n\n    # Reshape and compute mean\n    values = np.reshape(values, (x.size // factor, factor))\n    return values.mean(axis=1)\n\n\ndef discretize_oversample_2D(model, x_range, y_range, factor=10):\n    \"\"\"\n    Discretize model by taking the average on an oversampled grid.\n    \"\"\"\n    # Evaluate model on oversampled grid\n    x = np.linspace(\n        x_range[0] - 0.5 * (1 - 1 / factor),\n        x_range[1] - 0.5 * (1 + 1 / factor),\n        num=int((x_range[1] - x_range[0]) * factor),\n    )\n    y = np.linspace(\n        y_range[0] - 0.5 * (1 - 1 / factor),\n        y_range[1] - 0.5 * (1 + 1 / factor),\n        num=int((y_range[1] - y_range[0]) * factor),\n    )\n\n    x_grid, y_grid = np.meshgrid(x, y)\n    values = model(x_grid, y_grid)\n\n    # Reshape and compute mean\n    shape = (y.size // factor, factor, x.size // factor, factor)\n    values = np.reshape(values, shape)\n    return values.mean(axis=3).mean(axis=1)\n\n\ndef discretize_integrate_1D(model, x_range):\n    \"\"\"\n    Discretize model by integrating numerically the model over the bin.\n    \"\"\"\n    from scipy.integrate import quad\n\n    # Set up grid\n    x = np.arange(x_range[0] - 0.5, x_range[1] + 0.5)\n    values = np.array([])\n\n    # Integrate over all bins\n    for i in range(x.size - 1):\n        values = np.append(values, quad(model, x[i], x[i + 1])[0])\n    return values\n\n\ndef discretize_integrate_2D(model, x_range, y_range):\n    \"\"\"\n    Discretize model by integrating the model over the pixel.\n    \"\"\"\n    from scipy.integrate import dblquad\n\n    # Set up grid\n    x = np.arange(x_range[0] - 0.5, x_range[1] + 0.5)\n    y = np.arange(y_range[0] - 0.5, y_range[1] + 0.5)\n    values = np.empty((y.size - 1, x.size - 1))\n\n    # Integrate over all pixels\n    for i in range(x.size - 1):\n        for j in range(y.size - 1):\n            values[j, i] = dblquad(\n                func=lambda y, x: model(x, y),\n              "}, {"start_line": 56000, "end_line": 58000, "belongs_to": {"file_name": "functional_models.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "p.pi)\n    sqrt_ln2 = np.sqrt(np.log(2))\n    sqrt_ln2pi = np.sqrt(np.log(2) * np.pi)\n    _last_z = np.zeros(1, dtype=complex)\n    _last_w = np.zeros(1, dtype=float)\n    _faddeeva = None\n\n    def __init__(\n        self,\n        x_0=x_0.default,\n        amplitude_L=amplitude_L.default,\n        fwhm_L=fwhm_L.default,\n        fwhm_G=fwhm_G.default,\n        method=None,\n        **kwargs,\n    ):\n        if str(method).lower() == \"humlicek2\" and HAS_SCIPY:\n            warnings.warn(\n                f\"{method} has been deprecated since Astropy 5.3 and will be removed in a future version.\\n\"\n                \"It is recommended to always use the `~scipy.special.wofz` implementation \"\n                \"when `scipy` is installed.\",\n                AstropyDeprecationWarning,\n            )\n\n        if method is None:\n            if HAS_SCIPY:\n                method = \"wofz\"\n            else:\n                method = \"humlicek2\"\n\n        if str(method).lower() in (\"wofz\", \"scipy\"):\n            from scipy.special import wofz\n\n            self._faddeeva = wofz\n        elif str(method).lower() == \"humlicek2\":\n            self._faddeeva = self._hum2zpf16c\n        else:\n            raise ValueError(\n                f\"Not a valid method for Voigt1D Faddeeva function: {method}.\"\n            )\n        self.method = self._faddeeva.__name__\n\n        super().__init__(\n            x_0=x_0, amplitude_L=amplitude_L, fwhm_L=fwhm_L, fwhm_G=fwhm_G, **kwargs\n        )\n\n    def _wrap_wofz(self, z):\n        \"\"\"Call complex error (Faddeeva) function w(z) implemented by algorithm `method`;\n        cache results for consecutive calls from `evaluate`, `fit_deriv`.\n        \"\"\"\n        if z.shape == self._last_z.shape and np.allclose(\n            z, self._last_z, rtol=1.0e-14, atol=1.0e-15\n        ):\n            return self._last_w\n\n        self._last_z = (\n            z.to_value(u.dimensionless_unscaled) if isinstance(z, u.Quantity) else z\n        )\n        self._last_w = self._faddeeva(self._last_z)\n\n      "}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "utils.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/convolution", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ower limit of\"\n                \" 'y_range' must be a whole number.\"\n            )\n\n    if factor != int(factor):\n        raise ValueError(\"factor must have an integer value\")\n    factor = int(factor)\n\n    if ndim == 2 and y_range is None:\n        raise ValueError(\"y_range must be specified for a 2D model\")\n    if ndim == 1 and y_range is not None:\n        raise ValueError(\"y_range should not be input for a 1D model\")\n\n    match (mode, ndim):\n        case (\"center\", 1):\n            result = discretize_center_1D(model, x_range)\n        case (\"center\", 2):\n            result = discretize_center_2D(model, x_range, y_range)\n        case (\"linear_interp\", 1):\n            result = discretize_linear_1D(model, x_range)\n        case (\"linear_interp\", 2):\n            result = discretize_bilinear_2D(model, x_range, y_range)\n        case (\"oversample\", 1):\n            result = discretize_oversample_1D(model, x_range, factor)\n        case (\"oversample\", 2):\n            result = discretize_oversample_2D(model, x_range, y_range, factor)\n        case (\"integrate\", 1):\n            result = discretize_integrate_1D(model, x_range)\n        case (\"integrate\", 2):\n            result = discretize_integrate_2D(model, x_range, y_range)\n        case _:\n            raise ValueError(\n                \"Invalid (mode, ndim) combination for discretize_model. \"\n                f\"Got {mode=}, {ndim=}\"\n            )\n\n    return result\n\n\ndef discretize_center_1D(model, x_range):\n    \"\"\"\n    Discretize model by taking the value at the center of the bin.\n    \"\"\"\n    x = np.arange(*x_range)\n    return model(x)\n\n\ndef discretize_center_2D(model, x_range, y_range):\n    \"\"\"\n    Discretize model by taking the value at the center of the pixel.\n    \"\"\"\n    x = np.arange(*x_range)\n    y = np.arange(*y_range)\n    x, y = np.meshgrid(x, y)\n    return model(x, y)\n\n\ndef discretize_linear_1D(model, x_range):\n    \"\"\"\n    Discretize model by performing a linear interpolation.\n    \"\"\"\n    # Evaluate model 0.5 pixel outsid"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "utils.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/convolution", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_2D(model, x_range, y_range, factor)\n        case (\"integrate\", 1):\n            result = discretize_integrate_1D(model, x_range)\n        case (\"integrate\", 2):\n            result = discretize_integrate_2D(model, x_range, y_range)\n        case _:\n            raise ValueError(\n                \"Invalid (mode, ndim) combination for discretize_model. \"\n                f\"Got {mode=}, {ndim=}\"\n            )\n\n    return result\n\n\ndef discretize_center_1D(model, x_range):\n    \"\"\"\n    Discretize model by taking the value at the center of the bin.\n    \"\"\"\n    x = np.arange(*x_range)\n    return model(x)\n\n\ndef discretize_center_2D(model, x_range, y_range):\n    \"\"\"\n    Discretize model by taking the value at the center of the pixel.\n    \"\"\"\n    x = np.arange(*x_range)\n    y = np.arange(*y_range)\n    x, y = np.meshgrid(x, y)\n    return model(x, y)\n\n\ndef discretize_linear_1D(model, x_range):\n    \"\"\"\n    Discretize model by performing a linear interpolation.\n    \"\"\"\n    # Evaluate model 0.5 pixel outside the boundaries\n    x = np.arange(x_range[0] - 0.5, x_range[1] + 0.5)\n    values_intermediate_grid = model(x)\n    return 0.5 * (values_intermediate_grid[1:] + values_intermediate_grid[:-1])\n\n\ndef discretize_bilinear_2D(model, x_range, y_range):\n    \"\"\"\n    Discretize model by performing a bilinear interpolation.\n    \"\"\"\n    # Evaluate model 0.5 pixel outside the boundaries\n    x = np.arange(x_range[0] - 0.5, x_range[1] + 0.5)\n    y = np.arange(y_range[0] - 0.5, y_range[1] + 0.5)\n    x, y = np.meshgrid(x, y)\n    values_intermediate_grid = model(x, y)\n\n    # Mean in y direction\n    values = 0.5 * (values_intermediate_grid[1:, :] + values_intermediate_grid[:-1, :])\n    # Mean in x direction\n    return 0.5 * (values[:, 1:] + values[:, :-1])\n\n\ndef discretize_oversample_1D(model, x_range, factor=10):\n    \"\"\"\n    Discretize model by taking the average on an oversampled grid.\n    \"\"\"\n    # Evaluate model on oversampled grid\n    x = np.linspace(\n        x_range[0] - 0.5 * (1 - 1 / factor),\n   "}, {"start_line": 42000, "end_line": 44000, "belongs_to": {"file_name": "convolve.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/convolution", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " `~astropy.convolution.convolve_fft`.\n\n    Parameters\n    ----------\n    model : `~astropy.modeling.core.Model`\n        Functional model\n    kernel : `~astropy.modeling.core.Model`\n        Convolution kernel\n    mode : str\n        Keyword representing which function to use for convolution.\n            * 'convolve_fft' : use `~astropy.convolution.convolve_fft` function.\n            * 'convolve' : use `~astropy.convolution.convolve`.\n    **kwargs : dict\n        Keyword arguments to me passed either to `~astropy.convolution.convolve`\n        or `~astropy.convolution.convolve_fft` depending on ``mode``.\n\n    Returns\n    -------\n    default : `~astropy.modeling.core.CompoundModel`\n        Convolved model\n    \"\"\"\n    if mode == \"convolve_fft\":\n        operator = SPECIAL_OPERATORS.add(\n            \"convolve_fft\", partial(convolve_fft, **kwargs)\n        )\n    elif mode == \"convolve\":\n        operator = SPECIAL_OPERATORS.add(\"convolve\", partial(convolve, **kwargs))\n    else:\n        raise ValueError(f\"Mode {mode} is not supported.\")\n\n    return CompoundModel(operator, model, kernel)\n\n\ndef convolve_models_fft(model, kernel, bounding_box, resolution, cache=True, **kwargs):\n    \"\"\"\n    Convolve two models using `~astropy.convolution.convolve_fft`.\n\n    Parameters\n    ----------\n    model : `~astropy.modeling.core.Model`\n        Functional model\n    kernel : `~astropy.modeling.core.Model`\n        Convolution kernel\n    bounding_box : tuple\n        The bounding box which encompasses enough of the support of both\n        the ``model`` and ``kernel`` so that an accurate convolution can be\n        computed.\n    resolution : float\n        The resolution that one wishes to approximate the convolution\n        integral at.\n    cache : optional, bool\n        Default value True. Allow for the storage of the convolution\n        computation for later reuse.\n    **kwargs : dict\n        Keyword arguments to be passed either to `~astropy.convolution.convolve`\n        or `~astropy.convolution.c"}], "retrieved_count": 10, "cost_time": 1.2046139240264893}
{"question": "Why does the boolean flag indicating whether model outputs are independent in the base class for cylindrical sky projections enable performance optimization in astronomical coordinate transformation pipelines?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "projections.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ion\",\n    \"Zenithal\",\n    \"projcodes\",\n]\n\n__all__ += list(map(\"_\".join, product([\"Pix2Sky\", \"Sky2Pix\"], chain(*_PROJ_NAME_CODE))))\n\n\nclass _ParameterDS(Parameter):\n    \"\"\"\n    Same as `Parameter` but can indicate its modified status via the ``dirty``\n    property. This flag also gets set automatically when a parameter is\n    modified.\n\n    This ability to track parameter's modified status is needed for automatic\n    update of WCSLIB's prjprm structure (which may be a more-time intensive\n    operation) *only as required*.\n\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dirty = True\n\n    def validate(self, value):\n        super().validate(value)\n        self.dirty = True\n\n\nclass Projection(Model):\n    \"\"\"Base class for all sky projections.\"\"\"\n\n    # Radius of the generating sphere.\n    # This sets the circumference to 360 deg so that arc length is measured in deg.\n    r0 = 180 * u.deg / np.pi\n\n    _separable = False\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._prj = wcs.Prjprm()\n\n    @property\n    @abc.abstractmethod\n    def inverse(self):\n        \"\"\"\n        Inverse projection--all projection models must provide an inverse.\n        \"\"\"\n\n    @property\n    def prjprm(self):\n        \"\"\"WCSLIB ``prjprm`` structure.\"\"\"\n        self._update_prj()\n        return self._prj\n\n    def _update_prj(self):\n        \"\"\"\n        A default updater for projection's pv.\n\n        .. warning::\n            This method assumes that PV0 is never modified. If a projection\n            that uses PV0 is ever implemented in this module, that projection\n            class should override this method.\n\n        .. warning::\n            This method assumes that the order in which PVi values (i>0)\n            are to be assigned is identical to the order of model parameters\n            in ``param_names``. That is, pv[1] = model.parameters[0], ...\n\n        \"\"\"\n        if not self.param_names:\n  "}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "rotations.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " one of {self.axes}\"\n            )\n        self.axes_order = axes_order\n        qs = [isinstance(par, u.Quantity) for par in [phi, theta, psi]]\n        if any(qs) and not all(qs):\n            raise TypeError(\n                \"All parameters should be of the same type - float or Quantity.\"\n            )\n\n        super().__init__(phi=phi, theta=theta, psi=psi, **kwargs)\n        self._inputs = (\"alpha\", \"delta\")\n        self._outputs = (\"alpha\", \"delta\")\n\n    @property\n    def inverse(self):\n        return self.__class__(\n            phi=-self.psi,\n            theta=-self.theta,\n            psi=-self.phi,\n            axes_order=self.axes_order[::-1],\n        )\n\n    def evaluate(self, alpha, delta, phi, theta, psi):\n        a, b = super().evaluate(alpha, delta, phi, theta, psi, self.axes_order)\n        return a, b\n\n\nclass _SkyRotation(_EulerRotation, Model):\n    \"\"\"\n    Base class for RotateNative2Celestial and RotateCelestial2Native.\n    \"\"\"\n\n    lon = Parameter(\n        default=0, getter=_to_orig_unit, setter=_to_radian, description=\"Latitude\"\n    )\n    lat = Parameter(\n        default=0, getter=_to_orig_unit, setter=_to_radian, description=\"Longtitude\"\n    )\n    lon_pole = Parameter(\n        default=0,\n        getter=_to_orig_unit,\n        setter=_to_radian,\n        description=\"Longitude of a pole\",\n    )\n\n    def __init__(self, lon, lat, lon_pole, **kwargs):\n        qs = [isinstance(par, u.Quantity) for par in [lon, lat, lon_pole]]\n        if any(qs) and not all(qs):\n            raise TypeError(\n                \"All parameters should be of the same type - float or Quantity.\"\n            )\n        super().__init__(lon, lat, lon_pole, **kwargs)\n        self.axes_order = \"zxz\"\n\n    def _evaluate(self, phi, theta, lon, lat, lon_pole):\n        alpha, delta = super().evaluate(phi, theta, lon, lat, lon_pole, self.axes_order)\n        mask = alpha < 0\n        if isinstance(mask, np.ndarray):\n            alpha[mask] += 360\n        else:\n            alpha += 360\n        retur"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "projections.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "t__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._prj = wcs.Prjprm()\n\n    @property\n    @abc.abstractmethod\n    def inverse(self):\n        \"\"\"\n        Inverse projection--all projection models must provide an inverse.\n        \"\"\"\n\n    @property\n    def prjprm(self):\n        \"\"\"WCSLIB ``prjprm`` structure.\"\"\"\n        self._update_prj()\n        return self._prj\n\n    def _update_prj(self):\n        \"\"\"\n        A default updater for projection's pv.\n\n        .. warning::\n            This method assumes that PV0 is never modified. If a projection\n            that uses PV0 is ever implemented in this module, that projection\n            class should override this method.\n\n        .. warning::\n            This method assumes that the order in which PVi values (i>0)\n            are to be assigned is identical to the order of model parameters\n            in ``param_names``. That is, pv[1] = model.parameters[0], ...\n\n        \"\"\"\n        if not self.param_names:\n            return\n\n        pv = []\n        dirty = False\n\n        for p in self.param_names:\n            param = getattr(self, p)\n            pv.append(float(param.value))\n            dirty |= param.dirty\n            param.dirty = False\n\n        if dirty:\n            self._prj.pv = None, *pv\n            self._prj.set()\n\n    def __getstate__(self):\n        return {\n            \"p\": self.parameters,\n            \"fixed\": self.fixed,\n            \"tied\": self.tied,\n            \"bounds\": self.bounds,\n        }\n\n    def __setstate__(self, state):\n        params = state.pop(\"p\")\n        return self.__init__(*params, **state)\n\n\nclass Pix2SkyProjection(Projection):\n    \"\"\"Base class for all Pix2Sky projections.\"\"\"\n\n    n_inputs = 2\n    n_outputs = 2\n\n    _input_units_strict = True\n    _input_units_allow_dimensionless = True\n\n    def __new__(cls, *args, **kwargs):\n        long_name = cls.name.split(\"_\")[1]\n        cls.prj_code = _PROJ_NAME_CODE_MAP[long_name]\n        return super().__new__(cls)\n\n "}, {"start_line": 25000, "end_line": 27000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "table = False\n    linear = True\n    _separable = None\n    \"\"\" A boolean flag to indicate whether a model is separable.\"\"\"\n    meta = metadata.MetaData()\n    \"\"\"A dict-like object to store optional information.\"\"\"\n\n    # By default models either use their own inverse property or have no\n    # inverse at all, but users may also assign a custom inverse to a model,\n    # optionally; in that case it is of course up to the user to determine\n    # whether their inverse is *actually* an inverse to the model they assign\n    # it to.\n    _inverse = None\n    _user_inverse = None\n\n    _bounding_box = None\n    _user_bounding_box = None\n\n    _has_inverse_bounding_box = False\n\n    # Default n_models attribute, so that __len__ is still defined even when a\n    # model hasn't completed initialization yet\n    _n_models = 1\n\n    # New classes can set this as a boolean value.\n    # It is converted to a dictionary mapping input name to a boolean value.\n    _input_units_strict = False\n\n    # Allow dimensionless input (and corresponding output). If this is True,\n    # input values to evaluate will gain the units specified in input_units. If\n    # this is a dictionary then it should map input name to a bool to allow\n    # dimensionless numbers for that input.\n    # Only has an effect if input_units is defined.\n    _input_units_allow_dimensionless = False\n\n    # Default equivalencies to apply to input values. If set, this should be a\n    # dictionary where each key is a string that corresponds to one of the\n    # model inputs. Only has an effect if input_units is defined.\n    input_units_equivalencies = None\n\n    # Covariance matrix can be set by fitter if available.\n    # If cov_matrix is available, then std will set as well\n    _cov_matrix = None\n    _stds = None\n\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__()\n\n    def __init__(self, *args, meta=None, name=None, **kwargs):\n        super().__init__()\n        self._default_inputs_outputs()\n        if meta is no"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "rotations.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  n_outputs = 2\n\n    phi = Parameter(\n        default=0,\n        getter=_to_orig_unit,\n        setter=_to_radian,\n        description=\"1st Euler angle (Quantity or value in deg)\",\n    )\n    theta = Parameter(\n        default=0,\n        getter=_to_orig_unit,\n        setter=_to_radian,\n        description=\"2nd Euler angle (Quantity or value in deg)\",\n    )\n    psi = Parameter(\n        default=0,\n        getter=_to_orig_unit,\n        setter=_to_radian,\n        description=\"3rd Euler angle (Quantity or value in deg)\",\n    )\n\n    def __init__(self, phi, theta, psi, axes_order, **kwargs):\n        self.axes = [\"x\", \"y\", \"z\"]\n        if len(axes_order) != 3:\n            raise TypeError(\n                \"Expected axes_order to be a character sequence of length 3, \"\n                f\"got {axes_order}\"\n            )\n        unrecognized = set(axes_order).difference(self.axes)\n        if unrecognized:\n            raise ValueError(\n                f\"Unrecognized axis label {unrecognized}; should be one of {self.axes}\"\n            )\n        self.axes_order = axes_order\n        qs = [isinstance(par, u.Quantity) for par in [phi, theta, psi]]\n        if any(qs) and not all(qs):\n            raise TypeError(\n                \"All parameters should be of the same type - float or Quantity.\"\n            )\n\n        super().__init__(phi=phi, theta=theta, psi=psi, **kwargs)\n        self._inputs = (\"alpha\", \"delta\")\n        self._outputs = (\"alpha\", \"delta\")\n\n    @property\n    def inverse(self):\n        return self.__class__(\n            phi=-self.psi,\n            theta=-self.theta,\n            psi=-self.phi,\n            axes_order=self.axes_order[::-1],\n        )\n\n    def evaluate(self, alpha, delta, phi, theta, psi):\n        a, b = super().evaluate(alpha, delta, phi, theta, psi, self.axes_order)\n        return a, b\n\n\nclass _SkyRotation(_EulerRotation, Model):\n    \"\"\"\n    Base class for RotateNative2Celestial and RotateCelestial2Native.\n    \"\"\"\n\n    lon = Parameter(\n        default=0, getter"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "projections.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "          return\n\n        pv = []\n        dirty = False\n\n        for p in self.param_names:\n            param = getattr(self, p)\n            pv.append(float(param.value))\n            dirty |= param.dirty\n            param.dirty = False\n\n        if dirty:\n            self._prj.pv = None, *pv\n            self._prj.set()\n\n    def __getstate__(self):\n        return {\n            \"p\": self.parameters,\n            \"fixed\": self.fixed,\n            \"tied\": self.tied,\n            \"bounds\": self.bounds,\n        }\n\n    def __setstate__(self, state):\n        params = state.pop(\"p\")\n        return self.__init__(*params, **state)\n\n\nclass Pix2SkyProjection(Projection):\n    \"\"\"Base class for all Pix2Sky projections.\"\"\"\n\n    n_inputs = 2\n    n_outputs = 2\n\n    _input_units_strict = True\n    _input_units_allow_dimensionless = True\n\n    def __new__(cls, *args, **kwargs):\n        long_name = cls.name.split(\"_\")[1]\n        cls.prj_code = _PROJ_NAME_CODE_MAP[long_name]\n        return super().__new__(cls)\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self._prj.code = self.prj_code\n        self._update_prj()\n        if not self.param_names:\n            # force initial call to Prjprm.set() for projections\n            # with no parameters:\n            self._prj.set()\n\n        self.inputs = (\"x\", \"y\")\n        self.outputs = (\"phi\", \"theta\")\n\n    @property\n    def input_units(self):\n        return {self.inputs[0]: u.deg, self.inputs[1]: u.deg}\n\n    @property\n    def return_units(self):\n        return {self.outputs[0]: u.deg, self.outputs[1]: u.deg}\n\n    def evaluate(self, x, y, *args, **kwargs):\n        self._update_prj()\n        return self._prj.prjx2s(x, y)\n\n    @property\n    def inverse(self):\n        pv = [getattr(self, param).value for param in self.param_names]\n        return self._inv_cls(*pv)\n\n\nclass Sky2PixProjection(Projection):\n    \"\"\"Base class for all Sky2Pix projections.\"\"\"\n\n    n_inputs = 2\n    n_outputs = 2\n\n    _input_units_strict"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "separable.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "`` where\n        each element indicates whether the output is independent\n        and the result of a separable transform.\n\n    Examples\n    --------\n    >>> from astropy.modeling.models import Shift, Scale, Rotation2D, Polynomial2D\n    >>> is_separable(Shift(1) & Shift(2) | Scale(1) & Scale(2))\n        array([ True,  True]...)\n    >>> is_separable(Shift(1) & Shift(2) | Rotation2D(2))\n        array([False, False]...)\n    >>> is_separable(Shift(1) & Shift(2) | Mapping([0, 1, 0, 1]) | \\\n        Polynomial2D(1) & Polynomial2D(2))\n        array([False, False]...)\n    >>> is_separable(Shift(1) & Shift(2) | Mapping([0, 1, 0, 1]))\n        array([ True,  True,  True,  True]...)\n\n    \"\"\"\n    if transform.n_inputs == 1 and transform.n_outputs > 1:\n        is_separable = np.array([False] * transform.n_outputs).T\n        return is_separable\n    separable_matrix = _separable(transform)\n    is_separable = separable_matrix.sum(1)\n    is_separable = np.where(is_separable != 1, False, True)\n    return is_separable\n\n\ndef separability_matrix(transform):\n    \"\"\"\n    Compute the correlation between outputs and inputs.\n\n    Parameters\n    ----------\n    transform : `~astropy.modeling.core.Model`\n        A (compound) model.\n\n    Returns\n    -------\n    separable_matrix : ndarray\n        A boolean correlation matrix of shape (n_outputs, n_inputs).\n        Indicates the dependence of outputs on inputs. For completely\n        independent outputs, the diagonal elements are True and\n        off-diagonal elements are False.\n\n    Examples\n    --------\n    >>> from astropy.modeling.models import Shift, Scale, Rotation2D, Polynomial2D\n    >>> separability_matrix(Shift(1) & Shift(2) | Scale(1) & Scale(2))\n        array([[ True, False], [False,  True]]...)\n    >>> separability_matrix(Shift(1) & Shift(2) | Rotation2D(2))\n        array([[ True,  True], [ True,  True]]...)\n    >>> separability_matrix(Shift(1) & Shift(2) | Mapping([0, 1, 0, 1]) | \\\n        Polynomial2D(1) & Polynomial2D(2))\n        arra"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "projections.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n    (\"ZenithalEqualArea\", \"ZEA\"),\n    (\"Airy\", \"AIR\"),\n    (\"CylindricalPerspective\", \"CYP\"),\n    (\"CylindricalEqualArea\", \"CEA\"),\n    (\"PlateCarree\", \"CAR\"),\n    (\"Mercator\", \"MER\"),\n    (\"SansonFlamsteed\", \"SFL\"),\n    (\"Parabolic\", \"PAR\"),\n    (\"Molleweide\", \"MOL\"),\n    (\"HammerAitoff\", \"AIT\"),\n    (\"ConicPerspective\", \"COP\"),\n    (\"ConicEqualArea\", \"COE\"),\n    (\"ConicEquidistant\", \"COD\"),\n    (\"ConicOrthomorphic\", \"COO\"),\n    (\"BonneEqualArea\", \"BON\"),\n    (\"Polyconic\", \"PCO\"),\n    (\"TangentialSphericalCube\", \"TSC\"),\n    (\"COBEQuadSphericalCube\", \"CSC\"),\n    (\"QuadSphericalCube\", \"QSC\"),\n    (\"HEALPix\", \"HPX\"),\n    (\"HEALPixPolar\", \"XPH\"),\n]\n\n_NOT_SUPPORTED_PROJ_CODES = [\"ZPN\"]\n\n_PROJ_NAME_CODE_MAP = dict(_PROJ_NAME_CODE)\n\nprojcodes = [code for _, code in _PROJ_NAME_CODE]\n\n\n__all__ = [\n    \"AffineTransformation2D\",\n    \"Conic\",\n    \"Cylindrical\",\n    \"HEALPix\",\n    \"Pix2SkyProjection\",\n    \"Projection\",\n    \"PseudoConic\",\n    \"PseudoCylindrical\",\n    \"QuadCube\",\n    \"Sky2PixProjection\",\n    \"Zenithal\",\n    \"projcodes\",\n]\n\n__all__ += list(map(\"_\".join, product([\"Pix2Sky\", \"Sky2Pix\"], chain(*_PROJ_NAME_CODE))))\n\n\nclass _ParameterDS(Parameter):\n    \"\"\"\n    Same as `Parameter` but can indicate its modified status via the ``dirty``\n    property. This flag also gets set automatically when a parameter is\n    modified.\n\n    This ability to track parameter's modified status is needed for automatic\n    update of WCSLIB's prjprm structure (which may be a more-time intensive\n    operation) *only as required*.\n\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dirty = True\n\n    def validate(self, value):\n        super().validate(value)\n        self.dirty = True\n\n\nclass Projection(Model):\n    \"\"\"Base class for all sky projections.\"\"\"\n\n    # Radius of the generating sphere.\n    # This sets the circumference to 360 deg so that arc length is measured in deg.\n    r0 = 180 * u.deg / np.pi\n\n    _separable = False\n\n    def __ini"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "projections.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "   def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self._prj.code = self.prj_code\n        self._update_prj()\n        if not self.param_names:\n            # force initial call to Prjprm.set() for projections\n            # with no parameters:\n            self._prj.set()\n\n        self.inputs = (\"x\", \"y\")\n        self.outputs = (\"phi\", \"theta\")\n\n    @property\n    def input_units(self):\n        return {self.inputs[0]: u.deg, self.inputs[1]: u.deg}\n\n    @property\n    def return_units(self):\n        return {self.outputs[0]: u.deg, self.outputs[1]: u.deg}\n\n    def evaluate(self, x, y, *args, **kwargs):\n        self._update_prj()\n        return self._prj.prjx2s(x, y)\n\n    @property\n    def inverse(self):\n        pv = [getattr(self, param).value for param in self.param_names]\n        return self._inv_cls(*pv)\n\n\nclass Sky2PixProjection(Projection):\n    \"\"\"Base class for all Sky2Pix projections.\"\"\"\n\n    n_inputs = 2\n    n_outputs = 2\n\n    _input_units_strict = True\n    _input_units_allow_dimensionless = True\n\n    def __new__(cls, *args, **kwargs):\n        long_name = cls.name.split(\"_\")[1]\n        cls.prj_code = _PROJ_NAME_CODE_MAP[long_name]\n        return super().__new__(cls)\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self._prj.code = self.prj_code\n        self._update_prj()\n        if not self.param_names:\n            # force initial call to Prjprm.set() for projections\n            # without parameters:\n            self._prj.set()\n\n        self.inputs = (\"phi\", \"theta\")\n        self.outputs = (\"x\", \"y\")\n\n    @property\n    def input_units(self):\n        return {self.inputs[0]: u.deg, self.inputs[1]: u.deg}\n\n    @property\n    def return_units(self):\n        return {self.outputs[0]: u.deg, self.outputs[1]: u.deg}\n\n    def evaluate(self, phi, theta, *args, **kwargs):\n        self._update_prj()\n        return self._prj.prjs2x(phi, theta)\n\n    @property\n    def inverse(self):\n        pv = [ge"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "rotations.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "n alpha, delta\n\n\nclass RotateNative2Celestial(_SkyRotation):\n    \"\"\"\n    Transform from Native to Celestial Spherical Coordinates.\n\n    Parameters\n    ----------\n    lon : float or `~astropy.units.Quantity` ['angle']\n        Celestial longitude of the fiducial point.\n    lat : float or `~astropy.units.Quantity` ['angle']\n        Celestial latitude of the fiducial point.\n    lon_pole : float or `~astropy.units.Quantity` ['angle']\n        Longitude of the celestial pole in the native system.\n\n    Notes\n    -----\n    If ``lon``, ``lat`` and ``lon_pole`` are numerical values they\n    should be in units of deg. Inputs are angles on the native sphere.\n    Outputs are angles on the celestial sphere.\n    \"\"\"\n\n    n_inputs = 2\n    n_outputs = 2\n\n    @property\n    def input_units(self):\n        \"\"\"Input units.\"\"\"\n        return {self.inputs[0]: u.deg, self.inputs[1]: u.deg}\n\n    @property\n    def return_units(self):\n        \"\"\"Output units.\"\"\"\n        return {self.outputs[0]: u.deg, self.outputs[1]: u.deg}\n\n    def __init__(self, lon, lat, lon_pole, **kwargs):\n        super().__init__(lon, lat, lon_pole, **kwargs)\n        self.inputs = (\"phi_N\", \"theta_N\")\n        self.outputs = (\"alpha_C\", \"delta_C\")\n\n    def evaluate(self, phi_N, theta_N, lon, lat, lon_pole):\n        \"\"\"\n        Parameters\n        ----------\n        phi_N, theta_N : float or `~astropy.units.Quantity` ['angle']\n            Angles in the Native coordinate system.\n            it is assumed that numerical only inputs are in degrees.\n            If float, assumed in degrees.\n        lon, lat, lon_pole : float or `~astropy.units.Quantity` ['angle']\n            Parameter values when the model was initialized.\n            If float, assumed in degrees.\n\n        Returns\n        -------\n        alpha_C, delta_C : float or `~astropy.units.Quantity` ['angle']\n            Angles on the Celestial sphere.\n            If float, in degrees.\n        \"\"\"\n        # The values are in radians since they have already been through "}], "retrieved_count": 10, "cost_time": 1.209367036819458}
{"question": "How does the cache-clearing mechanism in the test verifying cache coherence between time objects and their array slices ensure consistency across views sharing underlying data arrays without creating circular references that prevent garbage collection?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 69000, "end_line": 71000, "belongs_to": {"file_name": "test_basic.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "__\n\n    # Clear the cache\n    del t.cache\n    assert \"cache\" not in t.__dict__\n    # Check accessing the cache creates an empty dictionary\n    assert not t.cache\n    assert \"cache\" in t.__dict__\n\n\n@pytest.mark.parametrize(\"masked\", [True, False])\ndef test_cache_coherence_with_views(masked):\n    # Create a time instance and a slice.\n    t = Time([\"2001:020\", \"2001:040\", \"2001:060\", \"2001:080\"], out_subfmt=\"date\")\n    if masked:\n        # Masked arrays do not own their data directly so worth testing.\n        t[1] = np.ma.masked\n    t01 = t[:2]\n    # These should share memory.\n    assert np.may_share_memory(t._time.jd1, t01._time.jd1)\n    # And have the same value, even though those are not shared,\n    # as they are calculated separately.\n    assert_array_equal(t01.value, t.value[:2])\n    assert not np.may_share_memory(t01.value, t.value)\n    # Check that we now have cached values.\n    assert \"format\" in t.cache\n    assert \"format\" in t01.cache\n    # This should still be the case if one or the other is set\n    # (regression test for gh-15452).\n    t[0] = \"1999:099\"\n    # Because the setting deletes all related caches.\n    assert not t.cache\n    assert not t01.cache\n    assert_array_equal(t01.jd1[:2], t.jd1[:2])\n    assert_array_equal(t01.value, t.value[:2])\n    # And also the other way around.\n    t01[1] = \"1999:100\"\n    assert not t.cache\n    assert not t01.cache\n    assert_array_equal(t01.jd1[:2], t.jd1[:2])\n    assert_array_equal(t01.value, t.value[:2])\n    # This works because they keep track of each other.\n    assert t01._id_cache is t._id_cache\n    assert set(t._id_cache) == {id(t), id(t01)}\n    # Check that our cache implementation does not keep objects alive\n    # unintentionally (i.e., that garbage collection works).\n    del t01\n    gc.collect()\n    assert set(t._id_cache) == {id(t)}\n    # Also check that deleting t01 did not remove the cache of t too.\n    assert \"format\" in t.cache\n    # If a copy was made, the cache is not shared.\n    tf = t.flatten()\n    as"}, {"start_line": 68000, "end_line": 70000, "belongs_to": {"file_name": "test_basic.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "atetime.tzname()\n    assert time == forced_to_astropy_time\n\n    # Test non-scalar time inputs:\n    time = Time([\"2010-09-03 00:00:00\", \"2005-09-03 06:00:00\", \"1990-09-03 06:00:00\"])\n    tz_aware_datetime = time.to_datetime(tz)\n    forced_to_astropy_time = Time(tz_aware_datetime)\n    for dt, tz_dt in zip(time.datetime, tz_aware_datetime):\n        assert tz.tzname(dt) == tz_dt.tzname()\n    assert np.all(time == forced_to_astropy_time)\n\n\ndef test_cache():\n    t = Time(\"2010-09-03 00:00:00\")\n    t2 = Time(\"2010-09-03 00:00:00\")\n\n    # Time starts out without a cache\n    assert \"cache\" not in t.__dict__\n\n    # Access the iso format and confirm that the cached version is as expected\n    t.iso\n    assert t.cache[\"format\"][\"iso\", \"*\", \"astropy\"] == t2.iso\n\n    # Access the TAI scale and confirm that the cached version is as expected\n    t.tai\n    assert t.cache[\"scale\"][\"tai\"] == t2.tai\n\n    # New Time object after scale transform does not have a cache yet\n    assert \"cache\" not in t.tt.__dict__\n\n    # Clear the cache\n    del t.cache\n    assert \"cache\" not in t.__dict__\n    # Check accessing the cache creates an empty dictionary\n    assert not t.cache\n    assert \"cache\" in t.__dict__\n\n\n@pytest.mark.parametrize(\"masked\", [True, False])\ndef test_cache_coherence_with_views(masked):\n    # Create a time instance and a slice.\n    t = Time([\"2001:020\", \"2001:040\", \"2001:060\", \"2001:080\"], out_subfmt=\"date\")\n    if masked:\n        # Masked arrays do not own their data directly so worth testing.\n        t[1] = np.ma.masked\n    t01 = t[:2]\n    # These should share memory.\n    assert np.may_share_memory(t._time.jd1, t01._time.jd1)\n    # And have the same value, even though those are not shared,\n    # as they are calculated separately.\n    assert_array_equal(t01.value, t.value[:2])\n    assert not np.may_share_memory(t01.value, t.value)\n    # Check that we now have cached values.\n    assert \"format\" in t.cache\n    assert \"format\" in t01.cache\n    # This should still be the case if one o"}, {"start_line": 70000, "end_line": 72000, "belongs_to": {"file_name": "test_basic.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "r the other is set\n    # (regression test for gh-15452).\n    t[0] = \"1999:099\"\n    # Because the setting deletes all related caches.\n    assert not t.cache\n    assert not t01.cache\n    assert_array_equal(t01.jd1[:2], t.jd1[:2])\n    assert_array_equal(t01.value, t.value[:2])\n    # And also the other way around.\n    t01[1] = \"1999:100\"\n    assert not t.cache\n    assert not t01.cache\n    assert_array_equal(t01.jd1[:2], t.jd1[:2])\n    assert_array_equal(t01.value, t.value[:2])\n    # This works because they keep track of each other.\n    assert t01._id_cache is t._id_cache\n    assert set(t._id_cache) == {id(t), id(t01)}\n    # Check that our cache implementation does not keep objects alive\n    # unintentionally (i.e., that garbage collection works).\n    del t01\n    gc.collect()\n    assert set(t._id_cache) == {id(t)}\n    # Also check that deleting t01 did not remove the cache of t too.\n    assert \"format\" in t.cache\n    # If a copy was made, the cache is not shared.\n    tf = t.flatten()\n    assert \"format\" in t.cache\n    assert not tf.cache\n    assert_array_equal(tf.value, t.value)\n    assert \"format\" in tf.cache\n    t[0] = \"2000:001\"\n    assert not t.cache\n    assert \"format\" in tf.cache\n    assert not np.all(tf.value == t.value)\n    assert tf._id_cache is not t._id_cache\n\n\ndef test_epoch_date_jd_is_day_fraction():\n    \"\"\"\n    Ensure that jd1 and jd2 of an epoch Time are respect the (day, fraction) convention\n    (see #6638)\n    \"\"\"\n    t0 = Time(\"J2000\", scale=\"tdb\")\n\n    assert t0.jd1 == 2451545.0\n    assert t0.jd2 == 0.0\n\n    t1 = Time(datetime.datetime(2000, 1, 1, 12, 0, 0), scale=\"tdb\")\n\n    assert t1.jd1 == 2451545.0\n    assert t1.jd2 == 0.0\n\n\ndef test_sum_is_equivalent():\n    \"\"\"\n    Ensure that two equal dates defined in different ways behave equally (#6638)\n    \"\"\"\n    t0 = Time(\"J2000\", scale=\"tdb\")\n    t1 = Time(\"2000-01-01 12:00:00\", scale=\"tdb\")\n\n    assert t0 == t1\n    assert (t0 + 1 * u.second) == (t1 + 1 * u.second)\n\n\ndef test_string_valued_columns():\n    #"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "test_basic.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "np.arange(len(mjd)), np.arange(len(mjd))),\n        )\n        t5a = t4[3]\n        assert t5a.location == t4.location[3]\n        assert t5a.location.shape == ()\n        t5b = t4[3:4]\n        assert t5b.location.shape == (1,)\n        # Check that indexing a size-1 array returns a scalar location as well;\n        # see gh-10113.\n        t5c = t5b[0]\n        assert t5c.location.shape == ()\n        t6 = t4[4:6]\n        assert np.all(t6.location == t4.location[4:6])\n        # check it is a view\n        # (via ndarray, since quantity setter problematic for structured array)\n        allzeros = np.array((0.0, 0.0, 0.0), dtype=t4.location.dtype)\n        assert t6.location.view(np.ndarray)[-1] != allzeros\n        assert t4.location.view(np.ndarray)[5] != allzeros\n        t6.location.view(np.ndarray)[-1] = allzeros\n        assert t4.location.view(np.ndarray)[5] == allzeros\n        # Test subscription also works for two-dimensional arrays.\n        frac = np.arange(0.0, 0.999, 0.2)\n        t7 = Time(\n            mjd[:, np.newaxis] + frac,\n            format=\"mjd\",\n            scale=\"utc\",\n            location=(\"45d\", \"50d\"),\n        )\n        assert t7[0, 0]._time.jd1 == t7._time.jd1[0, 0]\n        assert t7[0, 0].isscalar is True\n        assert np.all(t7[5]._time.jd1 == t7._time.jd1[5])\n        assert np.all(t7[5]._time.jd2 == t7._time.jd2[5])\n        assert np.all(t7[:, 2]._time.jd1 == t7._time.jd1[:, 2])\n        assert np.all(t7[:, 2]._time.jd2 == t7._time.jd2[:, 2])\n        assert np.all(t7[:, 0]._time.jd1 == t._time.jd1)\n        assert np.all(t7[:, 0]._time.jd2 == t._time.jd2)\n        # Get tdb to check that delta_tdb_tt attribute is sliced properly.\n        t7_tdb = t7.tdb\n        assert t7_tdb[0, 0].delta_tdb_tt == t7_tdb.delta_tdb_tt[0, 0]\n        assert np.all(t7_tdb[5].delta_tdb_tt == t7_tdb.delta_tdb_tt[5])\n        assert np.all(t7_tdb[:, 2].delta_tdb_tt == t7_tdb.delta_tdb_tt[:, 2])\n        # Explicitly set delta_tdb_tt attribute. Now it should not be sliced.\n        t7"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "test_basic.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "1\n        assert t._time.jd2 is t2._time.jd2\n\n        t2 = Time(t, copy=True)\n        assert np.all(t.jd - t2.jd == 0)\n        assert np.all((t - t2).jd == 0)\n        assert t._time.jd1 is not t2._time.jd1\n        assert t._time.jd2 is not t2._time.jd2\n\n        # Include initializers\n        t2 = Time(t, format=\"iso\", scale=\"tai\", precision=1)\n        assert t2.value == \"2010-01-01 00:00:34.0\"\n        t2 = Time(t, format=\"iso\", scale=\"tai\", out_subfmt=\"date\")\n        assert t2.value == \"2010-01-01\"\n\n    def test_getitem(self):\n        \"\"\"Test that Time objects holding arrays are properly subscriptable,\n        set isscalar as appropriate, and also subscript delta_ut1_utc, etc.\"\"\"\n\n        mjd = np.arange(50000, 50010)\n        t = Time(mjd, format=\"mjd\", scale=\"utc\", location=(\"45d\", \"50d\"))\n        t1 = t[3]\n        assert t1.isscalar is True\n        assert t1._time.jd1 == t._time.jd1[3]\n        assert t1.location is t.location\n        t1a = Time(mjd[3], format=\"mjd\", scale=\"utc\")\n        assert t1a.isscalar is True\n        assert np.all(t1._time.jd1 == t1a._time.jd1)\n        t1b = Time(t[3])\n        assert t1b.isscalar is True\n        assert np.all(t1._time.jd1 == t1b._time.jd1)\n        t2 = t[4:6]\n        assert t2.isscalar is False\n        assert np.all(t2._time.jd1 == t._time.jd1[4:6])\n        assert t2.location is t.location\n        t2a = Time(t[4:6])\n        assert t2a.isscalar is False\n        assert np.all(t2a._time.jd1 == t._time.jd1[4:6])\n        t2b = Time([t[4], t[5]])\n        assert t2b.isscalar is False\n        assert np.all(t2b._time.jd1 == t._time.jd1[4:6])\n        t2c = Time((t[4], t[5]))\n        assert t2c.isscalar is False\n        assert np.all(t2c._time.jd1 == t._time.jd1[4:6])\n        t.delta_tdb_tt = np.arange(len(t))  # Explicitly set (not testing .tdb)\n        t3 = t[4:6]\n        assert np.all(t3._delta_tdb_tt == t._delta_tdb_tt[4:6])\n        t4 = Time(\n            mjd,\n            format=\"mjd\",\n            scale=\"utc\",\n            location=("}, {"start_line": 71000, "end_line": 73000, "belongs_to": {"file_name": "test_basic.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "sert \"format\" in t.cache\n    assert not tf.cache\n    assert_array_equal(tf.value, t.value)\n    assert \"format\" in tf.cache\n    t[0] = \"2000:001\"\n    assert not t.cache\n    assert \"format\" in tf.cache\n    assert not np.all(tf.value == t.value)\n    assert tf._id_cache is not t._id_cache\n\n\ndef test_epoch_date_jd_is_day_fraction():\n    \"\"\"\n    Ensure that jd1 and jd2 of an epoch Time are respect the (day, fraction) convention\n    (see #6638)\n    \"\"\"\n    t0 = Time(\"J2000\", scale=\"tdb\")\n\n    assert t0.jd1 == 2451545.0\n    assert t0.jd2 == 0.0\n\n    t1 = Time(datetime.datetime(2000, 1, 1, 12, 0, 0), scale=\"tdb\")\n\n    assert t1.jd1 == 2451545.0\n    assert t1.jd2 == 0.0\n\n\ndef test_sum_is_equivalent():\n    \"\"\"\n    Ensure that two equal dates defined in different ways behave equally (#6638)\n    \"\"\"\n    t0 = Time(\"J2000\", scale=\"tdb\")\n    t1 = Time(\"2000-01-01 12:00:00\", scale=\"tdb\")\n\n    assert t0 == t1\n    assert (t0 + 1 * u.second) == (t1 + 1 * u.second)\n\n\ndef test_string_valued_columns():\n    # Columns have a nice shim that translates bytes to string as needed.\n    # Ensure Time can handle these.  Use multi-d array just to be sure.\n    times = [\n        [[f\"{y:04d}-{m:02d}-{d:02d}\" for d in range(1, 3)] for m in range(5, 7)]\n        for y in range(2012, 2014)\n    ]\n    cutf32 = Column(times)\n    cbytes = cutf32.astype(\"S\")\n    tutf32 = Time(cutf32)\n    tbytes = Time(cbytes)\n    assert np.all(tutf32 == tbytes)\n    tutf32 = Time(Column([\"B1950\"]))\n    tbytes = Time(Column([b\"B1950\"]))\n    assert tutf32 == tbytes\n    # Regression tests for arrays with entries with unequal length. gh-6903.\n    times = Column([b\"2012-01-01\", b\"2012-01-01T00:00:00\"])\n    assert np.all(Time(times) == Time([\"2012-01-01\", \"2012-01-01T00:00:00\"]))\n\n\ndef test_bytes_input():\n    tstring = \"2011-01-02T03:04:05\"\n    tbytes = b\"2011-01-02T03:04:05\"\n    assert tbytes.decode(\"ascii\") == tstring\n    t0 = Time(tstring)\n    t1 = Time(tbytes)\n    assert t1 == t0\n    tarray = np.array(tbytes)\n    assert tarray.dt"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "test_basic.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    assert t1a.isscalar is True\n        assert np.all(t1._time.jd1 == t1a._time.jd1)\n        t1b = Time(t[3])\n        assert t1b.isscalar is True\n        assert np.all(t1._time.jd1 == t1b._time.jd1)\n        t2 = t[4:6]\n        assert t2.isscalar is False\n        assert np.all(t2._time.jd1 == t._time.jd1[4:6])\n        assert t2.location is t.location\n        t2a = Time(t[4:6])\n        assert t2a.isscalar is False\n        assert np.all(t2a._time.jd1 == t._time.jd1[4:6])\n        t2b = Time([t[4], t[5]])\n        assert t2b.isscalar is False\n        assert np.all(t2b._time.jd1 == t._time.jd1[4:6])\n        t2c = Time((t[4], t[5]))\n        assert t2c.isscalar is False\n        assert np.all(t2c._time.jd1 == t._time.jd1[4:6])\n        t.delta_tdb_tt = np.arange(len(t))  # Explicitly set (not testing .tdb)\n        t3 = t[4:6]\n        assert np.all(t3._delta_tdb_tt == t._delta_tdb_tt[4:6])\n        t4 = Time(\n            mjd,\n            format=\"mjd\",\n            scale=\"utc\",\n            location=(np.arange(len(mjd)), np.arange(len(mjd))),\n        )\n        t5a = t4[3]\n        assert t5a.location == t4.location[3]\n        assert t5a.location.shape == ()\n        t5b = t4[3:4]\n        assert t5b.location.shape == (1,)\n        # Check that indexing a size-1 array returns a scalar location as well;\n        # see gh-10113.\n        t5c = t5b[0]\n        assert t5c.location.shape == ()\n        t6 = t4[4:6]\n        assert np.all(t6.location == t4.location[4:6])\n        # check it is a view\n        # (via ndarray, since quantity setter problematic for structured array)\n        allzeros = np.array((0.0, 0.0, 0.0), dtype=t4.location.dtype)\n        assert t6.location.view(np.ndarray)[-1] != allzeros\n        assert t4.location.view(np.ndarray)[5] != allzeros\n        t6.location.view(np.ndarray)[-1] = allzeros\n        assert t4.location.view(np.ndarray)[5] == allzeros\n        # Test subscription also works for two-dimensional arrays.\n        frac = np.arange(0.0, 0.999, 0.2)\n        t7 = Time("}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "test_shape_manipulation.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e[item])\n        assert np.may_share_memory(s2_0101.obstime.jd1, self.s2.obstime.jd1)\n        assert_array_equal(s2_0101.obsgeoloc.xyz, self.s2.obsgeoloc[item].xyz)\n        s3_0101 = self.s3[item]\n        assert s3_0101.shape == (1, 1)\n        assert s3_0101.obstime.shape == (1, 1)\n        assert np.all(s3_0101.obstime == self.s3.obstime[item])\n        assert np.may_share_memory(s3_0101.obstime.jd1, self.s3.obstime.jd1)\n        assert_array_equal(s3_0101.obsgeoloc.xyz, self.s3.obsgeoloc[item].xyz)\n        sc_0101 = self.sc[item]\n        assert sc_0101.shape == (1, 1)\n        assert_array_equal(sc_0101.data.lon, self.sc.data.lon[item])\n        assert np.may_share_memory(sc_0101.data.lat, self.sc.data.lat)\n        assert np.all(sc_0101.obstime == self.sc.obstime[item])\n        assert np.may_share_memory(sc_0101.obstime.jd1, self.sc.obstime.jd1)\n        assert_array_equal(sc_0101.obsgeoloc.xyz, self.sc.obsgeoloc[item].xyz)\n\n    def test_ravel(self):\n        s0_ravel = self.s0.ravel()\n        assert s0_ravel.shape == (self.s0.size,)\n        assert np.all(s0_ravel.data.lon == self.s0.data.lon.ravel())\n        assert np.may_share_memory(s0_ravel.data.lon, self.s0.data.lon)\n        assert np.may_share_memory(s0_ravel.data.lat, self.s0.data.lat)\n        # Since s1 lon, lat were broadcast, ravel needs to make a copy.\n        s1_ravel = self.s1.ravel()\n        assert s1_ravel.shape == (self.s1.size,)\n        assert np.all(s1_ravel.data.lon == self.s1.data.lon.ravel())\n        assert not np.may_share_memory(s1_ravel.data.lat, self.s1.data.lat)\n        assert np.all(s1_ravel.obstime == self.s1.obstime.ravel())\n        assert not np.may_share_memory(s1_ravel.obstime.jd1, self.s1.obstime.jd1)\n        assert np.all(s1_ravel.location == self.s1.location.ravel())\n        assert not np.may_share_memory(s1_ravel.location, self.s1.location)\n        assert np.all(s1_ravel.temperature == self.s1.temperature.ravel())\n        assert np.may_share_memory(s1_ravel.temperature, self.s1.tempera"}, {"start_line": 0, "end_line": 1201, "belongs_to": {"file_name": "test_pickle.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\nimport pickle\n\nimport numpy as np\n\nfrom astropy.time import Time\n\n\nclass TestPickle:\n    \"\"\"Basic pickle test of time\"\"\"\n\n    def test_pickle(self):\n        times = [\"1999-01-01 00:00:00.123456789\", \"2010-01-01 00:00:00\"]\n        t1 = Time(times, scale=\"utc\")\n\n        for prot in range(pickle.HIGHEST_PROTOCOL):\n            t1d = pickle.dumps(t1, prot)\n            t1l = pickle.loads(t1d)\n            assert np.all(t1l == t1)\n\n        t2 = Time(\"2012-06-30 12:00:00\", scale=\"utc\")\n\n        for prot in range(pickle.HIGHEST_PROTOCOL):\n            t2d = pickle.dumps(t2, prot)\n            t2l = pickle.loads(t2d)\n            assert t2l == t2\n\n    def test_cache_not_shared(self):\n        t = Time([\"2001:020\", \"2001:040\", \"2001:060\", \"2001:080\"], out_subfmt=\"date\")\n        # Ensure something is in the cache.\n        t.value\n        assert \"format\" in t.cache\n        td = pickle.dumps(t)\n        assert \"format\" in t.cache\n        tl = pickle.loads(td)\n        assert \"format\" in t.cache\n        assert \"format\" not in tl.cache\n        t[0] = \"1999:099\"\n        assert t.value[0] == \"1999:099\"\n        assert tl.value[0] == \"2001:020\"\n"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_basic.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "array\"\"\"\n\n        assert allclose_sec(\n            t.cxcsec, np.array([31536064.307456788, 378691266.18400002])\n        )\n\n    def test_different_dimensions(self):\n        \"\"\"Test scalars, vector, and higher-dimensions\"\"\"\n        # scalar\n        val, val1 = 2450000.0, 0.125\n        t1 = Time(val, val1, format=\"jd\")\n        assert t1.isscalar is True and t1.shape == ()\n        # vector\n        val = np.arange(2450000.0, 2450010.0)\n        t2 = Time(val, format=\"jd\")\n        assert t2.isscalar is False and t2.shape == val.shape\n        # explicitly check broadcasting for mixed vector, scalar.\n        val2 = 0.0\n        t3 = Time(val, val2, format=\"jd\")\n        assert t3.isscalar is False and t3.shape == val.shape\n        val2 = (np.arange(5.0) / 10.0).reshape(5, 1)\n        # now see if broadcasting to two-dimensional works\n        t4 = Time(val, val2, format=\"jd\")\n        assert t4.isscalar is False\n        assert t4.shape == np.broadcast(val, val2).shape\n\n    @pytest.mark.parametrize(\"format_\", Time.FORMATS)\n    def test_empty_value(self, format_):\n        t = Time([], format=format_)\n        assert t.size == 0\n        assert t.shape == (0,)\n        assert t.format == format_\n        t_value = t.value\n        assert t_value.size == 0\n        assert t_value.shape == (0,)\n        t2 = Time(t_value, format=format_)\n        assert t2.size == 0\n        assert t2.shape == (0,)\n        assert t2.format == format_\n        t3 = t2.tai\n        assert t3.size == 0\n        assert t3.shape == (0,)\n        assert t3.format == format_\n        assert t3.scale == \"tai\"\n\n    @pytest.mark.parametrize(\"value\", [2455197.5, [2455197.5]])\n    def test_copy_time(self, value):\n        \"\"\"Test copying the values of a Time object by passing it into the\n        Time initializer.\n        \"\"\"\n        t = Time(value, format=\"jd\", scale=\"utc\")\n\n        t2 = Time(t, copy=False)\n        assert np.all(t.jd - t2.jd == 0)\n        assert np.all((t - t2).jd == 0)\n        assert t._time.jd1 is t2._time.jd"}], "retrieved_count": 10, "cost_time": 1.243760108947754}
{"question": "Why does the test class that performs binary serialization roundtrips in the VOTable test suite incur performance overhead from repeatedly creating new in-memory binary stream objects during conversions between XML and binary representations of astronomical table data?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "test_vo.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "regression.bin.tabledata.truth.{votable.version}.xml\"\n        ),\n        encoding=\"utf-8\",\n    ) as fd:\n        truth = fd.readlines()\n    with open(str(tmp_path / \"regression.bin.tabledata.xml\"), encoding=\"utf-8\") as fd:\n        output = fd.readlines()\n\n    # If the lines happen to be different, print a diff\n    # This is convenient for debugging\n    sys.stdout.writelines(\n        difflib.unified_diff(truth, output, fromfile=\"truth\", tofile=\"output\")\n    )\n\n    assert truth == output\n\n    # Test implicit gzip saving\n    votable2.to_xml(\n        str(tmp_path / \"regression.bin.tabledata.xml.gz\"),\n        _astropy_version=\"testing\",\n        _debug_python_based_parser=_python_based,\n    )\n    with gzip.GzipFile(str(tmp_path / \"regression.bin.tabledata.xml.gz\"), \"rb\") as gzfd:\n        output = gzfd.readlines()\n    output = [x.decode(\"utf-8\").rstrip() for x in output]\n    truth = [x.rstrip() for x in truth]\n\n    assert truth == output\n\n\n@pytest.mark.xfail(\"legacy_float_repr\")\ndef test_regression(tmp_path):\n    # W39: Bit values can not be masked\n    with pytest.warns(W39), np.errstate(over=\"ignore\"):\n        _test_regression(tmp_path, False)\n\n\n@pytest.mark.xfail(\"legacy_float_repr\")\ndef test_regression_python_based_parser(tmp_path):\n    # W39: Bit values can not be masked\n    with pytest.warns(W39), np.errstate(over=\"ignore\"):\n        _test_regression(tmp_path, True)\n\n\n@pytest.mark.xfail(\"legacy_float_repr\")\ndef test_regression_binary2(tmp_path):\n    # W39: Bit values can not be masked\n    with pytest.warns(W39), np.errstate(over=\"ignore\"):\n        _test_regression(tmp_path, False, 2)\n\n\nclass TestFixups:\n    def setup_class(self):\n        with np.errstate(over=\"ignore\"):\n            # https://github.com/astropy/astropy/issues/13341\n            self.table = parse(\n                get_pkg_data_filename(\"data/regression.xml\")\n            ).get_first_table()\n        self.array = self.table.array\n        self.mask = self.table.array.mask\n\n    def test_implicit_id(self):\n     "}, {"start_line": 25000, "end_line": 27000, "belongs_to": {"file_name": "test_vo.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "o.BytesIO()\n        # W39: Bit values can not be masked\n        with pytest.warns(W39):\n            votable.to_xml(bio)\n        bio.seek(0)\n        self.votable = parse(bio)\n        self.table = self.votable.get_first_table()\n        self.array = self.table.array\n        self.mask = self.table.array.mask\n\n    def test_bit_mask(self):\n        assert_array_equal(self.mask[\"bit\"], [False, False, False, False, False])\n\n    def test_bitarray_mask(self):\n        assert not np.any(self.mask[\"bitarray\"])\n\n    def test_bit_array2_mask(self):\n        assert not np.any(self.mask[\"bitarray2\"])\n\n    def test_schema(self, tmp_path):\n        # have to use an actual file because assert_validate_schema only works\n        # on filenames, not file-like objects\n        fn = tmp_path / \"test_through_tabledata.xml\"\n        with open(fn, \"wb\") as f:\n            f.write(self.xmlout.getvalue())\n        assert_validate_schema(fn, \"1.1\")\n\n\nclass TestThroughBinary(TestParse):\n    def setup_class(self):\n        with np.errstate(over=\"ignore\"):\n            # https://github.com/astropy/astropy/issues/13341\n            votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n        votable.get_first_table().format = \"binary\"\n\n        self.xmlout = bio = io.BytesIO()\n        # W39: Bit values can not be masked\n        with pytest.warns(W39):\n            votable.to_xml(bio)\n        bio.seek(0)\n        self.votable = parse(bio)\n\n        self.table = self.votable.get_first_table()\n        self.array = self.table.array\n        self.mask = self.table.array.mask\n\n    # Masked values in bit fields don't roundtrip through the binary\n    # representation -- that's not a bug, just a limitation, so\n    # override the mask array checks here.\n    def test_bit_mask(self):\n        assert not np.any(self.mask[\"bit\"])\n\n    def test_bitarray_mask(self):\n        assert not np.any(self.mask[\"bitarray\"])\n\n    def test_bit_array2_mask(self):\n        assert not np.any(self.mask[\"bitarray2\"])\n\n    def test_null_inte"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "test_vo.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "bleComplex\", \"<c16\"),\n        (\"doubleComplexArray\", \"|O8\"),\n        (\"doubleComplexArrayFixed\", \"<c16\", (2,)),\n        (\"boolean\", \"|b1\"),\n        (\"booleanArray\", \"|b1\", (4,)),\n        (\"nulls\", \"<i4\"),\n        (\"nulls_array\", \"<i4\", (2, 2)),\n        (\"precision1\", \"<f8\"),\n        (\"precision2\", \"<f8\"),\n        (\"doublearray\", \"|O8\"),\n        (\"bitarray2\", \"|b1\", (16,)),\n    ]\n    if sys.byteorder == \"big\":\n        new_dtypes = []\n        for dtype in dtypes:\n            dtype = list(dtype)\n            dtype[1] = dtype[1].replace(\"<\", \">\")\n            new_dtypes.append(tuple(dtype))\n        dtypes = new_dtypes\n    assert table.array.dtype == dtypes\n\n    votable.to_xml(\n        str(tmp_path / \"regression.tabledata.xml\"),\n        _debug_python_based_parser=_python_based,\n    )\n    assert_validate_schema(str(tmp_path / \"regression.tabledata.xml\"), votable.version)\n\n    if binary_mode == 1:\n        votable.get_first_table().format = \"binary\"\n        votable.version = \"1.1\"\n    elif binary_mode == 2:\n        votable.get_first_table()._config[\"version_1_3_or_later\"] = True\n        votable.get_first_table().format = \"binary2\"\n        votable.version = \"1.3\"\n\n    # Also try passing a file handle\n    with open(str(tmp_path / \"regression.binary.xml\"), \"wb\") as fd:\n        votable.to_xml(fd, _debug_python_based_parser=_python_based)\n    assert_validate_schema(str(tmp_path / \"regression.binary.xml\"), votable.version)\n    # Also try passing a file handle\n    with open(str(tmp_path / \"regression.binary.xml\"), \"rb\") as fd:\n        votable2 = parse(fd, _debug_python_based_parser=_python_based)\n    votable2.get_first_table().format = \"tabledata\"\n    votable2.to_xml(\n        str(tmp_path / \"regression.bin.tabledata.xml\"),\n        _astropy_version=\"testing\",\n        _debug_python_based_parser=_python_based,\n    )\n    assert_validate_schema(\n        str(tmp_path / \"regression.bin.tabledata.xml\"), votable.version\n    )\n\n    with open(\n        get_pkg_data_filename(\n            f\"data/"}, {"start_line": 28000, "end_line": 30000, "belongs_to": {"file_name": "test_vo.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "        self.array = self.table.array\n        self.mask = self.table.array.mask\n\n    def test_get_coosys_by_id(self):\n        # No COOSYS in VOTable 1.2 or later\n        pass\n\n    def test_null_integer_binary2(self):\n        # Integers with no magic values should still be\n        # masked in BINARY2 format\n\n        self.array.mask[\"intNoNull\"][0] = True\n\n        table = self.votable.get_first_table()\n        array = table.array\n\n        assert array.mask[\"intNoNull\"][0]\n        assert array[\"intNoNull\"].mask[0]\n\n\n@pytest.mark.parametrize(\"format_\", [\"binary\", \"binary2\"])\ndef test_select_columns_binary(format_):\n    with np.errstate(over=\"ignore\"):\n        # https://github.com/astropy/astropy/issues/13341\n        votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n    if format_ == \"binary2\":\n        votable.version = \"1.3\"\n        votable.get_first_table()._config[\"version_1_3_or_later\"] = True\n    votable.get_first_table().format = format_\n\n    bio = io.BytesIO()\n    # W39: Bit values can not be masked\n    with pytest.warns(W39):\n        votable.to_xml(bio)\n    bio.seek(0)\n    votable = parse(bio, columns=[0, 1, 2])\n    table = votable.get_first_table().to_table()\n    assert table.colnames == [\"string_test\", \"string_test_2\", \"unicode_test\"]\n\n\ndef table_from_scratch():\n    from astropy.io.votable.tree import Field, Resource, TableElement, VOTableFile\n\n    # Create a new VOTable file...\n    votable = VOTableFile()\n\n    # ...with one resource...\n    resource = Resource()\n    votable.resources.append(resource)\n\n    # ... with one table\n    table = TableElement(votable)\n    resource.tables.append(table)\n\n    # Define some fields\n    table.fields.extend(\n        [\n            Field(votable, ID=\"filename\", datatype=\"char\"),\n            Field(votable, ID=\"matrix\", datatype=\"double\", arraysize=\"2x2\"),\n        ]\n    )\n\n    # Now, use those field definitions to create the numpy record arrays, with\n    # the given number of rows\n    table.create_arrays(2)\n\n    # Now "}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "test_table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  \"col15\",\n        \"col16\",\n        \"col17\",\n    ]\n\n\ndef test_table_read_with_unnamed_tables():\n    \"\"\"\n    Issue #927.\n    \"\"\"\n    with get_pkg_data_fileobj(\"data/names.xml\", encoding=\"binary\") as fd:\n        t = Table.read(fd, format=\"votable\")\n\n    assert len(t) == 1\n\n\ndef test_votable_path_object():\n    \"\"\"\n    Testing when votable is passed as pathlib.Path object #4412.\n    \"\"\"\n    fpath = pathlib.Path(get_pkg_data_filename(\"data/names.xml\"))\n    table = parse(fpath).get_first_table().to_table()\n\n    assert len(table) == 1\n    assert int(table[0][3]) == 266\n\n\ndef test_from_table_without_mask():\n    t = Table()\n    c = Column(data=[1, 2, 3], name=\"a\")\n    t.add_column(c)\n    output = io.BytesIO()\n    t.write(output, format=\"votable\")\n\n\ndef test_write_with_format():\n    t = Table()\n    c = Column(data=[1, 2, 3], name=\"a\")\n    t.add_column(c)\n\n    output = io.BytesIO()\n    t.write(output, format=\"votable\", tabledata_format=\"binary\")\n    obuff = output.getvalue()\n    assert b'VOTABLE version=\"1.4\"' in obuff\n    assert b\"BINARY\" in obuff\n    assert b\"TABLEDATA\" not in obuff\n\n    output = io.BytesIO()\n    t.write(output, format=\"votable\", tabledata_format=\"binary2\")\n    obuff = output.getvalue()\n    assert b'VOTABLE version=\"1.4\"' in obuff\n    assert b\"BINARY2\" in obuff\n    assert b\"TABLEDATA\" not in obuff\n\n\n@pytest.mark.skipif(not HAS_PYARROW, reason=\"requires pyarrow\")\n@pytest.mark.parametrize(\"overwrite\", [True, False])\ndef test_read_write_votable_parquet(tmp_path, overwrite):\n    \"\"\"\n    Test to write and read VOTable with Parquet serialization\n    \"\"\"\n\n    # Create some fake data\n    number_of_objects = 10\n    ids = [f\"COSMOS_{ii:03g}\" for ii in range(number_of_objects)]\n    redshift = np.random.uniform(low=0, high=3, size=number_of_objects)\n    mass = np.random.uniform(low=1e8, high=1e10, size=number_of_objects)\n    sfr = np.random.uniform(low=1, high=100, size=number_of_objects)\n    astropytab = Table([ids, redshift, mass, sfr], names=[\"id\", \"z\", \"mass\", \"sfr\""}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "test_vo.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "y_mode == 2:\n        votable.get_first_table()._config[\"version_1_3_or_later\"] = True\n        votable.get_first_table().format = \"binary2\"\n        votable.version = \"1.3\"\n\n    # Also try passing a file handle\n    with open(str(tmp_path / \"regression.binary.xml\"), \"wb\") as fd:\n        votable.to_xml(fd, _debug_python_based_parser=_python_based)\n    assert_validate_schema(str(tmp_path / \"regression.binary.xml\"), votable.version)\n    # Also try passing a file handle\n    with open(str(tmp_path / \"regression.binary.xml\"), \"rb\") as fd:\n        votable2 = parse(fd, _debug_python_based_parser=_python_based)\n    votable2.get_first_table().format = \"tabledata\"\n    votable2.to_xml(\n        str(tmp_path / \"regression.bin.tabledata.xml\"),\n        _astropy_version=\"testing\",\n        _debug_python_based_parser=_python_based,\n    )\n    assert_validate_schema(\n        str(tmp_path / \"regression.bin.tabledata.xml\"), votable.version\n    )\n\n    with open(\n        get_pkg_data_filename(\n            f\"data/regression.bin.tabledata.truth.{votable.version}.xml\"\n        ),\n        encoding=\"utf-8\",\n    ) as fd:\n        truth = fd.readlines()\n    with open(str(tmp_path / \"regression.bin.tabledata.xml\"), encoding=\"utf-8\") as fd:\n        output = fd.readlines()\n\n    # If the lines happen to be different, print a diff\n    # This is convenient for debugging\n    sys.stdout.writelines(\n        difflib.unified_diff(truth, output, fromfile=\"truth\", tofile=\"output\")\n    )\n\n    assert truth == output\n\n    # Test implicit gzip saving\n    votable2.to_xml(\n        str(tmp_path / \"regression.bin.tabledata.xml.gz\"),\n        _astropy_version=\"testing\",\n        _debug_python_based_parser=_python_based,\n    )\n    with gzip.GzipFile(str(tmp_path / \"regression.bin.tabledata.xml.gz\"), \"rb\") as gzfd:\n        output = gzfd.readlines()\n    output = [x.decode(\"utf-8\").rstrip() for x in output]\n    truth = [x.rstrip() for x in truth]\n\n    assert truth == output\n\n\n@pytest.mark.xfail(\"legacy_float_repr\")\ndef test_regre"}, {"start_line": 26000, "end_line": 28000, "belongs_to": {"file_name": "test_vo.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "th np.errstate(over=\"ignore\"):\n            # https://github.com/astropy/astropy/issues/13341\n            votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n        votable.get_first_table().format = \"binary\"\n\n        self.xmlout = bio = io.BytesIO()\n        # W39: Bit values can not be masked\n        with pytest.warns(W39):\n            votable.to_xml(bio)\n        bio.seek(0)\n        self.votable = parse(bio)\n\n        self.table = self.votable.get_first_table()\n        self.array = self.table.array\n        self.mask = self.table.array.mask\n\n    # Masked values in bit fields don't roundtrip through the binary\n    # representation -- that's not a bug, just a limitation, so\n    # override the mask array checks here.\n    def test_bit_mask(self):\n        assert not np.any(self.mask[\"bit\"])\n\n    def test_bitarray_mask(self):\n        assert not np.any(self.mask[\"bitarray\"])\n\n    def test_bit_array2_mask(self):\n        assert not np.any(self.mask[\"bitarray2\"])\n\n    def test_null_integer_binary(self):\n        # BINARY1 requires magic value to be specified\n\n        self.array.mask[\"intNoNull\"][0] = True\n\n        bio = io.BytesIO()\n\n        # W31: NaN's can not be represented in integer field\n        with pytest.warns(W31):\n            # https://github.com/astropy/astropy/issues/16090\n            self.votable.to_xml(bio)\n\n\nclass TestThroughBinary2(TestParse):\n    def setup_class(self):\n        with np.errstate(over=\"ignore\"):\n            # https://github.com/astropy/astropy/issues/13341\n            votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n        votable.version = \"1.3\"\n        votable.get_first_table()._config[\"version_1_3_or_later\"] = True\n        votable.get_first_table().format = \"binary2\"\n\n        self.xmlout = bio = io.BytesIO()\n        # W39: Bit values can not be masked\n        with pytest.warns(W39):\n            votable.to_xml(bio)\n        bio.seek(0)\n        self.votable = parse(bio)\n\n        self.table = self.votable.get_first_table()\n"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "test_table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "version=\"1.4\"' in obuff\n    assert b\"BINARY\" in obuff\n    assert b\"TABLEDATA\" not in obuff\n\n    output = io.BytesIO()\n    t.write(output, format=\"votable\", tabledata_format=\"binary2\")\n    obuff = output.getvalue()\n    assert b'VOTABLE version=\"1.4\"' in obuff\n    assert b\"BINARY2\" in obuff\n    assert b\"TABLEDATA\" not in obuff\n\n\n@pytest.mark.skipif(not HAS_PYARROW, reason=\"requires pyarrow\")\n@pytest.mark.parametrize(\"overwrite\", [True, False])\ndef test_read_write_votable_parquet(tmp_path, overwrite):\n    \"\"\"\n    Test to write and read VOTable with Parquet serialization\n    \"\"\"\n\n    # Create some fake data\n    number_of_objects = 10\n    ids = [f\"COSMOS_{ii:03g}\" for ii in range(number_of_objects)]\n    redshift = np.random.uniform(low=0, high=3, size=number_of_objects)\n    mass = np.random.uniform(low=1e8, high=1e10, size=number_of_objects)\n    sfr = np.random.uniform(low=1, high=100, size=number_of_objects)\n    astropytab = Table([ids, redshift, mass, sfr], names=[\"id\", \"z\", \"mass\", \"sfr\"])\n\n    # Create Column metadata\n    column_metadata = {\n        \"id\": {\"unit\": \"\", \"ucd\": \"meta.id\", \"utype\": \"none\"},\n        \"z\": {\"unit\": \"\", \"ucd\": \"src.redshift\", \"utype\": \"none\"},\n        \"mass\": {\"unit\": \"solMass\", \"ucd\": \"phys.mass\", \"utype\": \"none\"},\n        \"sfr\": {\"unit\": \"solMass / yr\", \"ucd\": \"phys.SFR\", \"utype\": \"none\"},\n    }\n\n    # Write VOTable with Parquet serialization\n    filename = tmp_path / \"test_votable_parquet.vot\"\n    astropytab.write(\n        filename,\n        column_metadata=column_metadata,\n        overwrite=overwrite,\n        format=\"votable.parquet\",\n    )\n\n    # Check both files are written out\n    assert set(os.listdir(tmp_path)) == {\n        \"test_votable_parquet.vot\",\n        \"test_votable_parquet.vot.parquet\",\n    }\n\n    # Open created VOTable with Parquet serialization\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"always\", ResourceWarning)\n        votable = parse(filename)\n\n    # Get table out\n    votable_table = votable.resour"}, {"start_line": 27000, "end_line": 29000, "belongs_to": {"file_name": "test_vo.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ger_binary(self):\n        # BINARY1 requires magic value to be specified\n\n        self.array.mask[\"intNoNull\"][0] = True\n\n        bio = io.BytesIO()\n\n        # W31: NaN's can not be represented in integer field\n        with pytest.warns(W31):\n            # https://github.com/astropy/astropy/issues/16090\n            self.votable.to_xml(bio)\n\n\nclass TestThroughBinary2(TestParse):\n    def setup_class(self):\n        with np.errstate(over=\"ignore\"):\n            # https://github.com/astropy/astropy/issues/13341\n            votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n        votable.version = \"1.3\"\n        votable.get_first_table()._config[\"version_1_3_or_later\"] = True\n        votable.get_first_table().format = \"binary2\"\n\n        self.xmlout = bio = io.BytesIO()\n        # W39: Bit values can not be masked\n        with pytest.warns(W39):\n            votable.to_xml(bio)\n        bio.seek(0)\n        self.votable = parse(bio)\n\n        self.table = self.votable.get_first_table()\n        self.array = self.table.array\n        self.mask = self.table.array.mask\n\n    def test_get_coosys_by_id(self):\n        # No COOSYS in VOTable 1.2 or later\n        pass\n\n    def test_null_integer_binary2(self):\n        # Integers with no magic values should still be\n        # masked in BINARY2 format\n\n        self.array.mask[\"intNoNull\"][0] = True\n\n        table = self.votable.get_first_table()\n        array = table.array\n\n        assert array.mask[\"intNoNull\"][0]\n        assert array[\"intNoNull\"].mask[0]\n\n\n@pytest.mark.parametrize(\"format_\", [\"binary\", \"binary2\"])\ndef test_select_columns_binary(format_):\n    with np.errstate(over=\"ignore\"):\n        # https://github.com/astropy/astropy/issues/13341\n        votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n    if format_ == \"binary2\":\n        votable.version = \"1.3\"\n        votable.get_first_table()._config[\"version_1_3_or_later\"] = True\n    votable.get_first_table().format = format_\n\n    bio = io.BytesIO()\n    # W39: "}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "test_vo.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ingle_table(get_pkg_data_filename(\"data/regression.xml\"), table_number=3)\n\n\ndef _test_regression(tmp_path, _python_based=False, binary_mode=1):\n    # Read the VOTABLE\n    votable = parse(\n        get_pkg_data_filename(\"data/regression.xml\"),\n        _debug_python_based_parser=_python_based,\n    )\n    table = votable.get_first_table()\n\n    dtypes = [\n        ((\"string test\", \"string_test\"), \"|O8\"),\n        ((\"fixed string test\", \"string_test_2\"), \"<U10\"),\n        (\"unicode_test\", \"|O8\"),\n        ((\"unicode test\", \"fixed_unicode_test\"), \"<U10\"),\n        ((\"string array test\", \"string_array_test\"), \"<U4\"),\n        (\"unsignedByte\", \"|u1\"),\n        (\"short\", \"<i2\"),\n        (\"int\", \"<i4\"),\n        (\"intNoNull\", \"<i4\"),\n        (\"long\", \"<i8\"),\n        (\"double\", \"<f8\"),\n        (\"float\", \"<f4\"),\n        (\"array\", \"|O8\"),\n        (\"bit\", \"|b1\"),\n        (\"bitarray\", \"|b1\", (3, 2)),\n        (\"bitvararray\", \"|O8\"),\n        (\"bitvararray2\", \"|O8\"),\n        (\"floatComplex\", \"<c8\"),\n        (\"doubleComplex\", \"<c16\"),\n        (\"doubleComplexArray\", \"|O8\"),\n        (\"doubleComplexArrayFixed\", \"<c16\", (2,)),\n        (\"boolean\", \"|b1\"),\n        (\"booleanArray\", \"|b1\", (4,)),\n        (\"nulls\", \"<i4\"),\n        (\"nulls_array\", \"<i4\", (2, 2)),\n        (\"precision1\", \"<f8\"),\n        (\"precision2\", \"<f8\"),\n        (\"doublearray\", \"|O8\"),\n        (\"bitarray2\", \"|b1\", (16,)),\n    ]\n    if sys.byteorder == \"big\":\n        new_dtypes = []\n        for dtype in dtypes:\n            dtype = list(dtype)\n            dtype[1] = dtype[1].replace(\"<\", \">\")\n            new_dtypes.append(tuple(dtype))\n        dtypes = new_dtypes\n    assert table.array.dtype == dtypes\n\n    votable.to_xml(\n        str(tmp_path / \"regression.tabledata.xml\"),\n        _debug_python_based_parser=_python_based,\n    )\n    assert_validate_schema(str(tmp_path / \"regression.tabledata.xml\"), votable.version)\n\n    if binary_mode == 1:\n        votable.get_first_table().format = \"binary\"\n        votable.version = \"1.1\"\n    elif binar"}], "retrieved_count": 10, "cost_time": 0.3268551826477051}
{"question": "Where in the sorting method of the list subclass that reorders tuples for astropy table column attributes in the table metadata module does the control flow preserve column metadata ordering while maintaining additional key-value pairs after the predefined column keys?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "meta.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " This is only for use\n    in generating a YAML map representation that has a fixed order.\n    \"\"\"\n\n    def items(self):\n        \"\"\"\n        Return items as a ColumnOrderList, which sorts in the preferred\n        way for column attributes.\n        \"\"\"\n        return ColumnOrderList(super().items())\n\n\ndef _construct_odict(load, node):\n    \"\"\"\n    Construct dict from !!omap in yaml safe load.\n\n    See ``get_header_from_yaml()`` for usage.\n\n    Source: https://gist.github.com/weaver/317164\n    License: Unspecified\n\n    This is the same as SafeConstructor.construct_yaml_omap(),\n    except the data type is changed to OrderedDict() and setitem is\n    used instead of append in the loop\n    \"\"\"\n    omap = {}\n    yield omap\n    if not isinstance(node, yaml.SequenceNode):\n        raise yaml.constructor.ConstructorError(\n            \"while constructing an ordered map\",\n            node.start_mark,\n            f\"expected a sequence, but found {node.id}\",\n            node.start_mark,\n        )\n\n    for subnode in node.value:\n        if not isinstance(subnode, yaml.MappingNode):\n            raise yaml.constructor.ConstructorError(\n                \"while constructing an ordered map\",\n                node.start_mark,\n                f\"expected a mapping of length 1, but found {subnode.id}\",\n                subnode.start_mark,\n            )\n\n        if len(subnode.value) != 1:\n            raise yaml.constructor.ConstructorError(\n                \"while constructing an ordered map\",\n                node.start_mark,\n                f\"expected a single mapping item, but found {len(subnode.value)} items\",\n                subnode.start_mark,\n            )\n\n        key_node, value_node = subnode.value[0]\n        key = load.construct_object(key_node)\n        value = load.construct_object(value_node)\n        omap[key] = value\n\n\ndef _repr_pairs(dump, tag, sequence, flow_style=None):\n    \"\"\"\n    This is the same code as BaseRepresenter.represent_sequence(),\n    but the value passed to dump.repr"}, {"start_line": 17000, "end_line": 19000, "belongs_to": {"file_name": "column.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "map[\"unit\"] = StructuredUnit(units, dtype)\n        map.update(dtype=dtype, shape=shape, length=len(data[dtype.names[0]]))\n        # Construct the empty column from `map` (note: 'data' removed above).\n        result = super()._construct_from_dict(map)\n        # Fill it with the structured data.\n        for name in dtype.names:\n            result[name] = data[name]\n        return result\n\n    def new_like(self, cols, length, metadata_conflicts=\"warn\", name=None):\n        \"\"\"\n        Return a new Column instance which is consistent with the\n        input ``cols`` and has ``length`` rows.\n\n        This is intended for creating an empty column object whose elements can\n        be set in-place for table operations like join or vstack.\n\n        Parameters\n        ----------\n        cols : list\n            List of input columns\n        length : int\n            Length of the output column object\n        metadata_conflicts : str ('warn'|'error'|'silent')\n            How to handle metadata conflicts\n        name : str\n            Output column name\n\n        Returns\n        -------\n        col : Column (or subclass)\n            New instance of this class consistent with ``cols``\n\n        \"\"\"\n        attrs = self.merge_cols_attributes(\n            cols, metadata_conflicts, name, (\"meta\", \"unit\", \"format\", \"description\")\n        )\n\n        return self._parent_cls(length=length, **attrs)\n\n    def get_sortable_arrays(self):\n        \"\"\"\n        Return a list of arrays which can be lexically sorted to represent\n        the order of the parent column.\n\n        For Column this is just the column itself.\n\n        Returns\n        -------\n        arrays : list of ndarray\n        \"\"\"\n        return [self._parent]\n\n\nclass BaseColumn(_ColumnGetitemShim, np.ndarray):\n    meta = MetaData(default_factory=dict)\n\n    def __new__(\n        cls,\n        data=None,\n        name=None,\n        dtype=None,\n        shape=(),\n        length=0,\n        description=None,\n        unit=None,\n        format=Non"}, {"start_line": 25000, "end_line": 27000, "belongs_to": {"file_name": "data_info.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "          return {\n                attr: getattr(col.info, attr)\n                for attr in attrs\n                if getattr(col.info, attr, None) is not None\n            }\n\n        out = getattrs(cols[0])\n        for col in cols[1:]:\n            out = metadata.merge(\n                out,\n                getattrs(col),\n                metadata_conflicts=metadata_conflicts,\n                warn_str_func=warn_str_func,\n            )\n\n        # Output dtype is the superset of all dtypes in in_cols\n        out[\"dtype\"] = metadata.common_dtype(cols)\n\n        # Make sure all input shapes are the same\n        uniq_shapes = {col.shape[1:] for col in cols}\n        if len(uniq_shapes) != 1:\n            raise TableMergeError(\"columns have different shapes\")\n        out[\"shape\"] = uniq_shapes.pop()\n\n        # \"Merged\" output name is the supplied name\n        if name is not None:\n            out[\"name\"] = str(name)\n\n        return out\n\n    def get_sortable_arrays(self):\n        \"\"\"\n        Return a list of arrays which can be lexically sorted to represent\n        the order of the parent column.\n\n        The base method raises NotImplementedError and must be overridden.\n\n        Returns\n        -------\n        arrays : list of ndarray\n        \"\"\"\n        raise NotImplementedError(f\"column {self.name} is not sortable\")\n\n\nclass MixinInfo(BaseColumnInfo):\n    @property\n    def name(self):\n        return self._attrs.get(\"name\")\n\n    @name.setter\n    def name(self, name: str | None):\n        if name is None:\n            new_name = None\n        elif isinstance(name, str):\n            new_name = str(name)\n        else:\n            raise TypeError(\n                f\"Expected a str value, got {name} with type {type(name).__name__}\"\n            )\n\n        # For mixin columns that live within a table, rename the column in the\n        # table when setting the name attribute.  This mirrors the same\n        # functionality in the BaseColumn class.\n        if self.parent_table is not None:\n   "}, {"start_line": 109000, "end_line": 111000, "belongs_to": {"file_name": "test_table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ySubMaskedColumn([5], mask=[True])\n\n    # Two different pathways for making table\n    t1 = MyTable([a, b, c, d, e], names=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n    t2 = MyTable()\n    t2[\"a\"] = a\n    t2[\"b\"] = b\n    t2[\"c\"] = c\n    t2[\"d\"] = d\n    t2[\"e\"] = e\n\n    for t in (t1, t2):\n        assert type(t[\"a\"]) is MyColumn\n        assert type(t[\"b\"]) is MyMaskedColumn  # upgrade\n        assert type(t[\"c\"]) is MyMaskedColumn\n        assert type(t[\"d\"]) is MySubColumn\n        assert type(t[\"e\"]) is MySubMaskedColumn  # sub-class not downgraded\n\n\ndef test_sort_with_mutable_skycoord():\n    \"\"\"Test sorting a table that has a mutable column such as SkyCoord.\n\n    In this case the sort is done in-place\n    \"\"\"\n    t = Table([[2, 1], SkyCoord([4, 3], [6, 5], unit=\"deg,deg\")], names=[\"a\", \"sc\"])\n    meta = {\"a\": [1, 2]}\n    ta = t[\"a\"]\n    tsc = t[\"sc\"]\n    t[\"sc\"].info.meta = meta\n    t.sort(\"a\")\n    assert np.all(t[\"a\"] == [1, 2])\n    assert np.allclose(t[\"sc\"].ra.to_value(u.deg), [3, 4])\n    assert np.allclose(t[\"sc\"].dec.to_value(u.deg), [5, 6])\n    assert t[\"a\"] is ta\n    assert t[\"sc\"] is tsc\n\n    # Prior to astropy 4.1 this was a deep copy of SkyCoord column; after 4.1\n    # it is a reference.\n    t[\"sc\"].info.meta[\"a\"][0] = 100\n    assert meta[\"a\"][0] == 100\n\n\ndef test_sort_with_non_mutable():\n    \"\"\"Test sorting a table that has a non-mutable column.\"\"\"\n    t = Table([[2, 1], [3, 4]], names=[\"a\", \"b\"])\n    ta = t[\"a\"]\n    tb = t[\"b\"]\n    t[\"b\"].setflags(write=False)\n    meta = {\"a\": [1, 2]}\n    t[\"b\"].info.meta = meta\n    t.sort(\"a\")\n    assert np.all(t[\"a\"] == [1, 2])\n    assert np.all(t[\"b\"] == [4, 3])\n    assert ta is t[\"a\"]\n    assert tb is not t[\"b\"]\n\n    # Prior to astropy 4.1 this was a deep copy of SkyCoord column; after 4.1\n    # it is a reference.\n    t[\"b\"].info.meta[\"a\"][0] = 100\n    assert meta[\"a\"][0] == 1\n\n\ndef test_init_with_list_of_masked_arrays():\n    \"\"\"Test the fix for #8977\"\"\"\n    m0 = np.ma.array([0, 1, 2], mask=[True, False, True])\n    m1 = np.ma.array([3,"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "meta.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " represent_mapping(self, tag, mapping, flow_style=None):\n            \"\"\"\n            This is a combination of the Python 2 and 3 versions of this method\n            in the PyYAML library to allow the required key ordering via the\n            ColumnOrderList object.  The Python 3 version insists on turning the\n            items() mapping into a list object and sorting, which results in\n            alphabetical order for the column keys.\n            \"\"\"\n            value = []\n            node = yaml.MappingNode(tag, value, flow_style=flow_style)\n            if self.alias_key is not None:\n                self.represented_objects[self.alias_key] = node\n            best_style = True\n            if hasattr(mapping, \"items\"):\n                mapping = mapping.items()\n                if hasattr(mapping, \"sort\"):\n                    mapping.sort()\n                else:\n                    mapping = list(mapping)\n                    try:\n                        mapping = sorted(mapping)\n                    except TypeError:\n                        pass\n\n            for item_key, item_value in mapping:\n                node_key = self.represent_data(item_key)\n                node_value = self.represent_data(item_value)\n                if not (isinstance(node_key, yaml.ScalarNode) and not node_key.style):\n                    best_style = False\n                if not (\n                    isinstance(node_value, yaml.ScalarNode) and not node_value.style\n                ):\n                    best_style = False\n                value.append((node_key, node_value))\n            if flow_style is None:\n                if self.default_flow_style is not None:\n                    node.flow_style = self.default_flow_style\n                else:\n                    node.flow_style = best_style\n            return node\n\n    TableDumper.add_representer(OrderedDict, _repr_odict)\n    TableDumper.add_representer(ColumnDict, _repr_column_dict)\n\n    header = copy.copy(header)  # Don't overwrite origi"}, {"start_line": 18000, "end_line": 20000, "belongs_to": {"file_name": "column.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ts\n        name : str\n            Output column name\n\n        Returns\n        -------\n        col : Column (or subclass)\n            New instance of this class consistent with ``cols``\n\n        \"\"\"\n        attrs = self.merge_cols_attributes(\n            cols, metadata_conflicts, name, (\"meta\", \"unit\", \"format\", \"description\")\n        )\n\n        return self._parent_cls(length=length, **attrs)\n\n    def get_sortable_arrays(self):\n        \"\"\"\n        Return a list of arrays which can be lexically sorted to represent\n        the order of the parent column.\n\n        For Column this is just the column itself.\n\n        Returns\n        -------\n        arrays : list of ndarray\n        \"\"\"\n        return [self._parent]\n\n\nclass BaseColumn(_ColumnGetitemShim, np.ndarray):\n    meta = MetaData(default_factory=dict)\n\n    def __new__(\n        cls,\n        data=None,\n        name=None,\n        dtype=None,\n        shape=(),\n        length=0,\n        description=None,\n        unit=None,\n        format=None,\n        meta=None,\n        copy=COPY_IF_NEEDED,\n        copy_indices=True,\n    ):\n        if data is None:\n            self_data = np.zeros((length,) + shape, dtype=dtype)\n        elif isinstance(data, BaseColumn) and hasattr(data, \"_name\"):\n            # When unpickling a MaskedColumn, ``data`` will be a bare\n            # BaseColumn with none of the expected attributes.  In this case\n            # do NOT execute this block which initializes from ``data``\n            # attributes.\n            self_data = np.array(data.data, dtype=dtype, copy=copy)\n            if description is None:\n                description = data.description\n            if unit is None:\n                unit = unit or data.unit\n            if format is None:\n                format = data.format\n            if meta is None:\n                meta = data.meta\n            if name is None:\n                name = data.name\n        elif isinstance(data, Quantity):\n            if unit is None:\n                self_data ="}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "polarization.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "pty column object whose elements can\n        be set in-place for table operations like join or vstack.\n\n        Parameters\n        ----------\n        cols : list\n            List of input columns\n        length : int\n            Length of the output column object\n        metadata_conflicts : str ('warn'|'error'|'silent')\n            How to handle metadata conflicts\n        name : str\n            Output column name\n\n        Returns\n        -------\n        col : `~astropy.coordinates.StokesCoord` (or subclass)\n            Empty instance of this class consistent with ``cols``\n\n        \"\"\"\n        # Get merged info attributes like shape, dtype, format, description, etc.\n        attrs = self.merge_cols_attributes(\n            cols, metadata_conflicts, name, (\"meta\", \"format\", \"description\")\n        )\n\n        # Make an empty StokesCoord.\n        shape = (length,) + attrs.pop(\"shape\")\n        data = np.zeros(shape=shape, dtype=attrs.pop(\"dtype\"))\n        # Get arguments needed to reconstruct class\n        out = self._construct_from_dict({\"value\": data})\n\n        # Set remaining info attributes\n        for attr, value in attrs.items():\n            setattr(out.info, attr, value)\n\n        return out\n\n    def get_sortable_arrays(self):\n        \"\"\"\n        Return a list of arrays which can be lexically sorted to represent\n        the order of the parent column.\n\n        For StokesCoord this is just the underlying values.\n\n        Returns\n        -------\n        arrays : list of ndarray\n        \"\"\"\n        return [self._parent._data]\n\n\nclass StokesCoord(ShapedLikeNDArray):\n    \"\"\"\n    A representation of stokes coordinates with helpers for converting to profile names.\n\n    Parameters\n    ----------\n    stokes : array-like\n        The numeric values representing stokes coordinates.\n    \"\"\"\n\n    info = StokesCoordInfo()\n\n    def __init__(self, stokes, copy=False):\n        if isinstance(stokes, type(self)):\n            data = stokes._data.copy() if copy else stokes._data\n         "}, {"start_line": 125000, "end_line": 127000, "belongs_to": {"file_name": "table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "es to\n            the originals.\n\n        See Also\n        --------\n        add_columns, astropy.table.hstack, replace_column\n\n        Examples\n        --------\n        Update a table with another table::\n\n            >>> t1 = Table({'a': ['foo', 'bar'], 'b': [0., 0.]}, meta={'i': 0})\n            >>> t2 = Table({'b': [1., 2.], 'c': [7., 11.]}, meta={'n': 2})\n            >>> t1.update(t2)\n            >>> t1\n            <Table length=2>\n             a      b       c\n            str3 float64 float64\n            ---- ------- -------\n             foo     1.0     7.0\n             bar     2.0    11.0\n            >>> t1.meta\n            {'i': 0, 'n': 2}\n\n        Update a table with a dictionary::\n\n            >>> t = Table({'a': ['foo', 'bar'], 'b': [0., 0.]})\n            >>> t.update({'b': [1., 2.]})\n            >>> t\n            <Table length=2>\n             a      b\n            str3 float64\n            ---- -------\n             foo     1.0\n             bar     2.0\n        \"\"\"\n        from .operations import _merge_table_meta\n\n        if not isinstance(other, Table):\n            other = self.__class__(other, copy=copy)\n        common_cols = set(self.colnames).intersection(other.colnames)\n        for name, col in other.items():\n            if name in common_cols:\n                self.replace_column(name, col, copy=copy)\n            else:\n                self.add_column(col, name=name, copy=copy)\n        _merge_table_meta(self, [self, other], metadata_conflicts=\"silent\")\n\n    def argsort(self, keys=None, kind=None, reverse=False):\n        \"\"\"\n        Return the indices which would sort the table according to one or\n        more key columns.  This simply calls the `numpy.argsort` function on\n        the table with the ``order`` parameter set to ``keys``.\n\n        Parameters\n        ----------\n        keys : str or list of str\n            The column name(s) to order the table by\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n            Sorting algo"}, {"start_line": 128000, "end_line": 130000, "belongs_to": {"file_name": "table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "me, which get stored as\n            # object arrays.\n            if len(keys) > 1:\n                kwargs[\"order\"] = keys\n                data = self.as_array(names=keys)\n            else:\n                data = self[keys[0]]\n        else:\n            # No keys provided so sort on all columns.\n            data = self.as_array()\n\n        if kind:\n            kwargs[\"kind\"] = kind\n\n        # np.argsort will look for a possible .argsort method (e.g., for Time),\n        # and if that fails cast to an array and try sorting that way.\n        idx = np.argsort(data, **kwargs)\n\n        return idx[::-1] if reverse else idx\n\n    def sort(self, keys=None, *, kind=None, reverse=False):\n        \"\"\"\n        Sort the table according to one or more keys. This operates\n        on the existing table and does not return a new table.\n\n        Parameters\n        ----------\n        keys : str or list of str\n            The key(s) to order the table by. If None, use the\n            primary index of the Table.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n            Sorting algorithm used by ``numpy.argsort``.\n        reverse : bool\n            Sort in reverse order (default=False)\n\n        Examples\n        --------\n        Create a table with 3 columns::\n\n            >>> t = Table([['Max', 'Jo', 'John'], ['Miller', 'Miller', 'Jackson'],\n            ...            [12, 15, 18]], names=('firstname', 'name', 'tel'))\n            >>> print(t)\n            firstname   name  tel\n            --------- ------- ---\n                  Max  Miller  12\n                   Jo  Miller  15\n                 John Jackson  18\n\n        Sorting according to standard sorting rules, first 'name' then 'firstname'::\n\n            >>> t.sort(['name', 'firstname'])\n            >>> print(t)\n            firstname   name  tel\n            --------- ------- ---\n                 John Jackson  18\n                   Jo  Miller  15\n                  Max  Miller  12\n\n        Sorting according to stand"}, {"start_line": 108000, "end_line": 110000, "belongs_to": {"file_name": "test_table.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "[1]  # Treat as broadcastable scalar, not length=1 array (which would fail)\n    assert np.all(t[\"a\"] == [[3, 4], [3, 4]])\n    assert np.all(t[\"b\"] == [5, 5])\n    assert np.all(t[\"c\"] == [1, 1])\n\n    # Test that broadcasted column is writeable\n    t[\"c\"][1] = 10\n    assert np.all(t[\"c\"] == [1, 10])\n\n\ndef test_custom_masked_column_in_nonmasked_table():\n    \"\"\"Test the refactor and change in column upgrades introduced\n    in 95902650f.  This fixes a regression introduced by #8789\n    (Change behavior of Table regarding masked columns).\"\"\"\n\n    class MyMaskedColumn(table.MaskedColumn):\n        pass\n\n    class MySubMaskedColumn(MyMaskedColumn):\n        pass\n\n    class MyColumn(table.Column):\n        pass\n\n    class MySubColumn(MyColumn):\n        pass\n\n    class MyTable(table.Table):\n        Column = MyColumn\n        MaskedColumn = MyMaskedColumn\n\n    a = table.Column([1])\n    b = table.MaskedColumn([2], mask=[True])\n    c = MyMaskedColumn([3], mask=[True])\n    d = MySubColumn([4])\n    e = MySubMaskedColumn([5], mask=[True])\n\n    # Two different pathways for making table\n    t1 = MyTable([a, b, c, d, e], names=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n    t2 = MyTable()\n    t2[\"a\"] = a\n    t2[\"b\"] = b\n    t2[\"c\"] = c\n    t2[\"d\"] = d\n    t2[\"e\"] = e\n\n    for t in (t1, t2):\n        assert type(t[\"a\"]) is MyColumn\n        assert type(t[\"b\"]) is MyMaskedColumn  # upgrade\n        assert type(t[\"c\"]) is MyMaskedColumn\n        assert type(t[\"d\"]) is MySubColumn\n        assert type(t[\"e\"]) is MySubMaskedColumn  # sub-class not downgraded\n\n\ndef test_sort_with_mutable_skycoord():\n    \"\"\"Test sorting a table that has a mutable column such as SkyCoord.\n\n    In this case the sort is done in-place\n    \"\"\"\n    t = Table([[2, 1], SkyCoord([4, 3], [6, 5], unit=\"deg,deg\")], names=[\"a\", \"sc\"])\n    meta = {\"a\": [1, 2]}\n    ta = t[\"a\"]\n    tsc = t[\"sc\"]\n    t[\"sc\"].info.meta = meta\n    t.sort(\"a\")\n    assert np.all(t[\"a\"] == [1, 2])\n    assert np.allclose(t[\"sc\"].ra.to_value(u.deg), [3, 4])\n    assert np.al"}], "retrieved_count": 10, "cost_time": 0.34023404121398926}
{"question": "At which level in the inheritance hierarchy does the class attribute specifying the function unit class control instantiation and conversion behavior inherited from the base class for logarithmic quantities?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "logarithmic.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/function", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "        converting it to the physical unit inferred from ``unit``.\n\n    unit : str, `~astropy.units.UnitBase`, or `~astropy.units.FunctionUnitBase`, optional\n        For an `~astropy.units.FunctionUnitBase` instance, the\n        physical unit will be taken from it; for other input, it will be\n        inferred from ``value``. By default, ``unit`` is set by the subclass.\n\n    dtype : `~numpy.dtype`, optional\n        The ``dtype`` of the resulting Numpy array or scalar that will\n        hold the value.  If not provided, is is determined automatically\n        from the input value.\n\n    copy : bool, optional\n        If `True` (default), then the value is copied.  Otherwise, a copy will\n        only be made if ``__array__`` returns a copy, if value is a nested\n        sequence, or if a copy is needed to satisfy an explicitly given\n        ``dtype``.  (The `False` option is intended mostly for internal use,\n        to speed up initialization where a copy is known to have been made.\n        Use with care.)\n\n    Examples\n    --------\n    Typically, use is made of an `~astropy.units.FunctionQuantity`\n    subclass, as in::\n\n        >>> import astropy.units as u\n        >>> u.Magnitude(-2.5)\n        <Magnitude -2.5 mag>\n        >>> u.Magnitude(10.*u.count/u.second)\n        <Magnitude -2.5 mag(ct / s)>\n        >>> u.Decibel(1.*u.W, u.DecibelUnit(u.mW))  # doctest: +FLOAT_CMP\n        <Decibel 30. dB(mW)>\n\n    \"\"\"\n\n    # only override of FunctionQuantity\n    _unit_class = LogUnit\n\n    # additions that work just for logarithmic units\n    def __add__(self, other):\n        # Add function units, thus multiplying physical units. If no unit is\n        # given, assume dimensionless_unscaled; this will give the appropriate\n        # exception in LogUnit.__add__.\n        new_unit = self.unit + getattr(other, \"unit\", dimensionless_unscaled)\n        # Add actual logarithmic values, rescaling, e.g., dB -> dex.\n        result = self._function_view + getattr(other, \"_function_view\", other)\n    "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "logarithmic.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/function", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tion_unit :  `~astropy.units.Unit` or `string`\n        By default, the same as the logarithmic unit set by the subclass.\n\n    \"\"\"\n\n    # the four essential overrides of FunctionUnitBase\n    @lazyproperty\n    def _default_function_unit(self):\n        from .units import dex\n\n        return dex\n\n    @property\n    def _quantity_class(self):\n        return LogQuantity\n\n    def from_physical(self, x):\n        \"\"\"Transformation from value in physical to value in logarithmic units.\n        Used in equivalency.\n        \"\"\"\n        # Local import to avoid circular dependency.\n        from .units import dex\n\n        return dex.to(self._function_unit, np.log10(x))\n\n    def to_physical(self, x):\n        \"\"\"Transformation from value in logarithmic to value in physical units.\n        Used in equivalency.\n        \"\"\"\n        from .units import dex\n\n        return 10 ** self._function_unit.to(dex, x)\n\n    # ^^^^ the four essential overrides of FunctionUnitBase\n\n    # add addition and subtraction, which imply multiplication/division of\n    # the underlying physical units\n    def _add_and_adjust_physical_unit(self, other, sign_self, sign_other):\n        \"\"\"Add/subtract LogUnit to/from another unit, and adjust physical unit.\n\n        self and other are multiplied by sign_self and sign_other, resp.\n\n        We wish to do:   lu_1 + lu_2  -> lu_f          (lu=logarithmic unit)\n                  and     pu_1^(1) * pu_2^(1) -> pu_f  (pu=physical unit)\n\n        Raises\n        ------\n        UnitsError\n            If function units are not equivalent.\n        \"\"\"\n        # First, insist on compatible logarithmic type. Here, plain u.mag,\n        # u.dex, and u.dB are OK, i.e., other does not have to be LogUnit\n        # (this will indirectly test whether other is a unit at all).\n        try:\n            getattr(other, \"function_unit\", other)._to(self._function_unit)\n        except AttributeError:\n            # if other is not a unit (i.e., does not have _to).\n            return NotImplemen"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "logarithmic.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/function", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "       If not given, dimensionless.\n\n    function_unit :  `~astropy.units.Unit` or `string`\n        By default, this is ``dB``, but this allows one to use an equivalent\n        unit such as ``2 dB``.\n    \"\"\"\n\n    @lazyproperty\n    def _default_function_unit(self):\n        from .units import dB\n\n        return dB\n\n    @property\n    def _quantity_class(self):\n        return Decibel\n\n\nclass LogQuantity(FunctionQuantity):\n    \"\"\"A representation of a (scaled) logarithm of a number with a unit.\n\n    Parameters\n    ----------\n    value : number, `~astropy.units.Quantity`, `~astropy.units.LogQuantity`, or sequence of quantity-like.\n        The numerical value of the logarithmic quantity. If a number or\n        a `~astropy.units.Quantity` with a logarithmic unit, it will be\n        converted to ``unit`` and the physical unit will be inferred from\n        ``unit``.  If a `~astropy.units.Quantity` with just a physical unit,\n        it will converted to the logarithmic unit, after, if necessary,\n        converting it to the physical unit inferred from ``unit``.\n\n    unit : str, `~astropy.units.UnitBase`, or `~astropy.units.FunctionUnitBase`, optional\n        For an `~astropy.units.FunctionUnitBase` instance, the\n        physical unit will be taken from it; for other input, it will be\n        inferred from ``value``. By default, ``unit`` is set by the subclass.\n\n    dtype : `~numpy.dtype`, optional\n        The ``dtype`` of the resulting Numpy array or scalar that will\n        hold the value.  If not provided, is is determined automatically\n        from the input value.\n\n    copy : bool, optional\n        If `True` (default), then the value is copied.  Otherwise, a copy will\n        only be made if ``__array__`` returns a copy, if value is a nested\n        sequence, or if a copy is needed to satisfy an explicitly given\n        ``dtype``.  (The `False` option is intended mostly for internal use,\n        to speed up initialization where a copy is known to have been made.\n        Us"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "logarithmic.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/function", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "d within the magnitude function unit.\n        If not given, dimensionless.\n\n    function_unit :  `~astropy.units.Unit` or `string`\n        By default, this is ``dex``, but this allows one to use an equivalent\n        unit such as ``0.5 dex``.\n    \"\"\"\n\n    @lazyproperty\n    def _default_function_unit(self):\n        from .units import dex\n\n        return dex\n\n    @property\n    def _quantity_class(self):\n        return Dex\n\n    def to_string(self, format=\"generic\"):\n        if format == \"cds\":\n            if self.physical_unit == dimensionless_unscaled:\n                return \"[-]\"  # by default, would get \"[---]\".\n            else:\n                return f\"[{self.physical_unit.to_string(format=format)}]\"\n        else:\n            return super().to_string()\n\n\nclass DecibelUnit(LogUnit):\n    \"\"\"Logarithmic physical units expressed in dB.\n\n    Parameters\n    ----------\n    physical_unit : `~astropy.units.Unit` or `string`\n        Unit that is encapsulated within the decibel function unit.\n        If not given, dimensionless.\n\n    function_unit :  `~astropy.units.Unit` or `string`\n        By default, this is ``dB``, but this allows one to use an equivalent\n        unit such as ``2 dB``.\n    \"\"\"\n\n    @lazyproperty\n    def _default_function_unit(self):\n        from .units import dB\n\n        return dB\n\n    @property\n    def _quantity_class(self):\n        return Decibel\n\n\nclass LogQuantity(FunctionQuantity):\n    \"\"\"A representation of a (scaled) logarithm of a number with a unit.\n\n    Parameters\n    ----------\n    value : number, `~astropy.units.Quantity`, `~astropy.units.LogQuantity`, or sequence of quantity-like.\n        The numerical value of the logarithmic quantity. If a number or\n        a `~astropy.units.Quantity` with a logarithmic unit, it will be\n        converted to ``unit`` and the physical unit will be inferred from\n        ``unit``.  If a `~astropy.units.Quantity` with just a physical unit,\n        it will converted to the logarithmic unit, after, if necessary,\n"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "logarithmic.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/function", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e with care.)\n\n    Examples\n    --------\n    Typically, use is made of an `~astropy.units.FunctionQuantity`\n    subclass, as in::\n\n        >>> import astropy.units as u\n        >>> u.Magnitude(-2.5)\n        <Magnitude -2.5 mag>\n        >>> u.Magnitude(10.*u.count/u.second)\n        <Magnitude -2.5 mag(ct / s)>\n        >>> u.Decibel(1.*u.W, u.DecibelUnit(u.mW))  # doctest: +FLOAT_CMP\n        <Decibel 30. dB(mW)>\n\n    \"\"\"\n\n    # only override of FunctionQuantity\n    _unit_class = LogUnit\n\n    # additions that work just for logarithmic units\n    def __add__(self, other):\n        # Add function units, thus multiplying physical units. If no unit is\n        # given, assume dimensionless_unscaled; this will give the appropriate\n        # exception in LogUnit.__add__.\n        new_unit = self.unit + getattr(other, \"unit\", dimensionless_unscaled)\n        # Add actual logarithmic values, rescaling, e.g., dB -> dex.\n        result = self._function_view + getattr(other, \"_function_view\", other)\n        return self._new_view(result, new_unit)\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __iadd__(self, other):\n        new_unit = self.unit + getattr(other, \"unit\", dimensionless_unscaled)\n        # Do calculation in-place using _function_view of array.\n        function_view = self._function_view\n        function_view += getattr(other, \"_function_view\", other)\n        self._set_unit(new_unit)\n        return self\n\n    def __sub__(self, other):\n        # Subtract function units, thus dividing physical units.\n        new_unit = self.unit - getattr(other, \"unit\", dimensionless_unscaled)\n        # Subtract actual logarithmic values, rescaling, e.g., dB -> dex.\n        result = self._function_view - getattr(other, \"_function_view\", other)\n        return self._new_view(result, new_unit)\n\n    def __rsub__(self, other):\n        new_unit = self.unit.__rsub__(getattr(other, \"unit\", dimensionless_unscaled))\n        result = self._function_view.__rsub__(getattr(other"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/function", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "@abstractmethod\n    def _quantity_class(self):\n        \"\"\"Function quantity class corresponding to this function unit.\n\n        This property should be overridden by subclasses, with, e.g.,\n        `~astropy.unit.MagUnit` returning `~astropy.unit.Magnitude`.\n        \"\"\"\n\n    @abstractmethod\n    def from_physical(self, x):\n        \"\"\"Transformation from value in physical to value in function units.\n\n        This method should be overridden by subclasses.  It is used to\n        provide automatic transformations using an equivalency.\n        \"\"\"\n\n    @abstractmethod\n    def to_physical(self, x):\n        \"\"\"Transformation from value in function to value in physical units.\n\n        This method should be overridden by subclasses.  It is used to\n        provide automatic transformations using an equivalency.\n        \"\"\"\n\n    #  the above four need to be set by subclasses\n\n    # have priority over arrays, regular units, and regular quantities\n    __array_priority__ = 30000\n\n    def __init__(self, physical_unit=None, function_unit=None):\n        if physical_unit is None:\n            physical_unit = dimensionless_unscaled\n        else:\n            physical_unit = Unit(physical_unit)\n            if not isinstance(physical_unit, UnitBase) or physical_unit.is_equivalent(\n                self._default_function_unit\n            ):\n                raise UnitConversionError(f\"{physical_unit} is not a physical unit.\")\n\n        if function_unit is None:\n            function_unit = self._default_function_unit\n        else:\n            # any function unit should be equivalent to subclass default\n            function_unit = Unit(getattr(function_unit, \"function_unit\", function_unit))\n            if not function_unit.is_equivalent(self._default_function_unit):\n                raise UnitConversionError(\n                    f\"Cannot initialize '{self.__class__.__name__}' instance with \"\n                    f\"function unit '{function_unit}', as it is not equivalent to \"\n                  "}, {"start_line": 18000, "end_line": 20000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/function", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  instantiated directly.  Rather, subclasses should be made which have\n    ``_unit_class`` pointing back to the corresponding function unit class.\n\n    Parameters\n    ----------\n    value : number, quantity-like, or sequence thereof\n        The numerical value of the function quantity. If a number or\n        a `~astropy.units.Quantity` with a function unit, it will be converted\n        to ``unit`` and the physical unit will be inferred from ``unit``.\n        If a `~astropy.units.Quantity` with just a physical unit, it will\n        converted to the function unit, after, if necessary, converting it to\n        the physical unit inferred from ``unit``.\n\n    unit : str, `~astropy.units.UnitBase`, or `~astropy.units.FunctionUnitBase`, optional\n        For an `~astropy.units.FunctionUnitBase` instance, the\n        physical unit will be taken from it; for other input, it will be\n        inferred from ``value``. By default, ``unit`` is set by the subclass.\n\n    dtype : `~numpy.dtype`, optional\n        The dtype of the resulting Numpy array or scalar that will\n        hold the value.  If not provided, it is determined from the input,\n        except that any input that cannot represent float (integer and bool)\n        is converted to float.\n\n    copy : bool, optional\n        If `True` (default), then the value is copied.  Otherwise, a copy will\n        only be made if ``__array__`` returns a copy, if value is a nested\n        sequence, or if a copy is needed to satisfy an explicitly given\n        ``dtype``.  (The `False` option is intended mostly for internal use,\n        to speed up initialization where a copy is known to have been made.\n        Use with care.)\n\n    order : {'C', 'F', 'A'}, optional\n        Specify the order of the array.  As in `~numpy.array`.  Ignored\n        if the input does not need to be converted and ``copy=False``.\n\n    subok : bool, optional\n        If `False` (default), the returned array will be forced to be of the\n        class used.  Otherwise, s"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/function", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    \"_ones_like\",\n        \"ones_like\",\n        \"positive\",\n    )\n    if hasattr(np_umath, ufunc)\n}\n\n# TODO: the following could work if helper changed relative to Quantity:\n# - spacing should return dimensionless, not same unit\n# - negative should negate unit too,\n# - add, subtract, comparisons can work if units added/subtracted\n\nSUPPORTED_FUNCTIONS = {\n    getattr(np, function)\n    for function in (\"clip\", \"trace\", \"mean\", \"min\", \"max\", \"round\")\n}\n\n\n# subclassing UnitBase or CompositeUnit was found to be problematic, requiring\n# a large number of overrides. Hence, define new class.\nclass FunctionUnitBase(metaclass=ABCMeta):\n    \"\"\"Abstract base class for function units.\n\n    Function units are functions containing a physical unit, such as dB(mW).\n    Most of the arithmetic operations on function units are defined in this\n    base class.\n\n    While instantiation is defined, this class should not be used directly.\n    Rather, subclasses should be used that override the abstract properties\n    `_default_function_unit` and `_quantity_class`, and the abstract methods\n    `from_physical`, and `to_physical`.\n\n    Parameters\n    ----------\n    physical_unit : `~astropy.units.Unit` or `string`\n        Unit that is encapsulated within the function unit.\n        If not given, dimensionless.\n\n    function_unit :  `~astropy.units.Unit` or `string`\n        By default, the same as the function unit set by the subclass.\n    \"\"\"\n\n    #  the following four need to be set by subclasses\n    # Make this a property so we can ensure subclasses define it.\n    @property\n    @abstractmethod\n    def _default_function_unit(self):\n        \"\"\"Default function unit corresponding to the function.\n\n        This property should be overridden by subclasses, with, e.g.,\n        `~astropy.unit.MagUnit` returning `~astropy.unit.mag`.\n        \"\"\"\n\n    # This has to be a property because the function quantity will not be\n    # known at unit definition time, as it gets defined after.\n    @property\n    "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "logarithmic.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/function", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numbers\n\nimport numpy as np\n\nfrom astropy.units import (\n    CompositeUnit,\n    Unit,\n    UnitConversionError,\n    UnitsError,\n    UnitTypeError,\n    dimensionless_unscaled,\n)\nfrom astropy.utils import lazyproperty\nfrom astropy.utils.compat.numpycompat import NUMPY_LT_2_0\n\nfrom .core import FunctionQuantity, FunctionUnitBase\n\n__all__ = [\n    \"Decibel\",\n    \"DecibelUnit\",\n    \"Dex\",\n    \"DexUnit\",\n    \"LogQuantity\",\n    \"LogUnit\",\n    \"MagUnit\",\n    \"Magnitude\",\n]\n\n\nclass LogUnit(FunctionUnitBase):\n    \"\"\"Logarithmic unit containing a physical one.\n\n    Usually, logarithmic units are instantiated via specific subclasses\n    such `~astropy.units.MagUnit`, `~astropy.units.DecibelUnit`, and\n    `~astropy.units.DexUnit`.\n\n    Parameters\n    ----------\n    physical_unit : `~astropy.units.Unit` or `string`\n        Unit that is encapsulated within the logarithmic function unit.\n        If not given, dimensionless.\n\n    function_unit :  `~astropy.units.Unit` or `string`\n        By default, the same as the logarithmic unit set by the subclass.\n\n    \"\"\"\n\n    # the four essential overrides of FunctionUnitBase\n    @lazyproperty\n    def _default_function_unit(self):\n        from .units import dex\n\n        return dex\n\n    @property\n    def _quantity_class(self):\n        return LogQuantity\n\n    def from_physical(self, x):\n        \"\"\"Transformation from value in physical to value in logarithmic units.\n        Used in equivalency.\n        \"\"\"\n        # Local import to avoid circular dependency.\n        from .units import dex\n\n        return dex.to(self._function_unit, np.log10(x))\n\n    def to_physical(self, x):\n        \"\"\"Transformation from value in logarithmic to value in physical units.\n        Used in equivalency.\n        \"\"\"\n        from .units import dex\n\n        return 10 ** self._function_unit.to(dex, x)\n\n    # ^^^^ the four essential overrides of FunctionUnitBase\n\n    # add addition and subtraction, which"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "logarithmic.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/function", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "gUnit; only equivalent one\n        # would be u.mag,u.dB,u.dex.  But might as well use common routine.\n        return self._add_and_adjust_physical_unit(other, -1, +1)\n\n\nclass MagUnit(LogUnit):\n    \"\"\"Logarithmic physical units expressed in magnitudes.\n\n    Parameters\n    ----------\n    physical_unit : `~astropy.units.Unit` or `string`\n        Unit that is encapsulated within the magnitude function unit.\n        If not given, dimensionless.\n\n    function_unit :  `~astropy.units.Unit` or `string`\n        By default, this is ``mag``, but this allows one to use an equivalent\n        unit such as ``2 mag``.\n    \"\"\"\n\n    @lazyproperty\n    def _default_function_unit(self):\n        from .units import mag\n\n        return mag\n\n    @property\n    def _quantity_class(self):\n        return Magnitude\n\n\nclass DexUnit(LogUnit):\n    \"\"\"Logarithmic physical units expressed in magnitudes.\n\n    Parameters\n    ----------\n    physical_unit : `~astropy.units.Unit` or `string`\n        Unit that is encapsulated within the magnitude function unit.\n        If not given, dimensionless.\n\n    function_unit :  `~astropy.units.Unit` or `string`\n        By default, this is ``dex``, but this allows one to use an equivalent\n        unit such as ``0.5 dex``.\n    \"\"\"\n\n    @lazyproperty\n    def _default_function_unit(self):\n        from .units import dex\n\n        return dex\n\n    @property\n    def _quantity_class(self):\n        return Dex\n\n    def to_string(self, format=\"generic\"):\n        if format == \"cds\":\n            if self.physical_unit == dimensionless_unscaled:\n                return \"[-]\"  # by default, would get \"[---]\".\n            else:\n                return f\"[{self.physical_unit.to_string(format=format)}]\"\n        else:\n            return super().to_string()\n\n\nclass DecibelUnit(LogUnit):\n    \"\"\"Logarithmic physical units expressed in dB.\n\n    Parameters\n    ----------\n    physical_unit : `~astropy.units.Unit` or `string`\n        Unit that is encapsulated within the decibel function unit.\n "}], "retrieved_count": 10, "cost_time": 0.3413548469543457}
{"question": "Where is the helper function that converts three angle unit inputs representing degree, arcminute, and arcsecond components to radian output units for the scipy special function that accepts three angle arguments?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "scipy_special.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "input in degrees and give dimensionless output.\ndegree_to_dimensionless_sps_ufuncs = (\"cosdg\", \"sindg\", \"tandg\", \"cotdg\")\nscipy_special_ufuncs += degree_to_dimensionless_sps_ufuncs\ntwo_arg_dimensionless_sps_ufuncs = (\n    \"jv\", \"jn\", \"jve\", \"yn\", \"yv\", \"yve\", \"kn\", \"kv\", \"kve\", \"iv\", \"ive\",\n    \"hankel1\", \"hankel1e\", \"hankel2\", \"hankel2e\",\n)  # fmt: skip\nscipy_special_ufuncs += two_arg_dimensionless_sps_ufuncs\n# ufuncs handled as special cases\nscipy_special_ufuncs += (\"cbrt\", \"radian\")\n\n\ndef helper_degree_to_dimensionless(f, unit):\n    from astropy.units.si import degree\n\n    try:\n        return [get_converter(unit, degree)], dimensionless_unscaled\n    except UnitsError:\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to quantities with angle units\"\n        )\n\n\ndef helper_degree_minute_second_to_radian(f, unit1, unit2, unit3):\n    from astropy.units.si import arcmin, arcsec, degree, radian\n\n    try:\n        return [\n            get_converter(unit1, degree),\n            get_converter(unit2, arcmin),\n            get_converter(unit3, arcsec),\n        ], radian\n    except UnitsError:\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to quantities with angle units\"\n        )\n\n\ndef get_scipy_special_helpers():\n    import scipy.special as sps\n\n    SCIPY_HELPERS = {}\n    for name in dimensionless_to_dimensionless_sps_ufuncs:\n        ufunc = getattr(sps, name, None)\n        SCIPY_HELPERS[ufunc] = helper_dimensionless_to_dimensionless\n\n    for ufunc in degree_to_dimensionless_sps_ufuncs:\n        SCIPY_HELPERS[getattr(sps, ufunc)] = helper_degree_to_dimensionless\n\n    for ufunc in two_arg_dimensionless_sps_ufuncs:\n        SCIPY_HELPERS[getattr(sps, ufunc)] = helper_two_arg_dimensionless\n\n    # ufuncs handled as special cases\n    SCIPY_HELPERS[sps.cbrt] = helper_cbrt\n    SCIPY_HELPERS[sps.radian] = helper_degree_minute_second_to_radian\n    return SCIPY_HELPERS\n\n\nUFUNC_HELPERS.register_module(\n    \"scipy.special\","}, {"start_line": 2000, "end_line": 3051, "belongs_to": {"file_name": "scipy_special.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ree),\n            get_converter(unit2, arcmin),\n            get_converter(unit3, arcsec),\n        ], radian\n    except UnitsError:\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to quantities with angle units\"\n        )\n\n\ndef get_scipy_special_helpers():\n    import scipy.special as sps\n\n    SCIPY_HELPERS = {}\n    for name in dimensionless_to_dimensionless_sps_ufuncs:\n        ufunc = getattr(sps, name, None)\n        SCIPY_HELPERS[ufunc] = helper_dimensionless_to_dimensionless\n\n    for ufunc in degree_to_dimensionless_sps_ufuncs:\n        SCIPY_HELPERS[getattr(sps, ufunc)] = helper_degree_to_dimensionless\n\n    for ufunc in two_arg_dimensionless_sps_ufuncs:\n        SCIPY_HELPERS[getattr(sps, ufunc)] = helper_two_arg_dimensionless\n\n    # ufuncs handled as special cases\n    SCIPY_HELPERS[sps.cbrt] = helper_cbrt\n    SCIPY_HELPERS[sps.radian] = helper_degree_minute_second_to_radian\n    return SCIPY_HELPERS\n\n\nUFUNC_HELPERS.register_module(\n    \"scipy.special\", scipy_special_ufuncs, get_scipy_special_helpers\n)\n"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "scipy_special.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"Quantity helpers for the scipy.special ufuncs.\n\nAvailable ufuncs in this module are at\nhttps://docs.scipy.org/doc/scipy/reference/special.html\n\"\"\"\n\nfrom astropy.units.core import dimensionless_unscaled\nfrom astropy.units.errors import UnitsError, UnitTypeError\n\nfrom . import UFUNC_HELPERS\nfrom .helpers import (\n    get_converter,\n    helper_cbrt,\n    helper_dimensionless_to_dimensionless,\n    helper_two_arg_dimensionless,\n)\n\ndimensionless_to_dimensionless_sps_ufuncs = (\n    \"erf\", \"erfc\", \"erfcx\", \"erfi\", \"erfinv\", \"erfcinv\",\n    \"gamma\", \"gammaln\", \"loggamma\", \"gammasgn\", \"psi\", \"rgamma\", \"digamma\",\n    \"wofz\", \"dawsn\", \"entr\", \"exprel\", \"expm1\", \"log1p\", \"exp2\", \"exp10\",\n    \"j0\", \"j1\", \"y0\", \"y1\", \"i0\", \"i0e\", \"i1\", \"i1e\",\n    \"k0\", \"k0e\", \"k1\", \"k1e\", \"itj0y0\", \"it2j0y0\", \"iti0k0\", \"it2i0k0\",\n    \"ndtr\", \"ndtri\",\n)  # fmt: skip\n\n\nscipy_special_ufuncs = dimensionless_to_dimensionless_sps_ufuncs\n# ufuncs that require input in degrees and give dimensionless output.\ndegree_to_dimensionless_sps_ufuncs = (\"cosdg\", \"sindg\", \"tandg\", \"cotdg\")\nscipy_special_ufuncs += degree_to_dimensionless_sps_ufuncs\ntwo_arg_dimensionless_sps_ufuncs = (\n    \"jv\", \"jn\", \"jve\", \"yn\", \"yv\", \"yve\", \"kn\", \"kv\", \"kve\", \"iv\", \"ive\",\n    \"hankel1\", \"hankel1e\", \"hankel2\", \"hankel2e\",\n)  # fmt: skip\nscipy_special_ufuncs += two_arg_dimensionless_sps_ufuncs\n# ufuncs handled as special cases\nscipy_special_ufuncs += (\"cbrt\", \"radian\")\n\n\ndef helper_degree_to_dimensionless(f, unit):\n    from astropy.units.si import degree\n\n    try:\n        return [get_converter(unit, degree)], dimensionless_unscaled\n    except UnitsError:\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to quantities with angle units\"\n        )\n\n\ndef helper_degree_minute_second_to_radian(f, unit1, unit2, unit3):\n    from astropy.units.si import arcmin, arcsec, degree, radian\n\n    try:\n        return [\n            get_converter(unit1, deg"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "erfa.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "t1, radian),\n            get_converter(unit2, radian),\n        ], dimensionless_unscaled\n    except UnitsError:\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to quantities with angle units\"\n        )\n\n\ndef helper_s2p(f, unit1, unit2, unit3):\n    from astropy.units.si import radian\n\n    try:\n        return [get_converter(unit1, radian), get_converter(unit2, radian), None], unit3\n    except UnitsError:\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to quantities with angle units\"\n        )\n\n\ndef helper_c2s(f, unit1):\n    from astropy.units.si import radian\n\n    return [None], (radian, radian)\n\n\ndef helper_p2s(f, unit1):\n    from astropy.units.si import radian\n\n    return [None], (radian, radian, unit1)\n\n\ndef helper_gc2gd(f, nounit, unit1):\n    from astropy.units.si import m, radian\n\n    if nounit is not None:\n        raise UnitTypeError(\"ellipsoid cannot be a quantity.\")\n    try:\n        return [None, get_converter(unit1, m)], (radian, radian, m, None)\n    except UnitsError:\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to quantities with length units\"\n        )\n\n\ndef helper_gc2gde(f, unit_r, unit_flat, unit_xyz):\n    from astropy.units.si import m, radian\n\n    return [\n        get_converter(unit_r, m),\n        get_converter(_d(unit_flat), dimensionless_unscaled),\n        get_converter(unit_xyz, m),\n    ], (\n        radian,\n        radian,\n        m,\n        None,\n    )\n\n\ndef helper_gd2gc(f, nounit, unit1, unit2, unit3):\n    from astropy.units.si import m, radian\n\n    if nounit is not None:\n        raise UnitTypeError(\"ellipsoid cannot be a quantity.\")\n    try:\n        return [\n            None,\n            get_converter(unit1, radian),\n            get_converter(unit2, radian),\n            get_converter(unit3, m),\n        ], (m, None)\n    except UnitsError:\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to lon, lat \"\n           "}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "formats.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates/angles", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "urangle, the result is (hour, minute, second)\n    \"\"\"\n    sign = np.copysign(1.0, a)\n    # assuming a in degree, these are (degree fraction, degree)\n    (df, d) = np.modf(np.fabs(a))\n\n    # assuming a in degree, these are (arcminute fraction, arcminute)\n    (mf, m) = np.modf(df * 60.0)\n    s = mf * 60.0\n\n    return np.floor(sign * d), sign * np.floor(m), sign * s\n\n\ndef _decimal_to_sexagesimal_string(\n    angle, precision=None, pad=False, sep=(\":\",), fields=3\n):\n    \"\"\"\n    Given a floating point angle, convert it to string\n    \"\"\"\n    values = _decimal_to_sexagesimal(angle)\n    # Check to see if values[0] is negative, using np.copysign to handle -0\n    sign = np.copysign(1.0, values[0])\n    # If the coordinates are negative, we need to take the absolute values.\n    # We use np.abs because abs(-0) is -0\n    # TODO: Is this true? (MHvK, 2018-02-01: not on my system)\n    values = [np.abs(value) for value in values]\n\n    if pad:\n        pad = 3 if sign == -1 else 2\n    else:\n        pad = 0\n\n    if not isinstance(sep, tuple):\n        sep = tuple(sep)\n\n    if fields < 1 or fields > 3:\n        raise ValueError(\"fields must be 1, 2, or 3\")\n\n    if not sep:  # empty string, False, or None, etc.\n        sep = (\"\", \"\", \"\")\n    elif len(sep) == 1:\n        if fields == 3:\n            sep = sep + (sep[0], \"\")\n        elif fields == 2:\n            sep = sep + (\"\", \"\")\n        else:\n            sep = (\"\", \"\", \"\")\n    elif len(sep) == 2:\n        sep = sep + (\"\",)\n    elif len(sep) != 3:\n        raise ValueError(\n            \"Invalid separator specification for converting angle to string.\"\n        )\n\n    # Simplify the expression based on the requested precision.  For\n    # example, if the seconds will round up to 60, we should convert\n    # it to 0 and carry upwards.  If the field is hidden (by the\n    # fields kwarg) we round up around the middle, 30.0.\n    rounding_thresh = 60.0 - (10.0 ** -(8 if precision is None else precision))\n\n    if fields == 3 and values[2] >= rounding_thr"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "funcs.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ovided for users if they really\n        want to use it, but it is recommended that you use the\n        `astropy.coordinates` coordinate systems.\n\n    Parameters\n    ----------\n    r : scalar, array-like, or `~astropy.units.Quantity`\n        The radial coordinate (in the same units as the inputs).\n    lat : scalar, array-like, or `~astropy.units.Quantity` ['angle']\n        The latitude (in radians if array or scalar)\n    lon : scalar, array-like, or `~astropy.units.Quantity` ['angle']\n        The longitude (in radians if array or scalar)\n\n    Returns\n    -------\n    x : float or array\n        The first cartesian coordinate.\n    y : float or array\n        The second cartesian coordinate.\n    z : float or array\n        The third cartesian coordinate.\n\n    \"\"\"\n    if not hasattr(r, \"unit\"):\n        r = r * u.dimensionless_unscaled\n    if not hasattr(lat, \"unit\"):\n        lat = lat * u.radian\n    if not hasattr(lon, \"unit\"):\n        lon = lon * u.radian\n\n    sph = SphericalRepresentation(distance=r, lat=lat, lon=lon)\n    cart = sph.represent_as(CartesianRepresentation)\n\n    return cart.x, cart.y, cart.z\n\n\ndef get_sun(time):\n    \"\"\"\n    Determines the location of the sun at a given time (or times, if the input\n    is an array `~astropy.time.Time` object), in geocentric coordinates.\n\n    Parameters\n    ----------\n    time : `~astropy.time.Time`\n        The time(s) at which to compute the location of the sun.\n\n    Returns\n    -------\n    newsc : `~astropy.coordinates.SkyCoord`\n        The location of the sun as a `~astropy.coordinates.SkyCoord` in the\n        `~astropy.coordinates.GCRS` frame.\n\n    Notes\n    -----\n    The algorithm for determining the sun/earth relative position is based\n    on the simplified version of VSOP2000 that is part of ERFA. Compared to\n    JPL's ephemeris, it should be good to about 4 km (in the Sun-Earth\n    vector) from 1900-2100 C.E., 8 km for the 1800-2200 span, and perhaps\n    250 km over the 1000-3000.\n\n    \"\"\"\n    earth_pv_helio, earth_pv_b"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "erfa.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n\n    return [\n        get_converter(unit_rc, radian),\n        get_converter(unit_dc, radian),\n        get_converter(unit_pr, radian / year),\n        get_converter(unit_pd, radian / year),\n        get_converter(unit_px, arcsec),\n        get_converter(unit_rv, km / s),\n        get_converter(unit_astrom, astrom_unit()),\n    ], (radian, radian)\n\n\ndef helper_atciqn(\n    f, unit_rc, unit_dc, unit_pr, unit_pd, unit_px, unit_rv, unit_astrom, unit_b\n):\n    from astropy.units.si import arcsec, km, radian, s, year\n\n    return [\n        get_converter(unit_rc, radian),\n        get_converter(unit_dc, radian),\n        get_converter(unit_pr, radian / year),\n        get_converter(unit_pd, radian / year),\n        get_converter(unit_px, arcsec),\n        get_converter(unit_rv, km / s),\n        get_converter(unit_astrom, astrom_unit()),\n        get_converter(unit_b, ldbody_unit()),\n    ], (radian, radian)\n\n\ndef helper_atciqz_aticq(f, unit_rc, unit_dc, unit_astrom):\n    from astropy.units.si import radian\n\n    return [\n        get_converter(unit_rc, radian),\n        get_converter(unit_dc, radian),\n        get_converter(unit_astrom, astrom_unit()),\n    ], (radian, radian)\n\n\ndef helper_aticqn(f, unit_rc, unit_dc, unit_astrom, unit_b):\n    from astropy.units.si import radian\n\n    return [\n        get_converter(unit_rc, radian),\n        get_converter(unit_dc, radian),\n        get_converter(unit_astrom, astrom_unit()),\n        get_converter(unit_b, ldbody_unit()),\n    ], (radian, radian)\n\n\ndef helper_atioq(f, unit_rc, unit_dc, unit_astrom):\n    from astropy.units.si import radian\n\n    return [\n        get_converter(unit_rc, radian),\n        get_converter(unit_dc, radian),\n        get_converter(unit_astrom, astrom_unit()),\n    ], (radian,) * 5\n\n\ndef helper_atoiq(f, unit_type, unit_ri, unit_di, unit_astrom):\n    from astropy.units.si import radian\n\n    if unit_type is not None:\n        raise UnitTypeError(\"argument 'type' should not have a unit\")\n\n    return [\n        None,\n        get_convert"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "erfa.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " get_converter(unit_yp, radian),\n            get_converter(unit_phpa, hPa),\n            get_converter(unit_tc, deg_C),\n            get_converter(_d(unit_rh), one),\n            get_converter(unit_wl, micron),\n        ], (astrom_unit(), radian, None)\n\n\ndef helper_apio(\n    f,\n    unit_sp,\n    unit_theta,\n    unit_elong,\n    unit_phi,\n    unit_hm,\n    unit_xp,\n    unit_yp,\n    unit_refa,\n    unit_refb,\n):\n    from astropy.units.si import m, radian\n\n    return [\n        get_converter(unit_sp, radian),\n        get_converter(unit_theta, radian),\n        get_converter(unit_elong, radian),\n        get_converter(unit_phi, radian),\n        get_converter(unit_hm, m),\n        get_converter(unit_xp, radian),\n        get_converter(unit_yp, radian),\n        get_converter(unit_refa, radian),\n        get_converter(unit_refb, radian),\n    ], astrom_unit()\n\n\ndef helper_atciq(f, unit_rc, unit_dc, unit_pr, unit_pd, unit_px, unit_rv, unit_astrom):\n    from astropy.units.si import arcsec, km, radian, s, year\n\n    return [\n        get_converter(unit_rc, radian),\n        get_converter(unit_dc, radian),\n        get_converter(unit_pr, radian / year),\n        get_converter(unit_pd, radian / year),\n        get_converter(unit_px, arcsec),\n        get_converter(unit_rv, km / s),\n        get_converter(unit_astrom, astrom_unit()),\n    ], (radian, radian)\n\n\ndef helper_atciqn(\n    f, unit_rc, unit_dc, unit_pr, unit_pd, unit_px, unit_rv, unit_astrom, unit_b\n):\n    from astropy.units.si import arcsec, km, radian, s, year\n\n    return [\n        get_converter(unit_rc, radian),\n        get_converter(unit_dc, radian),\n        get_converter(unit_pr, radian / year),\n        get_converter(unit_pd, radian / year),\n        get_converter(unit_px, arcsec),\n        get_converter(unit_rv, km / s),\n        get_converter(unit_astrom, astrom_unit()),\n        get_converter(unit_b, ldbody_unit()),\n    ], (radian, radian)\n\n\ndef helper_atciqz_aticq(f, unit_rc, unit_dc, unit_astrom):\n    from astropy.units.si import radian\n"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "erfa.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n    return [\n        get_converter(unit_rc, radian),\n        get_converter(unit_dc, radian),\n        get_converter(unit_astrom, astrom_unit()),\n    ], (radian, radian)\n\n\ndef helper_aticqn(f, unit_rc, unit_dc, unit_astrom, unit_b):\n    from astropy.units.si import radian\n\n    return [\n        get_converter(unit_rc, radian),\n        get_converter(unit_dc, radian),\n        get_converter(unit_astrom, astrom_unit()),\n        get_converter(unit_b, ldbody_unit()),\n    ], (radian, radian)\n\n\ndef helper_atioq(f, unit_rc, unit_dc, unit_astrom):\n    from astropy.units.si import radian\n\n    return [\n        get_converter(unit_rc, radian),\n        get_converter(unit_dc, radian),\n        get_converter(unit_astrom, astrom_unit()),\n    ], (radian,) * 5\n\n\ndef helper_atoiq(f, unit_type, unit_ri, unit_di, unit_astrom):\n    from astropy.units.si import radian\n\n    if unit_type is not None:\n        raise UnitTypeError(\"argument 'type' should not have a unit\")\n\n    return [\n        None,\n        get_converter(unit_ri, radian),\n        get_converter(unit_di, radian),\n        get_converter(unit_astrom, astrom_unit()),\n    ], (radian, radian)\n\n\ndef get_erfa_helpers():\n    return {\n        erfa_ufunc.apco13: helper_apco13,\n        erfa_ufunc.aper: helper_aper,\n        erfa_ufunc.apio: helper_apio,\n        erfa_ufunc.atciq: helper_atciq,\n        erfa_ufunc.atciqn: helper_atciqn,\n        erfa_ufunc.atciqz: helper_atciqz_aticq,\n        erfa_ufunc.aticq: helper_atciqz_aticq,\n        erfa_ufunc.aticqn: helper_aticqn,\n        erfa_ufunc.atioq: helper_atioq,\n        erfa_ufunc.atoiq: helper_atoiq,\n        erfa_ufunc.c2s: helper_c2s,\n        erfa_ufunc.cpv: helper_invariant,\n        erfa_ufunc.gc2gd: helper_gc2gd,\n        erfa_ufunc.gc2gde: helper_gc2gde,\n        erfa_ufunc.gd2gc: helper_gd2gc,\n        erfa_ufunc.gd2gce: helper_gd2gce,\n        erfa_ufunc.ldn: helper_ldn,\n        erfa_ufunc.p2pv: helper_p2pv,\n        erfa_ufunc.p2s: helper_p2s,\n        erfa_ufunc.pdp: helper_multiplication,\n        e"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "t):\n    if unit is None:\n        return [None], dimensionless_unscaled\n\n    try:\n        return ([get_converter(unit, dimensionless_unscaled)], dimensionless_unscaled)\n    except UnitsError:\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to dimensionless quantities\"\n        )\n\n\ndef helper_dimensionless_to_radian(f, unit):\n    from astropy.units.si import radian\n\n    if unit is None:\n        return [None], radian\n\n    try:\n        return [get_converter(unit, dimensionless_unscaled)], radian\n    except UnitsError:\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to dimensionless quantities\"\n        )\n\n\ndef helper_degree_to_radian(f, unit):\n    from astropy.units.si import degree, radian\n\n    try:\n        return [get_converter(unit, degree)], radian\n    except UnitsError:\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to quantities with angle units\"\n        )\n\n\ndef helper_radian_to_degree(f, unit):\n    from astropy.units.si import degree, radian\n\n    try:\n        return [get_converter(unit, radian)], degree\n    except UnitsError:\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to quantities with angle units\"\n        )\n\n\ndef helper_radian_to_dimensionless(f, unit):\n    from astropy.units.si import radian\n\n    try:\n        return [get_converter(unit, radian)], dimensionless_unscaled\n    except UnitsError:\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to quantities with angle units\"\n        )\n\n\ndef helper_frexp(f, unit):\n    if not unit.is_unity():\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to unscaled dimensionless\"\n            \" quantities\"\n        )\n    return [None], (None, None)\n\n\n# TWO ARGUMENT UFUNC HELPERS\n#\n# The functions below take a two arguments. The output of the helper function\n# should be two values: a tuple of two converters to be used to scale th"}], "retrieved_count": 10, "cost_time": 0.3353874683380127}
{"question": "Where in the logging system are file output handlers assigned character encoding from configuration settings versus platform-preferred encoding?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 17000, "end_line": 19000, "belongs_to": {"file_name": "logger.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "                log_file_path = (\n                        _config.get_config_dir_path(\"astropy\") / \"astropy.log\"\n                    )\n                else:\n                    log_file_path = Path(log_file_path).expanduser()\n\n                encoding = conf.log_file_encoding if conf.log_file_encoding else None\n                fh = logging.FileHandler(log_file_path, encoding=encoding)\n            except OSError as e:\n                warnings.warn(\n                    f\"log file {log_file_path!r} could not be opened for writing:\"\n                    f\" {str(e)}\",\n                    RuntimeWarning,\n                )\n            else:\n                formatter = logging.Formatter(conf.log_file_format)\n                fh.setFormatter(formatter)\n                fh.setLevel(conf.log_file_level)\n                self.addHandler(fh)\n\n        if conf.log_warnings:\n            self.enable_warnings_logging()\n\n        if conf.log_exceptions:\n            self.enable_exception_logging()\n\n\nclass StreamHandler(logging.StreamHandler):\n    \"\"\"\n    A specialized StreamHandler that logs INFO and DEBUG messages to\n    stdout, and all other messages to stderr.  Also provides coloring\n    of the output, if enabled in the parent logger.\n    \"\"\"\n\n    def emit(self, record):\n        \"\"\"\n        The formatter for stderr.\n        \"\"\"\n        if record.levelno <= logging.INFO:\n            stream = sys.stdout\n        else:\n            stream = sys.stderr\n\n        if record.levelno < logging.DEBUG or not _conf.use_color:\n            print(record.levelname, end=\"\", file=stream)\n        else:\n            # Import utils.console only if necessary and at the latest because\n            # the import takes a significant time [#4649]\n            from .utils.console import color_print\n\n            if record.levelno < logging.INFO:\n                color_print(record.levelname, \"magenta\", end=\"\", file=stream)\n            elif record.levelno < logging.WARNING:\n                color_print(record.levelname, \"gr"}, {"start_line": 15000, "end_line": 16028, "belongs_to": {"file_name": "test_logger.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "log\"\n    log_file = local_path.open(\"wb\")\n    log_path = str(local_path.resolve())\n\n    with log.log_to_file(log_path, filter_origin=\"astropy.wcs\"):\n        log.error(\"Error message\")\n        log.warning(\"Warning message\")\n\n    log_file.close()\n\n    log_file = local_path.open(\"rb\")\n    log_entries = log_file.readlines()\n    log_file.close()\n\n    assert len(log_entries) == 0\n\n\n@pytest.mark.parametrize(\"encoding\", [\"\", \"utf-8\", \"cp1252\"])\ndef test_log_to_file_encoding(tmp_path, encoding):\n    local_path = tmp_path / \"test.log\"\n    log_path = str(local_path.resolve())\n\n    orig_encoding = conf.log_file_encoding\n\n    conf.log_file_encoding = encoding\n\n    with log.log_to_file(log_path):\n        for handler in log.handlers:\n            if isinstance(handler, logging.FileHandler):\n                if encoding:\n                    assert handler.stream.encoding == encoding\n                else:\n                    assert handler.stream.encoding == locale.getpreferredencoding()\n\n    conf.log_file_encoding = orig_encoding\n"}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "test_logger.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "path = tmp_path / \"test.log\"\n    log_file = local_path.open(\"wb\")\n    log_path = str(local_path.resolve())\n\n    with log.log_to_file(log_path, filter_level=\"ERROR\"):\n        log.error(\"Error message\")\n        log.warning(\"Warning message\")\n\n    log_file.close()\n\n    log_file = local_path.open(\"rb\")\n    log_entries = log_file.readlines()\n    log_file.close()\n\n    assert len(log_entries) == 1\n    assert eval(log_entries[0].strip())[-2:] == (\"ERROR\", \"Error message\")\n\n\ndef test_log_to_file_origin1(tmp_path):\n    local_path = tmp_path / \"test.log\"\n    log_file = local_path.open(\"wb\")\n    log_path = str(local_path.resolve())\n\n    with log.log_to_file(log_path, filter_origin=\"astropy.tests\"):\n        log.error(\"Error message\")\n        log.warning(\"Warning message\")\n\n    log_file.close()\n\n    log_file = local_path.open(\"rb\")\n    log_entries = log_file.readlines()\n    log_file.close()\n\n    assert len(log_entries) == 2\n\n\ndef test_log_to_file_origin2(tmp_path):\n    local_path = tmp_path / \"test.log\"\n    log_file = local_path.open(\"wb\")\n    log_path = str(local_path.resolve())\n\n    with log.log_to_file(log_path, filter_origin=\"astropy.wcs\"):\n        log.error(\"Error message\")\n        log.warning(\"Warning message\")\n\n    log_file.close()\n\n    log_file = local_path.open(\"rb\")\n    log_entries = log_file.readlines()\n    log_file.close()\n\n    assert len(log_entries) == 0\n\n\n@pytest.mark.parametrize(\"encoding\", [\"\", \"utf-8\", \"cp1252\"])\ndef test_log_to_file_encoding(tmp_path, encoding):\n    local_path = tmp_path / \"test.log\"\n    log_path = str(local_path.resolve())\n\n    orig_encoding = conf.log_file_encoding\n\n    conf.log_file_encoding = encoding\n\n    with log.log_to_file(log_path):\n        for handler in log.handlers:\n            if isinstance(handler, logging.FileHandler):\n                if encoding:\n                    assert handler.stream.encoding == encoding\n                else:\n                    assert handler.stream.encoding == locale.getpreferredencoding()\n\n    conf.log_fi"}, {"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "logger.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ly installed hooks\n        if self.warnings_logging_enabled():\n            self.disable_warnings_logging()\n        if self.exception_logging_enabled():\n            self.disable_exception_logging()\n\n        # Remove all previous handlers\n        for handler in self.handlers[:]:\n            self.removeHandler(handler)\n\n        # Set levels\n        self.setLevel(conf.log_level)\n\n        # Set up the stdout handler\n        sh = StreamHandler()\n        self.addHandler(sh)\n\n        # Set up the main log file handler if requested (but this might fail if\n        # configuration directory or log file is not writeable).\n        if conf.log_to_file:\n            log_file_path = conf.log_file_path\n\n            # \"None\" as a string because it comes from config\n            try:\n                _ASTROPY_TEST_  # noqa: B018\n                testing_mode = True\n            except NameError:\n                testing_mode = False\n\n            try:\n                if log_file_path == \"\" or testing_mode:\n                    log_file_path = (\n                        _config.get_config_dir_path(\"astropy\") / \"astropy.log\"\n                    )\n                else:\n                    log_file_path = Path(log_file_path).expanduser()\n\n                encoding = conf.log_file_encoding if conf.log_file_encoding else None\n                fh = logging.FileHandler(log_file_path, encoding=encoding)\n            except OSError as e:\n                warnings.warn(\n                    f\"log file {log_file_path!r} could not be opened for writing:\"\n                    f\" {str(e)}\",\n                    RuntimeWarning,\n                )\n            else:\n                formatter = logging.Formatter(conf.log_file_format)\n                fh.setFormatter(formatter)\n                fh.setLevel(conf.log_file_level)\n                self.addHandler(fh)\n\n        if conf.log_warnings:\n            self.enable_warnings_logging()\n\n        if conf.log_exceptions:\n            self.enable_exception_logging()\n\n\nclass Stre"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "logger.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rd python interpreter\n            if sys.excepthook != self._excepthook:\n                raise LoggingError(\n                    \"Cannot disable exception logging: \"\n                    \"sys.excepthook was not set by this logger, \"\n                    \"or has been overridden\"\n                )\n            sys.excepthook = self._excepthook_orig\n            self._excepthook_orig = None\n\n    def enable_color(self):\n        \"\"\"\n        Enable colorized output.\n        \"\"\"\n        _conf.use_color = True\n\n    def disable_color(self):\n        \"\"\"\n        Disable colorized output.\n        \"\"\"\n        _conf.use_color = False\n\n    @contextmanager\n    def log_to_file(self, filename, filter_level=None, filter_origin=None):\n        \"\"\"\n        Context manager to temporarily log messages to a file.\n\n        Parameters\n        ----------\n        filename : str\n            The file to log messages to.\n        filter_level : str\n            If set, any log messages less important than ``filter_level`` will\n            not be output to the file. Note that this is in addition to the\n            top-level filtering for the logger, so if the logger has level\n            'INFO', then setting ``filter_level`` to ``INFO`` or ``DEBUG``\n            will have no effect, since these messages are already filtered\n            out.\n        filter_origin : str\n            If set, only log messages with an origin starting with\n            ``filter_origin`` will be output to the file.\n\n        Notes\n        -----\n        By default, the logger already outputs log messages to a file set in\n        the Astropy configuration file. Using this context manager does not\n        stop log messages from being output to that file, nor does it stop log\n        messages from being printed to standard output.\n\n        Examples\n        --------\n        The context manager is used as::\n\n            with logger.log_to_file('myfile.log'):\n                # your code here\n        \"\"\"\n        encoding = conf.log_file_e"}, {"start_line": 5000, "end_line": 7000, "belongs_to": {"file_name": "console.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "(s)``, but use a writer for the locale's preferred encoding in case\n    of a UnicodeEncodeError.  Failing that attempt to write with 'utf-8' or\n    'latin-1'.\n    \"\"\"\n    try:\n        write(s)\n        return write\n    except UnicodeEncodeError:\n        # Let's try the next approach...\n        pass\n\n    enc = locale.getpreferredencoding()\n    try:\n        Writer = codecs.getwriter(enc)\n    except LookupError:\n        Writer = codecs.getwriter(\"utf-8\")\n\n    f = Writer(fileobj)\n    write = f.write\n\n    try:\n        write(s)\n        return write\n    except UnicodeEncodeError:\n        Writer = codecs.getwriter(\"latin-1\")\n        f = Writer(fileobj)\n        write = f.write\n\n    # If this doesn't work let the exception bubble up; I'm out of ideas\n    write(s)\n    return write\n\n\ndef color_print(*args, end=\"\\n\", **kwargs):\n    \"\"\"\n    Prints colors and styles to the terminal uses ANSI escape\n    sequences.\n\n    ::\n\n       color_print('This is the color ', 'default', 'GREEN', 'green')\n\n    Parameters\n    ----------\n    positional args : str\n        The positional arguments come in pairs (*msg*, *color*), where\n        *msg* is the string to display and *color* is the color to\n        display it in.\n\n        *color* is an ANSI terminal color name.  Must be one of:\n        black, red, green, brown, blue, magenta, cyan, lightgrey,\n        default, darkgrey, lightred, lightgreen, yellow, lightblue,\n        lightmagenta, lightcyan, white, or '' (the empty string).\n\n    file : :term:`file-like (writeable)`, optional\n        Where to write to.  Defaults to `sys.stdout`.  If file is not\n        a tty (as determined by calling its `isatty` member, if one\n        exists), no coloring will be included.\n\n    end : str, optional\n        The ending of the message.  Defaults to ``\\\\n``.  The end will\n        be printed after resetting any color or font state.\n    \"\"\"\n    file = kwargs.get(\"file\", sys.stdout)\n\n    write = file.write\n    if isatty(file) and conf.use_color:\n        for i in ra"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "configobj.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/configobj", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "F16: ('utf_16', 'utf_16'),\n    }\n# All legal variants of the BOM codecs.\n# TODO: the list of aliases is not meant to be exhaustive, is there a\n#   better way ?\nBOM_LIST = {\n    'utf_16': 'utf_16',\n    'u16': 'utf_16',\n    'utf16': 'utf_16',\n    'utf-16': 'utf_16',\n    'utf16_be': 'utf16_be',\n    'utf_16_be': 'utf16_be',\n    'utf-16be': 'utf16_be',\n    'utf16_le': 'utf16_le',\n    'utf_16_le': 'utf16_le',\n    'utf-16le': 'utf16_le',\n    'utf_8': 'utf_8',\n    'u8': 'utf_8',\n    'utf': 'utf_8',\n    'utf8': 'utf_8',\n    'utf-8': 'utf_8',\n    }\n\n# Map of encodings to the BOM to write.\nBOM_SET = {\n    'utf_8': BOM_UTF8,\n    'utf_16': BOM_UTF16,\n    'utf16_be': BOM_UTF16_BE,\n    'utf16_le': BOM_UTF16_LE,\n    None: BOM_UTF8\n    }\n\n\ndef match_utf8(encoding):\n    return BOM_LIST.get(encoding.lower()) == 'utf_8'\n\n\n# Quote strings used for writing values\nsquot = \"'%s'\"\ndquot = '\"%s\"'\nnoquot = \"%s\"\nwspace_plus = ' \\r\\n\\v\\t\\'\"'\ntsquot = '\"\"\"%s\"\"\"'\ntdquot = \"'''%s'''\"\n\n# Sentinel for use in getattr calls to replace hasattr\nMISSING = object()\n\n__all__ = (\n    'DEFAULT_INDENT_TYPE',\n    'DEFAULT_INTERPOLATION',\n    'ConfigObjError',\n    'NestingError',\n    'ParseError',\n    'DuplicateError',\n    'ConfigspecError',\n    'ConfigObj',\n    'SimpleVal',\n    'InterpolationError',\n    'InterpolationLoopError',\n    'MissingInterpolationOption',\n    'RepeatSectionError',\n    'ReloadError',\n    'UnreprError',\n    'UnknownType',\n    'flatten_errors',\n    'get_extra_values'\n)\n\nDEFAULT_INTERPOLATION = 'configparser'\nDEFAULT_INDENT_TYPE = '    '\nMAX_INTERPOL_DEPTH = 10\n\nOPTION_DEFAULTS = {\n    'interpolation': True,\n    'raise_errors': False,\n    'list_values': True,\n    'create_empty': False,\n    'file_error': False,\n    'configspec': None,\n    'stringify': True,\n    # option may be set to one of ('', ' ', '\\t')\n    'indent_type': None,\n    'encoding': None,\n    'default_encoding': None,\n    'unrepr': False,\n    'write_empty_values': False,\n}\n\n# this could be replaced if six is used for compatibil"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "logger.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "will\n            not be output to the file. Note that this is in addition to the\n            top-level filtering for the logger, so if the logger has level\n            'INFO', then setting ``filter_level`` to ``INFO`` or ``DEBUG``\n            will have no effect, since these messages are already filtered\n            out.\n        filter_origin : str\n            If set, only log messages with an origin starting with\n            ``filter_origin`` will be output to the file.\n\n        Notes\n        -----\n        By default, the logger already outputs log messages to a file set in\n        the Astropy configuration file. Using this context manager does not\n        stop log messages from being output to that file, nor does it stop log\n        messages from being printed to standard output.\n\n        Examples\n        --------\n        The context manager is used as::\n\n            with logger.log_to_file('myfile.log'):\n                # your code here\n        \"\"\"\n        encoding = conf.log_file_encoding if conf.log_file_encoding else None\n        fh = logging.FileHandler(filename, encoding=encoding)\n        if filter_level is not None:\n            fh.setLevel(filter_level)\n        if filter_origin is not None:\n            fh.addFilter(FilterOrigin(filter_origin))\n        f = logging.Formatter(conf.log_file_format)\n        fh.setFormatter(f)\n        self.addHandler(fh)\n        yield\n        fh.close()\n        self.removeHandler(fh)\n\n    @contextmanager\n    def log_to_list(self, filter_level=None, filter_origin=None):\n        \"\"\"\n        Context manager to temporarily log messages to a list.\n\n        Parameters\n        ----------\n        filename : str\n            The file to log messages to.\n        filter_level : str\n            If set, any log messages less important than ``filter_level`` will\n            not be output to the file. Note that this is in addition to the\n            top-level filtering for the logger, so if the logger has level\n            'INFO', then setting `"}, {"start_line": 44000, "end_line": 46000, "belongs_to": {"file_name": "configobj.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/extern/configobj", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "mment = []\n        self.final_comment = []\n        self.configspec = None\n\n        if self._inspec:\n            self.list_values = False\n\n        # Clear section attributes as well\n        Section._initialise(self)\n\n\n    def __repr__(self):\n        def _getval(key):\n            try:\n                return self[key]\n            except MissingInterpolationOption:\n                return dict.__getitem__(self, key)\n        return ('%s({%s})' % (self.__class__.__name__,\n                ', '.join([('%s: %s' % (repr(key), repr(_getval(key))))\n                for key in (self.scalars + self.sections)])))\n\n\n    def _handle_bom(self, infile):\n        \"\"\"\n        Handle any BOM, and decode if necessary.\n\n        If an encoding is specified, that *must* be used - but the BOM should\n        still be removed (and the BOM attribute set).\n\n        (If the encoding is wrongly specified, then a BOM for an alternative\n        encoding won't be discovered or removed.)\n\n        If an encoding is not specified, UTF8 or UTF16 BOM will be detected and\n        removed. The BOM attribute will be set. UTF16 will be decoded to\n        unicode.\n\n        NOTE: This method must not be called with an empty ``infile``.\n\n        Specifying the *wrong* encoding is likely to cause a\n        ``UnicodeDecodeError``.\n\n        ``infile`` must always be returned as a list of lines, but may be\n        passed in as a single string.\n        \"\"\"\n\n        if ((self.encoding is not None) and\n            (self.encoding.lower() not in BOM_LIST)):\n            # No need to check for a BOM\n            # the encoding specified doesn't have one\n            # just decode\n            return self._decode(infile, self.encoding)\n\n        if isinstance(infile, (list, tuple)):\n            line = infile[0]\n        else:\n            line = infile\n\n        if isinstance(line, str):\n            # it's already decoded and there's no need to do anything\n            # else, just use the _decode utility method to handle\n            #"}, {"start_line": 18000, "end_line": 20000, "belongs_to": {"file_name": "logger.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "amHandler(logging.StreamHandler):\n    \"\"\"\n    A specialized StreamHandler that logs INFO and DEBUG messages to\n    stdout, and all other messages to stderr.  Also provides coloring\n    of the output, if enabled in the parent logger.\n    \"\"\"\n\n    def emit(self, record):\n        \"\"\"\n        The formatter for stderr.\n        \"\"\"\n        if record.levelno <= logging.INFO:\n            stream = sys.stdout\n        else:\n            stream = sys.stderr\n\n        if record.levelno < logging.DEBUG or not _conf.use_color:\n            print(record.levelname, end=\"\", file=stream)\n        else:\n            # Import utils.console only if necessary and at the latest because\n            # the import takes a significant time [#4649]\n            from .utils.console import color_print\n\n            if record.levelno < logging.INFO:\n                color_print(record.levelname, \"magenta\", end=\"\", file=stream)\n            elif record.levelno < logging.WARNING:\n                color_print(record.levelname, \"green\", end=\"\", file=stream)\n            elif record.levelno < logging.ERROR:\n                color_print(record.levelname, \"brown\", end=\"\", file=stream)\n            else:\n                color_print(record.levelname, \"red\", end=\"\", file=stream)\n        # Make lazy interpretation intentional to leave the option to use\n        # special characters without escaping in log messages.\n        if record.args:\n            record.message = f\"{record.msg % record.args} [{record.origin:s}]\"\n        else:\n            record.message = f\"{record.msg} [{record.origin:s}]\"\n        print(\": \" + record.message, file=stream)\n\n\nclass FilterOrigin:\n    \"\"\"A filter for the record origin.\"\"\"\n\n    def __init__(self, origin):\n        self.origin = origin\n\n    def filter(self, record):\n        return record.origin.startswith(self.origin)\n\n\nclass ListHandler(logging.Handler):\n    \"\"\"A handler that can be used to capture the records in a list.\"\"\"\n\n    def __init__(self, filter_level=None, filter_origin=None):\n    "}], "retrieved_count": 10, "cost_time": 0.33532238006591797}
{"question": "What performance overhead does the unit validation decorator in the astropy units module introduce when validating unit equivalence through repeated attribute lookups and method calls in high-frequency function invocations?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "decorators.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n__all__ = [\"quantity_input\"]\n\nimport contextlib\nimport inspect\nimport typing as T\nfrom collections.abc import Sequence\nfrom functools import wraps\nfrom numbers import Number\n\nimport numpy as np\n\nfrom .core import Unit, UnitBase, add_enabled_equivalencies, dimensionless_unscaled\nfrom .errors import UnitsError\nfrom .physical import PhysicalType, get_physical_type\nfrom .quantity import Quantity\n\nNoneType = type(None)\n\n\ndef _get_allowed_units(targets):\n    \"\"\"\n    From a list of target units (either as strings or unit objects) and physical\n    types, return a list of Unit objects.\n    \"\"\"\n    allowed_units = []\n    for target in targets:\n        try:\n            unit = Unit(target)\n        except (TypeError, ValueError):\n            try:\n                unit = get_physical_type(target)._unit\n            except (TypeError, ValueError, KeyError):  # KeyError for Enum\n                raise ValueError(f\"Invalid unit or physical type {target!r}.\") from None\n\n        allowed_units.append(unit)\n\n    return allowed_units\n\n\ndef _validate_arg_value(\n    param_name, func_name, arg, targets, equivalencies, strict_dimensionless=False\n):\n    \"\"\"\n    Validates the object passed in to the wrapped function, ``arg``, with target\n    unit or physical type, ``target``.\n    \"\"\"\n    if len(targets) == 0:\n        return\n\n    allowed_units = _get_allowed_units(targets)\n\n    # If dimensionless is an allowed unit and the argument is unit-less,\n    #   allow numbers or numpy arrays with numeric dtypes\n    if (\n        not strict_dimensionless\n        and not hasattr(arg, \"unit\")\n        and dimensionless_unscaled in allowed_units\n    ):\n        if isinstance(arg, Number):\n            return\n\n        elif isinstance(arg, np.ndarray) and np.issubdtype(arg.dtype, np.number):\n            return\n\n    for allowed_unit in allowed_units:\n        try:\n            if arg.unit.is_equivalent(allowed_unit, equivalencies=equivalencies):\n         "}, {"start_line": 24000, "end_line": 26000, "belongs_to": {"file_name": "test_equivalencies.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " equivalencies=eq)\n    )\n    np.testing.assert_almost_equal(1.0, tb.to_value(u.MJy / u.sr, equivalencies=eq))\n\n\ndef test_equivalency_context():\n    with u.set_enabled_equivalencies(u.dimensionless_angles()):\n        phase = u.Quantity(1.0, u.cycle)\n        assert_allclose(np.exp(1j * phase), 1.0)\n        Omega = u.cycle / (1.0 * u.minute)\n        assert_allclose(np.exp(1j * Omega * 60.0 * u.second), 1.0)\n        # ensure we can turn off equivalencies even within the scope\n        with pytest.raises(u.UnitsError):\n            phase.to(1, equivalencies=None)\n\n        # test the manager also works in the Quantity constructor.\n        q1 = u.Quantity(phase, u.dimensionless_unscaled)\n        assert_allclose(q1.value, u.cycle.to(u.radian))\n\n        # and also if we use a class that happens to have a unit attribute.\n        class MyQuantityLookalike(np.ndarray):\n            pass\n\n        mylookalike = np.array(1.0).view(MyQuantityLookalike)\n        mylookalike.unit = \"cycle\"\n        # test the manager also works in the Quantity constructor.\n        q2 = u.Quantity(mylookalike, u.dimensionless_unscaled)\n        assert_allclose(q2.value, u.cycle.to(u.radian))\n\n    with u.set_enabled_equivalencies(u.spectral()):\n        u.GHz.to(u.cm)\n        eq_on = u.GHz.find_equivalent_units()\n        with pytest.raises(u.UnitsError):\n            u.GHz.to(u.cm, equivalencies=None)\n\n    # without equivalencies, we should find a smaller (sub)set\n    eq_off = u.GHz.find_equivalent_units()\n    assert all(eq in set(eq_on) for eq in eq_off)\n    assert set(eq_off) < set(eq_on)\n\n    # Check the equivalency manager also works in ufunc evaluations,\n    # not just using (wrong) scaling. [#2496]\n    l2v = u.doppler_optical(6000 * u.angstrom)\n    l1 = 6010 * u.angstrom\n    assert l1.to(u.km / u.s, equivalencies=l2v) > 100.0 * u.km / u.s\n    with u.set_enabled_equivalencies(l2v):\n        assert l1 > 100.0 * u.km / u.s\n        assert abs((l1 - 500.0 * u.km / u.s).to(u.angstrom)) < 1.0 * u.km / u.s\n\n\ndef "}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "decorators.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n\n    # check if unit-like\n    try:\n        unit = Unit(target)\n    except (TypeError, ValueError):\n        try:\n            ptype = get_physical_type(target)\n        except (TypeError, ValueError, KeyError):  # KeyError for Enum\n            if isinstance(target, str):\n                raise ValueError(f\"invalid unit or physical type {target!r}.\") from None\n        else:\n            return ptype\n    else:\n        return unit\n\n    # could be a type hint\n    origin = T.get_origin(target)\n    if origin is T.Union:\n        return [_parse_annotation(t) for t in T.get_args(target)]\n    elif origin is not T.Annotated:  # can't be Quantity[]\n        return False\n\n    # parse type hint\n    cls, *annotations = T.get_args(target)\n    if not issubclass(cls, Quantity) or not annotations:\n        return False\n\n    # get unit from type hint\n    unit, *rest = annotations\n    if not isinstance(unit, (UnitBase, PhysicalType)):\n        return False\n\n    return unit\n\n\nclass QuantityInput:\n    @classmethod\n    def as_decorator(cls, func=None, **kwargs):\n        r\"\"\"\n        A decorator for validating the units of arguments to functions.\n\n        Unit specifications can be provided as keyword arguments to the\n        decorator, or by using function annotation syntax. Arguments to the\n        decorator take precedence over any function annotations present.\n\n        A `~astropy.units.UnitsError` will be raised if the unit attribute of\n        the argument is not equivalent to the unit specified to the decorator or\n        in the annotation. If the argument has no unit attribute, i.e. it is not\n        a Quantity object, a `ValueError` will be raised unless the argument is\n        an annotation. This is to allow non Quantity annotations to pass\n        through.\n\n        Where an equivalency is specified in the decorator, the function will be\n        executed with that equivalency in force.\n\n        Notes\n        -----\n        The checking of arguments inside variable arguments to a function "}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "test_quantity_annotations.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "solary_unit):\n    @u.quantity_input(equivalencies=u.mass_energy())\n    def myfunc_args(solarx: solarx_unit, solary: solary_unit):\n        return solarx, solary + (10 * u.J)  # Add an energy to check equiv is working\n\n    solarx, solary = myfunc_args(1 * u.arcsec, 100 * u.gram)\n\n    assert isinstance(solarx, Quantity)\n    assert isinstance(solary, Quantity)\n\n    assert solarx.unit == u.arcsec\n    assert solary.unit == u.gram\n\n\n@pytest.mark.parametrize(\n    \"solarx_unit,solary_unit\", [(u.arcsec, u.deg), (\"angle\", \"angle\")]\n)\ndef test_wrong_unit3(solarx_unit, solary_unit):\n    @u.quantity_input\n    def myfunc_args(solarx: solarx_unit, solary: solary_unit):\n        return solarx, solary\n\n    with pytest.raises(\n        u.UnitsError,\n        match=(\n            \"Argument 'solary' to function 'myfunc_args' must be in units \"\n            f\"convertible to '{str(solary_unit)}'.\"\n        ),\n    ):\n        solarx, solary = myfunc_args(1 * u.arcsec, 100 * u.km)\n\n\n@pytest.mark.parametrize(\n    \"solarx_unit,solary_unit\", [(u.arcsec, u.deg), (\"angle\", \"angle\")]\n)\ndef test_not_quantity3(solarx_unit, solary_unit):\n    @u.quantity_input\n    def myfunc_args(solarx: solarx_unit, solary: solary_unit):\n        return solarx, solary\n\n    with pytest.raises(\n        TypeError,\n        match=(\n            \"Argument 'solary' to function 'myfunc_args' has no 'unit' \"\n            \"attribute. You should pass in an astropy Quantity instead.\"\n        ),\n    ):\n        solarx, solary = myfunc_args(1 * u.arcsec, 100)\n\n\ndef test_decorator_override():\n    @u.quantity_input(solarx=u.arcsec)\n    def myfunc_args(solarx: u.km, solary: u.arcsec):\n        return solarx, solary\n\n    solarx, solary = myfunc_args(1 * u.arcsec, 1 * u.arcsec)\n\n    assert isinstance(solarx, Quantity)\n    assert isinstance(solary, Quantity)\n\n    assert solarx.unit == u.arcsec\n    assert solary.unit == u.arcsec\n\n\n@pytest.mark.parametrize(\n    \"solarx_unit,solary_unit\", [(u.arcsec, u.deg), (\"angle\", \"angle\")]\n)\ndef test_kwargs3(sol"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "decorators.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    def as_decorator(cls, func=None, **kwargs):\n        r\"\"\"\n        A decorator for validating the units of arguments to functions.\n\n        Unit specifications can be provided as keyword arguments to the\n        decorator, or by using function annotation syntax. Arguments to the\n        decorator take precedence over any function annotations present.\n\n        A `~astropy.units.UnitsError` will be raised if the unit attribute of\n        the argument is not equivalent to the unit specified to the decorator or\n        in the annotation. If the argument has no unit attribute, i.e. it is not\n        a Quantity object, a `ValueError` will be raised unless the argument is\n        an annotation. This is to allow non Quantity annotations to pass\n        through.\n\n        Where an equivalency is specified in the decorator, the function will be\n        executed with that equivalency in force.\n\n        Notes\n        -----\n        The checking of arguments inside variable arguments to a function is not\n        supported (i.e. \\*arg or \\**kwargs).\n\n        The original function is accessible by the attributed ``__wrapped__``.\n        See :func:`functools.wraps` for details.\n\n        Examples\n        --------\n        .. code-block:: python\n\n            import astropy.units as u\n            @u.quantity_input(myangle=u.arcsec)\n            def myfunction(myangle):\n                return myangle**2\n\n\n        .. code-block:: python\n\n            import astropy.units as u\n            @u.quantity_input\n            def myfunction(myangle: u.arcsec):\n                return myangle**2\n\n        Or using a unit-aware Quantity annotation.\n\n        .. code-block:: python\n\n            @u.quantity_input\n            def myfunction(myangle: u.Quantity[u.arcsec]):\n                return myangle**2\n\n        Also you can specify a return value annotation, which will\n        cause the function to always return a `~astropy.units.Quantity` in that\n        unit.\n\n        .. code-block:: python\n\n         "}, {"start_line": 28000, "end_line": 30000, "belongs_to": {"file_name": "test_quantity.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "y):\n        _equivalencies = u.spectral()\n\n        def __quantity_view__(self, obj, unit):\n            return obj.view(MySpectral)\n\n        def __quantity_instance__(self, *args, **kwargs):\n            return MySpectral(*args, **kwargs)\n\n    q1 = MySpectral([1000, 2000], unit=u.Hz)\n    q2 = q1.to(u.nm)\n    assert q2.unit == u.nm\n    q3 = q2.to(u.Hz)\n    assert q3.unit == u.Hz\n    assert_allclose(q3.value, q1.value)\n    q4 = MySpectral([1000, 2000], unit=u.nm)\n    q5 = q4.to(u.Hz).to(u.nm)\n    assert q5.unit == u.nm\n    assert_allclose(q4.value, q5.value)\n\n\ndef test_self_equivalency():\n    assert u.deg.is_equivalent(1 * u.radian)\n\n\ndef test_si():\n    q1 = 10.0 * u.m * u.s**2 / (200.0 * u.ms) ** 2  # 250 meters\n    assert q1.si.value == 250\n    assert q1.si.unit == u.m\n\n    q = 10.0 * u.m  # 10 meters\n    assert q.si.value == 10\n    assert q.si.unit == u.m\n\n    q = 10.0 / u.m  # 10 1 / meters\n    assert q.si.value == 10\n    assert q.si.unit == (1 / u.m)\n\n\ndef test_cgs():\n    q1 = 10.0 * u.cm * u.s**2 / (200.0 * u.ms) ** 2  # 250 centimeters\n    assert q1.cgs.value == 250\n    assert q1.cgs.unit == u.cm\n\n    q = 10.0 * u.m  # 10 centimeters\n    assert q.cgs.value == 1000\n    assert q.cgs.unit == u.cm\n\n    q = 10.0 / u.cm  # 10 1 / centimeters\n    assert q.cgs.value == 10\n    assert q.cgs.unit == (1 / u.cm)\n\n    q = 10.0 * u.Pa  # 10 pascals\n    assert q.cgs.value == 100\n    assert q.cgs.unit == u.barye\n\n\nclass TestQuantityComparison:\n    def test_quantity_equality(self):\n        assert u.Quantity(1000, unit=\"m\") == u.Quantity(1, unit=\"km\")\n        assert not (u.Quantity(1, unit=\"m\") == u.Quantity(1, unit=\"km\"))\n        # for ==, !=, return False, True if units do not match\n        assert (u.Quantity(1100, unit=u.m) != u.Quantity(1, unit=u.s)) is True\n        assert (u.Quantity(1100, unit=u.m) == u.Quantity(1, unit=u.s)) is False\n        assert (u.Quantity(0, unit=u.m) == u.Quantity(0, unit=u.s)) is False\n        # But allow comparison with 0, +/-inf if latter unitless\n "}, {"start_line": 25000, "end_line": 27000, "belongs_to": {"file_name": "test_equivalencies.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "e manager also works in the Quantity constructor.\n        q2 = u.Quantity(mylookalike, u.dimensionless_unscaled)\n        assert_allclose(q2.value, u.cycle.to(u.radian))\n\n    with u.set_enabled_equivalencies(u.spectral()):\n        u.GHz.to(u.cm)\n        eq_on = u.GHz.find_equivalent_units()\n        with pytest.raises(u.UnitsError):\n            u.GHz.to(u.cm, equivalencies=None)\n\n    # without equivalencies, we should find a smaller (sub)set\n    eq_off = u.GHz.find_equivalent_units()\n    assert all(eq in set(eq_on) for eq in eq_off)\n    assert set(eq_off) < set(eq_on)\n\n    # Check the equivalency manager also works in ufunc evaluations,\n    # not just using (wrong) scaling. [#2496]\n    l2v = u.doppler_optical(6000 * u.angstrom)\n    l1 = 6010 * u.angstrom\n    assert l1.to(u.km / u.s, equivalencies=l2v) > 100.0 * u.km / u.s\n    with u.set_enabled_equivalencies(l2v):\n        assert l1 > 100.0 * u.km / u.s\n        assert abs((l1 - 500.0 * u.km / u.s).to(u.angstrom)) < 1.0 * u.km / u.s\n\n\ndef test_equivalency_context_manager():\n    base_registry = u.get_current_unit_registry()\n\n    def just_to_from_units(equivalencies):\n        return [(equiv[0], equiv[1]) for equiv in equivalencies]\n\n    tf_dimensionless_angles = just_to_from_units(u.dimensionless_angles())\n    tf_spectral = just_to_from_units(u.spectral())\n\n    # <=1 b/c might have the dimensionless_redshift equivalency enabled.\n    assert len(base_registry.equivalencies) <= 1\n\n    with u.set_enabled_equivalencies(u.dimensionless_angles()):\n        new_registry = u.get_current_unit_registry()\n        assert set(just_to_from_units(new_registry.equivalencies)) == set(\n            tf_dimensionless_angles\n        )\n        assert set(new_registry.all_units) == set(base_registry.all_units)\n        with u.set_enabled_equivalencies(u.spectral()):\n            newer_registry = u.get_current_unit_registry()\n            assert set(just_to_from_units(newer_registry.equivalencies)) == set(\n                tf_spectral\n            )\n   "}, {"start_line": 11000, "end_line": 12517, "belongs_to": {"file_name": "decorators.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "try if we don't have\n                # equivalencies to add. (If we're wrapping a short function,\n                # the time spent duplicating the registry is quite noticeable.)\n                equiv_context = contextlib.nullcontext()\n            # Call the original function with any equivalencies in force.\n            with equiv_context:\n                return_ = wrapped_function(*func_args, **func_kwargs)\n\n            # Return\n            ra = wrapped_signature.return_annotation\n            valid_empty = (inspect.Signature.empty, None, NoneType, T.NoReturn)\n            if ra not in valid_empty:\n                target = (\n                    ra\n                    if T.get_origin(ra) not in (T.Annotated, T.Union)\n                    else _parse_annotation(ra)\n                )\n                if isinstance(target, str) or not isinstance(target, Sequence):\n                    target = [target]\n                valid_targets = [\n                    t for t in target if isinstance(t, (str, UnitBase, PhysicalType))\n                ]\n                _validate_arg_value(\n                    \"return\",\n                    wrapped_function.__name__,\n                    return_,\n                    valid_targets,\n                    self.equivalencies,\n                    self.strict_dimensionless,\n                )\n                if len(valid_targets) > 0:\n                    return_ <<= valid_targets[0]\n            return return_\n\n        return wrapper\n\n\nquantity_input = QuantityInput.as_decorator\n"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "test_units.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ssert unit.scale == 1e22\n\n\ndef test_register():\n    foo = u.def_unit(\"foo\", u.m**3, namespace=locals())\n    assert \"foo\" in locals()\n    with u.add_enabled_units(foo):\n        assert \"foo\" in u.get_current_unit_registry().registry\n    assert \"foo\" not in u.get_current_unit_registry().registry\n\n\ndef test_in_units_deprecation():\n    with pytest.warns(AstropyDeprecationWarning, match=r\"Use to\\(\\) instead\\.$\"):\n        assert (u.m / u.s).in_units(u.cm / u.s) == 100\n\n\ndef test_null_unit():\n    assert (u.m / u.m) == u.Unit(1)\n\n\ndef test_unrecognized_equivalency():\n    assert u.m.is_equivalent(\"foo\") is False\n    assert u.m.is_equivalent(\"pc\") is True\n\n\ndef test_convertible_exception():\n    with pytest.raises(u.UnitsError, match=r\"length.+ are not convertible\"):\n        u.AA.to(u.h * u.s**2)\n\n\ndef test_convertible_exception2():\n    with pytest.raises(u.UnitsError, match=r\"length. and .+time.+ are not convertible\"):\n        u.m.to(u.s)\n\n\ndef test_invalid_type():\n    class A:\n        pass\n\n    with pytest.raises(TypeError):\n        u.Unit(A())\n\n\ndef test_steradian():\n    \"\"\"\n    Issue #599\n    \"\"\"\n    assert u.sr.is_equivalent(u.rad * u.rad)\n\n    results = u.sr.compose(units=u.cgs.bases)\n    assert results[0].bases[0] is u.rad\n\n    results = u.sr.compose(units=u.cgs.__dict__)\n    assert results[0].bases[0] is u.sr\n\n\ndef test_decompose_bases():\n    \"\"\"\n    From issue #576\n    \"\"\"\n\n    from astropy.constants import e\n    from astropy.units import cgs\n\n    d = e.esu.unit.decompose(bases=cgs.bases)\n    assert d._bases == [u.cm, u.g, u.s]\n    assert d._powers == [Fraction(3, 2), 0.5, -1]\n    assert d._scale == 1.0\n\n\ndef test_complex_compose():\n    complex = u.cd * u.sr * u.Wb\n    composed = complex.compose()\n\n    assert set(composed[0]._bases) == {u.lm, u.Wb}\n\n\ndef test_equiv_compose():\n    composed = u.m.compose(equivalencies=u.spectral())\n    assert any([u.Hz] == x.bases for x in composed)\n\n\ndef test_empty_compose():\n    with pytest.raises(u.UnitsError):\n        u.m.compose(uni"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "decorators.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "target unit\n                if arg is None and param.default is None:\n                    continue\n\n                # Here, we check whether multiple target unit/physical type's\n                #   were specified in the decorator/annotation, or whether a\n                #   single string (unit or physical type) or a Unit object was\n                #   specified\n                if isinstance(targets, str) or not isinstance(targets, Sequence):\n                    valid_targets = [targets]\n\n                # Check for None in the supplied list of allowed units and, if\n                #   present and the passed value is also None, ignore.\n                elif None in targets or NoneType in targets:\n                    if arg is None:\n                        continue\n                    else:\n                        valid_targets = [t for t in targets if t is not None]\n\n                else:\n                    valid_targets = targets\n\n                # If we're dealing with an annotation, skip all the targets that\n                #    are not strings or subclasses of Unit. This is to allow\n                #    non unit related annotations to pass through\n                if is_annotation:\n                    valid_targets = [\n                        t\n                        for t in valid_targets\n                        if isinstance(t, (str, UnitBase, PhysicalType))\n                    ]\n\n                # Now we loop over the allowed units/physical types and validate\n                #   the value of the argument:\n                _validate_arg_value(\n                    param.name,\n                    wrapped_function.__name__,\n                    arg,\n                    valid_targets,\n                    self.equivalencies,\n                    self.strict_dimensionless,\n                )\n\n            if self.equivalencies:\n                equiv_context = add_enabled_equivalencies(self.equivalencies)\n            else:\n                # Avoid creating a duplicate regis"}], "retrieved_count": 10, "cost_time": 0.3479282855987549}
{"question": "Where in the data utilities module are the module-level functions responsible for raising the exception that indicates cache corruption during download cache consistency checks?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 70000, "end_line": 72000, "belongs_to": {"file_name": "data.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "nsensitive\n    # Also makes the http:// case-insensitive\n    urlobj = list(urllib.parse.urlsplit(url))\n    urlobj[1] = urlobj[1].lower()\n    if urlobj[0].lower() in [\"http\", \"https\"] and urlobj[1] and urlobj[2] == \"\":\n        urlobj[2] = \"/\"\n    url_c = urllib.parse.urlunsplit(urlobj)\n    return hashlib.md5(url_c.encode(\"utf-8\"), usedforsecurity=False).hexdigest()\n\n\n_NOTHING = MappingProxyType({})\n\n\nclass CacheDamaged(ValueError):\n    \"\"\"Record the URL or file that was a problem.\n    Using clear_download_cache on the .bad_file or .bad_url attribute,\n    whichever is not None, should resolve this particular problem.\n    \"\"\"\n\n    def __init__(self, *args, bad_urls=None, bad_files=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.bad_urls = bad_urls if bad_urls is not None else []\n        self.bad_files = bad_files if bad_files is not None else []\n\n\ndef check_download_cache(pkgname=\"astropy\"):\n    \"\"\"Do a consistency check on the cache.\n\n    .. note::\n\n        Since v5.0, this function no longer returns anything.\n\n    Because the cache is shared by all versions of ``astropy`` in all virtualenvs\n    run by your user, possibly concurrently, it could accumulate problems.\n    This could lead to hard-to-debug problems or wasted space. This function\n    detects a number of incorrect conditions, including nonexistent files that\n    are indexed, files that are indexed but in the wrong place, and, if you\n    request it, files whose content does not match the hash that is indexed.\n\n    This function also returns a list of non-indexed files. A few will be\n    associated with the shelve object; their exact names depend on the backend\n    used but will probably be based on ``urlmap``. The presence of other files\n    probably indicates that something has gone wrong and inaccessible files\n    have accumulated in the cache. These can be removed with\n    :func:`clear_download_cache`, either passing the filename returned here, or\n    with no arguments to empty the "}, {"start_line": 52000, "end_line": 54000, "belongs_to": {"file_name": "test_data.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_check_download_cache_finds_bogus_entries(temp_cache, valid_urls):\n    u, c = next(valid_urls)\n    download_file(u, cache=True)\n    dldir = _get_download_cache_loc()\n    bf = os.path.abspath(os.path.join(dldir, \"bogus\"))\n    with open(bf, \"w\") as f:\n        f.write(\"bogus file that exists\")\n    with pytest.raises(CacheDamaged) as e:\n        check_download_cache()\n    assert bf in e.value.bad_files\n    clear_download_cache()\n\n\n@pytest.mark.filterwarnings(\"ignore:unclosed:ResourceWarning\")\ndef test_check_download_cache_finds_bogus_subentries(temp_cache, valid_urls):\n    u, c = next(valid_urls)\n    f = download_file(u, cache=True)\n    bf = os.path.abspath(os.path.join(os.path.dirname(f), \"bogus\"))\n    with open(bf, \"w\") as f:\n        f.write(\"bogus file that exists\")\n    with pytest.raises(CacheDamaged) as e:\n        check_download_cache()\n    assert bf in e.value.bad_files\n    clear_download_cache()\n\n\n@pytest.mark.filterwarnings(\"ignore:unclosed:ResourceWarning\")\ndef test_check_download_cache_cleanup(temp_cache, valid_urls):\n    u, c = next(valid_urls)\n    fn = download_file(u, cache=True)\n    dldir = _get_download_cache_loc()\n\n    bf1 = os.path.abspath(os.path.join(dldir, \"bogus1\"))\n    with open(bf1, \"w\") as f:\n        f.write(\"bogus file that exists\")\n\n    bf2 = os.path.abspath(os.path.join(os.path.dirname(fn), \"bogus2\"))\n    with open(bf2, \"w\") as f:\n        f.write(\"other bogus file that exists\")\n\n    bf3 = os.path.abspath(os.path.join(dldir, \"contents\"))\n    with open(bf3, \"w\") as f:\n        f.write(\"awkwardly-named bogus file that exists\")\n\n    u2, c2 = next(valid_urls)\n    f2 = download_file(u, cache=True)\n    os.unlink(f2)\n    bf4 = os.path.dirname(f2)\n\n    with pytest.raises(CacheDamaged) as e:\n        check_download_cache()\n    assert set(e.value.bad_files) == {bf1, bf2, bf3, bf4}\n    for bf in e.value.bad_files:\n        clear_download_cache(bf)\n    # download cache will be checked on exit\n\n\n@pytest.mark.filterwarnings(\"ignore:unclosed:ResourceWarning\")\ndef"}, {"start_line": 51000, "end_line": 53000, "belongs_to": {"file_name": "test_data.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ile(u, sources=[])\n\n\n@pytest.mark.filterwarnings(\"ignore:unclosed:ResourceWarning\")\ndef test_download_file_local_directory(tmp_path):\n    \"\"\"Make sure we get a URLError rather than OSError even if it's a\n    local directory.\"\"\"\n    with pytest.raises(urllib.request.URLError):\n        download_file(url_to(tmp_path))\n\n\n@pytest.mark.filterwarnings(\"ignore:unclosed:ResourceWarning\")\ndef test_download_file_schedules_deletion(valid_urls):\n    u, c = next(valid_urls)\n    f = download_file(u)\n    assert f in _tempfilestodel\n    # how to test deletion actually occurs?\n\n\n@pytest.mark.filterwarnings(\"ignore:unclosed:ResourceWarning\")\ndef test_clear_download_cache_refuses_to_delete_outside_the_cache(tmp_path):\n    fn = str(tmp_path / \"file\")\n    with open(fn, \"w\") as f:\n        f.write(\"content\")\n    assert os.path.exists(fn)\n    with pytest.raises(RuntimeError):\n        clear_download_cache(fn)\n    assert os.path.exists(fn)\n\n\n@pytest.mark.filterwarnings(\"ignore:unclosed:ResourceWarning\")\ndef test_check_download_cache_finds_bogus_entries(temp_cache, valid_urls):\n    u, c = next(valid_urls)\n    download_file(u, cache=True)\n    dldir = _get_download_cache_loc()\n    bf = os.path.abspath(os.path.join(dldir, \"bogus\"))\n    with open(bf, \"w\") as f:\n        f.write(\"bogus file that exists\")\n    with pytest.raises(CacheDamaged) as e:\n        check_download_cache()\n    assert bf in e.value.bad_files\n    clear_download_cache()\n\n\n@pytest.mark.filterwarnings(\"ignore:unclosed:ResourceWarning\")\ndef test_check_download_cache_finds_bogus_subentries(temp_cache, valid_urls):\n    u, c = next(valid_urls)\n    f = download_file(u, cache=True)\n    bf = os.path.abspath(os.path.join(os.path.dirname(f), \"bogus\"))\n    with open(bf, \"w\") as f:\n        f.write(\"bogus file that exists\")\n    with pytest.raises(CacheDamaged) as e:\n        check_download_cache()\n    assert bf in e.value.bad_files\n    clear_download_cache()\n\n\n@pytest.mark.filterwarnings(\"ignore:unclosed:ResourceWarning\")\ndef test_check_download_"}, {"start_line": 69000, "end_line": 71000, "belongs_to": {"file_name": "data.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "nload cache. i.e. for\n        ``pkgname='astropy'`` the default cache location is\n        ``~/.astropy/cache``.\n\n    Returns\n    -------\n    datadir : str\n        The path to the data cache directory.\n    \"\"\"\n    try:\n        datadir = astropy.config.paths.get_cache_dir_path(pkgname) / \"download\" / \"url\"\n\n        if not datadir.exists():\n            try:\n                datadir.mkdir(parents=True)\n            except OSError:\n                if not datadir.exists():\n                    raise\n        elif not datadir.is_dir():\n            raise OSError(f\"Data cache directory {datadir} is not a directory\")\n\n        return datadir\n    except OSError as e:\n        msg = \"Remote data cache could not be accessed due to \"\n        estr = \"\" if len(e.args) < 1 else (\": \" + str(e))\n        warn(CacheMissingWarning(msg + e.__class__.__name__ + estr))\n        raise\n\n\ndef _url_to_dirname(url):\n    if not _is_url(url):\n        raise ValueError(f\"Malformed URL: '{url}'\")\n    # Make domain names case-insensitive\n    # Also makes the http:// case-insensitive\n    urlobj = list(urllib.parse.urlsplit(url))\n    urlobj[1] = urlobj[1].lower()\n    if urlobj[0].lower() in [\"http\", \"https\"] and urlobj[1] and urlobj[2] == \"\":\n        urlobj[2] = \"/\"\n    url_c = urllib.parse.urlunsplit(urlobj)\n    return hashlib.md5(url_c.encode(\"utf-8\"), usedforsecurity=False).hexdigest()\n\n\n_NOTHING = MappingProxyType({})\n\n\nclass CacheDamaged(ValueError):\n    \"\"\"Record the URL or file that was a problem.\n    Using clear_download_cache on the .bad_file or .bad_url attribute,\n    whichever is not None, should resolve this particular problem.\n    \"\"\"\n\n    def __init__(self, *args, bad_urls=None, bad_files=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.bad_urls = bad_urls if bad_urls is not None else []\n        self.bad_files = bad_files if bad_files is not None else []\n\n\ndef check_download_cache(pkgname=\"astropy\"):\n    \"\"\"Do a consistency check on the cache.\n\n    .. note::\n\n        Since "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "data.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n\"\"\"Functions for accessing, downloading, and caching data files.\"\"\"\n\nimport atexit\nimport contextlib\nimport errno\nimport fnmatch\nimport ftplib\nimport functools\nimport hashlib\nimport io\nimport os\nimport re\nimport shutil\nimport sys\nimport urllib.error\nimport urllib.parse\nimport urllib.request\nimport zipfile\nfrom importlib import import_module\nfrom tempfile import NamedTemporaryFile, TemporaryDirectory, gettempdir\nfrom types import MappingProxyType\nfrom warnings import warn\n\nimport astropy_iers_data\n\nimport astropy.config.paths\nfrom astropy import config as _config\nfrom astropy.utils.compat.optional_deps import (\n    HAS_BZ2,\n    HAS_CERTIFI,\n    HAS_FSSPEC,\n    HAS_LZMA,\n    HAS_UNCOMPRESSPY,\n)\nfrom astropy.utils.exceptions import AstropyDeprecationWarning, AstropyWarning\nfrom astropy.utils.introspection import find_current_module\n\n# Order here determines order in the autosummary\n__all__ = [\n    \"CacheDamaged\",\n    \"CacheMissingWarning\",\n    \"Conf\",\n    \"cache_contents\",\n    \"cache_total_size\",\n    \"check_download_cache\",\n    \"check_free_space_in_dir\",\n    \"clear_download_cache\",\n    \"compute_hash\",\n    \"conf\",\n    \"download_file\",\n    \"download_files_in_parallel\",\n    \"export_download_cache\",\n    \"get_cached_urls\",\n    \"get_file_contents\",\n    \"get_free_space_in_dir\",\n    \"get_pkg_data_contents\",\n    \"get_pkg_data_filename\",\n    \"get_pkg_data_filenames\",\n    \"get_pkg_data_fileobj\",\n    \"get_pkg_data_fileobjs\",\n    \"get_pkg_data_path\",\n    \"get_readable_fileobj\",\n    \"import_download_cache\",\n    \"import_file_to_cache\",\n    \"is_url\",\n    \"is_url_in_cache\",\n]\n\n_dataurls_to_alias = {}\n\n\n_IERS_DATA_REDIRECTS = {\n    \"Leap_Second.dat\": (\n        \"IERS_LEAP_SECOND_FILE\",\n        astropy_iers_data.IERS_LEAP_SECOND_FILE,\n    ),\n    \"ReadMe.finals2000A\": (\"IERS_A_README\", astropy_iers_data.IERS_A_README),\n    \"ReadMe.eopc04\": (\"IERS_B_README\", astropy_iers_data.IERS_B_README),\n    \"eopc04.1962-now\": (\"IERS_B_FILE"}, {"start_line": 58000, "end_line": 60000, "belongs_to": {"file_name": "data.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ad-only ({e}), unable to import \"\n                f\"downloaded file, providing data in temporary file {f_name} \"\n                \"instead.\"\n            )\n        # FIXME: other kinds of cache problem can occur?\n\n    if missing_cache:\n        warn(CacheMissingWarning(missing_cache, f_name))\n    if conf.delete_temporary_downloads_at_exit:\n        _tempfilestodel.append(f_name)\n    return os.path.abspath(f_name)\n\n\ndef is_url_in_cache(url_key, pkgname=\"astropy\"):\n    \"\"\"Check if a download for ``url_key`` is in the cache.\n\n    The provided ``url_key`` will be the name used in the cache. The contents\n    may have been downloaded from this URL or from a mirror or they may have\n    been provided by the user. See `~download_file` for details.\n\n    Parameters\n    ----------\n    url_key : str\n        The URL retrieved\n    pkgname : `str`, optional\n        The package name to use to locate the download cache. i.e. for\n        ``pkgname='astropy'`` the default cache location is\n        ``~/.astropy/cache``.\n\n\n    Returns\n    -------\n    in_cache : bool\n        `True` if a download for ``url_key`` is in the cache, `False` if not\n        or if the cache does not exist at all.\n\n    See Also\n    --------\n    cache_contents : obtain a dictionary listing everything in the cache\n    \"\"\"\n    try:\n        dldir = _get_download_cache_loc(pkgname)\n    except OSError:\n        return False\n    filename = os.path.join(dldir, _url_to_dirname(url_key), \"contents\")\n    return os.path.exists(filename)\n\n\ndef cache_total_size(pkgname=\"astropy\"):\n    \"\"\"Return the total size in bytes of all files in the cache.\"\"\"\n    size = 0\n    dldir = _get_download_cache_loc(pkgname=pkgname)\n    for root, _, files in os.walk(dldir):\n        size += sum(os.path.getsize(os.path.join(root, name)) for name in files)\n    return size\n\n\ndef _do_download_files_in_parallel(kwargs):\n    with astropy.config.paths.set_temp_config(kwargs.pop(\"temp_config\")):\n        with astropy.config.paths.set_temp_cache(kwargs.pop(\"temp_c"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "data.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ssingWarning\",\n    \"Conf\",\n    \"cache_contents\",\n    \"cache_total_size\",\n    \"check_download_cache\",\n    \"check_free_space_in_dir\",\n    \"clear_download_cache\",\n    \"compute_hash\",\n    \"conf\",\n    \"download_file\",\n    \"download_files_in_parallel\",\n    \"export_download_cache\",\n    \"get_cached_urls\",\n    \"get_file_contents\",\n    \"get_free_space_in_dir\",\n    \"get_pkg_data_contents\",\n    \"get_pkg_data_filename\",\n    \"get_pkg_data_filenames\",\n    \"get_pkg_data_fileobj\",\n    \"get_pkg_data_fileobjs\",\n    \"get_pkg_data_path\",\n    \"get_readable_fileobj\",\n    \"import_download_cache\",\n    \"import_file_to_cache\",\n    \"is_url\",\n    \"is_url_in_cache\",\n]\n\n_dataurls_to_alias = {}\n\n\n_IERS_DATA_REDIRECTS = {\n    \"Leap_Second.dat\": (\n        \"IERS_LEAP_SECOND_FILE\",\n        astropy_iers_data.IERS_LEAP_SECOND_FILE,\n    ),\n    \"ReadMe.finals2000A\": (\"IERS_A_README\", astropy_iers_data.IERS_A_README),\n    \"ReadMe.eopc04\": (\"IERS_B_README\", astropy_iers_data.IERS_B_README),\n    \"eopc04.1962-now\": (\"IERS_B_FILE\", astropy_iers_data.IERS_B_FILE),\n}\n\n\nclass _NonClosingBufferedReader(io.BufferedReader):\n    def __del__(self):\n        try:\n            # NOTE: self.raw will not be closed, but left in the state\n            # it was in at detactment\n            self.detach()\n        except Exception:\n            pass\n\n\nclass _NonClosingTextIOWrapper(io.TextIOWrapper):\n    def __del__(self):\n        try:\n            # NOTE: self.stream will not be closed, but left in the state\n            # it was in at detactment\n            self.detach()\n        except Exception:\n            pass\n\n\nclass Conf(_config.ConfigNamespace):\n    \"\"\"\n    Configuration parameters for `astropy.utils.data`.\n    \"\"\"\n\n    dataurl = _config.ConfigItem(\n        \"http://data.astropy.org/\", \"Primary URL for astropy remote data site.\"\n    )\n    dataurl_mirror = _config.ConfigItem(\n        \"http://www.astropy.org/astropy-data/\",\n        \"Mirror URL for astropy remote data site.\",\n    )\n    default_http_user_agent = _config.ConfigItem"}, {"start_line": 53000, "end_line": 55000, "belongs_to": {"file_name": "test_data.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "cache_cleanup(temp_cache, valid_urls):\n    u, c = next(valid_urls)\n    fn = download_file(u, cache=True)\n    dldir = _get_download_cache_loc()\n\n    bf1 = os.path.abspath(os.path.join(dldir, \"bogus1\"))\n    with open(bf1, \"w\") as f:\n        f.write(\"bogus file that exists\")\n\n    bf2 = os.path.abspath(os.path.join(os.path.dirname(fn), \"bogus2\"))\n    with open(bf2, \"w\") as f:\n        f.write(\"other bogus file that exists\")\n\n    bf3 = os.path.abspath(os.path.join(dldir, \"contents\"))\n    with open(bf3, \"w\") as f:\n        f.write(\"awkwardly-named bogus file that exists\")\n\n    u2, c2 = next(valid_urls)\n    f2 = download_file(u, cache=True)\n    os.unlink(f2)\n    bf4 = os.path.dirname(f2)\n\n    with pytest.raises(CacheDamaged) as e:\n        check_download_cache()\n    assert set(e.value.bad_files) == {bf1, bf2, bf3, bf4}\n    for bf in e.value.bad_files:\n        clear_download_cache(bf)\n    # download cache will be checked on exit\n\n\n@pytest.mark.filterwarnings(\"ignore:unclosed:ResourceWarning\")\ndef test_download_cache_update_doesnt_damage_cache(temp_cache, valid_urls):\n    u, _ = next(valid_urls)\n    download_file(u, cache=True)\n    download_file(u, cache=\"update\")\n\n\n@pytest.mark.filterwarnings(\"ignore:unclosed:ResourceWarning\")\ndef test_cache_dir_is_actually_a_file(tmp_path, valid_urls):\n    \"\"\"Ensure that bogus cache settings are handled sensibly.\n\n    Because the user can specify the cache location in a config file, and\n    because they might try to deduce the location by looking around at what's\n    in their directory tree, and because the cache directory is actual several\n    tree levels down from the directory set in the config file, it's important\n    to check what happens if each of the steps in the path is wrong somehow.\n    \"\"\"\n\n    def check_quietly_ignores_bogus_cache():\n        \"\"\"We want a broken cache to produce a warning but then astropy should\n        act like there isn't a cache.\n        \"\"\"\n        with pytest.warns(CacheMissingWarning):\n            assert not"}, {"start_line": 72000, "end_line": 74000, "belongs_to": {"file_name": "data.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "entire cache and return it to a\n    reasonable, if empty, state.\n\n    Parameters\n    ----------\n    pkgname : str, optional\n        The package name to use to locate the download cache, i.e., for\n        ``pkgname='astropy'`` the default cache location is\n        ``~/.astropy/cache``.\n\n    Raises\n    ------\n    `~astropy.utils.data.CacheDamaged`\n        To indicate a problem with the cache contents; the exception contains\n        a ``.bad_files`` attribute containing a set of filenames to allow the\n        user to use :func:`clear_download_cache` to remove the offending items.\n    OSError, RuntimeError\n        To indicate some problem with the cache structure. This may need a full\n        :func:`clear_download_cache` to resolve, or may indicate some kind of\n        misconfiguration.\n    \"\"\"\n    bad_files = set()\n    messages = set()\n    dldir = _get_download_cache_loc(pkgname=pkgname)\n    with os.scandir(dldir) as it:\n        for entry in it:\n            f = os.path.abspath(os.path.join(dldir, entry.name))\n            if entry.name.startswith(\"rmtree-\"):\n                if f not in _tempfilestodel:\n                    bad_files.add(f)\n                    messages.add(f\"Cache entry {entry.name} not scheduled for deletion\")\n            elif entry.is_dir():\n                for sf in os.listdir(f):\n                    if sf in [\"url\", \"contents\"]:\n                        continue\n                    sf = os.path.join(f, sf)\n                    bad_files.add(sf)\n                    messages.add(f\"Unexpected file f{sf}\")\n                urlf = os.path.join(f, \"url\")\n                url = None\n                if not os.path.isfile(urlf):\n                    bad_files.add(urlf)\n                    messages.add(f\"Problem with URL file f{urlf}\")\n                else:\n                    url = get_file_contents(urlf, encoding=\"utf-8\")\n                    if not _is_url(url):\n                        bad_files.add(f)\n                        messages.add(f\"Malformed URL: {url}\")\n"}, {"start_line": 71000, "end_line": 73000, "belongs_to": {"file_name": "data.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "v5.0, this function no longer returns anything.\n\n    Because the cache is shared by all versions of ``astropy`` in all virtualenvs\n    run by your user, possibly concurrently, it could accumulate problems.\n    This could lead to hard-to-debug problems or wasted space. This function\n    detects a number of incorrect conditions, including nonexistent files that\n    are indexed, files that are indexed but in the wrong place, and, if you\n    request it, files whose content does not match the hash that is indexed.\n\n    This function also returns a list of non-indexed files. A few will be\n    associated with the shelve object; their exact names depend on the backend\n    used but will probably be based on ``urlmap``. The presence of other files\n    probably indicates that something has gone wrong and inaccessible files\n    have accumulated in the cache. These can be removed with\n    :func:`clear_download_cache`, either passing the filename returned here, or\n    with no arguments to empty the entire cache and return it to a\n    reasonable, if empty, state.\n\n    Parameters\n    ----------\n    pkgname : str, optional\n        The package name to use to locate the download cache, i.e., for\n        ``pkgname='astropy'`` the default cache location is\n        ``~/.astropy/cache``.\n\n    Raises\n    ------\n    `~astropy.utils.data.CacheDamaged`\n        To indicate a problem with the cache contents; the exception contains\n        a ``.bad_files`` attribute containing a set of filenames to allow the\n        user to use :func:`clear_download_cache` to remove the offending items.\n    OSError, RuntimeError\n        To indicate some problem with the cache structure. This may need a full\n        :func:`clear_download_cache` to resolve, or may indicate some kind of\n        misconfiguration.\n    \"\"\"\n    bad_files = set()\n    messages = set()\n    dldir = _get_download_cache_loc(pkgname=pkgname)\n    with os.scandir(dldir) as it:\n        for entry in it:\n            f = os.path.abspath(os.path.joi"}], "retrieved_count": 10, "cost_time": 0.33797383308410645}
{"question": "Where in the control flow does the effective neutrino species parameter validation diverge when receiving a negative value versus a positive unitless quantity in the cosmology parameter test method?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "test_parameters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "effTestMixin(ParameterTestMixin):\n    \"\"\"Tests for `astropy.cosmology.Parameter` Neff on a Cosmology.\n\n    Neff is a descriptor, which are tested by mixin, here with ``TestFLRW``.\n    These tests expect dicts ``_cls_args`` and ``cls_kwargs`` which give the\n    args and kwargs for the cosmology class, respectively. See ``TestFLRW``.\n    \"\"\"\n\n    def test_Neff(self, cosmo_cls: type[Cosmology], cosmo: Cosmology):\n        \"\"\"Test Parameter ``Neff``.\"\"\"\n        # on the class\n        Neff = cosmo_cls.parameters[\"Neff\"]\n        assert isinstance(Neff, Parameter)\n        assert \"Number of effective neutrino species\" in Neff.__doc__\n        assert Neff.default == 3.04\n\n        # validation\n        assert Neff.validate(cosmo, 1) == 1\n        assert Neff.validate(cosmo, 10 * u.one) == 10\n        with pytest.raises(ValueError, match=\"Neff cannot be negative\"):\n            Neff.validate(cosmo, -1)\n\n        # on the instance\n        assert cosmo.Neff is cosmo.__dict__[\"Neff\"]\n        assert cosmo.Neff == self.cls_kwargs.get(\"Neff\", 3.04)\n        assert isinstance(cosmo.Neff, float)\n\n    def test_init_Neff(self, cosmo_cls: type[Cosmology], ba: BoundArguments):\n        \"\"\"Test initialization for values of ``Neff``.\"\"\"\n        # test that it works with units\n        ba.arguments[\"Neff\"] = (\n            cosmo_cls.parameters[\"Neff\"].default << u.one\n        )  # ensure units\n        cosmo = cosmo_cls(*ba.args, **ba.kwargs)\n        assert cosmo.Neff == ba.arguments[\"Neff\"]\n\n        # also without units\n        ba.arguments[\"Neff\"] = ba.arguments[\"Neff\"].value  # strip units\n        cosmo = cosmo_cls(*ba.args, **ba.kwargs)\n        assert cosmo.Neff == ba.arguments[\"Neff\"]\n\n        ba.arguments[\"Neff\"] = -1\n        with pytest.raises(ValueError):\n            cosmo_cls(*ba.args, **ba.kwargs)\n\n\n# =============================================================================\n\n\nclass Parameterm_nuTestMixin(ParameterTestMixin):\n    \"\"\"Tests for `astropy.cosmology.Parameter` m_nu on a Cosmolog"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "test_parameters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "eff == self.cls_kwargs.get(\"Neff\", 3.04)\n        assert isinstance(cosmo.Neff, float)\n\n    def test_init_Neff(self, cosmo_cls: type[Cosmology], ba: BoundArguments):\n        \"\"\"Test initialization for values of ``Neff``.\"\"\"\n        # test that it works with units\n        ba.arguments[\"Neff\"] = (\n            cosmo_cls.parameters[\"Neff\"].default << u.one\n        )  # ensure units\n        cosmo = cosmo_cls(*ba.args, **ba.kwargs)\n        assert cosmo.Neff == ba.arguments[\"Neff\"]\n\n        # also without units\n        ba.arguments[\"Neff\"] = ba.arguments[\"Neff\"].value  # strip units\n        cosmo = cosmo_cls(*ba.args, **ba.kwargs)\n        assert cosmo.Neff == ba.arguments[\"Neff\"]\n\n        ba.arguments[\"Neff\"] = -1\n        with pytest.raises(ValueError):\n            cosmo_cls(*ba.args, **ba.kwargs)\n\n\n# =============================================================================\n\n\nclass Parameterm_nuTestMixin(ParameterTestMixin):\n    \"\"\"Tests for `astropy.cosmology.Parameter` m_nu on a Cosmology.\n\n    m_nu is a descriptor, which are tested by mixin, here with ``TestFLRW``.\n    These tests expect dicts ``_cls_args`` and ``cls_kwargs`` which give the\n    args and kwargs for the cosmology class, respectively. See ``TestFLRW``.\n    \"\"\"\n\n    def test_m_nu(self, cosmo_cls: type[Cosmology], cosmo: Cosmology):\n        \"\"\"Test Parameter ``m_nu``.\"\"\"\n        # on the class\n        m_nu = cosmo_cls.parameters[\"m_nu\"]\n        assert isinstance(m_nu, Parameter)\n        assert \"Mass of neutrino species\" in m_nu.__doc__\n        assert m_nu.unit == u.eV\n        assert m_nu.equivalencies == u.mass_energy()\n        assert m_nu.default == 0.0 * u.eV\n\n        # on the instance\n        # assert cosmo.m_nu is cosmo._m_nu\n        assert_quantity_allclose(cosmo.m_nu, [0.0, 0.0, 0.0] * u.eV)\n\n        # set differently depending on the other inputs\n        if cosmo.Tnu0.value == 0:\n            assert cosmo.m_nu is None\n        elif not cosmo._massivenu:  # only massless\n            assert_quantity_al"}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "test_parameters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ses(ValueError, match=\"unexpected number of neutrino\"):\n            cosmo_cls(*tba.args, **tba.kwargs)\n\n        # No neutrinos, but Neff\n        tba.arguments[\"m_nu\"] = 0\n        cosmo = cosmo_cls(*tba.args, **tba.kwargs)\n        assert not cosmo.has_massive_nu\n        assert len(cosmo.m_nu) == 4\n        assert cosmo.m_nu.unit == u.eV\n        assert_quantity_allclose(cosmo.m_nu, 0 * u.eV)\n        # TODO! move this test when create ``test_nu_relative_density``\n        assert_quantity_allclose(\n            cosmo.nu_relative_density(1.0), 0.22710731766 * 4.05, rtol=1e-6\n        )\n\n        # All massive neutrinos case, len from Neff\n        tba.arguments[\"m_nu\"] = 0.1 * u.eV\n        cosmo = cosmo_cls(*tba.args, **tba.kwargs)\n        assert cosmo.has_massive_nu\n        assert len(cosmo.m_nu) == 4\n        assert cosmo.m_nu.unit == u.eV\n        assert_quantity_allclose(cosmo.m_nu, [0.1, 0.1, 0.1, 0.1] * u.eV)\n\n    def test_init_m_nu_override_by_Tcmb0(\n        self, cosmo_cls: type[Cosmology], ba: BoundArguments\n    ):\n        \"\"\"Test initialization for values of ``m_nu``.\n\n        Note this test requires ``Tcmb0`` as constructor input, and a property\n        ``has_massive_nu``.\n        \"\"\"\n        # If Neff = 0, m_nu is None.\n        tba = copy.copy(ba)\n        tba.arguments[\"Neff\"] = 0\n        cosmo = cosmo_cls(*ba.args, **ba.kwargs)\n        assert cosmo.m_nu is None\n        assert not cosmo.has_massive_nu\n\n        # If Tcmb0 = 0, m_nu is None\n        tba = copy.copy(ba)\n        tba.arguments[\"Tcmb0\"] = 0\n        cosmo = cosmo_cls(*ba.args, **ba.kwargs)\n        assert cosmo.m_nu is None\n        assert not cosmo.has_massive_nu\n\n\n# =============================================================================\n\n\nclass ParameterOb0TestMixin(ParameterTestMixin):\n    \"\"\"Tests for `astropy.cosmology.Parameter` Ob0 on a Cosmology.\n\n    Ob0 is a descriptor, which are tested by mixin, here with ``TestFLRW``.\n    These tests expect dicts ``_cls_args`` and ``cls_kwargs`` which give th"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "test_parameters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ve units.\n        ba.arguments[\"m_nu\"] = cosmo_cls.parameters[\"m_nu\"].default.value  # strip units\n        cosmo = cosmo_cls(*ba.args, **ba.kwargs)\n        np.testing.assert_array_equal(cosmo.m_nu.value, ba.arguments[\"m_nu\"])\n        assert not cosmo.has_massive_nu\n\n        # A negative m_nu raises an exception.\n        tba = copy.copy(ba)\n        tba.arguments[\"m_nu\"] = u.Quantity([-0.3, 0.2, 0.1], u.eV)\n        with pytest.raises(ValueError, match=\"invalid\"):\n            cosmo_cls(*tba.args, **tba.kwargs)\n\n    def test_init_m_nu_and_Neff(self, cosmo_cls: type[Cosmology], ba: BoundArguments):\n        \"\"\"Test initialization for values of ``m_nu`` and ``Neff``.\n\n        Note this test requires ``Neff`` as constructor input, and a property\n        ``has_massive_nu``.\n        \"\"\"\n        # Mismatch with Neff = wrong number of neutrinos\n        tba = copy.copy(ba)\n        tba.arguments[\"Neff\"] = 4.05\n        tba.arguments[\"m_nu\"] = u.Quantity([0.15, 0.2, 0.1], u.eV)\n        with pytest.raises(ValueError, match=\"unexpected number of neutrino\"):\n            cosmo_cls(*tba.args, **tba.kwargs)\n\n        # No neutrinos, but Neff\n        tba.arguments[\"m_nu\"] = 0\n        cosmo = cosmo_cls(*tba.args, **tba.kwargs)\n        assert not cosmo.has_massive_nu\n        assert len(cosmo.m_nu) == 4\n        assert cosmo.m_nu.unit == u.eV\n        assert_quantity_allclose(cosmo.m_nu, 0 * u.eV)\n        # TODO! move this test when create ``test_nu_relative_density``\n        assert_quantity_allclose(\n            cosmo.nu_relative_density(1.0), 0.22710731766 * 4.05, rtol=1e-6\n        )\n\n        # All massive neutrinos case, len from Neff\n        tba.arguments[\"m_nu\"] = 0.1 * u.eV\n        cosmo = cosmo_cls(*tba.args, **tba.kwargs)\n        assert cosmo.has_massive_nu\n        assert len(cosmo.m_nu) == 4\n        assert cosmo.m_nu.unit == u.eV\n        assert_quantity_allclose(cosmo.m_nu, [0.1, 0.1, 0.1, 0.1] * u.eV)\n\n    def test_init_m_nu_override_by_Tcmb0(\n        self, cosmo_cls: type[Cosmology],"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    massivenu_mass = self.m_nu[massive].value if massivenu else None\n            nmasslessnu = nneutrinos - nmassivenu\n\n        object.__setattr__(self, \"_nneutrinos\", nneutrinos)\n        object.__setattr__(self, \"_neff_per_nu\", neff_per_nu)\n        object.__setattr__(self, \"_massivenu\", massivenu)\n        object.__setattr__(self, \"_massivenu_mass\", massivenu_mass)\n        object.__setattr__(self, \"_nmassivenu\", nmassivenu)\n        object.__setattr__(self, \"_nmasslessnu\", nmasslessnu)\n\n        # Compute Neutrino Omega and total relativistic component for massive\n        # neutrinos. We also store a list version, since that is more efficient\n        # to do integrals with (perhaps surprisingly! But small python lists\n        # are more efficient than small NumPy arrays).\n        if self._massivenu:  # (`_massivenu` set in `m_nu`)\n            nu_y = (self._massivenu_mass / (_kB_evK * self.Tnu0)).value\n            nu_y_list = nu_y.tolist()\n        else:\n            nu_y = nu_y_list = None\n        object.__setattr__(self, \"_nu_y\", nu_y)\n        object.__setattr__(self, \"_nu_y_list\", nu_y_list)\n\n        # Subclasses should override this reference if they provide\n        #  more efficient scalar versions of inv_efunc.\n        object.__setattr__(self, \"_inv_efunc_scalar\", self.inv_efunc)\n        object.__setattr__(self, \"_inv_efunc_scalar_args\", ())\n\n    # ---------------------------------------------------------------\n    # Parameter details\n\n    @Ob0.validator\n    def Ob0(self, param, value):\n        \"\"\"Validate baryon density to a non-negative float > matter density.\"\"\"\n        if value is None:\n            warnings.warn(\n                \"Ob0=None is deprecated, use Ob0=0 instead, \"\n                \"which never causes methods to raise exceptions.\",\n                category=DeprecationWarning,\n                stacklevel=2,\n            )\n            return 0.0\n\n        value = validate_non_negative(self, param, value)\n        if value > self.Om0:\n            raise ValueEr"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "base.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n        object.__setattr__(self, \"_nu_y\", nu_y)\n        object.__setattr__(self, \"_nu_y_list\", nu_y_list)\n\n        # Subclasses should override this reference if they provide\n        #  more efficient scalar versions of inv_efunc.\n        object.__setattr__(self, \"_inv_efunc_scalar\", self.inv_efunc)\n        object.__setattr__(self, \"_inv_efunc_scalar_args\", ())\n\n    # ---------------------------------------------------------------\n    # Parameter details\n\n    @Ob0.validator\n    def Ob0(self, param, value):\n        \"\"\"Validate baryon density to a non-negative float > matter density.\"\"\"\n        if value is None:\n            warnings.warn(\n                \"Ob0=None is deprecated, use Ob0=0 instead, \"\n                \"which never causes methods to raise exceptions.\",\n                category=DeprecationWarning,\n                stacklevel=2,\n            )\n            return 0.0\n\n        value = validate_non_negative(self, param, value)\n        if value > self.Om0:\n            raise ValueError(\n                \"baryonic density can not be larger than total matter density.\"\n            )\n        return value\n\n    @m_nu.validator\n    def m_nu(self, param, value):\n        \"\"\"Validate neutrino masses to right value, units, and shape.\n\n        There are no neutrinos if floor(Neff) or Tcmb0 are 0. The number of\n        neutrinos must match floor(Neff). Neutrino masses cannot be\n        negative.\n        \"\"\"\n        # Check if there are any neutrinos\n        if (nneutrinos := floor(self.Neff)) == 0 or self.Tcmb0.value == 0:\n            return None  # None, regardless of input\n\n        # Validate / set units\n        value = validate_with_unit(self, param, value)\n\n        # Check values and data shapes\n        if value.shape not in ((), (nneutrinos,)):\n            raise ValueError(\n                \"unexpected number of neutrino masses  \"\n                f\"expected {nneutrinos}, got {len(value)}.\"\n            )\n        elif np.any(value.value < 0):\n            raise ValueError(\"i"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "test_parameters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "instance\n        assert cosmo.Tcmb0 is cosmo.__dict__[\"Tcmb0\"]\n        assert cosmo.Tcmb0 == self.cls_kwargs[\"Tcmb0\"]\n        assert isinstance(cosmo.Tcmb0, u.Quantity) and cosmo.Tcmb0.unit == u.K\n\n    def test_init_Tcmb0(self, cosmo_cls: type[Cosmology], ba: BoundArguments):\n        \"\"\"Test initialization for values of ``Tcmb0``.\"\"\"\n        # test that it works with units\n        cosmo = cosmo_cls(*ba.args, **ba.kwargs)\n        assert cosmo.Tcmb0 == ba.arguments[\"Tcmb0\"]\n\n        # also without units\n        ba.arguments[\"Tcmb0\"] = ba.arguments[\"Tcmb0\"].value  # strip units\n        cosmo = cosmo_cls(*ba.args, **ba.kwargs)\n        assert cosmo.Tcmb0.value == ba.arguments[\"Tcmb0\"]\n\n        # must be a scalar\n        ba.arguments[\"Tcmb0\"] = u.Quantity([0.0, 2], u.K)\n        with pytest.raises(ValueError, match=\"Tcmb0 is a non-scalar quantity\"):\n            cosmo_cls(*ba.args, **ba.kwargs)\n\n\n# =============================================================================\n\n\nclass ParameterNeffTestMixin(ParameterTestMixin):\n    \"\"\"Tests for `astropy.cosmology.Parameter` Neff on a Cosmology.\n\n    Neff is a descriptor, which are tested by mixin, here with ``TestFLRW``.\n    These tests expect dicts ``_cls_args`` and ``cls_kwargs`` which give the\n    args and kwargs for the cosmology class, respectively. See ``TestFLRW``.\n    \"\"\"\n\n    def test_Neff(self, cosmo_cls: type[Cosmology], cosmo: Cosmology):\n        \"\"\"Test Parameter ``Neff``.\"\"\"\n        # on the class\n        Neff = cosmo_cls.parameters[\"Neff\"]\n        assert isinstance(Neff, Parameter)\n        assert \"Number of effective neutrino species\" in Neff.__doc__\n        assert Neff.default == 3.04\n\n        # validation\n        assert Neff.validate(cosmo, 1) == 1\n        assert Neff.validate(cosmo, 10 * u.one) == 10\n        with pytest.raises(ValueError, match=\"Neff cannot be negative\"):\n            Neff.validate(cosmo, -1)\n\n        # on the instance\n        assert cosmo.Neff is cosmo.__dict__[\"Neff\"]\n        assert cosmo.N"}, {"start_line": 15000, "end_line": 17000, "belongs_to": {"file_name": "test_parameters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " ba: BoundArguments\n    ):\n        \"\"\"Test initialization for values of ``m_nu``.\n\n        Note this test requires ``Tcmb0`` as constructor input, and a property\n        ``has_massive_nu``.\n        \"\"\"\n        # If Neff = 0, m_nu is None.\n        tba = copy.copy(ba)\n        tba.arguments[\"Neff\"] = 0\n        cosmo = cosmo_cls(*ba.args, **ba.kwargs)\n        assert cosmo.m_nu is None\n        assert not cosmo.has_massive_nu\n\n        # If Tcmb0 = 0, m_nu is None\n        tba = copy.copy(ba)\n        tba.arguments[\"Tcmb0\"] = 0\n        cosmo = cosmo_cls(*ba.args, **ba.kwargs)\n        assert cosmo.m_nu is None\n        assert not cosmo.has_massive_nu\n\n\n# =============================================================================\n\n\nclass ParameterOb0TestMixin(ParameterTestMixin):\n    \"\"\"Tests for `astropy.cosmology.Parameter` Ob0 on a Cosmology.\n\n    Ob0 is a descriptor, which are tested by mixin, here with ``TestFLRW``.\n    These tests expect dicts ``_cls_args`` and ``cls_kwargs`` which give the\n    args and kwargs for the cosmology class, respectively. See ``TestFLRW``.\n    \"\"\"\n\n    def test_Ob0(self, cosmo_cls: type[Cosmology], cosmo: Cosmology):\n        \"\"\"Test Parameter ``Ob0``.\"\"\"\n        # on the class\n        Ob0 = cosmo_cls.parameters[\"Ob0\"]\n        assert isinstance(Ob0, Parameter)\n        assert \"Omega baryon;\" in Ob0.__doc__\n        assert Ob0.default == 0.0\n\n        # validation\n        with pytest.warns(DeprecationWarning, match=\"Ob0=None is deprecated\"):\n            assert Ob0.validate(cosmo, None) == 0.0\n        assert Ob0.validate(cosmo, 0.1) == 0.1\n        assert Ob0.validate(cosmo, 0.1 * u.one) == 0.1\n        with pytest.raises(ValueError, match=\"Ob0 cannot be negative\"):\n            Ob0.validate(cosmo, -1)\n        with pytest.raises(ValueError, match=\"baryonic density can not be larger\"):\n            Ob0.validate(cosmo, cosmo.Om0 + 1)\n\n        # on the instance\n        assert cosmo.Ob0 is cosmo.__dict__[\"Ob0\"]\n        assert cosmo.Ob0 == 0.03\n\n    def test"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "test_parameters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "lclose(cosmo.m_nu, 0 * u.eV)\n        elif self._nmasslessnu == 0:  # only massive\n            assert cosmo.m_nu == cosmo._massivenu_mass\n        else:  # a mix -- the most complicated case\n            assert_quantity_allclose(cosmo.m_nu[: self._nmasslessnu], 0 * u.eV)\n            assert_quantity_allclose(\n                cosmo.m_nu[self._nmasslessnu], cosmo._massivenu_mass\n            )\n\n    def test_init_m_nu(self, cosmo_cls: type[Cosmology], ba: BoundArguments):\n        \"\"\"Test initialization for values of ``m_nu``.\n\n        Note this requires the class to have a property ``has_massive_nu``.\n        \"\"\"\n        # Test that it works when m_nu has units.\n        cosmo = cosmo_cls(*ba.args, **ba.kwargs)\n        np.testing.assert_array_equal(\n            cosmo.m_nu, cosmo_cls.parameters[\"m_nu\"].default\n        )  # (& checks len, unit)\n        assert not cosmo.has_massive_nu\n        assert cosmo.m_nu.unit == u.eV  # explicitly check unit once.\n\n        # And it works when m_nu doesn't have units.\n        ba.arguments[\"m_nu\"] = cosmo_cls.parameters[\"m_nu\"].default.value  # strip units\n        cosmo = cosmo_cls(*ba.args, **ba.kwargs)\n        np.testing.assert_array_equal(cosmo.m_nu.value, ba.arguments[\"m_nu\"])\n        assert not cosmo.has_massive_nu\n\n        # A negative m_nu raises an exception.\n        tba = copy.copy(ba)\n        tba.arguments[\"m_nu\"] = u.Quantity([-0.3, 0.2, 0.1], u.eV)\n        with pytest.raises(ValueError, match=\"invalid\"):\n            cosmo_cls(*tba.args, **tba.kwargs)\n\n    def test_init_m_nu_and_Neff(self, cosmo_cls: type[Cosmology], ba: BoundArguments):\n        \"\"\"Test initialization for values of ``m_nu`` and ``Neff``.\n\n        Note this test requires ``Neff`` as constructor input, and a property\n        ``has_massive_nu``.\n        \"\"\"\n        # Mismatch with Neff = wrong number of neutrinos\n        tba = copy.copy(ba)\n        tba.arguments[\"Neff\"] = 4.05\n        tba.arguments[\"m_nu\"] = u.Quantity([0.15, 0.2, 0.1], u.eV)\n        with pytest.rai"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "test_parameters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "fails for negative numbers\n        ba.arguments[\"Om0\"] = -0.27\n        with pytest.raises(ValueError, match=\"Om0 cannot be negative.\"):\n            cosmo_cls(*ba.args, **ba.kwargs)\n\n\n# =============================================================================\n\n\nclass ParameterOde0TestMixin(ParameterTestMixin):\n    \"\"\"Tests for `astropy.cosmology.Parameter` Ode0 on a Cosmology.\n\n    Ode0 is a descriptor, which are tested by mixin, here with ``TestFLRW``.\n    These tests expect dicts ``_cls_args`` and ``cls_kwargs`` which give the\n    args and kwargs for the cosmology class, respectively. See ``TestFLRW``.\n    \"\"\"\n\n    def test_Parameter_Ode0(self, cosmo_cls: type[Cosmology]):\n        \"\"\"Test Parameter ``Ode0`` on the class.\"\"\"\n        Ode0 = cosmo_cls.parameters.get(\n            \"Ode0\", cosmo_cls._derived_parameters.get(\"Ode0\")\n        )\n        assert isinstance(Ode0, Parameter)\n        assert \"Omega dark energy\" in Ode0.__doc__\n        if issubclass(cosmo_cls, FlatFLRWMixin):\n            assert Ode0.default == 0\n        else:\n            assert Ode0.default is MISSING\n\n    def test_Parameter_Ode0_validation(\n        self, cosmo_cls: type[Cosmology], cosmo: Cosmology\n    ):\n        \"\"\"Test Parameter ``Ode0`` validation.\"\"\"\n        Ode0 = cosmo_cls.parameters.get(\n            \"Ode0\", cosmo_cls._derived_parameters.get(\"Ode0\")\n        )\n        assert Ode0.validate(cosmo, 1.1) == 1.1\n        assert Ode0.validate(cosmo, 10 * u.one) == 10.0\n        with pytest.raises(TypeError, match=\"only dimensionless\"):\n            Ode0.validate(cosmo, 10 * u.km)\n\n    def test_Ode0(self, cosmo: Cosmology):\n        \"\"\"Test Parameter ``Ode0`` validation.\"\"\"\n        # if Ode0 is a parameter, test its value\n        assert cosmo.Ode0 is cosmo.__dict__[\"Ode0\"]\n        assert cosmo.Ode0 == self._cls_args[\"Ode0\"]\n        assert isinstance(cosmo.Ode0, float)\n\n    def test_init_Ode0(self, cosmo_cls: type[Cosmology], ba: BoundArguments):\n        \"\"\"Test initialization for values of ``Ode0``.\""}], "retrieved_count": 10, "cost_time": 0.34505796432495117}
{"question": "What is the semantic interpretation of the three-dimensional cartesian velocity differential object returned by the function that updates coordinate differentials to match a velocity reference while preserving spatial position when the parameter controlling whether the result remains in the original coordinate frame is False?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "spectral_coordinate.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ".eV)\n        or squantity.unit.is_equivalent(1 / u.m)\n    ):\n        return squantity / doppler_factor\n    elif squantity.unit.is_equivalent(KMS):  # velocity\n        return (squantity.to(u.Hz) / doppler_factor).to(squantity.unit)\n    else:  # pragma: no cover\n        raise RuntimeError(\n            f\"Unexpected units in velocity shift: {squantity.unit}. This should not\"\n            \" happen, so please report this in the astropy issue tracker!\"\n        )\n\n\ndef update_differentials_to_match(\n    original, velocity_reference, preserve_observer_frame=False\n):\n    \"\"\"\n    Given an original coordinate object, update the differentials so that\n    the final coordinate is at the same location as the original coordinate\n    but co-moving with the velocity reference object.\n\n    If preserve_original_frame is set to True, the resulting object will be in\n    the frame of the original coordinate, otherwise it will be in the frame of\n    the velocity reference.\n    \"\"\"\n    if not velocity_reference.data.differentials:\n        raise ValueError(\"Reference frame has no velocities\")\n\n    # If the reference has an obstime already defined, we should ignore\n    # it and stick with the original observer obstime.\n    if \"obstime\" in velocity_reference.frame_attributes and hasattr(\n        original, \"obstime\"\n    ):\n        velocity_reference = velocity_reference.replicate(obstime=original.obstime)\n\n    # We transform both coordinates to ICRS for simplicity and because we know\n    # it's a simple frame that is not time-dependent (it could be that both\n    # the original and velocity_reference frame are time-dependent)\n\n    original_icrs = original.transform_to(ICRS())\n    velocity_reference_icrs = velocity_reference.transform_to(ICRS())\n\n    differentials = velocity_reference_icrs.data.represent_as(\n        CartesianRepresentation, CartesianDifferential\n    ).differentials\n\n    data_with_differentials = original_icrs.data.represent_as(\n        CartesianRepresentation\n    ).with_differentia"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "affine.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates/transformations", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ldiff += offset.differentials[\"s\"]\n\n            newrep = newrep.with_differentials({\"s\": veldiff})\n\n        if isinstance(fromcoord.data, UnitSphericalRepresentation):\n            # Special-case this because otherwise the return object will think\n            # it has a valid distance with the default return (a\n            # CartesianRepresentation instance)\n\n            if has_velocity and not unit_vel_diff and not rad_vel_diff:\n                # We have to first represent as the Unit types we converted to,\n                # then put the d_distance information back in to the\n                # differentials and re-represent as their original forms\n                newdiff = newrep.differentials[\"s\"]\n                _unit_cls = fromcoord.data.differentials[\"s\"]._unit_differential\n                newdiff = newdiff.represent_as(_unit_cls, newrep)\n\n                kwargs = {comp: getattr(newdiff, comp) for comp in newdiff.components}\n                kwargs[\"d_distance\"] = fromcoord.data.differentials[\"s\"].d_distance\n                diffs = {\n                    \"s\": type(fromcoord.data.differentials[\"s\"])(copy=False, **kwargs)\n                }\n\n            elif has_velocity and unit_vel_diff:\n                newdiff = newrep.differentials[\"s\"].represent_as(\n                    fromcoord.data.differentials[\"s\"].__class__, newrep\n                )\n                diffs = {\"s\": newdiff}\n\n            else:\n                diffs = newrep.differentials\n\n            newrep = newrep.represent_as(type(fromcoord.data)).with_differentials(diffs)\n\n        elif has_velocity and unit_vel_diff:\n            # Here, we're in the case where the representation is not\n            # UnitSpherical, but the differential *is* one of the UnitSpherical\n            # types. We have to convert back to that differential class or the\n            # resulting frame will think it has a valid radial_velocity. This\n            # can probably be cleaned up: we currently have to go through the\n            #"}, {"start_line": 22000, "end_line": 24000, "belongs_to": {"file_name": "spectral_coordinate.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      ----------\n        frame : str, `~astropy.coordinates.BaseCoordinateFrame` or `~astropy.coordinates.SkyCoord`\n            The observation frame in which the observer will be stationary. This\n            can be the name of a frame (e.g. 'icrs'), a frame class, frame instance\n            with no data, or instance with data. This can optionally include\n            velocities.\n        velocity : `~astropy.units.Quantity` or `~astropy.coordinates.CartesianDifferential`, optional\n            If ``frame`` does not contain velocities, these can be specified as\n            a 3-element `~astropy.units.Quantity`. In the case where this is\n            also not specified, the velocities default to zero.\n        preserve_observer_frame : bool\n            If `True`, the final observer frame class will be the same as the\n            original one, and if `False` it will be the frame of the velocity\n            reference class.\n\n        Returns\n        -------\n        new_coord : `SpectralCoord`\n            The new coordinate object representing the spectral data\n            transformed based on the observer's new velocity frame.\n        \"\"\"\n        if self.observer is None or self.target is None:\n            raise ValueError(\n                \"This method can only be used if both observer \"\n                \"and target are defined on the SpectralCoord.\"\n            )\n\n        # Start off by extracting frame if a SkyCoord was passed in\n        if isinstance(frame, SkyCoord):\n            frame = frame.frame\n\n        if isinstance(frame, BaseCoordinateFrame):\n            if not frame.has_data:\n                frame = frame.realize_frame(\n                    CartesianRepresentation(0 * u.km, 0 * u.km, 0 * u.km)\n                )\n\n            if frame.data.differentials:\n                if velocity is not None:\n                    raise ValueError(\n                        \"frame already has differentials, cannot also specify velocity\"\n                    )\n                # otherwise"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "spectral_coordinate.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "data.differentials:\n        raise ValueError(\"Reference frame has no velocities\")\n\n    # If the reference has an obstime already defined, we should ignore\n    # it and stick with the original observer obstime.\n    if \"obstime\" in velocity_reference.frame_attributes and hasattr(\n        original, \"obstime\"\n    ):\n        velocity_reference = velocity_reference.replicate(obstime=original.obstime)\n\n    # We transform both coordinates to ICRS for simplicity and because we know\n    # it's a simple frame that is not time-dependent (it could be that both\n    # the original and velocity_reference frame are time-dependent)\n\n    original_icrs = original.transform_to(ICRS())\n    velocity_reference_icrs = velocity_reference.transform_to(ICRS())\n\n    differentials = velocity_reference_icrs.data.represent_as(\n        CartesianRepresentation, CartesianDifferential\n    ).differentials\n\n    data_with_differentials = original_icrs.data.represent_as(\n        CartesianRepresentation\n    ).with_differentials(differentials)\n\n    final_icrs = original_icrs.realize_frame(data_with_differentials)\n\n    if preserve_observer_frame:\n        final = final_icrs.transform_to(original)\n    else:\n        final = final_icrs.transform_to(velocity_reference)\n\n    return final.replicate(\n        representation_type=CartesianRepresentation,\n        differential_type=CartesianDifferential,\n    )\n\n\ndef attach_zero_velocities(coord):\n    \"\"\"\n    Set the differentials to be stationary on a coordinate object.\n    \"\"\"\n    new_data = coord.cartesian.with_differentials(ZERO_VELOCITIES)\n    return coord.realize_frame(new_data)\n\n\ndef _get_velocities(coord):\n    if \"s\" in coord.data.differentials:\n        return coord.velocity\n    else:\n        return ZERO_VELOCITIES\n\n\nclass SpectralCoord(SpectralQuantity):\n    \"\"\"\n    A spectral coordinate with its corresponding unit.\n\n    .. note:: The |SpectralCoord| class is new in Astropy v4.1 and should be\n              considered experimental at this time. Note that we do no"}, {"start_line": 23000, "end_line": 25000, "belongs_to": {"file_name": "spectral_coordinate.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "           The new coordinate object representing the spectral data\n            transformed based on the observer's new velocity frame.\n        \"\"\"\n        if self.observer is None or self.target is None:\n            raise ValueError(\n                \"This method can only be used if both observer \"\n                \"and target are defined on the SpectralCoord.\"\n            )\n\n        # Start off by extracting frame if a SkyCoord was passed in\n        if isinstance(frame, SkyCoord):\n            frame = frame.frame\n\n        if isinstance(frame, BaseCoordinateFrame):\n            if not frame.has_data:\n                frame = frame.realize_frame(\n                    CartesianRepresentation(0 * u.km, 0 * u.km, 0 * u.km)\n                )\n\n            if frame.data.differentials:\n                if velocity is not None:\n                    raise ValueError(\n                        \"frame already has differentials, cannot also specify velocity\"\n                    )\n                # otherwise frame is ready to go\n            else:\n                if velocity is None:\n                    differentials = ZERO_VELOCITIES\n                else:\n                    differentials = CartesianDifferential(velocity)\n                frame = frame.realize_frame(\n                    frame.data.with_differentials(differentials)\n                )\n\n        if isinstance(frame, (type, str)):\n            if isinstance(frame, type):\n                frame_cls = frame\n            elif isinstance(frame, str):\n                frame_cls = frame_transform_graph.lookup_name(frame)\n            if velocity is None:\n                velocity = 0 * u.m / u.s, 0 * u.m / u.s, 0 * u.m / u.s\n            elif velocity.shape != (3,):\n                raise ValueError(\"velocity should be a Quantity vector with 3 elements\")\n            frame = frame_cls(\n                0 * u.m,\n                0 * u.m,\n                0 * u.m,\n                *velocity,\n                representation_type=\"cartesian\",\n         "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "spectral_coordinate.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ectralQuantity` and a velocity, return a new `SpectralQuantity`\n    that is Doppler shifted by this amount.\n\n    Note that the Doppler shift applied is the full relativistic one, so\n    `SpectralQuantity` currently expressed in velocity and not using the\n    relativistic convention will temporarily be converted to use the\n    relativistic convention while the shift is applied.\n\n    Positive velocities are assumed to redshift the spectral quantity,\n    while negative velocities blueshift the spectral quantity.\n    \"\"\"\n    # NOTE: we deliberately don't keep sub-classes of SpectralQuantity intact\n    # since we can't guarantee that their metadata would be correct/consistent.\n    squantity = scoord.view(SpectralQuantity)\n\n    beta = velocity / c\n    doppler_factor = np.sqrt((1 + beta) / (1 - beta))\n\n    if squantity.unit.is_equivalent(u.m):  # wavelength\n        return squantity * doppler_factor\n    elif (\n        squantity.unit.is_equivalent(u.Hz)\n        or squantity.unit.is_equivalent(u.eV)\n        or squantity.unit.is_equivalent(1 / u.m)\n    ):\n        return squantity / doppler_factor\n    elif squantity.unit.is_equivalent(KMS):  # velocity\n        return (squantity.to(u.Hz) / doppler_factor).to(squantity.unit)\n    else:  # pragma: no cover\n        raise RuntimeError(\n            f\"Unexpected units in velocity shift: {squantity.unit}. This should not\"\n            \" happen, so please report this in the astropy issue tracker!\"\n        )\n\n\ndef update_differentials_to_match(\n    original, velocity_reference, preserve_observer_frame=False\n):\n    \"\"\"\n    Given an original coordinate object, update the differentials so that\n    the final coordinate is at the same location as the original coordinate\n    but co-moving with the velocity reference object.\n\n    If preserve_original_frame is set to True, the resulting object will be in\n    the frame of the original coordinate, otherwise it will be in the frame of\n    the velocity reference.\n    \"\"\"\n    if not velocity_reference."}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "test_spectral_coordinate.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "         0 * u.km,\n                0 * u.km,\n                2**0.5 * u.km / u.s,\n                0 * u.km / u.s,\n                2**0.5 * u.km / u.s,\n                representation_type=\"cartesian\",\n                differential_type=\"cartesian\",\n            )\n        )\n    )\n    assert_quantity_allclose(\n        sc10.radial_velocity, 0 * u.km / u.s, atol=1e-10 * u.km / u.s\n    )\n\n    # But we shouldn't be able to pass both a frame with velocities, and explicit velocities\n\n    with pytest.raises(\n        ValueError,\n        match=\"frame already has differentials, cannot also specify velocity\",\n    ):\n        sc2.with_observer_stationary_relative_to(\n            ICRS(\n                0 * u.km,\n                0 * u.km,\n                0 * u.km,\n                2**0.5 * u.km / u.s,\n                0 * u.km / u.s,\n                2**0.5 * u.km / u.s,\n                representation_type=\"cartesian\",\n                differential_type=\"cartesian\",\n            ),\n            velocity=[-(2**0.5), 0, -(2**0.5)] * u.km / u.s,\n        )\n\n    # And velocities should have three elements\n\n    with pytest.raises(\n        ValueError, match=\"velocity should be a Quantity vector with 3 elements\"\n    ):\n        sc2.with_observer_stationary_relative_to(\n            ICRS, velocity=[-(2**0.5), 0, -(2**0.5), -3] * u.km / u.s\n        )\n\n    # Make sure things don't change depending on what frame class is used for reference\n    sc11 = sc2.with_observer_stationary_relative_to(\n        SkyCoord(\n            ICRS(\n                0 * u.km,\n                0 * u.km,\n                0 * u.km,\n                2**0.5 * u.km / u.s,\n                0 * u.km / u.s,\n                2**0.5 * u.km / u.s,\n                representation_type=\"cartesian\",\n                differential_type=\"cartesian\",\n            )\n        ).transform_to(Galactic)\n    )\n    assert_quantity_allclose(\n        sc11.radial_velocity, 0 * u.km / u.s, atol=1e-10 * u.km / u.s\n    )\n\n    # Check that it is possible to preserve the "}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "function.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates/transformations", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ame)\n        # this is the finite difference case\n\n        if callable(self.finite_difference_dt):\n            dt = self.finite_difference_dt(fromcoord, toframe)\n        else:\n            dt = self.finite_difference_dt\n        halfdt = dt / 2\n\n        from_diffless = fromcoord.realize_frame(fromcoord.data.without_differentials())\n        reprwithoutdiff = supcall(from_diffless, toframe)\n\n        # first we use the existing differential to compute an offset due to\n        # the already-existing velocity, but in the new frame\n        fromcoord_cart = fromcoord.cartesian\n        if self.symmetric_finite_difference:\n            fwdxyz = (\n                fromcoord_cart.xyz + fromcoord_cart.differentials[\"s\"].d_xyz * halfdt\n            )\n            fwd = supcall(\n                fromcoord.realize_frame(CartesianRepresentation(fwdxyz)), toframe\n            )\n            backxyz = (\n                fromcoord_cart.xyz - fromcoord_cart.differentials[\"s\"].d_xyz * halfdt\n            )\n            back = supcall(\n                fromcoord.realize_frame(CartesianRepresentation(backxyz)), toframe\n            )\n        else:\n            fwdxyz = fromcoord_cart.xyz + fromcoord_cart.differentials[\"s\"].d_xyz * dt\n            fwd = supcall(\n                fromcoord.realize_frame(CartesianRepresentation(fwdxyz)), toframe\n            )\n            back = reprwithoutdiff\n        diffxyz = (fwd.cartesian - back.cartesian).xyz / dt\n\n        # now we compute the \"induced\" velocities due to any movement in\n        # the frame itself over time\n        attrname = self.finite_difference_frameattr_name\n        if attrname is not None:\n            if self.symmetric_finite_difference:\n                if self._diff_attr_in_fromsys:\n                    kws = {attrname: getattr(from_diffless, attrname) + halfdt}\n                    from_diffless_fwd = from_diffless.replicate(**kws)\n                else:\n                    from_diffless_fwd = from_diffless\n                if self._diff_attr_in_tosy"}, {"start_line": 4000, "end_line": 6000, "belongs_to": {"file_name": "affine.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates/transformations", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ance(data, UnitSphericalRepresentation)\n            and not (unit_vel_diff or rad_vel_diff)\n        ):\n            # retrieve just velocity differential\n            unit_diff = data.differentials[\"s\"].represent_as(\n                data.differentials[\"s\"]._unit_differential, data\n            )\n            data = data.with_differentials({\"s\": unit_diff})  # updates key\n\n        # If it's a RadialDifferential, we flat-out ignore the differentials\n        # This is because, by this point (past the validation above), we can\n        # only possibly be doing a rotation-only transformation, and that\n        # won't change the radial differential. We later add it back in\n        elif rad_vel_diff:\n            data = data.without_differentials()\n\n        # Convert the representation and differentials to cartesian without\n        # having them attached to a frame\n        rep = data.to_cartesian()\n        diffs = {\n            k: diff.represent_as(CartesianDifferential, data)\n            for k, diff in data.differentials.items()\n        }\n        rep = rep.with_differentials(diffs)\n\n        # Only do transform if matrix is specified. This is for speed in\n        # transformations that only specify an offset (e.g., LSR)\n        if matrix is not None:\n            # Note: this applies to both representation and differentials\n            rep = rep.transform(matrix)\n\n        # TODO: if we decide to allow arithmetic between representations that\n        # contain differentials, this can be tidied up\n        newrep = rep.without_differentials()\n        if offset is not None:\n            newrep += offset.without_differentials()\n\n        # We need a velocity (time derivative) and, for now, are strict: the\n        # representation can only contain a velocity differential and no others.\n        if has_velocity and not rad_vel_diff:\n            veldiff = rep.differentials[\"s\"]  # already in Cartesian form\n\n            if offset is not None and \"s\" in offset.differentials:\n                ve"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "affine.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates/transformations", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "erentials[\"s\"].d_distance\n                diffs = {\n                    \"s\": type(fromcoord.data.differentials[\"s\"])(copy=False, **kwargs)\n                }\n\n            elif has_velocity and unit_vel_diff:\n                newdiff = newrep.differentials[\"s\"].represent_as(\n                    fromcoord.data.differentials[\"s\"].__class__, newrep\n                )\n                diffs = {\"s\": newdiff}\n\n            else:\n                diffs = newrep.differentials\n\n            newrep = newrep.represent_as(type(fromcoord.data)).with_differentials(diffs)\n\n        elif has_velocity and unit_vel_diff:\n            # Here, we're in the case where the representation is not\n            # UnitSpherical, but the differential *is* one of the UnitSpherical\n            # types. We have to convert back to that differential class or the\n            # resulting frame will think it has a valid radial_velocity. This\n            # can probably be cleaned up: we currently have to go through the\n            # dimensional version of the differential before representing as the\n            # unit differential so that the units work out (the distance length\n            # unit shouldn't appear in the resulting proper motions)\n\n            diff_cls = fromcoord.data.differentials[\"s\"].__class__\n            newrep = newrep.represent_as(\n                type(fromcoord.data), diff_cls._dimensional_differential\n            ).represent_as(type(fromcoord.data), diff_cls)\n\n        # We pulled the radial differential off of the representation\n        # earlier, so now we need to put it back. But, in order to do that, we\n        # have to turn the representation into a repr that is compatible with\n        # having a RadialDifferential\n        if has_velocity and rad_vel_diff:\n            newrep = newrep.represent_as(fromcoord.data.__class__)\n            newrep = newrep.with_differentials({\"s\": fromcoord.data.differentials[\"s\"]})\n\n        return newrep\n\n    def __call__(self, fromcoord, toframe):\n        p"}], "retrieved_count": 10, "cost_time": 0.32403016090393066}
{"question": "Where in the codebase is the classmethod that converts FITS keyword strings to uppercase before they are used as keys in the header's keyword-to-index mapping dictionary located?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 21000, "end_line": 23000, "belongs_to": {"file_name": "card.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "       # there in the first place\n            image = image.decode(\"latin1\")\n\n        card._image = _pad(image)\n        card._verified = False\n        return card\n\n    @classmethod\n    def normalize_keyword(cls, keyword):\n        \"\"\"\n        `classmethod` to convert a keyword value that may contain a\n        field-specifier to uppercase.  The effect is to raise the key to\n        uppercase and leave the field specifier in its original case.\n\n        Parameters\n        ----------\n        keyword : or str\n            A keyword value or a ``keyword.field-specifier`` value\n        \"\"\"\n        # Test first for the most common case: a standard FITS keyword provided\n        # in standard all-caps\n        if len(keyword) <= KEYWORD_LENGTH and cls._keywd_FSC_RE.match(keyword):\n            return keyword\n\n        # Test if this is a record-valued keyword\n        match = cls._rvkc_keyword_name_RE.match(keyword)\n\n        if match:\n            return \".\".join(\n                (match.group(\"keyword\").strip().upper(), match.group(\"field_specifier\"))\n            )\n        elif len(keyword) > 9 and keyword[:9].upper() == \"HIERARCH \":\n            # Remove 'HIERARCH' from HIERARCH keywords; this could lead to\n            # ambiguity if there is actually a keyword card containing\n            # \"HIERARCH HIERARCH\", but shame on you if you do that.\n            return keyword[9:].strip().upper()\n        else:\n            # A normal FITS keyword, but provided in non-standard case\n            return keyword.strip().upper()\n\n    def _check_if_rvkc(self, *args):\n        \"\"\"\n        Determine whether or not the card is a record-valued keyword card.\n\n        If one argument is given, that argument is treated as a full card image\n        and parsed as such.  If two arguments are given, the first is treated\n        as the card keyword (including the field-specifier if the card is\n        intended as a RVKC), and the second as the card value OR the first value\n        can be the base keyword, and "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  encode_ascii(r\"(?:(?P<valid>END {77}) *)|(?P<invalid>END$|END {0,76}[^A-Z0-9_-])\")\n)\n\n\n# According to the FITS standard the only characters that may appear in a\n# header record are the restricted ASCII chars from 0x20 through 0x7E.\nVALID_HEADER_CHARS = set(map(chr, range(0x20, 0x7F)))\nEND_CARD = \"END\" + \" \" * 77\n\n_commentary_keywords = Card._commentary_keywords\n\n__doctest_skip__ = [\n    \"Header\",\n    \"Header.comments\",\n    \"Header.fromtextfile\",\n    \"Header.totextfile\",\n    \"Header.update\",\n]\n\n\nclass Header:\n    \"\"\"\n    FITS header class.  This class exposes both a dict-like interface and a\n    list-like interface to FITS headers.\n\n    The header may be indexed by keyword and, like a dict, the associated value\n    will be returned.  When the header contains cards with duplicate keywords,\n    only the value of the first card with the given keyword will be returned.\n    It is also possible to use a 2-tuple as the index in the form (keyword,\n    n)--this returns the n-th value with that keyword, in the case where there\n    are duplicate keywords.\n\n    For example::\n\n        >>> header['NAXIS']\n        0\n        >>> header[('FOO', 1)]  # Return the value of the second FOO keyword\n        'foo'\n\n    The header may also be indexed by card number::\n\n        >>> header[0]  # Return the value of the first card in the header\n        'T'\n\n    Commentary keywords such as HISTORY and COMMENT are special cases: When\n    indexing the Header object with either 'HISTORY' or 'COMMENT' a list of all\n    the HISTORY/COMMENT values is returned::\n\n        >>> header['HISTORY']\n        This is the first history entry in this header.\n        This is the second history entry in this header.\n        ...\n\n    See the Astropy documentation for more details on working with headers.\n\n    Notes\n    -----\n    Although FITS keywords must be exclusively upper case, retrieving an item\n    in a `Header` object is case insensitive.\n    \"\"\"\n\n    def __init__(self, cards=[], copy=False):\n        \"\"\"\n  "}, {"start_line": 83000, "end_line": 85000, "belongs_to": {"file_name": "test_header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n        h[\"BAR\"] = np.bool_(False)\n        assert h[\"FOO\"] is True\n        assert h[\"BAR\"] is False\n        assert str(h.cards[\"FOO\"]) == fooimg\n        assert str(h.cards[\"BAR\"]) == barimg\n\n        h = fits.Header()\n        h.append(fits.Card.fromstring(fooimg))\n        h.append(fits.Card.fromstring(barimg))\n        assert h[\"FOO\"] is True\n        assert h[\"BAR\"] is False\n        assert str(h.cards[\"FOO\"]) == fooimg\n        assert str(h.cards[\"BAR\"]) == barimg\n\n    def test_header_method_keyword_normalization(self):\n        \"\"\"\n        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/149\n\n        Basically ensures that all public Header methods are case-insensitive\n        w.r.t. keywords.\n\n        Provides a reasonably comprehensive test of several methods at once.\n        \"\"\"\n\n        h = fits.Header([(\"abC\", 1), (\"Def\", 2), (\"GeH\", 3)])\n        assert list(h) == [\"ABC\", \"DEF\", \"GEH\"]\n        assert \"abc\" in h\n        assert \"dEf\" in h\n\n        assert h[\"geh\"] == 3\n\n        # Case insensitivity of wildcards\n        assert len(h[\"g*\"]) == 1\n\n        h[\"aBc\"] = 2\n        assert h[\"abc\"] == 2\n        # ABC already existed so assigning to aBc should not have added any new\n        # cards\n        assert len(h) == 3\n\n        del h[\"gEh\"]\n        assert list(h) == [\"ABC\", \"DEF\"]\n        assert len(h) == 2\n        assert h.get(\"def\") == 2\n\n        h.set(\"Abc\", 3)\n        assert h[\"ABC\"] == 3\n        h.set(\"gEh\", 3, before=\"Abc\")\n        assert list(h) == [\"GEH\", \"ABC\", \"DEF\"]\n\n        assert h.pop(\"abC\") == 3\n        assert len(h) == 2\n\n        assert h.setdefault(\"def\", 3) == 2\n        assert len(h) == 2\n        assert h.setdefault(\"aBc\", 1) == 1\n        assert len(h) == 3\n        assert list(h) == [\"GEH\", \"DEF\", \"ABC\"]\n\n        h.update({\"GeH\": 1, \"iJk\": 4})\n        assert len(h) == 4\n        assert list(h) == [\"GEH\", \"DEF\", \"ABC\", \"IJK\"]\n        assert h[\"GEH\"] == 1\n\n        assert h.count(\"ijk\") == 1\n        assert h.index(\"ijk\") == 3\n\n        h.rem"}, {"start_line": 82000, "end_line": 84000, "belongs_to": {"file_name": "test_header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "0\")\n        assert str(c) == _pad(\"APERTURE= +0.000000000000E+000\")\n        assert c.value == 0.0\n        c = fits.Card.fromstring(\"APERTURE= 0.000000000000E+000\")\n        assert str(c) == _pad(\"APERTURE= 0.000000000000E+000\")\n        assert c.value == 0.0\n        c = fits.Card.fromstring(\"APERTURE= 017\")\n        assert str(c) == _pad(\"APERTURE= 017\")\n        assert c.value == 17\n\n    def test_assign_boolean(self):\n        \"\"\"\n        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/123\n\n        Tests assigning Python and Numpy boolean values to keyword values.\n        \"\"\"\n\n        fooimg = _pad(\"FOO     =                    T\")\n        barimg = _pad(\"BAR     =                    F\")\n        h = fits.Header()\n        h[\"FOO\"] = True\n        h[\"BAR\"] = False\n        assert h[\"FOO\"] is True\n        assert h[\"BAR\"] is False\n        assert str(h.cards[\"FOO\"]) == fooimg\n        assert str(h.cards[\"BAR\"]) == barimg\n\n        h = fits.Header()\n        h[\"FOO\"] = np.bool_(True)\n        h[\"BAR\"] = np.bool_(False)\n        assert h[\"FOO\"] is True\n        assert h[\"BAR\"] is False\n        assert str(h.cards[\"FOO\"]) == fooimg\n        assert str(h.cards[\"BAR\"]) == barimg\n\n        h = fits.Header()\n        h.append(fits.Card.fromstring(fooimg))\n        h.append(fits.Card.fromstring(barimg))\n        assert h[\"FOO\"] is True\n        assert h[\"BAR\"] is False\n        assert str(h.cards[\"FOO\"]) == fooimg\n        assert str(h.cards[\"BAR\"]) == barimg\n\n    def test_header_method_keyword_normalization(self):\n        \"\"\"\n        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/149\n\n        Basically ensures that all public Header methods are case-insensitive\n        w.r.t. keywords.\n\n        Provides a reasonably comprehensive test of several methods at once.\n        \"\"\"\n\n        h = fits.Header([(\"abC\", 1), (\"Def\", 2), (\"GeH\", 3)])\n        assert list(h) == [\"ABC\", \"DEF\", \"GEH\"]\n        assert \"abc\" in h\n        assert \"dEf\" in h\n\n        assert h[\"geh\"] == 3"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "card.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "r()\n            if len(keyword) <= KEYWORD_LENGTH and self._keywd_FSC_RE.match(\n                keyword_upper\n            ):\n                # For keywords with length > 8 they will be HIERARCH cards,\n                # and can have arbitrary case keywords\n                if keyword_upper == \"END\":\n                    raise ValueError(\"Keyword 'END' not allowed.\")\n                keyword = keyword_upper\n            elif self._keywd_hierarch_RE.match(keyword):\n                # In prior versions of PyFITS (*) HIERARCH cards would only be\n                # created if the user-supplied keyword explicitly started with\n                # 'HIERARCH '.  Now we will create them automatically for long\n                # keywords, but we still want to support the old behavior too;\n                # the old behavior makes it possible to create HIERARCH cards\n                # that would otherwise be recognized as RVKCs\n                # (*) This has never affected Astropy, because it was changed\n                # before PyFITS was merged into Astropy!\n                self._hierarch = True\n                self._value_indicator = HIERARCH_VALUE_INDICATOR\n\n                if keyword_upper[:9] == \"HIERARCH \":\n                    # The user explicitly asked for a HIERARCH card, so don't\n                    # bug them about it...\n                    keyword = keyword[9:].strip()\n                else:\n                    # We'll gladly create a HIERARCH card, but a warning is\n                    # also displayed\n                    warnings.warn(\n                        f\"Keyword name {keyword!r} is greater than 8 characters or \"\n                        \"contains characters not allowed by the FITS \"\n                        \"standard; a HIERARCH card will be created.\",\n                        VerifyWarning,\n                    )\n            else:\n                raise ValueError(f\"Illegal keyword name: {keyword!r}.\")\n            self._keyword = keyword\n            self._modified = True\n "}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "test_header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "iginal.\n    copied_header[\"a\"] = 0\n    assert original_header[\"a\"] == 1\n\n\ndef test_init_with_header():\n    \"\"\"Make sure that creating a Header from another Header makes a copy if\n    copy is True.\"\"\"\n\n    original_header = fits.Header([(\"a\", 10)])\n    new_header = fits.Header(original_header, copy=True)\n    original_header[\"a\"] = 20\n    assert new_header[\"a\"] == 10\n\n    new_header[\"a\"] = 0\n    assert original_header[\"a\"] == 20\n\n\ndef test_init_with_dict():\n    dict1 = {\"a\": 11, \"b\": 12, \"c\": 13, \"d\": 14, \"e\": 15}\n    h1 = fits.Header(dict1)\n    for i, expected in dict1.items():\n        assert h1[i] == expected\n\n\ndef test_init_with_ordereddict():\n    # Create a list of tuples. Each tuple consisting of a letter and the number\n    list1 = [(i, j) for j, i in enumerate(\"abcdefghijklmnopqrstuvwxyz\")]\n    # Create an ordered dictionary and a header from this dictionary\n    dict1 = collections.OrderedDict(list1)\n    h1 = fits.Header(dict1)\n    # Check that the order is preserved of the initial list\n    assert all(h1[val] == list1[i][1] for i, val in enumerate(h1))\n\n\nclass TestHeaderFunctions(FitsTestCase):\n    \"\"\"Test Header and Card objects.\"\"\"\n\n    def test_rename_keyword(self):\n        \"\"\"Test renaming keyword with rename_keyword.\"\"\"\n        header = fits.Header([(\"A\", \"B\", \"C\"), (\"D\", \"E\", \"F\")])\n        header.rename_keyword(\"A\", \"B\")\n        assert \"A\" not in header\n        assert \"B\" in header\n        assert header[0] == \"B\"\n        assert header[\"B\"] == \"B\"\n        assert header.comments[\"B\"] == \"C\"\n\n    @pytest.mark.parametrize(\"key\", [\"A\", \"a\"])\n    def test_indexing_case(self, key):\n        \"\"\"Check that indexing is case insensitive\"\"\"\n        header = fits.Header([(\"A\", \"B\", \"C\"), (\"D\", \"E\", \"F\")])\n        assert key in header\n        assert header[key] == \"B\"\n        assert header.get(key) == \"B\"\n        assert header.index(key) == 0\n        assert header.comments[key] == \"C\"\n        assert header.count(key) == 1\n        header.remove(key, ignore_missing=False)"}, {"start_line": 69000, "end_line": 71000, "belongs_to": {"file_name": "header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rd name from its index.\n        key = self.header._keys[key]\n        # then we get the card from the _BasicHeader._cards list, or parse it\n        # if needed.\n        try:\n            return self.header._cards[key]\n        except KeyError:\n            cardstr = self.header._raw_cards[key]\n            card = Card.fromstring(cardstr)\n            self.header._cards[key] = card\n            return card\n\n\nclass _BasicHeader(collections.abc.Mapping):\n    \"\"\"This class provides a fast header parsing, without all the additional\n    features of the Header class. Here only standard keywords are parsed, no\n    support for CONTINUE, HIERARCH, COMMENT, HISTORY, or rvkc.\n\n    The raw card images are stored and parsed only if needed. The idea is that\n    to create the HDU objects, only a small subset of standard cards is needed.\n    Once a card is parsed, which is deferred to the Card class, the Card object\n    is kept in a cache. This is useful because a small subset of cards is used\n    a lot in the HDU creation process (NAXIS, XTENSION, ...).\n\n    \"\"\"\n\n    def __init__(self, cards):\n        # dict of (keywords, card images)\n        self._raw_cards = cards\n        self._keys = list(cards.keys())\n        # dict of (keyword, Card object) storing the parsed cards\n        self._cards = {}\n        # the _BasicHeaderCards object allows to access Card objects from\n        # keyword indices\n        self.cards = _BasicHeaderCards(self)\n\n        self._modified = False\n\n    def __getitem__(self, key):\n        if isinstance(key, numbers.Integral):\n            key = self._keys[key]\n\n        try:\n            return self._cards[key].value\n        except KeyError:\n            # parse the Card and store it\n            cardstr = self._raw_cards[key]\n            self._cards[key] = card = Card.fromstring(cardstr)\n            return card.value\n\n    def __len__(self):\n        return len(self._raw_cards)\n\n    def __iter__(self):\n        return iter(self._raw_cards)\n\n    def index(self, keyword):\n     "}, {"start_line": 107000, "end_line": 109000, "belongs_to": {"file_name": "test_header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "   assert c.field_specifier == \"NAXIS1\"\n        assert c.keyword == \"DP1.NAXIS1\"\n        assert c.value == 2.0\n        assert c.comment == \"A comment\"\n        assert str(c).rstrip() == \"DP1     = 'NAXIS1: 2' / A comment\"\n\n    def test_field_specifier_case_senstivity(self):\n        \"\"\"\n        The keyword portion of an RVKC should still be case-insensitive, but\n        the field-specifier portion should be case-sensitive.\n        \"\"\"\n\n        header = fits.Header()\n        header.set(\"abc.def\", 1)\n        header.set(\"abc.DEF\", 2)\n        assert header[\"abc.def\"] == 1\n        assert header[\"ABC.def\"] == 1\n        assert header[\"aBc.def\"] == 1\n        assert header[\"ABC.DEF\"] == 2\n        assert \"ABC.dEf\" not in header\n\n    def test_get_rvkc_by_index(self):\n        \"\"\"\n        Returning a RVKC from a header via index lookup should return the\n        float value of the card.\n        \"\"\"\n\n        assert self._test_header[0] == 2.0\n        assert isinstance(self._test_header[0], float)\n        assert self._test_header[1] == 1.0\n        assert isinstance(self._test_header[1], float)\n\n    def test_get_rvkc_by_keyword(self):\n        \"\"\"\n        Returning a RVKC just via the keyword name should return the full value\n        string of the first card with that keyword.\n\n        This test was changed to reflect the requirement in ticket\n        https://aeon.stsci.edu/ssb/trac/pyfits/ticket/184--previously it required\n        _test_header['DP1'] to return the parsed float value.\n        \"\"\"\n\n        assert self._test_header[\"DP1\"] == \"NAXIS: 2\"\n\n    def test_get_rvkc_by_keyword_and_field_specifier(self):\n        \"\"\"\n        Returning a RVKC via the full keyword/field-specifier combination\n        should return the floating point value associated with the RVKC.\n        \"\"\"\n\n        assert self._test_header[\"DP1.NAXIS\"] == 2.0\n        assert isinstance(self._test_header[\"DP1.NAXIS\"], float)\n        assert self._test_header[\"DP1.AUX.1.COEFF.1\"] == 0.00048828125\n\n    def test_access_"}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "card.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " blank card\n            return self._image == BLANK_CARD\n\n        # If the keyword, value, and comment are all empty (for self.value\n        # explicitly check that it is a string value, since a blank value is\n        # returned as '')\n        return (\n            not self.keyword\n            and (isinstance(self.value, str) and not self.value)\n            and not self.comment\n        )\n\n    @classmethod\n    def fromstring(cls, image):\n        \"\"\"\n        Construct a `Card` object from a (raw) string. It will pad the string\n        if it is not the length of a card image (80 columns).  If the card\n        image is longer than 80 columns, assume it contains ``CONTINUE``\n        card(s).\n        \"\"\"\n        card = cls()\n        if isinstance(image, bytes):\n            # FITS supports only ASCII, but decode as latin1 and just take all\n            # bytes for now; if it results in mojibake due to e.g. UTF-8\n            # encoded data in a FITS header that's OK because it shouldn't be\n            # there in the first place\n            image = image.decode(\"latin1\")\n\n        card._image = _pad(image)\n        card._verified = False\n        return card\n\n    @classmethod\n    def normalize_keyword(cls, keyword):\n        \"\"\"\n        `classmethod` to convert a keyword value that may contain a\n        field-specifier to uppercase.  The effect is to raise the key to\n        uppercase and leave the field specifier in its original case.\n\n        Parameters\n        ----------\n        keyword : or str\n            A keyword value or a ``keyword.field-specifier`` value\n        \"\"\"\n        # Test first for the most common case: a standard FITS keyword provided\n        # in standard all-caps\n        if len(keyword) <= KEYWORD_LENGTH and cls._keywd_FSC_RE.match(keyword):\n            return keyword\n\n        # Test if this is a record-valued keyword\n        match = cls._rvkc_keyword_name_RE.match(keyword)\n\n        if match:\n            return \".\".join(\n                (match.group(\"keyword\""}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "header.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/fits", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see PYFITS.rst\n\nimport collections\nimport copy\nimport itertools\nimport numbers\nimport os\nimport re\nimport warnings\n\nimport numpy as np\n\nfrom astropy.utils.exceptions import AstropyUserWarning\n\nfrom ._utils import parse_header\nfrom .card import KEYWORD_LENGTH, UNDEFINED, Card, _pad\nfrom .file import _File\nfrom .util import (\n    decode_ascii,\n    encode_ascii,\n    fileobj_closed,\n    fileobj_is_binary,\n    path_like,\n)\n\nBLOCK_SIZE = 2880  # the FITS block size\n\n# This regular expression can match a *valid* END card which just consists of\n# the string 'END' followed by all spaces, or an *invalid* end card which\n# consists of END, followed by any character that is *not* a valid character\n# for a valid FITS keyword (that is, this is not a keyword like 'ENDER' which\n# starts with 'END' but is not 'END'), followed by any arbitrary bytes.  An\n# invalid end card may also consist of just 'END' with no trailing bytes.\nHEADER_END_RE = re.compile(\n    encode_ascii(r\"(?:(?P<valid>END {77}) *)|(?P<invalid>END$|END {0,76}[^A-Z0-9_-])\")\n)\n\n\n# According to the FITS standard the only characters that may appear in a\n# header record are the restricted ASCII chars from 0x20 through 0x7E.\nVALID_HEADER_CHARS = set(map(chr, range(0x20, 0x7F)))\nEND_CARD = \"END\" + \" \" * 77\n\n_commentary_keywords = Card._commentary_keywords\n\n__doctest_skip__ = [\n    \"Header\",\n    \"Header.comments\",\n    \"Header.fromtextfile\",\n    \"Header.totextfile\",\n    \"Header.update\",\n]\n\n\nclass Header:\n    \"\"\"\n    FITS header class.  This class exposes both a dict-like interface and a\n    list-like interface to FITS headers.\n\n    The header may be indexed by keyword and, like a dict, the associated value\n    will be returned.  When the header contains cards with duplicate keywords,\n    only the value of the first card with the given keyword will be returned.\n    It is also possible to use a 2-tuple as the index in the form (keyword,\n    n)--this returns the n-th value with that"}], "retrieved_count": 10, "cost_time": 0.3451423645019531}
{"question": "Where in the control flow of the Simple Imaging Polynomial distortion correction method does the transformation of input coordinates pass through intermediate shifted values before polynomial distortion application?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 51000, "end_line": 53000, "belongs_to": {"file_name": "polynomial.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "index(attr)][0]\n        return mat\n\n    def _eval_sip(self, x, y, coef):\n        x = np.asarray(x, dtype=np.float64)\n        y = np.asarray(y, dtype=np.float64)\n        if self.coeff_prefix == \"A\":\n            result = np.zeros(x.shape)\n        else:\n            result = np.zeros(y.shape)\n\n        for i in range(coef.shape[0]):\n            for j in range(coef.shape[1]):\n                if 1 < i + j < self.order + 1:\n                    result = result + coef[i, j] * x**i * y**j\n        return result\n\n\nclass SIP(Model):\n    \"\"\"\n    Simple Imaging Polynomial (SIP) model.\n\n    The SIP convention is used to represent distortions in FITS image headers.\n    See [1]_ for a description of the SIP convention.\n\n    Parameters\n    ----------\n    crpix : list or (2,) ndarray\n        CRPIX values\n    a_order : int\n        SIP polynomial order for first axis\n    b_order : int\n        SIP order for second axis\n    a_coeff : dict\n        SIP coefficients for first axis\n    b_coeff : dict\n        SIP coefficients for the second axis\n    ap_order : int\n        order for the inverse transformation (AP coefficients)\n    bp_order : int\n        order for the inverse transformation (BP coefficients)\n    ap_coeff : dict\n        coefficients for the inverse transform\n    bp_coeff : dict\n        coefficients for the inverse transform\n\n    References\n    ----------\n    .. [1] `David Shupe, et al, ADASS, ASP Conference Series, Vol. 347, 2005\n        <https://ui.adsabs.harvard.edu/abs/2005ASPC..347..491S>`_\n    \"\"\"\n\n    n_inputs = 2\n    n_outputs = 2\n\n    _separable = False\n\n    def __init__(\n        self,\n        crpix,\n        a_order,\n        b_order,\n        a_coeff={},\n        b_coeff={},\n        ap_order=None,\n        bp_order=None,\n        ap_coeff={},\n        bp_coeff={},\n        n_models=None,\n        model_set_axis=None,\n        name=None,\n        meta=None,\n    ):\n        self._crpix = crpix\n        self._a_order = a_order\n        self._b_order = b_order\n        self._a_coeff = a_coe"}, {"start_line": 63000, "end_line": 65000, "belongs_to": {"file_name": "wcs.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/wcs", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "  # https://github.com/astropy/astropy/pull/2373\n        #\n        #\n        #                  ### Background ###\n        #\n        #\n        # I will refer here to the [SIP Paper]\n        # (http://fits.gsfc.nasa.gov/registry/sip/SIP_distortion_v1_0.pdf).\n        # According to this paper, the effect of distortions as\n        # described in *their* equation (1) is:\n        #\n        # (1)   x = CD*(u+f(u)),\n        #\n        # where `x` is a *vector* of \"intermediate spherical\n        # coordinates\" (equivalent to (x,y) in the paper) and `u`\n        # is a *vector* of \"pixel coordinates\", and `f` is a vector\n        # function describing geometrical distortions\n        # (see equations 2 and 3 in SIP Paper.\n        # However, I prefer to use `w` for \"intermediate world\n        # coordinates\", `x` for pixel coordinates, and assume that\n        # transformation `W` performs the **linear**\n        # (CD matrix + projection onto celestial sphere) part of the\n        # conversion from pixel coordinates to world coordinates.\n        # Then we can re-write (1) as:\n        #\n        # (2)   w = W*(x+f(x)) = T(x)\n        #\n        # In `astropy.wcs.WCS` transformation `W` is represented by\n        # the `wcs_pix2world` member, while the combined (\"total\")\n        # transformation (linear part + distortions) is performed by\n        # `all_pix2world`. Below I summarize the notations and their\n        # equivalents in `astropy.wcs.WCS`:\n        #\n        # | Equation term | astropy.WCS/meaning          |\n        # | ------------- | ---------------------------- |\n        # | `x`           | pixel coordinates            |\n        # | `w`           | world coordinates            |\n        # | `W`           | `wcs_pix2world()`            |\n        # | `W^{-1}`      | `wcs_world2pix()`            |\n        # | `T`           | `all_pix2world()`            |\n        # | `x+f(x)`      | `pix2foc()`                  |\n        #\n        #\n        #      ### Direct Solving of Equation ("}, {"start_line": 60000, "end_line": 62000, "belongs_to": {"file_name": "docstrings.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/wcs", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " must be a sequence of tuples of the form (*i*, *m*,\n    *value*):\n\n    - *i*: int.  Axis number, as in ``PVi_ma``, (i.e. 1-relative)\n\n    - *m*: int.  Parameter number, as in ``PVi_ma``, (i.e. 0-relative)\n\n    - *value*: float.  Parameter value.\n\nSee also\n--------\nastropy.wcs.Wcsprm.get_pv\n\"\"\"\n\nsip = \"\"\"\nGet/set the `~astropy.wcs.Sip` object for performing `SIP`_ distortion\ncorrection.\n\"\"\"\n\nSip = \"\"\"\nSip(*a, b, ap, bp, crpix*)\n\nThe `~astropy.wcs.Sip` class performs polynomial distortion correction\nusing the `SIP`_ convention in both directions.\n\nParameters\n----------\na : ndarray\n    The ``A_i_j`` polynomial for pixel to focal plane transformation as ``double array[m+1][m+1]``.\n    Its size must be (*m* + 1, *m* + 1) where *m* = ``A_ORDER``.\n\nb : ndarray\n    The ``B_i_j`` polynomial for pixel to focal plane transformation as ``double array[m+1][m+1]``.\n    Its size must be (*m* + 1, *m* + 1) where *m* = ``B_ORDER``.\n\nap : ndarray\n    The ``AP_i_j`` polynomial for pixel to focal plane transformation as ``double array[m+1][m+1]``.\n    Its size must be (*m* + 1, *m* + 1) where *m* = ``AP_ORDER``.\n\nbp : ndarray\n    The ``BP_i_j`` polynomial for pixel to focal plane transformation as ``double array[m+1][m+1]``.\n    Its size must be (*m* + 1, *m* + 1) where *m* = ``BP_ORDER``.\n\ncrpix : ndarray\n    The reference pixel as ``double array[2]``.\n\nNotes\n-----\nShupe, D. L., M. Moshir, J. Li, D. Makovoz and R. Narron.  2005.\n\"The SIP Convention for Representing Distortion in FITS Image\nHeaders.\"  ADASS XIV.\n\"\"\"\n\nsip_foc2pix = f\"\"\"\nsip_foc2pix(*foccrd, origin*) -> ``double array[ncoord][nelem]``\n\nConvert focal plane coordinates to pixel coordinates using the `SIP`_\npolynomial distortion convention.\n\nParameters\n----------\nfoccrd : ndarray\n    Array of focal plane coordinates as ``double array[ncoord][nelem]``.\n\n{ORIGIN()}\n\nReturns\n-------\npixcrd : ndarray\n    Returns an array of pixel coordinates as ``double array[ncoord][nelem]``.\n\nRaises\n------\nMemoryError\n    Memory allocation f"}, {"start_line": 50000, "end_line": 52000, "belongs_to": {"file_name": "polynomial.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     names.append(f\"{coeff_prefix}_{i}_{0}\")\n        for i in range(2, self.order + 1):\n            names.append(f\"{coeff_prefix}_{0}_{i}\")\n        for i in range(1, self.order):\n            for j in range(1, self.order):\n                if i + j < self.order + 1:\n                    names.append(f\"{coeff_prefix}_{i}_{j}\")\n        return tuple(names)\n\n    def _coeff_matrix(self, coeff_prefix, coeffs):\n        mat = np.zeros((self.order + 1, self.order + 1))\n        for i in range(2, self.order + 1):\n            attr = f\"{coeff_prefix}_{i}_{0}\"\n            mat[i, 0] = coeffs[self.param_names.index(attr)][0]\n        for i in range(2, self.order + 1):\n            attr = f\"{coeff_prefix}_{0}_{i}\"\n            mat[0, i] = coeffs[self.param_names.index(attr)][0]\n        for i in range(1, self.order):\n            for j in range(1, self.order):\n                if i + j < self.order + 1:\n                    attr = f\"{coeff_prefix}_{i}_{j}\"\n                    mat[i, j] = coeffs[self.param_names.index(attr)][0]\n        return mat\n\n    def _eval_sip(self, x, y, coef):\n        x = np.asarray(x, dtype=np.float64)\n        y = np.asarray(y, dtype=np.float64)\n        if self.coeff_prefix == \"A\":\n            result = np.zeros(x.shape)\n        else:\n            result = np.zeros(y.shape)\n\n        for i in range(coef.shape[0]):\n            for j in range(coef.shape[1]):\n                if 1 < i + j < self.order + 1:\n                    result = result + coef[i, j] * x**i * y**j\n        return result\n\n\nclass SIP(Model):\n    \"\"\"\n    Simple Imaging Polynomial (SIP) model.\n\n    The SIP convention is used to represent distortions in FITS image headers.\n    See [1]_ for a description of the SIP convention.\n\n    Parameters\n    ----------\n    crpix : list or (2,) ndarray\n        CRPIX values\n    a_order : int\n        SIP polynomial order for first axis\n    b_order : int\n        SIP order for second axis\n    a_coeff : dict\n        SIP coefficients for first axis\n    b_coeff : dict\n        SIP c"}, {"start_line": 97000, "end_line": 99000, "belongs_to": {"file_name": "wcs.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/wcs", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "      Convert pixel coordinates to focal plane coordinates using the\n        `SIP`_ polynomial distortion convention and `distortion\n        paper`_ table-lookup correction.\n\n        The output is in absolute pixel coordinates, not relative to\n        ``CRPIX``.\n\n        Parameters\n        ----------\n\n        {docstrings.TWO_OR_MORE_ARGS(\"2\", 8)}\n\n        Returns\n        -------\n\n        {docstrings.RETURNS(\"focal coordinates\", 8)}\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.\n\n        ValueError\n            Invalid coordinate transformation parameters.\n        \"\"\"\n\n    def p4_pix2foc(self, *args):\n        return self._array_converter(self._p4_pix2foc, None, *args)\n\n    p4_pix2foc.__doc__ = f\"\"\"\n        Convert pixel coordinates to focal plane coordinates using\n        `distortion paper`_ table-lookup correction.\n\n        The output is in absolute pixel coordinates, not relative to\n        ``CRPIX``.\n\n        Parameters\n        ----------\n\n        {docstrings.TWO_OR_MORE_ARGS(\"2\", 8)}\n\n        Returns\n        -------\n\n        {docstrings.RETURNS(\"focal coordinates\", 8)}\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.\n\n        ValueError\n            Invalid coordinate transformation parameters.\n        \"\"\"\n\n    def det2im(self, *args):\n        return self._array_converter(self._det2im, None, *args)\n\n    det2im.__doc__ = f\"\"\"\n        Convert detector coordinates to image plane coordinates using\n        `distortion paper`_ table-lookup correction.\n\n        The output is in absolute pixel coordinates, not relative to\n        ``CRPIX``.\n\n        Parameters\n        ----------\n\n        {docstrings.TWO_OR_MORE_ARGS(\"2\", 8)}\n\n        Returns\n        -------\n\n        {docstrings.RETURNS(\"pixel coordinates\", 8)}\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.\n\n        ValueError\n            Invalid coordinate transformation parameters.\n        \"\"\"\n\n    d"}, {"start_line": 54000, "end_line": 56000, "belongs_to": {"file_name": "polynomial.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_str__(self):\n        parts = [f\"Model: {self.__class__.__name__}\"]\n        for model in [self.shift_a, self.shift_b, self.sip1d_a, self.sip1d_b]:\n            parts.append(indent(str(model), 4 * \" \"))\n            parts.append(\"\")\n\n        return \"\\n\".join(parts)\n\n    @property\n    def inverse(self):\n        if self._ap_order is not None and self._bp_order is not None:\n            return InverseSIP(\n                self._ap_order, self._bp_order, self._ap_coeff, self._bp_coeff\n            )\n        else:\n            raise NotImplementedError(\"SIP inverse coefficients are not available.\")\n\n    def evaluate(self, x, y):\n        u = self.shift_a.evaluate(x, *self.shift_a.param_sets)\n        v = self.shift_b.evaluate(y, *self.shift_b.param_sets)\n        f = self.sip1d_a.evaluate(u, v, *self.sip1d_a.param_sets)\n        g = self.sip1d_b.evaluate(u, v, *self.sip1d_b.param_sets)\n        return f, g\n\n\nclass InverseSIP(Model):\n    \"\"\"\n    Inverse Simple Imaging Polynomial.\n\n    Parameters\n    ----------\n    ap_order : int\n        order for the inverse transformation (AP coefficients)\n    bp_order : int\n        order for the inverse transformation (BP coefficients)\n    ap_coeff : dict\n        coefficients for the inverse transform\n    bp_coeff : dict\n        coefficients for the inverse transform\n\n    \"\"\"\n\n    n_inputs = 2\n    n_outputs = 2\n\n    _separable = False\n\n    def __init__(\n        self,\n        ap_order,\n        bp_order,\n        ap_coeff={},\n        bp_coeff={},\n        n_models=None,\n        model_set_axis=None,\n        name=None,\n        meta=None,\n    ):\n        self._ap_order = ap_order\n        self._bp_order = bp_order\n        self._ap_coeff = ap_coeff\n        self._bp_coeff = bp_coeff\n\n        # define the 0th term in order to use Polynomial2D\n        ap_coeff.setdefault(\"AP_0_0\", 0)\n        bp_coeff.setdefault(\"BP_0_0\", 0)\n\n        ap_coeff_params = {k.replace(\"AP_\", \"c\"): v for k, v in ap_coeff.items()}\n        bp_coeff_params = {k.replace(\"BP_\", \"c\"): v for "}, {"start_line": 53000, "end_line": 55000, "belongs_to": {"file_name": "polynomial.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ff\n        self._b_coeff = b_coeff\n        self._ap_order = ap_order\n        self._bp_order = bp_order\n        self._ap_coeff = ap_coeff\n        self._bp_coeff = bp_coeff\n        self.shift_a = Shift(-crpix[0])\n        self.shift_b = Shift(-crpix[1])\n        self.sip1d_a = _SIP1D(\n            a_order,\n            coeff_prefix=\"A\",\n            n_models=n_models,\n            model_set_axis=model_set_axis,\n            **a_coeff,\n        )\n        self.sip1d_b = _SIP1D(\n            b_order,\n            coeff_prefix=\"B\",\n            n_models=n_models,\n            model_set_axis=model_set_axis,\n            **b_coeff,\n        )\n        super().__init__(\n            n_models=n_models, model_set_axis=model_set_axis, name=name, meta=meta\n        )\n        self._inputs = (\"u\", \"v\")\n        self._outputs = (\"x\", \"y\")\n\n    def __repr__(self):\n        return (\n            f\"<{self.__class__.__name__}\"\n            f\"({[self.shift_a, self.shift_b, self.sip1d_a, self.sip1d_b]!r})>\"\n        )\n\n    def __str__(self):\n        parts = [f\"Model: {self.__class__.__name__}\"]\n        for model in [self.shift_a, self.shift_b, self.sip1d_a, self.sip1d_b]:\n            parts.append(indent(str(model), 4 * \" \"))\n            parts.append(\"\")\n\n        return \"\\n\".join(parts)\n\n    @property\n    def inverse(self):\n        if self._ap_order is not None and self._bp_order is not None:\n            return InverseSIP(\n                self._ap_order, self._bp_order, self._ap_coeff, self._bp_coeff\n            )\n        else:\n            raise NotImplementedError(\"SIP inverse coefficients are not available.\")\n\n    def evaluate(self, x, y):\n        u = self.shift_a.evaluate(x, *self.shift_a.param_sets)\n        v = self.shift_b.evaluate(y, *self.shift_b.param_sets)\n        f = self.sip1d_a.evaluate(u, v, *self.sip1d_a.param_sets)\n        g = self.sip1d_b.evaluate(u, v, *self.sip1d_b.param_sets)\n        return f, g\n\n\nclass InverseSIP(Model):\n    \"\"\"\n    Inverse Simple Imaging Polynomial.\n\n    Parameters\n    ---"}, {"start_line": 61000, "end_line": 63000, "belongs_to": {"file_name": "docstrings.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/wcs", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ransformation as ``double array[m+1][m+1]``.\n    Its size must be (*m* + 1, *m* + 1) where *m* = ``AP_ORDER``.\n\nbp : ndarray\n    The ``BP_i_j`` polynomial for pixel to focal plane transformation as ``double array[m+1][m+1]``.\n    Its size must be (*m* + 1, *m* + 1) where *m* = ``BP_ORDER``.\n\ncrpix : ndarray\n    The reference pixel as ``double array[2]``.\n\nNotes\n-----\nShupe, D. L., M. Moshir, J. Li, D. Makovoz and R. Narron.  2005.\n\"The SIP Convention for Representing Distortion in FITS Image\nHeaders.\"  ADASS XIV.\n\"\"\"\n\nsip_foc2pix = f\"\"\"\nsip_foc2pix(*foccrd, origin*) -> ``double array[ncoord][nelem]``\n\nConvert focal plane coordinates to pixel coordinates using the `SIP`_\npolynomial distortion convention.\n\nParameters\n----------\nfoccrd : ndarray\n    Array of focal plane coordinates as ``double array[ncoord][nelem]``.\n\n{ORIGIN()}\n\nReturns\n-------\npixcrd : ndarray\n    Returns an array of pixel coordinates as ``double array[ncoord][nelem]``.\n\nRaises\n------\nMemoryError\n    Memory allocation failed.\n\nValueError\n    Invalid coordinate transformation parameters.\n\"\"\"\n\nsip_pix2foc = f\"\"\"\nsip_pix2foc(*pixcrd, origin*) -> ``double array[ncoord][nelem]``\n\nConvert pixel coordinates to focal plane coordinates using the `SIP`_\npolynomial distortion convention.\n\nParameters\n----------\npixcrd : ndarray\n    Array of pixel coordinates as ``double array[ncoord][nelem]``.\n\n{ORIGIN()}\n\nReturns\n-------\nfoccrd : ndarray\n    Returns an array of focal plane coordinates as ``double array[ncoord][nelem]``.\n\nRaises\n------\nMemoryError\n    Memory allocation failed.\n\nValueError\n    Invalid coordinate transformation parameters.\n\"\"\"\n\nspcfix = \"\"\"\nspcfix() -> int\n\nTranslates AIPS-convention spectral coordinate types.  {``FREQ``,\n``VELO``, ``FELO``}-{``OBS``, ``HEL``, ``LSR``} (e.g. ``FREQ-LSR``,\n``VELO-OBS``, ``FELO-HEL``)\n\nReturns\n-------\nsuccess : int\n    Returns ``0`` for success; ``-1`` if no change required.\n\"\"\"\n\nspec = \"\"\"\n``int`` (read-only) The index containing the spectral axis values.\n\"\"\"\n\nspec"}, {"start_line": 99000, "end_line": 101000, "belongs_to": {"file_name": "wcs.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/wcs", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ef sip_pix2foc(self, *args):\n        if self.sip is None:\n            if len(args) == 2:\n                return args[0]\n            elif len(args) == 3:\n                return args[:2]\n            else:\n                raise TypeError(\"Wrong number of arguments\")\n        return self._array_converter(self.sip.pix2foc, None, *args)\n\n    sip_pix2foc.__doc__ = f\"\"\"\n        Convert pixel coordinates to focal plane coordinates using the\n        `SIP`_ polynomial distortion convention.\n\n        The output is in pixel coordinates, relative to ``CRPIX``.\n\n        FITS WCS `distortion paper`_ table lookup correction is not\n        applied, even if that information existed in the FITS file\n        that initialized this :class:`~astropy.wcs.WCS` object.  To\n        correct for that, use `~astropy.wcs.WCS.pix2foc` or\n        `~astropy.wcs.WCS.p4_pix2foc`.\n\n        Parameters\n        ----------\n\n        {docstrings.TWO_OR_MORE_ARGS(\"2\", 8)}\n\n        Returns\n        -------\n\n        {docstrings.RETURNS(\"focal coordinates\", 8)}\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.\n\n        ValueError\n            Invalid coordinate transformation parameters.\n        \"\"\"\n\n    def sip_foc2pix(self, *args):\n        if self.sip is None:\n            if len(args) == 2:\n                return args[0]\n            elif len(args) == 3:\n                return args[:2]\n            else:\n                raise TypeError(\"Wrong number of arguments\")\n        return self._array_converter(self.sip.foc2pix, None, *args)\n\n    sip_foc2pix.__doc__ = f\"\"\"\n        Convert focal plane coordinates to pixel coordinates using the\n        `SIP`_ polynomial distortion convention.\n\n        FITS WCS `distortion paper`_ table lookup distortion\n        correction is not applied, even if that information existed in\n        the FITS file that initialized this `~astropy.wcs.WCS` object.\n\n        Parameters\n        ----------\n\n        {docstrings.TWO_OR_MORE_ARGS(\"2\", 8)}\n\n        Retu"}, {"start_line": 68000, "end_line": 70000, "belongs_to": {"file_name": "wcs.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/wcs", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ess (and we also\n        # set `x_0=x'`). By using `pix2foc` at each iteration instead\n        # of `all_pix2world` we get about 25% increase in performance\n        # (by not performing the linear `W` transformation at each\n        # step) and we also avoid the \"RA wrapping\" issue described\n        # above (by working in focal plane coordinates and avoiding\n        # pix->world transformations).\n        #\n        # As an added benefit, the process converges to the correct\n        # solution in just one iteration when distortions are not\n        # present (compare to\n        # https://github.com/astropy/astropy/issues/1977 and\n        # https://github.com/astropy/astropy/pull/2294): in this case\n        # `pix2foc` is the identical transformation\n        # `x_i=pix2foc(x_i)` and from equation (7) we get:\n        #\n        # x' = x_0 = wcs_world2pix(w)\n        # x_1 = x' - pix2foc(x_0) + x_0 = x' - pix2foc(x') + x' = x'\n        #     = wcs_world2pix(w) = x_0\n        # =>\n        # |x_1-x_0| = 0 < tolerance (with tolerance > 0)\n        #\n        # However, for performance reasons, it is still better to\n        # avoid iterations altogether and return the exact linear\n        # solution (`wcs_world2pix`) right-away when non-linear\n        # distortions are not present by checking that attributes\n        # `sip`, `cpdis1`, `cpdis2`, `det2im1`, and `det2im2` are\n        # *all* `None`.\n        #\n        #\n        #         ### Outline of the Algorithm ###\n        #\n        #\n        # While the proposed code is relatively long (considering\n        # the simplicity of the algorithm), this is due to: 1)\n        # checking if iterative solution is necessary at all; 2)\n        # checking for divergence; 3) re-implementation of the\n        # completely vectorized algorithm as an \"adaptive\" vectorized\n        # algorithm (for cases when some points diverge for which we\n        # want to stop iterations). In my tests, the adaptive version\n        # of the algorithm is about 50% "}], "retrieved_count": 10, "cost_time": 0.3630185127258301}
{"question": "Where is the base class for all fitters defined and which module implements constraint processing for the Sequential Least Squares Programming fitter?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 44000, "end_line": 46000, "belongs_to": {"file_name": "fitting.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "                 inplace=inplace,\n                    **kwargs,\n                )\n\n            # Stop iteration if the masked points are no longer changing (with\n            # cumulative rejection we only need to compare how many there are):\n            this_n_masked = filtered_data.mask.sum()  # (minimal overhead)\n            if this_n_masked == last_n_masked:\n                break\n            last_n_masked = this_n_masked\n\n        self.fit_info = {\"niter\": niter}\n        self.fit_info.update(getattr(self.fitter, \"fit_info\", {}))\n\n        return fitted_model, filtered_data.mask\n\n\nclass _NonLinearLSQFitter(Fitter):\n    \"\"\"\n    Base class for Non-Linear least-squares fitters.\n\n    Parameters\n    ----------\n    calc_uncertainties : bool\n        If the covariance matrix should be computed and set in the fit_info.\n        Default: False\n    use_min_max_bounds : bool\n        If set, the parameter bounds for a model will be enforced for each given\n        parameter while fitting via a simple min/max condition.\n        Default: True\n    \"\"\"\n\n    supported_constraints = [\"fixed\", \"tied\", \"bounds\"]\n    \"\"\"\n    The constraint types supported by this fitter type.\n    \"\"\"\n\n    def __init__(self, calc_uncertainties=False, use_min_max_bounds=True):\n        self.fit_info = None\n        self._calc_uncertainties = calc_uncertainties\n        self._use_min_max_bounds = use_min_max_bounds\n\n    def objective_function(self, fps, *args, fit_param_indices=None):\n        \"\"\"\n        Function to minimize.\n\n        Parameters\n        ----------\n        fps : list\n            parameters returned by the fitter\n        args : list\n            [model, [weights], [input coordinates]]\n        fit_param_indices : list, optional\n            The ``fit_param_indices`` as returned by ``model_to_fit_params``.\n            This is a list of the parameter indices being fit, so excluding any\n            tied or fixed parameters.  This can be passed in to the objective\n            function to prevent it havin"}, {"start_line": 66000, "end_line": 68000, "belongs_to": {"file_name": "fitting.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "TER,\n        acc=DEFAULT_ACC,\n        epsilon=DEFAULT_EPS,\n        estimate_jacobian=False,\n        filter_non_finite=False,\n        inplace=False,\n    ):\n        # Since there are several fitters with proper support for bounds, it\n        # is not a good idea to keep supporting the hacky bounds algorithm\n        # from LevMarLSQFitter here, and better to communicate with users\n        # that they should use another fitter. Once we remove the deprecation,\n        # we should update ``supported_constraints`` and change ``True`` to\n        # ``False`` in the call to ``super().__init__`` above.\n        if model.has_bounds:\n            warnings.warn(\n                \"Using LMLSQFitter for models with bounds is now \"\n                \"deprecated since astropy 7.0. We recommend you use another non-linear \"\n                \"fitter such as TRFLSQFitter or DogBoxLSQFitter instead \"\n                \"as these have full support for fitting models with \"\n                \"bounds\",\n                AstropyDeprecationWarning,\n                stacklevel=2,\n            )\n        return super().__call__(\n            model,\n            x,\n            y,\n            z=z,\n            weights=weights,\n            maxiter=maxiter,\n            acc=acc,\n            epsilon=epsilon,\n            estimate_jacobian=estimate_jacobian,\n            filter_non_finite=filter_non_finite,\n            inplace=inplace,\n        )\n\n\nclass SLSQPLSQFitter(Fitter):\n    \"\"\"\n    Sequential Least Squares Programming (SLSQP) optimization algorithm and\n    least squares statistic.\n\n    Raises\n    ------\n    ModelLinearityError\n        A linear model is passed to a nonlinear fitter\n\n    Notes\n    -----\n    See also the `~astropy.modeling.optimizers.SLSQP` optimizer.\n\n    \"\"\"\n\n    supported_constraints = SLSQP.supported_constraints\n\n    def __init__(self):\n        super().__init__(optimizer=SLSQP, statistic=leastsquare)\n        self.fit_info = {}\n\n    @fitter_unit_support\n    def __call__(\n        self,\n        model,"}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "fitting.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "nally we add back units to the parameters\n                if add_back_units:\n                    model_new = model_new.with_units_from_data(**rename_data)\n                return model_new\n\n            else:\n                raise NotImplementedError(\n                    \"This model does not support being fit to data with units.\"\n                )\n\n        else:\n            return func(self, model, x, y, z=z, **kwargs)\n\n    return wrapper\n\n\nclass Fitter:\n    \"\"\"\n    Base class for all fitters.\n\n    Parameters\n    ----------\n    optimizer : callable\n        A callable implementing an optimization algorithm\n    statistic : callable\n        Statistic function\n\n    \"\"\"\n\n    _subclass_registry = set()\n\n    def __init_subclass__(cls) -> None:\n        if not (inspect.isabstract(cls) or cls.__name__.startswith(\"_\")):\n            Fitter._subclass_registry.add(cls)\n\n    supported_constraints = []\n\n    def __init__(self, optimizer, statistic):\n        if optimizer is None:\n            raise ValueError(\"Expected an optimizer.\")\n        if statistic is None:\n            raise ValueError(\"Expected a statistic function.\")\n        if isinstance(optimizer, type):\n            # a callable class\n            self._opt_method = optimizer()\n        elif inspect.isfunction(optimizer):\n            self._opt_method = optimizer\n        else:\n            raise ValueError(\"Expected optimizer to be a callable class or a function.\")\n        if isinstance(statistic, type):\n            self._stat_method = statistic()\n        else:\n            self._stat_method = statistic\n\n    def objective_function(self, fps, *args):\n        \"\"\"\n        Function to minimize.\n\n        Parameters\n        ----------\n        fps : list\n            parameters returned by the fitter\n        args : list\n            [model, [other_args], [input coordinates]]\n            other_args may include weights or any other quantities specific for\n            a statistic\n\n        Notes\n        -----\n        The list of arguments (arg"}, {"start_line": 43000, "end_line": 45000, "belongs_to": {"file_name": "fitting.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "                   AstropyUserWarning,\n                )\n\n            # Recombine newly-masked residuals with model to get masked values:\n            filtered_data += model_vals\n\n            # Re-fit the data after filtering, passing masked/unmasked values\n            # for single models / sets, respectively:\n            if model_set_axis is None:\n                good = ~filtered_data.mask\n\n                if weights is not None:\n                    filtered_weights = weights[good]\n\n                fitted_model = self.fitter(\n                    fitted_model,\n                    *(c[good] for c in coords),\n                    filtered_data.data[good],\n                    weights=filtered_weights,\n                    inplace=inplace,\n                    **kwargs,\n                )\n            else:\n                fitted_model = self.fitter(\n                    fitted_model,\n                    *coords,\n                    filtered_data,\n                    weights=filtered_weights,\n                    inplace=inplace,\n                    **kwargs,\n                )\n\n            # Stop iteration if the masked points are no longer changing (with\n            # cumulative rejection we only need to compare how many there are):\n            this_n_masked = filtered_data.mask.sum()  # (minimal overhead)\n            if this_n_masked == last_n_masked:\n                break\n            last_n_masked = this_n_masked\n\n        self.fit_info = {\"niter\": niter}\n        self.fit_info.update(getattr(self.fitter, \"fit_info\", {}))\n\n        return fitted_model, filtered_data.mask\n\n\nclass _NonLinearLSQFitter(Fitter):\n    \"\"\"\n    Base class for Non-Linear least-squares fitters.\n\n    Parameters\n    ----------\n    calc_uncertainties : bool\n        If the covariance matrix should be computed and set in the fit_info.\n        Default: False\n    use_min_max_bounds : bool\n        If set, the parameter bounds for a model will be enforced for each given\n        parameter while fitting via a simple"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "fitting.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "s) is set in the `__call__` method.\n        Fitters may overwrite this method, e.g. when statistic functions\n        require other arguments.\n\n        \"\"\"\n        model = args[0]\n        meas = args[-1]\n        fitter_to_model_params(model, fps)\n        res = self._stat_method(meas, model, *args[1:-1])\n        return res\n\n    @staticmethod\n    def _add_fitting_uncertainties(*args):\n        \"\"\"\n        When available, calculate and sets the parameter covariance matrix\n        (model.cov_matrix) and standard deviations (model.stds).\n        \"\"\"\n        return None\n\n    @abc.abstractmethod\n    def __call__(self):\n        \"\"\"\n        This method performs the actual fitting and modifies the parameter list\n        of a model.\n        Fitter subclasses should implement this method.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses should implement this method.\")\n\n\nclass LinearLSQFitter(Fitter):\n    \"\"\"\n    A class performing a linear least square fitting.\n    Uses `numpy.linalg.lstsq` to do the fitting.\n    Given a model and data, fits the model to the data and changes the\n    model's parameters. Keeps a dictionary of auxiliary fitting information.\n\n    Notes\n    -----\n    Note that currently LinearLSQFitter does not support compound models.\n    \"\"\"\n\n    supported_constraints = [\"fixed\"]\n    supports_masked_input = True\n\n    def __init__(self, calc_uncertainties=False):\n        self.fit_info = {\n            \"residuals\": None,\n            \"rank\": None,\n            \"singular_values\": None,\n            \"params\": None,\n        }\n        self._calc_uncertainties = calc_uncertainties\n\n    @staticmethod\n    def _is_invertible(m):\n        \"\"\"Check if inverse of matrix can be obtained.\"\"\"\n        if m.shape[0] != m.shape[1]:\n            return False\n        if np.linalg.matrix_rank(m) < m.shape[0]:\n            return False\n        return True\n\n    def _add_fitting_uncertainties(self, model, a, n_coeff, x, y, z=None, resids=None):\n        \"\"\"\n        Calculate and parameter "}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "fitting.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ror(\"Expected an optimizer.\")\n        if statistic is None:\n            raise ValueError(\"Expected a statistic function.\")\n        if isinstance(optimizer, type):\n            # a callable class\n            self._opt_method = optimizer()\n        elif inspect.isfunction(optimizer):\n            self._opt_method = optimizer\n        else:\n            raise ValueError(\"Expected optimizer to be a callable class or a function.\")\n        if isinstance(statistic, type):\n            self._stat_method = statistic()\n        else:\n            self._stat_method = statistic\n\n    def objective_function(self, fps, *args):\n        \"\"\"\n        Function to minimize.\n\n        Parameters\n        ----------\n        fps : list\n            parameters returned by the fitter\n        args : list\n            [model, [other_args], [input coordinates]]\n            other_args may include weights or any other quantities specific for\n            a statistic\n\n        Notes\n        -----\n        The list of arguments (args) is set in the `__call__` method.\n        Fitters may overwrite this method, e.g. when statistic functions\n        require other arguments.\n\n        \"\"\"\n        model = args[0]\n        meas = args[-1]\n        fitter_to_model_params(model, fps)\n        res = self._stat_method(meas, model, *args[1:-1])\n        return res\n\n    @staticmethod\n    def _add_fitting_uncertainties(*args):\n        \"\"\"\n        When available, calculate and sets the parameter covariance matrix\n        (model.cov_matrix) and standard deviations (model.stds).\n        \"\"\"\n        return None\n\n    @abc.abstractmethod\n    def __call__(self):\n        \"\"\"\n        This method performs the actual fitting and modifies the parameter list\n        of a model.\n        Fitter subclasses should implement this method.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses should implement this method.\")\n\n\nclass LinearLSQFitter(Fitter):\n    \"\"\"\n    A class performing a linear least square fitting.\n    Uses `numpy.linalg.lstsq`"}, {"start_line": 64000, "end_line": 66000, "belongs_to": {"file_name": "fitting.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "QFitter):\n    \"\"\"\n    Trust Region Reflective algorithm and least squares statistic.\n\n    Parameters\n    ----------\n    calc_uncertainties : bool\n        If the covariance matrix should be computed and set in the fit_info.\n        Default: False\n\n    Attributes\n    ----------\n    fit_info :\n        A `scipy.optimize.OptimizeResult` class which contains all of\n        the most recent fit information\n    \"\"\"\n\n    @deprecated_renamed_argument(\"use_min_max_bounds\", None, \"7.0\")\n    def __init__(self, calc_uncertainties=False, use_min_max_bounds=False):\n        super().__init__(\"trf\", calc_uncertainties, use_min_max_bounds)\n\n\nclass DogBoxLSQFitter(_NLLSQFitter):\n    \"\"\"\n    DogBox algorithm and least squares statistic.\n\n    Parameters\n    ----------\n    calc_uncertainties : bool\n        If the covariance matrix should be computed and set in the fit_info.\n        Default: False\n\n    Attributes\n    ----------\n    fit_info :\n        A `scipy.optimize.OptimizeResult` class which contains all of\n        the most recent fit information\n    \"\"\"\n\n    @deprecated_renamed_argument(\"use_min_max_bounds\", None, \"7.0\")\n    def __init__(self, calc_uncertainties=False, use_min_max_bounds=False):\n        super().__init__(\"dogbox\", calc_uncertainties, use_min_max_bounds)\n\n\nclass LMLSQFitter(_NLLSQFitter):\n    \"\"\"\n    `scipy.optimize.least_squares` Levenberg-Marquardt algorithm and least squares statistic.\n\n    Parameters\n    ----------\n    calc_uncertainties : bool\n        If the covariance matrix should be computed and set in the fit_info.\n        Default: False\n\n    Attributes\n    ----------\n    fit_info :\n        A `scipy.optimize.OptimizeResult` class which contains all of\n        the most recent fit information\n    \"\"\"\n\n    def __init__(self, calc_uncertainties=False):\n        super().__init__(\"lm\", calc_uncertainties, True)\n\n    @fitter_unit_support\n    def __call__(\n        self,\n        model,\n        x,\n        y,\n        z=None,\n        weights=None,\n        maxiter=DEFAULT_MAXI"}, {"start_line": 67000, "end_line": 69000, "belongs_to": {"file_name": "fitting.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ropyDeprecationWarning,\n                stacklevel=2,\n            )\n        return super().__call__(\n            model,\n            x,\n            y,\n            z=z,\n            weights=weights,\n            maxiter=maxiter,\n            acc=acc,\n            epsilon=epsilon,\n            estimate_jacobian=estimate_jacobian,\n            filter_non_finite=filter_non_finite,\n            inplace=inplace,\n        )\n\n\nclass SLSQPLSQFitter(Fitter):\n    \"\"\"\n    Sequential Least Squares Programming (SLSQP) optimization algorithm and\n    least squares statistic.\n\n    Raises\n    ------\n    ModelLinearityError\n        A linear model is passed to a nonlinear fitter\n\n    Notes\n    -----\n    See also the `~astropy.modeling.optimizers.SLSQP` optimizer.\n\n    \"\"\"\n\n    supported_constraints = SLSQP.supported_constraints\n\n    def __init__(self):\n        super().__init__(optimizer=SLSQP, statistic=leastsquare)\n        self.fit_info = {}\n\n    @fitter_unit_support\n    def __call__(\n        self,\n        model,\n        x,\n        y,\n        z=None,\n        weights=None,\n        *,\n        inplace=False,\n        **kwargs,\n    ):\n        \"\"\"\n        Fit data to this model.\n\n        Parameters\n        ----------\n        model : `~astropy.modeling.FittableModel`\n            model to fit to x, y, z\n        x : array\n            input coordinates\n        y : array\n            input coordinates\n        z : array, optional\n            input coordinates\n        weights : array, optional\n            Weights for fitting.\n            For data with Gaussian uncertainties, the weights should be\n            1/sigma.\n        inplace : bool, optional\n            If `False` (the default), a copy of the model with the fitted\n            parameters set will be returned. If `True`, the returned model will\n            be the same instance as the model passed in, and the parameter\n            values will be changed inplace.\n        kwargs : dict\n            optional keyword arguments to be passed to the optimizer "}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "optimizers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n# pylint: disable=invalid-name\n\n\"\"\"\nOptimization algorithms used in `~astropy.modeling.fitting`.\n\"\"\"\n\nimport warnings\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\n\nfrom astropy.utils.exceptions import AstropyUserWarning\n\n__all__ = [\"SLSQP\", \"Optimization\", \"Simplex\"]\n\n# Maximum number of iterations\nDEFAULT_MAXITER = 100\n\n# Step for the forward difference approximation of the Jacobian\nDEFAULT_EPS = np.sqrt(np.finfo(float).eps)\n\n# Default requested accuracy\nDEFAULT_ACC = 1e-07\n\nDEFAULT_BOUNDS = (-(10**12), 10**12)\n\n\nclass Optimization(ABC):\n    \"\"\"\n    Base class for optimizers.\n\n    Parameters\n    ----------\n    opt_method : callable\n        Implements optimization method\n\n    Notes\n    -----\n    The base Optimizer does not support any constraints by default; individual\n    optimizers should explicitly set this list to the specific constraints\n    it supports.\n\n    \"\"\"\n\n    supported_constraints = []\n\n    @abstractmethod\n    def __init__(self) -> None: ...\n\n    def _init_opt_method(self, opt_method):\n        self._opt_method = opt_method\n        self._maxiter = DEFAULT_MAXITER\n        self._eps = DEFAULT_EPS\n        self._acc = DEFAULT_ACC\n\n    @property\n    def maxiter(self):\n        \"\"\"Maximum number of iterations.\"\"\"\n        return self._maxiter\n\n    @maxiter.setter\n    def maxiter(self, val):\n        \"\"\"Set maxiter.\"\"\"\n        self._maxiter = val\n\n    @property\n    def eps(self):\n        \"\"\"Step for the forward difference approximation of the Jacobian.\"\"\"\n        return self._eps\n\n    @eps.setter\n    def eps(self, val):\n        \"\"\"Set eps value.\"\"\"\n        self._eps = val\n\n    @property\n    def acc(self):\n        \"\"\"Requested accuracy.\"\"\"\n        return self._acc\n\n    @acc.setter\n    def acc(self, val):\n        \"\"\"Set accuracy.\"\"\"\n        self._acc = val\n\n    def __repr__(self):\n        fmt = f\"{self.__class__.__name__}()\"\n        return fmt\n\n    @property\n    def opt_method(self):\n       "}, {"start_line": 65000, "end_line": 67000, "belongs_to": {"file_name": "fitting.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "\n        the most recent fit information\n    \"\"\"\n\n    @deprecated_renamed_argument(\"use_min_max_bounds\", None, \"7.0\")\n    def __init__(self, calc_uncertainties=False, use_min_max_bounds=False):\n        super().__init__(\"dogbox\", calc_uncertainties, use_min_max_bounds)\n\n\nclass LMLSQFitter(_NLLSQFitter):\n    \"\"\"\n    `scipy.optimize.least_squares` Levenberg-Marquardt algorithm and least squares statistic.\n\n    Parameters\n    ----------\n    calc_uncertainties : bool\n        If the covariance matrix should be computed and set in the fit_info.\n        Default: False\n\n    Attributes\n    ----------\n    fit_info :\n        A `scipy.optimize.OptimizeResult` class which contains all of\n        the most recent fit information\n    \"\"\"\n\n    def __init__(self, calc_uncertainties=False):\n        super().__init__(\"lm\", calc_uncertainties, True)\n\n    @fitter_unit_support\n    def __call__(\n        self,\n        model,\n        x,\n        y,\n        z=None,\n        weights=None,\n        maxiter=DEFAULT_MAXITER,\n        acc=DEFAULT_ACC,\n        epsilon=DEFAULT_EPS,\n        estimate_jacobian=False,\n        filter_non_finite=False,\n        inplace=False,\n    ):\n        # Since there are several fitters with proper support for bounds, it\n        # is not a good idea to keep supporting the hacky bounds algorithm\n        # from LevMarLSQFitter here, and better to communicate with users\n        # that they should use another fitter. Once we remove the deprecation,\n        # we should update ``supported_constraints`` and change ``True`` to\n        # ``False`` in the call to ``super().__init__`` above.\n        if model.has_bounds:\n            warnings.warn(\n                \"Using LMLSQFitter for models with bounds is now \"\n                \"deprecated since astropy 7.0. We recommend you use another non-linear \"\n                \"fitter such as TRFLSQFitter or DogBoxLSQFitter instead \"\n                \"as these have full support for fitting models with \"\n                \"bounds\",\n                Ast"}], "retrieved_count": 10, "cost_time": 0.3449845314025879}
{"question": "Where during XML serialization in the VOTable format handling module is the warning condition that detects masked bit datatype values evaluated?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 30000, "end_line": 32000, "belongs_to": {"file_name": "exceptions.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "t values do not support masking.  This warning is raised upon\n    setting masked data in a bit column.\n\n    **References**: `1.1\n    <http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#sec:datatypes>`__,\n    `1.2\n    <http://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html#sec:datatypes>`__\n    \"\"\"\n\n    message_template = \"Bit values can not be masked\"\n\n\nclass W40(VOTableSpecWarning):\n    \"\"\"\n    This is a terrible hack to support Simple Image Access Protocol\n    results from `NOIRLab Astro Data Archive <https://astroarchive.noirlab.edu/>`__.  It\n    creates a field for the coordinate projection type of type \"double\",\n    which actually contains character data.  We have to hack the field\n    to store character data, or we can't read it in.  A warning will be\n    raised when this happens.\n    \"\"\"\n\n    message_template = \"'cprojection' datatype repaired\"\n\n\nclass W41(VOTableSpecWarning):\n    \"\"\"\n    An XML namespace was specified on the ``VOTABLE`` element, but the\n    namespace does not match what is expected for a ``VOTABLE`` file.\n\n    The ``VOTABLE`` namespace is::\n\n      http://www.ivoa.net/xml/VOTable/vX.X\n\n    where \"X.X\" is the version number.\n\n    Some files in the wild set the namespace to the location of the\n    VOTable schema, which is not correct and will not pass some\n    validating parsers.\n    \"\"\"\n\n    message_template = (\n        \"An XML namespace is specified, but is incorrect.  Expected '{}', got '{}'\"\n    )\n    default_args = (\"x\", \"y\")\n\n\nclass W42(VOTableSpecWarning):\n    \"\"\"The root element should specify a namespace.\n\n    The ``VOTABLE`` namespace is::\n\n        http://www.ivoa.net/xml/VOTable/vX.X\n\n    where \"X.X\" is the version number.\n    \"\"\"\n\n    message_template = \"No XML namespace specified\"\n\n\nclass W43(VOTableSpecWarning):\n    \"\"\"Referenced elements should be defined before referees.\n\n    From the VOTable 1.2 spec:\n\n       In VOTable1.2, it is further recommended to place the ID\n       attribute prior t"}, {"start_line": 25000, "end_line": 27000, "belongs_to": {"file_name": "test_vo.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "o.BytesIO()\n        # W39: Bit values can not be masked\n        with pytest.warns(W39):\n            votable.to_xml(bio)\n        bio.seek(0)\n        self.votable = parse(bio)\n        self.table = self.votable.get_first_table()\n        self.array = self.table.array\n        self.mask = self.table.array.mask\n\n    def test_bit_mask(self):\n        assert_array_equal(self.mask[\"bit\"], [False, False, False, False, False])\n\n    def test_bitarray_mask(self):\n        assert not np.any(self.mask[\"bitarray\"])\n\n    def test_bit_array2_mask(self):\n        assert not np.any(self.mask[\"bitarray2\"])\n\n    def test_schema(self, tmp_path):\n        # have to use an actual file because assert_validate_schema only works\n        # on filenames, not file-like objects\n        fn = tmp_path / \"test_through_tabledata.xml\"\n        with open(fn, \"wb\") as f:\n            f.write(self.xmlout.getvalue())\n        assert_validate_schema(fn, \"1.1\")\n\n\nclass TestThroughBinary(TestParse):\n    def setup_class(self):\n        with np.errstate(over=\"ignore\"):\n            # https://github.com/astropy/astropy/issues/13341\n            votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n        votable.get_first_table().format = \"binary\"\n\n        self.xmlout = bio = io.BytesIO()\n        # W39: Bit values can not be masked\n        with pytest.warns(W39):\n            votable.to_xml(bio)\n        bio.seek(0)\n        self.votable = parse(bio)\n\n        self.table = self.votable.get_first_table()\n        self.array = self.table.array\n        self.mask = self.table.array.mask\n\n    # Masked values in bit fields don't roundtrip through the binary\n    # representation -- that's not a bug, just a limitation, so\n    # override the mask array checks here.\n    def test_bit_mask(self):\n        assert not np.any(self.mask[\"bit\"])\n\n    def test_bitarray_mask(self):\n        assert not np.any(self.mask[\"bitarray\"])\n\n    def test_bit_array2_mask(self):\n        assert not np.any(self.mask[\"bitarray2\"])\n\n    def test_null_inte"}, {"start_line": 26000, "end_line": 28000, "belongs_to": {"file_name": "test_vo.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "th np.errstate(over=\"ignore\"):\n            # https://github.com/astropy/astropy/issues/13341\n            votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n        votable.get_first_table().format = \"binary\"\n\n        self.xmlout = bio = io.BytesIO()\n        # W39: Bit values can not be masked\n        with pytest.warns(W39):\n            votable.to_xml(bio)\n        bio.seek(0)\n        self.votable = parse(bio)\n\n        self.table = self.votable.get_first_table()\n        self.array = self.table.array\n        self.mask = self.table.array.mask\n\n    # Masked values in bit fields don't roundtrip through the binary\n    # representation -- that's not a bug, just a limitation, so\n    # override the mask array checks here.\n    def test_bit_mask(self):\n        assert not np.any(self.mask[\"bit\"])\n\n    def test_bitarray_mask(self):\n        assert not np.any(self.mask[\"bitarray\"])\n\n    def test_bit_array2_mask(self):\n        assert not np.any(self.mask[\"bitarray2\"])\n\n    def test_null_integer_binary(self):\n        # BINARY1 requires magic value to be specified\n\n        self.array.mask[\"intNoNull\"][0] = True\n\n        bio = io.BytesIO()\n\n        # W31: NaN's can not be represented in integer field\n        with pytest.warns(W31):\n            # https://github.com/astropy/astropy/issues/16090\n            self.votable.to_xml(bio)\n\n\nclass TestThroughBinary2(TestParse):\n    def setup_class(self):\n        with np.errstate(over=\"ignore\"):\n            # https://github.com/astropy/astropy/issues/13341\n            votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n        votable.version = \"1.3\"\n        votable.get_first_table()._config[\"version_1_3_or_later\"] = True\n        votable.get_first_table().format = \"binary2\"\n\n        self.xmlout = bio = io.BytesIO()\n        # W39: Bit values can not be masked\n        with pytest.warns(W39):\n            votable.to_xml(bio)\n        bio.seek(0)\n        self.votable = parse(bio)\n\n        self.table = self.votable.get_first_table()\n"}, {"start_line": 29000, "end_line": 31000, "belongs_to": {"file_name": "test_vo.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "Bit values can not be masked\n    with pytest.warns(W39):\n        votable.to_xml(bio)\n    bio.seek(0)\n    votable = parse(bio, columns=[0, 1, 2])\n    table = votable.get_first_table().to_table()\n    assert table.colnames == [\"string_test\", \"string_test_2\", \"unicode_test\"]\n\n\ndef table_from_scratch():\n    from astropy.io.votable.tree import Field, Resource, TableElement, VOTableFile\n\n    # Create a new VOTable file...\n    votable = VOTableFile()\n\n    # ...with one resource...\n    resource = Resource()\n    votable.resources.append(resource)\n\n    # ... with one table\n    table = TableElement(votable)\n    resource.tables.append(table)\n\n    # Define some fields\n    table.fields.extend(\n        [\n            Field(votable, ID=\"filename\", datatype=\"char\"),\n            Field(votable, ID=\"matrix\", datatype=\"double\", arraysize=\"2x2\"),\n        ]\n    )\n\n    # Now, use those field definitions to create the numpy record arrays, with\n    # the given number of rows\n    table.create_arrays(2)\n\n    # Now table.array can be filled with data\n    table.array[0] = (\"test1.xml\", [[1, 0], [0, 1]])\n    table.array[1] = (\"test2.xml\", [[0.5, 0.3], [0.2, 0.1]])\n\n    # Now write the whole thing to a file.\n    # Note, we have to use the top-level votable file object\n    out = io.StringIO()\n    votable.to_xml(out)\n\n\n# https://github.com/astropy/astropy/issues/13341\n@np.errstate(over=\"ignore\")\ndef test_open_files():\n    for filename in get_pkg_data_filenames(\"data\", pattern=\"*.xml\"):\n        if not filename.endswith(\n            (\"custom_datatype.xml\", \"timesys_errors.xml\", \"parquet_binary.xml\")\n        ):\n            parse(filename)\n\n\ndef test_too_many_columns():\n    with pytest.raises(VOTableSpecError):\n        parse(get_pkg_data_filename(\"data/too_many_columns.xml.gz\"))\n\n\ndef test_build_from_scratch(tmp_path):\n    # Create a new VOTable file...\n    votable = tree.VOTableFile()\n\n    # ...with one resource...\n    resource = tree.Resource()\n    votable.resources.append(resource)\n\n    # ... with one "}, {"start_line": 29000, "end_line": 31000, "belongs_to": {"file_name": "exceptions.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "type, setting to 0\"\n    default_args = (\"x\",)\n\n\nclass W37(UnimplementedWarning):\n    \"\"\"\n    The 4 datatypes defined in the VOTable specification and supported by\n    ``astropy.io.votable`` are ``TABLEDATA``, ``BINARY``, ``BINARY2`` and ``FITS``.\n    In addition, ``astropy.io.votable`` also supports ``PARQUET`` serialization, which is\n    a candidate for addition to the VOTable specification.\n\n    **References:** `1.1\n    <http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#sec:data>`__,\n    `1.2\n    <http://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html#sec:data>`__\n    \"\"\"\n\n    message_template = \"Unsupported data format '{}'\"\n    default_args = (\"x\",)\n\n\nclass W38(VOTableSpecWarning):\n    \"\"\"\n    The only encoding for local binary data supported by the VOTable\n    specification is base64.\n    \"\"\"\n\n    message_template = \"Inline binary data must be base64 encoded, got '{}'\"\n    default_args = (\"x\",)\n\n\nclass W39(VOTableSpecWarning):\n    \"\"\"\n    Bit values do not support masking.  This warning is raised upon\n    setting masked data in a bit column.\n\n    **References**: `1.1\n    <http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#sec:datatypes>`__,\n    `1.2\n    <http://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html#sec:datatypes>`__\n    \"\"\"\n\n    message_template = \"Bit values can not be masked\"\n\n\nclass W40(VOTableSpecWarning):\n    \"\"\"\n    This is a terrible hack to support Simple Image Access Protocol\n    results from `NOIRLab Astro Data Archive <https://astroarchive.noirlab.edu/>`__.  It\n    creates a field for the coordinate projection type of type \"double\",\n    which actually contains character data.  We have to hack the field\n    to store character data, or we can't read it in.  A warning will be\n    raised when this happens.\n    \"\"\"\n\n    message_template = \"'cprojection' datatype repaired\"\n\n\nclass W41(VOTableSpecWarning):\n    \"\"\"\n    An XML namespace was specified on the ``VOTABLE`` el"}, {"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "converters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "= []\n    for v in value:\n        if v:\n            byte |= 1 << bit_no\n        if bit_no == 0:\n            bytes.append(byte)\n            bit_no = 7\n            byte = 0\n        else:\n            bit_no -= 1\n    if bit_no != 7:\n        bytes.append(byte)\n\n    return struct.pack(f\"{len(bytes)}B\", *bytes)\n\n\nclass Converter:\n    \"\"\"\n    The base class for all converters.  Each subclass handles\n    converting a specific VOTABLE data type to/from the TABLEDATA_ and\n    BINARY_ on-disk representations.\n\n    Parameters\n    ----------\n    field : `~astropy.io.votable.tree.Field`\n        object describing the datatype\n\n    config : dict\n        The parser configuration dictionary\n\n    pos : tuple\n        The position in the XML file where the FIELD object was\n        found.  Used for error messages.\n\n    \"\"\"\n\n    def __init__(self, field, config=None, pos=None):\n        pass\n\n    @staticmethod\n    def _parse_length(read):\n        return struct.unpack(\">I\", read(4))[0]\n\n    @staticmethod\n    def _write_length(length):\n        return struct.pack(\">I\", int(length))\n\n    def supports_empty_values(self, config):\n        \"\"\"\n        Returns True when the field can be completely empty.\n        \"\"\"\n        return config.get(\"version_1_3_or_later\")\n\n    def parse(self, value, config=None, pos=None):\n        \"\"\"\n        Convert the string *value* from the TABLEDATA_ format into an\n        object with the correct native in-memory datatype and mask flag.\n\n        Parameters\n        ----------\n        value : str\n            value in TABLEDATA format\n\n        Returns\n        -------\n        native : tuple\n            A two-element tuple of: value, mask.\n            The value as a Numpy array or scalar, and *mask* is True\n            if the value is missing.\n        \"\"\"\n        raise NotImplementedError(\"This datatype must implement a 'parse' method.\")\n\n    def parse_scalar(self, value, config=None, pos=None):\n        \"\"\"\n        Parse a single scalar of the underlying type of the convert"}, {"start_line": 24000, "end_line": 26000, "belongs_to": {"file_name": "test_vo.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     assert info.value == \"OK\"\n\n        if self.votable.version != \"1.1\":\n            info = self.votable.get_info_by_id(\"ErrorInfo\")\n            assert info.value == \"One might expect to find some INFO here, too...\"\n\n    def test_repr(self):\n        assert \"3 tables\" in repr(self.votable)\n        assert (\n            repr(list(self.votable.iter_fields_and_params())[0])\n            == '<PARAM ID=\"awesome\" arraysize=\"*\" datatype=\"float\" '\n            'name=\"INPUT\" unit=\"deg\" value=\"[0.0 0.0]\"/>'\n        )\n        # Smoke test\n        repr(list(self.votable.iter_groups()))\n\n        # Resource\n        assert repr(self.votable.resources) == \"[</>]\"\n\n        # Table\n        assert repr(self.table).startswith(\"<VOTable\")\n\n\nclass TestThroughTableData(TestParse):\n    def setup_class(self):\n        with np.errstate(over=\"ignore\"):\n            # https://github.com/astropy/astropy/issues/13341\n            votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n\n        self.xmlout = bio = io.BytesIO()\n        # W39: Bit values can not be masked\n        with pytest.warns(W39):\n            votable.to_xml(bio)\n        bio.seek(0)\n        self.votable = parse(bio)\n        self.table = self.votable.get_first_table()\n        self.array = self.table.array\n        self.mask = self.table.array.mask\n\n    def test_bit_mask(self):\n        assert_array_equal(self.mask[\"bit\"], [False, False, False, False, False])\n\n    def test_bitarray_mask(self):\n        assert not np.any(self.mask[\"bitarray\"])\n\n    def test_bit_array2_mask(self):\n        assert not np.any(self.mask[\"bitarray2\"])\n\n    def test_schema(self, tmp_path):\n        # have to use an actual file because assert_validate_schema only works\n        # on filenames, not file-like objects\n        fn = tmp_path / \"test_through_tabledata.xml\"\n        with open(fn, \"wb\") as f:\n            f.write(self.xmlout.getvalue())\n        assert_validate_schema(fn, \"1.1\")\n\n\nclass TestThroughBinary(TestParse):\n    def setup_class(self):\n        wi"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "converters.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"\nThis module handles the conversion of various VOTABLE datatypes\nto/from TABLEDATA_ and BINARY_ formats.\n\"\"\"\n\n# STDLIB\nimport re\nimport struct\nimport sys\nfrom math import prod\n\n# THIRD-PARTY\nimport numpy as np\nfrom numpy import ma\n\n# ASTROPY\nfrom astropy.utils.xml.writer import xml_escape_cdata\n\n# LOCAL\nfrom .exceptions import (\n    E01,\n    E02,\n    E03,\n    E04,\n    E05,\n    E06,\n    E24,\n    W01,\n    W30,\n    W31,\n    W39,\n    W46,\n    W47,\n    W49,\n    W51,\n    W55,\n    vo_raise,\n    vo_warn,\n    warn_or_raise,\n)\n\n__all__ = [\"Converter\", \"get_converter\", \"table_column_to_votable_datatype\"]\n\n\npedantic_array_splitter = re.compile(r\" +\")\narray_splitter = re.compile(r\"\\s+|(?:\\s*,\\s*)\")\n\"\"\"\nA regex to handle splitting values on either whitespace or commas.\n\nSPEC: Usage of commas is not actually allowed by the spec, but many\nfiles in the wild use them.\n\"\"\"\n\n_zero_int = b\"\\0\\0\\0\\0\"\n_empty_bytes = b\"\"\n_zero_byte = b\"\\0\"\n\n\nif sys.byteorder == \"little\":\n\n    def _ensure_bigendian(x):\n        if x.dtype.byteorder != \">\":\n            return x.byteswap()\n        return x\n\nelse:\n\n    def _ensure_bigendian(x):\n        if x.dtype.byteorder == \"<\":\n            return x.byteswap()\n        return x\n\n\ndef _make_masked_array(data, mask):\n    \"\"\"\n    Masked arrays of zero length that also have a mask of zero length\n    cause problems in Numpy (at least in 1.6.2).  This function\n    creates a masked array from data and a mask, unless it is zero\n    length.\n    \"\"\"\n    # np.ma doesn't like setting mask to []\n    if len(data):\n        return ma.array(np.array(data), mask=np.array(mask, dtype=\"bool\"))\n    else:\n        return ma.array(np.array(data))\n\n\ndef bitarray_to_bool(data, length):\n    \"\"\"\n    Converts a bit array (a string of bits in a bytes object) to a\n    boolean Numpy array.\n\n    Parameters\n    ----------\n    data : bytes\n        The bit array.  The most significant byte is read first.\n\n    length : int\n       "}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "test_vo.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ssion(tmp_path):\n    # W39: Bit values can not be masked\n    with pytest.warns(W39), np.errstate(over=\"ignore\"):\n        _test_regression(tmp_path, False)\n\n\n@pytest.mark.xfail(\"legacy_float_repr\")\ndef test_regression_python_based_parser(tmp_path):\n    # W39: Bit values can not be masked\n    with pytest.warns(W39), np.errstate(over=\"ignore\"):\n        _test_regression(tmp_path, True)\n\n\n@pytest.mark.xfail(\"legacy_float_repr\")\ndef test_regression_binary2(tmp_path):\n    # W39: Bit values can not be masked\n    with pytest.warns(W39), np.errstate(over=\"ignore\"):\n        _test_regression(tmp_path, False, 2)\n\n\nclass TestFixups:\n    def setup_class(self):\n        with np.errstate(over=\"ignore\"):\n            # https://github.com/astropy/astropy/issues/13341\n            self.table = parse(\n                get_pkg_data_filename(\"data/regression.xml\")\n            ).get_first_table()\n        self.array = self.table.array\n        self.mask = self.table.array.mask\n\n    def test_implicit_id(self):\n        assert_array_equal(self.array[\"string_test_2\"], self.array[\"fixed string test\"])\n\n\nclass TestReferences:\n    def setup_class(self):\n        with np.errstate(over=\"ignore\"):\n            # https://github.com/astropy/astropy/issues/13341\n            self.votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n        self.table = self.votable.get_first_table()\n        self.array = self.table.array\n        self.mask = self.table.array.mask\n\n    def test_fieldref(self):\n        fieldref = self.table.groups[1].entries[0]\n        assert isinstance(fieldref, tree.FieldRef)\n        assert fieldref.get_ref().name == \"boolean\"\n        assert fieldref.get_ref().datatype == \"boolean\"\n\n    def test_paramref(self):\n        paramref = self.table.groups[0].entries[0]\n        assert isinstance(paramref, tree.ParamRef)\n        assert paramref.get_ref().name == \"INPUT\"\n        assert paramref.get_ref().datatype == \"float\"\n\n    def test_iter_fields_and_params_on_a_group(self):\n        assert len(l"}, {"start_line": 27000, "end_line": 29000, "belongs_to": {"file_name": "test_vo.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/io/votable/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ger_binary(self):\n        # BINARY1 requires magic value to be specified\n\n        self.array.mask[\"intNoNull\"][0] = True\n\n        bio = io.BytesIO()\n\n        # W31: NaN's can not be represented in integer field\n        with pytest.warns(W31):\n            # https://github.com/astropy/astropy/issues/16090\n            self.votable.to_xml(bio)\n\n\nclass TestThroughBinary2(TestParse):\n    def setup_class(self):\n        with np.errstate(over=\"ignore\"):\n            # https://github.com/astropy/astropy/issues/13341\n            votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n        votable.version = \"1.3\"\n        votable.get_first_table()._config[\"version_1_3_or_later\"] = True\n        votable.get_first_table().format = \"binary2\"\n\n        self.xmlout = bio = io.BytesIO()\n        # W39: Bit values can not be masked\n        with pytest.warns(W39):\n            votable.to_xml(bio)\n        bio.seek(0)\n        self.votable = parse(bio)\n\n        self.table = self.votable.get_first_table()\n        self.array = self.table.array\n        self.mask = self.table.array.mask\n\n    def test_get_coosys_by_id(self):\n        # No COOSYS in VOTable 1.2 or later\n        pass\n\n    def test_null_integer_binary2(self):\n        # Integers with no magic values should still be\n        # masked in BINARY2 format\n\n        self.array.mask[\"intNoNull\"][0] = True\n\n        table = self.votable.get_first_table()\n        array = table.array\n\n        assert array.mask[\"intNoNull\"][0]\n        assert array[\"intNoNull\"].mask[0]\n\n\n@pytest.mark.parametrize(\"format_\", [\"binary\", \"binary2\"])\ndef test_select_columns_binary(format_):\n    with np.errstate(over=\"ignore\"):\n        # https://github.com/astropy/astropy/issues/13341\n        votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n    if format_ == \"binary2\":\n        votable.version = \"1.3\"\n        votable.get_first_table()._config[\"version_1_3_or_later\"] = True\n    votable.get_first_table().format = format_\n\n    bio = io.BytesIO()\n    # W39: "}], "retrieved_count": 10, "cost_time": 0.342479944229126}
{"question": "Where are the lower-level mathematical transformation functions that the sky-to-pixel zenithal equal area projection class delegates to for converting the zenithal angle theta into the radial distance R_theta during the sky-to-pixel projection?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 16000, "end_line": 18000, "belongs_to": {"file_name": "projections.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "finition of the full transformation.\n\n    .. math::\n        \\theta = 90^\\circ - R_\\theta\n    \"\"\"\n\n\nclass Sky2Pix_ZenithalEquidistant(Sky2PixProjection, Zenithal):\n    r\"\"\"\n    Zenithal equidistant projection - sky to pixel.\n\n    Corresponds to the ``ARC`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    .. math::\n        R_\\theta = 90^\\circ - \\theta\n    \"\"\"\n\n\nclass Pix2Sky_ZenithalEqualArea(Pix2SkyProjection, Zenithal):\n    r\"\"\"\n    Zenithal equidistant projection - pixel to sky.\n\n    Corresponds to the ``ZEA`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    .. math::\n        \\theta = 90^\\circ - 2 \\sin^{-1} \\left(\\frac{\\pi R_\\theta}{360^\\circ}\\right)\n    \"\"\"\n\n\nclass Sky2Pix_ZenithalEqualArea(Sky2PixProjection, Zenithal):\n    r\"\"\"\n    Zenithal equidistant projection - sky to pixel.\n\n    Corresponds to the ``ZEA`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    .. math::\n        R_\\theta &= \\frac{180^\\circ}{\\pi} \\sqrt{2(1 - \\sin\\theta)} \\\\\n                 &= \\frac{360^\\circ}{\\pi} \\sin\\left(\\frac{90^\\circ - \\theta}{2}\\right)\n    \"\"\"\n\n\nclass Pix2Sky_Airy(Pix2SkyProjection, Zenithal):\n    r\"\"\"\n    Airy projection - pixel to sky.\n\n    Corresponds to the ``AIR`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    Parameters\n    ----------\n    theta_b : float\n        The latitude :math:`\\theta_b` at which to minimize the error,\n        in degrees.  Default is 90.\n    \"\"\"\n\n    theta_b = _ParameterDS(default=90.0)\n\n\nclass Sky2Pix_Airy(Sky2PixProjection, Zenithal):\n    r\"\"\"\n    Airy - sky to pixel.\n\n    Corresponds to the ``AIR`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    .. math::\n        R_\\theta = -2 \\frac{180^\\circ}{\\pi}\\left(\\frac{\\ln(\\cos \\xi)}{\\tan \\xi} +\n            \\frac{\\ln(\\cos \\xi_b)}{\\tan^2 \\xi_b} \\tan \\xi \\right)\n\n    where:\n\n    .. ma"}, {"start_line": 15000, "end_line": 17000, "belongs_to": {"file_name": "projections.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "s parameter\")\n    eta = _ParameterDS(default=0.0, description=\"Obliqueness parameter\")\n\n\nclass Sky2Pix_SlantOrthographic(Sky2PixProjection, Zenithal):\n    r\"\"\"\n    Slant orthographic projection - sky to pixel.\n\n    Corresponds to the ``SIN`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    The following transformation applies when :math:`\\xi` and\n    :math:`\\eta` are both zero.\n\n    .. math::\n        R_\\theta = \\frac{180^{\\circ}}{\\pi}\\cos \\theta\n\n    But more specifically are:\n\n    .. math::\n        x &= \\frac{180^\\circ}{\\pi}[\\cos \\theta \\sin \\phi + \\xi(1 - \\sin \\theta)] \\\\\n        y &= \\frac{180^\\circ}{\\pi}[\\cos \\theta \\cos \\phi + \\eta(1 - \\sin \\theta)]\n\n    \"\"\"\n\n    xi = _ParameterDS(default=0.0)\n    eta = _ParameterDS(default=0.0)\n\n\nclass Pix2Sky_ZenithalEquidistant(Pix2SkyProjection, Zenithal):\n    r\"\"\"\n    Zenithal equidistant projection - pixel to sky.\n\n    Corresponds to the ``ARC`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    .. math::\n        \\theta = 90^\\circ - R_\\theta\n    \"\"\"\n\n\nclass Sky2Pix_ZenithalEquidistant(Sky2PixProjection, Zenithal):\n    r\"\"\"\n    Zenithal equidistant projection - sky to pixel.\n\n    Corresponds to the ``ARC`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    .. math::\n        R_\\theta = 90^\\circ - \\theta\n    \"\"\"\n\n\nclass Pix2Sky_ZenithalEqualArea(Pix2SkyProjection, Zenithal):\n    r\"\"\"\n    Zenithal equidistant projection - pixel to sky.\n\n    Corresponds to the ``ZEA`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    .. math::\n        \\theta = 90^\\circ - 2 \\sin^{-1} \\left(\\frac{\\pi R_\\theta}{360^\\circ}\\right)\n    \"\"\"\n\n\nclass Sky2Pix_ZenithalEqualArea(Sky2PixProjection, Zenithal):\n    r\"\"\"\n    Zenithal equidistant projection - sky to pixel.\n\n    Corresponds to the ``ZEA`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformati"}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "projections.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "ransformation.\n\n    .. math::\n        R_\\theta = \\frac{180^{\\circ}}{\\pi}\\frac{2 \\cos \\theta}{1 + \\sin \\theta}\n    \"\"\"\n\n\nclass Pix2Sky_SlantOrthographic(Pix2SkyProjection, Zenithal):\n    r\"\"\"\n    Slant orthographic projection - pixel to sky.\n\n    Corresponds to the ``SIN`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    The following transformation applies when :math:`\\xi` and\n    :math:`\\eta` are both zero.\n\n    .. math::\n        \\theta = \\cos^{-1}\\left(\\frac{\\pi}{180^{\\circ}}R_\\theta\\right)\n\n    The parameters :math:`\\xi` and :math:`\\eta` are defined from the\n    reference point :math:`(\\phi_c, \\theta_c)` as:\n\n    .. math::\n        \\xi &= \\cot \\theta_c \\sin \\phi_c \\\\\n        \\eta &= - \\cot \\theta_c \\cos \\phi_c\n\n    Parameters\n    ----------\n    xi : float\n        Obliqueness parameter, .  Default is 0.0.\n\n    eta : float\n        Obliqueness parameter, .  Default is 0.0.\n\n    \"\"\"\n\n    xi = _ParameterDS(default=0.0, description=\"Obliqueness parameter\")\n    eta = _ParameterDS(default=0.0, description=\"Obliqueness parameter\")\n\n\nclass Sky2Pix_SlantOrthographic(Sky2PixProjection, Zenithal):\n    r\"\"\"\n    Slant orthographic projection - sky to pixel.\n\n    Corresponds to the ``SIN`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    The following transformation applies when :math:`\\xi` and\n    :math:`\\eta` are both zero.\n\n    .. math::\n        R_\\theta = \\frac{180^{\\circ}}{\\pi}\\cos \\theta\n\n    But more specifically are:\n\n    .. math::\n        x &= \\frac{180^\\circ}{\\pi}[\\cos \\theta \\sin \\phi + \\xi(1 - \\sin \\theta)] \\\\\n        y &= \\frac{180^\\circ}{\\pi}[\\cos \\theta \\cos \\phi + \\eta(1 - \\sin \\theta)]\n\n    \"\"\"\n\n    xi = _ParameterDS(default=0.0)\n    eta = _ParameterDS(default=0.0)\n\n\nclass Pix2Sky_ZenithalEquidistant(Pix2SkyProjection, Zenithal):\n    r\"\"\"\n    Zenithal equidistant projection - pixel to sky.\n\n    Corresponds to the ``ARC`` projection in FITS WCS.\n\n    See `Zenithal` for a de"}, {"start_line": 6000, "end_line": 8000, "belongs_to": {"file_name": "projections.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " = True\n    _input_units_allow_dimensionless = True\n\n    def __new__(cls, *args, **kwargs):\n        long_name = cls.name.split(\"_\")[1]\n        cls.prj_code = _PROJ_NAME_CODE_MAP[long_name]\n        return super().__new__(cls)\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self._prj.code = self.prj_code\n        self._update_prj()\n        if not self.param_names:\n            # force initial call to Prjprm.set() for projections\n            # without parameters:\n            self._prj.set()\n\n        self.inputs = (\"phi\", \"theta\")\n        self.outputs = (\"x\", \"y\")\n\n    @property\n    def input_units(self):\n        return {self.inputs[0]: u.deg, self.inputs[1]: u.deg}\n\n    @property\n    def return_units(self):\n        return {self.outputs[0]: u.deg, self.outputs[1]: u.deg}\n\n    def evaluate(self, phi, theta, *args, **kwargs):\n        self._update_prj()\n        return self._prj.prjs2x(phi, theta)\n\n    @property\n    def inverse(self):\n        pv = [getattr(self, param).value for param in self.param_names]\n        return self._inv_cls(*pv)\n\n\nclass Zenithal(Projection):\n    r\"\"\"Base class for all Zenithal projections.\n\n    Zenithal (or azimuthal) projections map the sphere directly onto a\n    plane.  All zenithal projections are specified by defining the\n    radius as a function of native latitude, :math:`R_\\theta`.\n\n    The pixel-to-sky transformation is defined as:\n\n    .. math::\n        \\phi &= \\arg(-y, x) \\\\\n        R_\\theta &= \\sqrt{x^2 + y^2}\n\n    and the inverse (sky-to-pixel) is defined as:\n\n    .. math::\n        x &= R_\\theta \\sin \\phi \\\\\n        y &= R_\\theta \\cos \\phi\n    \"\"\"\n\n\nclass Pix2Sky_ZenithalPerspective(Pix2SkyProjection, Zenithal):\n    r\"\"\"\n    Zenithal perspective projection - pixel to sky.\n\n    Corresponds to the ``AZP`` projection in FITS WCS.\n\n    .. math::\n        \\phi &= \\arg(-y \\cos \\gamma, x) \\\\\n        \\theta &= \\left\\{\\genfrac{}{}{0pt}{}{\\psi - \\omega}{\\psi + \\omega + 180^{\\circ}}\\right.\n\n    where:\n\n   "}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "projections.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "efinition of the full transformation.\n\n    .. math::\n        \\theta = \\tan^{-1}\\left(\\frac{180^{\\circ}}{\\pi R_\\theta}\\right)\n    \"\"\"\n\n\nclass Sky2Pix_Gnomonic(Sky2PixProjection, Zenithal):\n    r\"\"\"\n    Gnomonic Projection - sky to pixel.\n\n    Corresponds to the ``TAN`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    .. math::\n        R_\\theta = \\frac{180^{\\circ}}{\\pi}\\cot \\theta\n    \"\"\"\n\n\nclass Pix2Sky_Stereographic(Pix2SkyProjection, Zenithal):\n    r\"\"\"\n    Stereographic Projection - pixel to sky.\n\n    Corresponds to the ``STG`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    .. math::\n        \\theta = 90^{\\circ} - 2 \\tan^{-1}\\left(\\frac{\\pi R_\\theta}{360^{\\circ}}\\right)\n    \"\"\"\n\n\nclass Sky2Pix_Stereographic(Sky2PixProjection, Zenithal):\n    r\"\"\"\n    Stereographic Projection - sky to pixel.\n\n    Corresponds to the ``STG`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    .. math::\n        R_\\theta = \\frac{180^{\\circ}}{\\pi}\\frac{2 \\cos \\theta}{1 + \\sin \\theta}\n    \"\"\"\n\n\nclass Pix2Sky_SlantOrthographic(Pix2SkyProjection, Zenithal):\n    r\"\"\"\n    Slant orthographic projection - pixel to sky.\n\n    Corresponds to the ``SIN`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    The following transformation applies when :math:`\\xi` and\n    :math:`\\eta` are both zero.\n\n    .. math::\n        \\theta = \\cos^{-1}\\left(\\frac{\\pi}{180^{\\circ}}R_\\theta\\right)\n\n    The parameters :math:`\\xi` and :math:`\\eta` are defined from the\n    reference point :math:`(\\phi_c, \\theta_c)` as:\n\n    .. math::\n        \\xi &= \\cot \\theta_c \\sin \\phi_c \\\\\n        \\eta &= - \\cot \\theta_c \\cos \\phi_c\n\n    Parameters\n    ----------\n    xi : float\n        Obliqueness parameter, .  Default is 0.0.\n\n    eta : float\n        Obliqueness parameter, .  Default is 0.0.\n\n    \"\"\"\n\n    xi = _ParameterDS(default=0.0, description=\"Obliquenes"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "projections.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "tattr(self, param).value for param in self.param_names]\n        return self._inv_cls(*pv)\n\n\nclass Zenithal(Projection):\n    r\"\"\"Base class for all Zenithal projections.\n\n    Zenithal (or azimuthal) projections map the sphere directly onto a\n    plane.  All zenithal projections are specified by defining the\n    radius as a function of native latitude, :math:`R_\\theta`.\n\n    The pixel-to-sky transformation is defined as:\n\n    .. math::\n        \\phi &= \\arg(-y, x) \\\\\n        R_\\theta &= \\sqrt{x^2 + y^2}\n\n    and the inverse (sky-to-pixel) is defined as:\n\n    .. math::\n        x &= R_\\theta \\sin \\phi \\\\\n        y &= R_\\theta \\cos \\phi\n    \"\"\"\n\n\nclass Pix2Sky_ZenithalPerspective(Pix2SkyProjection, Zenithal):\n    r\"\"\"\n    Zenithal perspective projection - pixel to sky.\n\n    Corresponds to the ``AZP`` projection in FITS WCS.\n\n    .. math::\n        \\phi &= \\arg(-y \\cos \\gamma, x) \\\\\n        \\theta &= \\left\\{\\genfrac{}{}{0pt}{}{\\psi - \\omega}{\\psi + \\omega + 180^{\\circ}}\\right.\n\n    where:\n\n    .. math::\n        \\psi &= \\arg(\\rho, 1) \\\\\n        \\omega &= \\sin^{-1}\\left(\\frac{\\rho \\mu}{\\sqrt{\\rho^2 + 1}}\\right) \\\\\n        \\rho &= \\frac{R}{\\frac{180^{\\circ}}{\\pi}(\\mu + 1) + y \\sin \\gamma} \\\\\n        R &= \\sqrt{x^2 + y^2 \\cos^2 \\gamma}\n\n    Parameters\n    ----------\n    mu : float\n        Distance from point of projection to center of sphere\n        in spherical radii, .  Default is 0.\n\n    gamma : float\n        Look angle  in degrees.  Default is 0.\n\n    \"\"\"\n\n    mu = _ParameterDS(\n        default=0.0, description=\"Distance from point of projection to center of sphere\"\n    )\n    gamma = _ParameterDS(\n        default=0.0,\n        getter=_to_orig_unit,\n        setter=_to_radian,\n        description=\"Look angle  in degrees (Default = 0)\",\n    )\n\n    def _mu_validator(self, value):\n        if np.any(np.equal(value, -1.0)):\n            raise InputParameterError(\n                \"Zenithal perspective projection is not defined for mu = -1\"\n            )\n\n    mu._validator = _mu_"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "projections.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "latitude  of the reference point, in degrees.  Default\n        is 90.\n\n    \"\"\"\n\n    mu = _ParameterDS(\n        default=0.0, description=\"Distance from point of projection to center of sphere\"\n    )\n    phi0 = _ParameterDS(\n        default=0.0,\n        getter=_to_orig_unit,\n        setter=_to_radian,\n        description=\"The longitude  of the reference point in degrees\",\n    )\n    theta0 = _ParameterDS(\n        default=0.0,\n        getter=_to_orig_unit,\n        setter=_to_radian,\n        description=\"The latitude  of the reference point, in degrees\",\n    )\n\n    def _mu_validator(self, value):\n        if np.any(np.equal(value, -1.0)):\n            raise InputParameterError(\n                \"Zenithal perspective projection is not defined for mu = -1\"\n            )\n\n    mu._validator = _mu_validator\n\n\nclass Pix2Sky_Gnomonic(Pix2SkyProjection, Zenithal):\n    r\"\"\"\n    Gnomonic projection - pixel to sky.\n\n    Corresponds to the ``TAN`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    .. math::\n        \\theta = \\tan^{-1}\\left(\\frac{180^{\\circ}}{\\pi R_\\theta}\\right)\n    \"\"\"\n\n\nclass Sky2Pix_Gnomonic(Sky2PixProjection, Zenithal):\n    r\"\"\"\n    Gnomonic Projection - sky to pixel.\n\n    Corresponds to the ``TAN`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    .. math::\n        R_\\theta = \\frac{180^{\\circ}}{\\pi}\\cot \\theta\n    \"\"\"\n\n\nclass Pix2Sky_Stereographic(Pix2SkyProjection, Zenithal):\n    r\"\"\"\n    Stereographic Projection - pixel to sky.\n\n    Corresponds to the ``STG`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    .. math::\n        \\theta = 90^{\\circ} - 2 \\tan^{-1}\\left(\\frac{\\pi R_\\theta}{360^{\\circ}}\\right)\n    \"\"\"\n\n\nclass Sky2Pix_Stereographic(Sky2PixProjection, Zenithal):\n    r\"\"\"\n    Stereographic Projection - sky to pixel.\n\n    Corresponds to the ``STG`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full t"}, {"start_line": 17000, "end_line": 19000, "belongs_to": {"file_name": "projections.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "on.\n\n    .. math::\n        R_\\theta &= \\frac{180^\\circ}{\\pi} \\sqrt{2(1 - \\sin\\theta)} \\\\\n                 &= \\frac{360^\\circ}{\\pi} \\sin\\left(\\frac{90^\\circ - \\theta}{2}\\right)\n    \"\"\"\n\n\nclass Pix2Sky_Airy(Pix2SkyProjection, Zenithal):\n    r\"\"\"\n    Airy projection - pixel to sky.\n\n    Corresponds to the ``AIR`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    Parameters\n    ----------\n    theta_b : float\n        The latitude :math:`\\theta_b` at which to minimize the error,\n        in degrees.  Default is 90.\n    \"\"\"\n\n    theta_b = _ParameterDS(default=90.0)\n\n\nclass Sky2Pix_Airy(Sky2PixProjection, Zenithal):\n    r\"\"\"\n    Airy - sky to pixel.\n\n    Corresponds to the ``AIR`` projection in FITS WCS.\n\n    See `Zenithal` for a definition of the full transformation.\n\n    .. math::\n        R_\\theta = -2 \\frac{180^\\circ}{\\pi}\\left(\\frac{\\ln(\\cos \\xi)}{\\tan \\xi} +\n            \\frac{\\ln(\\cos \\xi_b)}{\\tan^2 \\xi_b} \\tan \\xi \\right)\n\n    where:\n\n    .. math::\n        \\xi &= \\frac{90^\\circ - \\theta}{2} \\\\\n        \\xi_b &= \\frac{90^\\circ - \\theta_b}{2}\n\n    Parameters\n    ----------\n    theta_b : float\n        The latitude :math:`\\theta_b` at which to minimize the error,\n        in degrees.  Default is 90.\n\n    \"\"\"\n\n    theta_b = _ParameterDS(\n        default=90.0,\n        description=\"The latitude at which to minimize the error,in degrees\",\n    )\n\n\nclass Cylindrical(Projection):\n    r\"\"\"Base class for Cylindrical projections.\n\n    Cylindrical projections are so-named because the surface of\n    projection is a cylinder.\n    \"\"\"\n\n    _separable = True\n\n\nclass Pix2Sky_CylindricalPerspective(Pix2SkyProjection, Cylindrical):\n    r\"\"\"\n    Cylindrical perspective - pixel to sky.\n\n    Corresponds to the ``CYP`` projection in FITS WCS.\n\n    .. math::\n        \\phi &= \\frac{x}{\\lambda} \\\\\n        \\theta &= \\arg(1, \\eta) + \\sin{-1}\\left(\\frac{\\eta \\mu}{\\sqrt{\\eta^2 + 1}}\\right)\n\n    where:\n\n    .. math::\n        \\eta = \\frac{\\pi}{180^{\\circ}}\\frac"}, {"start_line": 35000, "end_line": 37000, "belongs_to": {"file_name": "projections.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "                  {\\tan\\left(\\frac{90^\\circ-\\theta_1}{2}\\right)} \\right] } \\\\\n        R_\\theta &= \\psi \\left[ \\tan \\left( \\frac{90^\\circ - \\theta}{2} \\right) \\right]^C \\\\\n        Y_0 &= \\psi \\left[ \\tan \\left( \\frac{90^\\circ - \\theta_a}{2} \\right) \\right]^C\n\n    where:\n\n    .. math::\n\n        \\psi = \\frac{180^\\circ}{\\pi} \\frac{\\cos \\theta}\n               {C\\left[\\tan\\left(\\frac{90^\\circ-\\theta}{2}\\right)\\right]^C}\n\n    Parameters\n    ----------\n    sigma : float\n        :math:`(\\theta_1 + \\theta_2) / 2`, where :math:`\\theta_1` and\n        :math:`\\theta_2` are the latitudes of the standard parallels,\n        in degrees.  Default is 90.\n\n    delta : float\n        :math:`(\\theta_1 - \\theta_2) / 2`, where :math:`\\theta_1` and\n        :math:`\\theta_2` are the latitudes of the standard parallels,\n        in degrees.  Default is 0.\n    \"\"\"\n\n\nclass PseudoConic(Projection):\n    r\"\"\"Base class for pseudoconic projections.\n\n    Pseudoconics are a subclass of conics with concentric parallels.\n    \"\"\"\n\n\nclass Pix2Sky_BonneEqualArea(Pix2SkyProjection, PseudoConic):\n    r\"\"\"\n    Bonne's equal area pseudoconic projection - pixel to sky.\n\n    Corresponds to the ``BON`` projection in FITS WCS.\n\n    .. math::\n\n        \\phi &= \\frac{\\pi}{180^\\circ} A_\\phi R_\\theta / \\cos \\theta \\\\\n        \\theta &= Y_0 - R_\\theta\n\n    where:\n\n    .. math::\n\n        R_\\theta &= \\mathrm{sign} \\theta_1 \\sqrt{x^2 + (Y_0 - y)^2} \\\\\n        A_\\phi &= \\arg\\left(\\frac{Y_0 - y}{R_\\theta}, \\frac{x}{R_\\theta}\\right)\n\n    Parameters\n    ----------\n    theta1 : float\n        Bonne conformal latitude, in degrees.\n    \"\"\"\n\n    _separable = True\n\n    theta1 = _ParameterDS(default=0.0, getter=_to_orig_unit, setter=_to_radian)\n\n\nclass Sky2Pix_BonneEqualArea(Sky2PixProjection, PseudoConic):\n    r\"\"\"\n    Bonne's equal area pseudoconic projection - sky to pixel.\n\n    Corresponds to the ``BON`` projection in FITS WCS.\n\n    .. math::\n        x &= R_\\theta \\sin A_\\phi \\\\\n        y &= -R_\\theta \\cos A_\\phi + Y_0\n\n    where:\n\n "}, {"start_line": 21000, "end_line": 23000, "belongs_to": {"file_name": "projections.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/modeling", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "y_CylindricalEqualArea(Pix2SkyProjection, Cylindrical):\n    r\"\"\"\n    Cylindrical equal area projection - pixel to sky.\n\n    Corresponds to the ``CEA`` projection in FITS WCS.\n\n    .. math::\n        \\phi &= x \\\\\n        \\theta &= \\sin^{-1}\\left(\\frac{\\pi}{180^{\\circ}}\\lambda y\\right)\n\n    Parameters\n    ----------\n    lam : float\n        Radius of the cylinder in spherical radii, .  Default is 1.\n    \"\"\"\n\n    lam = _ParameterDS(default=1)\n\n\nclass Sky2Pix_CylindricalEqualArea(Sky2PixProjection, Cylindrical):\n    r\"\"\"\n    Cylindrical equal area projection - sky to pixel.\n\n    Corresponds to the ``CEA`` projection in FITS WCS.\n\n    .. math::\n        x &= \\phi \\\\\n        y &= \\frac{180^{\\circ}}{\\pi}\\frac{\\sin \\theta}{\\lambda}\n\n    Parameters\n    ----------\n    lam : float\n        Radius of the cylinder in spherical radii, .  Default is 0.\n    \"\"\"\n\n    lam = _ParameterDS(default=1)\n\n\nclass Pix2Sky_PlateCarree(Pix2SkyProjection, Cylindrical):\n    r\"\"\"\n    Plate carre projection - pixel to sky.\n\n    Corresponds to the ``CAR`` projection in FITS WCS.\n\n    .. math::\n        \\phi &= x \\\\\n        \\theta &= y\n    \"\"\"\n\n    @staticmethod\n    def evaluate(x, y):\n        # The intermediate variables are only used here for clarity\n        phi = np.array(x)\n        theta = np.array(y)\n        return phi, theta\n\n\nclass Sky2Pix_PlateCarree(Sky2PixProjection, Cylindrical):\n    r\"\"\"\n    Plate carre projection - sky to pixel.\n\n    Corresponds to the ``CAR`` projection in FITS WCS.\n\n    .. math::\n        x &= \\phi \\\\\n        y &= \\theta\n    \"\"\"\n\n    @staticmethod\n    def evaluate(phi, theta):\n        # The intermediate variables are only used here for clarity\n        x = np.array(phi)\n        y = np.array(theta)\n        return x, y\n\n\nclass Pix2Sky_Mercator(Pix2SkyProjection, Cylindrical):\n    r\"\"\"\n    Mercator - pixel to sky.\n\n    Corresponds to the ``MER`` projection in FITS WCS.\n\n    .. math::\n        \\phi &= x \\\\\n        \\theta &= 2 \\tan^{-1}\\left(e^{y \\pi / 180^{\\circ}}\\right)-90^{\\"}], "retrieved_count": 10, "cost_time": 0.3409433364868164}
{"question": "Where in the astropy codebase is the mechanism that propagates boolean exclusion flags during array shape expansion implemented, coordinating between array wrapper objects that track excluded elements and the coordinate grid generation routine from the numerical computing library?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 3000, "end_line": 5000, "belongs_to": {"file_name": "column.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/table", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " corresponding to the mixin\n    column data shape.  The ``mask`` looks like a normal numpy array but an\n    exception will be raised if ``True`` is assigned to any element.  The\n    consequences of the limitation are most obvious in the high-level table\n    operations.\n\n    Parameters\n    ----------\n    shape : tuple\n        Data shape\n    \"\"\"\n\n    def __new__(cls, shape):\n        obj = np.zeros(shape, dtype=bool).view(cls)\n        return obj\n\n    def __setitem__(self, item, val):\n        val = np.asarray(val)\n        if np.any(val):\n            raise ValueError(\n                f\"Cannot set any element of {type(self).__name__} class to True\"\n            )\n\n\ndef _expand_string_array_for_values(arr, values):\n    \"\"\"\n    For string-dtype return a version of ``arr`` that is wide enough for ``values``.\n    If ``arr`` is not string-dtype or does not need expansion then return ``arr``.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Input array\n    values : scalar or array-like\n        Values for width comparison for string arrays\n\n    Returns\n    -------\n    arr_expanded : np.ndarray\n\n    \"\"\"\n    if arr.dtype.kind in (\"U\", \"S\") and values is not np.ma.masked:\n        # Starting with numpy 2.0, np.char.str_len() propagates the mask for\n        # masked data. We want masked values to be preserved so unmask\n        # `values` prior to counting string lengths.\n        values = np.asarray(values)\n        # Find the length of the longest string in the new values.\n        values_str_len = np.char.str_len(values).max()\n\n        # Determine character repeat count of arr.dtype.  Returns a positive\n        # int or None (something like 'U0' is not possible in numpy).  If new values\n        # are longer than current then make a new (wider) version of arr.\n        arr_str_len = dtype_bytes_or_chars(arr.dtype)\n        if arr_str_len and values_str_len > arr_str_len:\n            arr_dtype = arr.dtype.byteorder + arr.dtype.kind + str(values_str_len)\n            arr = arr.ast"}, {"start_line": 33000, "end_line": 35000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "        # masked too (TODO: for multiple core dimensions\n                            # this may be too strong).\n                            mask = np.logical_or.reduce(\n                                mask, axis=axis, keepdims=keepdims\n                            )\n                        in_masks.append(mask)\n\n                if ufunc.nout == 1 and out_sig[0] == ():\n                    # Special-case where possible in-place is easy.\n                    mask = combine_masks(in_masks, out=out_mask, copy=False)\n                else:\n                    # Here, some masks may need expansion, so we forego in-place.\n                    mask = combine_masks(in_masks, copy=False)\n                    result_masks = []\n                    for os, omask, axis in zip(out_sig, out_masks, axes[ufunc.nin :]):\n                        if os:\n                            # Output has core dimensions.  Assume all those\n                            # get the same mask.\n                            if axis is None:\n                                axis = tuple(range(-1, -1 - len(os), -1))\n                            result_mask = np.expand_dims(mask, axis)\n                        else:\n                            result_mask = mask\n                        if omask is not None:\n                            omask[...] = result_mask\n                        result_masks.append(result_mask)\n\n                    mask = result_masks if ufunc.nout > 1 else result_masks[0]\n\n        elif method == \"__call__\":\n            # Regular ufunc call.\n            # Combine the masks from the input, possibly selecting elements.\n            mask = combine_masks(masks, out=out_mask, where=where_unmasked)\n            # If relevant, also mask output elements for which where was masked.\n            if where_mask is not None:\n                mask |= where_mask\n            if out_mask is not None:\n                # Check for any additional explicitly given outputs.\n                for m in out_masks[1:]:\n            "}, {"start_line": 20000, "end_line": 22000, "belongs_to": {"file_name": "function_helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " \"g\":\n            result = b.astype(\"G\")\n        else:\n            result = b.astype(\"D\")\n    else:\n        result = b\n\n    return result, None, None\n\n\n@dispatched_function\ndef concatenate(arrays, axis=0, out=None, dtype=None, casting=\"same_kind\"):\n    data, masks = _get_data_and_mask_arrays(arrays)\n    if out is None:\n        return (\n            np.concatenate(data, axis=axis, dtype=dtype, casting=casting),\n            np.concatenate(masks, axis=axis),\n            None,\n        )\n    else:\n        from astropy.utils.masked import Masked\n\n        if not isinstance(out, Masked):\n            raise NotImplementedError\n        np.concatenate(masks, out=out.mask, axis=axis)\n        np.concatenate(data, out=out.unmasked, axis=axis, dtype=dtype, casting=casting)\n        return out, None, None\n\n\n@apply_to_both\ndef append(arr, values, axis=None):\n    data, masks = _get_data_and_mask_arrays((arr, values))\n    return data, masks, dict(axis=axis), None\n\n\n@dispatched_function\ndef block(arrays):\n    # We need to override block since the numpy implementation can take two\n    # different paths, one for concatenation, one for creating a large empty\n    # result array in which parts are set.  Each assumes array input and\n    # cannot be used directly.  Since it would be very costly to inspect all\n    # arrays and then turn them back into a nested list, we just copy here the\n    # second implementation, np.core.shape_base._block_slicing, since it is\n    # shortest and easiest.\n    from astropy.utils.masked import Masked\n\n    arrays, list_ndim, result_ndim, final_size = np_core.shape_base._block_setup(arrays)\n    shape, slices, arrays = np_core.shape_base._block_info_recursion(\n        arrays, list_ndim, result_ndim\n    )\n    dtype = np.result_type(*[arr.dtype for arr in arrays])\n    F_order = all(arr.flags[\"F_CONTIGUOUS\"] for arr in arrays)\n    C_order = all(arr.flags[\"C_CONTIGUOUS\"] for arr in arrays)\n    order = \"F\" if F_order and not C_order else \"C\"\n    result = Masked(np.empty(s"}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "test_representation_methods.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "        )\n        assert 0 not in s2.lon.strides\n        assert 0 in s2.lat.strides\n        with pytest.raises(AttributeError):\n            s2.shape = (42,)\n        assert s2.shape == oldshape\n        assert s2.lon.shape == oldshape\n        assert s2.lat.shape == oldshape\n        assert s2.distance.shape == oldshape\n        assert 0 not in s2.lon.strides\n        assert 0 in s2.lat.strides\n\n\nclass TestShapeFunctions(ShapeSetup):\n    @needs_array_function\n    def test_broadcast_to(self):\n        s0_broadcast = np.broadcast_to(self.s0, (3, 6, 7))\n        s0_diff = s0_broadcast.differentials[\"s\"]\n        assert type(s0_broadcast) is type(self.s0)\n        assert s0_broadcast.shape == (3, 6, 7)\n        assert s0_diff.shape == s0_broadcast.shape\n        assert np.all(s0_broadcast.lon == self.s0.lon)\n        assert np.all(s0_broadcast.lat == self.s0.lat)\n        assert np.all(s0_broadcast.distance == self.s0.distance)\n        assert np.may_share_memory(s0_broadcast.lon, self.s0.lon)\n        assert np.may_share_memory(s0_broadcast.lat, self.s0.lat)\n        assert np.may_share_memory(s0_broadcast.distance, self.s0.distance)\n\n        s1_broadcast = np.broadcast_to(self.s1, shape=(3, 6, 7))\n        s1_diff = s1_broadcast.differentials[\"s\"]\n        assert s1_broadcast.shape == (3, 6, 7)\n        assert s1_diff.shape == s1_broadcast.shape\n        assert np.all(s1_broadcast.lat == self.s1.lat)\n        assert np.all(s1_broadcast.lon == self.s1.lon)\n        assert np.all(s1_broadcast.distance == self.s1.distance)\n        assert s1_broadcast.distance.shape == (3, 6, 7)\n        assert np.may_share_memory(s1_broadcast.lat, self.s1.lat)\n        assert np.may_share_memory(s1_broadcast.lon, self.s1.lon)\n        assert np.may_share_memory(s1_broadcast.distance, self.s1.distance)\n\n        # A final test that \"may_share_memory\" equals \"does_share_memory\"\n        # Do this on a copy, to keep self.s0 unchanged.\n        sc = self.s0.copy()\n        assert not np.may_share_memory(sc.lon, self.s0.l"}, {"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "test_representation_methods.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/coordinates/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "on)\n        assert not np.may_share_memory(sc.lat, self.s0.lat)\n        sc_broadcast = np.broadcast_to(sc, (3, 6, 7))\n        assert np.may_share_memory(sc_broadcast.lon, sc.lon)\n        # Can only write to copy, not to broadcast version.\n        sc.lon[0, 0] = 22.0 * u.hourangle\n        assert np.all(sc_broadcast.lon[:, 0, 0] == 22.0 * u.hourangle)\n\n    @needs_array_function\n    def test_atleast_1d(self):\n        s00 = self.s0.ravel()[0]\n        assert s00.ndim == 0\n        s00_1d = np.atleast_1d(s00)\n        assert s00_1d.ndim == 1\n        assert np.all(representation_equal(s00[np.newaxis], s00_1d))\n        assert np.may_share_memory(s00_1d.lon, s00.lon)\n\n    @needs_array_function\n    def test_atleast_2d(self):\n        s0r = self.s0.ravel()\n        assert s0r.ndim == 1\n        s0r_2d = np.atleast_2d(s0r)\n        assert s0r_2d.ndim == 2\n        assert np.all(representation_equal(s0r[np.newaxis], s0r_2d))\n        assert np.may_share_memory(s0r_2d.lon, s0r.lon)\n\n    @needs_array_function\n    def test_atleast_3d(self):\n        assert self.s0.ndim == 2\n        s0_3d, s1_3d = np.atleast_3d(self.s0, self.s1)\n        assert s0_3d.ndim == s1_3d.ndim == 3\n        assert np.all(representation_equal(self.s0[:, :, np.newaxis], s0_3d))\n        assert np.all(representation_equal(self.s1[:, :, np.newaxis], s1_3d))\n        assert np.may_share_memory(s0_3d.lon, self.s0.lon)\n\n    def test_move_axis(self):\n        # Goes via transpose so works without __array_function__ as well.\n        s0_10 = np.moveaxis(self.s0, 0, 1)\n        assert s0_10.shape == (self.s0.shape[1], self.s0.shape[0])\n        assert np.all(representation_equal(self.s0.T, s0_10))\n        assert np.may_share_memory(s0_10.lon, self.s0.lon)\n\n    def test_roll_axis(self):\n        # Goes via transpose so works without __array_function__ as well.\n        s0_10 = np.rollaxis(self.s0, 1)\n        assert s0_10.shape == (self.s0.shape[1], self.s0.shape[0])\n        assert np.all(representation_equal(self.s0.T, s0_10))\n        a"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "function_helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "rn np.unwrap(p.unmasked, *args, **kwargs), p.mask.copy(), None\n\n\n@dispatched_function\ndef nan_to_num(x, copy=True, nan=0.0, posinf=None, neginf=None):\n    data = np.nan_to_num(x.unmasked, copy=copy, nan=nan, posinf=posinf, neginf=neginf)\n    if copy:\n        return (data, x.mask.copy(), None)\n    else:\n        return (x, None, None)\n\n\n# Following are simple functions related to shapes, where the same function\n# should be applied to the data and the mask.  They cannot all share the\n# same helper, because the first arguments have different names.\n@apply_to_both(\n    helps=(\n        {np.copy, np.resize, np.moveaxis, np.rollaxis, np.roll}\n        | ({np.asfarray} if NUMPY_LT_2_0 else set())  # noqa: NPY201\n    )\n)\ndef masked_a_helper(a, *args, **kwargs):\n    data, mask = _get_data_and_mask_array(a)\n    return (data,) + args, (mask,) + args, kwargs, None\n\n\n@apply_to_both(helps={np.flip, np.flipud, np.fliplr, np.rot90, np.triu, np.tril})\ndef masked_m_helper(m, *args, **kwargs):\n    data, mask = _get_data_and_mask_array(m)\n    return (data,) + args, (mask,) + args, kwargs, None\n\n\n@apply_to_both(helps={np.diag, np.diagflat})\ndef masked_v_helper(v, *args, **kwargs):\n    data, mask = _get_data_and_mask_array(v)\n    return (data,) + args, (mask,) + args, kwargs, None\n\n\n@apply_to_both(helps={np.delete})\ndef masked_arr_helper(array, *args, **kwargs):\n    data, mask = _get_data_and_mask_array(array)\n    return (data,) + args, (mask,) + args, kwargs, None\n\n\n@apply_to_both\ndef broadcast_to(array, shape, subok=False):\n    \"\"\"Broadcast array to the given shape.\n\n    Like `numpy.broadcast_to`, and applied to both unmasked data and mask.\n    Note that ``subok`` is taken to mean whether or not subclasses of\n    the unmasked data and mask are allowed, i.e., for ``subok=False``,\n    a `~astropy.utils.masked.MaskedNDArray` will be returned.\n    \"\"\"\n    data, mask = _get_data_and_mask_array(array)\n    return (data,), (mask,), dict(shape=shape, subok=subok), None\n\n\n@dispatched_function\ndef o"}, {"start_line": 17000, "end_line": 19000, "belongs_to": {"file_name": "test_methods.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ".broadcast_to(self.t1, shape=(3, 10, 5))\n        assert t1_broadcast.shape == (3, 10, 5)\n        assert np.all(t1_broadcast.jd1 == self.t1.jd1)\n        assert np.may_share_memory(t1_broadcast.jd1, self.t1.jd1)\n        assert t1_broadcast.location is self.t1.location\n        t2_broadcast = np.broadcast_to(self.t2, shape=(3, 10, 5))\n        assert t2_broadcast.shape == (3, 10, 5)\n        assert np.all(t2_broadcast.jd1 == self.t2.jd1)\n        assert np.may_share_memory(t2_broadcast.jd1, self.t2.jd1)\n        assert t2_broadcast.location.shape == t2_broadcast.shape\n        assert np.may_share_memory(t2_broadcast.location, self.t2.location)\n\n    @needs_array_function\n    def test_atleast_1d(self, use_mask):\n        self.create_data(use_mask)\n\n        t00 = self.t0.ravel()[0]\n        assert t00.ndim == 0\n        t00_1d = np.atleast_1d(t00)\n        assert t00_1d.ndim == 1\n        assert_time_all_equal(t00[np.newaxis], t00_1d)\n        # Actual jd1 will not share memory, as cast to scalar.\n        assert np.may_share_memory(t00_1d._time.jd1, t00._time.jd1)\n\n    @needs_array_function\n    def test_atleast_2d(self, use_mask):\n        self.create_data(use_mask)\n\n        t0r = self.t0.ravel()\n        assert t0r.ndim == 1\n        t0r_2d = np.atleast_2d(t0r)\n        assert t0r_2d.ndim == 2\n        assert_time_all_equal(t0r[np.newaxis], t0r_2d)\n        assert np.may_share_memory(t0r_2d.jd1, t0r.jd1)\n\n    @needs_array_function\n    def test_atleast_3d(self, use_mask):\n        self.create_data(use_mask)\n\n        assert self.t0.ndim == 2\n        t0_3d, t1_3d = np.atleast_3d(self.t0, self.t1)\n        assert t0_3d.ndim == t1_3d.ndim == 3\n        assert_time_all_equal(self.t0[:, :, np.newaxis], t0_3d)\n        assert_time_all_equal(self.t1[:, :, np.newaxis], t1_3d)\n        assert np.may_share_memory(t0_3d.jd2, self.t0.jd2)\n\n    def test_move_axis(self, use_mask):\n        # Goes via transpose so works without __array_function__ as well.\n        self.create_data(use_mask)\n\n        t0_10 = np.m"}, {"start_line": 36000, "end_line": 38000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ")\n\n            if mask is not False:\n                # By default, we simply propagate masks, since for\n                # things like np.sum, it makes no sense to do otherwise.\n                # Individual methods need to override as needed.\n                if method == \"reduce\":\n                    axis = kwargs.get(\"axis\")\n                    keepdims = kwargs.get(\"keepdims\", False)\n                    mask = np.logical_or.reduce(\n                        mask,\n                        where=where_unmasked,\n                        axis=axis,\n                        keepdims=keepdims,\n                        out=out_mask,\n                    )\n                    if where_unmasked is not True:\n                        # Mask also whole rows in which no elements were selected;\n                        # those will have been left as unmasked above.\n                        mask |= ~np.logical_or.reduce(\n                            where_unmasked, axis=axis, keepdims=keepdims\n                        )\n\n                else:\n                    # Accumulate\n                    axis = kwargs.get(\"axis\", 0)\n                    mask = np.logical_or.accumulate(mask, axis=axis, out=out_mask)\n\n            elif out is None:\n                # Can only get here if neither input nor output was masked, but\n                # perhaps where was masked (possible in \"not NUMPY_LT_1_25\").\n                # We don't support this.\n                return NotImplemented\n\n        elif method in {\"reduceat\", \"at\"}:  # pragma: no cover\n            raise NotImplementedError(\n                \"masked instances cannot yet deal with 'reduceat' or 'at'.\"\n            )\n\n        if result is None:  # pragma: no cover\n            # This happens for the \"at\" method.\n            return result\n\n        if out is not None and ufunc.nout == 1:\n            out = out[0]\n        return self._masked_result(result, mask, out)\n\n    def __array_function__(self, function, types, args, kwargs):\n        # TODO: go throug"}, {"start_line": 15000, "end_line": 17000, "belongs_to": {"file_name": "test_methods.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "_reshape_t.shape = (10, 5)  # Cannot be done without copy.\n        # check no shape was changed.\n        assert t0_reshape_t.shape == t0_reshape.T.shape\n        assert t0_reshape_t.jd1.shape == t0_reshape.T.shape\n        assert t0_reshape_t.jd2.shape == t0_reshape.T.shape\n        t1_reshape = self.t1.copy()\n        t1_reshape.shape = (2, 5, 5)\n        assert t1_reshape.shape == (2, 5, 5)\n        assert np.all(t1_reshape.jd1 == self.t1.jd1.reshape(2, 5, 5))\n        # location is a single element, so its shape should not change.\n        assert t1_reshape.location.shape == ()\n        # For reshape(5, 2, 5), the location array can remain the same.\n        # Note that we need to work directly on self.t2 here, since any\n        # copy would cause location to have the full shape.\n        self.t2.shape = (5, 2, 5)\n        assert self.t2.shape == (5, 2, 5)\n        assert self.t2.jd1.shape == (5, 2, 5)\n        assert self.t2.jd2.shape == (5, 2, 5)\n        assert self.t2.location.shape == (5, 2, 5)\n        assert self.t2.location.strides == (0, 0, 24)\n        # But for reshape(50), location would need to be copied, so this\n        # should fail.\n        oldshape = self.t2.shape\n        with pytest.raises(AttributeError):\n            self.t2.shape = (50,)\n        # check no shape was changed.\n        assert self.t2.jd1.shape == oldshape\n        assert self.t2.jd2.shape == oldshape\n        assert self.t2.location.shape == oldshape\n\n\n@pytest.mark.parametrize(\"use_mask\", (\"masked\", \"not_masked\"))\nclass TestShapeFunctions(ShapeSetup):\n    @needs_array_function\n    def test_broadcast(self, use_mask):\n        \"\"\"Test as supported numpy function.\"\"\"\n        self.create_data(use_mask)\n\n        t0_broadcast = np.broadcast_to(self.t0, shape=(3, 10, 5))\n        assert t0_broadcast.shape == (3, 10, 5)\n        assert np.all(t0_broadcast.jd1 == self.t0.jd1)\n        assert np.may_share_memory(t0_broadcast.jd1, self.t0.jd1)\n        assert t0_broadcast.location is None\n        t1_broadcast = np"}, {"start_line": 32000, "end_line": 34000, "belongs_to": {"file_name": "core.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/masked", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "kwargs.get(\"axes\")\n                if axes is None:\n                    # Maybe axis was given? (Note: ufunc will not take both.)\n                    axes = [kwargs.get(\"axis\")] * ufunc.nargs\n                elif len(axes) < ufunc.nargs:\n                    # All outputs have no core dimensions, which means axes\n                    # is not needed, but add None's for the zip below.\n                    axes = axes + [None] * (ufunc.nargs - len(axes))  # not inplace!\n                keepdims = kwargs.get(\"keepdims\", False)\n                in_masks = []\n                for sig, mask, axis in zip(in_sig, masks, axes[: ufunc.nin]):\n                    if mask is not None:\n                        if sig:\n                            if axis is None:\n                                axis = tuple(range(-1, -1 - len(sig), -1))\n                            # Input has core dimensions.  Assume that if any\n                            # value in those is masked, the output will be\n                            # masked too (TODO: for multiple core dimensions\n                            # this may be too strong).\n                            mask = np.logical_or.reduce(\n                                mask, axis=axis, keepdims=keepdims\n                            )\n                        in_masks.append(mask)\n\n                if ufunc.nout == 1 and out_sig[0] == ():\n                    # Special-case where possible in-place is easy.\n                    mask = combine_masks(in_masks, out=out_mask, copy=False)\n                else:\n                    # Here, some masks may need expansion, so we forego in-place.\n                    mask = combine_masks(in_masks, copy=False)\n                    result_masks = []\n                    for os, omask, axis in zip(out_sig, out_masks, axes[ufunc.nin :]):\n                        if os:\n                            # Output has core dimensions.  Assume all those\n                            # get the same mask.\n                            if axis i"}], "retrieved_count": 10, "cost_time": 0.35394716262817383}
{"question": "What modules imported in the helper module that provides Quantity-specific implementations for numpy function overrides would be affected if the numpy functions that examine array dimensions and structure were removed from the dictionary mapping numpy functions to their custom implementations?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 39000, "end_line": 41000, "belongs_to": {"file_name": "function_helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " unit = _quantities2arrays(ar1, ar2)\n    if return_indices:\n        unit = [unit, None, None]\n    return (ar1, ar2, assume_unique, return_indices), {}, unit, None\n\n\n@function_helper(helps=(np.setxor1d, np.union1d, np.setdiff1d))\ndef twosetop(ar1, ar2, *args, **kwargs):\n    (ar1, ar2), unit = _quantities2arrays(ar1, ar2)\n    return (ar1, ar2) + args, kwargs, unit, None\n\n\n@function_helper\ndef isin(element, test_elements, *args, **kwargs):\n    # This tests whether element is in test_elements, so we should change the unit of\n    # element to that of test_elements.\n    (ar1, ar2), unit = _quantities2arrays(element, test_elements)\n    return (ar1, ar2) + args, kwargs, None, None\n\n\n@function_helper  # np.in1d deprecated in not NUMPY_LT_2_0.\ndef in1d(ar1, ar2, *args, **kwargs):\n    # This tests whether ar1 is in ar2, so we should change the unit of\n    # ar1 to that of ar2.\n    (ar2, ar1), unit = _quantities2arrays(ar2, ar1)\n    return (ar1, ar2) + args, kwargs, None, None\n\n\n@dispatched_function\ndef apply_over_axes(func, a, axes):\n    # Copied straight from numpy/lib/shape_base, just to omit its\n    # val = asarray(a); if only it had been asanyarray, or just not there\n    # since a is assumed to an an array in the next line...\n    # Which is what we do here - we can only get here if it is a Quantity.\n    val = a\n    N = a.ndim\n    if np.array(axes).ndim == 0:\n        axes = (axes,)\n    for axis in axes:\n        if axis < 0:\n            axis = N + axis\n        args = (val, axis)\n        res = func(*args)\n        if res.ndim == val.ndim:\n            val = res\n        else:\n            res = np.expand_dims(res, axis)\n            if res.ndim == val.ndim:\n                val = res\n            else:\n                raise ValueError(\n                    \"function is not returning an array of the correct shape\"\n                )\n    # Returning unit is None to signal nothing should happen to\n    # the output.\n    return val, None, None\n\n\n@dispatched_function\ndef array_repr(arr, *ar"}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "function_helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "Error(\"function_helper requires at least one argument.\")\n\n\nfunction_helper = FunctionAssigner(FUNCTION_HELPERS)\n\ndispatched_function = FunctionAssigner(DISPATCHED_FUNCTIONS)\n\n\n@function_helper(\n    helps={\n        np.copy, np.real_if_close, np.sort_complex, np.resize,\n        np.fft.fft, np.fft.ifft, np.fft.rfft, np.fft.irfft,\n        np.fft.fft2, np.fft.ifft2, np.fft.rfft2, np.fft.irfft2,\n        np.fft.fftn, np.fft.ifftn, np.fft.rfftn, np.fft.irfftn,\n        np.fft.hfft, np.fft.ihfft,\n        np.linalg.eigvals, np.linalg.eigvalsh,\n    } | ({np.asfarray} if NUMPY_LT_2_0 else set())  # noqa: NPY201\n)  # fmt: skip\ndef invariant_a_helper(a, *args, **kwargs):\n    return (a.view(np.ndarray),) + args, kwargs, a.unit, None\n\n\n@function_helper(helps={np.tril, np.triu})\ndef invariant_m_helper(m, *args, **kwargs):\n    return (m.view(np.ndarray),) + args, kwargs, m.unit, None\n\n\n@function_helper(helps={np.fft.fftshift, np.fft.ifftshift})\ndef invariant_x_helper(x, *args, **kwargs):\n    return (x.view(np.ndarray),) + args, kwargs, x.unit, None\n\n\n# Note that ones_like does *not* work by default since if one creates an empty\n# array with a unit, one cannot just fill it with unity.  Indeed, in this\n# respect, it is a bit of an odd function for Quantity. On the other hand, it\n# matches the idea that a unit is the same as the quantity with that unit and\n# value of 1. Also, it used to work without __array_function__.\n# zeros_like does work by default for regular quantities, because numpy first\n# creates an empty array with the unit and then fills it with 0 (which can have\n# any unit), but for structured dtype this fails (0 cannot have an arbitrary\n# structured unit), so we include it here too.\n@function_helper(helps={np.ones_like, np.zeros_like})\ndef like_helper(a, *args, **kwargs):\n    subok = args[2] if len(args) > 2 else kwargs.pop(\"subok\", True)\n    unit = a.unit if subok else None\n    return (a.view(np.ndarray),) + args, kwargs, unit, None\n\n\ndef _quantity_out_as_array(out):\n    fr"}, {"start_line": 21000, "end_line": 23000, "belongs_to": {"file_name": "function_helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    @function_helper(helps={np.empty, np.ones, np.zeros})\n    def creation_helper(shape, dtype=None, order=\"C\", *, device=None):\n        return (shape, dtype, order), {\"device\": device}, UNIT_FROM_LIKE_ARG, None\n\n\nif NUMPY_LT_2_0:\n\n    @function_helper\n    def full(shape, fill_value, dtype=None, order=\"C\"):\n        return full_impl(shape, fill_value, dtype, order)\nelse:\n\n    @function_helper\n    def full(shape, fill_value, dtype=None, order=\"C\", *, device=None):\n        return full_impl(shape, fill_value, dtype, order, device=device)\n\n\ndef full_impl(shape, fill_value, *args, **kwargs):\n    out_unit = getattr(fill_value, \"unit\", UNIT_FROM_LIKE_ARG)\n    if out_unit is not UNIT_FROM_LIKE_ARG:\n        fill_value = _as_quantity(fill_value).value\n    return (shape, fill_value) + args, kwargs, out_unit, None\n\n\n@function_helper\ndef require(a, dtype=None, requirements=None):\n    out_unit = getattr(a, \"unit\", UNIT_FROM_LIKE_ARG)\n    if out_unit is not UNIT_FROM_LIKE_ARG:\n        a = _as_quantity(a).value\n    return (a, dtype, requirements), {}, out_unit, None\n\n\n@function_helper\ndef array(object, dtype=None, *, copy=True, order=\"K\", subok=False, ndmin=0):\n    out_unit = getattr(object, \"unit\", UNIT_FROM_LIKE_ARG)\n    if out_unit is not UNIT_FROM_LIKE_ARG:\n        object = _as_quantity(object).value\n    kwargs = {\"copy\": copy, \"order\": order, \"subok\": subok, \"ndmin\": ndmin}\n    return (object, dtype), kwargs, out_unit, None\n\n\nif NUMPY_LT_2_0:\n    asarray_impl_1_helps = {np.asarray, np.asanyarray}\n    asarray_impl_2_helps = {}\nelif NUMPY_LT_2_1:\n    asarray_impl_1_helps = {np.asanyarray}\n    asarray_impl_2_helps = {np.asarray}\nelse:\n    asarray_impl_1_helps = {}\n    asarray_impl_2_helps = {np.asarray, np.asanyarray}\n\n\n@function_helper(helps=asarray_impl_1_helps)\ndef asarray_impl_1(a, dtype=None, order=None):\n    out_unit = getattr(a, \"unit\", UNIT_FROM_LIKE_ARG)\n    if out_unit is not UNIT_FROM_LIKE_ARG:\n        a = _as_quantity(a).value\n    return (a, dtype, order), {}, out_unit"}, {"start_line": 29000, "end_line": 31000, "belongs_to": {"file_name": "function_helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "kwargs):\n    from astropy.units import percent\n\n    return quantile(a, q, *args, _q_unit=percent, **kwargs)\n\n\n@function_helper\ndef nanmedian(a, axis=None, out=None, overwrite_input=False, keepdims=np._NoValue):\n    return _iterable_helper(\n        a, axis=axis, out=out, overwrite_input=overwrite_input, keepdims=keepdims\n    )\n\n\n@function_helper\ndef count_nonzero(a, *args, **kwargs):\n    return (a.value,) + args, kwargs, None, None\n\n\n@function_helper(helps={np.isclose, np.allclose})\ndef close(a, b, rtol=1e-05, atol=1e-08, *args, **kwargs):\n    from astropy.units import Quantity\n\n    (a, b), unit = _quantities2arrays(a, b, unit_from_first=True)\n    # Allow number without a unit as having the unit.\n    atol = Quantity(atol, unit).value\n\n    return (a, b, rtol, atol) + args, kwargs, None, None\n\n\n@dispatched_function\ndef array_equal(a1, a2, equal_nan=False):\n    try:\n        args, unit = _quantities2arrays(a1, a2)\n    except UnitConversionError:\n        return False, None, None\n    return np.array_equal(*args, equal_nan=equal_nan), None, None\n\n\n@dispatched_function\ndef array_equiv(a1, a2):\n    try:\n        args, unit = _quantities2arrays(a1, a2)\n    except UnitConversionError:\n        return False, None, None\n    return np.array_equiv(*args), None, None\n\n\n@function_helper(helps={np.dot, np.outer})\ndef dot_like(a, b, out=None):\n    from astropy.units import Quantity\n\n    a, b = _as_quantities(a, b)\n    unit = a.unit * b.unit\n    if out is not None:\n        if not isinstance(out, Quantity):\n            raise NotImplementedError\n        return tuple(x.view(np.ndarray) for x in (a, b, out)), {}, unit, out\n    else:\n        return (a.view(np.ndarray), b.view(np.ndarray)), {}, unit, None\n\n\n@function_helper(\n    helps={\n        np.cross,\n        np.kron,\n        np.tensordot,\n    }\n)\ndef cross_like_a_b(a, b, *args, **kwargs):\n    a, b = _as_quantities(a, b)\n    unit = a.unit * b.unit\n    return (a.view(np.ndarray), b.view(np.ndarray)) + args, kwargs, unit, None\n\n\n@function_help"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "function_helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license. See LICENSE.rst except\n# for parts explicitly labelled as being (largely) copies of numpy\n# implementations; for those, see licenses/NUMPY_LICENSE.rst.\n\"\"\"Helpers for overriding numpy functions.\n\nWe override numpy functions in `~astropy.units.Quantity.__array_function__`.\nIn this module, the numpy functions are split in four groups, each of\nwhich has an associated `set` or `dict`:\n\n1. SUBCLASS_SAFE_FUNCTIONS (set), if the numpy implementation\n   supports Quantity; we pass on to ndarray.__array_function__.\n2. FUNCTION_HELPERS (dict), if the numpy implementation is usable\n   after converting quantities to arrays with suitable units,\n   and possibly setting units on the result.\n3. DISPATCHED_FUNCTIONS (dict), if the function makes sense but\n   requires a Quantity-specific implementation\n4. UNSUPPORTED_FUNCTIONS (set), if the function does not make sense.\n\nFor the FUNCTION_HELPERS `dict`, the value is a function that does the\nunit conversion.  It should take the same arguments as the numpy\nfunction would (though one can use ``*args`` and ``**kwargs``) and\nreturn a tuple of ``args, kwargs, unit, out``, where ``args`` and\n``kwargs`` will be will be passed on to the numpy implementation,\n``unit`` is a possible unit of the result (`None` if it should not be\nconverted to Quantity), and ``out`` is a possible output Quantity passed\nin, which will be filled in-place.\n\nFor the DISPATCHED_FUNCTIONS `dict`, the value is a function that\nimplements the numpy functionality for Quantity input. It should\nreturn a tuple of ``result, unit, out``, where ``result`` is generally\na plain array with the result, and ``unit`` and ``out`` are as above.\nIf unit is `None`, result gets returned directly, so one can also\nreturn a Quantity directly using ``quantity_result, None, None``.\n\n\"\"\"\n\nimport functools\nimport operator\n\nimport numpy as np\nfrom numpy.lib import recfunctions as rfn\n\nfrom astropy.units.core import dimensionless_unscaled\nfrom astropy.u"}, {"start_line": 2000, "end_line": 4000, "belongs_to": {"file_name": "function_helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "nits.errors import UnitConversionError, UnitsError, UnitTypeError\nfrom astropy.utils.compat import (\n    COPY_IF_NEEDED,\n    NUMPY_LT_2_0,\n    NUMPY_LT_2_1,\n    NUMPY_LT_2_2,\n)\n\nif NUMPY_LT_2_0:\n    import numpy.core as np_core\nelse:\n    import numpy._core as np_core\n\n# In 1.17, overrides are enabled by default, but it is still possible to\n# turn them off using an environment variable.  We use getattr since it\n# is planned to remove that possibility in later numpy versions.\nARRAY_FUNCTION_ENABLED = getattr(np_core.overrides, \"ENABLE_ARRAY_FUNCTION\", True)\nSUBCLASS_SAFE_FUNCTIONS = set()\n\"\"\"Functions with implementations supporting subclasses like Quantity.\"\"\"\nFUNCTION_HELPERS = {}\n\"\"\"Functions with implementations usable with proper unit conversion.\"\"\"\nDISPATCHED_FUNCTIONS = {}\n\"\"\"Functions for which we provide our own implementation.\"\"\"\n\nif NUMPY_LT_2_2:\n    # in numpy 2.2 these are auto detected by numpy itself\n    # xref https://github.com/numpy/numpy/issues/27451\n    SUPPORTED_NEP35_FUNCTIONS = {\n        np.arange,\n        np.empty, np.ones, np.zeros, np.full,\n        np.array, np.asarray, np.asanyarray, np.ascontiguousarray, np.asfortranarray,\n        np.frombuffer, np.fromfile, np.fromfunction, np.fromiter, np.fromstring,\n        np.require, np.identity, np.eye, np.tri, np.genfromtxt, np.loadtxt,\n    }  # fmt: skip\n    \"\"\"Functions that support a 'like' keyword argument and dispatch on it (NEP 35)\"\"\"\nelse:\n    # When our minimum becomes numpy>=2.2, this can be removed, here and in the tests\n    SUPPORTED_NEP35_FUNCTIONS = set()\n\n\"\"\"Functions that support a 'like' keyword argument and dispatch on it (NEP 35)\"\"\"\nUNSUPPORTED_FUNCTIONS = set()\n\"\"\"Functions that cannot sensibly be used with quantities.\"\"\"\nSUBCLASS_SAFE_FUNCTIONS |= {\n    np.shape, np.size, np.ndim,\n    np.reshape, np.ravel, np.moveaxis, np.rollaxis, np.swapaxes,\n    np.transpose, np.atleast_1d, np.atleast_2d, np.atleast_3d,\n    np.expand_dims, np.squeeze, np.broadcast_to, np.broadcast_arrays,\n    n"}, {"start_line": 43000, "end_line": 45000, "belongs_to": {"file_name": "function_helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "    try:\n                ff = _get_format_function(a.value, **options)\n            except Exception:\n                # Shouldn't happen, but possibly we're just not being smart\n                # enough, so let's pass things on as is.\n                pass\n            else:\n                # If the selected format function is that of numpy, we know\n                # things will fail if we pass in the Quantity, so use .value.\n                if \"numpy\" in ff.__module__:\n                    a = a.value\n\n    return (a,) + args, kwargs, None, None\n\n\n@function_helper\ndef diag(v, *args, **kwargs):\n    # Function works for *getting* the diagonal, but not *setting*.\n    # So, override always.\n    return (v.value,) + args, kwargs, v.unit, None\n\n\n@function_helper(module=np.linalg)\ndef svd(a, full_matrices=True, compute_uv=True, hermitian=False):\n    unit = a.unit\n    if compute_uv:\n        unit = (None, unit, None)\n\n    return ((a.view(np.ndarray), full_matrices, compute_uv, hermitian), {}, unit, None)\n\n\ndef _interpret_tol(tol, unit):\n    from astropy.units import Quantity\n\n    return Quantity(tol, unit).value\n\n\n@function_helper(module=np.linalg)\ndef matrix_rank(A, tol=None, *args, **kwargs):\n    if tol is not None:\n        tol = _interpret_tol(tol, A.unit)\n\n    return (A.view(np.ndarray), tol) + args, kwargs, None, None\n\n\n@function_helper(helps={np.linalg.inv, np.linalg.tensorinv})\ndef inv(a, *args, **kwargs):\n    return (a.view(np.ndarray),) + args, kwargs, 1 / a.unit, None\n\n\nif NUMPY_LT_2_0:\n\n    @function_helper(module=np.linalg)\n    def pinv(a, rcond=1e-15, *args, **kwargs):\n        rcond = _interpret_tol(rcond, a.unit)\n\n        return (a.view(np.ndarray), rcond) + args, kwargs, 1 / a.unit, None\n\nelse:\n\n    @function_helper(module=np.linalg)\n    def pinv(a, rcond=None, hermitian=False, *, rtol=np._NoValue):\n        if rcond is not None:\n            rcond = _interpret_tol(rcond, a.unit)\n        if rtol is not np._NoValue and rtol is not None:\n            rtol = _interpret_t"}, {"start_line": 1000, "end_line": 3000, "belongs_to": {"file_name": "function_helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " It should take the same arguments as the numpy\nfunction would (though one can use ``*args`` and ``**kwargs``) and\nreturn a tuple of ``args, kwargs, unit, out``, where ``args`` and\n``kwargs`` will be will be passed on to the numpy implementation,\n``unit`` is a possible unit of the result (`None` if it should not be\nconverted to Quantity), and ``out`` is a possible output Quantity passed\nin, which will be filled in-place.\n\nFor the DISPATCHED_FUNCTIONS `dict`, the value is a function that\nimplements the numpy functionality for Quantity input. It should\nreturn a tuple of ``result, unit, out``, where ``result`` is generally\na plain array with the result, and ``unit`` and ``out`` are as above.\nIf unit is `None`, result gets returned directly, so one can also\nreturn a Quantity directly using ``quantity_result, None, None``.\n\n\"\"\"\n\nimport functools\nimport operator\n\nimport numpy as np\nfrom numpy.lib import recfunctions as rfn\n\nfrom astropy.units.core import dimensionless_unscaled\nfrom astropy.units.errors import UnitConversionError, UnitsError, UnitTypeError\nfrom astropy.utils.compat import (\n    COPY_IF_NEEDED,\n    NUMPY_LT_2_0,\n    NUMPY_LT_2_1,\n    NUMPY_LT_2_2,\n)\n\nif NUMPY_LT_2_0:\n    import numpy.core as np_core\nelse:\n    import numpy._core as np_core\n\n# In 1.17, overrides are enabled by default, but it is still possible to\n# turn them off using an environment variable.  We use getattr since it\n# is planned to remove that possibility in later numpy versions.\nARRAY_FUNCTION_ENABLED = getattr(np_core.overrides, \"ENABLE_ARRAY_FUNCTION\", True)\nSUBCLASS_SAFE_FUNCTIONS = set()\n\"\"\"Functions with implementations supporting subclasses like Quantity.\"\"\"\nFUNCTION_HELPERS = {}\n\"\"\"Functions with implementations usable with proper unit conversion.\"\"\"\nDISPATCHED_FUNCTIONS = {}\n\"\"\"Functions for which we provide our own implementation.\"\"\"\n\nif NUMPY_LT_2_2:\n    # in numpy 2.2 these are auto detected by numpy itself\n    # xref https://github.com/numpy/numpy/issues/27451\n    SUPPORTED_NEP3"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "function_helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " UNSUPPORTED_FUNCTIONS |= {  # removed in numpy 2.0\n        np.sometrue, np.alltrue,  # noqa: NPY003, NPY201\n    }  # fmt: skip\n\n# Could be supported if we had a natural logarithm unit.\nUNSUPPORTED_FUNCTIONS |= {np.linalg.slogdet}\nTBD_FUNCTIONS = {\n    rfn.drop_fields, rfn.rename_fields, rfn.append_fields, rfn.join_by,\n    rfn.apply_along_fields, rfn.assign_fields_by_name,\n    rfn.find_duplicates, rfn.recursive_fill_fields, rfn.require_fields,\n    rfn.repack_fields, rfn.stack_arrays,\n}  # fmt: skip\nUNSUPPORTED_FUNCTIONS |= TBD_FUNCTIONS\nIGNORED_FUNCTIONS = {\n    # I/O - useless for Quantity, since no way to store the unit.\n    np.save, np.savez, np.savetxt, np.savez_compressed,\n    # Polynomials\n    np.poly, np.polyadd, np.polyder, np.polydiv, np.polyfit, np.polyint,\n    np.polymul, np.polysub, np.polyval, np.roots, np.vander,\n    # functions taking record arrays (which are deprecated)\n    rfn.rec_append_fields, rfn.rec_drop_fields, rfn.rec_join,\n}  # fmt: skip\nUNSUPPORTED_FUNCTIONS |= IGNORED_FUNCTIONS\n\n\nclass FunctionAssigner:\n    def __init__(self, assignments):\n        self.assignments = assignments\n\n    def __call__(self, f=None, helps=None, module=np):\n        \"\"\"Add a helper to a numpy function.\n\n        Normally used as a decorator.\n\n        If ``helps`` is given, it should be the numpy function helped (or an\n        iterable of numpy functions helped).\n\n        If ``helps`` is not given, it is assumed the function helped is the\n        numpy function with the same name as the decorated function.\n        \"\"\"\n        if f is not None:\n            if helps is None:\n                helps = getattr(module, f.__name__)\n            if not np.iterable(helps):\n                helps = (helps,)\n            for h in helps:\n                self.assignments[h] = f\n            return f\n        elif helps is not None or module is not np:\n            return functools.partial(self.__call__, helps=helps, module=module)\n        else:  # pragma: no cover\n            raise Value"}, {"start_line": 42000, "end_line": 44000, "belongs_to": {"file_name": "function_helpers.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/units/quantity_helper", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": ", **kwargs):\n    # array2string breaks on quantities as it tries to turn individual\n    # items into float, which works only for dimensionless.  Since the\n    # defaults would not keep any unit anyway, this is rather pointless -\n    # we're better off just passing on the array view.  However, one can\n    # also work around this by passing on a formatter (as is done in Angle).\n    # So, we do nothing if the formatter argument is present and has the\n    # relevant formatter for our dtype.\n    formatter = args[6] if len(args) >= 7 else kwargs.get(\"formatter\")\n\n    if formatter is None:\n        a = a.value\n    else:\n        # See whether it covers our dtype.\n        if NUMPY_LT_2_0:\n            from numpy.core.arrayprint import _get_format_function, _make_options_dict\n        else:\n            from numpy._core.arrayprint import _get_format_function, _make_options_dict\n\n        with np.printoptions(formatter=formatter) as options:\n            options = _make_options_dict(**options)\n            try:\n                ff = _get_format_function(a.value, **options)\n            except Exception:\n                # Shouldn't happen, but possibly we're just not being smart\n                # enough, so let's pass things on as is.\n                pass\n            else:\n                # If the selected format function is that of numpy, we know\n                # things will fail if we pass in the Quantity, so use .value.\n                if \"numpy\" in ff.__module__:\n                    a = a.value\n\n    return (a,) + args, kwargs, None, None\n\n\n@function_helper\ndef diag(v, *args, **kwargs):\n    # Function works for *getting* the diagonal, but not *setting*.\n    # So, override always.\n    return (v.value,) + args, kwargs, v.unit, None\n\n\n@function_helper(module=np.linalg)\ndef svd(a, full_matrices=True, compute_uv=True, hermitian=False):\n    unit = a.unit\n    if compute_uv:\n        unit = (None, unit, None)\n\n    return ((a.view(np.ndarray), full_matrices, compute_uv, hermitian), {}, unit, "}], "retrieved_count": 10, "cost_time": 0.44492030143737793}
{"question": "How does the class-level initialization method in the test class that validates remote URL functionality for leap second data manipulate the Earth rotation and reference systems service configuration to prevent the bundled IERS-B Earth orientation data table from loading during test execution?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"start_line": 14000, "end_line": 16000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "apSeconds.open()\n            assert ls2.expires > Time.now()\n            assert ls2.meta[\"data_url\"] == SYSTEM_FILE\n\n    @pytest.mark.remote_data\n    def test_auto_open_urls_always_good_enough(self):\n        # Avoid using the erfa, built-in and system files, as they might\n        # be good enough already.\n        try:\n            # Need auto_download so that IERS_B won't be loaded and\n            # cause tests to fail.\n            iers.conf.auto_download = True\n\n            self.remove_auto_open_files(\n                \"erfa\", iers.IERS_LEAP_SECOND_FILE, \"system_leap_second_file\"\n            )\n            ls = iers.LeapSeconds.open()\n            assert ls.expires > self.good_enough\n            assert ls.meta[\"data_url\"].startswith(\"http\")\n        finally:\n            # This setting is to be consistent with astropy/conftest.py\n            iers.conf.auto_download = False\n\n\nclass ERFALeapSecondsSafe:\n    \"\"\"Base class for tests that change the ERFA leap-second tables.\n\n    It ensures the original state is restored.\n    \"\"\"\n\n    def setup_method(self):\n        # Keep current leap-second table and expiration.\n        self.erfa_ls = self._erfa_ls = erfa.leap_seconds.get()\n        self.erfa_expires = self._expires = erfa.leap_seconds._expires\n\n    def teardown_method(self):\n        # Restore leap-second table and expiration.\n        erfa.leap_seconds.set(self.erfa_ls)\n        erfa.leap_seconds._expires = self._expires\n\n\nclass TestFromERFA(ERFALeapSecondsSafe):\n    def test_get_erfa_ls(self):\n        ls = iers.LeapSeconds.from_erfa()\n        assert ls.colnames == [\"year\", \"month\", \"tai_utc\"]\n        assert isinstance(ls.expires, Time)\n        assert ls.expires == self.erfa_expires\n        ls_array = np.array(ls[\"year\", \"month\", \"tai_utc\"])\n        assert np.all(ls_array == self.erfa_ls)\n\n    def test_get_built_in_erfa_ls(self):\n        ls = iers.LeapSeconds.from_erfa(built_in=True)\n        assert ls.colnames == [\"year\", \"month\", \"tai_utc\"]\n        assert isinstance(ls.expire"}, {"start_line": 13000, "end_line": 15000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "     not os.path.isfile(SYSTEM_FILE), reason=f\"system does not have {SYSTEM_FILE}\"\n    )\n    def test_system_file_used_if_not_expired(self, tmp_path):\n        # We skip the test if the system file is on a CI and is expired -\n        # we should not depend on CI keeping it up to date, but if it is,\n        # we should check that it is used if possible.\n        if iers.LeapSeconds.open(SYSTEM_FILE).expires <= self.good_enough:\n            pytest.skip(\"System leap second file is expired.\")\n\n        self.remove_auto_open_files(\"erfa\")\n        with iers.conf.set_temp(\"system_leap_second_file\", SYSTEM_FILE):\n            ls = iers.LeapSeconds.open()\n            assert ls.expires > self.good_enough\n            assert ls.meta[\"data_url\"] in (iers.IERS_LEAP_SECOND_FILE, SYSTEM_FILE)\n\n            # Also check with a \"built-in\" file that is expired\n            fake_file = make_fake_file(\"28 June 2017\", tmp_path)\n            iers.LeapSeconds._auto_open_files[0] = fake_file\n            ls2 = iers.LeapSeconds.open()\n            assert ls2.expires > Time.now()\n            assert ls2.meta[\"data_url\"] == SYSTEM_FILE\n\n    @pytest.mark.remote_data\n    def test_auto_open_urls_always_good_enough(self):\n        # Avoid using the erfa, built-in and system files, as they might\n        # be good enough already.\n        try:\n            # Need auto_download so that IERS_B won't be loaded and\n            # cause tests to fail.\n            iers.conf.auto_download = True\n\n            self.remove_auto_open_files(\n                \"erfa\", iers.IERS_LEAP_SECOND_FILE, \"system_leap_second_file\"\n            )\n            ls = iers.LeapSeconds.open()\n            assert ls.expires > self.good_enough\n            assert ls.meta[\"data_url\"].startswith(\"http\")\n        finally:\n            # This setting is to be consistent with astropy/conftest.py\n            iers.conf.auto_download = False\n\n\nclass ERFALeapSecondsSafe:\n    \"\"\"Base class for tests that change the ERFA leap-second tables.\n\n    It ensures the o"}, {"start_line": 8000, "end_line": 10000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "eapSeconds.auto_open([fake_file1, iers.IERS_LEAP_SECOND_FILE])\n        assert ls3.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n        with iers.conf.set_temp(\"auto_max_age\", None):\n            ls4 = iers.LeapSeconds.auto_open([fake_file1, fake_file2])\n        assert ls4.meta[\"data_url\"] == fake_file2\n\n\n@pytest.mark.remote_data\nclass TestRemoteURLs:\n    def setup_class(cls):\n        # Need auto_download so that IERS_B won't be loaded and cause tests to\n        # fail.\n        iers.conf.auto_download = True\n\n    def teardown_class(cls):\n        # This setting is to be consistent with astropy/conftest.py\n        iers.conf.auto_download = False\n\n    # In these tests, the results may be cached.\n    # This is fine - no need to download again.\n    def test_iers_url(self):\n        ls = iers.LeapSeconds.auto_open([iers.IERS_LEAP_SECOND_URL])\n        assert ls.expires > Time.now()\n\n    def test_ietf_url(self):\n        ls = iers.LeapSeconds.auto_open([iers.IETF_LEAP_SECOND_URL])\n        assert ls.expires > Time.now()\n\n\nclass TestDefaultAutoOpen:\n    \"\"\"Test auto_open with different _auto_open_files.\"\"\"\n\n    def setup_method(self):\n        # Identical to what is used in LeapSeconds.auto_open().\n        self.good_enough = iers.LeapSeconds._today() + TimeDelta(\n            180 - iers._none_to_float(iers.conf.auto_max_age), format=\"jd\"\n        )\n        self._auto_open_files = iers.LeapSeconds._auto_open_files.copy()\n\n    def teardown_method(self):\n        iers.LeapSeconds._auto_open_files = self._auto_open_files\n\n    def remove_auto_open_files(self, *files):\n        \"\"\"Remove some files from the auto-opener.\n\n        The default set is restored in teardown.\n        \"\"\"\n        for f in files:\n            iers.LeapSeconds._auto_open_files.remove(f)\n\n    def test_erfa_found(self):\n        # Set huge maximum age such that whatever ERFA has is OK.\n        # Since it is checked first, it should thus be found.\n        with iers.conf.set_temp(\"auto_max_age\", 100000):\n            ls ="}, {"start_line": 10000, "end_line": 12000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " iers.LeapSeconds.open()\n        assert ls.meta[\"data_url\"] == \"erfa\"\n\n    def test_builtin_found(self):\n        # Set huge maximum age such that built-in file is always OK.\n        # If we remove 'erfa', it should thus be found.\n        self.remove_auto_open_files(\"erfa\")\n        with iers.conf.set_temp(\"auto_max_age\", 100000):\n            ls = iers.LeapSeconds.open()\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n    # The test below is marked remote_data only to ensure it runs\n    # as an allowed-fail job on CI: i.e., we will notice it (eventually)\n    # but will not be misled in thinking that a PR is bad.\n    @pytest.mark.remote_data\n    def test_builtin_not_expired(self):\n        # TODO: would be nice to have automatic PRs for this!\n        ls = iers.LeapSeconds.open(iers.IERS_LEAP_SECOND_FILE)\n        assert ls.expires > self.good_enough, (\n            \"The leap second file built in to astropy is expired. Fix with:\\n\"\n            \"cd astropy/utils/iers/data/; . update_builtin_iers.sh\\n\"\n            \"and commit as a PR (for details, see release procedure).\"\n        )\n\n    def test_fake_future_file(self, tmp_path):\n        fake_file = make_fake_file(\"28 June 2345\", tmp_path)\n        # Try as system file for auto_open, setting auto_max_age such\n        # that any ERFA or system files are guaranteed to be expired.\n        with (\n            iers.conf.set_temp(\"auto_max_age\", -100000),\n            iers.conf.set_temp(\"system_leap_second_file\", fake_file),\n        ):\n            ls = iers.LeapSeconds.open()\n        assert ls.expires == Time(\"2345-06-28\", scale=\"tai\")\n        assert ls.meta[\"data_url\"] == str(fake_file)\n        # And as URL\n        fake_url = \"file:\" + urllib.request.pathname2url(fake_file)\n        with (\n            iers.conf.set_temp(\"auto_max_age\", -100000),\n            iers.conf.set_temp(\"iers_leap_second_auto_url\", fake_url),\n        ):\n            ls2 = iers.LeapSeconds.open()\n        assert ls2.expires == Time(\"2345-06-28\", s"}, {"start_line": 7000, "end_line": 9000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "str(fake_url)\n\n    def test_fake_expired_file(self, tmp_path):\n        fake_file1 = make_fake_file(\"28 June 2010\", tmp_path)\n        fake_file2 = make_fake_file(\"27 June 2012\", tmp_path)\n        # Between these and the built-in one, the built-in file is best.\n        ls = iers.LeapSeconds.auto_open(\n            [fake_file1, fake_file2, iers.IERS_LEAP_SECOND_FILE]\n        )\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n        # But if we remove the built-in one, the least expired one will be\n        # used and we get a warning that it is stale.\n        with pytest.warns(iers.IERSStaleWarning):\n            ls2 = iers.LeapSeconds.auto_open([fake_file1, fake_file2])\n        assert ls2.meta[\"data_url\"] == fake_file2\n        assert ls2.expires == Time(\"2012-06-27\", scale=\"tai\")\n\n        # Use the fake files to make sure auto_max_age is safe.\n        # Should have no warning in either example.\n        with iers.conf.set_temp(\"auto_max_age\", None):\n            ls3 = iers.LeapSeconds.auto_open([fake_file1, iers.IERS_LEAP_SECOND_FILE])\n        assert ls3.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n        with iers.conf.set_temp(\"auto_max_age\", None):\n            ls4 = iers.LeapSeconds.auto_open([fake_file1, fake_file2])\n        assert ls4.meta[\"data_url\"] == fake_file2\n\n\n@pytest.mark.remote_data\nclass TestRemoteURLs:\n    def setup_class(cls):\n        # Need auto_download so that IERS_B won't be loaded and cause tests to\n        # fail.\n        iers.conf.auto_download = True\n\n    def teardown_class(cls):\n        # This setting is to be consistent with astropy/conftest.py\n        iers.conf.auto_download = False\n\n    # In these tests, the results may be cached.\n    # This is fine - no need to download again.\n    def test_iers_url(self):\n        ls = iers.LeapSeconds.auto_open([iers.IERS_LEAP_SECOND_URL])\n        assert ls.expires > Time.now()\n\n    def test_ietf_url(self):\n        ls = iers.LeapSeconds.auto_open([iers.IETF_LEAP_SECOND_URL])\n        assert"}, {"start_line": 11000, "end_line": 13000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " update_builtin_iers.sh\\n\"\n            \"and commit as a PR (for details, see release procedure).\"\n        )\n\n    def test_fake_future_file(self, tmp_path):\n        fake_file = make_fake_file(\"28 June 2345\", tmp_path)\n        # Try as system file for auto_open, setting auto_max_age such\n        # that any ERFA or system files are guaranteed to be expired.\n        with (\n            iers.conf.set_temp(\"auto_max_age\", -100000),\n            iers.conf.set_temp(\"system_leap_second_file\", fake_file),\n        ):\n            ls = iers.LeapSeconds.open()\n        assert ls.expires == Time(\"2345-06-28\", scale=\"tai\")\n        assert ls.meta[\"data_url\"] == str(fake_file)\n        # And as URL\n        fake_url = \"file:\" + urllib.request.pathname2url(fake_file)\n        with (\n            iers.conf.set_temp(\"auto_max_age\", -100000),\n            iers.conf.set_temp(\"iers_leap_second_auto_url\", fake_url),\n        ):\n            ls2 = iers.LeapSeconds.open()\n        assert ls2.expires == Time(\"2345-06-28\", scale=\"tai\")\n        assert ls2.meta[\"data_url\"] == str(fake_url)\n\n    def test_fake_expired_file(self, tmp_path):\n        self.remove_auto_open_files(\n            \"erfa\", \"iers_leap_second_auto_url\", \"ietf_leap_second_auto_url\"\n        )\n        fake_file = make_fake_file(\"28 June 2010\", tmp_path)\n        with iers.conf.set_temp(\"system_leap_second_file\", fake_file):\n            # If we try this directly, the built-in file will be found.\n            ls = iers.LeapSeconds.open()\n            assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n            # But if we remove the built-in one, the expired one will be\n            # used and we get a warning that it is stale.\n            self.remove_auto_open_files(iers.IERS_LEAP_SECOND_FILE)\n            with pytest.warns(iers.IERSStaleWarning):\n                ls2 = iers.LeapSeconds.open()\n            assert ls2.meta[\"data_url\"] == fake_file\n            assert ls2.expires == Time(\"2010-06-28\", scale=\"tai\")\n\n    @pytest.mark.skipif(\n   "}, {"start_line": 9000, "end_line": 11000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": " ls.expires > Time.now()\n\n\nclass TestDefaultAutoOpen:\n    \"\"\"Test auto_open with different _auto_open_files.\"\"\"\n\n    def setup_method(self):\n        # Identical to what is used in LeapSeconds.auto_open().\n        self.good_enough = iers.LeapSeconds._today() + TimeDelta(\n            180 - iers._none_to_float(iers.conf.auto_max_age), format=\"jd\"\n        )\n        self._auto_open_files = iers.LeapSeconds._auto_open_files.copy()\n\n    def teardown_method(self):\n        iers.LeapSeconds._auto_open_files = self._auto_open_files\n\n    def remove_auto_open_files(self, *files):\n        \"\"\"Remove some files from the auto-opener.\n\n        The default set is restored in teardown.\n        \"\"\"\n        for f in files:\n            iers.LeapSeconds._auto_open_files.remove(f)\n\n    def test_erfa_found(self):\n        # Set huge maximum age such that whatever ERFA has is OK.\n        # Since it is checked first, it should thus be found.\n        with iers.conf.set_temp(\"auto_max_age\", 100000):\n            ls = iers.LeapSeconds.open()\n        assert ls.meta[\"data_url\"] == \"erfa\"\n\n    def test_builtin_found(self):\n        # Set huge maximum age such that built-in file is always OK.\n        # If we remove 'erfa', it should thus be found.\n        self.remove_auto_open_files(\"erfa\")\n        with iers.conf.set_temp(\"auto_max_age\", 100000):\n            ls = iers.LeapSeconds.open()\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n    # The test below is marked remote_data only to ensure it runs\n    # as an allowed-fail job on CI: i.e., we will notice it (eventually)\n    # but will not be misled in thinking that a PR is bad.\n    @pytest.mark.remote_data\n    def test_builtin_not_expired(self):\n        # TODO: would be nice to have automatic PRs for this!\n        ls = iers.LeapSeconds.open(iers.IERS_LEAP_SECOND_FILE)\n        assert ls.expires > self.good_enough, (\n            \"The leap second file built in to astropy is expired. Fix with:\\n\"\n            \"cd astropy/utils/iers/data/; ."}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport locale\nimport os\nimport pkgutil\nimport platform\nimport urllib.request\n\nimport erfa\nimport numpy as np\nimport pytest\nfrom numpy.testing import assert_array_equal\n\nimport astropy\nfrom astropy.time import Time, TimeDelta\nfrom astropy.utils.data import get_pkg_data_filename\nfrom astropy.utils.iers import iers\n\n# Import every astropy module as a test that the ERFA leap second\n# table is not updated for normal imports.\nfor finder, name, _ in pkgutil.walk_packages(astropy.__path__, prefix=\"astropy.\"):\n    finder.find_spec(name)\n\n# Now test that the erfa leap_seconds table has not been updated. This must be\n# done at the module level, which unfortunately will abort the entire test run\n# if if fails. Running within a normal pytest test will not work because the\n# other tests will end up updating this attribute by virtue of doing Time UTC\n# transformations.\nassert erfa.leap_seconds._expires is None\n\n# Tests in this module assume that the erfa.leap_seconds attribute has been\n# updated from the `erfa` package built-in table to the astropy built-in\n# leap-second table. That has the effect of ensuring that the\n# `erfa.leap_seconds.expires` property is sufficiently in the future.\niers_table = iers.LeapSeconds.auto_open()\nerfa.leap_seconds.update(iers_table)\nassert erfa.leap_seconds._expires is not None\n\nSYSTEM_FILE = \"/usr/share/zoneinfo/leap-seconds.list\"\n\n# Test leap_seconds.list in test/data.\nLEAP_SECOND_LIST = get_pkg_data_filename(\"data/leap-seconds.list\")\n\n\ndef test_configuration():\n    # This test just ensures things stay consistent.\n    # Adjust if changes are made.\n    assert iers.conf.iers_leap_second_auto_url == iers.IERS_LEAP_SECOND_URL\n    assert iers.conf.ietf_leap_second_auto_url == iers.IETF_LEAP_SECOND_URL\n\n\nclass TestReading:\n    \"\"\"Basic tests that leap seconds can be read.\"\"\"\n\n    def verify_day_month_year(self, ls):\n        assert np.all(ls[\"day\"] == 1)\n        assert np.all((ls[\"month\"] =="}, {"start_line": 12000, "end_line": 14000, "belongs_to": {"file_name": "test_leap_second.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/utils/iers/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "cale=\"tai\")\n        assert ls2.meta[\"data_url\"] == str(fake_url)\n\n    def test_fake_expired_file(self, tmp_path):\n        self.remove_auto_open_files(\n            \"erfa\", \"iers_leap_second_auto_url\", \"ietf_leap_second_auto_url\"\n        )\n        fake_file = make_fake_file(\"28 June 2010\", tmp_path)\n        with iers.conf.set_temp(\"system_leap_second_file\", fake_file):\n            # If we try this directly, the built-in file will be found.\n            ls = iers.LeapSeconds.open()\n            assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n            # But if we remove the built-in one, the expired one will be\n            # used and we get a warning that it is stale.\n            self.remove_auto_open_files(iers.IERS_LEAP_SECOND_FILE)\n            with pytest.warns(iers.IERSStaleWarning):\n                ls2 = iers.LeapSeconds.open()\n            assert ls2.meta[\"data_url\"] == fake_file\n            assert ls2.expires == Time(\"2010-06-28\", scale=\"tai\")\n\n    @pytest.mark.skipif(\n        not os.path.isfile(SYSTEM_FILE), reason=f\"system does not have {SYSTEM_FILE}\"\n    )\n    def test_system_file_used_if_not_expired(self, tmp_path):\n        # We skip the test if the system file is on a CI and is expired -\n        # we should not depend on CI keeping it up to date, but if it is,\n        # we should check that it is used if possible.\n        if iers.LeapSeconds.open(SYSTEM_FILE).expires <= self.good_enough:\n            pytest.skip(\"System leap second file is expired.\")\n\n        self.remove_auto_open_files(\"erfa\")\n        with iers.conf.set_temp(\"system_leap_second_file\", SYSTEM_FILE):\n            ls = iers.LeapSeconds.open()\n            assert ls.expires > self.good_enough\n            assert ls.meta[\"data_url\"] in (iers.IERS_LEAP_SECOND_FILE, SYSTEM_FILE)\n\n            # Also check with a \"built-in\" file that is expired\n            fake_file = make_fake_file(\"28 June 2017\", tmp_path)\n            iers.LeapSeconds._auto_open_files[0] = fake_file\n            ls2 = iers.Le"}, {"start_line": 0, "end_line": 2000, "belongs_to": {"file_name": "test_ut1.py", "upper_path": "/data2/raymone/swebench-repos/astropy/astropy/time/tests", "module": "", "define_class": [], "imports": []}, "relative_function": [], "code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport functools\n\nimport numpy as np\nimport pytest\n\nfrom astropy.time import Time\nfrom astropy.utils.iers import conf as iers_conf\nfrom astropy.utils.iers import iers  # used in testing\n\nallclose_jd = functools.partial(np.allclose, rtol=0, atol=1e-9)\nallclose_sec = functools.partial(np.allclose, rtol=1e-15, atol=1e-4)\n# 0.1 ms atol; IERS-B files change at that level.\n\ntry:\n    iers.IERS_A.open()  # check if IERS_A is available\nexcept OSError:\n    HAS_IERS_A = False\nelse:\n    HAS_IERS_A = True\n\n\ndef do_ut1_prediction_tst(iers_type):\n    tnow = Time.now()\n    iers_tab = iers_type.open()\n    tnow.delta_ut1_utc, status = iers_tab.ut1_utc(tnow, return_status=True)\n    assert status == iers.FROM_IERS_A_PREDICTION\n    tnow_ut1_jd = tnow.ut1.jd\n    assert allclose_jd(tnow_ut1_jd - tnow.jd, tnow.delta_ut1_utc / 86400)\n\n    delta_ut1_utc = tnow.delta_ut1_utc\n    with iers.earth_orientation_table.set(iers_type.open()):\n        delta2, status2 = tnow.get_delta_ut1_utc(return_status=True)\n        assert status2 == status\n        assert delta2.to_value(\"s\") == delta_ut1_utc\n\n        tnow_ut1 = tnow.ut1\n        assert tnow_ut1._delta_ut1_utc == delta_ut1_utc\n        assert allclose_jd(tnow_ut1.jd - tnow.jd, tnow.delta_ut1_utc / 86400)\n\n\n@pytest.mark.remote_data\nclass TestTimeUT1Remote:\n    def setup_class(cls):\n        # Need auto_download so that IERS_B won't be loaded and cause tests to\n        # fail.\n        iers_conf.auto_download = True\n\n    def teardown_class(cls):\n        # This setting is to be consistent with astropy/conftest.py\n        iers_conf.auto_download = False\n\n    def test_utc_to_ut1(self):\n        \"Test conversion of UTC to UT1, making sure to include a leap second\"\n        t = Time(\n            [\n                \"2012-06-30 12:00:00\",\n                \"2012-06-30 23:59:59\",\n                \"2012-06-30 23:59:60\",\n                \"2012-07-01 00:00:00\",\n                \"2012-07-01 12:00:00\",\n         "}], "retrieved_count": 10, "cost_time": 1.5934844017028809}
