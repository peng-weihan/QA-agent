{"question": "What architectural trade-offs does the base metadata container class for masked arrays introduce by delegating the mechanism for choosing serialization strategies to a runtime context manager rather than an inheritance-based approach?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "_represent_as_dict", "is_method": true, "class_name": "MaskedArraySubclassInfo", "parameters": ["self"], "calls": ["_represent_as_dict", "out.setdefault", "super"], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked", "start_line": 499, "end_line": 506}, "code_snippet": "    def _represent_as_dict(self):\n        # Use the data_cls as the class name for serialization,\n        # so that we do not have to store all possible masked classes\n        # in astropy.table.serialize.__construct_mixin_classes.\n        out = super()._represent_as_dict()\n        data_cls = self._parent._data_cls\n        out.setdefault(\"__class__\", data_cls.__module__ + \".\" + data_cls.__name__)\n        return out\n", "type": "function"}, {"name": "MaskedNDArrayInfo", "docstring": "Container for meta information like name, description, format.", "methods": ["_represent_as_dict", "_construct_from_dict"], "attributes": ["attr_names", "_represent_as_dict_primary_data"], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked", "start_line": 437, "end_line": 490}, "type": "class"}, {"name": "_represent_as_dict", "is_method": true, "class_name": "MaskedColumnInfo", "parameters": ["self"], "calls": ["_represent_as_dict", "np.any", "super", "ValueError"], "code_location": {"file": "column.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 1473, "end_line": 1507}, "code_snippet": "    def _represent_as_dict(self):\n        out = super()._represent_as_dict()\n        # If we are a structured masked column, then our parent class,\n        # ColumnInfo, will already have set up a dict with masked parts,\n        # which will be serialized later, so no further work needed here.\n        if self._parent.dtype.names is not None:\n            return out\n\n        col = self._parent\n\n        # If the serialize method for this context (e.g. 'fits' or 'ecsv') is\n        # 'data_mask', that means to serialize using an explicit mask column.\n        method = self.serialize_method[self._serialize_context]\n\n        if method == \"data_mask\":\n            # Note: a driver here is a performance issue in #8443 where repr() of a\n            # np.ma.MaskedArray value is up to 10 times slower than repr of a normal array\n            # value.  So regardless of whether there are masked elements it is useful to\n            # explicitly define this as a serialized column and use col.data.data (ndarray)\n            # instead of letting it fall through to the \"standard\" serialization machinery.\n            out[\"data\"] = col.data.data\n\n            if np.any(col.mask):\n                # Only if there are actually masked elements do we add the ``mask`` column\n                out[\"mask\"] = col.mask\n\n        elif method == \"null_value\":\n            pass\n\n        else:\n            raise ValueError(\n                'serialize method must be either \"data_mask\" or \"null_value\"'\n            )\n\n        return out\n", "type": "function"}, {"name": "_represent_as_dict", "is_method": true, "class_name": "MaskedNDArrayInfo", "parameters": ["self"], "calls": ["_represent_as_dict", "np.any", "super", "np.ma.MaskedArray", "ValueError"], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked", "start_line": 456, "end_line": 482}, "code_snippet": "    def _represent_as_dict(self):\n        out = super()._represent_as_dict()\n\n        masked_array = self._parent\n\n        # If the serialize method for this context (e.g. 'fits' or 'ecsv') is\n        # 'data_mask', that means to serialize using an explicit mask column.\n        method = self.serialize_method[self._serialize_context]\n\n        if method == \"data_mask\":\n            out[\"data\"] = masked_array.unmasked\n\n            if np.any(masked_array.mask):\n                # Only if there are actually masked elements do we add the ``mask`` column\n                out[\"mask\"] = masked_array.mask\n\n        elif method == \"null_value\":\n            out[\"data\"] = np.ma.MaskedArray(\n                masked_array.unmasked, mask=masked_array.mask\n            )\n\n        else:\n            raise ValueError(\n                'serialize method must be either \"data_mask\" or \"null_value\"'\n            )\n\n        return out\n", "type": "function"}, {"name": "_baseclass", "is_method": true, "class_name": "MaskedNDArray", "parameters": ["self"], "calls": [], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked", "start_line": 664, "end_line": 671}, "code_snippet": "    def _baseclass(self):\n        \"\"\"Work-around for MaskedArray initialization.\n\n        Allows the base class to be inferred correctly when a masked instance\n        is used to initialize (or viewed as) a `~numpy.ma.MaskedArray`.\n\n        \"\"\"\n        return self._data_cls\n", "type": "function"}, {"name": "_represent_as_dict", "is_method": true, "class_name": "MaskedRecarrayInfo", "parameters": ["self"], "calls": ["self._parent.view", "masked_ndarray.info._represent_as_dict"], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked", "start_line": 1392, "end_line": 1394}, "code_snippet": "    def _represent_as_dict(self):\n        masked_ndarray = self._parent.view(np.ndarray)\n        return masked_ndarray.info._represent_as_dict()\n", "type": "function"}, {"name": "MaskedColumnInfo", "docstring": "Container for meta information like name, description, format.\n\nThis is required when the object is used as a mixin column within a table,\nbut can be used as a general way to store meta information.  In this case\nit just adds the ``mask_val`` attribute.", "methods": ["__init__", "_represent_as_dict"], "attributes": ["attr_names", "_represent_as_dict_primary_data", "mask_val"], "code_location": {"file": "column.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 1433, "end_line": 1507}, "type": "class"}, {"name": "MaskedConstant", "docstring": "A trivial extension of numpy.ma.masked.\n\nWe want to be able to put the generic term ``masked`` into a dictionary.\nThe constant ``numpy.ma.masked`` is not hashable (see\nhttps://github.com/numpy/numpy/issues/4660), so we need to extend it\nhere with a hash value.\n\nSee https://github.com/numpy/numpy/issues/11021 for rationale for\n__copy__ and __deepcopy__ methods.", "methods": ["__hash__", "__copy__", "__deepcopy__"], "attributes": [], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/ascii", "start_line": 163, "end_line": 185}, "type": "class"}, {"name": "SerializedColumn", "docstring": "Subclass of dict used to serialize  mixin columns.\n\nIt is used in the representation to contain the name and possible\nother info for a mixin column or attribute (either primary data or an\narray-like attribute) that is serialized as a column in the table.", "methods": ["shape"], "attributes": ["info"], "code_location": {"file": "serialize.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 94, "end_line": 114}, "type": "class"}, {"name": "test_recarray_represent_as_dict", "is_method": true, "class_name": "TestMaskedRecarray", "parameters": ["self"], "calls": ["self.mra.info._represent_as_dict", "info._construct_from_dict", "assert_array_equal", "assert_array_equal", "type", "type", "type", "type", "type"], "code_location": {"file": "test_masked.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked/tests", "start_line": 1566, "end_line": 1573}, "code_snippet": "    def test_recarray_represent_as_dict(self):\n        rasd = self.mra.info._represent_as_dict()\n        assert type(rasd[\"data\"]) is np.ma.MaskedArray\n        assert type(rasd[\"data\"].base) is np.ndarray\n        mra2 = type(self.mra).info._construct_from_dict(rasd)\n        assert type(mra2) is type(self.mra)\n        assert_array_equal(mra2.unmasked, self.ra)\n        assert_array_equal(mra2.mask, self.mra.mask)\n", "type": "function"}], "retrieved_count": 10, "cost_time": 2.993185043334961}
{"question": "What architectural design does the template-style string interpolation class employ to separate identifying template variables from retrieving values from configuration sections?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "TemplateInterpolation", "docstring": "Behaves like string.Template.", "methods": ["_parse_match"], "attributes": ["_cookie", "_delimiter", "_KEYCRE"], "code_location": {"file": "configobj.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/configobj", "start_line": 420, "end_line": 443}, "type": "class"}, {"name": "InterpolationEngine", "docstring": "A helper class to help perform string interpolation.\n\nThis class is an abstract base class; its descendants perform\nthe actual work.", "methods": ["__init__", "interpolate", "_fetch", "_parse_match"], "attributes": ["_KEYCRE", "_cookie"], "code_location": {"file": "configobj.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/configobj", "start_line": 284, "end_line": 404}, "type": "class"}, {"name": "ConfigParserInterpolation", "docstring": "Behaves like ConfigParser.", "methods": ["_parse_match"], "attributes": ["_cookie", "_KEYCRE"], "code_location": {"file": "configobj.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/configobj", "start_line": 408, "end_line": 416}, "type": "class"}, {"name": "Section", "docstring": "A dictionary-like object that represents a section in a config file.\n\nIt does string interpolation if the 'interpolation' attribute\nof the 'main' object is set to True.\n\nInterpolation is tried first from this object, then from the 'DEFAULT'\nsection of this object, next from the parent and its 'DEFAULT' section,\nand so on until the main object is reached.\n\nA Section will behave like an ordered dictionary - following the\norder of the ``scalars`` and ``sections`` attributes.\nYou can use this to change the order of members.\n\nIteration follows the order: scalars, then sections.", "methods": ["__setstate__", "__reduce__", "__init__", "_initialise", "_interpolate", "__getitem__", "__setitem__", "__delitem__", "get", "update", "pop", "popitem", "clear", "setdefault", "items", "keys", "values", "iteritems", "iterkeys", "itervalues", "__repr__", "dict", "merge", "rename", "walk", "as_bool", "as_int", "as_float", "as_list", "restore_default", "restore_defaults", "__init__", "shape", "dtype", "__getitem__", "_getdata"], "attributes": ["__iter__", "__str__"], "code_location": {"file": "configobj.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/configobj", "start_line": 456, "end_line": 1066}, "type": "class"}, {"name": "get", "is_method": true, "class_name": "Section", "parameters": ["self", "key", "default"], "calls": [], "code_location": {"file": "configobj.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/configobj", "start_line": 637, "end_line": 642}, "code_snippet": "    def get(self, key, default=None):\n        \"\"\"A version of ``get`` that doesn't bypass string interpolation.\"\"\"\n        try:\n            return self[key]\n        except KeyError:\n            return default\n", "type": "function"}, {"name": "__getitem__", "is_method": true, "class_name": "Section", "parameters": ["self", "key"], "calls": ["dict.__getitem__", "isinstance", "isinstance", "self._interpolate", "isinstance", "_check", "self._interpolate"], "code_location": {"file": "configobj.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/configobj", "start_line": 550, "end_line": 564}, "code_snippet": "    def __getitem__(self, key):\n        \"\"\"Fetch the item and do string interpolation.\"\"\"\n        val = dict.__getitem__(self, key)\n        if self.main.interpolation:\n            if isinstance(val, str):\n                return self._interpolate(key, val)\n            if isinstance(val, list):\n                def _check(entry):\n                    if isinstance(entry, str):\n                        return self._interpolate(key, entry)\n                    return entry\n                new = [_check(entry) for entry in val]\n                if new != val:\n                    return new\n        return val\n", "type": "function"}, {"name": "MissingInterpolationOption", "docstring": "A value specified for interpolation was missing.", "methods": ["__init__"], "attributes": [], "code_location": {"file": "configobj.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/configobj", "start_line": 272, "end_line": 276}, "type": "class"}, {"name": "_fetch", "is_method": true, "class_name": "InterpolationEngine", "parameters": ["self", "key"], "calls": ["current_section.get", "get", "MissingInterpolationOption", "isinstance", "current_section.get", "isinstance"], "code_location": {"file": "configobj.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/configobj", "start_line": 354, "end_line": 385}, "code_snippet": "    def _fetch(self, key):\n        \"\"\"Helper function to fetch values from owning section.\n\n        Returns a 2-tuple: the value, and the section where it was found.\n        \"\"\"\n        # switch off interpolation before we try and fetch anything !\n        save_interp = self.section.main.interpolation\n        self.section.main.interpolation = False\n\n        # Start at section that \"owns\" this InterpolationEngine\n        current_section = self.section\n        while True:\n            # try the current section first\n            val = current_section.get(key)\n            if val is not None and not isinstance(val, Section):\n                break\n            # try \"DEFAULT\" next\n            val = current_section.get('DEFAULT', {}).get(key)\n            if val is not None and not isinstance(val, Section):\n                break\n            # move up to parent and try again\n            # top-level's parent is itself\n            if current_section.parent is current_section:\n                # reached top level, time to give up\n                break\n            current_section = current_section.parent\n\n        # restore interpolation to previous value before returning\n        self.section.main.interpolation = save_interp\n        if val is None:\n            raise MissingInterpolationOption(key)\n        return val, current_section\n", "type": "function"}, {"name": "InterpolationError", "docstring": "Base class for the two interpolation errors.", "methods": [], "attributes": [], "code_location": {"file": "configobj.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/configobj", "start_line": 252, "end_line": 253}, "type": "class"}, {"name": "BaseFormatterLocator", "docstring": "A joint formatter/locator.", "methods": ["__init__", "values", "values", "number", "number", "spacing", "spacing", "minor_locator", "format_unit", "format_unit", "_locate_values"], "attributes": [], "code_location": {"file": "formatter_locator.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/visualization/wcsaxes", "start_line": 68, "end_line": 159}, "type": "class"}], "retrieved_count": 10, "cost_time": 3.0709755420684814}
{"question": "What is the semantic relationship between the expiration threshold computed in the test initialization method and the early-exit optimization tested across multiple test methods?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_never_expired_if_connected", "is_method": true, "class_name": "TestUpdateLeapSeconds", "parameters": ["self"], "calls": ["datetime.now"], "code_location": {"file": "test_update_leap_seconds.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 46, "end_line": 48}, "code_snippet": "    def test_never_expired_if_connected(self):\n        assert self.erfa_ls.expires > datetime.now()\n        assert self.erfa_ls.expires >= self.good_enough\n", "type": "function"}, {"name": "test_fake_expired_file", "is_method": true, "class_name": "TestAutoOpenExplicitLists", "parameters": ["self", "tmp_path"], "calls": ["make_fake_file", "make_fake_file", "iers.LeapSeconds.auto_open", "pytest.warns", "iers.LeapSeconds.auto_open", "Time", "iers.conf.set_temp", "iers.LeapSeconds.auto_open", "iers.conf.set_temp", "iers.LeapSeconds.auto_open"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 179, "end_line": 202}, "code_snippet": "    def test_fake_expired_file(self, tmp_path):\n        fake_file1 = make_fake_file(\"28 June 2010\", tmp_path)\n        fake_file2 = make_fake_file(\"27 June 2012\", tmp_path)\n        # Between these and the built-in one, the built-in file is best.\n        ls = iers.LeapSeconds.auto_open(\n            [fake_file1, fake_file2, iers.IERS_LEAP_SECOND_FILE]\n        )\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n        # But if we remove the built-in one, the least expired one will be\n        # used and we get a warning that it is stale.\n        with pytest.warns(iers.IERSStaleWarning):\n            ls2 = iers.LeapSeconds.auto_open([fake_file1, fake_file2])\n        assert ls2.meta[\"data_url\"] == fake_file2\n        assert ls2.expires == Time(\"2012-06-27\", scale=\"tai\")\n\n        # Use the fake files to make sure auto_max_age is safe.\n        # Should have no warning in either example.\n        with iers.conf.set_temp(\"auto_max_age\", None):\n            ls3 = iers.LeapSeconds.auto_open([fake_file1, iers.IERS_LEAP_SECOND_FILE])\n        assert ls3.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n        with iers.conf.set_temp(\"auto_max_age\", None):\n            ls4 = iers.LeapSeconds.auto_open([fake_file1, fake_file2])\n        assert ls4.meta[\"data_url\"] == fake_file2\n", "type": "function"}, {"name": "test_fake_expired_file", "is_method": true, "class_name": "TestDefaultAutoOpen", "parameters": ["self", "tmp_path"], "calls": ["self.remove_auto_open_files", "make_fake_file", "iers.conf.set_temp", "iers.LeapSeconds.open", "self.remove_auto_open_files", "pytest.warns", "iers.LeapSeconds.open", "Time"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 297, "end_line": 313}, "code_snippet": "    def test_fake_expired_file(self, tmp_path):\n        self.remove_auto_open_files(\n            \"erfa\", \"iers_leap_second_auto_url\", \"ietf_leap_second_auto_url\"\n        )\n        fake_file = make_fake_file(\"28 June 2010\", tmp_path)\n        with iers.conf.set_temp(\"system_leap_second_file\", fake_file):\n            # If we try this directly, the built-in file will be found.\n            ls = iers.LeapSeconds.open()\n            assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n            # But if we remove the built-in one, the expired one will be\n            # used and we get a warning that it is stale.\n            self.remove_auto_open_files(iers.IERS_LEAP_SECOND_FILE)\n            with pytest.warns(iers.IERSStaleWarning):\n                ls2 = iers.LeapSeconds.open()\n            assert ls2.meta[\"data_url\"] == fake_file\n            assert ls2.expires == Time(\"2010-06-28\", scale=\"tai\")\n", "type": "function"}, {"name": "test_init_thread_safety", "is_method": true, "class_name": "TestUpdateLeapSeconds", "parameters": ["self", "monkeypatch"], "calls": ["expired.update_erfa_leap_seconds", "monkeypatch.setattr", "ThreadPoolExecutor", "executor.submit", "future.result", "range", "str", "Time"], "code_location": {"file": "test_update_leap_seconds.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 85, "end_line": 102}, "code_snippet": "    def test_init_thread_safety(self, monkeypatch):\n        # Set up expired ERFA leap seconds.\n        expired = self.erfa_ls[self.erfa_ls[\"year\"] < 2017]\n        expired.update_erfa_leap_seconds(initialize_erfa=\"empty\")\n        # Force re-initialization, even if another test already did it\n        monkeypatch.setattr(\n            astropy.time.core,\n            \"_LEAP_SECONDS_CHECK\",\n            astropy.time.core._LeapSecondsCheck.NOT_STARTED,\n        )\n        workers = 4\n        with ThreadPoolExecutor(max_workers=workers) as executor:\n            futures = [\n                executor.submit(lambda: str(Time(\"2019-01-01 00:00:00.000\").tai))\n                for i in range(workers)\n            ]\n            results = [future.result() for future in futures]\n            assert results == [\"2019-01-01 00:00:37.000\"] * workers\n", "type": "function"}, {"name": "test_auto_update_always_good", "is_method": true, "class_name": "TestUpdateLeapSeconds", "parameters": ["self"], "calls": ["self.erfa_ls.update_erfa_leap_seconds", "update_leap_seconds"], "code_location": {"file": "test_update_leap_seconds.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 51, "end_line": 55}, "code_snippet": "    def test_auto_update_always_good(self):\n        self.erfa_ls.update_erfa_leap_seconds(initialize_erfa=\"only\")\n        update_leap_seconds()\n        assert not erfa.leap_seconds.expired\n        assert erfa.leap_seconds.expires > self.good_enough\n", "type": "function"}, {"name": "setup_method", "is_method": true, "class_name": "TestDefaultAutoOpen", "parameters": ["self"], "calls": ["iers.LeapSeconds._auto_open_files.copy", "iers.LeapSeconds._today", "TimeDelta", "iers._none_to_float"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 230, "end_line": 235}, "code_snippet": "    def setup_method(self):\n        # Identical to what is used in LeapSeconds.auto_open().\n        self.good_enough = iers.LeapSeconds._today() + TimeDelta(\n            180 - iers._none_to_float(iers.conf.auto_max_age), format=\"jd\"\n        )\n        self._auto_open_files = iers.LeapSeconds._auto_open_files.copy()\n", "type": "function"}, {"name": "test_system_file_used_if_not_expired", "is_method": true, "class_name": "TestDefaultAutoOpen", "parameters": ["self", "tmp_path"], "calls": ["pytest.mark.skipif", "self.remove_auto_open_files", "pytest.skip", "iers.conf.set_temp", "iers.LeapSeconds.open", "make_fake_file", "iers.LeapSeconds.open", "os.path.isfile", "iers.LeapSeconds.open", "Time.now"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 318, "end_line": 336}, "code_snippet": "    def test_system_file_used_if_not_expired(self, tmp_path):\n        # We skip the test if the system file is on a CI and is expired -\n        # we should not depend on CI keeping it up to date, but if it is,\n        # we should check that it is used if possible.\n        if iers.LeapSeconds.open(SYSTEM_FILE).expires <= self.good_enough:\n            pytest.skip(\"System leap second file is expired.\")\n\n        self.remove_auto_open_files(\"erfa\")\n        with iers.conf.set_temp(\"system_leap_second_file\", SYSTEM_FILE):\n            ls = iers.LeapSeconds.open()\n            assert ls.expires > self.good_enough\n            assert ls.meta[\"data_url\"] in (iers.IERS_LEAP_SECOND_FILE, SYSTEM_FILE)\n\n            # Also check with a \"built-in\" file that is expired\n            fake_file = make_fake_file(\"28 June 2017\", tmp_path)\n            iers.LeapSeconds._auto_open_files[0] = fake_file\n            ls2 = iers.LeapSeconds.open()\n            assert ls2.expires > Time.now()\n            assert ls2.meta[\"data_url\"] == SYSTEM_FILE\n", "type": "function"}, {"name": "test_auto_update_expired_file", "is_method": true, "class_name": "TestUpdateLeapSeconds", "parameters": ["self", "tmp_path"], "calls": ["expired.update_erfa_leap_seconds", "str", "write_text", "join", "pytest.warns", "update_leap_seconds", "Path", "str"], "code_location": {"file": "test_update_leap_seconds.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 71, "end_line": 83}, "code_snippet": "    def test_auto_update_expired_file(self, tmp_path):\n        # Set up expired ERFA leap seconds.\n        expired = self.erfa_ls[self.erfa_ls[\"year\"] < 2017]\n        expired.update_erfa_leap_seconds(initialize_erfa=\"empty\")\n        # Create similarly expired file.\n        expired_file = str(tmp_path / \"expired.dat\")\n        Path(expired_file).write_text(\n            \"\\n\".join(\n                [\"# File expires on 28 June 2010\"] + [str(item) for item in expired]\n            )\n        )\n        with pytest.warns(iers.IERSStaleWarning):\n            update_leap_seconds([\"erfa\", expired_file])\n", "type": "function"}, {"name": "setup_method", "is_method": true, "class_name": "ERFALeapSecondsSafe", "parameters": ["self"], "calls": ["erfa.leap_seconds.get"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 364, "end_line": 367}, "code_snippet": "    def setup_method(self):\n        # Keep current leap-second table and expiration.\n        self.erfa_ls = self._erfa_ls = erfa.leap_seconds.get()\n        self.erfa_expires = self._expires = erfa.leap_seconds._expires\n", "type": "function"}, {"name": "test_builtin_not_expired", "is_method": true, "class_name": "TestDefaultAutoOpen", "parameters": ["self"], "calls": ["iers.LeapSeconds.open"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 267, "end_line": 274}, "code_snippet": "    def test_builtin_not_expired(self):\n        # TODO: would be nice to have automatic PRs for this!\n        ls = iers.LeapSeconds.open(iers.IERS_LEAP_SECOND_FILE)\n        assert ls.expires > self.good_enough, (\n            \"The leap second file built in to astropy is expired. Fix with:\\n\"\n            \"cd astropy/utils/iers/data/; . update_builtin_iers.sh\\n\"\n            \"and commit as a PR (for details, see release procedure).\"\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.1451022624969482}
{"question": "What is the dependency relationship between the class that manages macro definitions and performs token expansion in the ANSI-C style preprocessor and the data container class that stores macro information including name, token sequence, and variadic flag, in handling macros that accept a variable number of arguments during replacement of macro references with their expanded token sequences?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "macro_prescan", "is_method": true, "class_name": "Preprocessor", "parameters": ["self", "macro"], "calls": ["macro.patch.sort", "len", "macro.arglist.index", "copy.copy", "macro.str_patch.append", "macro.patch.append", "macro.var_comma_patch.append", "macro.patch.append", "macro.patch.append", "len", "len"], "code_location": {"file": "cpp.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/ply", "start_line": 395, "end_line": 429}, "code_snippet": "    def macro_prescan(self,macro):\n        macro.patch     = []             # Standard macro arguments\n        macro.str_patch = []             # String conversion expansion\n        macro.var_comma_patch = []       # Variadic macro comma patch\n        i = 0\n        while i < len(macro.value):\n            if macro.value[i].type == self.t_ID and macro.value[i].value in macro.arglist:\n                argnum = macro.arglist.index(macro.value[i].value)\n                # Conversion of argument to a string\n                if i > 0 and macro.value[i-1].value == '#':\n                    macro.value[i] = copy.copy(macro.value[i])\n                    macro.value[i].type = self.t_STRING\n                    del macro.value[i-1]\n                    macro.str_patch.append((argnum,i-1))\n                    continue\n                # Concatenation\n                elif (i > 0 and macro.value[i-1].value == '##'):\n                    macro.patch.append(('c',argnum,i-1))\n                    del macro.value[i-1]\n                    i -= 1\n                    continue\n                elif ((i+1) < len(macro.value) and macro.value[i+1].value == '##'):\n                    macro.patch.append(('c',argnum,i))\n                    del macro.value[i + 1]\n                    continue\n                # Standard expansion\n                else:\n                    macro.patch.append(('e',argnum,i))\n            elif macro.value[i].value == '##':\n                if macro.variadic and (i > 0) and (macro.value[i-1].value == ',') and \\\n                        ((i+1) < len(macro.value)) and (macro.value[i+1].type == self.t_ID) and \\\n                        (macro.value[i+1].value == macro.vararg):\n                    macro.var_comma_patch.append(i-1)\n            i += 1\n        macro.patch.sort(key=lambda x: x[2],reverse=True)\n", "type": "function"}, {"name": "define", "is_method": true, "class_name": "Preprocessor", "parameters": ["self", "tokens"], "calls": ["isinstance", "self.tokenize", "len", "Macro", "print", "Macro", "self.tokenstrip", "self.collect_args", "print", "join", "self.tokenstrip", "Macro", "self.macro_prescan", "print", "print", "len", "str", "len", "len"], "code_location": {"file": "cpp.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/ply", "start_line": 800, "end_line": 862}, "code_snippet": "    def define(self,tokens):\n        if isinstance(tokens,STRING_TYPES):\n            tokens = self.tokenize(tokens)\n\n        linetok = tokens\n        try:\n            name = linetok[0]\n            if len(linetok) > 1:\n                mtype = linetok[1]\n            else:\n                mtype = None\n            if not mtype:\n                m = Macro(name.value,[])\n                self.macros[name.value] = m\n            elif mtype.type in self.t_WS:\n                # A normal macro\n                m = Macro(name.value,self.tokenstrip(linetok[2:]))\n                self.macros[name.value] = m\n            elif mtype.value == '(':\n                # A macro with arguments\n                tokcount, args, positions = self.collect_args(linetok[1:])\n                variadic = False\n                for a in args:\n                    if variadic:\n                        print(\"No more arguments may follow a variadic argument\")\n                        break\n                    astr = \"\".join([str(_i.value) for _i in a])\n                    if astr == \"...\":\n                        variadic = True\n                        a[0].type = self.t_ID\n                        a[0].value = '__VA_ARGS__'\n                        variadic = True\n                        del a[1:]\n                        continue\n                    elif astr[-3:] == \"...\" and a[0].type == self.t_ID:\n                        variadic = True\n                        del a[1:]\n                        # If, for some reason, \".\" is part of the identifier, strip off the name for the purposes\n                        # of macro expansion\n                        if a[0].value[-3:] == '...':\n                            a[0].value = a[0].value[:-3]\n                        continue\n                    if len(a) > 1 or a[0].type != self.t_ID:\n                        print(\"Invalid macro argument\")\n                        break\n                else:\n                    mvalue = self.tokenstrip(linetok[1+tokcount:])\n                    i = 0\n                    while i < len(mvalue):\n                        if i+1 < len(mvalue):\n                            if mvalue[i].type in self.t_WS and mvalue[i+1].value == '##':\n                                del mvalue[i]\n                                continue\n                            elif mvalue[i].value == '##' and mvalue[i+1].type in self.t_WS:\n                                del mvalue[i+1]\n                        i += 1\n                    m = Macro(name.value,mvalue,[x[0].value for x in args],variadic)\n                    self.macro_prescan(m)\n                    self.macros[name.value] = m\n            else:\n                print(\"Bad macro definition\")\n        except LookupError:\n            print(\"Bad macro definition\")\n", "type": "function"}, {"name": "Macro", "docstring": "", "methods": ["__init__"], "attributes": [], "code_location": {"file": "cpp.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/ply", "start_line": 142, "end_line": 150}, "type": "class"}, {"name": "macro_expand_args", "is_method": true, "class_name": "Preprocessor", "parameters": ["self", "macro", "args"], "calls": ["copy.copy", "copy.copy", "replace", "self.expand_macros", "join"], "code_location": {"file": "cpp.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/ply", "start_line": 439, "end_line": 478}, "code_snippet": "    def macro_expand_args(self,macro,args):\n        # Make a copy of the macro token sequence\n        rep = [copy.copy(_x) for _x in macro.value]\n\n        # Make string expansion patches.  These do not alter the length of the replacement sequence\n\n        str_expansion = {}\n        for argnum, i in macro.str_patch:\n            if argnum not in str_expansion:\n                str_expansion[argnum] = ('\"%s\"' % \"\".join([x.value for x in args[argnum]])).replace(\"\\\\\",\"\\\\\\\\\")\n            rep[i] = copy.copy(rep[i])\n            rep[i].value = str_expansion[argnum]\n\n        # Make the variadic macro comma patch.  If the variadic macro argument is empty, we get rid\n        comma_patch = False\n        if macro.variadic and not args[-1]:\n            for i in macro.var_comma_patch:\n                rep[i] = None\n                comma_patch = True\n\n        # Make all other patches.   The order of these matters.  It is assumed that the patch list\n        # has been sorted in reverse order of patch location since replacements will cause the\n        # size of the replacement sequence to expand from the patch point.\n\n        expanded = { }\n        for ptype, argnum, i in macro.patch:\n            # Concatenation.   Argument is left unexpanded\n            if ptype == 'c':\n                rep[i:i+1] = args[argnum]\n            # Normal expansion.  Argument is macro expanded first\n            elif ptype == 'e':\n                if argnum not in expanded:\n                    expanded[argnum] = self.expand_macros(args[argnum])\n                rep[i:i+1] = expanded[argnum]\n\n        # Get rid of removed comma if necessary\n        if comma_patch:\n            rep = [_i for _i in rep if _i]\n\n        return rep\n", "type": "function"}, {"name": "expand_macros", "is_method": true, "class_name": "Preprocessor", "parameters": ["self", "tokens", "expanded"], "calls": ["len", "self.expand_macros", "len", "self.t_INTEGER_TYPE", "self.collect_args", "copy.copy", "len", "len", "self.error", "len", "len", "self.macro_expand_args", "self.expand_macros", "len", "len", "len", "self.error", "self.error", "len", "len", "len", "args.append", "len", "len", "len", "len", "len", "len"], "code_location": {"file": "cpp.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/ply", "start_line": 489, "end_line": 552}, "code_snippet": "    def expand_macros(self,tokens,expanded=None):\n        if expanded is None:\n            expanded = {}\n        i = 0\n        while i < len(tokens):\n            t = tokens[i]\n            if t.type == self.t_ID:\n                if t.value in self.macros and t.value not in expanded:\n                    # Yes, we found a macro match\n                    expanded[t.value] = True\n\n                    m = self.macros[t.value]\n                    if not m.arglist:\n                        # A simple macro\n                        ex = self.expand_macros([copy.copy(_x) for _x in m.value],expanded)\n                        for e in ex:\n                            e.lineno = t.lineno\n                        tokens[i:i+1] = ex\n                        i += len(ex)\n                    else:\n                        # A macro with arguments\n                        j = i + 1\n                        while j < len(tokens) and tokens[j].type in self.t_WS:\n                            j += 1\n                        if j < len(tokens) and tokens[j].value == '(':\n                            tokcount,args,positions = self.collect_args(tokens[j:])\n                            if not m.variadic and len(args) !=  len(m.arglist):\n                                self.error(self.source,t.lineno,\"Macro %s requires %d arguments\" % (t.value,len(m.arglist)))\n                                i = j + tokcount\n                            elif m.variadic and len(args) < len(m.arglist)-1:\n                                if len(m.arglist) > 2:\n                                    self.error(self.source,t.lineno,\"Macro %s must have at least %d arguments\" % (t.value, len(m.arglist)-1))\n                                else:\n                                    self.error(self.source,t.lineno,\"Macro %s must have at least %d argument\" % (t.value, len(m.arglist)-1))\n                                i = j + tokcount\n                            else:\n                                if m.variadic:\n                                    if len(args) == len(m.arglist)-1:\n                                        args.append([])\n                                    else:\n                                        args[len(m.arglist)-1] = tokens[j+positions[len(m.arglist)-1]:j+tokcount-1]\n                                        del args[len(m.arglist):]\n\n                                # Get macro replacement text\n                                rep = self.macro_expand_args(m,args)\n                                rep = self.expand_macros(rep,expanded)\n                                for r in rep:\n                                    r.lineno = t.lineno\n                                tokens[i:j+tokcount] = rep\n                                i += len(rep)\n                        else:\n                            # This is not a macro. It is just a word which\n                            # equals to name of the macro. Hence, go to the\n                            # next token.\n                            i += 1\n\n                    del expanded[t.value]\n                    continue\n                elif t.value == '__LINE__':\n                    t.type = self.t_INTEGER\n                    t.value = self.t_INTEGER_TYPE(t.lineno)\n\n            i += 1\n        return tokens\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "Macro", "parameters": ["self", "name", "value", "arglist", "variadic"], "calls": [], "code_location": {"file": "cpp.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/ply", "start_line": 143, "end_line": 150}, "code_snippet": "    def __init__(self,name,value,arglist=None,variadic=False):\n        self.name = name\n        self.value = value\n        self.arglist = arglist\n        self.variadic = variadic\n        if variadic:\n            self.vararg = arglist[-1]\n        self.source = None\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "Preprocessor", "parameters": ["self", "lexer"], "calls": ["self.lexprobe", "time.localtime", "self.define", "self.define", "time.strftime", "time.strftime"], "code_location": {"file": "cpp.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/ply", "start_line": 160, "end_line": 174}, "code_snippet": "    def __init__(self,lexer=None):\n        if lexer is None:\n            lexer = lex.lexer\n        self.lexer = lexer\n        self.macros = { }\n        self.path = []\n        self.temp_path = []\n\n        # Probe the lexer for selected tokens\n        self.lexprobe()\n\n        tm = time.localtime()\n        self.define(\"__DATE__ \\\"%s\\\"\" % time.strftime(\"%b %d %Y\",tm))\n        self.define(\"__TIME__ \\\"%s\\\"\" % time.strftime(\"%H:%M:%S\",tm))\n        self.parser = None\n", "type": "function"}, {"name": "undef", "is_method": true, "class_name": "Preprocessor", "parameters": ["self", "tokens"], "calls": [], "code_location": {"file": "cpp.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/ply", "start_line": 870, "end_line": 875}, "code_snippet": "    def undef(self,tokens):\n        id = tokens[0].value\n        try:\n            del self.macros[id]\n        except LookupError:\n            pass\n", "type": "function"}, {"name": "parsegen", "is_method": true, "class_name": "Preprocessor", "parameters": ["self", "input", "source"], "calls": ["trigraph", "self.group_lines", "self.define", "self.expand_macros", "enumerate", "self.tokenstrip", "self.tokenstrip", "chunk.extend", "chunk.append", "self.expand_macros", "self.define", "self.expand_macros", "self.include", "self.expand_macros", "self.undef", "ifstack.append", "ifstack.append", "ifstack.append", "self.evalexpr", "self.error", "self.error", "ifstack.pop", "self.error", "self.evalexpr"], "code_location": {"file": "cpp.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/ply", "start_line": 620, "end_line": 743}, "code_snippet": "    def parsegen(self,input,source=None):\n\n        # Replace trigraph sequences\n        t = trigraph(input)\n        lines = self.group_lines(t)\n\n        if not source:\n            source = \"\"\n\n        self.define(\"__FILE__ \\\"%s\\\"\" % source)\n\n        self.source = source\n        chunk = []\n        enable = True\n        iftrigger = False\n        ifstack = []\n\n        for x in lines:\n            for i,tok in enumerate(x):\n                if tok.type not in self.t_WS: break\n            if tok.value == '#':\n                # Preprocessor directive\n\n                # insert necessary whitespace instead of eaten tokens\n                for tok in x:\n                    if tok.type in self.t_WS and '\\n' in tok.value:\n                        chunk.append(tok)\n\n                dirtokens = self.tokenstrip(x[i+1:])\n                if dirtokens:\n                    name = dirtokens[0].value\n                    args = self.tokenstrip(dirtokens[1:])\n                else:\n                    name = \"\"\n                    args = []\n\n                if name == 'define':\n                    if enable:\n                        for tok in self.expand_macros(chunk):\n                            yield tok\n                        chunk = []\n                        self.define(args)\n                elif name == 'include':\n                    if enable:\n                        for tok in self.expand_macros(chunk):\n                            yield tok\n                        chunk = []\n                        oldfile = self.macros['__FILE__']\n                        for tok in self.include(args):\n                            yield tok\n                        self.macros['__FILE__'] = oldfile\n                        self.source = source\n                elif name == 'undef':\n                    if enable:\n                        for tok in self.expand_macros(chunk):\n                            yield tok\n                        chunk = []\n                        self.undef(args)\n                elif name == 'ifdef':\n                    ifstack.append((enable,iftrigger))\n                    if enable:\n                        if not args[0].value in self.macros:\n                            enable = False\n                            iftrigger = False\n                        else:\n                            iftrigger = True\n                elif name == 'ifndef':\n                    ifstack.append((enable,iftrigger))\n                    if enable:\n                        if args[0].value in self.macros:\n                            enable = False\n                            iftrigger = False\n                        else:\n                            iftrigger = True\n                elif name == 'if':\n                    ifstack.append((enable,iftrigger))\n                    if enable:\n                        result = self.evalexpr(args)\n                        if not result:\n                            enable = False\n                            iftrigger = False\n                        else:\n                            iftrigger = True\n                elif name == 'elif':\n                    if ifstack:\n                        if ifstack[-1][0]:     # We only pay attention if outer \"if\" allows this\n                            if enable:         # If already true, we flip enable False\n                                enable = False\n                            elif not iftrigger:   # If False, but not triggered yet, we'll check expression\n                                result = self.evalexpr(args)\n                                if result:\n                                    enable  = True\n                                    iftrigger = True\n                    else:\n                        self.error(self.source,dirtokens[0].lineno,\"Misplaced #elif\")\n\n                elif name == 'else':\n                    if ifstack:\n                        if ifstack[-1][0]:\n                            if enable:\n                                enable = False\n                            elif not iftrigger:\n                                enable = True\n                                iftrigger = True\n                    else:\n                        self.error(self.source,dirtokens[0].lineno,\"Misplaced #else\")\n\n                elif name == 'endif':\n                    if ifstack:\n                        enable,iftrigger = ifstack.pop()\n                    else:\n                        self.error(self.source,dirtokens[0].lineno,\"Misplaced #endif\")\n                else:\n                    # Unknown preprocessor directive\n                    pass\n\n            else:\n                # Normal text\n                if enable:\n                    chunk.extend(x)\n\n        for tok in self.expand_macros(chunk):\n            yield tok\n        chunk = []\n", "type": "function"}, {"name": "Preprocessor", "docstring": "", "methods": ["__init__", "tokenize", "error", "lexprobe", "add_path", "group_lines", "tokenstrip", "collect_args", "macro_prescan", "macro_expand_args", "expand_macros", "evalexpr", "parsegen", "include", "define", "undef", "parse", "token"], "attributes": [], "code_location": {"file": "cpp.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/extern/ply", "start_line": 159, "end_line": 898}, "type": "class"}], "retrieved_count": 10, "cost_time": 3.2601091861724854}
{"question": "What does verifying different values for the metadata field that records which source was selected from the priority list indicate about the source selection algorithm in the test class that validates automatic loading of leap second tables with different priority configurations?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_auto_open_urls_always_good_enough", "is_method": true, "class_name": "TestDefaultAutoOpen", "parameters": ["self"], "calls": ["self.remove_auto_open_files", "iers.LeapSeconds.open", "startswith"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 339, "end_line": 355}, "code_snippet": "    def test_auto_open_urls_always_good_enough(self):\n        # Avoid using the erfa, built-in and system files, as they might\n        # be good enough already.\n        try:\n            # Need auto_download so that IERS_B won't be loaded and\n            # cause tests to fail.\n            iers.conf.auto_download = True\n\n            self.remove_auto_open_files(\n                \"erfa\", iers.IERS_LEAP_SECOND_FILE, \"system_leap_second_file\"\n            )\n            ls = iers.LeapSeconds.open()\n            assert ls.expires > self.good_enough\n            assert ls.meta[\"data_url\"].startswith(\"http\")\n        finally:\n            # This setting is to be consistent with astropy/conftest.py\n            iers.conf.auto_download = False\n", "type": "function"}, {"name": "test_auto_open_erfa", "is_method": true, "class_name": "TestAutoOpenExplicitLists", "parameters": ["self"], "calls": ["pytest.mark.filterwarnings", "iers.LeapSeconds.auto_open"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 155, "end_line": 157}, "code_snippet": "    def test_auto_open_erfa(self):\n        ls = iers.LeapSeconds.auto_open([\"erfa\", iers.IERS_LEAP_SECOND_FILE])\n        assert ls.meta[\"data_url\"] in [\"erfa\", iers.IERS_LEAP_SECOND_FILE]\n", "type": "function"}, {"name": "test_system_file_used_if_not_expired", "is_method": true, "class_name": "TestDefaultAutoOpen", "parameters": ["self", "tmp_path"], "calls": ["pytest.mark.skipif", "self.remove_auto_open_files", "pytest.skip", "iers.conf.set_temp", "iers.LeapSeconds.open", "make_fake_file", "iers.LeapSeconds.open", "os.path.isfile", "iers.LeapSeconds.open", "Time.now"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 318, "end_line": 336}, "code_snippet": "    def test_system_file_used_if_not_expired(self, tmp_path):\n        # We skip the test if the system file is on a CI and is expired -\n        # we should not depend on CI keeping it up to date, but if it is,\n        # we should check that it is used if possible.\n        if iers.LeapSeconds.open(SYSTEM_FILE).expires <= self.good_enough:\n            pytest.skip(\"System leap second file is expired.\")\n\n        self.remove_auto_open_files(\"erfa\")\n        with iers.conf.set_temp(\"system_leap_second_file\", SYSTEM_FILE):\n            ls = iers.LeapSeconds.open()\n            assert ls.expires > self.good_enough\n            assert ls.meta[\"data_url\"] in (iers.IERS_LEAP_SECOND_FILE, SYSTEM_FILE)\n\n            # Also check with a \"built-in\" file that is expired\n            fake_file = make_fake_file(\"28 June 2017\", tmp_path)\n            iers.LeapSeconds._auto_open_files[0] = fake_file\n            ls2 = iers.LeapSeconds.open()\n            assert ls2.expires > Time.now()\n            assert ls2.meta[\"data_url\"] == SYSTEM_FILE\n", "type": "function"}, {"name": "test_erfa_found", "is_method": true, "class_name": "TestDefaultAutoOpen", "parameters": ["self"], "calls": ["iers.conf.set_temp", "iers.LeapSeconds.open"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 248, "end_line": 253}, "code_snippet": "    def test_erfa_found(self):\n        # Set huge maximum age such that whatever ERFA has is OK.\n        # Since it is checked first, it should thus be found.\n        with iers.conf.set_temp(\"auto_max_age\", 100000):\n            ls = iers.LeapSeconds.open()\n        assert ls.meta[\"data_url\"] == \"erfa\"\n", "type": "function"}, {"name": "test_fake_future_file", "is_method": true, "class_name": "TestAutoOpenExplicitLists", "parameters": ["self", "tmp_path"], "calls": ["pytest.mark.filterwarnings", "make_fake_file", "iers.conf.set_temp", "iers.LeapSeconds.auto_open", "iers.LeapSeconds.auto_open", "Time", "str", "urllib.request.pathname2url", "Time", "str"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 160, "end_line": 177}, "code_snippet": "    def test_fake_future_file(self, tmp_path):\n        fake_file = make_fake_file(\"28 June 2345\", tmp_path)\n        # Try as system file for auto_open, setting auto_max_age such\n        # that any ERFA or system files are guaranteed to be expired,\n        # while the fake file is guaranteed to be OK.\n        with iers.conf.set_temp(\"auto_max_age\", -100000):\n            ls = iers.LeapSeconds.auto_open(\n                [\"erfa\", iers.IERS_LEAP_SECOND_FILE, fake_file]\n            )\n            assert ls.expires == Time(\"2345-06-28\", scale=\"tai\")\n            assert ls.meta[\"data_url\"] == str(fake_file)\n            # And as URL\n            fake_url = \"file:\" + urllib.request.pathname2url(fake_file)\n            ls2 = iers.LeapSeconds.auto_open(\n                [\"erfa\", iers.IERS_LEAP_SECOND_FILE, fake_url]\n            )\n            assert ls2.expires == Time(\"2345-06-28\", scale=\"tai\")\n            assert ls2.meta[\"data_url\"] == str(fake_url)\n", "type": "function"}, {"name": "test_auto_open_simple", "is_method": true, "class_name": "TestAutoOpenExplicitLists", "parameters": ["self"], "calls": ["pytest.mark.filterwarnings", "iers.LeapSeconds.auto_open"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 150, "end_line": 152}, "code_snippet": "    def test_auto_open_simple(self):\n        ls = iers.LeapSeconds.auto_open([iers.IERS_LEAP_SECOND_FILE])\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n", "type": "function"}, {"name": "test_fake_future_file", "is_method": true, "class_name": "TestDefaultAutoOpen", "parameters": ["self", "tmp_path"], "calls": ["make_fake_file", "iers.conf.set_temp", "iers.conf.set_temp", "iers.LeapSeconds.open", "Time", "str", "urllib.request.pathname2url", "iers.conf.set_temp", "iers.conf.set_temp", "iers.LeapSeconds.open", "Time", "str"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 276, "end_line": 295}, "code_snippet": "    def test_fake_future_file(self, tmp_path):\n        fake_file = make_fake_file(\"28 June 2345\", tmp_path)\n        # Try as system file for auto_open, setting auto_max_age such\n        # that any ERFA or system files are guaranteed to be expired.\n        with (\n            iers.conf.set_temp(\"auto_max_age\", -100000),\n            iers.conf.set_temp(\"system_leap_second_file\", fake_file),\n        ):\n            ls = iers.LeapSeconds.open()\n        assert ls.expires == Time(\"2345-06-28\", scale=\"tai\")\n        assert ls.meta[\"data_url\"] == str(fake_file)\n        # And as URL\n        fake_url = \"file:\" + urllib.request.pathname2url(fake_file)\n        with (\n            iers.conf.set_temp(\"auto_max_age\", -100000),\n            iers.conf.set_temp(\"iers_leap_second_auto_url\", fake_url),\n        ):\n            ls2 = iers.LeapSeconds.open()\n        assert ls2.expires == Time(\"2345-06-28\", scale=\"tai\")\n        assert ls2.meta[\"data_url\"] == str(fake_url)\n", "type": "function"}, {"name": "test_ietf_url", "is_method": true, "class_name": "TestRemoteURLs", "parameters": ["self"], "calls": ["iers.LeapSeconds.auto_open", "Time.now"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 222, "end_line": 224}, "code_snippet": "    def test_ietf_url(self):\n        ls = iers.LeapSeconds.auto_open([iers.IETF_LEAP_SECOND_URL])\n        assert ls.expires > Time.now()\n", "type": "function"}, {"name": "test_fake_expired_file", "is_method": true, "class_name": "TestAutoOpenExplicitLists", "parameters": ["self", "tmp_path"], "calls": ["make_fake_file", "make_fake_file", "iers.LeapSeconds.auto_open", "pytest.warns", "iers.LeapSeconds.auto_open", "Time", "iers.conf.set_temp", "iers.LeapSeconds.auto_open", "iers.conf.set_temp", "iers.LeapSeconds.auto_open"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 179, "end_line": 202}, "code_snippet": "    def test_fake_expired_file(self, tmp_path):\n        fake_file1 = make_fake_file(\"28 June 2010\", tmp_path)\n        fake_file2 = make_fake_file(\"27 June 2012\", tmp_path)\n        # Between these and the built-in one, the built-in file is best.\n        ls = iers.LeapSeconds.auto_open(\n            [fake_file1, fake_file2, iers.IERS_LEAP_SECOND_FILE]\n        )\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n\n        # But if we remove the built-in one, the least expired one will be\n        # used and we get a warning that it is stale.\n        with pytest.warns(iers.IERSStaleWarning):\n            ls2 = iers.LeapSeconds.auto_open([fake_file1, fake_file2])\n        assert ls2.meta[\"data_url\"] == fake_file2\n        assert ls2.expires == Time(\"2012-06-27\", scale=\"tai\")\n\n        # Use the fake files to make sure auto_max_age is safe.\n        # Should have no warning in either example.\n        with iers.conf.set_temp(\"auto_max_age\", None):\n            ls3 = iers.LeapSeconds.auto_open([fake_file1, iers.IERS_LEAP_SECOND_FILE])\n        assert ls3.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n        with iers.conf.set_temp(\"auto_max_age\", None):\n            ls4 = iers.LeapSeconds.auto_open([fake_file1, fake_file2])\n        assert ls4.meta[\"data_url\"] == fake_file2\n", "type": "function"}, {"name": "test_builtin_found", "is_method": true, "class_name": "TestDefaultAutoOpen", "parameters": ["self"], "calls": ["self.remove_auto_open_files", "iers.conf.set_temp", "iers.LeapSeconds.open"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 255, "end_line": 261}, "code_snippet": "    def test_builtin_found(self):\n        # Set huge maximum age such that built-in file is always OK.\n        # If we remove 'erfa', it should thus be found.\n        self.remove_auto_open_files(\"erfa\")\n        with iers.conf.set_temp(\"auto_max_age\", 100000):\n            ls = iers.LeapSeconds.open()\n        assert ls.meta[\"data_url\"] == iers.IERS_LEAP_SECOND_FILE\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.2995991706848145}
{"question": "What architectural pattern does the header initialization process use to prevent modifications when creating new header data unit instances from existing headers in the FITS file I/O module?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_init_with_header", "is_method": false, "class_name": null, "parameters": [], "calls": ["fits.Header", "fits.Header"], "code_location": {"file": "test_header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 38, "end_line": 48}, "code_snippet": "def test_init_with_header():\n    \"\"\"Make sure that creating a Header from another Header makes a copy if\n    copy is True.\"\"\"\n\n    original_header = fits.Header([(\"a\", 10)])\n    new_header = fits.Header(original_header, copy=True)\n    original_header[\"a\"] = 20\n    assert new_header[\"a\"] == 10\n\n    new_header[\"a\"] = 0\n    assert original_header[\"a\"] == 20\n", "type": "function"}, {"name": "test_constructor_copies_header", "is_method": true, "class_name": "TestImageFunctions", "parameters": ["self"], "calls": ["fits.HDUList", "fits.PrimaryHDU", "fits.HDUList", "fits.PrimaryHDU"], "code_location": {"file": "test_image.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 72, "end_line": 89}, "code_snippet": "    def test_constructor_copies_header(self):\n        \"\"\"\n        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/153\n\n        Ensure that a header from one HDU is copied when used to initialize new\n        HDU.\n        \"\"\"\n\n        ifd = fits.HDUList(fits.PrimaryHDU())\n        phdr = ifd[0].header\n        phdr[\"FILENAME\"] = \"labq01i3q_rawtag.fits\"\n\n        primary_hdu = fits.PrimaryHDU(header=phdr)\n        ofd = fits.HDUList(primary_hdu)\n        ofd[0].header[\"FILENAME\"] = \"labq01i3q_flt.fits\"\n\n        # Original header should be unchanged\n        assert phdr[\"FILENAME\"] == \"labq01i3q_rawtag.fits\"\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "_ValidHDU", "parameters": ["self", "data", "header", "name", "ver"], "calls": ["__init__", "ValueError", "super", "isinstance"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/hdu", "start_line": 855, "end_line": 874}, "code_snippet": "    def __init__(self, data=None, header=None, name=None, ver=None, **kwargs):\n        super().__init__(data=data, header=header)\n\n        if header is not None and not isinstance(header, (Header, _BasicHeader)):\n            # TODO: Instead maybe try initializing a new Header object from\n            # whatever is passed in as the header--there are various types\n            # of objects that could work for this...\n            raise ValueError(\"header must be a Header object\")\n\n        # NOTE:  private data members _checksum and _datasum are used by the\n        # utility script \"fitscheck\" to detect missing checksums.\n        self._checksum = None\n        self._checksum_valid = None\n        self._datasum = None\n        self._datasum_valid = None\n\n        if name is not None:\n            self.name = name\n        if ver is not None:\n            self.ver = ver\n", "type": "function"}, {"name": "_DelayedHeader", "docstring": "Descriptor used to create the Header object from the header string that\nwas stored in HDU._header_str when parsing the file.", "methods": ["__get__", "__set__", "__delete__"], "attributes": [], "code_location": {"file": "header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits", "start_line": 1920, "end_line": 1945}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "_BaseHDU", "parameters": ["self", "data", "header"], "calls": ["Header"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/hdu", "start_line": 150, "end_line": 173}, "code_snippet": "    def __init__(self, data=None, header=None, *args, **kwargs):\n        if header is None:\n            header = Header()\n        self._header = header\n        self._header_str = None\n        self._file = None\n        self._buffer = None\n        self._header_offset = None\n        self._data_offset = None\n        self._data_size = None\n\n        # This internal variable is used to track whether the data attribute\n        # still points to the same data array as when the HDU was originally\n        # created (this does not track whether the data is actually the same\n        # content-wise)\n        self._data_replaced = False\n        self._data_needs_rescale = False\n        self._new = True\n        self._output_checksum = False\n\n        if \"DATASUM\" in self._header and \"CHECKSUM\" not in self._header:\n            self._output_checksum = \"datasum\"\n        elif \"CHECKSUM\" in self._header:\n            self._output_checksum = True\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "_ImageBaseHDU", "parameters": ["self", "data", "header", "do_not_scale_image_data", "uint", "scale_back", "ignore_blank"], "calls": ["__init__", "self._header.get", "self._header.get", "self._header.get", "self._header.get", "self._header.get", "self._verify_blank", "self._header.get", "kwargs.get", "kwargs.get", "isinstance", "isinstance", "isinstance", "Header", "self._header.get", "self._header.get", "self._header.get", "self._header.get", "self._header.get", "super", "ValueError", "cards.append", "cards.append", "cards.append", "new_header.extend", "range", "header.copy", "str", "self._header.get"], "code_location": {"file": "image.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/hdu", "start_line": 46, "end_line": 169}, "code_snippet": "    def __init__(\n        self,\n        data=None,\n        header=None,\n        do_not_scale_image_data=False,\n        uint=True,\n        scale_back=False,\n        ignore_blank=False,\n        **kwargs,\n    ):\n        super().__init__(data=data, header=header)\n\n        if data is DELAYED:\n            # Presumably if data is DELAYED then this HDU is coming from an\n            # open file, and was not created in memory\n            if header is None:\n                # this should never happen\n                raise ValueError(\"No header to setup HDU.\")\n        else:\n            # TODO: Some of this card manipulation should go into the\n            # PrimaryHDU and GroupsHDU subclasses\n            # construct a list of cards of minimal header\n            from .groups import GroupsHDU\n\n            if isinstance(self, ExtensionHDU):\n                c0 = (\"XTENSION\", \"IMAGE\", self.standard_keyword_comments[\"XTENSION\"])\n            else:\n                c0 = (\"SIMPLE\", True, self.standard_keyword_comments[\"SIMPLE\"])\n            cards = [\n                c0,\n                (\"BITPIX\", 8, self.standard_keyword_comments[\"BITPIX\"]),\n                (\"NAXIS\", 0, self.standard_keyword_comments[\"NAXIS\"]),\n            ]\n\n            if isinstance(self, GroupsHDU):\n                cards.append((\"GROUPS\", True, self.standard_keyword_comments[\"GROUPS\"]))\n\n            if isinstance(self, (ExtensionHDU, GroupsHDU)):\n                cards.append((\"PCOUNT\", 0, self.standard_keyword_comments[\"PCOUNT\"]))\n                cards.append((\"GCOUNT\", 1, self.standard_keyword_comments[\"GCOUNT\"]))\n\n            new_header = Header(cards)\n            if header is not None:\n                new_header.extend(header.copy(), strip=True, update=True, end=True)\n            self._header = new_header\n\n        self._do_not_scale_image_data = do_not_scale_image_data\n        self._uint = uint\n        self._scale_back = scale_back\n\n        # Keep track of whether BZERO/BSCALE were set from the header so that\n        # values for self._orig_bzero and self._orig_bscale can be set\n        # properly, if necessary, once the data has been set.\n        bzero_in_header = \"BZERO\" in self._header\n        bscale_in_header = \"BSCALE\" in self._header\n        self._bzero = self._header.get(\"BZERO\", 0)\n        self._bscale = self._header.get(\"BSCALE\", 1)\n\n        # Save off other important values from the header needed to interpret\n        # the image data\n        self._axes = [\n            self._header.get(\"NAXIS\" + str(axis + 1), 0)\n            for axis in range(self._header.get(\"NAXIS\", 0))\n        ]\n\n        # Not supplying a default for BITPIX makes sense because BITPIX\n        # is either in the header or should be determined from the dtype of\n        # the data (which occurs when the data is set).\n        self._bitpix = self._header.get(\"BITPIX\")\n        self._gcount = self._header.get(\"GCOUNT\", 1)\n        self._pcount = self._header.get(\"PCOUNT\", 0)\n        self._blank = None if ignore_blank else self._header.get(\"BLANK\")\n        self._verify_blank()\n\n        self._orig_bitpix = self._bitpix\n        self._orig_blank = self._header.get(\"BLANK\")\n\n        # These get set again below, but need to be set to sensible defaults\n        # here.\n        self._orig_bzero = self._bzero\n        self._orig_bscale = self._bscale\n\n        # Set the name attribute if it was provided (if this is an ImageHDU\n        # this will result in setting the EXTNAME keyword of the header as\n        # well)\n        if kwargs.get(\"name\"):\n            self.name = kwargs[\"name\"]\n        if kwargs.get(\"ver\"):\n            self.ver = kwargs[\"ver\"]\n\n        # Set to True if the data or header is replaced, indicating that\n        # update_header should be called\n        self._modified = False\n\n        if data is DELAYED:\n            if not do_not_scale_image_data and (self._bscale != 1 or self._bzero != 0):\n                # This indicates that when the data is accessed or written out\n                # to a new file it will need to be rescaled\n                self._data_needs_rescale = True\n            return\n        else:\n            # Setting data will update the header and set _bitpix, _bzero,\n            # and _bscale to the appropriate BITPIX for the data, and always\n            # sets _bzero=0 and _bscale=1.\n            self.data = data\n\n            # Check again for BITPIX/BSCALE/BZERO in case they changed when the\n            # data was assigned. This can happen, for example, if the input\n            # data is an unsigned int numpy array.\n            self._bitpix = self._header.get(\"BITPIX\")\n\n            # Do not provide default values for BZERO and BSCALE here because\n            # the keywords will have been deleted in the header if appropriate\n            # after scaling. We do not want to put them back in if they\n            # should not be there.\n            self._bzero = self._header.get(\"BZERO\")\n            self._bscale = self._header.get(\"BSCALE\")\n\n        # Handle case where there was no BZERO/BSCALE in the initial header\n        # but there should be a BSCALE/BZERO now that the data has been set.\n        if not bzero_in_header:\n            self._orig_bzero = self._bzero\n        if not bscale_in_header:\n            self._orig_bscale = self._bscale\n", "type": "function"}, {"name": "test_constructor_copies_header", "is_method": true, "class_name": "TestTableFunctions", "parameters": ["self"], "calls": ["fits.HDUList", "fits.BinTableHDU", "fits.HDUList", "fits.PrimaryHDU", "fits.BinTableHDU"], "code_location": {"file": "test_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 131, "end_line": 151}, "code_snippet": "    def test_constructor_copies_header(self):\n        \"\"\"\n        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/153\n\n        Ensure that a header from one HDU is copied when used to initialize new\n        HDU.\n\n        This is like the test of the same name in test_image, but tests this\n        for tables as well.\n        \"\"\"\n\n        ifd = fits.HDUList([fits.PrimaryHDU(), fits.BinTableHDU()])\n        thdr = ifd[1].header\n        thdr[\"FILENAME\"] = \"labq01i3q_rawtag.fits\"\n\n        thdu = fits.BinTableHDU(header=thdr)\n        ofd = fits.HDUList(thdu)\n        ofd[0].header[\"FILENAME\"] = \"labq01i3q_flt.fits\"\n\n        # Original header should be unchanged\n        assert thdr[\"FILENAME\"] == \"labq01i3q_rawtag.fits\"\n", "type": "function"}, {"name": "_BasicHeader", "docstring": "This class provides a fast header parsing, without all the additional\nfeatures of the Header class. Here only standard keywords are parsed, no\nsupport for CONTINUE, HIERARCH, COMMENT, HISTORY, or rvkc.\n\nThe raw card images are stored and parsed only if needed. The idea is that\nto create the HDU objects, only a small subset of standard cards is needed.\nOnce a card is parsed, which is deferred to the Card class, the Card object\nis kept in a cache. This is useful because a small subset of cards is used\na lot in the HDU creation process (NAXIS, XTENSION, ...).", "methods": ["__init__", "__getitem__", "__len__", "__iter__", "index", "data_size", "data_size_padded", "fromfile"], "attributes": [], "code_location": {"file": "header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits", "start_line": 1976, "end_line": 2054}, "type": "class"}, {"name": "_BasicHeaderCards", "docstring": "This class allows to access cards with the _BasicHeader.cards attribute.\n\nThis is needed because during the HDU class detection, some HDUs uses\nthe .cards interface.  Cards cannot be modified here as the _BasicHeader\nobject will be deleted once the HDU object is created.", "methods": ["__init__", "__getitem__"], "attributes": [], "code_location": {"file": "header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits", "start_line": 1948, "end_line": 1973}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "CompImageHeader", "parameters": ["self"], "calls": ["warnings.warn", "__init__", "super"], "code_location": {"file": "header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/hdu/compressed", "start_line": 68, "end_line": 73}, "code_snippet": "    def __init__(self, *args, **kwargs):\n        warnings.warn(\n            \"The CompImageHeader class is deprecated and will be removed in future\",\n            AstropyDeprecationWarning,\n        )\n        super().__init__(*args, **kwargs)\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.350311756134033}
{"question": "What semantic constraints does the unit mapping specification field in the representation-to-frame attribute mapping tuple impose on astronomical coordinate system conversions when set to the null value versus the automatic unit inference string?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_converting_units", "is_method": false, "class_name": null, "parameters": [], "calls": ["re.compile", "ICRS", "ICRS", "transform_to", "transform_to", "join", "join", "join", "join", "FakeICRS", "join", "join", "re.sub", "ICRS", "ICRS", "rexrepr.split", "rexrepr.split", "rexrepr.split", "rexrepr.split", "rexrepr.split", "rexrepr.split", "i2.transform_to", "i2_many.transform_to", "repr", "repr", "repr", "repr", "repr", "repr", "FK5", "FK5", "RepresentationMapping", "RepresentationMapping", "RepresentationMapping"], "code_location": {"file": "test_frames.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/tests", "start_line": 313, "end_line": 360}, "code_snippet": "def test_converting_units():\n    # this is a regular expression that with split (see below) removes what's\n    # the decimal point  to fix rounding problems\n    rexrepr = re.compile(r\"(.*?=\\d\\.).*?( .*?=\\d\\.).*?( .*)\")\n\n    # Use values that aren't subject to rounding down to X.9999...\n    i2 = ICRS(ra=2.0 * u.deg, dec=2.0 * u.deg)\n    i2_many = ICRS(ra=[2.0, 4.0] * u.deg, dec=[2.0, -8.1] * u.deg)\n\n    # converting from FK5 to ICRS and back changes the *internal* representation,\n    # but it should still come out in the preferred form\n\n    i4 = i2.transform_to(FK5()).transform_to(ICRS())\n    i4_many = i2_many.transform_to(FK5()).transform_to(ICRS())\n\n    ri2 = \"\".join(rexrepr.split(repr(i2)))\n    ri4 = \"\".join(rexrepr.split(repr(i4)))\n    assert ri2 == ri4\n    assert i2.data.lon.unit != i4.data.lon.unit  # Internal repr changed\n\n    ri2_many = \"\".join(rexrepr.split(repr(i2_many)))\n    ri4_many = \"\".join(rexrepr.split(repr(i4_many)))\n\n    assert ri2_many == ri4_many\n    assert i2_many.data.lon.unit != i4_many.data.lon.unit  # Internal repr changed\n\n    # but that *shouldn't* hold if we turn off units for the representation\n    class FakeICRS(ICRS):\n        frame_specific_representation_info = {\n            \"spherical\": [\n                RepresentationMapping(\"lon\", \"ra\", u.hourangle),\n                RepresentationMapping(\"lat\", \"dec\", None),\n                RepresentationMapping(\"distance\", \"distance\"),\n            ]  # should fall back to default of None unit\n        }\n\n    fi = FakeICRS(i4.data)\n    ri2 = \"\".join(rexrepr.split(repr(i2)))\n    rfi = \"\".join(rexrepr.split(repr(fi)))\n    rfi = re.sub(\"FakeICRS\", \"ICRS\", rfi)  # Force frame name to match\n    assert ri2 != rfi\n\n    # the attributes should also get the right units\n    assert i2.dec.unit == i4.dec.unit\n    # unless no/explicitly given units\n    assert i2.dec.unit != fi.dec.unit\n    assert i2.ra.unit != fi.ra.unit\n    assert fi.ra.unit == u.hourangle\n", "type": "function"}, {"name": "test_representation_info", "is_method": false, "class_name": null, "parameters": [], "calls": ["NewICRS1", "allclose", "allclose", "allclose", "allclose", "allclose", "i1.set_representation_cls", "allclose", "allclose", "allclose", "allclose", "NewICRS2", "allclose", "allclose", "allclose", "NewICRS3", "allclose", "allclose", "allclose", "RepresentationMapping", "RepresentationMapping", "RepresentationMapping", "RepresentationMapping", "RepresentationMapping", "RepresentationMapping", "RepresentationMapping", "RepresentationMapping", "RepresentationMapping"], "code_location": {"file": "test_frames.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/tests", "start_line": 363, "end_line": 438}, "code_snippet": "def test_representation_info():\n    class NewICRS1(ICRS):\n        frame_specific_representation_info = {\n            r.SphericalRepresentation: [\n                RepresentationMapping(\"lon\", \"rara\", u.hourangle),\n                RepresentationMapping(\"lat\", \"decdec\", u.degree),\n                RepresentationMapping(\"distance\", \"distance\", u.kpc),\n            ]\n        }\n\n    i1 = NewICRS1(\n        rara=10 * u.degree,\n        decdec=-12 * u.deg,\n        distance=1000 * u.pc,\n        pm_rara_cosdecdec=100 * u.mas / u.yr,\n        pm_decdec=17 * u.mas / u.yr,\n        radial_velocity=10 * u.km / u.s,\n    )\n    assert allclose(i1.rara, 10 * u.deg)\n    assert i1.rara.unit == u.hourangle\n    assert allclose(i1.decdec, -12 * u.deg)\n    assert allclose(i1.distance, 1000 * u.pc)\n    assert i1.distance.unit == u.kpc\n    assert allclose(i1.pm_rara_cosdecdec, 100 * u.mas / u.yr)\n    assert allclose(i1.pm_decdec, 17 * u.mas / u.yr)\n\n    # this should auto-set the names of UnitSpherical:\n    i1.set_representation_cls(\n        r.UnitSphericalRepresentation, s=r.UnitSphericalCosLatDifferential\n    )\n    assert allclose(i1.rara, 10 * u.deg)\n    assert allclose(i1.decdec, -12 * u.deg)\n    assert allclose(i1.pm_rara_cosdecdec, 100 * u.mas / u.yr)\n    assert allclose(i1.pm_decdec, 17 * u.mas / u.yr)\n\n    # For backwards compatibility, we also support the string name in the\n    # representation info dictionary:\n    class NewICRS2(ICRS):\n        frame_specific_representation_info = {\n            \"spherical\": [\n                RepresentationMapping(\"lon\", \"ang1\", u.hourangle),\n                RepresentationMapping(\"lat\", \"ang2\", u.degree),\n                RepresentationMapping(\"distance\", \"howfar\", u.kpc),\n            ]\n        }\n\n    i2 = NewICRS2(ang1=10 * u.degree, ang2=-12 * u.deg, howfar=1000 * u.pc)\n    assert allclose(i2.ang1, 10 * u.deg)\n    assert i2.ang1.unit == u.hourangle\n    assert allclose(i2.ang2, -12 * u.deg)\n    assert allclose(i2.howfar, 1000 * u.pc)\n    assert i2.howfar.unit == u.kpc\n\n    # Test that the differential kwargs get overridden\n    class NewICRS3(ICRS):\n        frame_specific_representation_info = {\n            r.SphericalCosLatDifferential: [\n                RepresentationMapping(\"d_lon_coslat\", \"pm_ang1\", u.hourangle / u.year),\n                RepresentationMapping(\"d_lat\", \"pm_ang2\"),\n                RepresentationMapping(\"d_distance\", \"vlos\", u.kpc / u.Myr),\n            ]\n        }\n\n    i3 = NewICRS3(\n        lon=10 * u.degree,\n        lat=-12 * u.deg,\n        distance=1000 * u.pc,\n        pm_ang1=1 * u.mas / u.yr,\n        pm_ang2=2 * u.mas / u.yr,\n        vlos=100 * u.km / u.s,\n    )\n    assert allclose(i3.pm_ang1, 1 * u.mas / u.yr)\n    assert i3.pm_ang1.unit == u.hourangle / u.year\n    assert allclose(i3.pm_ang2, 2 * u.mas / u.yr)\n    assert allclose(i3.vlos, 100 * u.km / u.s)\n    assert i3.vlos.unit == u.kpc / u.Myr\n", "type": "function"}, {"name": "helper_starpv", "is_method": false, "class_name": null, "parameters": ["f", "unit_ra", "unit_dec", "unit_pmr", "unit_pmd", "unit_px", "unit_rv"], "calls": ["get_converter", "get_converter", "get_converter", "get_converter", "get_converter", "get_converter", "StructuredUnit"], "code_location": {"file": "erfa.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 219, "end_line": 230}, "code_snippet": "def helper_starpv(f, unit_ra, unit_dec, unit_pmr, unit_pmd, unit_px, unit_rv):\n    from astropy.units.astrophys import AU\n    from astropy.units.si import arcsec, day, km, radian, s, year\n\n    return [\n        get_converter(unit_ra, radian),\n        get_converter(unit_dec, radian),\n        get_converter(unit_pmr, radian / year),\n        get_converter(unit_pmd, radian / year),\n        get_converter(unit_px, arcsec),\n        get_converter(unit_rv, km / s),\n    ], (StructuredUnit((AU, AU / day)), None)\n", "type": "function"}, {"name": "unit", "is_method": true, "class_name": "Info", "parameters": ["self", "unit"], "calls": ["_get_default_unit_format", "u.Unit", "isinstance", "_get_unit_format", "self._config.get", "warn_or_raise", "warn_or_raise", "u.Unit"], "code_location": {"file": "tree.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 914, "end_line": 935}, "code_snippet": "    def unit(self, unit):\n        if unit is None:\n            self._unit = None\n            return\n\n        from astropy import units as u\n\n        if not self._config.get(\"version_1_2_or_later\"):\n            warn_or_raise(W28, W28, (\"unit\", \"INFO\", \"1.2\"), self._config, self._pos)\n\n        # First, parse the unit in the default way, so that we can\n        # still emit a warning if the unit is not to spec.\n        default_format = _get_default_unit_format(self._config)\n        unit_obj = u.Unit(unit, format=default_format, parse_strict=\"silent\")\n        if isinstance(unit_obj, u.UnrecognizedUnit):\n            warn_or_raise(W50, W50, (unit,), self._config, self._pos)\n\n        format = _get_unit_format(self._config)\n        if format != default_format:\n            unit_obj = u.Unit(unit, format=format, parse_strict=\"silent\")\n\n        self._unit = unit_obj\n", "type": "function"}, {"name": "helper_atciq", "is_method": false, "class_name": null, "parameters": ["f", "unit_rc", "unit_dc", "unit_pr", "unit_pd", "unit_px", "unit_rv", "unit_astrom"], "calls": ["get_converter", "get_converter", "get_converter", "get_converter", "get_converter", "get_converter", "get_converter", "astrom_unit"], "code_location": {"file": "erfa.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 400, "end_line": 411}, "code_snippet": "def helper_atciq(f, unit_rc, unit_dc, unit_pr, unit_pd, unit_px, unit_rv, unit_astrom):\n    from astropy.units.si import arcsec, km, radian, s, year\n\n    return [\n        get_converter(unit_rc, radian),\n        get_converter(unit_dc, radian),\n        get_converter(unit_pr, radian / year),\n        get_converter(unit_pd, radian / year),\n        get_converter(unit_px, arcsec),\n        get_converter(unit_rv, km / s),\n        get_converter(unit_astrom, astrom_unit()),\n    ], (radian, radian)\n", "type": "function"}, {"name": "test_frame_repr_vels", "is_method": false, "class_name": null, "parameters": [], "calls": ["ICRS", "repr"], "code_location": {"file": "test_frames.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/tests", "start_line": 295, "end_line": 310}, "code_snippet": "def test_frame_repr_vels():\n    i = ICRS(\n        ra=1 * u.deg,\n        dec=2 * u.deg,\n        pm_ra_cosdec=1 * u.marcsec / u.yr,\n        pm_dec=2 * u.marcsec / u.yr,\n    )\n\n    # unit comes out as mas/yr because of the preferred units defined in the\n    # frame RepresentationMapping\n    assert (\n        repr(i) == \"<ICRS Coordinate: (ra, dec) in deg\\n\"\n        \"    (1., 2.)\\n\"\n        \" (pm_ra_cosdec, pm_dec) in mas / yr\\n\"\n        \"    (1., 2.)>\"\n    )\n", "type": "function"}, {"name": "helper_atoiq", "is_method": false, "class_name": null, "parameters": ["f", "unit_type", "unit_ri", "unit_di", "unit_astrom"], "calls": ["UnitTypeError", "get_converter", "get_converter", "get_converter", "astrom_unit"], "code_location": {"file": "erfa.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 462, "end_line": 473}, "code_snippet": "def helper_atoiq(f, unit_type, unit_ri, unit_di, unit_astrom):\n    from astropy.units.si import radian\n\n    if unit_type is not None:\n        raise UnitTypeError(\"argument 'type' should not have a unit\")\n\n    return [\n        None,\n        get_converter(unit_ri, radian),\n        get_converter(unit_di, radian),\n        get_converter(unit_astrom, astrom_unit()),\n    ], (radian, radian)\n", "type": "function"}, {"name": "_parameter_units_for_data_units", "is_method": true, "class_name": "Polynomial2D", "parameters": ["self", "inputs_unit", "outputs_unit"], "calls": ["range", "range", "getattr"], "code_location": {"file": "polynomial.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 1310, "end_line": 1322}, "code_snippet": "    def _parameter_units_for_data_units(self, inputs_unit, outputs_unit):\n        mapping = {}\n        for i in range(self.degree + 1):\n            for j in range(self.degree + 1):\n                if i + j > 2:\n                    continue\n                par = getattr(self, f\"c{i}_{j}\")\n                mapping[par.name] = (\n                    outputs_unit[self.outputs[0]]\n                    / inputs_unit[self.inputs[0]] ** i\n                    / inputs_unit[self.inputs[1]] ** j\n                )\n        return mapping\n", "type": "function"}, {"name": "test_non_spherical_representation_unit_creation", "is_method": false, "class_name": null, "parameters": ["unitphysics"], "calls": ["PhysicsICRS", "isinstance", "PhysicsICRS", "isinstance"], "code_location": {"file": "test_frames.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/tests", "start_line": 1474, "end_line": 1482}, "code_snippet": "def test_non_spherical_representation_unit_creation(unitphysics):  # noqa: F811\n    class PhysicsICRS(ICRS):\n        default_representation = r.PhysicsSphericalRepresentation\n\n    pic = PhysicsICRS(phi=1 * u.deg, theta=25 * u.deg, r=1 * u.kpc)\n    assert isinstance(pic.data, r.PhysicsSphericalRepresentation)\n\n    picu = PhysicsICRS(phi=1 * u.deg, theta=25 * u.deg)\n    assert isinstance(picu.data, unitphysics)\n", "type": "function"}, {"name": "test_uses_quantity_no_param", "is_method": false, "class_name": null, "parameters": [], "calls": ["Mapping", "Pix2Sky_TAN"], "code_location": {"file": "test_quantities_model.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling/tests", "start_line": 108, "end_line": 111}, "code_snippet": "def test_uses_quantity_no_param():\n    comp = Mapping((0, 1)) | Pix2Sky_TAN()\n\n    assert comp.uses_quantity\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.3732054233551025}
{"question": "How does the base context manager class in astropy.config.paths that temporarily overrides configuration and cache directory paths ensure atomicity and exception safety when overriding its class-level temporary path attribute during context manager entry?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "__enter__", "is_method": true, "class_name": "_SetTempPath", "parameters": ["self"], "calls": ["str", "self.__class__._get_dir_path"], "code_location": {"file": "paths.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/config", "start_line": 133, "end_line": 139}, "code_snippet": "    def __enter__(self) -> str:\n        self.__class__._temp_path = self._path\n        try:\n            return str(self.__class__._get_dir_path(rootname=\"astropy\"))\n        except Exception:\n            self.__class__._temp_path = self._prev_path\n            raise\n", "type": "function"}, {"name": "set_temp_cache", "docstring": "Context manager to set a temporary path for the Astropy download cache,\nprimarily for use with testing (though there may be other applications\nfor setting a different cache directory, for example to switch to a cache\ndedicated to large files).\n\nIf the path set by this context manager does not already exist it will be\ncreated, if possible.\n\nThis may also be used as a decorator on a function to set the cache path\njust within that function.\n\nParameters\n----------\npath : str\n    The directory (which must exist) in which to find the Astropy cache\n    files, or create them if they do not already exist.  If None, this\n    restores the cache path to the user's default cache path as returned\n    by `get_cache_dir` as though this context manager were not in effect\n    (this is useful for testing).  In this case the ``delete`` argument is\n    always ignored.\n\ndelete : bool, optional\n    If True, cleans up the temporary directory after exiting the temp\n    context (default: False).", "methods": [], "attributes": ["_directory_type", "_directory_env_var"], "code_location": {"file": "paths.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/config", "start_line": 269, "end_line": 298}, "type": "class"}, {"name": "set_temp_config", "docstring": "Context manager to set a temporary path for the Astropy config, primarily\nfor use with testing.\n\nIf the path set by this context manager does not already exist it will be\ncreated, if possible.\n\nThis may also be used as a decorator on a function to set the config path\njust within that function.\n\nParameters\n----------\npath : str, optional\n    The directory (which must exist) in which to find the Astropy config\n    files, or create them if they do not already exist.  If None, this\n    restores the config path to the user's default config path as returned\n    by `get_config_dir` as though this context manager were not in effect\n    (this is useful for testing).  In this case the ``delete`` argument is\n    always ignored.\n\ndelete : bool, optional\n    If True, cleans up the temporary directory after exiting the temp\n    context (default: False).", "methods": ["__enter__", "__exit__"], "attributes": ["_directory_type", "_directory_env_var"], "code_location": {"file": "paths.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/config", "start_line": 215, "end_line": 266}, "type": "class"}, {"name": "test_set_temp_cache", "is_method": false, "class_name": null, "parameters": ["tmp_path", "monkeypatch"], "calls": ["monkeypatch.setattr", "paths.get_cache_dir", "mkdir", "paths.set_temp_cache", "test_func", "paths.set_temp_cache", "temp_cache_dir.exists", "paths.get_cache_dir", "str", "paths.set_temp_cache", "paths.get_cache_dir", "str", "paths.get_cache_dir"], "code_location": {"file": "test_configs.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/config/tests", "start_line": 87, "end_line": 109}, "code_snippet": "def test_set_temp_cache(tmp_path, monkeypatch):\n    monkeypatch.setattr(paths.set_temp_cache, \"_temp_path\", None)\n\n    orig_cache_dir = paths.get_cache_dir(rootname=\"astropy\")\n    (temp_cache_dir := tmp_path / \"cache\").mkdir()\n    temp_astropy_cache = temp_cache_dir / \"astropy\"\n\n    # Test decorator mode\n    @paths.set_temp_cache(temp_cache_dir)\n    def test_func():\n        assert paths.get_cache_dir(rootname=\"astropy\") == str(temp_astropy_cache)\n\n        # Test temporary restoration of original default\n        with paths.set_temp_cache() as d:\n            assert d == orig_cache_dir == paths.get_cache_dir(rootname=\"astropy\")\n\n    test_func()\n\n    # Test context manager mode (with cleanup)\n    with paths.set_temp_cache(temp_cache_dir, delete=True):\n        assert paths.get_cache_dir(rootname=\"astropy\") == str(temp_astropy_cache)\n\n    assert not temp_cache_dir.exists()\n", "type": "function"}, {"name": "test_set_temp_config", "is_method": false, "class_name": null, "parameters": ["tmp_path", "monkeypatch"], "calls": ["monkeypatch.setattr", "paths.get_config_dir", "mkdir", "paths.set_temp_config", "test_func", "paths.set_temp_config", "temp_config_dir.exists", "paths.get_config_dir", "str", "paths.get_config_dir_path", "paths.set_temp_config", "paths.get_config_dir", "str", "paths.get_config_dir_path", "paths.get_config_dir"], "code_location": {"file": "test_configs.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/config/tests", "start_line": 55, "end_line": 84}, "code_snippet": "def test_set_temp_config(tmp_path, monkeypatch):\n    # Check that we start in an understood state.\n    assert configuration._cfgobjs == OLD_CONFIG\n    # Temporarily remove any temporary overrides of the configuration dir.\n    monkeypatch.setattr(paths.set_temp_config, \"_temp_path\", None)\n\n    orig_config_dir = paths.get_config_dir(rootname=\"astropy\")\n    (temp_config_dir := tmp_path / \"config\").mkdir()\n    temp_astropy_config = temp_config_dir / \"astropy\"\n\n    # Test decorator mode\n    @paths.set_temp_config(temp_config_dir)\n    def test_func():\n        assert paths.get_config_dir(rootname=\"astropy\") == str(temp_astropy_config)\n        assert paths.get_config_dir_path(rootname=\"astropy\") == temp_astropy_config\n\n        # Test temporary restoration of original default\n        with paths.set_temp_config() as d:\n            assert d == orig_config_dir == paths.get_config_dir(rootname=\"astropy\")\n\n    test_func()\n\n    # Test context manager mode (with cleanup)\n    with paths.set_temp_config(temp_config_dir, delete=True):\n        assert paths.get_config_dir(rootname=\"astropy\") == str(temp_astropy_config)\n        assert paths.get_config_dir_path(rootname=\"astropy\") == temp_astropy_config\n\n    assert not temp_config_dir.exists()\n    # Check that we have returned to our old configuration.\n    assert configuration._cfgobjs == OLD_CONFIG\n", "type": "function"}, {"name": "test_temp_cache", "is_method": false, "class_name": null, "parameters": ["tmp_path"], "calls": ["pytest.mark.filterwarnings", "_get_download_cache_loc", "check_download_cache", "_get_download_cache_loc", "check_download_cache", "_get_download_cache_loc", "check_download_cache", "paths.set_temp_cache", "_get_download_cache_loc", "check_download_cache", "paths.set_temp_cache", "_get_download_cache_loc", "check_download_cache"], "code_location": {"file": "test_data.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 302, "end_line": 332}, "code_snippet": "def test_temp_cache(tmp_path):\n    dldir0 = _get_download_cache_loc()\n    check_download_cache()\n\n    with paths.set_temp_cache(tmp_path):\n        dldir1 = _get_download_cache_loc()\n        check_download_cache()\n        assert dldir1 != dldir0\n\n    dldir2 = _get_download_cache_loc()\n    check_download_cache()\n    assert dldir2 != dldir1\n    assert dldir2 == dldir0\n\n    # Check that things are okay even if we exit via an exception\n    class Special(Exception):\n        pass\n\n    try:\n        with paths.set_temp_cache(tmp_path):\n            dldir3 = _get_download_cache_loc()\n            check_download_cache()\n            assert dldir3 == dldir1\n            raise Special\n    except Special:\n        pass\n\n    dldir4 = _get_download_cache_loc()\n    check_download_cache()\n    assert dldir4 != dldir3\n    assert dldir4 == dldir0\n", "type": "function"}, {"name": "__enter__", "is_method": true, "class_name": "set_temp_config", "parameters": ["self"], "calls": ["_cfgobjs.copy", "_cfgobjs.clear", "__enter__", "super"], "code_location": {"file": "paths.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/config", "start_line": 244, "end_line": 253}, "code_snippet": "    def __enter__(self) -> str:\n        # Special case for the config case, where we need to reset all the\n        # cached config objects.  We do keep the cache, since some of it\n        # may have been set programmatically rather than be stored in the\n        # config file (e.g., iers.conf.auto_download=False for our tests).\n        from .configuration import _cfgobjs\n\n        self._cfgobjs_copy = _cfgobjs.copy()\n        _cfgobjs.clear()\n        return super().__enter__()\n", "type": "function"}, {"name": "__exit__", "is_method": true, "class_name": "_SetTempPath", "parameters": ["self", "type", "value", "tb"], "calls": ["shutil.rmtree"], "code_location": {"file": "paths.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/config", "start_line": 141, "end_line": 150}, "code_snippet": "    def __exit__(\n        self,\n        type: type[BaseException] | None,\n        value: BaseException | None,\n        tb: TracebackType | None,\n    ) -> None:\n        self.__class__._temp_path = self._prev_path\n\n        if self._delete and self._path is not None:\n            shutil.rmtree(self._path)\n", "type": "function"}, {"name": "temp_cache", "is_method": false, "class_name": null, "parameters": ["tmp_path"], "calls": ["paths.set_temp_cache", "check_download_cache"], "code_location": {"file": "test_data.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 132, "end_line": 135}, "code_snippet": "def temp_cache(tmp_path):\n    with paths.set_temp_cache(tmp_path):\n        yield None\n        check_download_cache()\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "_SetTempPath", "parameters": ["self", "path", "delete"], "calls": ["resolve", "Path"], "code_location": {"file": "paths.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/config", "start_line": 123, "end_line": 131}, "code_snippet": "    def __init__(\n        self, path: os.PathLike[str] | str | None = None, delete: bool = False\n    ) -> None:\n        if path is not None:\n            path = Path(path).resolve()\n\n        self._path = path\n        self._delete = delete\n        self._prev_path = self.__class__._temp_path\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.416490316390991}
{"question": "How does the cosmology trait mixin that provides Hubble parameter functionality leverage the functools caching decorator to optimize repeated access to dimensionless, time, and distance properties while maintaining unit consistency?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_hubble_distance", "is_method": true, "class_name": "FLRWTest", "parameters": ["self", "cosmo_cls", "cosmo"], "calls": ["isinstance", "to"], "code_location": {"file": "test_base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 187, "end_line": 193}, "code_snippet": "    def test_hubble_distance(self, cosmo_cls, cosmo):\n        \"\"\"Test ``cached_property`` ``hubble_distance``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.hubble_distance, cached_property)\n\n        # on the instance\n        assert cosmo.hubble_distance == (const.c / cosmo.H0).to(u.Mpc)\n", "type": "function"}, {"name": "test_hubble_time", "is_method": true, "class_name": "FLRWTest", "parameters": ["self", "cosmo_cls", "cosmo"], "calls": ["isinstance", "u.allclose"], "code_location": {"file": "test_base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 179, "end_line": 185}, "code_snippet": "    def test_hubble_time(self, cosmo_cls, cosmo):\n        \"\"\"Test ``cached_property`` ``hubble_time``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.hubble_time, cached_property)\n\n        # on the instance\n        assert u.allclose(cosmo.hubble_time, (1 / cosmo.H0) << u.Gyr)\n", "type": "function"}, {"name": "test_h", "is_method": true, "class_name": "FLRWTest", "parameters": ["self", "cosmo_cls", "cosmo"], "calls": ["isinstance", "np.allclose"], "code_location": {"file": "test_base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 171, "end_line": 177}, "code_snippet": "    def test_h(self, cosmo_cls, cosmo):\n        \"\"\"Test ``cached_property`` ``h``.\"\"\"\n        # on the class\n        assert isinstance(cosmo_cls.h, cached_property)\n\n        # on the instance\n        assert np.allclose(cosmo.h, cosmo.H0.value / 100.0)\n", "type": "function"}, {"name": "ParameterH0TestMixin", "docstring": "Tests for `astropy.cosmology.Parameter` H0 on a Cosmology.\n\nH0 is a descriptor, which are tested by mixin, here with ``TestFLRW``.\nThese tests expect dicts ``_cls_args`` and ``cls_kwargs`` which give the\nargs and kwargs for the cosmology class, respectively. See ``TestFLRW``.", "methods": ["test_H0", "test_init_H0"], "attributes": [], "code_location": {"file": "test_parameters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 18, "end_line": 62}, "type": "class"}, {"name": "Parameterm_nuTestMixin", "docstring": "Tests for `astropy.cosmology.Parameter` m_nu on a Cosmology.\n\nm_nu is a descriptor, which are tested by mixin, here with ``TestFLRW``.\nThese tests expect dicts ``_cls_args`` and ``cls_kwargs`` which give the\nargs and kwargs for the cosmology class, respectively. See ``TestFLRW``.", "methods": ["test_m_nu", "test_init_m_nu", "test_init_m_nu_and_Neff", "test_init_m_nu_override_by_Tcmb0"], "attributes": [], "code_location": {"file": "test_parameters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 279, "end_line": 392}, "type": "class"}, {"name": "ParameterOb0TestMixin", "docstring": "Tests for `astropy.cosmology.Parameter` Ob0 on a Cosmology.\n\nOb0 is a descriptor, which are tested by mixin, here with ``TestFLRW``.\nThese tests expect dicts ``_cls_args`` and ``cls_kwargs`` which give the\nargs and kwargs for the cosmology class, respectively. See ``TestFLRW``.", "methods": ["test_Ob0", "test_init_Ob0"], "attributes": [], "code_location": {"file": "test_parameters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 398, "end_line": 470}, "type": "class"}, {"name": "test_littleh", "is_method": false, "class_name": null, "parameters": [], "calls": ["assert_quantity_allclose", "assert_quantity_allclose", "assert_quantity_allclose", "assert_quantity_allclose", "h70dist.to", "cosmodist.to", "h1lum.to", "withlittlehmag.to", "cu.with_H0", "cu.with_H0", "cu.with_H0", "u.MagUnit", "cu.with_H0", "default_cosmology.get"], "code_location": {"file": "test_units.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests", "start_line": 16, "end_line": 36}, "code_snippet": "def test_littleh():\n    \"\"\"Test :func:`astropy.cosmology.units.with_H0`.\"\"\"\n    H0_70 = 70 * u.km / u.s / u.Mpc\n    h70dist = 70 * u.Mpc / cu.littleh\n\n    assert_quantity_allclose(h70dist.to(u.Mpc, cu.with_H0(H0_70)), 100 * u.Mpc)\n\n    # make sure using the default cosmology works\n    cosmodist = default_cosmology.get().H0.value * u.Mpc / cu.littleh\n    assert_quantity_allclose(cosmodist.to(u.Mpc, cu.with_H0()), 100 * u.Mpc)\n\n    # Now try a luminosity scaling\n    h1lum = 0.49 * u.Lsun * cu.littleh**-2\n    assert_quantity_allclose(h1lum.to(u.Lsun, cu.with_H0(H0_70)), 1 * u.Lsun)\n\n    # And the trickiest one: magnitudes.  Using H0=10 here for the round numbers\n    H0_10 = 10 * u.km / u.s / u.Mpc\n    # assume the \"true\" magnitude M = 12.\n    # Then M - 5*log_10(h)  = M + 5 = 17\n    withlittlehmag = 17 * (u.mag - u.MagUnit(cu.littleh**2))\n    assert_quantity_allclose(withlittlehmag.to(u.mag, cu.with_H0(H0_10)), 12 * u.mag)\n", "type": "function"}, {"name": "ParameterTcmb0TestMixin", "docstring": "Tests for `astropy.cosmology.Parameter` Tcmb0 on a Cosmology.\n\nTcmb0 is a descriptor, which are tested by mixin, here with ``TestFLRW``.\nThese tests expect dicts ``_cls_args`` and ``cls_kwargs`` which give the\nargs and kwargs for the cosmology class, respectively. See ``TestFLRW``.", "methods": ["test_Tcmb0", "test_init_Tcmb0"], "attributes": [], "code_location": {"file": "test_parameters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 182, "end_line": 224}, "type": "class"}, {"name": "redshift_hubble", "is_method": false, "class_name": null, "parameters": ["cosmology"], "calls": ["u.Equivalency", "default_cosmology.get", "default_cosmology.set", "default_cosmology.get", "cosmology.H", "z_at_value", "hubble_to_z", "to_value", "z_to_hubble"], "code_location": {"file": "units_equivalencies.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src", "start_line": 132, "end_line": 198}, "code_snippet": "def redshift_hubble(\n    cosmology: Union[\"astropy.cosmology.Cosmology\", str, None] = None,\n    **atzkw: _UnpackZAtValueKWArgs,\n) -> u.Equivalency:\n    \"\"\"Convert quantities between redshift and Hubble parameter and little-h.\n\n    Care should be taken to not misinterpret a relativistic, gravitational, etc\n    redshift as a cosmological one.\n\n    Parameters\n    ----------\n    cosmology : `~astropy.cosmology.Cosmology`, str, or None, optional\n        A cosmology realization or built-in cosmology's name (e.g. 'Planck18').\n        If None, will use the default cosmology\n        (controlled by |default_cosmology|).\n    **atzkw\n        keyword arguments for :func:`~astropy.cosmology.z_at_value`\n\n    Returns\n    -------\n    `~astropy.units.equivalencies.Equivalency`\n        Equivalency between redshift and Hubble parameter and little-h unit.\n\n    Examples\n    --------\n    >>> import astropy.units as u\n    >>> import astropy.cosmology.units as cu\n    >>> from astropy.cosmology import WMAP9\n\n    >>> z = 1100 * cu.redshift\n    >>> equivalency = cu.redshift_hubble(WMAP9)  # construct equivalency\n\n    >>> z.to(u.km / u.s / u.Mpc, equivalency)  # doctest: +FLOAT_CMP\n    <Quantity 1565637.40154275 km / (Mpc s)>\n\n    >>> z.to(cu.littleh, equivalency)  # doctest: +FLOAT_CMP\n    <Quantity 15656.37401543 littleh>\n    \"\"\"\n    # get cosmology: None -> default and process str / class\n    cosmology = cosmology if cosmology is not None else default_cosmology.get()\n    with default_cosmology.set(cosmology):  # if already cosmo, passes through\n        cosmology = default_cosmology.get()\n\n    def z_to_hubble(z):\n        \"\"\"Redshift to Hubble parameter.\"\"\"\n        return cosmology.H(z)\n\n    def hubble_to_z(H):\n        \"\"\"Hubble parameter to redshift.\"\"\"\n        return z_at_value(cosmology.H, H << (u.km / u.s / u.Mpc), **atzkw)\n\n    def z_to_littleh(z):\n        \"\"\"Redshift to :math:`h`-unit Quantity.\"\"\"\n        return z_to_hubble(z).to_value(u.km / u.s / u.Mpc) / 100 * littleh\n\n    def littleh_to_z(h):\n        \"\"\":math:`h`-unit Quantity to redshift.\"\"\"\n        return hubble_to_z(h * 100)\n\n    return u.Equivalency(\n        [\n            (redshift, u.km / u.s / u.Mpc, z_to_hubble, hubble_to_z),\n            (redshift, littleh, z_to_littleh, littleh_to_z),\n        ],\n        \"redshift_hubble\",\n        {\"cosmology\": cosmology},\n    )\n", "type": "function"}, {"name": "with_redshift", "is_method": false, "class_name": null, "parameters": ["cosmology"], "calls": ["u.Equivalency", "default_cosmology.get", "default_cosmology.set", "default_cosmology.get", "equivs.extend", "equivs.extend", "equivs.extend", "redshift_hubble", "redshift_temperature", "redshift_distance"], "code_location": {"file": "units_equivalencies.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src", "start_line": 252, "end_line": 343}, "code_snippet": "def with_redshift(\n    cosmology: Union[\"astropy.cosmology.Cosmology\", str, None] = None,\n    *,\n    distance: Literal[\"comoving\", \"lookback\", \"luminosity\"] = \"comoving\",\n    hubble: bool = True,\n    Tcmb: bool = True,\n    atzkw: _ZAtValueKWArgs | None = None,\n) -> u.Equivalency:\n    \"\"\"Convert quantities between measures of cosmological distance.\n\n    Note: by default all equivalencies are on and must be explicitly turned off.\n    Care should be taken to not misinterpret a relativistic, gravitational, etc\n    redshift as a cosmological one.\n\n    Parameters\n    ----------\n    cosmology : `~astropy.cosmology.Cosmology`, str, or None, optional\n        A cosmology realization or built-in cosmology's name (e.g. 'Planck18').\n        If `None`, will use the default cosmology\n        (controlled by |default_cosmology|).\n\n    distance : {'comoving', 'lookback', 'luminosity'} or None (optional, keyword-only)\n        The type of distance equivalency to create or `None`.\n        Default is 'comoving'.\n    hubble : bool (optional, keyword-only)\n        Whether to create a Hubble parameter <-> redshift equivalency, using\n        ``Cosmology.H``. Default is `True`.\n    Tcmb : bool (optional, keyword-only)\n        Whether to create a CMB temperature <-> redshift equivalency, using\n        ``Cosmology.Tcmb``. Default is `True`.\n\n    atzkw : dict or None (optional, keyword-only)\n        keyword arguments for :func:`~astropy.cosmology.z_at_value`\n\n    Returns\n    -------\n    `~astropy.units.equivalencies.Equivalency`\n        With equivalencies between redshift and distance / Hubble / temperature.\n\n    Examples\n    --------\n    >>> import astropy.units as u\n    >>> import astropy.cosmology.units as cu\n    >>> from astropy.cosmology import WMAP9\n\n    >>> equivalency = cu.with_redshift(WMAP9)\n    >>> z = 1100 * cu.redshift\n\n    Redshift to (comoving) distance:\n\n    >>> z.to(u.Mpc, equivalency)  # doctest: +FLOAT_CMP\n    <Quantity 14004.03157418 Mpc>\n\n    Redshift to the Hubble parameter:\n\n    >>> z.to(u.km / u.s / u.Mpc, equivalency)  # doctest: +FLOAT_CMP\n    <Quantity 1565637.40154275 km / (Mpc s)>\n\n    >>> z.to(cu.littleh, equivalency)  # doctest: +FLOAT_CMP\n    <Quantity 15656.37401543 littleh>\n\n    Redshift to CMB temperature:\n\n    >>> z.to(u.K, equivalency)\n    <Quantity 3000.225 K>\n    \"\"\"\n    # get cosmology: None -> default and process str / class\n    cosmology = cosmology if cosmology is not None else default_cosmology.get()\n    with default_cosmology.set(cosmology):  # if already cosmo, passes through\n        cosmology = default_cosmology.get()\n\n    atzkw = atzkw if atzkw is not None else {}\n    equivs: list[u.Equivalency] = []  # will append as built\n\n    # Hubble <-> Redshift\n    if hubble:\n        equivs.extend(redshift_hubble(cosmology, **atzkw))\n\n    # CMB Temperature <-> Redshift\n    if Tcmb:\n        equivs.extend(redshift_temperature(cosmology, **atzkw))\n\n    # Distance <-> Redshift, but need to choose which distance\n    if distance is not None:\n        equivs.extend(redshift_distance(cosmology, kind=distance, **atzkw))\n\n    # -----------\n    return u.Equivalency(\n        equivs,\n        \"with_redshift\",\n        {\"cosmology\": cosmology, \"distance\": distance, \"hubble\": hubble, \"Tcmb\": Tcmb},\n    )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.440138101577759}
{"question": "How does the method that processes argument values in the custom argparse action class used for command-line options that accept either comma-separated lists or file references in the FITS file comparison script prevent unauthorized access to files outside the intended directory when processing file path arguments that begin with the '@' character?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "_expand_user_if_path", "is_method": false, "class_name": null, "parameters": ["argument"], "calls": ["isinstance", "isinstance", "exists", "str", "expanduser", "Path"], "code_location": {"file": "ui.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/ascii", "start_line": 322, "end_line": 335}, "code_snippet": "def _expand_user_if_path(argument):\n    if isinstance(argument, (str, bytes, os.PathLike)):\n        # For the `read()` method, a `str` input can be either a file path or\n        # the table data itself. File names for io.ascii cannot have newlines\n        # in them and io.ascii does not accept table data as `bytes`, so we can\n        # attempt to detect data strings like this.\n        is_str_data = isinstance(argument, str) and (\n            \"\\n\" in argument or \"\\r\" in argument\n        )\n        if not is_str_data:\n            # Remain conservative in expanding the presumed-path\n            if (ex_user := Path(argument).expanduser()).exists():\n                argument = str(ex_user)\n    return argument\n", "type": "function"}, {"name": "test_read_write_tilde_paths", "is_method": false, "class_name": null, "parameters": ["home_is_tmpdir"], "calls": ["create_ccd_data", "os.path.join", "ccd_data.write", "CCDData.read", "np.testing.assert_array_equal", "os.path.exists"], "code_location": {"file": "test_ccddata.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/nddata/tests", "start_line": 1099, "end_line": 1110}, "code_snippet": "def test_read_write_tilde_paths(home_is_tmpdir):\n    # Test for reading and writing to tilde-prefixed paths without errors\n    ccd_data = create_ccd_data()\n    filename = os.path.join(\"~\", \"test.fits\")\n    ccd_data.write(filename)\n\n    ccd_disk = CCDData.read(filename, unit=ccd_data.unit)\n    np.testing.assert_array_equal(ccd_data.data, ccd_disk.data)\n\n    # Ensure the unexpanded path doesn't exist (e.g. no directory whose name is\n    # a literal ~ was created)\n    assert not os.path.exists(filename)\n", "type": "function"}, {"name": "handle_options", "is_method": false, "class_name": null, "parameters": ["argv"], "calls": ["argparse.ArgumentParser", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument_group", "group.add_argument", "group.add_argument", "parser.add_argument_group", "group.add_argument", "parser.parse_args", "len", "parser.error"], "code_location": {"file": "fitsdiff.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/scripts", "start_line": 95, "end_line": 277}, "code_snippet": "def handle_options(argv=None):\n    parser = argparse.ArgumentParser(\n        description=DESCRIPTION,\n        epilog=EPILOG,\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    # TODO: pass color and suggest_on_error as kwargs when PYTHON_LT_14 is dropped\n    parser.color = True\n    parser.suggest_on_error = True\n\n    parser.add_argument(\n        \"--version\", action=\"version\", version=f\"%(prog)s {__version__}\"\n    )\n\n    parser.add_argument(\n        \"fits_files\", metavar=\"file\", nargs=\"+\", help=\".fits files to process.\"\n    )\n\n    parser.add_argument(\n        \"-q\",\n        \"--quiet\",\n        action=\"store_true\",\n        help=\"Produce no output and just return a status code.\",\n    )\n\n    parser.add_argument(\n        \"-n\",\n        \"--num-diffs\",\n        type=int,\n        default=10,\n        dest=\"numdiffs\",\n        metavar=\"INTEGER\",\n        help=(\n            \"Max number of data differences (image pixel or table element) \"\n            \"to report per extension (default %(default)s).\"\n        ),\n    )\n\n    parser.add_argument(\n        \"-r\",\n        \"--rtol\",\n        \"--relative-tolerance\",\n        type=float,\n        default=None,\n        dest=\"rtol\",\n        metavar=\"NUMBER\",\n        help=(\n            \"The relative tolerance for comparison of two numbers, \"\n            \"specifically two floating point numbers.  This applies to data \"\n            \"in both images and tables, and to floating point keyword values \"\n            \"in headers (default %(default)s).\"\n        ),\n    )\n\n    parser.add_argument(\n        \"-a\",\n        \"--atol\",\n        \"--absolute-tolerance\",\n        type=float,\n        default=None,\n        dest=\"atol\",\n        metavar=\"NUMBER\",\n        help=(\n            \"The absolute tolerance for comparison of two numbers, \"\n            \"specifically two floating point numbers.  This applies to data \"\n            \"in both images and tables, and to floating point keyword values \"\n            \"in headers (default %(default)s).\"\n        ),\n    )\n\n    parser.add_argument(\n        \"-b\",\n        \"--no-ignore-blanks\",\n        action=\"store_false\",\n        dest=\"ignore_blanks\",\n        default=True,\n        help=(\n            \"Don't ignore trailing blanks (whitespace) in string values.  \"\n            \"Otherwise trailing blanks both in header keywords/values and in \"\n            \"table column values) are not treated as significant i.e., \"\n            \"without this option 'ABCDEF   ' and 'ABCDEF' are considered \"\n            \"equivalent. \"\n        ),\n    )\n\n    parser.add_argument(\n        \"--no-ignore-blank-cards\",\n        action=\"store_false\",\n        dest=\"ignore_blank_cards\",\n        default=True,\n        help=(\n            \"Don't ignore entirely blank cards in headers.  Normally fitsdiff \"\n            \"does not consider blank cards when comparing headers, but this \"\n            \"will ensure that even blank cards match up. \"\n        ),\n    )\n\n    parser.add_argument(\n        \"--exact\",\n        action=\"store_true\",\n        dest=\"exact_comparisons\",\n        default=False,\n        help=(\n            \"Report ALL differences, \"\n            \"overriding command-line options and FITSDIFF_SETTINGS. \"\n        ),\n    )\n\n    parser.add_argument(\n        \"-o\",\n        \"--output-file\",\n        metavar=\"FILE\",\n        help=\"Output results to this file; otherwise results are printed to stdout.\",\n    )\n\n    parser.add_argument(\n        \"-u\",\n        \"--ignore-hdus\",\n        action=StoreListAction,\n        default=[],\n        dest=\"ignore_hdus\",\n        metavar=\"HDU_NAMES\",\n        help=(\n            \"Comma-separated list of HDU names not to be compared.  HDU \"\n            \"names may contain wildcard patterns.\"\n        ),\n    )\n\n    group = parser.add_argument_group(\"Header Comparison Options\")\n\n    group.add_argument(\n        \"-k\",\n        \"--ignore-keywords\",\n        action=StoreListAction,\n        default=[],\n        dest=\"ignore_keywords\",\n        metavar=\"KEYWORDS\",\n        help=(\n            \"Comma-separated list of keywords not to be compared.  Keywords \"\n            \"may contain wildcard patterns.  To exclude all keywords, use \"\n            '\"*\"; make sure to have double or single quotes around the '\n            \"asterisk on the command-line.\"\n        ),\n    )\n\n    group.add_argument(\n        \"-c\",\n        \"--ignore-comments\",\n        action=StoreListAction,\n        default=[],\n        dest=\"ignore_comments\",\n        metavar=\"COMMENTS\",\n        help=(\n            \"Comma-separated list of keywords whose comments will not be \"\n            \"compared.  Wildcards may be used as with --ignore-keywords.\"\n        ),\n    )\n\n    group = parser.add_argument_group(\"Table Comparison Options\")\n\n    group.add_argument(\n        \"-f\",\n        \"--ignore-fields\",\n        action=StoreListAction,\n        default=[],\n        dest=\"ignore_fields\",\n        metavar=\"COLUMNS\",\n        help=(\n            \"Comma-separated list of fields (i.e. columns) not to be \"\n            'compared.  All columns may be excluded using \"*\" as with '\n            \"--ignore-keywords.\"\n        ),\n    )\n\n    options = parser.parse_args(argv)\n\n    # Determine which filenames to compare\n    if len(options.fits_files) != 2:\n        parser.error(\n            \"\\nfitsdiff requires two arguments; see `fitsdiff --help` for more details.\"\n        )\n\n    return options\n", "type": "function"}, {"name": "_expand_user_in_args", "is_method": false, "class_name": null, "parameters": ["args"], "calls": ["len", "isinstance", "os.path.expanduser", "os.path.exists", "os.path.dirname"], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/registry", "start_line": 14, "end_line": 21}, "code_snippet": "def _expand_user_in_args(args):\n    # Conservatively attempt to apply `os.path.expanduser` to the first\n    # argument, which can be either a path or the contents of a table.\n    if len(args) and isinstance(args[0], PATH_TYPES):\n        ex_user = os.path.expanduser(args[0])\n        if ex_user != args[0] and os.path.exists(os.path.dirname(ex_user)):\n            args = (ex_user,) + args[1:]\n    return args\n", "type": "function"}, {"name": "TestTildePaths", "docstring": "Exercises a few functions, just to ensure they run with tilde paths (i.e.\npaths like '~/filename.fits'). Also exercises a few subclasses. Most of the\nrest of the testing of tilde path handling is done by adding `home_is_data`\nand `home_is_temp` fixtures (defined and explained in __init__.py) to\nappropriate test cases, so that they are run both with and without tilde\npaths.", "methods": ["test_fits_info", "test_fits_printdiff", "test_fits_get_data", "test_fits_get_header", "test_fits_get_set_del_val", "test_header_formatter", "test_BinTableHDU_dump_load", "test_BinTableHDU_writeto", "test_TableHDU_writeto", "fits_tabledump", "test_ImageHDU_writeto", "test_CompImageHDU_writeto"], "attributes": [], "code_location": {"file": "test_tilde_path.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 15, "end_line": 176}, "type": "class"}, {"name": "handle_options", "is_method": false, "class_name": null, "parameters": ["args"], "calls": ["argparse.ArgumentParser", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.parse_args", "len"], "code_location": {"file": "fitscheck.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/scripts", "start_line": 65, "end_line": 149}, "code_snippet": "def handle_options(args):\n    if not len(args):\n        args = [\"-h\"]\n\n    parser = argparse.ArgumentParser(\n        description=DESCRIPTION, formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    # TODO: pass color and suggest_on_error as kwargs when PYTHON_LT_14 is dropped\n    parser.color = True\n    parser.suggest_on_error = True\n\n    parser.add_argument(\n        \"--version\", action=\"version\", version=f\"%(prog)s {__version__}\"\n    )\n\n    parser.add_argument(\n        \"fits_files\", metavar=\"file\", nargs=\"+\", help=\".fits files to process.\"\n    )\n\n    parser.add_argument(\n        \"-k\",\n        \"--checksum\",\n        dest=\"checksum_kind\",\n        choices=[\"standard\", \"remove\", \"none\"],\n        help=\"Choose FITS checksum mode or none.  Defaults standard.\",\n        default=\"standard\",\n    )\n\n    parser.add_argument(\n        \"-w\",\n        \"--write\",\n        dest=\"write_file\",\n        help=\"Write out file checksums and/or FITS compliance fixes.\",\n        default=False,\n        action=\"store_true\",\n    )\n\n    parser.add_argument(\n        \"-f\",\n        \"--force\",\n        dest=\"force\",\n        help=\"Do file update even if original checksum was bad.\",\n        default=False,\n        action=\"store_true\",\n    )\n\n    parser.add_argument(\n        \"-c\",\n        \"--compliance\",\n        dest=\"compliance\",\n        help=\"Do FITS compliance checking; fix if possible.\",\n        default=False,\n        action=\"store_true\",\n    )\n\n    parser.add_argument(\n        \"-i\",\n        \"--ignore-missing\",\n        dest=\"ignore_missing\",\n        help=\"Ignore missing checksums.\",\n        default=False,\n        action=\"store_true\",\n    )\n\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        dest=\"verbose\",\n        help=\"Generate extra output.\",\n        default=False,\n        action=\"store_true\",\n    )\n\n    global OPTIONS\n    OPTIONS = parser.parse_args(args)\n\n    if OPTIONS.checksum_kind == \"none\":\n        OPTIONS.checksum_kind = False\n    elif OPTIONS.checksum_kind == \"standard\":\n        OPTIONS.checksum_kind = True\n    elif OPTIONS.checksum_kind == \"remove\":\n        OPTIONS.write_file = True\n        OPTIONS.force = True\n\n    return OPTIONS.fits_files\n", "type": "function"}, {"name": "test_wildcard", "is_method": true, "class_name": "TestFITSDiff_script", "parameters": ["self"], "calls": ["self.temp", "pytest.raises", "fitsdiff.main"], "code_location": {"file": "test_fitsdiff.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 192, "end_line": 196}, "code_snippet": "    def test_wildcard(self):\n        tmp1 = self.temp(\"tmp_file1\")\n        with pytest.raises(SystemExit) as e:\n            fitsdiff.main([tmp1 + \"*\", \"ACME\"])\n        assert e.value.code == 2\n", "type": "function"}, {"name": "test_compressed_fits", "is_method": true, "class_name": "TestFits2Bitmap", "parameters": ["self", "tmp_path", "file_exten"], "calls": ["pytest.mark.parametrize", "str", "fits.writeto", "main"], "code_location": {"file": "test_fits2bitmap.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/visualization/scripts/tests", "start_line": 50, "end_line": 53}, "code_snippet": "    def test_compressed_fits(self, tmp_path, file_exten):\n        filename = str(tmp_path / f\"test.fits{file_exten}\")\n        fits.writeto(filename, self.array)\n        main([filename, \"-e\", \"0\"])\n", "type": "function"}, {"name": "test_fits_file_path_object", "is_method": true, "class_name": "TestCore", "parameters": ["self"], "calls": ["pathlib.Path", "self.data", "fits.open", "filebytes", "filebytes", "fits.open", "self.data", "FITSDiff"], "code_location": {"file": "test_core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 75, "end_line": 85}, "code_snippet": "    def test_fits_file_path_object(self):\n        \"\"\"\n        Testing when fits file is passed as pathlib.Path object #4412.\n        \"\"\"\n        fpath = pathlib.Path(self.data(\"tdim.fits\"))\n        with fits.open(fpath) as hdulist:\n            assert hdulist[0].filebytes() == 2880\n            assert hdulist[1].filebytes() == 5760\n\n            with fits.open(self.data(\"tdim.fits\")) as hdulist2:\n                assert FITSDiff(hdulist2, hdulist).identical is True\n", "type": "function"}, {"name": "test_read_from_tilde_path", "is_method": false, "class_name": null, "parameters": ["home_is_data"], "calls": ["os.path.join", "np.errstate", "parse", "Table.read"], "code_location": {"file": "test_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable/tests", "start_line": 120, "end_line": 126}, "code_snippet": "def test_read_from_tilde_path(home_is_data):\n    # Just test that these run without error for tilde-paths\n    path = os.path.join(\"~\", \"regression.xml\")\n    with np.errstate(over=\"ignore\"):\n        # https://github.com/astropy/astropy/issues/13341\n        votable = parse(path)\n        Table.read(path, format=\"votable\", table_id=\"main_table\")\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.494229555130005}
{"question": "How does the method that appends multiple keyword-value cards to a FITS header handle inserting cards with commentary keywords differently from standard keyword-value cards when the parameter that prevents duplicate keywords is enabled, to ensure the header maintains a length of 5 cards in the test that verifies this behavior?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_header_extend_update_commentary", "is_method": true, "class_name": "TestHeaderFunctions", "parameters": ["self"], "calls": ["fits.PrimaryHDU", "fits.ImageHDU", "hdu.header.extend", "len"], "code_location": {"file": "test_header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 1475, "end_line": 1492}, "code_snippet": "    def test_header_extend_update_commentary(self):\n        \"\"\"\n        Test extending header with and without unique=True and commentary\n        cards in the header being added.\n\n        Though not quite the same as astropy/astropy#3967, update=True hits\n        the same if statement as that issue.\n        \"\"\"\n        for commentary_card in [\"\", \"COMMENT\", \"HISTORY\"]:\n            for is_update in [True, False]:\n                hdu = fits.PrimaryHDU()\n                # Make sure we are testing the case we want.\n                assert commentary_card not in hdu.header\n                hdu2 = fits.ImageHDU()\n                hdu2.header[commentary_card] = \"My text\"\n                hdu.header.extend(hdu2.header, update=is_update)\n                assert len(hdu.header) == 5\n                assert hdu.header[commentary_card][0] == \"My text\"\n", "type": "function"}, {"name": "test_header_extend_unique_commentary", "is_method": true, "class_name": "TestHeaderFunctions", "parameters": ["self"], "calls": ["fits.PrimaryHDU", "fits.ImageHDU", "hdu.header.extend", "len"], "code_location": {"file": "test_header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 1428, "end_line": 1442}, "code_snippet": "    def test_header_extend_unique_commentary(self):\n        \"\"\"\n        Test extending header with and without unique=True and commentary\n        cards in the header being added. Issue astropy/astropy#3967\n        \"\"\"\n        for commentary_card in [\"\", \"COMMENT\", \"HISTORY\"]:\n            for is_unique in [True, False]:\n                hdu = fits.PrimaryHDU()\n                # Make sure we are testing the case we want.\n                assert commentary_card not in hdu.header\n                hdu2 = fits.ImageHDU()\n                hdu2.header[commentary_card] = \"My text\"\n                hdu.header.extend(hdu2.header, unique=is_unique)\n                assert len(hdu.header) == 5\n                assert hdu.header[commentary_card][0] == \"My text\"\n", "type": "function"}, {"name": "test_long_commentary_card", "is_method": true, "class_name": "TestHeaderFunctions", "parameters": ["self"], "calls": ["fits.Header", "header.set", "fits.Header", "header.update", "header.update", "header.add_history", "header.update", "header.add_history", "header.add_history", "len", "str", "rstrip", "len", "str", "rstrip", "len", "str", "rstrip", "len", "str", "rstrip", "list", "str", "str", "str", "str"], "code_location": {"file": "test_header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 1745, "end_line": 1780}, "code_snippet": "    def test_long_commentary_card(self):\n        header = fits.Header()\n        header[\"FOO\"] = \"BAR\"\n        header[\"BAZ\"] = \"QUX\"\n        longval = \"ABC\" * 30\n        header[\"HISTORY\"] = longval\n        header[\"FRED\"] = \"BARNEY\"\n        header[\"HISTORY\"] = longval\n\n        assert len(header) == 7\n        assert list(header)[2] == \"FRED\"\n        assert str(header.cards[3]) == \"HISTORY \" + longval[:72]\n        assert str(header.cards[4]).rstrip() == \"HISTORY \" + longval[72:]\n\n        header.set(\"HISTORY\", longval, after=\"FOO\")\n        assert len(header) == 9\n        assert str(header.cards[1]) == \"HISTORY \" + longval[:72]\n        assert str(header.cards[2]).rstrip() == \"HISTORY \" + longval[72:]\n\n        header = fits.Header()\n        header.update({\"FOO\": \"BAR\"})\n        header.update({\"BAZ\": \"QUX\"})\n        longval = \"ABC\" * 30\n        header.add_history(longval)\n        header.update({\"FRED\": \"BARNEY\"})\n        header.add_history(longval)\n\n        assert len(header.cards) == 7\n        assert header.cards[2].keyword == \"FRED\"\n        assert str(header.cards[3]) == \"HISTORY \" + longval[:72]\n        assert str(header.cards[4]).rstrip() == \"HISTORY \" + longval[72:]\n\n        header.add_history(longval, after=\"FOO\")\n        assert len(header.cards) == 9\n        assert str(header.cards[1]) == \"HISTORY \" + longval[:72]\n        assert str(header.cards[2]).rstrip() == \"HISTORY \" + longval[72:]\n", "type": "function"}, {"name": "test_header_append_keyword_only", "is_method": true, "class_name": "TestHeaderFunctions", "parameters": ["self"], "calls": ["fits.Header", "header.append", "header.append", "len", "len", "list", "list"], "code_location": {"file": "test_header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 1543, "end_line": 1564}, "code_snippet": "    def test_header_append_keyword_only(self):\n        \"\"\"\n        Test appending a new card with just the keyword, and no value or\n        comment given.\n        \"\"\"\n\n        header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n        header.append(\"E\")\n        assert len(header) == 3\n        assert list(header)[-1] == \"E\"\n        assert header[-1] is None\n        assert header.comments[\"E\"] == \"\"\n\n        # Try appending a blank--normally this can be accomplished with just\n        # header.append(), but header.append('') should also work (and is maybe\n        # a little more clear)\n        header.append(\"\")\n        assert len(header) == 4\n\n        assert list(header)[-1] == \"\"\n        assert header[\"\"] == \"\"\n        assert header.comments[\"\"] == \"\"\n", "type": "function"}, {"name": "test_long_commentary_card_appended_to_header", "is_method": true, "class_name": "TestHeaderFunctions", "parameters": ["self"], "calls": ["fits.Header", "header.append", "_split", "fits.PrimaryHDU", "hdu.writeto", "len", "self.temp"], "code_location": {"file": "test_header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 2633, "end_line": 2655}, "code_snippet": "    def test_long_commentary_card_appended_to_header(self):\n        \"\"\"\n        If a HISTORY or COMMENT card with a too-long value is appended to a\n        header with Header.append (as opposed to assigning to hdr['HISTORY']\n        it fails verification.\n\n        Regression test for https://github.com/astropy/astropy/issues/11486\n        \"\"\"\n\n        header = fits.Header()\n        value = \"abc\" * 90\n        # this is what Table does when saving its history metadata key to a\n        # FITS file\n        header.append((\"history\", value))\n        assert len(header.cards) == 1\n\n        # Test Card._split() directly since this was the main problem area\n        key, val = header.cards[0]._split()\n        assert key == \"HISTORY\" and val == value\n\n        # Try writing adding this header to an HDU and writing it to a file\n        hdu = fits.PrimaryHDU(header=header)\n        hdu.writeto(self.temp(\"test.fits\"), overwrite=True)\n", "type": "function"}, {"name": "append", "is_method": true, "class_name": "Header", "parameters": ["self", "card", "useblanks", "bottom", "end"], "calls": ["isinstance", "Card.normalize_keyword", "append", "Card", "isinstance", "self._cards.append", "self._cards.insert", "self._updateindices", "append", "Card", "len", "len", "sort", "self._countblanks", "self._useblanks", "Card", "isinstance", "ValueError", "len", "str"], "code_location": {"file": "header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits", "start_line": 1091, "end_line": 1178}, "code_snippet": "    def append(self, card=None, useblanks=True, bottom=False, end=False):\n        \"\"\"\n        Appends a new keyword+value card to the end of the Header, similar\n        to `list.append`.\n\n        By default if the last cards in the Header have commentary keywords,\n        this will append the new keyword before the commentary (unless the new\n        keyword is also commentary).\n\n        Also differs from `list.append` in that it can be called with no\n        arguments: In this case a blank card is appended to the end of the\n        Header.  In the case all the keyword arguments are ignored.\n\n        Parameters\n        ----------\n        card : str, tuple\n            A keyword or a (keyword, value, [comment]) tuple representing a\n            single header card; the comment is optional in which case a\n            2-tuple may be used\n\n        useblanks : bool, optional\n            If there are blank cards at the end of the Header, replace the\n            first blank card so that the total number of cards in the Header\n            does not increase.  Otherwise preserve the number of blank cards.\n\n        bottom : bool, optional\n            If True, instead of appending after the last non-commentary card,\n            append after the last non-blank card.\n\n        end : bool, optional\n            If True, ignore the useblanks and bottom options, and append at the\n            very end of the Header.\n\n        \"\"\"\n        if isinstance(card, str):\n            card = Card(card)\n        elif isinstance(card, tuple):\n            card = Card(*card)\n        elif card is None:\n            card = Card()\n        elif not isinstance(card, Card):\n            raise ValueError(\n                \"The value appended to a Header must be either a keyword or \"\n                f\"(keyword, value, [comment]) tuple; got: {card!r}\"\n            )\n\n        if not end and card.is_blank:\n            # Blank cards should always just be appended to the end\n            end = True\n\n        if end:\n            self._cards.append(card)\n            idx = len(self._cards) - 1\n        else:\n            idx = len(self._cards) - 1\n            while idx >= 0 and self._cards[idx].is_blank:\n                idx -= 1\n\n            if not bottom and card.keyword not in _commentary_keywords:\n                while idx >= 0 and self._cards[idx].keyword in _commentary_keywords:\n                    idx -= 1\n\n            idx += 1\n            self._cards.insert(idx, card)\n            self._updateindices(idx)\n\n        keyword = Card.normalize_keyword(card.keyword)\n        self._keyword_indices[keyword].append(idx)\n        if card.field_specifier is not None:\n            self._rvkc_indices[card.rawkeyword].append(idx)\n\n        if not end:\n            # If the appended card was a commentary card, and it was appended\n            # before existing cards with the same keyword, the indices for\n            # cards with that keyword may have changed\n            if not bottom and card.keyword in _commentary_keywords:\n                self._keyword_indices[keyword].sort()\n\n            # Finally, if useblanks, delete a blank cards from the end\n            if useblanks and self._countblanks():\n                # Don't do this unless there is at least one blanks at the end\n                # of the header; we need to convert the card to its string\n                # image to see how long it is.  In the vast majority of cases\n                # this will just be 80 (Card.length) but it may be longer for\n                # CONTINUE cards\n                self._useblanks(len(str(card)) // Card.length)\n\n        self._modified = True\n", "type": "function"}, {"name": "extend", "is_method": true, "class_name": "Header", "parameters": ["self", "cards", "strip", "unique", "update", "update_first", "useblanks", "bottom", "end"], "calls": ["self.__class__", "len", "enumerate", "temp.strip", "self.append", "extend_cards.append", "extend_cards.append", "extend_cards.append", "extend_cards.append", "self.insert", "extend_cards.append"], "code_location": {"file": "header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits", "start_line": 1180, "end_line": 1287}, "code_snippet": "    def extend(\n        self,\n        cards,\n        strip=True,\n        unique=False,\n        update=False,\n        update_first=False,\n        useblanks=True,\n        bottom=False,\n        end=False,\n    ):\n        \"\"\"\n        Appends multiple keyword+value cards to the end of the header, similar\n        to `list.extend`.\n\n        Parameters\n        ----------\n        cards : iterable\n            An iterable of (keyword, value, [comment]) tuples; see\n            `Header.append`.\n\n        strip : bool, optional\n            Remove any keywords that have meaning only to specific types of\n            HDUs, so that only more general keywords are added from extension\n            Header or Card list (default: `True`).\n\n        unique : bool, optional\n            If `True`, ensures that no duplicate keywords are appended;\n            keywords already in this header are simply discarded.  The\n            exception is commentary keywords (COMMENT, HISTORY, etc.): they are\n            only treated as duplicates if their values match.\n\n        update : bool, optional\n            If `True`, update the current header with the values and comments\n            from duplicate keywords in the input header.  This supersedes the\n            ``unique`` argument.  Commentary keywords are treated the same as\n            if ``unique=True``.\n\n        update_first : bool, optional\n            If the first keyword in the header is 'SIMPLE', and the first\n            keyword in the input header is 'XTENSION', the 'SIMPLE' keyword is\n            replaced by the 'XTENSION' keyword.  Likewise if the first keyword\n            in the header is 'XTENSION' and the first keyword in the input\n            header is 'SIMPLE', the 'XTENSION' keyword is replaced by the\n            'SIMPLE' keyword.  This behavior is otherwise dumb as to whether or\n            not the resulting header is a valid primary or extension header.\n            This is mostly provided to support backwards compatibility with the\n            old ``Header.fromTxtFile`` method, and only applies if\n            ``update=True``.\n\n        useblanks, bottom, end : bool, optional\n            These arguments are passed to :meth:`Header.append` while appending\n            new cards to the header.\n        \"\"\"\n        temp = self.__class__(cards)\n        if strip:\n            temp.strip()\n\n        if len(self):\n            first = self._cards[0].keyword\n        else:\n            first = None\n\n        # We don't immediately modify the header, because first we need to sift\n        # out any duplicates in the new header prior to adding them to the\n        # existing header, but while *allowing* duplicates from the header\n        # being extended from (see ticket #156)\n        extend_cards = []\n\n        for idx, card in enumerate(temp.cards):\n            keyword = card.keyword\n            if keyword not in _commentary_keywords:\n                if unique and not update and keyword in self:\n                    continue\n                elif update:\n                    if idx == 0 and update_first:\n                        # Dumbly update the first keyword to either SIMPLE or\n                        # XTENSION as the case may be, as was in the case in\n                        # Header.fromTxtFile\n                        if (keyword == \"SIMPLE\" and first == \"XTENSION\") or (\n                            keyword == \"XTENSION\" and first == \"SIMPLE\"\n                        ):\n                            del self[0]\n                            self.insert(0, card)\n                        else:\n                            self[keyword] = (card.value, card.comment)\n                    elif keyword in self:\n                        self[keyword] = (card.value, card.comment)\n                    else:\n                        extend_cards.append(card)\n                else:\n                    extend_cards.append(card)\n            else:\n                if (unique or update) and keyword in self:\n                    if card.is_blank:\n                        extend_cards.append(card)\n                        continue\n\n                    for value in self[keyword]:\n                        if value == card.value:\n                            break\n                    else:\n                        extend_cards.append(card)\n                else:\n                    extend_cards.append(card)\n\n        for card in extend_cards:\n            self.append(card, useblanks=useblanks, bottom=bottom, end=end)\n", "type": "function"}, {"name": "test_header_insert_use_blanks", "is_method": true, "class_name": "TestHeaderFunctions", "parameters": ["self"], "calls": ["fits.Header", "header.append", "header.append", "header.insert", "header.insert", "len", "len"], "code_location": {"file": "test_header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 1566, "end_line": 1584}, "code_snippet": "    def test_header_insert_use_blanks(self):\n        header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n\n        # Append a couple blanks\n        header.append()\n        header.append()\n\n        # Insert a new card; should use up one of the blanks\n        header.insert(1, (\"E\", \"F\"))\n        assert len(header) == 4\n        assert header[1] == \"F\"\n        assert header[-1] == \"\"\n        assert header[-2] == \"D\"\n\n        # Insert a new card without using blanks\n        header.insert(1, (\"G\", \"H\"), useblanks=False)\n        assert len(header) == 5\n        assert header[1] == \"H\"\n        assert header[-1] == \"\"\n", "type": "function"}, {"name": "test_header_append_use_blanks", "is_method": true, "class_name": "TestHeaderFunctions", "parameters": ["self"], "calls": ["fits.Header", "header.append", "header.append", "header.append", "header.append", "len", "len", "len"], "code_location": {"file": "test_header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 1516, "end_line": 1541}, "code_snippet": "    def test_header_append_use_blanks(self):\n        \"\"\"\n        Tests that blank cards can be appended, and that future appends will\n        use blank cards when available (unless useblanks=False)\n        \"\"\"\n\n        header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n\n        # Append a couple blanks\n        header.append()\n        header.append()\n        assert len(header) == 4\n        assert header[-1] == \"\"\n        assert header[-2] == \"\"\n\n        # New card should fill the first blank by default\n        header.append((\"E\", \"F\"))\n        assert len(header) == 4\n        assert header[-2] == \"F\"\n        assert header[-1] == \"\"\n\n        # This card should not use up a blank spot\n        header.append((\"G\", \"H\"), useblanks=False)\n        assert len(header) == 5\n        assert header[-1] == \"\"\n        assert header[-2] == \"H\"\n", "type": "function"}, {"name": "_update", "is_method": true, "class_name": "Header", "parameters": ["self", "card"], "calls": ["removeprefix", "upper", "self._splitcommentary", "self.append", "self.index", "reversed", "keyword.strip", "self.insert", "self.append", "len"], "code_location": {"file": "header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits", "start_line": 1634, "end_line": 1671}, "code_snippet": "    def _update(self, card):\n        \"\"\"\n        The real update code.  If keyword already exists, its value and/or\n        comment will be updated.  Otherwise a new card will be appended.\n\n        This will not create a duplicate keyword except in the case of\n        commentary cards.  The only other way to force creation of a duplicate\n        is to use the insert(), append(), or extend() methods.\n        \"\"\"\n        keyword, value, comment = card\n\n        # Lookups for existing/known keywords are case-insensitive\n        keyword = keyword.strip().upper().removeprefix(\"HIERARCH \")\n\n        if keyword not in _commentary_keywords and keyword in self._keyword_indices:\n            # Easy; just update the value/comment\n            idx = self._keyword_indices[keyword][0]\n            existing_card = self._cards[idx]\n            existing_card.value = value\n            if comment is not None:\n                # '' should be used to explicitly blank a comment\n                existing_card.comment = comment\n            if existing_card._modified:\n                self._modified = True\n        elif keyword in _commentary_keywords:\n            cards = self._splitcommentary(keyword, value)\n            if keyword in self._keyword_indices:\n                # Append after the last keyword of the same type\n                idx = self.index(keyword, start=len(self) - 1, stop=-1)\n                isblank = not (keyword or value or comment)\n                for c in reversed(cards):\n                    self.insert(idx + 1, c, useblanks=(not isblank))\n            else:\n                for c in cards:\n                    self.append(c, bottom=True)\n        else:\n            # A new keyword! self.append() will handle updating _modified\n            self.append(card)\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.524873971939087}
{"question": "How does clearing the cache property in the output subformat setter propagate through the base time class hierarchy via the instance tracking dictionary to invalidate cached format and scale conversions?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "out_subfmt", "is_method": true, "class_name": "TimeBase", "parameters": ["self", "val"], "calls": [], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time", "start_line": 879, "end_line": 882}, "code_snippet": "    def out_subfmt(self, val):\n        # Setting the out_subfmt property here does validation of ``val``\n        self._time.out_subfmt = val\n        del self.cache\n", "type": "function"}, {"name": "in_subfmt", "is_method": true, "class_name": "TimeBase", "parameters": ["self", "val"], "calls": [], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time", "start_line": 867, "end_line": 869}, "code_snippet": "    def in_subfmt(self, val):\n        self._time.in_subfmt = val\n        del self.cache\n", "type": "function"}, {"name": "__init_subclass__", "is_method": true, "class_name": "TimeFormat", "parameters": ["cls"], "calls": ["__init_subclass__", "_regexify_subfmts", "ValueError", "super", "hasattr"], "code_location": {"file": "formats.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time", "start_line": 173, "end_line": 192}, "code_snippet": "    def __init_subclass__(cls, **kwargs):\n        # Register time formats that define a name, but leave out astropy_time since\n        # it is not a user-accessible format and is only used for initialization into\n        # a different format.\n        if \"name\" in cls.__dict__ and cls.name != \"astropy_time\":\n            # FIXME: check here that we're not introducing a collision with\n            # an existing method or attribute; problem is it could be either\n            # astropy.time.Time or astropy.time.TimeDelta, and at the point\n            # where this is run neither of those classes have necessarily been\n            # constructed yet.\n            if \"value\" in cls.__dict__ and not hasattr(cls.value, \"fget\"):\n                raise ValueError(\"If defined, 'value' must be a property\")\n\n            cls._registry[cls.name] = cls\n\n        # If this class defines its own subfmts, preprocess the definitions.\n        if \"subfmts\" in cls.__dict__:\n            cls.subfmts = _regexify_subfmts(cls.subfmts)\n\n        return super().__init_subclass__(**kwargs)\n", "type": "function"}, {"name": "test_basic_subformat_cache_does_not_crash", "is_method": true, "class_name": "TestNumericalSubFormat", "parameters": ["self"], "calls": ["Time", "t.to_value", "t.to_value"], "code_location": {"file": "test_basic.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 1241, "end_line": 1245}, "code_snippet": "    def test_basic_subformat_cache_does_not_crash(self):\n        t = Time(\"2001\", format=\"jyear\", scale=\"tai\")\n        t.to_value(\"mjd\", subfmt=\"str\")\n        assert (\"mjd\", \"str\", \"astropy\") in t.cache[\"format\"]\n        t.to_value(\"mjd\", \"str\")\n", "type": "function"}, {"name": "__init_subclass__", "is_method": true, "class_name": "Base", "parameters": ["cls"], "calls": ["__init_subclass__", "cls.__name__.lower", "super"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/format", "start_line": 39, "end_line": 46}, "code_snippet": "    def __init_subclass__(cls, **kwargs):\n        # Keep a registry of all formats.  Key by the class name unless a name\n        # is explicitly set (i.e., one *not* inherited from a superclass).\n        if \"name\" not in cls.__dict__:\n            cls.name = cls.__name__.lower()\n\n        Base.registry[cls.name] = cls\n        super().__init_subclass__(**kwargs)\n", "type": "function"}, {"name": "test_basic_subformat_setting", "is_method": true, "class_name": "TestNumericalSubFormat", "parameters": ["self"], "calls": ["Time", "t.value.startswith"], "code_location": {"file": "test_basic.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 1235, "end_line": 1239}, "code_snippet": "    def test_basic_subformat_setting(self):\n        t = Time(\"2001\", format=\"jyear\", scale=\"tai\")\n        t.format = \"mjd\"\n        t.out_subfmt = \"str\"\n        assert t.value.startswith(\"5\")\n", "type": "function"}, {"name": "test_cache", "is_method": false, "class_name": null, "parameters": [], "calls": ["Time", "Time"], "code_location": {"file": "test_basic.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 1820, "end_line": 1843}, "code_snippet": "def test_cache():\n    t = Time(\"2010-09-03 00:00:00\")\n    t2 = Time(\"2010-09-03 00:00:00\")\n\n    # Time starts out without a cache\n    assert \"cache\" not in t.__dict__\n\n    # Access the iso format and confirm that the cached version is as expected\n    t.iso\n    assert t.cache[\"format\"][\"iso\", \"*\", \"astropy\"] == t2.iso\n\n    # Access the TAI scale and confirm that the cached version is as expected\n    t.tai\n    assert t.cache[\"scale\"][\"tai\"] == t2.tai\n\n    # New Time object after scale transform does not have a cache yet\n    assert \"cache\" not in t.tt.__dict__\n\n    # Clear the cache\n    del t.cache\n    assert \"cache\" not in t.__dict__\n    # Check accessing the cache creates an empty dictionary\n    assert not t.cache\n    assert \"cache\" in t.__dict__\n", "type": "function"}, {"name": "to_value", "is_method": true, "class_name": "TimeFormat", "parameters": ["self", "parent", "out_subfmt"], "calls": [], "code_location": {"file": "formats.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time", "start_line": 397, "end_line": 433}, "code_snippet": "    def to_value(self, parent=None, out_subfmt=None):\n        \"\"\"\n        Return time representation from internal jd1 and jd2 in specified\n        ``out_subfmt``.\n\n        This is the base method that ignores ``parent`` and uses the ``value``\n        property to compute the output. This is done by temporarily setting\n        ``self.out_subfmt`` and calling ``self.value``. This is required for\n        legacy Format subclasses prior to astropy 4.0  New code should instead\n        implement the value functionality in ``to_value()`` and then make the\n        ``value`` property be a simple call to ``self.to_value()``.\n\n        Parameters\n        ----------\n        parent : object\n            Parent `~astropy.time.Time` object associated with this\n            `~astropy.time.TimeFormat` object\n        out_subfmt : str or None\n            Output subformt (use existing self.out_subfmt if `None`)\n\n        Returns\n        -------\n        value : numpy.array, numpy.ma.array\n            Array or masked array of formatted time representation values\n        \"\"\"\n        # Get value via ``value`` property, overriding out_subfmt temporarily if needed.\n        if out_subfmt is not None:\n            out_subfmt_orig = self.out_subfmt\n            try:\n                self.out_subfmt = out_subfmt\n                value = self.value\n            finally:\n                self.out_subfmt = out_subfmt_orig\n        else:\n            value = self.value\n\n        return value\n", "type": "function"}, {"name": "test_subformat_output_not_always_preserved", "is_method": true, "class_name": "TestNumericalSubFormat", "parameters": ["self"], "calls": ["Time"], "code_location": {"file": "test_basic.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 1208, "end_line": 1216}, "code_snippet": "    def test_subformat_output_not_always_preserved(self):\n        t = Time(\"2000-01-02\", format=\"fits\", out_subfmt=\"longdate\")\n        assert t.value == \"+02000-01-02\"\n        t.format = \"iso\"\n        assert t.out_subfmt == \"*\"\n        assert t.value == \"2000-01-02 00:00:00.000\"\n        t.format = \"fits\"\n        assert t.out_subfmt == \"*\"\n        assert t.value == \"2000-01-02T00:00:00.000\"\n", "type": "function"}, {"name": "out_subfmt", "is_method": true, "class_name": "TimeFormat", "parameters": ["self", "subfmt"], "calls": ["self._select_subfmts"], "code_location": {"file": "formats.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time", "start_line": 223, "end_line": 226}, "code_snippet": "    def out_subfmt(self, subfmt):\n        # Validate subfmt value for this class, raises ValueError if not.\n        self._select_subfmts(subfmt)\n        self._out_subfmt = subfmt\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.5620903968811035}
{"question": "How does the converter class for single-precision complex VOTable datatypes use its numpy dtype format string attribute to convert complex values to bytes for binary format serialization?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "Converter", "docstring": "The base class for all converters.  Each subclass handles\nconverting a specific VOTABLE data type to/from the TABLEDATA_ and\nBINARY_ on-disk representations.\n\nParameters\n----------\nfield : `~astropy.io.votable.tree.Field`\n    object describing the datatype\n\nconfig : dict\n    The parser configuration dictionary\n\npos : tuple\n    The position in the XML file where the FIELD object was\n    found.  Used for error messages.", "methods": ["__init__", "_parse_length", "_write_length", "supports_empty_values", "parse", "parse_scalar", "output", "binparse", "binoutput"], "attributes": [], "code_location": {"file": "converters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 156, "end_line": 296}, "type": "class"}, {"name": "binoutput", "is_method": true, "class_name": "Converter", "parameters": ["self", "value", "mask"], "calls": ["NotImplementedError"], "code_location": {"file": "converters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 275, "end_line": 296}, "code_snippet": "    def binoutput(self, value, mask):\n        \"\"\"\n        Convert the object *value* in the native in-memory datatype to\n        a string of bytes suitable for serialization in the BINARY_\n        format.\n\n        Parameters\n        ----------\n        value\n            The value, the native type corresponding to this converter\n\n        mask : bool\n            If `True`, will return the string representation of a\n            masked value.\n\n        Returns\n        -------\n        bytes : bytes\n            The binary representation of the value, suitable for\n            serialization in the BINARY_ format.\n        \"\"\"\n        raise NotImplementedError(\"This datatype must implement a 'binoutput' method.\")\n", "type": "function"}, {"name": "E03", "docstring": "Complex numbers should be two values separated by whitespace.\n\n**References**: `1.1\n<http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#sec:datatypes>`__,\n`1.2\n<http://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html#sec:datatypes>`__", "methods": [], "attributes": ["message_template", "default_args"], "code_location": {"file": "exceptions.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 1210, "end_line": 1220}, "type": "class"}, {"name": "output", "is_method": true, "class_name": "Complex", "parameters": ["self", "value", "mask"], "calls": ["self._output_format.format", "self._output_format.format", "float", "float", "real.removesuffix", "imag.removesuffix"], "code_location": {"file": "converters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 1083, "end_line": 1094}, "code_snippet": "    def output(self, value, mask):\n        if mask:\n            if self.null is None:\n                return \"NaN\"\n            else:\n                value = self.null\n        real = self._output_format.format(float(value.real))\n        imag = self._output_format.format(float(value.imag))\n        if self._output_format[2] == \"s\":\n            real = real.removesuffix(\".0\")\n            imag = imag.removesuffix(\".0\")\n        return real + \" \" + imag\n", "type": "function"}, {"name": "_convert_to_valid_data_type", "is_method": true, "class_name": "Column", "parameters": ["self", "array"], "calls": ["isinstance", "array.reshape", "len", "chararray.array", "_convert_array", "np.dtype", "np.dtype", "np.where", "np.where", "_convert_array", "_convert_array", "np.dtype", "ord", "ord", "ord", "ord", "np.dtype", "np.uint16", "np.uint32", "np.uint64", "numpy_format.replace", "np.dtype"], "code_location": {"file": "column.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits", "start_line": 1366, "end_line": 1433}, "code_snippet": "    def _convert_to_valid_data_type(self, array):\n        # Convert the format to a type we understand\n        if isinstance(array, Delayed):\n            return array\n        elif array is None:\n            return array\n        else:\n            format = self.format\n            dims = self._dims\n            if dims and format.format not in \"PQ\":\n                shape = dims[:-1] if \"A\" in format else dims\n                shape = (len(array),) + shape\n                array = array.reshape(shape)\n\n            if \"P\" in format or \"Q\" in format:\n                return array\n            elif \"A\" in format:\n                if array.dtype.char in \"SU\":\n                    if dims:\n                        # The 'last' dimension (first in the order given\n                        # in the TDIMn keyword itself) is the number of\n                        # characters in each string\n                        fsize = dims[-1]\n                    else:\n                        fsize = np.dtype(format.recformat).itemsize\n                    return chararray.array(array, itemsize=fsize, copy=False)\n                else:\n                    return _convert_array(array, np.dtype(format.recformat))\n            elif \"L\" in format:\n                # boolean needs to be scaled back to storage values ('T', 'F')\n                if array.dtype == np.dtype(\"bool\"):\n                    return np.where(array == np.False_, ord(\"F\"), ord(\"T\"))\n                else:\n                    return np.where(array == 0, ord(\"F\"), ord(\"T\"))\n            elif \"X\" in format:\n                return _convert_array(array, np.dtype(\"uint8\"))\n            else:\n                # Preserve byte order of the original array for now; see #77\n                numpy_format = array.dtype.byteorder + format.recformat\n\n                # Handle arrays passed in as unsigned ints as pseudo-unsigned\n                # int arrays; blatantly tacked in here for now--we need columns\n                # to have explicit knowledge of whether they treated as\n                # pseudo-unsigned\n                bzeros = {\n                    2: np.uint16(2**15),\n                    4: np.uint32(2**31),\n                    8: np.uint64(2**63),\n                }\n                if (\n                    array.dtype.kind == \"u\"\n                    and array.dtype.itemsize in bzeros\n                    and self.bscale in (1, None, \"\")\n                    and self.bzero == bzeros[array.dtype.itemsize]\n                ):\n                    # Basically the array is uint, has scale == 1.0, and the\n                    # bzero is the appropriate value for a pseudo-unsigned\n                    # integer of the input dtype, then go ahead and assume that\n                    # uint is assumed\n                    numpy_format = numpy_format.replace(\"i\", \"u\")\n                    self._pseudo_unsigned_ints = True\n\n                # The .base here means we're dropping the shape information,\n                # which is only used to format recarray fields, and is not\n                # useful for converting input arrays to the correct data type\n                dtype = np.dtype(numpy_format).base\n\n                return _convert_array(array, dtype)\n", "type": "function"}, {"name": "_write_binary", "is_method": true, "class_name": "TableElement", "parameters": ["self", "mode", "w"], "calls": ["w.tag", "w.tag", "io.BytesIO", "range", "w._flush", "w.write", "len", "decode", "enumerate", "np.array", "data.write", "data.write", "converters.bool_to_bitarray", "issubclass", "base64.b64encode", "np.all", "converter", "converter", "type", "vo_reraise", "data.getvalue"], "code_location": {"file": "tree.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 3452, "end_line": 3495}, "code_snippet": "    def _write_binary(self, mode, w, **kwargs):\n        fields = self.fields\n        array = self.array\n        if mode == 1:\n            tag_name = \"BINARY\"\n        else:\n            tag_name = \"BINARY2\"\n\n        with w.tag(tag_name):\n            with w.tag(\"STREAM\", encoding=\"base64\"):\n                fields_basic = [\n                    (i, field.converter.binoutput) for (i, field) in enumerate(fields)\n                ]\n\n                data = io.BytesIO()\n                for row in range(len(array)):\n                    array_row = array.data[row]\n                    array_mask = array.mask[row]\n                    if mode == 2:\n                        flattened = np.array([np.all(x) for x in array_mask])\n                        data.write(converters.bool_to_bitarray(flattened))\n\n                    for i, converter in fields_basic:\n                        try:\n                            # BINARY2 cannot handle individual array element masks\n                            converter_type = converter.__self__.__class__\n                            # Delegate converter to handle the mask\n                            delegate_condition = issubclass(\n                                converter_type, converters.Array\n                            )\n                            if mode == 1 or delegate_condition:\n                                chunk = converter(array_row[i], array_mask[i])\n                            else:\n                                # Mask is already handled by BINARY2 behaviour\n                                chunk = converter(array_row[i], None)\n                            assert type(chunk) == bytes\n                        except Exception as e:\n                            vo_reraise(\n                                e, additional=f\"(in row {row:d}, col '{fields[i].ID}')\"\n                            )\n                        data.write(chunk)\n\n                w._flush()\n                w.write(base64.b64encode(data.getvalue()).decode(\"ascii\"))\n", "type": "function"}, {"name": "binoutput", "is_method": true, "class_name": "BooleanArray", "parameters": ["self", "value", "mask"], "calls": ["np.asarray", "np.asarray", "_empty_bytes.join", "binoutput", "np.broadcast"], "code_location": {"file": "converters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 1226, "end_line": 1231}, "code_snippet": "    def binoutput(self, value, mask):\n        binoutput = self._base.binoutput\n        value = np.asarray(value)\n        mask = np.asarray(mask)\n        result = [binoutput(x, m) for x, m in np.broadcast(value.flat, mask.flat)]\n        return _empty_bytes.join(result)\n", "type": "function"}, {"name": "_dtype_to_recformat", "is_method": false, "class_name": null, "parameters": ["dtype"], "calls": ["isinstance", "np.dtype", "str"], "code_location": {"file": "column.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits", "start_line": 2504, "end_line": 2521}, "code_snippet": "def _dtype_to_recformat(dtype):\n    \"\"\"\n    Utility function for converting a dtype object or string that instantiates\n    a dtype (e.g. 'float32') into one of the two character Numpy format codes\n    that have been traditionally used by Astropy.\n    \"\"\"\n    if not isinstance(dtype, np.dtype):\n        dtype = np.dtype(dtype)\n\n    kind = dtype.base.kind\n\n    if kind in (\"U\", \"S\"):\n        recformat = kind = \"S\"\n    else:\n        itemsize = dtype.base.itemsize\n        recformat = kind + str(itemsize)\n\n    return recformat, kind, dtype\n", "type": "function"}, {"name": "output", "is_method": true, "class_name": "BitArray", "parameters": ["self", "value", "mask"], "calls": ["np.any", "np.asarray", "join", "vo_warn"], "code_location": {"file": "converters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 1137, "end_line": 1142}, "code_snippet": "    def output(self, value, mask):\n        if np.any(mask):\n            vo_warn(W39)\n        value = np.asarray(value)\n        mapping = {False: \"0\", True: \"1\"}\n        return \"\".join(mapping[x] for x in value.flat)\n", "type": "function"}, {"name": "recformat", "is_method": true, "class_name": "_ColumnFormat", "parameters": ["self"], "calls": ["_convert_format"], "code_location": {"file": "column.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits", "start_line": 325, "end_line": 327}, "code_snippet": "    def recformat(self):\n        \"\"\"Returns the equivalent Numpy record format string.\"\"\"\n        return _convert_format(self)\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.6043593883514404}
{"question": "What design mechanisms in the numpy.ndarray subclass that represents numbers with associated physical units in the astropy.units module enforce conversion prevention boundaries between quantities without physical dimensions and Python's built-in scalar type conversion mechanisms?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_numeric_converters", "is_method": true, "class_name": "TestQuantityOperations", "parameters": ["self"], "calls": ["u.Quantity", "u.Quantity", "u.Quantity", "u.Quantity", "u.Quantity", "pytest.raises", "float", "pytest.raises", "int", "pytest.raises", "q1.__index__", "float", "float", "int", "int", "pytest.raises", "q2.__index__", "float", "int", "pytest.raises", "q3.__index__", "float", "int", "q4.__index__", "pytest.raises", "float", "pytest.raises", "int", "pytest.raises", "q5.__index__", "q2.to_value", "q2.to_value"], "code_location": {"file": "test_quantity.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 630, "end_line": 702}, "code_snippet": "    def test_numeric_converters(self):\n        # float, int, long, and __index__ should only work for single\n        # quantities, of appropriate type, and only if they are dimensionless.\n        # for index, this should be unscaled as well\n        # (Check on __index__ is also a regression test for #1557)\n\n        # quantities with units should never convert, or be usable as an index\n        q1 = u.Quantity(1, u.m)\n\n        converter_err_msg = (\n            \"only dimensionless scalar quantities can be converted to Python scalars\"\n        )\n        index_err_msg = (\n            \"only integer dimensionless scalar quantities \"\n            \"can be converted to a Python index\"\n        )\n        with pytest.raises(TypeError) as exc:\n            float(q1)\n        assert exc.value.args[0] == converter_err_msg\n\n        with pytest.raises(TypeError) as exc:\n            int(q1)\n        assert exc.value.args[0] == converter_err_msg\n\n        # We used to test `q1 * ['a', 'b', 'c'] here, but that that worked\n        # at all was a really odd confluence of bugs.  Since it doesn't work\n        # in numpy >=1.10 any more, just go directly for `__index__` (which\n        # makes the test more similar to the `int`, `long`, etc., tests).\n        with pytest.raises(TypeError) as exc:\n            q1.__index__()\n        assert exc.value.args[0] == index_err_msg\n\n        # dimensionless but scaled is OK, however\n        q2 = u.Quantity(1.23, u.m / u.km)\n\n        assert float(q2) == float(q2.to_value(u.dimensionless_unscaled))\n        assert int(q2) == int(q2.to_value(u.dimensionless_unscaled))\n\n        with pytest.raises(TypeError) as exc:\n            q2.__index__()\n        assert exc.value.args[0] == index_err_msg\n\n        # dimensionless unscaled is OK, though for index needs to be int\n        q3 = u.Quantity(1.23, u.dimensionless_unscaled)\n\n        assert float(q3) == 1.23\n        assert int(q3) == 1\n\n        with pytest.raises(TypeError) as exc:\n            q3.__index__()\n        assert exc.value.args[0] == index_err_msg\n\n        # integer dimensionless unscaled is good for all\n        q4 = u.Quantity(2, u.dimensionless_unscaled, dtype=int)\n\n        assert float(q4) == 2.0\n        assert int(q4) == 2\n\n        assert q4.__index__() == 2\n\n        # but arrays are not OK\n        q5 = u.Quantity([1, 2], u.m)\n        with pytest.raises(TypeError) as exc:\n            float(q5)\n        assert exc.value.args[0] == converter_err_msg\n\n        with pytest.raises(TypeError) as exc:\n            int(q5)\n        assert exc.value.args[0] == converter_err_msg\n\n        with pytest.raises(TypeError) as exc:\n            q5.__index__()\n        assert exc.value.args[0] == index_err_msg\n", "type": "function"}, {"name": "__int__", "is_method": true, "class_name": "Quantity", "parameters": ["self"], "calls": ["int", "self.to_value", "TypeError"], "code_location": {"file": "quantity.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units", "start_line": 1334, "end_line": 1341}, "code_snippet": "    def __int__(self):\n        try:\n            return int(self.to_value(dimensionless_unscaled))\n        except (UnitsError, TypeError):\n            raise TypeError(\n                \"only dimensionless scalar quantities can be \"\n                \"converted to Python scalars\"\n            )\n", "type": "function"}, {"name": "_quantity_out_as_array", "is_method": false, "class_name": null, "parameters": ["out"], "calls": ["isinstance", "out.view"], "code_location": {"file": "function_helpers.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 261, "end_line": 269}, "code_snippet": "def _quantity_out_as_array(out):\n    from astropy.units import Quantity\n\n    if isinstance(out, Quantity):\n        return out.view(np.ndarray)\n    else:\n        # TODO: for an ndarray output, one could in principle\n        # try converting the input to dimensionless.\n        raise NotImplementedError\n", "type": "function"}, {"name": "__float__", "is_method": true, "class_name": "Quantity", "parameters": ["self"], "calls": ["float", "self.to_value", "TypeError"], "code_location": {"file": "quantity.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units", "start_line": 1325, "end_line": 1332}, "code_snippet": "    def __float__(self):\n        try:\n            return float(self.to_value(dimensionless_unscaled))\n        except (UnitsError, TypeError):\n            raise TypeError(\n                \"only dimensionless scalar quantities can be \"\n                \"converted to Python scalars\"\n            )\n", "type": "function"}, {"name": "__array_ufunc__", "is_method": true, "class_name": "SpectralQuantity", "parameters": ["self", "function", "method"], "calls": ["__array_ufunc__", "result.view", "result.__array_finalize__", "super", "TypeError", "result.view", "result.view"], "code_location": {"file": "spectral_quantity.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates", "start_line": 85, "end_line": 110}, "code_snippet": "    def __array_ufunc__(self, function, method, *inputs, **kwargs):\n        # We always return Quantity except in a few specific cases\n        result = super().__array_ufunc__(function, method, *inputs, **kwargs)\n        if (\n            (\n                function is np.multiply\n                or (function is np.true_divide and inputs[0] is self)\n            )\n            and result.unit == self.unit\n        ) or (\n            function in (np.minimum, np.maximum, np.fmax, np.fmin)\n            and method in (\"reduce\", \"reduceat\")\n        ):\n            result = result.view(self.__class__)\n            result.__array_finalize__(self)\n        else:\n            if result is self:\n                raise TypeError(\n                    \"Cannot store the result of this operation in\"\n                    f\" {self.__class__.__name__}\"\n                )\n            if result.dtype.kind == \"b\":\n                result = result.view(np.ndarray)\n            else:\n                result = result.view(Quantity)\n        return result\n", "type": "function"}, {"name": "__index__", "is_method": true, "class_name": "Quantity", "parameters": ["self"], "calls": ["self.unit.is_unity", "TypeError", "self.value.__index__"], "code_location": {"file": "quantity.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units", "start_line": 1346, "end_line": 1358}, "code_snippet": "    def __index__(self):\n        # for indices, we do not want to mess around with scaling at all,\n        # so unlike for float, int, we insist here on unscaled dimensionless\n        if self.unit.is_unity():\n            try:\n                return self.value.__index__()\n            except AttributeError:\n                pass\n\n        raise TypeError(\n            \"only integer dimensionless scalar quantities \"\n            \"can be converted to a Python index\"\n        )\n", "type": "function"}, {"name": "test_distr_angle", "is_method": false, "class_name": null, "parameters": [], "calls": ["Distribution", "Angle", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "assert_array_equal", "isinstance", "pytest.raises"], "code_location": {"file": "test_distribution.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/uncertainty/tests", "start_line": 403, "end_line": 422}, "code_snippet": "def test_distr_angle():\n    # Check that Quantity subclasses decay to Quantity appropriately.\n    distr = Distribution([2.0, 3.0, 4.0])\n    ad = Angle(distr, \"deg\")\n    ad_plus_ad = ad + ad\n    assert isinstance(ad_plus_ad, Angle)\n    assert isinstance(ad_plus_ad, Distribution)\n\n    ad_times_ad = ad * ad\n    assert not isinstance(ad_times_ad, Angle)\n    assert isinstance(ad_times_ad, u.Quantity)\n    assert isinstance(ad_times_ad, Distribution)\n\n    ad += ad\n    assert isinstance(ad, Angle)\n    assert isinstance(ad, Distribution)\n    assert_array_equal(ad.distribution, ad_plus_ad.distribution)\n\n    with pytest.raises(u.UnitTypeError):\n        ad *= ad\n", "type": "function"}, {"name": "__quantity_subclass__", "is_method": true, "class_name": "Constant", "parameters": ["self", "unit"], "calls": ["__quantity_subclass__", "super"], "code_location": {"file": "constant.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/constants", "start_line": 181, "end_line": 182}, "code_snippet": "    def __quantity_subclass__(self, unit):\n        return super().__quantity_subclass__(unit)[0], False\n", "type": "function"}, {"name": "test_array_converters", "is_method": true, "class_name": "TestQuantityOperations", "parameters": ["self"], "calls": ["u.Quantity", "np.all", "u.Quantity", "np.all", "np.array", "np.array", "np.array", "np.array"], "code_location": {"file": "test_quantity.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 712, "end_line": 719}, "code_snippet": "    def test_array_converters(self):\n        # Scalar quantity\n        q = u.Quantity(1.23, u.m)\n        assert np.all(np.array(q) == np.array([1.23]))\n\n        # Array quantity\n        q = u.Quantity([1.0, 2.0, 3.0], u.m)\n        assert np.all(np.array(q) == np.array([1.0, 2.0, 3.0]))\n", "type": "function"}, {"name": "__array_ufunc__", "is_method": true, "class_name": "Latitude", "parameters": ["self"], "calls": ["__array_ufunc__", "_no_angle_subclass", "super"], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/angles", "start_line": 647, "end_line": 649}, "code_snippet": "    def __array_ufunc__(self, *args, **kwargs):\n        results = super().__array_ufunc__(*args, **kwargs)\n        return _no_angle_subclass(results)\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.637615442276001}
{"question": "How does the unit validation decorator that validates function argument and return value units in the astropy units module handle return type annotation when explicitly set to None in the decorated function signature?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_return_annotation_none", "is_method": false, "class_name": null, "parameters": [], "calls": ["myfunc_args"], "code_location": {"file": "test_quantity_annotations.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 324, "end_line": 330}, "code_snippet": "def test_return_annotation_none():\n    @u.quantity_input\n    def myfunc_args(solarx: u.arcsec) -> None:\n        pass\n\n    solarx = myfunc_args(1 * u.arcsec)\n    assert solarx is None\n", "type": "function"}, {"name": "as_decorator", "is_method": true, "class_name": "QuantityInput", "parameters": ["cls", "func"], "calls": ["cls", "self"], "code_location": {"file": "decorators.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units", "start_line": 136, "end_line": 210}, "code_snippet": "    def as_decorator(cls, func=None, **kwargs):\n        r\"\"\"\n        A decorator for validating the units of arguments to functions.\n\n        Unit specifications can be provided as keyword arguments to the\n        decorator, or by using function annotation syntax. Arguments to the\n        decorator take precedence over any function annotations present.\n\n        A `~astropy.units.UnitsError` will be raised if the unit attribute of\n        the argument is not equivalent to the unit specified to the decorator or\n        in the annotation. If the argument has no unit attribute, i.e. it is not\n        a Quantity object, a `ValueError` will be raised unless the argument is\n        an annotation. This is to allow non Quantity annotations to pass\n        through.\n\n        Where an equivalency is specified in the decorator, the function will be\n        executed with that equivalency in force.\n\n        Notes\n        -----\n        The checking of arguments inside variable arguments to a function is not\n        supported (i.e. \\*arg or \\**kwargs).\n\n        The original function is accessible by the attributed ``__wrapped__``.\n        See :func:`functools.wraps` for details.\n\n        Examples\n        --------\n        .. code-block:: python\n\n            import astropy.units as u\n            @u.quantity_input(myangle=u.arcsec)\n            def myfunction(myangle):\n                return myangle**2\n\n\n        .. code-block:: python\n\n            import astropy.units as u\n            @u.quantity_input\n            def myfunction(myangle: u.arcsec):\n                return myangle**2\n\n        Or using a unit-aware Quantity annotation.\n\n        .. code-block:: python\n\n            @u.quantity_input\n            def myfunction(myangle: u.Quantity[u.arcsec]):\n                return myangle**2\n\n        Also you can specify a return value annotation, which will\n        cause the function to always return a `~astropy.units.Quantity` in that\n        unit.\n\n        .. code-block:: python\n\n            import astropy.units as u\n            @u.quantity_input\n            def myfunction(myangle: u.arcsec) -> u.deg**2:\n                return myangle**2\n\n        Using equivalencies::\n\n            import astropy.units as u\n            @u.quantity_input(myenergy=u.eV, equivalencies=u.mass_energy())\n            def myfunction(myenergy):\n                return myenergy**2\n\n        \"\"\"\n        self = cls(**kwargs)\n        if func is not None and not kwargs:\n            return self(func)\n        else:\n            return self\n", "type": "function"}, {"name": "test_return_annotation_notUnit", "is_method": false, "class_name": null, "parameters": [], "calls": ["myfunc_args"], "code_location": {"file": "test_quantity_annotations.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 333, "end_line": 339}, "code_snippet": "def test_return_annotation_notUnit():\n    @u.quantity_input\n    def myfunc_args(solarx: u.arcsec) -> int:\n        return 0\n\n    solarx = myfunc_args(1 * u.arcsec)\n    assert solarx == 0\n", "type": "function"}, {"name": "test_return_annotation", "is_method": false, "class_name": null, "parameters": [], "calls": ["myfunc_args"], "code_location": {"file": "test_quantity_annotations.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 315, "end_line": 321}, "code_snippet": "def test_return_annotation():\n    @u.quantity_input\n    def myfunc_args(solarx: u.arcsec) -> u.deg:\n        return solarx\n\n    solarx = myfunc_args(1 * u.arcsec)\n    assert solarx.unit is u.deg\n", "type": "function"}, {"name": "__call__", "is_method": true, "class_name": "QuantityInput", "parameters": ["self", "wrapped_function"], "calls": ["inspect.signature", "wraps", "wrapped_signature.bind", "wrapped_signature.parameters.values", "_validate_arg_value", "add_enabled_equivalencies", "contextlib.nullcontext", "wrapped_function", "_validate_arg_value", "_parse_annotation", "isinstance", "_parse_annotation", "isinstance", "len", "isinstance", "T.get_origin", "isinstance", "isinstance", "isinstance"], "code_location": {"file": "decorators.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units", "start_line": 217, "end_line": 344}, "code_snippet": "    def __call__(self, wrapped_function):\n        # Extract the function signature for the function we are wrapping.\n        wrapped_signature = inspect.signature(wrapped_function)\n\n        # Define a new function to return in place of the wrapped one\n        @wraps(wrapped_function)\n        def wrapper(*func_args, **func_kwargs):\n            # Bind the arguments to our new function to the signature of the original.\n            bound_args = wrapped_signature.bind(*func_args, **func_kwargs)\n\n            # Iterate through the parameters of the original signature\n            for param in wrapped_signature.parameters.values():\n                # We do not support variable arguments (*args, **kwargs)\n                if param.kind in (\n                    inspect.Parameter.VAR_KEYWORD,\n                    inspect.Parameter.VAR_POSITIONAL,\n                ):\n                    continue\n\n                # Catch the (never triggered) case where bind relied on a default value.\n                if (\n                    param.name not in bound_args.arguments\n                    and param.default is not param.empty\n                ):\n                    bound_args.arguments[param.name] = param.default\n\n                # Get the value of this parameter (argument to new function)\n                arg = bound_args.arguments[param.name]\n\n                # Get target unit or physical type, either from decorator kwargs\n                #   or annotations\n                if param.name in self.decorator_kwargs:\n                    targets = self.decorator_kwargs[param.name]\n                    is_annotation = False\n                else:\n                    targets = param.annotation\n                    is_annotation = True\n\n                    # parses to unit if it's an annotation (or list thereof)\n                    targets = _parse_annotation(targets)\n\n                # If the targets is empty, then no target units or physical\n                #   types were specified so we can continue to the next arg\n                if targets is inspect.Parameter.empty:\n                    continue\n\n                # If the argument value is None, and the default value is None,\n                #   pass through the None even if there is a target unit\n                if arg is None and param.default is None:\n                    continue\n\n                # Here, we check whether multiple target unit/physical type's\n                #   were specified in the decorator/annotation, or whether a\n                #   single string (unit or physical type) or a Unit object was\n                #   specified\n                if isinstance(targets, str) or not isinstance(targets, Sequence):\n                    valid_targets = [targets]\n\n                # Check for None in the supplied list of allowed units and, if\n                #   present and the passed value is also None, ignore.\n                elif None in targets or NoneType in targets:\n                    if arg is None:\n                        continue\n                    else:\n                        valid_targets = [t for t in targets if t is not None]\n\n                else:\n                    valid_targets = targets\n\n                # If we're dealing with an annotation, skip all the targets that\n                #    are not strings or subclasses of Unit. This is to allow\n                #    non unit related annotations to pass through\n                if is_annotation:\n                    valid_targets = [\n                        t\n                        for t in valid_targets\n                        if isinstance(t, (str, UnitBase, PhysicalType))\n                    ]\n\n                # Now we loop over the allowed units/physical types and validate\n                #   the value of the argument:\n                _validate_arg_value(\n                    param.name,\n                    wrapped_function.__name__,\n                    arg,\n                    valid_targets,\n                    self.equivalencies,\n                    self.strict_dimensionless,\n                )\n\n            if self.equivalencies:\n                equiv_context = add_enabled_equivalencies(self.equivalencies)\n            else:\n                # Avoid creating a duplicate registry if we don't have\n                # equivalencies to add. (If we're wrapping a short function,\n                # the time spent duplicating the registry is quite noticeable.)\n                equiv_context = contextlib.nullcontext()\n            # Call the original function with any equivalencies in force.\n            with equiv_context:\n                return_ = wrapped_function(*func_args, **func_kwargs)\n\n            # Return\n            ra = wrapped_signature.return_annotation\n            valid_empty = (inspect.Signature.empty, None, NoneType, T.NoReturn)\n            if ra not in valid_empty:\n                target = (\n                    ra\n                    if T.get_origin(ra) not in (T.Annotated, T.Union)\n                    else _parse_annotation(ra)\n                )\n                if isinstance(target, str) or not isinstance(target, Sequence):\n                    target = [target]\n                valid_targets = [\n                    t for t in target if isinstance(t, (str, UnitBase, PhysicalType))\n                ]\n                _validate_arg_value(\n                    \"return\",\n                    wrapped_function.__name__,\n                    return_,\n                    valid_targets,\n                    self.equivalencies,\n                    self.strict_dimensionless,\n                )\n                if len(valid_targets) > 0:\n                    return_ <<= valid_targets[0]\n            return return_\n\n        return wrapper\n", "type": "function"}, {"name": "test_not_quantity_annotated", "is_method": false, "class_name": null, "parameters": ["x_input", "y_input"], "calls": ["pytest.raises", "myfunc_args"], "code_location": {"file": "test_quantity_decorator.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 128, "end_line": 143}, "code_snippet": "def test_not_quantity_annotated(x_input, y_input):\n    x_target, x_unit = x_input\n    y_target, y_unit = y_input\n\n    @u.quantity_input\n    def myfunc_args(x: x_target, y: y_target):\n        return x, y\n\n    with pytest.raises(\n        TypeError,\n        match=(\n            \"Argument 'y' to function 'myfunc_args' has no 'unit' attribute. \"\n            \"You should pass in an astropy Quantity instead.\"\n        ),\n    ):\n        x, y = myfunc_args(1 * x_unit, 100)\n", "type": "function"}, {"name": "test_optional_and_annotated", "is_method": true, "class_name": "TestQuantityUnitAnnotations", "parameters": ["self"], "calls": ["opt_func", "opt_func"], "code_location": {"file": "test_quantity_annotations.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 58, "end_line": 72}, "code_snippet": "    def test_optional_and_annotated(self):\n        @u.quantity_input\n        def opt_func(x: Quantity[u.m] | None = None) -> Quantity[u.km]:\n            if x is None:\n                return 1 * u.km\n            return x\n\n        i_q = 250 * u.m\n        o_q = opt_func(i_q)\n        assert o_q.unit == u.km\n        assert o_q == i_q\n\n        i_q = None\n        o_q = opt_func(i_q)\n        assert o_q == 1 * u.km\n", "type": "function"}, {"name": "test_args_None", "is_method": false, "class_name": null, "parameters": [], "calls": ["u.quantity_input", "myfunc_args", "isinstance", "myfunc_args", "isinstance"], "code_location": {"file": "test_quantity_decorator.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 376, "end_line": 394}, "code_snippet": "def test_args_None():\n    x_target = u.deg\n    x_unit = u.arcsec\n    y_target = u.km\n    y_unit = u.kpc\n\n    @u.quantity_input(x=[x_target, None], y=[None, y_target])\n    def myfunc_args(x, y):\n        return x, y\n\n    x, y = myfunc_args(1 * x_unit, None)\n    assert isinstance(x, u.Quantity)\n    assert x.unit == x_unit\n    assert y is None\n\n    x, y = myfunc_args(None, 1 * y_unit)\n    assert isinstance(y, u.Quantity)\n    assert y.unit == y_unit\n    assert x is None\n", "type": "function"}, {"name": "test_args_None_kwarg", "is_method": false, "class_name": null, "parameters": [], "calls": ["u.quantity_input", "myfunc_args", "isinstance", "myfunc_args", "isinstance", "pytest.raises", "myfunc_args"], "code_location": {"file": "test_quantity_decorator.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 397, "end_line": 417}, "code_snippet": "def test_args_None_kwarg():\n    x_target = u.deg\n    x_unit = u.arcsec\n    y_target = u.km\n\n    @u.quantity_input(x=x_target, y=y_target)\n    def myfunc_args(x, y=None):\n        return x, y\n\n    x, y = myfunc_args(1 * x_unit)\n    assert isinstance(x, u.Quantity)\n    assert x.unit == x_unit\n    assert y is None\n\n    x, y = myfunc_args(1 * x_unit, None)\n    assert isinstance(x, u.Quantity)\n    assert x.unit == x_unit\n    assert y is None\n\n    with pytest.raises(TypeError):\n        x, y = myfunc_args(None, None)\n", "type": "function"}, {"name": "_parse_annotation", "is_method": false, "class_name": null, "parameters": ["target"], "calls": ["T.get_origin", "T.get_args", "Unit", "isinstance", "_parse_annotation", "issubclass", "get_physical_type", "T.get_args", "isinstance", "ValueError"], "code_location": {"file": "decorators.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units", "start_line": 96, "end_line": 131}, "code_snippet": "def _parse_annotation(target):\n    if target in (None, NoneType, inspect._empty):\n        return target\n\n    # check if unit-like\n    try:\n        unit = Unit(target)\n    except (TypeError, ValueError):\n        try:\n            ptype = get_physical_type(target)\n        except (TypeError, ValueError, KeyError):  # KeyError for Enum\n            if isinstance(target, str):\n                raise ValueError(f\"invalid unit or physical type {target!r}.\") from None\n        else:\n            return ptype\n    else:\n        return unit\n\n    # could be a type hint\n    origin = T.get_origin(target)\n    if origin is T.Union:\n        return [_parse_annotation(t) for t in T.get_args(target)]\n    elif origin is not T.Annotated:  # can't be Quantity[]\n        return False\n\n    # parse type hint\n    cls, *annotations = T.get_args(target)\n    if not issubclass(cls, Quantity) or not annotations:\n        return False\n\n    # get unit from type hint\n    unit, *rest = annotations\n    if not isinstance(unit, (UnitBase, PhysicalType)):\n        return False\n\n    return unit\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.6767754554748535}
{"question": "How does the mixin class that adds input/output methods to N-dimensional data containers ensure extensibility of the centralized file format registry without breaking existing subclasses of the base data container when new format handlers are registered?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "__init__", "is_method": true, "class_name": "MetaBaseReader", "parameters": ["cls", "name", "bases", "dct"], "calls": ["__init__", "dct.get", "dct.get", "dct.get", "dct.get", "functools.partial", "connect.io_registry.register_identifier", "functools.partial", "strip", "connect.io_registry.register_reader", "dct.get", "super", "functools.partial", "strip", "connect.io_registry.register_writer", "re.sub", "inspect.cleandoc", "re.sub", "inspect.cleandoc", "strip", "strip", "inspect.cleandoc", "inspect.cleandoc"], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/ascii", "start_line": 1217, "end_line": 1261}, "code_snippet": "    def __init__(cls, name, bases, dct):\n        super().__init__(name, bases, dct)\n\n        format = dct.get(\"_format_name\")\n        if format is None:\n            return\n\n        fast = dct.get(\"_fast\")\n        if fast is not None:\n            FAST_CLASSES[format] = cls\n\n        FORMAT_CLASSES[format] = cls\n\n        io_formats = [\"ascii.\" + format] + dct.get(\"_io_registry_format_aliases\", [])\n\n        if dct.get(\"_io_registry_suffix\"):\n            func = functools.partial(connect.io_identify, dct[\"_io_registry_suffix\"])\n            connect.io_registry.register_identifier(io_formats[0], Table, func)\n\n        for io_format in io_formats:\n            func = functools.partial(connect.io_read, io_format)\n            header = f\"ASCII reader '{io_format}' details\\n\"\n            func.__doc__ = (\n                inspect.cleandoc(READ_DOCSTRING).strip()\n                + \"\\n\\n\"\n                + header\n                + re.sub(\".\", \"=\", header)\n                + \"\\n\"\n            )\n            # NOTE: cls.__doc__ is None for -OO flag\n            func.__doc__ += inspect.cleandoc(cls.__doc__ or \"\").strip()\n            connect.io_registry.register_reader(io_format, Table, func)\n\n            if dct.get(\"_io_registry_can_write\", True):\n                func = functools.partial(connect.io_write, io_format)\n                header = f\"ASCII writer '{io_format}' details\\n\"\n                func.__doc__ = (\n                    inspect.cleandoc(WRITE_DOCSTRING).strip()\n                    + \"\\n\\n\"\n                    + header\n                    + re.sub(\".\", \"=\", header)\n                    + \"\\n\"\n                )\n                func.__doc__ += inspect.cleandoc(cls.__doc__ or \"\").strip()\n                connect.io_registry.register_writer(io_format, Table, func)\n", "type": "function"}, {"name": "_UnifiedIORegistryBase", "docstring": "Base class for registries in Astropy's Unified IO.\n\nThis base class provides identification functions and miscellaneous\nutilities. For an example how to build a registry subclass we suggest\n:class:`~astropy.io.registry.UnifiedInputRegistry`, which enables\nread-only registries. These higher-level subclasses will probably serve\nbetter as a baseclass, for instance\n:class:`~astropy.io.registry.UnifiedIORegistry` subclasses both\n:class:`~astropy.io.registry.UnifiedInputRegistry` and\n:class:`~astropy.io.registry.UnifiedOutputRegistry` to enable both\nreading from and writing to files.\n\n.. versionadded:: 5.0", "methods": ["__init__", "available_registries", "get_formats", "delay_doc_updates", "register_identifier", "unregister_identifier", "identify_format", "_get_format_table_str", "_is_best_match", "_get_valid_format", "_get_highest_priority_format", "_update__doc__"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/registry", "start_line": 20, "end_line": 473}, "type": "class"}, {"name": "UnifiedInputRegistry", "docstring": "Read-only Unified Registry.\n\n.. versionadded:: 5.0\n\nExamples\n--------\nFirst let's start by creating a read-only registry.\n\n.. code-block:: python\n\n    >>> from astropy.io.registry import UnifiedInputRegistry\n    >>> read_reg = UnifiedInputRegistry()\n\nThere is nothing in this registry. Let's make a reader for the\n:class:`~astropy.table.Table` class::\n\n    from astropy.table import Table\n\n    def my_table_reader(filename, some_option=1):\n        # Read in the table by any means necessary\n        return table  # should be an instance of Table\n\nSuch a function can then be registered with the I/O registry::\n\n    read_reg.register_reader('my-table-format', Table, my_table_reader)\n\nNote that we CANNOT then read in a table with::\n\n    d = Table.read('my_table_file.mtf', format='my-table-format')\n\nWhy? because ``Table.read`` uses Astropy's default global registry and this\nis a separate registry.\nInstead we can read by the read method on the registry::\n\n    d = read_reg.read(Table, 'my_table_file.mtf', format='my-table-format')", "methods": ["__init__", "register_reader", "unregister_reader", "get_reader", "read"], "attributes": [], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/registry", "start_line": 27, "end_line": 237}, "type": "class"}, {"name": "UnifiedIORegistry", "docstring": "Unified I/O Registry.\n\n.. versionadded:: 5.0", "methods": ["__init__", "get_formats"], "attributes": [], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/registry", "start_line": 392, "end_line": 422}, "type": "class"}, {"name": "__init_subclass__", "is_method": true, "class_name": "Base", "parameters": ["cls"], "calls": ["__init_subclass__", "cls.__name__.lower", "super"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/format", "start_line": 39, "end_line": 46}, "code_snippet": "    def __init_subclass__(cls, **kwargs):\n        # Keep a registry of all formats.  Key by the class name unless a name\n        # is explicitly set (i.e., one *not* inherited from a superclass).\n        if \"name\" not in cls.__dict__:\n            cls.name = cls.__name__.lower()\n\n        Base.registry[cls.name] = cls\n        super().__init_subclass__(**kwargs)\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "_UnifiedIORegistryBase", "parameters": ["self"], "calls": ["set"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/registry", "start_line": 37, "end_line": 52}, "code_snippet": "    def __init__(self):\n        # registry of identifier functions\n        self._identifiers = {}\n\n        # what this class can do: e.g. 'read' &/or 'write'\n        self._registries = {}\n        self._registries[\"identify\"] = {\n            \"attr\": \"_identifiers\",\n            \"column\": \"Auto-identify\",\n        }\n        self._registries_order = (\"identify\",)  # match keys in `_registries`\n\n        # If multiple formats are added to one class the update of the docs is quite\n        # expensive. Classes for which the doc update is temporarily delayed are added\n        # to this set.\n        self._delayed_docs_classes = set()\n", "type": "function"}, {"name": "UnifiedIORegistryBaseSubClass", "docstring": "Non-abstract subclass of UnifiedIORegistryBase for testing.", "methods": ["get_formats"], "attributes": [], "code_location": {"file": "test_registries.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/registry/tests", "start_line": 37, "end_line": 41}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "UnifiedReadWrite", "parameters": ["self", "instance", "cls", "method_name", "registry"], "calls": [], "code_location": {"file": "interface.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/registry", "start_line": 42, "end_line": 49}, "code_snippet": "    def __init__(self, instance, cls, method_name, registry=None):\n        if registry is None:\n            from astropy.io.registry.compat import default_registry as registry\n\n        self._registry = registry\n        self._instance = instance\n        self._cls = cls\n        self._method_name = method_name  # 'read' or 'write'\n", "type": "function"}, {"name": "register_hdu", "is_method": true, "class_name": "_BaseHDU", "parameters": ["cls", "hducls"], "calls": ["cls._hdu_registry.add"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/hdu", "start_line": 282, "end_line": 283}, "code_snippet": "    def register_hdu(cls, hducls):\n        cls._hdu_registry.add(hducls)\n", "type": "function"}, {"name": "test_identifier_origin", "is_method": true, "class_name": "TestUnifiedIORegistry", "parameters": ["self", "registry", "fmtcls1", "fmtcls2"], "calls": ["registry.register_identifier", "registry.register_identifier", "registry.register_reader", "registry.register_writer", "cls.read", "write", "startswith", "startswith", "pytest.raises", "cls.read", "pytest.raises", "write", "cls", "str", "str", "cls"], "code_location": {"file": "test_registries.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/registry/tests", "start_line": 1016, "end_line": 1039}, "code_snippet": "    def test_identifier_origin(self, registry, fmtcls1, fmtcls2):\n        fmt1, cls = fmtcls1\n        fmt2, _ = fmtcls2\n\n        registry.register_identifier(fmt1, cls, lambda o, *x, **y: o == \"read\")\n        registry.register_identifier(fmt2, cls, lambda o, *x, **y: o == \"write\")\n        registry.register_reader(fmt1, cls, empty_reader)\n        registry.register_writer(fmt2, cls, empty_writer)\n\n        # There should not be too many formats defined\n        cls.read(registry=registry)\n        cls().write(registry=registry)\n\n        with pytest.raises(IORegistryError) as exc:\n            cls.read(format=fmt2, registry=registry)\n        assert str(exc.value).startswith(\n            f\"No reader defined for format '{fmt2}' and class '{cls.__name__}'\"\n        )\n\n        with pytest.raises(IORegistryError) as exc:\n            cls().write(format=fmt1, registry=registry)\n        assert str(exc.value).startswith(\n            f\"No writer defined for format '{fmt1}' and class '{cls.__name__}'\"\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.6897218227386475}
{"question": "Why is the test function in the console utilities test module that verifies terminal color output without assertions implemented as a smoke test rather than a comprehensive unit test?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_color_print", "is_method": false, "class_name": null, "parameters": [], "calls": ["console.color_print", "console.color_print"], "code_location": {"file": "test_console.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 67, "end_line": 71}, "code_snippet": "def test_color_print():\n    # This stuff is hard to test, at least smoke test it\n    console.color_print(\"foo\", \"green\")\n\n    console.color_print(\"foo\", \"green\", \"bar\", \"red\")\n", "type": "function"}, {"name": "test_color_text", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.mark.skipif", "sys.platform.startswith", "console._color_text"], "code_location": {"file": "test_console.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 63, "end_line": 64}, "code_snippet": "def test_color_text():\n    assert console._color_text(\"foo\", \"green\") == \"\\033[0;32mfoo\\033[0m\"\n", "type": "function"}, {"name": "test_color_print2", "is_method": false, "class_name": null, "parameters": [], "calls": ["io.StringIO", "console.color_print", "io.StringIO", "console.color_print", "stream.getvalue", "stream.getvalue"], "code_location": {"file": "test_console.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 74, "end_line": 83}, "code_snippet": "def test_color_print2():\n    # Test that this automatically detects that io.StringIO is\n    # not a tty\n    stream = io.StringIO()\n    console.color_print(\"foo\", \"green\", file=stream)\n    assert stream.getvalue() == \"foo\\n\"\n\n    stream = io.StringIO()\n    console.color_print(\"foo\", \"green\", \"bar\", \"red\", \"baz\", file=stream)\n    assert stream.getvalue() == \"foobarbaz\\n\"\n", "type": "function"}, {"name": "test_color_print_invalid_color", "is_method": false, "class_name": null, "parameters": [], "calls": ["console.color_print"], "code_location": {"file": "test_console.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 103, "end_line": 104}, "code_snippet": "def test_color_print_invalid_color():\n    console.color_print(\"foo\", \"unknown\")\n", "type": "function"}, {"name": "test_color_print3", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.mark.skipif", "FakeTTY", "console.color_print", "FakeTTY", "console.color_print", "sys.platform.startswith", "stream.getvalue", "stream.getvalue"], "code_location": {"file": "test_console.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 87, "end_line": 96}, "code_snippet": "def test_color_print3():\n    # Test that this thinks the FakeTTY is a tty and applies colors.\n\n    stream = FakeTTY()\n    console.color_print(\"foo\", \"green\", file=stream)\n    assert stream.getvalue() == \"\\x1b[0;32mfoo\\x1b[0m\\n\"\n\n    stream = FakeTTY()\n    console.color_print(\"foo\", \"green\", \"bar\", \"red\", \"baz\", file=stream)\n    assert stream.getvalue() == \"\\x1b[0;32mfoo\\x1b[0m\\x1b[0;31mbar\\x1b[0mbaz\\n\"\n", "type": "function"}, {"name": "test_color_print_unicode", "is_method": false, "class_name": null, "parameters": [], "calls": ["console.color_print"], "code_location": {"file": "test_console.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 99, "end_line": 100}, "code_snippet": "def test_color_print_unicode():\n    console.color_print(\"berbr\", \"red\")\n", "type": "function"}, {"name": "test_progress_bar", "is_method": false, "class_name": null, "parameters": [], "calls": ["console.ProgressBar", "range", "bar.update"], "code_location": {"file": "test_console.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 122, "end_line": 126}, "code_snippet": "def test_progress_bar():\n    # This stuff is hard to test, at least smoke test it\n    with console.ProgressBar(50) as bar:\n        for _ in range(50):\n            bar.update()\n", "type": "function"}, {"name": "color_print", "is_method": false, "class_name": null, "parameters": [], "calls": ["kwargs.get", "isatty", "range", "write", "range", "write", "len", "_write_with_fallback", "len", "write", "len", "_color_text"], "code_location": {"file": "console.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils", "start_line": 226, "end_line": 281}, "code_snippet": "def color_print(*args, end=\"\\n\", **kwargs):\n    \"\"\"\n    Prints colors and styles to the terminal uses ANSI escape\n    sequences.\n\n    ::\n\n       color_print('This is the color ', 'default', 'GREEN', 'green')\n\n    Parameters\n    ----------\n    positional args : str\n        The positional arguments come in pairs (*msg*, *color*), where\n        *msg* is the string to display and *color* is the color to\n        display it in.\n\n        *color* is an ANSI terminal color name.  Must be one of:\n        black, red, green, brown, blue, magenta, cyan, lightgrey,\n        default, darkgrey, lightred, lightgreen, yellow, lightblue,\n        lightmagenta, lightcyan, white, or '' (the empty string).\n\n    file : :term:`file-like (writeable)`, optional\n        Where to write to.  Defaults to `sys.stdout`.  If file is not\n        a tty (as determined by calling its `isatty` member, if one\n        exists), no coloring will be included.\n\n    end : str, optional\n        The ending of the message.  Defaults to ``\\\\n``.  The end will\n        be printed after resetting any color or font state.\n    \"\"\"\n    file = kwargs.get(\"file\", sys.stdout)\n\n    write = file.write\n    if isatty(file) and conf.use_color:\n        for i in range(0, len(args), 2):\n            msg = args[i]\n            if i + 1 == len(args):\n                color = \"\"\n            else:\n                color = args[i + 1]\n\n            if color:\n                msg = _color_text(msg, color)\n\n            # Some file objects support writing unicode sensibly on some Python\n            # versions; if this fails try creating a writer using the locale's\n            # preferred encoding. If that fails too give up.\n\n            write = _write_with_fallback(msg, write, file)\n\n        write(end)\n    else:\n        for i in range(0, len(args), 2):\n            msg = args[i]\n            write(msg)\n        write(end)\n", "type": "function"}, {"name": "test_fake_tty", "is_method": false, "class_name": null, "parameters": [], "calls": ["FakeTTY", "f1.isatty", "f1.write", "FakeTTY", "f2.isatty", "pytest.raises", "f1.getvalue", "f2.getvalue"], "code_location": {"file": "test_console.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 45, "end_line": 59}, "code_snippet": "def test_fake_tty():\n    # First test without a specified encoding; we should be able to write\n    # arbitrary unicode strings\n    f1 = FakeTTY()\n    assert f1.isatty()\n    f1.write(\"\")\n    assert f1.getvalue() == \"\"\n\n    # Now test an ASCII-only TTY--it should raise a UnicodeEncodeError when\n    # trying to write a string containing non-ASCII characters\n    f2 = FakeTTY(\"ascii\")\n    assert f2.isatty()\n    assert f2.__class__.__name__ == \"AsciiFakeTTY\"\n    assert pytest.raises(UnicodeEncodeError, f2.write, \"\")\n    assert f2.getvalue() == \"\"\n", "type": "function"}, {"name": "test_progress_bar2", "is_method": false, "class_name": null, "parameters": [], "calls": ["console.ProgressBar", "range"], "code_location": {"file": "test_console.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 129, "end_line": 131}, "code_snippet": "def test_progress_bar2():\n    for _ in console.ProgressBar(range(50)):\n        pass\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.7103443145751953}
{"question": "How does the standard deviation uncertainty class's property that references the associated n-dimensional data container instance handle access when there is no associated parent object?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "parent_nddata", "is_method": true, "class_name": "NDUncertainty", "parameters": ["self"], "calls": ["MissingDataAssociationException", "MissingDataAssociationException", "isinstance", "self._parent_nddata", "log.info", "log.info"], "code_location": {"file": "nduncertainty.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/nddata", "start_line": 214, "end_line": 247}, "code_snippet": "    def parent_nddata(self):\n        \"\"\"`NDData` : reference to `NDData` instance with this uncertainty.\n\n        In case the reference is not set uncertainty propagation will not be\n        possible since propagation might need the uncertain data besides the\n        uncertainty.\n        \"\"\"\n        no_parent_message = \"uncertainty is not associated with an NDData object\"\n        parent_lost_message = (\n            \"the associated NDData object was deleted and cannot be accessed \"\n            \"anymore. You can prevent the NDData object from being deleted by \"\n            \"assigning it to a variable. If this happened after unpickling \"\n            \"make sure you pickle the parent not the uncertainty directly.\"\n        )\n        try:\n            parent = self._parent_nddata\n        except AttributeError:\n            raise MissingDataAssociationException(no_parent_message)\n        else:\n            if parent is None:\n                raise MissingDataAssociationException(no_parent_message)\n            else:\n                # The NDData is saved as weak reference so we must call it\n                # to get the object the reference points to. However because\n                # we have a weak reference here it's possible that the parent\n                # was deleted because its reference count dropped to zero.\n                if isinstance(self._parent_nddata, weakref.ref):\n                    resolved_parent = self._parent_nddata()\n                    if resolved_parent is None:\n                        log.info(parent_lost_message)\n                    return resolved_parent\n                else:\n                    log.info(\"parent_nddata should be a weakref to an NDData object.\")\n                    return self._parent_nddata\n", "type": "function"}, {"name": "test_stddevuncertainty_compat_descriptor_no_parent", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.raises", "StdDevUncertainty", "np.ones"], "code_location": {"file": "test_ccddata.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/nddata/tests", "start_line": 1055, "end_line": 1057}, "code_snippet": "def test_stddevuncertainty_compat_descriptor_no_parent():\n    with pytest.raises(MissingDataAssociationException):\n        StdDevUncertainty(np.ones((10, 10))).parent_nddata\n", "type": "function"}, {"name": "parent_nddata", "is_method": true, "class_name": "NDUncertainty", "parameters": ["self", "value"], "calls": ["weakref.ref", "getattr", "isinstance", "self._data_unit_to_uncertainty_unit", "self._data_unit_to_uncertainty_unit", "self._data_unit_to_uncertainty_unit", "unit_from_data.to", "UnitConversionError"], "code_location": {"file": "nduncertainty.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/nddata", "start_line": 250, "end_line": 285}, "code_snippet": "    def parent_nddata(self, value):\n        if value is not None and not isinstance(value, weakref.ref):\n            # Save a weak reference on the uncertainty that points to this\n            # instance of NDData. Direct references should NOT be used:\n            # https://github.com/astropy/astropy/pull/4799#discussion_r61236832\n            value = weakref.ref(value)\n        # Set _parent_nddata here and access below with the property because value\n        # is a weakref\n        self._parent_nddata = value\n        # set uncertainty unit to that of the parent if it was not already set, unless initializing\n        # with empty parent (Value=None)\n        if value is not None:\n            parent_unit = self.parent_nddata.unit\n            # this will get the unit for masked quantity input:\n            parent_data_unit = getattr(self.parent_nddata.data, \"unit\", None)\n            if parent_unit is None and parent_data_unit is None:\n                self.unit = None\n            elif self.unit is None and parent_unit is not None:\n                # Set the uncertainty's unit to the appropriate value\n                self.unit = self._data_unit_to_uncertainty_unit(parent_unit)\n            elif parent_data_unit is not None:\n                # if the parent_nddata object has a unit, use it:\n                self.unit = self._data_unit_to_uncertainty_unit(parent_data_unit)\n            else:\n                # Check that units of uncertainty are compatible with those of\n                # the parent. If they are, no need to change units of the\n                # uncertainty or the data. If they are not, let the user know.\n                unit_from_data = self._data_unit_to_uncertainty_unit(parent_unit)\n                try:\n                    unit_from_data.to(self.unit)\n                except UnitConversionError:\n                    raise UnitConversionError(\n                        f\"Unit {self.unit} of uncertainty \"\n                        f\"incompatible with unit {parent_unit} of \"\n                        \"data\"\n                    )\n", "type": "function"}, {"name": "_parent", "is_method": true, "class_name": "DataInfo", "parameters": ["self"], "calls": ["self._parent_ref", "AttributeError"], "code_location": {"file": "data_info.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils", "start_line": 320, "end_line": 338}, "code_snippet": "    def _parent(self):\n        try:\n            parent = self._parent_ref()\n        except AttributeError:\n            return None\n\n        if parent is None:\n            raise AttributeError(\n                \"\"\"\\\nfailed to access \"info\" attribute on a temporary object.\n\nIt looks like you have done something like ``col[3:5].info`` or\n``col.quantity.info``, i.e.  you accessed ``info`` from a temporary slice\nobject that only exists momentarily.  This has failed because the reference to\nthat temporary object is now lost.  Instead force a permanent reference (e.g.\n``c = col[3:5]`` followed by ``c.info``).\"\"\"\n            )\n\n        return parent\n", "type": "function"}, {"name": "uncertainty", "is_method": true, "class_name": "CCDData", "parameters": ["self", "value"], "calls": ["isinstance", "isinstance", "getattr", "value.__class__", "StdDevUncertainty", "log.info", "TypeError", "ValueError"], "code_location": {"file": "ccddata.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/nddata", "start_line": 273, "end_line": 294}, "code_snippet": "    def uncertainty(self, value):\n        if value is not None:\n            if isinstance(value, NDUncertainty):\n                if getattr(value, \"_parent_nddata\", None) is not None:\n                    value = value.__class__(value, copy=False)\n                self._uncertainty = value\n            elif isinstance(value, np.ndarray):\n                if value.shape != self.shape:\n                    raise ValueError(\"uncertainty must have same shape as data.\")\n                self._uncertainty = StdDevUncertainty(value)\n                log.info(\n                    \"array provided for uncertainty; assuming it is a \"\n                    \"StdDevUncertainty.\"\n                )\n            else:\n                raise TypeError(\n                    \"uncertainty must be an instance of a \"\n                    \"NDUncertainty object or a numpy array.\"\n                )\n            self._uncertainty.parent_nddata = self\n        else:\n            self._uncertainty = value\n", "type": "function"}, {"name": "MissingDataAssociationException", "docstring": "This exception should be used to indicate that an uncertainty instance\nhas not been associated with a parent `~astropy.nddata.NDData` object.", "methods": [], "attributes": [], "code_location": {"file": "nduncertainty.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/nddata", "start_line": 81, "end_line": 84}, "type": "class"}, {"name": "uncertainty", "is_method": true, "class_name": "NDDataArray", "parameters": ["self", "value"], "calls": ["isinstance", "TypeError", "ValueError"], "code_location": {"file": "compat.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/nddata", "start_line": 113, "end_line": 131}, "code_snippet": "    def uncertainty(self, value):\n        if value is not None:\n            if isinstance(value, NDUncertainty):\n                class_name = self.__class__.__name__\n                if not self.unit and value._unit:\n                    # Raise an error if uncertainty has unit and data does not\n                    raise ValueError(\n                        \"Cannot assign an uncertainty with unit \"\n                        f\"to {class_name} without \"\n                        \"a unit\"\n                    )\n                self._uncertainty = value\n                self._uncertainty.parent_nddata = self\n            else:\n                raise TypeError(\n                    \"Uncertainty must be an instance of a NDUncertainty object\"\n                )\n        else:\n            self._uncertainty = value\n", "type": "function"}, {"name": "uncertainty", "is_method": true, "class_name": "NDData", "parameters": ["self", "value"], "calls": ["isinstance", "hasattr", "log.info", "UnknownUncertainty", "value.__class__"], "code_location": {"file": "nddata.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/nddata", "start_line": 425, "end_line": 447}, "code_snippet": "    def uncertainty(self, value):\n        if value is not None:\n            # There is one requirements on the uncertainty: That\n            # it has an attribute 'uncertainty_type'.\n            # If it does not match this requirement convert it to an unknown\n            # uncertainty.\n            if not hasattr(value, \"uncertainty_type\"):\n                log.info(\"uncertainty should have attribute uncertainty_type.\")\n                value = UnknownUncertainty(value, copy=False)\n\n            # If it is a subclass of NDUncertainty we must set the\n            # parent_nddata attribute. (#4152)\n            if isinstance(value, NDUncertainty):\n                # In case the uncertainty already has a parent create a new\n                # instance because we need to assume that we don't want to\n                # steal the uncertainty from another NDData object\n                if value._parent_nddata is not None:\n                    value = value.__class__(value, copy=False)\n                # Then link it to this NDData instance (internally this needs\n                # to be saved as weakref but that's done by NDUncertainty\n                # setter).\n                value.parent_nddata = self\n        self._uncertainty = value\n", "type": "function"}, {"name": "uncertainty", "is_method": true, "class_name": "NDData", "parameters": ["self"], "calls": [], "code_location": {"file": "nddata.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/nddata", "start_line": 413, "end_line": 422}, "code_snippet": "    def uncertainty(self):\n        \"\"\"\n        any type : Uncertainty in the dataset, if any.\n\n        Should have an attribute ``uncertainty_type`` that defines what kind of\n        uncertainty is stored, such as ``'std'`` for standard deviation or\n        ``'var'`` for variance. A metaclass defining such an interface is\n        `~astropy.nddata.NDUncertainty` but isn't mandatory.\n        \"\"\"\n        return self._uncertainty\n", "type": "function"}, {"name": "uncertainty", "is_method": true, "class_name": "NDDataArray", "parameters": ["self"], "calls": [], "code_location": {"file": "compat.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/nddata", "start_line": 109, "end_line": 110}, "code_snippet": "    def uncertainty(self):\n        return self._uncertainty\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.7541332244873047}
{"question": "How does the class-level initialization method in the test class that validates remote URL functionality for leap second data manipulate the Earth rotation and reference systems service configuration to prevent the bundled IERS-B Earth orientation data table from loading during test execution?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "setup_class", "is_method": true, "class_name": "TestHelioBaryCentric", "parameters": ["cls"], "calls": [], "code_location": {"file": "test_corrs.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 22, "end_line": 24}, "code_snippet": "    def setup_class(cls):\n        cls.orig_auto_download = iers.conf.auto_download\n        iers.conf.auto_download = False\n", "type": "function"}, {"name": "setup_class", "is_method": true, "class_name": "TestTimeUT1Remote", "parameters": ["cls"], "calls": [], "code_location": {"file": "test_ut1.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 44, "end_line": 47}, "code_snippet": "    def setup_class(cls):\n        # Need auto_download so that IERS_B won't be loaded and cause tests to\n        # fail.\n        iers_conf.auto_download = True\n", "type": "function"}, {"name": "test_auto_open_urls_always_good_enough", "is_method": true, "class_name": "TestDefaultAutoOpen", "parameters": ["self"], "calls": ["self.remove_auto_open_files", "iers.LeapSeconds.open", "startswith"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 339, "end_line": 355}, "code_snippet": "    def test_auto_open_urls_always_good_enough(self):\n        # Avoid using the erfa, built-in and system files, as they might\n        # be good enough already.\n        try:\n            # Need auto_download so that IERS_B won't be loaded and\n            # cause tests to fail.\n            iers.conf.auto_download = True\n\n            self.remove_auto_open_files(\n                \"erfa\", iers.IERS_LEAP_SECOND_FILE, \"system_leap_second_file\"\n            )\n            ls = iers.LeapSeconds.open()\n            assert ls.expires > self.good_enough\n            assert ls.meta[\"data_url\"].startswith(\"http\")\n        finally:\n            # This setting is to be consistent with astropy/conftest.py\n            iers.conf.auto_download = False\n", "type": "function"}, {"name": "setup_class", "is_method": true, "class_name": "TestRemoteURLs", "parameters": ["cls"], "calls": [], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 207, "end_line": 210}, "code_snippet": "    def setup_class(cls):\n        # Need auto_download so that IERS_B won't be loaded and cause tests to\n        # fail.\n        iers.conf.auto_download = True\n", "type": "function"}, {"name": "setup_class", "is_method": true, "class_name": "TestModelInterpretation", "parameters": ["cls"], "calls": ["Time"], "code_location": {"file": "test_sidereal.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 267, "end_line": 271}, "code_snippet": "    def setup_class(cls):\n        cls.orig_auto_download = iers.conf.auto_download\n        iers.conf.auto_download = False\n\n        cls.t = Time([\"2012-06-30 12:00:00\"], scale=\"utc\", location=(\"120d\", \"10d\"))\n", "type": "function"}, {"name": "setup_method", "is_method": true, "class_name": "ERFALeapSecondsSafe", "parameters": ["self"], "calls": ["erfa.leap_seconds.get"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 364, "end_line": 367}, "code_snippet": "    def setup_method(self):\n        # Keep current leap-second table and expiration.\n        self.erfa_ls = self._erfa_ls = erfa.leap_seconds.get()\n        self.erfa_expires = self._expires = erfa.leap_seconds._expires\n", "type": "function"}, {"name": "setup_method", "is_method": true, "class_name": "TestUpdateLeapSeconds", "parameters": ["self"], "calls": ["setup_method", "iers.LeapSeconds.from_iers_leap_seconds", "erfa.leap_seconds.set", "erfa.leap_seconds.get", "super"], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 414, "end_line": 420}, "code_snippet": "    def setup_method(self):\n        super().setup_method()\n        # Read default leap second table.\n        self.ls = iers.LeapSeconds.from_iers_leap_seconds()\n        # For tests, reset ERFA table to built-in default.\n        erfa.leap_seconds.set()\n        self.erfa_ls = erfa.leap_seconds.get()\n", "type": "function"}, {"name": "teardown_class", "is_method": true, "class_name": "TestTimeUT1Remote", "parameters": ["cls"], "calls": [], "code_location": {"file": "test_ut1.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 49, "end_line": 51}, "code_snippet": "    def teardown_class(cls):\n        # This setting is to be consistent with astropy/conftest.py\n        iers_conf.auto_download = False\n", "type": "function"}, {"name": "setup_module", "is_method": false, "class_name": null, "parameters": [], "calls": ["Time"], "code_location": {"file": "test_precision.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 43, "end_line": 46}, "code_snippet": "def setup_module():\n    # Pre-load leap seconds table to avoid flakiness in hypothesis runs.\n    # See https://github.com/astropy/astropy/issues/11030\n    Time(\"2020-01-01\").ut1\n", "type": "function"}, {"name": "teardown_class", "is_method": true, "class_name": "TestRemoteURLs", "parameters": ["cls"], "calls": [], "code_location": {"file": "test_leap_second.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/iers/tests", "start_line": 212, "end_line": 214}, "code_snippet": "    def teardown_class(cls):\n        # This setting is to be consistent with astropy/conftest.py\n        iers.conf.auto_download = False\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.7682271003723145}
{"question": "Why does the time delta format class that represents time intervals as human-readable strings with quantity components use two separate Julian Date components (an integer part and a fractional part) instead of a single floating-point representation?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "TimeDeltaJD", "docstring": "Time delta in Julian days (86400 SI seconds).", "methods": [], "attributes": ["name", "unit"], "code_location": {"file": "formats.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time", "start_line": 2227, "end_line": 2231}, "type": "class"}, {"name": "TimeDeltaQuantityString", "docstring": "Time delta as a string with one or more Quantity components.\n\nThis format provides a human-readable multi-scale string representation of a time\ndelta. It is convenient for applications like a configuration file or a command line\noption.\n\nThe format is specified as follows:\n\n- The string is a sequence of one or more components.\n- Each component is a number followed by an astropy unit of time.\n- For input, whitespace within the string is allowed but optional.\n- For output, there is a single space between components.\n- The allowed components are listed below.\n- The order (yr, d, hr, min, s) is fixed but individual components are optional.\n\nThe allowed input component units are shown below:\n\n- \"yr\": years (365.25 days)\n- \"d\": days (24 hours)\n- \"hr\": hours (60 minutes)\n- \"min\": minutes (60 seconds)\n- \"s\": seconds\n\n.. Note:: These definitions correspond to physical units of time and are NOT\n   calendar date intervals. Thus adding \"1yr\" to \"2000-01-01 00:00:00\" will give\n   \"2000-12-31 06:00:00\" instead of \"2001-01-01 00:00:00\".\n\nThe ``out_subfmt`` attribute specifies the components to be included in the string\noutput.  The default is ``\"multi\"`` which represents the time delta as\n``\"<days>d <hours>hr <minutes>min <seconds>s\"``, where only non-zero components are\nincluded.\n\n- \"multi\": multiple components, e.g. \"2d 3hr 15min 5.6s\"\n- \"yr\": years\n- \"d\": days\n- \"hr\": hours\n- \"min\": minutes\n- \"s\": seconds\n\nExamples\n--------\n>>> from astropy.time import Time, TimeDelta\n>>> import astropy.units as u\n\n>>> print(TimeDelta(\"1yr\"))\n365d 6hr\n\n>>> print(Time(\"2000-01-01\") + TimeDelta(\"1yr\"))\n2000-12-31 06:00:00.000\n>>> print(TimeDelta(\"+3.6d\"))\n3d 14hr 24min\n>>> print(TimeDelta(\"-3.6d\"))\n-3d 14hr 24min\n>>> print(TimeDelta(\"1yr 3.6d\", out_subfmt=\"d\"))\n368.85d\n\n>>> td = TimeDelta(40 * u.hr)\n>>> print(td.to_value(format=\"quantity_str\"))\n1d 16hr\n>>> print(td.to_value(format=\"quantity_str\", subfmt=\"d\"))\n1.667d\n>>> td.precision = 9\n>>> print(td.to_value(format=\"quantity_str\", subfmt=\"d\"))\n1.666666667d", "methods": ["_check_val_type", "parse_string", "set_jds", "to_value", "get_multi_comps", "fix_comp_vals_overflow", "value"], "attributes": ["name", "subfmts", "re_float", "re_ydhms"], "code_location": {"file": "formats.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time", "start_line": 2280, "end_line": 2498}, "type": "class"}, {"name": "test_quantity_str_internal_precision", "is_method": false, "class_name": null, "parameters": [], "calls": ["TimeDelta", "allclose_sec"], "code_location": {"file": "test_delta.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 830, "end_line": 833}, "code_snippet": "def test_quantity_str_internal_precision():\n    dt = TimeDelta(\"100000000d 1.0123456789012345s\")\n    assert dt.jd1 == 100000000\n    assert allclose_sec(dt.jd2 * 86400, 1.0123456789012345)\n", "type": "function"}, {"name": "test_timedelta_from_parts", "is_method": false, "class_name": null, "parameters": ["scale", "days", "day_frac"], "calls": ["given", "example", "pytest.mark.parametrize", "dict", "TimeDelta", "integers", "floats", "TimeDelta", "TimeDelta"], "code_location": {"file": "test_precision.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 752, "end_line": 756}, "code_snippet": "def test_timedelta_from_parts(scale, days, day_frac):\n    kwargs = dict(format=\"jd\", scale=scale)\n    whole = TimeDelta(days, day_frac, **kwargs)\n    from_parts = TimeDelta(days, **kwargs) + TimeDelta(day_frac, **kwargs)\n    assert whole == from_parts\n", "type": "function"}, {"name": "TimeJD", "docstring": "Julian Date time format.\n\nThis represents the number of days since the beginning of\nthe Julian Period.\nFor example, 2451544.5 in JD is midnight on January 1, 2000.", "methods": ["set_jds"], "attributes": ["name"], "code_location": {"file": "formats.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time", "start_line": 596, "end_line": 609}, "type": "class"}, {"name": "test_mjd_initialization_precise", "is_method": false, "class_name": null, "parameters": ["i", "f"], "calls": ["given", "example", "Time", "day_frac", "day_frac", "integers", "floats", "to", "int", "abs"], "code_location": {"file": "test_precision.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 387, "end_line": 391}, "code_snippet": "def test_mjd_initialization_precise(i, f):\n    t = Time(val=i, val2=f, format=\"mjd\", scale=\"tai\")\n    jd1, jd2 = day_frac(i + erfa.DJM0, f)\n    jd1_t, jd2_t = day_frac(t.jd1, t.jd2)\n    assert (abs((jd1 - jd1_t) + (jd2 - jd2_t)) * u.day).to(u.ns) < 1 * u.ns\n", "type": "function"}, {"name": "get_multi_comps", "is_method": true, "class_name": "TimeDeltaQuantityString", "parameters": ["self", "jd1", "jd2"], "calls": ["two_sum", "int", "int", "int", "np.round", "np.floor", "np.floor", "np.floor", "self.fix_comp_vals_overflow", "zip"], "code_location": {"file": "formats.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time", "start_line": 2460, "end_line": 2484}, "code_snippet": "    def get_multi_comps(self, jd1, jd2):\n        jd, remainder = two_sum(jd1, jd2)\n        days = int(np.floor(jd))\n        jd -= days\n        jd += remainder\n\n        hours = int(np.floor(jd * 24.0))\n        jd -= hours / 24.0\n        mins = int(np.floor(jd * 1440.0))\n        jd -= mins / 1440.0\n        secs = np.round(jd * 86400.0, self.precision)\n\n        comp_vals = [days, hours, mins, secs]\n        if secs >= 60.0:\n            self.fix_comp_vals_overflow(comp_vals)\n\n        comps = [\n            f\"{comp_val}{name}\"\n            for comp_val, name in zip(comp_vals, (\"d\", \"hr\", \"min\", \"s\"))\n            if comp_val != 0\n        ]\n        if not comps:\n            comps = [\"0.0s\"]\n\n        return comps\n", "type": "function"}, {"name": "set_jds", "is_method": true, "class_name": "TimeDecimalYear", "parameters": ["self", "val1", "val2"], "calls": ["self._check_scale", "two_sum", "astype", "two_sum", "astype", "astype", "np.ones_like", "np.ones_like", "np.zeros_like", "np.zeros_like", "np.zeros_like", "encode", "erfa.dtf2d", "erfa.dtf2d", "Time", "Time", "day_frac", "np.trunc", "np.trunc", "self.scale.upper"], "code_location": {"file": "formats.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time", "start_line": 678, "end_line": 705}, "code_snippet": "    def set_jds(self, val1, val2):\n        self._check_scale(self._scale)  # Validate scale.\n\n        sum12, err12 = two_sum(val1, val2)\n        iy_start = np.trunc(sum12).astype(int)\n        extra, y_frac = two_sum(sum12, -iy_start)\n        y_frac += extra + err12\n\n        val = (val1 + val2).astype(np.double)\n        iy_start = np.trunc(val).astype(int)\n\n        imon = np.ones_like(iy_start)\n        iday = np.ones_like(iy_start)\n        ihr = np.zeros_like(iy_start)\n        imin = np.zeros_like(iy_start)\n        isec = np.zeros_like(y_frac)\n\n        # Possible enhancement: use np.unique to only compute start, stop\n        # for unique values of iy_start.\n        scale = self.scale.upper().encode(\"ascii\")\n        jd1_start, jd2_start = erfa.dtf2d(scale, iy_start, imon, iday, ihr, imin, isec)\n        jd1_end, jd2_end = erfa.dtf2d(scale, iy_start + 1, imon, iday, ihr, imin, isec)\n\n        t_start = Time(jd1_start, jd2_start, scale=self.scale, format=\"jd\")\n        t_end = Time(jd1_end, jd2_end, scale=self.scale, format=\"jd\")\n        t_frac = t_start + (t_end - t_start) * y_frac\n\n        self.jd1, self.jd2 = day_frac(t_frac.jd1, t_frac.jd2)\n", "type": "function"}, {"name": "test_epoch_date_jd_is_day_fraction", "is_method": false, "class_name": null, "parameters": [], "calls": ["Time", "Time", "datetime.datetime"], "code_location": {"file": "test_basic.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 1900, "end_line": 1913}, "code_snippet": "def test_epoch_date_jd_is_day_fraction():\n    \"\"\"\n    Ensure that jd1 and jd2 of an epoch Time are respect the (day, fraction) convention\n    (see #6638)\n    \"\"\"\n    t0 = Time(\"J2000\", scale=\"tdb\")\n\n    assert t0.jd1 == 2451545.0\n    assert t0.jd2 == 0.0\n\n    t1 = Time(datetime.datetime(2000, 1, 1, 12, 0, 0), scale=\"tdb\")\n\n    assert t1.jd1 == 2451545.0\n    assert t1.jd2 == 0.0\n", "type": "function"}, {"name": "test_abs_jd2_always_less_than_half_on_construction", "is_method": false, "class_name": null, "parameters": ["jds"], "calls": ["given", "Time", "target", "np.all", "np.all", "np.all", "jd_arrays", "np.amax", "unreasonable_jd", "np.abs", "abs", "abs"], "code_location": {"file": "test_precision.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 192, "end_line": 198}, "code_snippet": "def test_abs_jd2_always_less_than_half_on_construction(jds):\n    jd1, jd2 = jds\n    t = Time(jd1, jd2, format=\"jd\")\n    target(np.amax(np.abs(t.jd2)))\n    assert np.all(t.jd1 % 1 == 0)\n    assert np.all(abs(t.jd2) <= 0.5)\n    assert np.all((abs(t.jd2) < 0.5) | (t.jd1 % 2 == 0))\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.817262887954712}
{"question": "Why does the test class that verifies mask assignment behavior in the masked array utilities module check memory sharing between the mask property and the input mask array after assignment instead of only verifying value equality?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_shares_memory", "is_method": true, "class_name": "TestMemoryFunctions", "parameters": ["self"], "calls": ["np.shares_memory", "np.shares_memory"], "code_location": {"file": "test_function_helpers.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked/tests", "start_line": 1406, "end_line": 1408}, "code_snippet": "    def test_shares_memory(self):\n        assert np.shares_memory(self.ma, self.ma.unmasked)\n        assert not np.shares_memory(self.ma, self.ma.mask)\n", "type": "function"}, {"name": "test_may_share_memory", "is_method": true, "class_name": "TestMemoryFunctions", "parameters": ["self"], "calls": ["np.may_share_memory", "np.may_share_memory"], "code_location": {"file": "test_function_helpers.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked/tests", "start_line": 1410, "end_line": 1412}, "code_snippet": "    def test_may_share_memory(self):\n        assert np.may_share_memory(self.ma, self.ma.unmasked)\n        assert not np.may_share_memory(self.ma, self.ma.mask)\n", "type": "function"}, {"name": "test_whole_mask_setting_simple", "is_method": true, "class_name": "TestMaskSetting", "parameters": ["self"], "calls": ["Masked", "ma.mask.all", "assert_array_equal", "assert_array_equal", "np.may_share_memory", "ma.mask.any", "np.array"], "code_location": {"file": "test_masked.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked/tests", "start_line": 376, "end_line": 390}, "code_snippet": "    def test_whole_mask_setting_simple(self):\n        ma = Masked(self.a)\n        assert ma.mask.shape == ma.shape\n        assert not ma.mask.any()\n        ma.mask = True\n        assert ma.mask.shape == ma.shape\n        assert ma.mask.all()\n        ma.mask = [[True], [False]]\n        assert ma.mask.shape == ma.shape\n        assert_array_equal(ma.mask, np.array([[True] * 3, [False] * 3]))\n        ma.mask = self.mask_a\n        assert ma.mask.shape == ma.shape\n        assert_array_equal(ma.mask, self.mask_a)\n        assert ma.mask is not self.mask_a\n        assert np.may_share_memory(ma.mask, self.mask_a)\n", "type": "function"}, {"name": "test_copy", "is_method": true, "class_name": "TestMaskedArrayCopyFilled", "parameters": ["self"], "calls": ["self.ma.copy", "assert_array_equal", "assert_array_equal", "type", "type", "np.may_share_memory", "np.may_share_memory"], "code_location": {"file": "test_masked.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked/tests", "start_line": 498, "end_line": 504}, "code_snippet": "    def test_copy(self):\n        ma_copy = self.ma.copy()\n        assert type(ma_copy) is type(self.ma)\n        assert_array_equal(ma_copy.unmasked, self.ma.unmasked)\n        assert_array_equal(ma_copy.mask, self.ma.mask)\n        assert not np.may_share_memory(ma_copy.unmasked, self.ma.unmasked)\n        assert not np.may_share_memory(ma_copy.mask, self.ma.mask)\n", "type": "function"}, {"name": "test_set_mask_and_not_ref", "is_method": true, "class_name": "TestMaskedColumnInit", "parameters": ["self"], "calls": ["np.all", "np.all", "np.all", "np.all", "np.all"], "code_location": {"file": "test_masked.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table/tests", "start_line": 171, "end_line": 178}, "code_snippet": "    def test_set_mask_and_not_ref(self):\n        \"\"\"Check that mask gets set properly and that it is a copy, not ref\"\"\"\n        assert np.all(~self.a.mask)\n        assert np.all(self.b.mask)\n        assert np.all(~self.c.mask)\n        assert np.all(self.d.mask == self.d_mask)\n        self.d.mask[0] = True\n        assert not np.all(self.d.mask == self.d_mask)\n", "type": "function"}, {"name": "test_whole_mask_setting_structured", "is_method": true, "class_name": "TestMaskSetting", "parameters": ["self"], "calls": ["Masked", "assert_array_equal", "assert_array_equal", "np.may_share_memory", "all", "all", "np.array", "any", "any"], "code_location": {"file": "test_masked.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked/tests", "start_line": 392, "end_line": 409}, "code_snippet": "    def test_whole_mask_setting_structured(self):\n        ma = Masked(self.sa)\n        assert ma.mask.shape == ma.shape\n        assert not ma.mask[\"a\"].any() and not ma.mask[\"b\"].any()\n        ma.mask = True\n        assert ma.mask.shape == ma.shape\n        assert ma.mask[\"a\"].all() and ma.mask[\"b\"].all()\n        ma.mask = [[True], [False]]\n        assert ma.mask.shape == ma.shape\n        assert_array_equal(\n            ma.mask,\n            np.array([[(True, True)] * 2, [(False, False)] * 2], dtype=self.mask_sdt),\n        )\n        ma.mask = self.mask_sa\n        assert ma.mask.shape == ma.shape\n        assert_array_equal(ma.mask, self.mask_sa)\n        assert ma.mask is not self.mask_sa\n        assert np.may_share_memory(ma.mask, self.mask_sa)\n", "type": "function"}, {"name": "test_set_mask", "is_method": true, "class_name": "TestSphericalRepresentationSeparateMasks", "parameters": ["self"], "calls": ["self.msph.copy", "assert_array_equal", "assert_array_equal", "np.concatenate"], "code_location": {"file": "test_masked.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/tests", "start_line": 79, "end_line": 84}, "code_snippet": "    def test_set_mask(self):\n        msph = self.msph.copy()\n        msph[0] = np.ma.masked\n        assert_array_equal(msph.mask, np.concatenate(([True], self.mask[1:])))\n        msph[0] = np.ma.nomask\n        assert_array_equal(msph.mask, self.mask)\n", "type": "function"}, {"name": "test_simple", "is_method": true, "class_name": "TestMaskedArrayInitialization", "parameters": ["self"], "calls": ["Masked", "isinstance", "isinstance", "isinstance", "assert_array_equal", "assert_array_equal", "np.may_share_memory", "type"], "code_location": {"file": "test_masked.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked/tests", "start_line": 105, "end_line": 113}, "code_snippet": "    def test_simple(self):\n        ma = Masked(self.a, mask=self.mask_a)\n        assert isinstance(ma, np.ndarray)\n        assert isinstance(ma, type(self.a))\n        assert isinstance(ma, Masked)\n        assert_array_equal(ma.unmasked, self.a)\n        assert_array_equal(ma.mask, self.mask_a)\n        assert ma.mask is not self.mask_a\n        assert np.may_share_memory(ma.mask, self.mask_a)\n", "type": "function"}, {"name": "test_structured", "is_method": true, "class_name": "TestMaskedArrayInitialization", "parameters": ["self"], "calls": ["Masked", "isinstance", "isinstance", "isinstance", "assert_array_equal", "assert_array_equal", "np.may_share_memory", "type"], "code_location": {"file": "test_masked.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked/tests", "start_line": 115, "end_line": 123}, "code_snippet": "    def test_structured(self):\n        ma = Masked(self.sa, mask=self.mask_sa)\n        assert isinstance(ma, np.ndarray)\n        assert isinstance(ma, type(self.sa))\n        assert isinstance(ma, Masked)\n        assert_array_equal(ma.unmasked, self.sa)\n        assert_array_equal(ma.mask, self.mask_sa)\n        assert ma.mask is not self.mask_sa\n        assert np.may_share_memory(ma.mask, self.mask_sa)\n", "type": "function"}, {"name": "TestMaskedArrayInteractionWithNumpyMA", "docstring": "", "methods": ["test_masked_array_from_masked", "test_view_as_masked_array"], "attributes": [], "code_location": {"file": "test_masked.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked/tests", "start_line": 1576, "end_line": 1593}, "type": "class"}], "retrieved_count": 10, "cost_time": 3.8288962841033936}
{"question": "Why does the regression test that verifies compatibility with one-dimensional labeled array structures from the pandas library validate the interaction between these structures and the input conversion process in the convolution function that transforms various array-like inputs into numpy arrays for processing?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_regression_6099", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.mark.skipif", "np.array", "convolve", "pd.Series", "convolve", "assert_array_almost_equal", "np.linspace", "np.ones", "np.ones"], "code_location": {"file": "test_convolve.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/convolution/tests", "start_line": 1187, "end_line": 1197}, "code_snippet": "def test_regression_6099():\n    import pandas as pd\n\n    wave = np.array(np.linspace(5000, 5100, 10))\n    boxcar = 3\n    nonseries_result = convolve(wave, np.ones((boxcar,)) / boxcar)\n\n    wave_series = pd.Series(wave)\n    series_result = convolve(wave_series, np.ones((boxcar,)) / boxcar)\n\n    assert_array_almost_equal(nonseries_result, series_result)\n", "type": "function"}, {"name": "test_input_unmodified", "is_method": true, "class_name": "TestConvolve1D", "parameters": ["self", "boundary", "nan_treatment", "normalize_kernel", "preserve_nan", "dtype"], "calls": ["pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.mark.parametrize", "np.array", "np.array", "convolve", "np.all", "np.all", "np.array", "np.array"], "code_location": {"file": "test_convolve.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/convolution/tests", "start_line": 91, "end_line": 117}, "code_snippet": "    def test_input_unmodified(\n        self, boundary, nan_treatment, normalize_kernel, preserve_nan, dtype\n    ):\n        \"\"\"\n        Test that convolve works correctly when inputs are lists\n        \"\"\"\n\n        array = [1.0, 4.0, 5.0, 6.0, 5.0, 7.0, 8.0]\n        kernel = [0.2, 0.6, 0.2]\n        x = np.array(array, dtype=dtype)\n        y = np.array(kernel, dtype=dtype)\n\n        # Make pseudoimmutable\n        x.flags.writeable = False\n        y.flags.writeable = False\n\n        convolve(\n            x,\n            y,\n            boundary=boundary,\n            nan_treatment=nan_treatment,\n            normalize_kernel=normalize_kernel,\n            preserve_nan=preserve_nan,\n        )\n\n        assert np.all(np.array(array, dtype=dtype) == x)\n        assert np.all(np.array(kernel, dtype=dtype) == y)\n", "type": "function"}, {"name": "test_input_unmodified", "is_method": false, "class_name": null, "parameters": ["boundary", "nan_treatment", "normalize_kernel", "preserve_nan", "dtype"], "calls": ["pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.mark.parametrize", "np.array", "np.array", "np.all", "np.all", "expected_boundary_warning", "convolve_fft", "np.array", "np.array"], "code_location": {"file": "test_convolve_fft.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/convolution/tests", "start_line": 904, "end_line": 931}, "code_snippet": "def test_input_unmodified(\n    boundary, nan_treatment, normalize_kernel, preserve_nan, dtype\n):\n    \"\"\"\n    Test that convolve_fft works correctly when inputs are lists\n    \"\"\"\n\n    array = [1.0, 4.0, 5.0, 6.0, 5.0, 7.0, 8.0]\n    kernel = [0.2, 0.6, 0.2]\n    x = np.array(array, dtype=dtype)\n    y = np.array(kernel, dtype=dtype)\n\n    # Make pseudoimmutable\n    x.flags.writeable = False\n    y.flags.writeable = False\n\n    with expected_boundary_warning(boundary=boundary):\n        convolve_fft(\n            x,\n            y,\n            boundary=boundary,\n            nan_treatment=nan_treatment,\n            normalize_kernel=normalize_kernel,\n            preserve_nan=preserve_nan,\n        )\n\n    assert np.all(np.array(array, dtype=dtype) == x)\n    assert np.all(np.array(kernel, dtype=dtype) == y)\n", "type": "function"}, {"name": "test_input_shape_1d", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.mark.skipif", "Const1D", "Const1D", "convolve_models_fft", "model", "np.arange", "model"], "code_location": {"file": "test_convolution.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling/tests", "start_line": 31, "end_line": 42}, "code_snippet": "def test_input_shape_1d():\n    m1 = Const1D()\n    m2 = Const1D()\n\n    model = convolve_models_fft(m1, m2, (-1, 1), 0.01)\n\n    results = model(0)\n    assert results.shape == (1,)\n\n    x = np.arange(-1, 1, 0.1)\n    results = model(x)\n    assert results.shape == x.shape\n", "type": "function"}, {"name": "test_regressiontest_issue9168", "is_method": false, "class_name": null, "parameters": [], "calls": ["np.array", "Gaussian2DKernel", "convolve_fft", "convolve"], "code_location": {"file": "test_convolve.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/convolution/tests", "start_line": 1264, "end_line": 1281}, "code_snippet": "def test_regressiontest_issue9168():\n    \"\"\"\n    Issue #9168 pointed out that kernels can be (unitless) quantities, which\n    leads to crashes when inplace modifications are made to arrays in\n    convolve/convolve_fft, so we now strip the quantity aspects off of kernels.\n    \"\"\"\n\n    x = np.array(\n        [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],\n    )\n\n    kernel_fwhm = 1 * u.arcsec\n    pixel_size = 1 * u.arcsec\n\n    kernel = Gaussian2DKernel(x_stddev=kernel_fwhm / pixel_size)\n\n    convolve_fft(x, kernel, boundary=\"fill\", fill_value=np.nan, preserve_nan=True)\n    convolve(x, kernel, boundary=\"fill\", fill_value=np.nan, preserve_nan=True)\n", "type": "function"}, {"name": "test__convolution_inputs", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.mark.skipif", "Const2D", "Const2D", "convolve_models_fft", "np.arange", "np.arange", "np.meshgrid", "np.meshgrid", "np.all", "np.all", "model._convolution_inputs", "pytest.raises", "model._convolution_inputs", "np.array", "model._convolution_inputs", "model._convolution_inputs", "model._convolution_inputs", "np.reshape", "model._convolution_inputs", "np.reshape"], "code_location": {"file": "test_convolution.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling/tests", "start_line": 68, "end_line": 96}, "code_snippet": "def test__convolution_inputs():\n    m1 = Const2D()\n    m2 = Const2D()\n\n    model = convolve_models_fft(m1, m2, ((-1, 1), (-1, 1)), 0.01)\n\n    x = np.arange(-1, 1, 0.1)\n    y = np.arange(-2, 2, 0.1)\n    grid0 = np.meshgrid(x, x)\n    grid1 = np.meshgrid(y, y)\n\n    # scalar inputs\n    assert (np.array([1]), (1,)) == model._convolution_inputs(1)\n\n    # Multiple inputs\n    assert np.all(\n        model._convolution_inputs(*grid0)[0]\n        == np.reshape([grid0[0], grid0[1]], (2, -1)).T\n    )\n    assert model._convolution_inputs(*grid0)[1] == grid0[0].shape\n    assert np.all(\n        model._convolution_inputs(*grid1)[0]\n        == np.reshape([grid1[0], grid1[1]], (2, -1)).T\n    )\n    assert model._convolution_inputs(*grid1)[1] == grid1[0].shape\n\n    # Error\n    with pytest.raises(ValueError, match=r\"Values have differing shapes\"):\n        model._convolution_inputs(grid0[0], grid1[1])\n", "type": "function"}, {"name": "test_invalid_array_convolve", "is_method": false, "class_name": null, "parameters": [], "calls": ["np.ones", "pytest.raises", "convolve"], "code_location": {"file": "test_convolve.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/convolution/tests", "start_line": 1200, "end_line": 1204}, "code_snippet": "def test_invalid_array_convolve():\n    kernel = np.ones(3) / 3.0\n\n    with pytest.raises(TypeError):\n        convolve(\"glork\", kernel)\n", "type": "function"}, {"name": "test_asymmetric_kernel", "is_method": false, "class_name": null, "parameters": ["boundary"], "calls": ["pytest.mark.parametrize", "np.array", "np.array", "convolve", "assert_array_almost_equal_nulp", "np.array", "assert_array_almost_equal_nulp", "np.array", "assert_array_almost_equal_nulp", "np.array", "assert_array_almost_equal_nulp", "np.array"], "code_location": {"file": "test_convolve.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/convolution/tests", "start_line": 1122, "end_line": 1143}, "code_snippet": "def test_asymmetric_kernel(boundary):\n    \"\"\"\n    Regression test for #6264: make sure that asymmetric convolution\n    functions go the right direction\n    \"\"\"\n\n    x = np.array([3.0, 0.0, 1.0], dtype=\">f8\")\n\n    y = np.array([1, 2, 3], dtype=\">f8\")\n\n    z = convolve(x, y, boundary=boundary, normalize_kernel=False)\n\n    if boundary == \"fill\":\n        assert_array_almost_equal_nulp(z, np.array([6.0, 10.0, 2.0], dtype=\"float\"), 10)\n    elif boundary is None:\n        assert_array_almost_equal_nulp(z, np.array([0.0, 10.0, 0.0], dtype=\"float\"), 10)\n    elif boundary == \"extend\":\n        assert_array_almost_equal_nulp(\n            z, np.array([15.0, 10.0, 3.0], dtype=\"float\"), 10\n        )\n    elif boundary == \"wrap\":\n        assert_array_almost_equal_nulp(z, np.array([9.0, 10.0, 5.0], dtype=\"float\"), 10)\n", "type": "function"}, {"name": "test_list", "is_method": true, "class_name": "TestConvolve3D", "parameters": ["self"], "calls": ["convolve", "assert_array_almost_equal_nulp"], "code_location": {"file": "test_convolve.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/convolution/tests", "start_line": 751, "end_line": 762}, "code_snippet": "    def test_list(self):\n        \"\"\"\n        Test that convolve works correctly when inputs are lists\n        \"\"\"\n        x = [\n            [[1, 1, 1], [1, 1, 1], [1, 1, 1]],\n            [[1, 1, 1], [1, 1, 1], [1, 1, 1]],\n            [[1, 1, 1], [1, 1, 1], [1, 1, 1]],\n        ]\n\n        z = convolve(x, x, boundary=\"fill\", fill_value=1, normalize_kernel=False)\n        assert_array_almost_equal_nulp(z / 27, x, 10)\n", "type": "function"}, {"name": "test_atleast_1d", "is_method": true, "class_name": "TestShapeFunctions", "parameters": ["self", "use_mask"], "calls": ["self.create_data", "np.atleast_1d", "assert_time_all_equal", "np.may_share_memory", "self.t0.ravel"], "code_location": {"file": "test_methods.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 388, "end_line": 397}, "code_snippet": "    def test_atleast_1d(self, use_mask):\n        self.create_data(use_mask)\n\n        t00 = self.t0.ravel()[0]\n        assert t00.ndim == 0\n        t00_1d = np.atleast_1d(t00)\n        assert t00_1d.ndim == 1\n        assert_time_all_equal(t00[np.newaxis], t00_1d)\n        # Actual jd1 will not share memory, as cast to scalar.\n        assert np.may_share_memory(t00_1d._time.jd1, t00._time.jd1)\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.8382203578948975}
{"question": "Why does the base class for non-corrupted header data units that enable checksum verification exist in the FITS header data unit hierarchy?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "_ValidHDU", "docstring": "Base class for all HDUs which are not corrupted.", "methods": ["__init__", "match_header", "size", "filebytes", "fileinfo", "copy", "_verify", "_prewriteto", "req_cards", "add_datasum", "add_checksum", "verify_datasum", "verify_checksum", "_verify_checksum_datasum", "_update_checksum", "_get_timestamp", "_calculate_datasum", "_calculate_checksum", "_compute_checksum", "_encode_byte", "_char_encode"], "attributes": ["_MASK", "_EXCLUDE"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/hdu", "start_line": 850, "end_line": 1509}, "type": "class"}, {"name": "_CorruptedHDU", "docstring": "A Corrupted HDU class.\n\nThis class is used when one or more mandatory `Card`s are\ncorrupted (unparsable), such as the ``BITPIX``, ``NAXIS``, or\n``END`` cards.  A corrupted HDU usually means that the data size\ncannot be calculated or the ``END`` card is not found.  In the case\nof a missing ``END`` card, the `Header` may also contain the binary\ndata\n\n.. note::\n   In future, it may be possible to decipher where the last block\n   of the `Header` ends, but this task may be difficult when the\n   extension is a `TableHDU` containing ASCII data.", "methods": ["size", "_summary", "verify"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/hdu", "start_line": 733, "end_line": 766}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "_ValidHDU", "parameters": ["self", "data", "header", "name", "ver"], "calls": ["__init__", "ValueError", "super", "isinstance"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/hdu", "start_line": 855, "end_line": 874}, "code_snippet": "    def __init__(self, data=None, header=None, name=None, ver=None, **kwargs):\n        super().__init__(data=data, header=header)\n\n        if header is not None and not isinstance(header, (Header, _BasicHeader)):\n            # TODO: Instead maybe try initializing a new Header object from\n            # whatever is passed in as the header--there are various types\n            # of objects that could work for this...\n            raise ValueError(\"header must be a Header object\")\n\n        # NOTE:  private data members _checksum and _datasum are used by the\n        # utility script \"fitscheck\" to detect missing checksums.\n        self._checksum = None\n        self._checksum_valid = None\n        self._datasum = None\n        self._datasum_valid = None\n\n        if name is not None:\n            self.name = name\n        if ver is not None:\n            self.ver = ver\n", "type": "function"}, {"name": "_BaseHDU", "docstring": "Base class for all HDU (header data unit) classes.", "methods": ["__init__", "__init_subclass__", "header", "header", "name", "name", "ver", "ver", "level", "level", "is_image", "_data_loaded", "_has_data", "register_hdu", "unregister_hdu", "match_header", "fromstring", "readfrom", "writeto", "_from_data", "_readfrom_internal", "_get_raw_data", "_postwriteto", "_writeheader", "_writedata", "_writedata_internal", "_writedata_direct_copy", "_writeto", "_writeto_internal", "_close"], "attributes": ["_hdu_registry", "_standard", "_padding_byte", "_default_name", "_header"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/hdu", "start_line": 133, "end_line": 719}, "type": "class"}, {"name": "InvalidHDUException", "docstring": "A custom exception class used mainly to signal to _BaseHDU.__new__ that\nan HDU cannot possibly be considered valid, and must be assumed to be\ncorrupted.", "methods": [], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/hdu", "start_line": 75, "end_line": 80}, "type": "class"}, {"name": "_NonstandardHDU", "docstring": "A Non-standard HDU class.\n\nThis class is used for a Primary HDU when the ``SIMPLE`` Card has\na value of `False`.  A non-standard HDU comes from a file that\nresembles a FITS file but departs from the standards in some\nsignificant way.  One example would be files where the numbers are\nin the DEC VAX internal storage format rather than the standard\nFITS most significant byte first.  The header for this HDU should\nbe valid.  The data for this HDU is read from the file as a byte\nstream that begins at the first byte after the header ``END`` card\nand continues until the end of the file.", "methods": ["match_header", "size", "_writedata", "_summary", "data", "_verify"], "attributes": ["_standard"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/hdu", "start_line": 769, "end_line": 847}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "_BaseHDU", "parameters": ["self", "data", "header"], "calls": ["Header"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/hdu", "start_line": 150, "end_line": 173}, "code_snippet": "    def __init__(self, data=None, header=None, *args, **kwargs):\n        if header is None:\n            header = Header()\n        self._header = header\n        self._header_str = None\n        self._file = None\n        self._buffer = None\n        self._header_offset = None\n        self._data_offset = None\n        self._data_size = None\n\n        # This internal variable is used to track whether the data attribute\n        # still points to the same data array as when the HDU was originally\n        # created (this does not track whether the data is actually the same\n        # content-wise)\n        self._data_replaced = False\n        self._data_needs_rescale = False\n        self._new = True\n        self._output_checksum = False\n\n        if \"DATASUM\" in self._header and \"CHECKSUM\" not in self._header:\n            self._output_checksum = \"datasum\"\n        elif \"CHECKSUM\" in self._header:\n            self._output_checksum = True\n", "type": "function"}, {"name": "match_header", "is_method": true, "class_name": "_ImageBaseHDU", "parameters": ["cls", "header"], "calls": [], "code_location": {"file": "image.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/hdu", "start_line": 172, "end_line": 177}, "code_snippet": "    def match_header(cls, header):\n        \"\"\"\n        _ImageBaseHDU is sort of an abstract class for HDUs containing image\n        data (as opposed to table data) and should never be used directly.\n        \"\"\"\n        raise NotImplementedError\n", "type": "function"}, {"name": "_ImageBaseHDU", "docstring": "FITS image HDU base class.\n\nAttributes\n----------\nheader\n    image header\n\ndata\n    image data", "methods": ["__init__", "match_header", "is_image", "section", "shape", "header", "header", "data", "data", "_data_shape", "update_header", "_update_header_scale_info", "_update_pseudo_int_scale_keywords", "scale", "_scale_internal", "_verify", "_verify_blank", "_prewriteto", "_writedata_internal", "_writeinternal_dask", "_dtype_for_bitpix", "_convert_pseudo_integer", "_get_scaled_image_data", "_scale_data", "_summary", "_calculate_datasum"], "attributes": ["standard_keyword_comments"], "code_location": {"file": "image.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/hdu", "start_line": 24, "end_line": 949}, "type": "class"}, {"name": "_verify", "is_method": true, "class_name": "CompImageHDU", "parameters": ["self"], "calls": ["_verify", "_ErrList", "super", "errs_filtered.append", "len"], "code_location": {"file": "compressed.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/hdu/compressed", "start_line": 670, "end_line": 689}, "code_snippet": "    def _verify(self, *args, **kwargs):\n        # The following is the default _verify for ImageHDU\n        errs = super()._verify(*args, **kwargs)\n\n        # However in some cases the decompressed header is actually like a\n        # PrimaryHDU header rather than an ImageHDU header, in which case\n        # there are certain errors we can ignore\n        if \"SIMPLE\" in self.header:\n            errs_filtered = []\n            for err in errs:\n                if len(err) >= 2 and err[1] in (\n                    \"'XTENSION' card does not exist.\",\n                    \"'PCOUNT' card does not exist.\",\n                    \"'GCOUNT' card does not exist.\",\n                ):\n                    continue\n                errs_filtered.append(err)\n            return _ErrList(errs_filtered)\n        else:\n            return errs\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.907078742980957}
{"question": "How does the test class that validates table initialization from heterogeneous column sources enforce separation between column name and type resolution logic and parent table reference assignment when creating tables from mixed column inputs?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_init_bad_dtype_in_empty_table", "is_method": false, "class_name": null, "parameters": ["dtype"], "calls": ["pytest.mark.parametrize", "pytest.raises", "Table"], "code_location": {"file": "test_init_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table/tests", "start_line": 627, "end_line": 631}, "code_snippet": "def test_init_bad_dtype_in_empty_table(dtype):\n    with pytest.raises(\n        ValueError, match=\"type was specified but could not be parsed for column names\"\n    ):\n        Table(dtype=dtype)\n", "type": "function"}, {"name": "test_init_from_columns_mix", "is_method": true, "class_name": "TestInitFromTable", "parameters": ["self", "table_type"], "calls": ["self._setup", "table_type", "table_type"], "code_location": {"file": "test_init_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table/tests", "start_line": 468, "end_line": 473}, "code_snippet": "    def test_init_from_columns_mix(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data)\n        t2 = table_type([t.columns[0], t.columns[\"z\"]])\n        assert t2.colnames == [\"x\", \"z\"]\n        assert t2.dtype.names == (\"x\", \"z\")\n", "type": "function"}, {"name": "test_table_init_from_degenerate_arrays", "is_method": false, "class_name": null, "parameters": ["table_types"], "calls": ["table_types.Table", "table_types.Table", "np.array", "len", "pytest.raises", "table_types.Table", "np.array", "len", "np.array"], "code_location": {"file": "test_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table/tests", "start_line": 2055, "end_line": 2063}, "code_snippet": "def test_table_init_from_degenerate_arrays(table_types):\n    t = table_types.Table(np.array([]))\n    assert len(t.columns) == 0\n\n    with pytest.raises(ValueError):\n        t = table_types.Table(np.array(0))\n\n    t = table_types.Table(np.array([1, 2, 3]))\n    assert len(t.columns) == 3\n", "type": "function"}, {"name": "test_name_none", "is_method": true, "class_name": "TestNewFromColumns", "parameters": ["self", "table_types"], "calls": ["table_types.Column", "table_types.Column", "table_types.Table", "table_types.Table"], "code_location": {"file": "test_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table/tests", "start_line": 342, "end_line": 349}, "code_snippet": "    def test_name_none(self, table_types):\n        \"\"\"Column with name=None can init a table whether or not names are supplied\"\"\"\n        c = table_types.Column(data=[1, 2], name=\"c\")\n        d = table_types.Column(data=[3, 4])\n        t = table_types.Table([c, d], names=(None, \"d\"))\n        assert t.colnames == [\"c\", \"d\"]\n        t = table_types.Table([c, d])\n        assert t.colnames == [\"c\", \"col1\"]\n", "type": "function"}, {"name": "test_ref", "is_method": true, "class_name": "TestInitFromColsList", "parameters": ["self", "table_type"], "calls": ["self._setup", "table_type"], "code_location": {"file": "test_init_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table/tests", "start_line": 293, "end_line": 298}, "code_snippet": "    def test_ref(self, table_type):\n        \"\"\"Test that initializing from a list of columns can be done by reference\"\"\"\n        self._setup(table_type)\n        t = table_type(self.data, copy=False)\n        t[\"x\"][0] = 100\n        assert self.data[0][0] == 100\n", "type": "function"}, {"name": "test_init_data_type_not_allowed_to_init_table", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.raises", "Table"], "code_location": {"file": "test_init_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table/tests", "start_line": 634, "end_line": 638}, "code_snippet": "def test_init_data_type_not_allowed_to_init_table():\n    with pytest.raises(\n        ValueError, match=\"Data type <class 'str'> not allowed to init Table\"\n    ):\n        Table(\"hello\")\n", "type": "function"}, {"name": "test_1", "is_method": true, "class_name": "TestAddPosition", "parameters": ["self", "table_types"], "calls": ["self._setup", "table_types.Table", "t.add_column"], "code_location": {"file": "test_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table/tests", "start_line": 486, "end_line": 489}, "code_snippet": "    def test_1(self, table_types):\n        self._setup(table_types)\n        t = table_types.Table()\n        t.add_column(self.a, 0)\n", "type": "function"}, {"name": "test_init_from_columns", "is_method": true, "class_name": "TestInitFromTable", "parameters": ["self", "table_type"], "calls": ["self._setup", "table_type", "table_type"], "code_location": {"file": "test_init_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table/tests", "start_line": 454, "end_line": 459}, "code_snippet": "    def test_init_from_columns(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data)\n        t2 = table_type(t.columns[\"z\", \"x\", \"y\"])\n        assert t2.colnames == [\"z\", \"x\", \"y\"]\n        assert t2.dtype.names == (\"z\", \"x\", \"y\")\n", "type": "function"}, {"name": "test_basic_init", "is_method": true, "class_name": "BaseInitFrom", "parameters": ["self", "table_type"], "calls": ["self._setup", "table_type", "np.all", "np.all", "np.all", "all", "np.array", "np.array", "np.array"], "code_location": {"file": "test_init_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table/tests", "start_line": 75, "end_line": 82}, "code_snippet": "    def test_basic_init(self, table_type):\n        self._setup(table_type)\n        t = table_type(self.data, names=(\"a\", \"b\", \"c\"))\n        assert t.colnames == [\"a\", \"b\", \"c\"]\n        assert np.all(t[\"a\"] == np.array([1, 3]))\n        assert np.all(t[\"b\"] == np.array([2, 4]))\n        assert np.all(t[\"c\"] == np.array([3, 5]))\n        assert all(t[name].name == name for name in t.colnames)\n", "type": "function"}, {"name": "test_initialization_with_all_columns", "is_method": true, "class_name": "TestTableInit", "parameters": ["self"], "calls": ["Table", "Table", "np.all", "np.all", "getattr", "getattr"], "code_location": {"file": "test_masked.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table/tests", "start_line": 275, "end_line": 287}, "code_snippet": "    def test_initialization_with_all_columns(self):\n        t1 = Table([self.a, self.b, self.c, self.d, self.ca, self.sc])\n        assert t1.colnames == [\"a\", \"b\", \"c\", \"d\", \"ca\", \"sc\"]\n        # Check we get the same result by passing in as list of dict.\n        # (Regression test for error uncovered by scintillometry package.)\n        lofd = [{k: row[k] for k in t1.colnames} for row in t1]\n        t2 = Table(lofd)\n        for k in t1.colnames:\n            assert t1[k].dtype == t2[k].dtype\n            assert np.all(t1[k] == t2[k]) in (True, np.ma.masked)\n            assert np.all(\n                getattr(t1[k], \"mask\", False) == getattr(t2[k], \"mask\", False)\n            )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.9444358348846436}
{"question": "Why does the function that validates unified content descriptor strings enforce stricter standard compliance when the configuration flag for VOTable version 1.2 or later is enabled?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "check_ucd", "is_method": false, "class_name": null, "parameters": ["ucd", "config", "pos"], "calls": ["config.get", "ucd_mod.parse_ucd", "config.get", "config.get", "config.get", "vo_raise", "config.get", "vo_warn", "str", "str"], "code_location": {"file": "tree.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 349, "end_line": 382}, "code_snippet": "def check_ucd(ucd, config=None, pos=None):\n    \"\"\"\n    Warns or raises a\n    `~astropy.io.votable.exceptions.VOTableSpecError` if *ucd* is not\n    a valid `unified content descriptor`_ string as defined by the\n    VOTABLE standard.\n\n    Parameters\n    ----------\n    ucd : str\n        A UCD string.\n\n    config, pos : optional\n        Information about the source of the value\n    \"\"\"\n    if config is None:\n        config = {}\n    if config.get(\"version_1_1_or_later\"):\n        try:\n            ucd_mod.parse_ucd(\n                ucd,\n                check_controlled_vocabulary=config.get(\"version_1_2_or_later\", False),\n                has_colon=config.get(\"version_1_2_or_later\", False),\n            )\n        except ValueError as e:\n            # This weird construction is for Python 3 compatibility\n            if config.get(\"verify\", \"ignore\") == \"exception\":\n                vo_raise(W06, (ucd, str(e)), config, pos)\n            elif config.get(\"verify\", \"ignore\") == \"warn\":\n                vo_warn(W06, (ucd, str(e)), config, pos)\n                return False\n            else:\n                return False\n    return True\n", "type": "function"}, {"name": "check_ucd", "is_method": false, "class_name": null, "parameters": ["ucd", "check_controlled_vocabulary", "has_colon"], "calls": ["parse_ucd"], "code_location": {"file": "ucd.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 163, "end_line": 196}, "code_snippet": "def check_ucd(ucd, check_controlled_vocabulary=False, has_colon=False):\n    \"\"\"\n    Returns False if *ucd* is not a valid `unified content descriptor`_.\n\n    Parameters\n    ----------\n    ucd : str\n        The UCD string\n\n    check_controlled_vocabulary : bool, optional\n        If `True`, then each word in the UCD will be verified against\n        the UCD1+ controlled vocabulary, (as required by the VOTable\n        specification version 1.2), otherwise not.\n\n    has_colon : bool, optional\n        If `True`, the UCD may contain a colon (as defined in earlier\n        versions of the standard).\n\n    Returns\n    -------\n    valid : bool\n    \"\"\"\n    if ucd is None:\n        return True\n\n    try:\n        parse_ucd(\n            ucd,\n            check_controlled_vocabulary=check_controlled_vocabulary,\n            has_colon=has_colon,\n        )\n    except ValueError:\n        return False\n    return True\n", "type": "function"}, {"name": "W06", "docstring": "This warning is emitted when a ``ucd`` attribute does not match\nthe syntax of a `unified content descriptor\n<https://vizier.unistra.fr/doc/UCD.htx>`__.\n\nIf the VOTable version is 1.2 or later, the UCD will also be\nchecked to ensure it conforms to the controlled vocabulary defined\nby UCD1+.\n\n**References**: `1.1\n<http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#sec:ucd>`__,\n`1.2\n<http://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html#sec:ucd>`__", "methods": [], "attributes": ["message_template", "default_args"], "code_location": {"file": "exceptions.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 417, "end_line": 434}, "type": "class"}, {"name": "ucd", "is_method": true, "class_name": "_UcdProperty", "parameters": ["self", "ucd"], "calls": ["check_ucd", "ucd.strip", "warn_or_raise", "self._config.get"], "code_location": {"file": "tree.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 477, "end_line": 490}, "code_snippet": "    def ucd(self, ucd):\n        if ucd is not None and ucd.strip() == \"\":\n            ucd = None\n        if ucd is not None:\n            if self._ucd_in_v1_2 and not self._config.get(\"version_1_2_or_later\"):\n                warn_or_raise(\n                    W28,\n                    W28,\n                    (\"ucd\", self._element_name, \"1.2\"),\n                    self._config,\n                    self._pos,\n                )\n            check_ucd(ucd, self._config, self._pos)\n        self._ucd = ucd\n", "type": "function"}, {"name": "W04", "docstring": "The ``content-type`` attribute must use MIME content-type syntax as\ndefined in `RFC 2046 <https://tools.ietf.org/html/rfc2046>`__.\n\nThe current check for validity is somewhat over-permissive.\n\n**References**: `1.1\n<http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#sec:link>`__,\n`1.2\n<http://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html#sec:link>`__", "methods": [], "attributes": ["message_template", "default_args"], "code_location": {"file": "exceptions.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 390, "end_line": 404}, "type": "class"}, {"name": "W02", "docstring": "Nonstandard XML id.\n\nXML ids must match the following regular expression::\n\n    ^[A-Za-z_][A-Za-z0-9_\\.\\-]*$\n\nThe VOTable 1.1 says the following:\n\n    According to the XML standard, the attribute ``ID`` is a\n    string beginning with a letter or underscore (``_``), followed\n    by a sequence of letters, digits, or any of the punctuation\n    characters ``.`` (dot), ``-`` (dash), ``_`` (underscore), or\n    ``:`` (colon).\n\nHowever, this is in conflict with the XML standard, which says\ncolons may not be used.  VOTable 1.1's own schema does not allow a\ncolon here.  Therefore, ``astropy.io.votable`` disallows the colon.\n\nVOTable 1.2 corrects this error in the specification.\n\n**References**: `1.1\n<http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#sec:name>`__,\n`XML Names <https://www.w3.org/TR/xml-names/>`__", "methods": [], "attributes": ["message_template", "default_args"], "code_location": {"file": "exceptions.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 306, "end_line": 333}, "type": "class"}, {"name": "W29", "docstring": "Some VOTable files specify their version number in the form \"v1.0\",\nwhen the only supported forms in the spec are \"1.0\".\n\n**References**: `1.1\n<http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#ToC54>`__,\n`1.2\n<http://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html#ToC58>`__", "methods": [], "attributes": ["message_template", "default_args"], "code_location": {"file": "exceptions.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 760, "end_line": 772}, "type": "class"}, {"name": "W01", "docstring": "Array uses commas rather than whitespace.\n\nThe VOTable spec states:\n\n    If a cell contains an array or complex number, it should be\n    encoded as multiple numbers separated by whitespace.\n\nMany VOTable files in the wild use commas as a separator instead,\nand ``astropy.io.votable`` can support this convention depending on the\n:ref:`astropy:verifying-votables` setting.\n\n``astropy.io.votable`` always outputs files using only spaces, regardless of\nhow they were input.\n\n**References**: `1.1\n<http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#toc-header-35>`__,\n`1.2\n<http://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html#sec:TABLEDATA>`__", "methods": [], "attributes": ["message_template"], "code_location": {"file": "exceptions.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 282, "end_line": 303}, "type": "class"}, {"name": "W09", "docstring": "The VOTable specification uses the attribute name ``ID`` (with\nuppercase letters) to specify unique identifiers.  Some\nVOTable-producing tools use the more standard lowercase ``id``\ninstead. ``astropy.io.votable`` accepts ``id`` and emits this warning if\n``verify`` is ``'warn'``.\n\n**References**: `1.1\n<http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#sec:name>`__,\n`1.2\n<http://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html#sec:name>`__", "methods": [], "attributes": ["message_template"], "code_location": {"file": "exceptions.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 470, "end_line": 484}, "type": "class"}, {"name": "utype", "is_method": true, "class_name": "_UtypeProperty", "parameters": ["self", "utype"], "calls": ["check_string", "warn_or_raise", "self._config.get"], "code_location": {"file": "tree.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 451, "end_line": 461}, "code_snippet": "    def utype(self, utype):\n        if (\n            self._utype_in_v1_2\n            and utype is not None\n            and not self._config.get(\"version_1_2_or_later\")\n        ):\n            warn_or_raise(\n                W28, W28, (\"utype\", self._element_name, \"1.2\"), self._config, self._pos\n            )\n        check_string(utype, \"utype\", self._config, self._pos)\n        self._utype = utype\n", "type": "function"}], "retrieved_count": 10, "cost_time": 3.9539577960968018}
{"question": "How should the validation methods that orchestrate unit serialization and deserialization testing be refactored to separate the format-specific string conversion operations from the unit decomposition and scale comparison operations in the pytest test classes?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "check_roundtrip_decompose", "is_method": true, "class_name": "RoundtripBase", "parameters": ["self", "unit"], "calls": ["unit.decompose", "ud.to_string", "Unit", "assert_allclose", "a.decompose"], "code_location": {"file": "test_format.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 372, "end_line": 377}, "code_snippet": "    def check_roundtrip_decompose(self, unit):\n        ud = unit.decompose()\n        s = ud.to_string(self.format_)\n        assert \"  \" not in s\n        a = Unit(s, format=self.format_)\n        assert_allclose(a.decompose().scale, ud.scale, rtol=1e-5)\n", "type": "function"}, {"name": "check_roundtrip", "is_method": true, "class_name": "RoundtripBase", "parameters": ["self", "unit", "output_format"], "calls": ["assert_allclose", "warnings.catch_warnings", "warnings.simplefilter", "unit.to_string", "Unit", "pytest.warns", "Unit", "len", "a.decompose", "unit.decompose"], "code_location": {"file": "test_format.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 356, "end_line": 370}, "code_snippet": "    def check_roundtrip(self, unit, output_format=None):\n        if output_format is None:\n            output_format = self.format_.name\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")  # Same warning shows up multiple times\n            s = unit.to_string(output_format)\n\n        if s in self.format_._deprecated_units:\n            with pytest.warns(UnitsWarning, match=\"deprecated\") as w:\n                a = Unit(s, format=self.format_)\n            assert len(w) == 1\n        else:\n            a = Unit(s, format=self.format_)  # No warning\n\n        assert_allclose(a.decompose().scale, unit.decompose().scale, rtol=1e-9)\n", "type": "function"}, {"name": "test_roundtrip", "is_method": true, "class_name": "TestRoundtripOGIP", "parameters": ["self", "unit"], "calls": ["pytest.mark.parametrize", "np.log10", "str", "assert_allclose", "self.check_roundtrip", "str", "abs", "pytest.warns", "self.check_roundtrip_decompose", "pytest.warns", "unit.to_string", "Unit", "unit.decompose", "str", "pytest.warns", "nullcontext", "u_format.OGIP._units.values", "a.decompose", "unit.decompose", "round", "isinstance", "isinstance"], "code_location": {"file": "test_format.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 461, "end_line": 485}, "code_snippet": "    def test_roundtrip(self, unit):\n        if str(unit) == \"0.001 Crab\":\n            # Special-case mCrab, which the default check does not recognize\n            # as a deprecated unit.\n            with pytest.warns(UnitsWarning):\n                s = unit.to_string(self.format_)\n                a = Unit(s, format=self.format_)\n            assert_allclose(a.decompose().scale, unit.decompose().scale, rtol=1e-9)\n        else:\n            self.check_roundtrip(unit)\n        if str(unit) in (\"mag\", \"byte\", \"Crab\"):\n            # Skip mag and byte, which decompose into dex and bit, resp.,\n            # both of which are unknown to OGIP, as well as Crab, which does\n            # not decompose, and thus gives a deprecated unit warning.\n            return\n\n        power_of_ten = np.log10(unit.decompose().scale)\n        if abs(power_of_ten - round(power_of_ten)) > 1e-3:\n            ctx = pytest.warns(UnitsWarning, match=\"power of 10\")\n        elif str(unit) == \"0.001 Crab\":\n            ctx = pytest.warns(UnitsWarning, match=\"deprecated\")\n        else:\n            ctx = nullcontext()\n        with ctx:\n            self.check_roundtrip_decompose(unit)\n", "type": "function"}, {"name": "test_roundtrip", "is_method": true, "class_name": "TestRoundtripVOUnit", "parameters": ["self", "unit"], "calls": ["pytest.mark.parametrize", "self.check_roundtrip", "self.check_roundtrip_decompose", "u_format.VOUnit._units.values", "isinstance"], "code_location": {"file": "test_format.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 406, "end_line": 409}, "code_snippet": "    def test_roundtrip(self, unit):\n        self.check_roundtrip(unit)\n        if unit not in (u.mag, u.dB):\n            self.check_roundtrip_decompose(unit)\n", "type": "function"}, {"name": "test_scale_only", "is_method": false, "class_name": null, "parameters": ["test_pair"], "calls": ["pytest.mark.parametrize", "list_format_string_pairs", "to_string", "u.Unit"], "code_location": {"file": "test_format.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 650, "end_line": 651}, "code_snippet": "def test_scale_only(test_pair: FormatStringPair):\n    assert u.Unit(10).to_string(test_pair.format) == test_pair.string\n", "type": "function"}, {"name": "test_vounit_scale_factor", "is_method": false, "class_name": null, "parameters": ["unit", "vounit", "number", "scale", "voscale"], "calls": ["pytest.mark.parametrize", "u.Unit", "x.to_string", "u.Unit"], "code_location": {"file": "test_format.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 812, "end_line": 815}, "code_snippet": "def test_vounit_scale_factor(unit, vounit, number, scale, voscale):\n    x = u.Unit(f\"{scale} {unit}\")\n    assert x == number * u.Unit(unit)\n    assert x.to_string(format=\"vounit\") == voscale + vounit\n", "type": "function"}, {"name": "test_format_styles", "is_method": false, "class_name": null, "parameters": ["format_spec", "string", "decomposed"], "calls": ["pytest.mark.parametrize", "format", "format", "pytest.warns", "format", "fluxunit.decompose"], "code_location": {"file": "test_format.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 550, "end_line": 561}, "code_snippet": "def test_format_styles(format_spec, string, decomposed):\n    fluxunit = u.erg / (u.cm**2 * u.s * u.Angstrom)\n    if format_spec == \"vounit\":\n        # erg and Angstrom are deprecated in vounit.\n        with pytest.warns(UnitsWarning, match=\"deprecated\"):\n            formatted = format(fluxunit, format_spec)\n    else:\n        formatted = format(fluxunit, format_spec)\n    assert formatted == string\n    # Decomposed mostly to test that scale factors are dealt with properly\n    # in the various formats.\n    assert format(fluxunit.decompose(), format_spec) == decomposed\n", "type": "function"}, {"name": "test_fits_scale_factor", "is_method": false, "class_name": null, "parameters": ["scale", "number", "string"], "calls": ["pytest.mark.parametrize", "u.Unit", "u.Unit", "x.to_string", "x.to_string"], "code_location": {"file": "test_format.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 896, "end_line": 903}, "code_snippet": "def test_fits_scale_factor(scale, number, string):\n    x = u.Unit(scale + \" erg/(s cm**2 Angstrom)\", format=\"fits\")\n    assert x == number * (u.erg / u.s / u.cm**2 / u.Angstrom)\n    assert x.to_string(format=\"fits\") == string + \" erg Angstrom-1 s-1 cm-2\"\n\n    x = u.Unit(scale + \"*erg/(s cm**2 Angstrom)\", format=\"fits\")\n    assert x == number * (u.erg / u.s / u.cm**2 / u.Angstrom)\n    assert x.to_string(format=\"fits\") == string + \" erg Angstrom-1 s-1 cm-2\"\n", "type": "function"}, {"name": "test_roundtrip_dex", "is_method": true, "class_name": "TestRoundtripCDS", "parameters": ["self", "unit"], "calls": ["pytest.mark.parametrize", "unit.to_string", "u.Unit", "u.dex"], "code_location": {"file": "test_format.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 443, "end_line": 446}, "code_snippet": "    def test_roundtrip_dex(self, unit):\n        string = unit.to_string(format=\"cds\")\n        recovered = u.Unit(string, format=\"cds\")\n        assert recovered == unit\n", "type": "function"}, {"name": "test_roundtrip", "is_method": true, "class_name": "TestRoundtripCDS", "parameters": ["self", "unit"], "calls": ["pytest.mark.parametrize", "self.check_roundtrip", "self.check_roundtrip_decompose", "u_format.CDS._units.values", "isinstance"], "code_location": {"file": "test_format.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 432, "end_line": 438}, "code_snippet": "    def test_roundtrip(self, unit):\n        self.check_roundtrip(unit)\n        if unit == u.mag:\n            # Skip mag: decomposes into dex, which is unknown to CDS.\n            return\n\n        self.check_roundtrip_decompose(unit)\n", "type": "function"}], "retrieved_count": 10, "cost_time": 4.020954132080078}
{"question": "What attributes must be implemented by the object type tested in the test class that verifies compatibility with numpy's shape, size, and dimensionality inspection functions to satisfy the dependencies of those functions?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_shape_attribute_functions", "is_method": true, "class_name": "TestShapeFunctions", "parameters": ["self", "attribute"], "calls": ["pytest.mark.parametrize", "getattr", "function", "getattr"], "code_location": {"file": "test_representation_methods.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/tests", "start_line": 407, "end_line": 410}, "code_snippet": "    def test_shape_attribute_functions(self, attribute):\n        function = getattr(np, attribute)\n        result = function(self.s0)\n        assert result == getattr(self.s0, attribute)\n", "type": "function"}, {"name": "test_shape_attribute_functions", "is_method": false, "class_name": null, "parameters": ["t", "attribute"], "calls": ["pytest.mark.xfail", "pytest.mark.parametrize", "pytest.mark.parametrize", "getattr", "function", "getattr", "Time", "Time", "TimeDelta", "reshape", "np.arange"], "code_location": {"file": "test_functions.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 57, "end_line": 62}, "code_snippet": "def test_shape_attribute_functions(t, attribute):\n    # Regression test for\n    # https://github.com/astropy/astropy/issues/8610#issuecomment-736855217\n    function = getattr(np, attribute)\n    result = function(t)\n    assert result == getattr(t, attribute)\n", "type": "function"}, {"name": "test_size", "is_method": true, "class_name": "TestShapeInformation", "parameters": ["self"], "calls": ["np.size"], "code_location": {"file": "test_function_helpers.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked/tests", "start_line": 80, "end_line": 81}, "code_snippet": "    def test_size(self):\n        assert np.size(self.ma) == 6\n", "type": "function"}, {"name": "test_shape", "is_method": true, "class_name": "TestShapeInformation", "parameters": ["self"], "calls": ["np.shape"], "code_location": {"file": "test_function_helpers.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked/tests", "start_line": 77, "end_line": 78}, "code_snippet": "    def test_shape(self):\n        assert np.shape(self.ma) == (2, 3)\n", "type": "function"}, {"name": "test_size", "is_method": true, "class_name": "TestShapeInformation", "parameters": ["self"], "calls": ["np.size"], "code_location": {"file": "test_quantity_non_ufuncs.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 119, "end_line": 120}, "code_snippet": "    def test_size(self):\n        assert np.size(self.q) == 9\n", "type": "function"}, {"name": "test_shape_setting", "is_method": true, "class_name": "TestSetShape", "parameters": ["self"], "calls": ["SphericalRepresentation", "pytest.raises", "pytest.raises", "self.s1.lon.copy", "pytest.raises"], "code_location": {"file": "test_representation_methods.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/tests", "start_line": 253, "end_line": 300}, "code_snippet": "    def test_shape_setting(self):\n        # Shape-setting should be on the object itself, since copying removes\n        # zero-strides due to broadcasting.  Hence, this should be the only\n        # test in this class.\n        self.s0.shape = (2, 3, 7)\n        assert self.s0.shape == (2, 3, 7)\n        assert self.s0.lon.shape == (2, 3, 7)\n        assert self.s0.lat.shape == (2, 3, 7)\n        assert self.s0.distance.shape == (2, 3, 7)\n        assert self.diff.shape == (2, 3, 7)\n        assert self.diff.d_lon.shape == (2, 3, 7)\n        assert self.diff.d_lat.shape == (2, 3, 7)\n        assert self.diff.d_distance.shape == (2, 3, 7)\n\n        # this works with the broadcasting.\n        self.s1.shape = (2, 3, 7)\n        assert self.s1.shape == (2, 3, 7)\n        assert self.s1.lon.shape == (2, 3, 7)\n        assert self.s1.lat.shape == (2, 3, 7)\n        assert self.s1.distance.shape == (2, 3, 7)\n        assert self.s1.distance.strides == (0, 0, 0)\n\n        # but this one does not.\n        oldshape = self.s1.shape\n        with pytest.raises(ValueError):\n            self.s1.shape = (1,)\n        with pytest.raises(AttributeError):\n            self.s1.shape = (42,)\n        assert self.s1.shape == oldshape\n        assert self.s1.lon.shape == oldshape\n        assert self.s1.lat.shape == oldshape\n        assert self.s1.distance.shape == oldshape\n\n        # Finally, a more complicated one that checks that things get reset\n        # properly if it is not the first component that fails.\n        s2 = SphericalRepresentation(\n            self.s1.lon.copy(), self.s1.lat, self.s1.distance, copy=False\n        )\n        assert 0 not in s2.lon.strides\n        assert 0 in s2.lat.strides\n        with pytest.raises(AttributeError):\n            s2.shape = (42,)\n        assert s2.shape == oldshape\n        assert s2.lon.shape == oldshape\n        assert s2.lat.shape == oldshape\n        assert s2.distance.shape == oldshape\n        assert 0 not in s2.lon.strides\n        assert 0 in s2.lat.strides\n", "type": "function"}, {"name": "test_nodata_len_shape", "is_method": false, "class_name": null, "parameters": [], "calls": ["ICRS", "pytest.raises", "len"], "code_location": {"file": "test_frames.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/tests", "start_line": 1100, "end_line": 1104}, "code_snippet": "def test_nodata_len_shape():\n    i = ICRS()\n    assert i.shape == ()\n    with pytest.raises(TypeError, match=\"Scalar.*has no len()\"):\n        len(i)\n", "type": "function"}, {"name": "test_shape_setting", "is_method": true, "class_name": "TestSetShape", "parameters": ["self", "use_mask"], "calls": ["self.create_data", "self.t0.copy", "np.all", "np.all", "self.t1.copy", "np.all", "pytest.raises", "pytest.raises", "pytest.raises", "self.t0._time.jd1.reshape", "self.t0._time.jd2.reshape", "self.t1.jd1.reshape"], "code_location": {"file": "test_methods.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 313, "end_line": 360}, "code_snippet": "    def test_shape_setting(self, use_mask):\n        # Shape-setting should be on the object itself, since copying removes\n        # zero-strides due to broadcasting.  Hence, this should be the only\n        # test in this class.\n        self.create_data(use_mask)\n\n        t0_reshape = self.t0.copy()\n        mjd = t0_reshape.mjd  # Creates a cache of the mjd attribute\n        t0_reshape.shape = (5, 2, 5)\n        assert t0_reshape.shape == (5, 2, 5)\n        assert mjd.shape != t0_reshape.mjd.shape  # Cache got cleared\n        assert np.all(t0_reshape.jd1 == self.t0._time.jd1.reshape(5, 2, 5))\n        assert np.all(t0_reshape.jd2 == self.t0._time.jd2.reshape(5, 2, 5))\n        assert t0_reshape.location is None\n        # But if the shape doesn't work, one should get an error.\n        t0_reshape_t = t0_reshape.T\n        with pytest.raises(ValueError):\n            t0_reshape_t.shape = (12,)  # Wrong number of elements.\n        with pytest.raises(AttributeError):\n            t0_reshape_t.shape = (10, 5)  # Cannot be done without copy.\n        # check no shape was changed.\n        assert t0_reshape_t.shape == t0_reshape.T.shape\n        assert t0_reshape_t.jd1.shape == t0_reshape.T.shape\n        assert t0_reshape_t.jd2.shape == t0_reshape.T.shape\n        t1_reshape = self.t1.copy()\n        t1_reshape.shape = (2, 5, 5)\n        assert t1_reshape.shape == (2, 5, 5)\n        assert np.all(t1_reshape.jd1 == self.t1.jd1.reshape(2, 5, 5))\n        # location is a single element, so its shape should not change.\n        assert t1_reshape.location.shape == ()\n        # For reshape(5, 2, 5), the location array can remain the same.\n        # Note that we need to work directly on self.t2 here, since any\n        # copy would cause location to have the full shape.\n        self.t2.shape = (5, 2, 5)\n        assert self.t2.shape == (5, 2, 5)\n        assert self.t2.jd1.shape == (5, 2, 5)\n        assert self.t2.jd2.shape == (5, 2, 5)\n        assert self.t2.location.shape == (5, 2, 5)\n        assert self.t2.location.strides == (0, 0, 24)\n        # But for reshape(50), location would need to be copied, so this\n        # should fail.\n        oldshape = self.t2.shape\n        with pytest.raises(AttributeError):\n            self.t2.shape = (50,)\n        # check no shape was changed.\n        assert self.t2.jd1.shape == oldshape\n        assert self.t2.jd2.shape == oldshape\n        assert self.t2.location.shape == oldshape\n", "type": "function"}, {"name": "test_ndim", "is_method": true, "class_name": "TestShapeInformation", "parameters": ["self"], "calls": ["np.ndim"], "code_location": {"file": "test_quantity_non_ufuncs.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 122, "end_line": 123}, "code_snippet": "    def test_ndim(self):\n        assert np.ndim(self.q) == 2\n", "type": "function"}, {"name": "test_ndim", "is_method": true, "class_name": "TestShapeInformation", "parameters": ["self"], "calls": ["np.ndim"], "code_location": {"file": "test_function_helpers.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked/tests", "start_line": 83, "end_line": 84}, "code_snippet": "    def test_ndim(self):\n        assert np.ndim(self.ma) == 2\n", "type": "function"}], "retrieved_count": 10, "cost_time": 4.042236328125}
{"question": "Why does the boolean flag indicating whether model outputs are independent in the base class for cylindrical sky projections enable performance optimization in astronomical coordinate transformation pipelines?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "Cylindrical", "docstring": "Base class for Cylindrical projections.\n\nCylindrical projections are so-named because the surface of\nprojection is a cylinder.", "methods": [], "attributes": ["_separable"], "code_location": {"file": "projections.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 690, "end_line": 697}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "RotateCelestial2Native", "parameters": ["self", "lon", "lat", "lon_pole"], "calls": ["__init__", "super"], "code_location": {"file": "rotations.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 435, "end_line": 441}, "code_snippet": "    def __init__(self, lon, lat, lon_pole, **kwargs):\n        super().__init__(lon, lat, lon_pole, **kwargs)\n\n        # Inputs are angles on the celestial sphere\n        self.inputs = (\"alpha_C\", \"delta_C\")\n        # Outputs are angles on the native sphere\n        self.outputs = (\"phi_N\", \"theta_N\")\n", "type": "function"}, {"name": "PseudoCylindrical", "docstring": "Base class for pseudocylindrical projections.\n\nPseudocylindrical projections are like cylindrical projections\nexcept the parallels of latitude are projected at diminishing\nlengths toward the polar regions in order to reduce lateral\ndistortion there.  Consequently, the meridians are curved.", "methods": [], "attributes": ["_separable"], "code_location": {"file": "projections.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 883, "end_line": 892}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "RotateNative2Celestial", "parameters": ["self", "lon", "lat", "lon_pole"], "calls": ["__init__", "super"], "code_location": {"file": "rotations.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 361, "end_line": 364}, "code_snippet": "    def __init__(self, lon, lat, lon_pole, **kwargs):\n        super().__init__(lon, lat, lon_pole, **kwargs)\n        self.inputs = (\"phi_N\", \"theta_N\")\n        self.outputs = (\"alpha_C\", \"delta_C\")\n", "type": "function"}, {"name": "_SkyRotation", "docstring": "Base class for RotateNative2Celestial and RotateCelestial2Native.", "methods": ["__init__", "_evaluate"], "attributes": ["lon", "lat", "lon_pole"], "code_location": {"file": "rotations.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 291, "end_line": 325}, "type": "class"}, {"name": "test_Pix2Sky_CylindricalEqualArea_inverse", "is_method": false, "class_name": null, "parameters": [], "calls": ["projections.Pix2Sky_CylindricalEqualArea", "isinstance"], "code_location": {"file": "test_projections.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling/tests", "start_line": 779, "end_line": 783}, "code_snippet": "def test_Pix2Sky_CylindricalEqualArea_inverse():\n    model = projections.Pix2Sky_CylindricalEqualArea(0.567)\n    inverse = model.inverse\n    assert isinstance(inverse, projections.Sky2Pix_CylindricalEqualArea)\n    assert inverse.lam == model.lam == 0.567\n", "type": "function"}, {"name": "test_Sky2Pix_CylindricalEqualArea_inverse", "is_method": false, "class_name": null, "parameters": [], "calls": ["projections.Sky2Pix_CylindricalEqualArea", "isinstance"], "code_location": {"file": "test_projections.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling/tests", "start_line": 786, "end_line": 790}, "code_snippet": "def test_Sky2Pix_CylindricalEqualArea_inverse():\n    model = projections.Sky2Pix_CylindricalEqualArea(0.765)\n    inverse = model.inverse\n    assert isinstance(inverse, projections.Pix2Sky_CylindricalEqualArea)\n    assert inverse.lam == model.lam == 0.765\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "Sky2PixProjection", "parameters": ["self"], "calls": ["__init__", "self._update_prj", "self._prj.set", "super"], "code_location": {"file": "projections.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 234, "end_line": 245}, "code_snippet": "    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self._prj.code = self.prj_code\n        self._update_prj()\n        if not self.param_names:\n            # force initial call to Prjprm.set() for projections\n            # without parameters:\n            self._prj.set()\n\n        self.inputs = (\"phi\", \"theta\")\n        self.outputs = (\"x\", \"y\")\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "Pix2SkyProjection", "parameters": ["self"], "calls": ["__init__", "self._update_prj", "self._prj.set", "super"], "code_location": {"file": "projections.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 189, "end_line": 200}, "code_snippet": "    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self._prj.code = self.prj_code\n        self._update_prj()\n        if not self.param_names:\n            # force initial call to Prjprm.set() for projections\n            # with no parameters:\n            self._prj.set()\n\n        self.inputs = (\"x\", \"y\")\n        self.outputs = (\"phi\", \"theta\")\n", "type": "function"}, {"name": "col_fit_deriv", "is_method": true, "class_name": "CompoundModel", "parameters": ["self"], "calls": [], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 3391, "end_line": 3392}, "code_snippet": "    def col_fit_deriv(self):\n        return True\n", "type": "function"}], "retrieved_count": 10, "cost_time": 4.046764135360718}
{"question": "Why does the test mixin class for verifying cosmology read/write operations through HTML table format exist to ensure the integrity of converting cosmology instances to HTML tables when default values defined in cosmology class parameters must compensate for columns absent from the HTML table during reading?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "ReadWriteHTMLTestMixin", "docstring": "Tests for a Cosmology[Read/Write] with ``format=\"ascii.html\"``.\nThis class will not be directly called by :mod:`pytest` since its name does\nnot begin with ``Test``. To activate the contained tests this class must\nbe inherited in a subclass. Subclasses must define a :func:`pytest.fixture`\n``cosmo`` that returns/yields an instance of a |Cosmology|.\nSee ``TestCosmology`` for an example.", "methods": ["test_to_html_table_bad_index", "test_to_html_table_failed_cls", "test_to_html_table_cls", "test_readwrite_html_table_instance", "test_rename_html_table_columns", "test_readwrite_html_subclass_partial_info", "test_readwrite_html_mutlirow"], "attributes": [], "code_location": {"file": "test_html.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "start_line": 19, "end_line": 226}, "type": "class"}, {"name": "ToFromTableTestMixin", "docstring": "Tests for a Cosmology[To/From]Format with ``format=\"astropy.table\"``.\nThis class will not be directly called by :mod:`pytest` since its name does\nnot begin with ``Test``. To activate the contained tests this class must\nbe inherited in a subclass. Subclasses must define a :func:`pytest.fixture`\n``cosmo`` that returns/yields an instance of a |Cosmology|.\nSee ``TestCosmology`` for an example.", "methods": ["test_to_table_bad_index", "test_to_table_failed_cls", "test_to_table_cls", "test_to_table_in_meta", "test_to_table", "test_from_not_table", "test_tofrom_table_instance", "test_fromformat_table_subclass_partial_info", "test_tofrom_table_mutlirow", "test_tofrom_table_rename", "test_from_table_renamed_index_column", "test_is_equivalent_to_table"], "attributes": [], "code_location": {"file": "test_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "start_line": 15, "end_line": 251}, "type": "class"}, {"name": "test_readwrite_html_subclass_partial_info", "is_method": true, "class_name": "ReadWriteHTMLTestMixin", "parameters": ["self", "cosmo_cls", "cosmo", "read", "write", "latex_names", "tmp_path", "add_cu"], "calls": ["pytest.mark.skipif", "pytest.mark.parametrize", "write", "QTable.read", "tbl.write", "cosmo_cls.read", "read", "read", "got.clone"], "code_location": {"file": "test_html.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "start_line": 144, "end_line": 175}, "code_snippet": "    def test_readwrite_html_subclass_partial_info(\n        self, cosmo_cls, cosmo, read, write, latex_names, tmp_path, add_cu\n    ):\n        \"\"\"\n        Test writing from an instance and reading from that class.\n        This works with missing information.\n        \"\"\"\n        fp = tmp_path / \"test_read_html_subclass_partial_info.html\"\n\n        # test write\n        write(fp, format=\"ascii.html\", latex_names=latex_names)\n\n        # partial information\n        tbl = QTable.read(fp)\n\n        # tbl.meta.pop(\"cosmology\", None) # metadata not implemented\n        cname = \"$$T_{0}$$\" if latex_names else \"Tcmb0\"\n        del tbl[cname]  # format is not converted to original units\n        tbl.write(fp, overwrite=True)\n\n        # read with the same class that wrote fills in the missing info with\n        # the default value\n        got = cosmo_cls.read(fp, format=\"ascii.html\")\n        got2 = read(fp, format=\"ascii.html\", cosmology=cosmo_cls)\n        got3 = read(fp, format=\"ascii.html\", cosmology=cosmo_cls.__qualname__)\n\n        assert (got == got2) and (got2 == got3)  # internal consistency\n\n        # not equal, because Tcmb0 is changed, which also changes m_nu\n        assert got != cosmo\n        assert got.Tcmb0 == cosmo_cls.parameters[\"Tcmb0\"].default\n        assert got.clone(name=cosmo.name, Tcmb0=cosmo.Tcmb0, m_nu=cosmo.m_nu) == cosmo\n", "type": "function"}, {"name": "TestReadWriteHTML", "docstring": "Directly test ``read/write_html``.\nThese are not public API and are discouraged from use, in favor of\n``Cosmology.read/write(..., format=\"ascii.html\")``, but should be\ntested regardless b/c they are used internally.", "methods": ["setup_class", "test_rename_direct_html_table_columns"], "attributes": [], "code_location": {"file": "test_html.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "start_line": 229, "end_line": 261}, "type": "class"}, {"name": "ReadWriteTestMixin", "docstring": "Tests for a CosmologyRead/Write on a |Cosmology|.\nThis class will not be directly called by :mod:`pytest` since its name does\nnot begin with ``Test``. To activate the contained tests this class must\nbe inherited in a subclass. Subclasses must define a :func:`pytest.fixture`\n``cosmo`` that returns/yields an instance of a |Cosmology|.\nSee ``TestReadWriteCosmology`` or ``TestCosmology`` for examples.", "methods": ["test_readwrite_complete_info", "test_readwrite_from_subclass_complete_info"], "attributes": [], "code_location": {"file": "test_connect.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "start_line": 57, "end_line": 131}, "type": "class"}, {"name": "ReadWriteTestMixinBase", "docstring": "Tests for a Cosmology[Read/Write].\n\nThis class will not be directly called by :mod:`pytest` since its name does\nnot begin with ``Test``. To activate the contained tests this class must\nbe inherited in a subclass. Subclasses must define a :func:`pytest.fixture`\n``cosmo`` that returns/yields an instance of a |Cosmology|.\nSee ``TestCosmology`` for an example.", "methods": ["read", "write", "add_cu"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "start_line": 53, "end_line": 78}, "type": "class"}, {"name": "ReadWriteECSVTestMixin", "docstring": "Tests for a Cosmology[Read/Write] with ``format=\"ascii.ecsv\"``.\nThis class will not be directly called by :mod:`pytest` since its name does\nnot begin with ``Test``. To activate the contained tests this class must\nbe inherited in a subclass. Subclasses must define a :func:`pytest.fixture`\n``cosmo`` that returns/yields an instance of a |Cosmology|.\nSee ``TestCosmology`` for an example.", "methods": ["test_to_ecsv_bad_index", "test_to_ecsv_failed_cls", "test_to_ecsv_cls", "test_to_ecsv_in_meta", "test_readwrite_ecsv_instance", "test_readwrite_ecsv_renamed_columns", "test_readwrite_ecsv_subclass_partial_info", "test_readwrite_ecsv_mutlirow"], "attributes": [], "code_location": {"file": "test_ecsv.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "start_line": 14, "end_line": 218}, "type": "class"}, {"name": "test_readwrite_html_table_instance", "is_method": true, "class_name": "ReadWriteHTMLTestMixin", "parameters": ["self", "cosmo_cls", "cosmo", "read", "write", "tmp_path", "add_cu"], "calls": ["pytest.mark.skipif", "write", "QTable.read", "tbl.write", "read", "tbl.remove_column", "tbl.write", "read", "read", "read", "read", "pytest.raises", "read"], "code_location": {"file": "test_html.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "start_line": 63, "end_line": 118}, "code_snippet": "    def test_readwrite_html_table_instance(\n        self, cosmo_cls, cosmo, read, write, tmp_path, add_cu\n    ):\n        \"\"\"Test cosmology -> ascii.html -> cosmology.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_table_instance.html\"\n\n        # ------------\n        # To Table\n\n        write(fp, format=\"ascii.html\")\n\n        # some checks on the saved file\n        tbl = QTable.read(fp)\n        # assert tbl.meta[\"cosmology\"] == cosmo_cls.__qualname__  # metadata read not implemented\n        assert tbl[\"name\"] == cosmo.name\n\n        # ------------\n        # From Table\n\n        tbl[\"mismatching\"] = \"will error\"\n        tbl.write(fp, format=\"ascii.html\", overwrite=True)\n\n        # tests are different if the last argument is a **kwarg\n        if cosmo._init_has_kwargs:\n            got = read(fp, format=\"ascii.html\")\n\n            assert got.__class__ is cosmo_cls\n            assert got.name == cosmo.name\n            # assert \"mismatching\" not in got.meta # metadata read not implemented\n\n            return  # don't continue testing\n\n        # read with mismatching parameters errors\n        with pytest.raises(TypeError, match=\"there are unused parameters\"):\n            read(fp, format=\"ascii.html\")\n\n        # unless mismatched are moved to meta\n        got = read(fp, format=\"ascii.html\", move_to_meta=True)\n        assert got == cosmo\n        # assert got.meta[\"mismatching\"] == \"will error\" # metadata read not implemented\n\n        # it won't error if everything matches up\n        tbl.remove_column(\"mismatching\")\n        tbl.write(fp, format=\"ascii.html\", overwrite=True)\n        got = read(fp, format=\"ascii.html\")\n        assert got == cosmo\n\n        # and it will also work if the cosmology is a class\n        # Note this is not the default output of ``write``.\n        # tbl.meta[\"cosmology\"] = _COSMOLOGY_CLASSES[tbl.meta[\"cosmology\"]] #\n        # metadata read not implemented\n        got = read(fp, format=\"ascii.html\")\n        assert got == cosmo\n\n        got = read(fp)\n        assert got == cosmo\n", "type": "function"}, {"name": "ToFromRowTestMixin", "docstring": "Tests for a Cosmology[To/From]Format with ``format=\"astropy.row\"``.\nThis class will not be directly called by :mod:`pytest` since its name does\nnot begin with ``Test``. To activate the contained tests this class must\nbe inherited in a subclass. Subclasses must define a :func:`pytest.fixture`\n``cosmo`` that returns/yields an instance of a |Cosmology|.\nSee ``TestCosmologyToFromFormat`` or ``TestCosmology`` for examples.", "methods": ["test_to_row_in_meta", "test_from_not_row", "test_tofrom_row_instance", "test_tofrom_row_rename", "test_fromformat_row_subclass_partial_info", "test_is_equivalent_to_row"], "attributes": [], "code_location": {"file": "test_row.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "start_line": 14, "end_line": 132}, "type": "class"}, {"name": "test_fromformat_table_subclass_partial_info", "is_method": true, "class_name": "ToFromTableTestMixin", "parameters": ["self", "cosmo_cls", "cosmo", "from_format", "to_format"], "calls": ["to_format", "isinstance", "tbl.meta.pop", "cosmo_cls.from_format", "from_format", "from_format", "got.clone"], "code_location": {"file": "test_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/io", "start_line": 133, "end_line": 163}, "code_snippet": "    def test_fromformat_table_subclass_partial_info(\n        self, cosmo_cls, cosmo, from_format, to_format\n    ):\n        \"\"\"\n        Test writing from an instance and reading from that class.\n        This works with missing information.\n        \"\"\"\n        # test to_format\n        tbl = to_format(\"astropy.table\")\n        assert isinstance(tbl, QTable)\n\n        # partial information\n        tbl.meta.pop(\"cosmology\", None)\n        del tbl[\"Tcmb0\"]\n\n        # read with the same class that wrote fills in the missing info with\n        # the default value\n        got = cosmo_cls.from_format(tbl, format=\"astropy.table\")\n        got2 = from_format(tbl, format=\"astropy.table\", cosmology=cosmo_cls)\n        got3 = from_format(\n            tbl, format=\"astropy.table\", cosmology=cosmo_cls.__qualname__\n        )\n\n        assert (got == got2) and (got2 == got3)  # internal consistency\n\n        # not equal, because Tcmb0 is changed, which also changes m_nu\n        assert got != cosmo\n        assert got.Tcmb0 == cosmo_cls.parameters[\"Tcmb0\"].default\n        assert got.clone(name=cosmo.name, Tcmb0=cosmo.Tcmb0, m_nu=cosmo.m_nu) == cosmo\n        # but the metadata is the same\n        assert got.meta == cosmo.meta\n", "type": "function"}], "retrieved_count": 10, "cost_time": 4.071702480316162}
{"question": "Why does the boolean caching control parameter in the wrapper class initialization method for convolution models create a trade-off between memory consumption and computational overhead when the discretization step size for integration approximation limits is dynamically increased during runtime?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "Convolution", "docstring": "Wrapper class for a convolution model.\n\nParameters\n----------\noperator: tuple\n    The SPECIAL_OPERATORS entry for the convolution being used.\nmodel : Model\n    The model for the convolution.\nkernel: Model\n    The kernel model for the convolution.\nbounding_box : tuple\n    A bounding box to define the limits of the integration\n    approximation for the convolution.\nresolution : float\n    The resolution for the approximation of the convolution.\ncache : bool, optional\n    Allow convolution computation to be cached for reuse. This is\n    enabled by default.\n\nNotes\n-----\nThis is wrapper is necessary to handle the limitations of the\npseudospectral convolution binary operator implemented in\nastropy.convolution under `~astropy.convolution.convolve_fft`. In this\n`~astropy.convolution.convolve_fft` it is assumed that the inputs ``array``\nand ``kernel`` span a sufficient portion of the support of the functions of\nthe convolution. Consequently, the ``Compound`` created by the\n`~astropy.convolution.convolve_models` function makes the assumption that\none should pass an input array that sufficiently spans this space. This means\nthat slightly different input arrays to this model will result in different\noutputs, even on points of intersection between these arrays.\n\nThis issue is solved by requiring a ``bounding_box`` together with a\nresolution so that one can pre-calculate the entire domain and then\n(by default) cache the convolution values. The function then just\ninterpolates the results from this cache.", "methods": ["__init__", "clear_cache", "_get_convolution", "_convolution_inputs", "_convolution_outputs", "__call__"], "attributes": [], "code_location": {"file": "convolution.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 11, "end_line": 116}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "Convolution", "parameters": ["self", "operator", "model", "kernel", "bounding_box", "resolution", "cache"], "calls": ["__init__", "super"], "code_location": {"file": "convolution.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 51, "end_line": 59}, "code_snippet": "    def __init__(self, operator, model, kernel, bounding_box, resolution, cache=True):\n        super().__init__(operator, model, kernel)\n\n        self.bounding_box = bounding_box\n        self._resolution = resolution\n\n        self._cache_convolution = cache\n        self._kwargs = None\n        self._convolution = None\n", "type": "function"}, {"name": "test_clear_cache", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.mark.skipif", "Const1D", "Const1D", "convolve_models_fft", "model", "model.clear_cache", "results.all", "all", "np.array"], "code_location": {"file": "test_convolution.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling/tests", "start_line": 12, "end_line": 27}, "code_snippet": "def test_clear_cache():\n    m1 = Const1D()\n    m2 = Const1D()\n\n    model = convolve_models_fft(m1, m2, (-1, 1), 0.01)\n    assert model._kwargs is None\n    assert model._convolution is None\n\n    results = model(0)\n    assert results.all() == np.array([1.0]).all()\n    assert model._kwargs is not None\n    assert model._convolution is not None\n\n    model.clear_cache()\n    assert model._kwargs is None\n    assert model._convolution is None\n", "type": "function"}, {"name": "_get_convolution", "is_method": true, "class_name": "Convolution", "parameters": ["self"], "calls": ["self.bounding_box.domain", "np.meshgrid", "__call__", "RegularGridInterpolator", "super"], "code_location": {"file": "convolution.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 68, "end_line": 85}, "code_snippet": "    def _get_convolution(self, **kwargs):\n        if (self._convolution is None) or (self._kwargs != kwargs):\n            domain = self.bounding_box.domain(self._resolution)\n            mesh = np.meshgrid(*domain)\n            data = super().__call__(*mesh, **kwargs)\n\n            from scipy.interpolate import RegularGridInterpolator\n\n            convolution = RegularGridInterpolator(domain, data)\n\n            if self._cache_convolution:\n                self._kwargs = kwargs\n                self._convolution = convolution\n\n        else:\n            convolution = self._convolution\n\n        return convolution\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "Kernel1D", "parameters": ["self", "model", "x_size", "array"], "calls": ["__init__", "discretize_model", "TypeError", "TypeError", "super", "int", "TypeError", "int", "int", "int", "int"], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/convolution", "start_line": 220, "end_line": 246}, "code_snippet": "    def __init__(self, model=None, x_size=None, array=None, **kwargs):\n        # Initialize from model\n        if self._model:\n            if array is not None:\n                # Reject \"array\" keyword for kernel models, to avoid them not being\n                # populated as expected.\n                raise TypeError(\"Array argument not allowed for kernel models.\")\n\n            if x_size is None:\n                x_size = self._default_size\n            elif x_size != int(x_size):\n                raise TypeError(\"x_size should be an integer\")\n\n            # Set ranges where to evaluate the model\n\n            if x_size % 2 == 0:  # even kernel\n                x_range = (-(int(x_size)) // 2 + 0.5, (int(x_size)) // 2 + 0.5)\n            else:  # odd kernel\n                x_range = (-(int(x_size) - 1) // 2, (int(x_size) - 1) // 2 + 1)\n\n            array = discretize_model(self._model, x_range, **kwargs)\n\n        # Initialize from array\n        elif array is None:\n            raise TypeError(\"Must specify either array or model.\")\n\n        super().__init__(array)\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "Kernel2D", "parameters": ["self", "model", "x_size", "y_size", "array"], "calls": ["__init__", "discretize_model", "TypeError", "TypeError", "super", "int", "TypeError", "int", "TypeError", "int", "int", "int", "int", "int", "int", "int", "int"], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/convolution", "start_line": 285, "end_line": 320}, "code_snippet": "    def __init__(self, model=None, x_size=None, y_size=None, array=None, **kwargs):\n        # Initialize from model\n        if self._model:\n            if array is not None:\n                # Reject \"array\" keyword for kernel models, to avoid them not being\n                # populated as expected.\n                raise TypeError(\"Array argument not allowed for kernel models.\")\n            if x_size is None:\n                x_size = self._default_size\n            elif x_size != int(x_size):\n                raise TypeError(\"x_size should be an integer\")\n\n            if y_size is None:\n                y_size = x_size\n            elif y_size != int(y_size):\n                raise TypeError(\"y_size should be an integer\")\n\n            # Set ranges where to evaluate the model\n\n            if x_size % 2 == 0:  # even kernel\n                x_range = (-(int(x_size)) // 2 + 0.5, (int(x_size)) // 2 + 0.5)\n            else:  # odd kernel\n                x_range = (-(int(x_size) - 1) // 2, (int(x_size) - 1) // 2 + 1)\n\n            if y_size % 2 == 0:  # even kernel\n                y_range = (-(int(y_size)) // 2 + 0.5, (int(y_size)) // 2 + 0.5)\n            else:  # odd kernel\n                y_range = (-(int(y_size) - 1) // 2, (int(y_size) - 1) // 2 + 1)\n\n            array = discretize_model(self._model, x_range, y_range, **kwargs)\n\n        # Initialize from array\n        elif array is None:\n            raise TypeError(\"Must specify either array or model.\")\n\n        super().__init__(array)\n", "type": "function"}, {"name": "discretize_oversample_1D", "is_method": false, "class_name": null, "parameters": ["model", "x_range", "factor"], "calls": ["np.linspace", "model", "np.reshape", "values.mean", "int"], "code_location": {"file": "utils.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/convolution", "start_line": 275, "end_line": 290}, "code_snippet": "def discretize_oversample_1D(model, x_range, factor=10):\n    \"\"\"\n    Discretize model by taking the average on an oversampled grid.\n    \"\"\"\n    # Evaluate model on oversampled grid\n    x = np.linspace(\n        x_range[0] - 0.5 * (1 - 1 / factor),\n        x_range[1] - 0.5 * (1 + 1 / factor),\n        num=int((x_range[1] - x_range[0]) * factor),\n    )\n\n    values = model(x)\n\n    # Reshape and compute mean\n    values = np.reshape(values, (x.size // factor, factor))\n    return values.mean(axis=1)\n", "type": "function"}, {"name": "convolve_models_fft", "is_method": false, "class_name": null, "parameters": ["model", "kernel", "bounding_box", "resolution", "cache"], "calls": ["SPECIAL_OPERATORS.add", "Convolution", "partial"], "code_location": {"file": "convolve.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/convolution", "start_line": 1028, "end_line": 1059}, "code_snippet": "def convolve_models_fft(model, kernel, bounding_box, resolution, cache=True, **kwargs):\n    \"\"\"\n    Convolve two models using `~astropy.convolution.convolve_fft`.\n\n    Parameters\n    ----------\n    model : `~astropy.modeling.core.Model`\n        Functional model\n    kernel : `~astropy.modeling.core.Model`\n        Convolution kernel\n    bounding_box : tuple\n        The bounding box which encompasses enough of the support of both\n        the ``model`` and ``kernel`` so that an accurate convolution can be\n        computed.\n    resolution : float\n        The resolution that one wishes to approximate the convolution\n        integral at.\n    cache : optional, bool\n        Default value True. Allow for the storage of the convolution\n        computation for later reuse.\n    **kwargs : dict\n        Keyword arguments to be passed either to `~astropy.convolution.convolve`\n        or `~astropy.convolution.convolve_fft` depending on ``mode``.\n\n    Returns\n    -------\n    default : `~astropy.modeling.core.CompoundModel`\n        Convolved model\n    \"\"\"\n    operator = SPECIAL_OPERATORS.add(\"convolve_fft\", partial(convolve_fft, **kwargs))\n\n    return Convolution(operator, model, kernel, bounding_box, resolution, cache)\n", "type": "function"}, {"name": "test__fcache", "is_method": false, "class_name": null, "parameters": [], "calls": ["OrthoPolynomialBase", "Hermite2D", "Legendre2D", "Chebyshev2D", "pytest.raises", "model._fcache", "model._fcache", "model._fcache", "model._fcache", "np.asanyarray", "np.asanyarray", "np.asanyarray", "np.asanyarray", "np.asanyarray", "np.asanyarray", "np.asanyarray", "np.asanyarray", "np.asanyarray", "np.asanyarray", "np.asanyarray", "np.asanyarray", "np.asanyarray", "np.asanyarray", "np.asanyarray", "np.asanyarray", "np.asanyarray", "np.asanyarray"], "code_location": {"file": "test_polynomial.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling/tests", "start_line": 634, "end_line": 668}, "code_snippet": "def test__fcache():\n    model = OrthoPolynomialBase(x_degree=2, y_degree=2)\n    MESSAGE = r\"Subclasses should implement this\"\n    with pytest.raises(NotImplementedError, match=MESSAGE):\n        model._fcache(np.asanyarray(1), np.asanyarray(1))\n\n    model = Hermite2D(x_degree=2, y_degree=2)\n    assert model._fcache(np.asanyarray(1), np.asanyarray(1)) == {\n        0: np.asanyarray(1),\n        1: 2,\n        3: np.asanyarray(1),\n        4: 2,\n        2: 2.0,\n        5: -4.0,\n    }\n\n    model = Legendre2D(x_degree=2, y_degree=2)\n    assert model._fcache(np.asanyarray(1), np.asanyarray(1)) == {\n        0: np.asanyarray(1),\n        1: np.asanyarray(1),\n        2: 1.0,\n        3: np.asanyarray(1),\n        4: np.asanyarray(1),\n        5: 1.0,\n    }\n\n    model = Chebyshev2D(x_degree=2, y_degree=2)\n    assert model._fcache(np.asanyarray(1), np.asanyarray(1)) == {\n        0: np.asanyarray(1),\n        1: np.asanyarray(1),\n        2: 1.0,\n        3: np.asanyarray(1),\n        4: np.asanyarray(1),\n        5: 1.0,\n    }\n", "type": "function"}, {"name": "discretize_integrate_1D", "is_method": false, "class_name": null, "parameters": ["model", "x_range"], "calls": ["np.arange", "np.array", "range", "np.append", "quad"], "code_location": {"file": "utils.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/convolution", "start_line": 318, "end_line": 331}, "code_snippet": "def discretize_integrate_1D(model, x_range):\n    \"\"\"\n    Discretize model by integrating numerically the model over the bin.\n    \"\"\"\n    from scipy.integrate import quad\n\n    # Set up grid\n    x = np.arange(x_range[0] - 0.5, x_range[1] + 0.5)\n    values = np.array([])\n\n    # Integrate over all bins\n    for i in range(x.size - 1):\n        values = np.append(values, quad(model, x[i], x[i + 1])[0])\n    return values\n", "type": "function"}], "retrieved_count": 10, "cost_time": 4.111511468887329}
{"question": "Why does the serialization state preparation method use the mixin-safe copying function conditionally only for columns that are not instances of the base column class?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "col_copy", "is_method": false, "class_name": null, "parameters": ["col", "copy_indices"], "calls": ["isinstance", "col.copy", "hasattr", "col.copy", "deepcopy", "deepcopy", "index.replace_col"], "code_location": {"file": "column.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 67, "end_line": 96}, "code_snippet": "def col_copy(col, copy_indices=True):\n    \"\"\"\n    Mixin-safe version of Column.copy() (with copy_data=True).\n\n    Parameters\n    ----------\n    col : Column or mixin column\n        Input column\n    copy_indices : bool\n        Copy the column ``indices`` attribute\n\n    Returns\n    -------\n    col : Copy of input column\n    \"\"\"\n    if isinstance(col, BaseColumn):\n        return col.copy()\n\n    newcol = col.copy() if hasattr(col, \"copy\") else deepcopy(col)\n    # If the column has info defined, we copy it and adjust any indices\n    # to point to the copied column.  By guarding with the if statement,\n    # we avoid side effects (of creating the default info instance).\n    if \"info\" in col.__dict__:\n        newcol.info = col.info\n        if copy_indices and col.info.indices:\n            newcol.info.indices = deepcopy(col.info.indices)\n            for index in newcol.info.indices:\n                index.replace_col(col, newcol)\n\n    return newcol\n", "type": "function"}, {"name": "__getstate__", "is_method": true, "class_name": "Table", "parameters": ["self"], "calls": ["OrderedDict", "self.columns.items", "isinstance", "col_copy"], "code_location": {"file": "table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 965, "end_line": 970}, "code_snippet": "    def __getstate__(self):\n        columns = OrderedDict(\n            (key, col if isinstance(col, BaseColumn) else col_copy(col))\n            for key, col in self.columns.items()\n        )\n        return (columns, self.meta)\n", "type": "function"}, {"name": "_represent_mixin_as_column", "is_method": false, "class_name": null, "parameters": ["col", "name", "new_cols", "mixin_cols", "exclude_classes"], "calls": ["col.info._represent_as_dict", "new_cols.append", "getattr", "nontrivial", "_represent_mixin_as_column", "SerializedColumn", "isinstance", "obj_attrs.setdefault", "obj_attrs.items", "has_info_class", "col_cls", "obj_attrs.pop", "getattr", "hasattr", "np.any", "np.zeros"], "code_location": {"file": "serialize.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 117, "end_line": 230}, "code_snippet": "def _represent_mixin_as_column(col, name, new_cols, mixin_cols, exclude_classes=()):\n    \"\"\"Carry out processing needed to serialize ``col`` in an output table\n    consisting purely of plain ``Column`` or ``MaskedColumn`` columns.  This\n    relies on the object determine if any transformation is required and may\n    depend on the ``serialize_method`` and ``serialize_context`` context\n    variables.  For instance a ``MaskedColumn`` may be stored directly to\n    FITS, but can also be serialized as separate data and mask columns.\n\n    This function builds up a list of plain columns in the ``new_cols`` arg (which\n    is passed as a persistent list).  This includes both plain columns from the\n    original table and plain columns that represent data from serialized columns\n    (e.g. ``jd1`` and ``jd2`` arrays from a ``Time`` column).\n\n    For serialized columns the ``mixin_cols`` dict is updated with required\n    attributes and information to subsequently reconstruct the table.\n\n    Table mixin columns are always serialized and get represented by one\n    or more data columns.  In earlier versions of the code *only* mixin\n    columns were serialized, hence the use within this code of \"mixin\"\n    to imply serialization.  Starting with version 3.1, the non-mixin\n    ``MaskedColumn`` can also be serialized.\n    \"\"\"\n    obj_attrs = col.info._represent_as_dict()\n\n    # If serialization is not required (see function docstring above)\n    # or explicitly specified as excluded, then treat as a normal column.\n    if not obj_attrs or col.__class__ in exclude_classes:\n        new_cols.append(col)\n        return\n\n    # Subtlety here is handling mixin info attributes.  The basic list of such\n    # attributes is: 'name', 'unit', 'dtype', 'format', 'description', 'meta'.\n    # - name: handled directly [DON'T store]\n    # - unit: DON'T store if this is a parent attribute\n    # - dtype: captured in plain Column if relevant [DON'T store]\n    # - format: possibly irrelevant but settable post-object creation [DO store]\n    # - description: DO store\n    # - meta: DO store\n    info = {}\n    for attr, nontrivial in (\n        (\"unit\", lambda x: x is not None and x != \"\"),\n        (\"format\", lambda x: x is not None),\n        (\"description\", lambda x: x is not None),\n        (\"meta\", lambda x: x),\n    ):\n        col_attr = getattr(col.info, attr)\n        if nontrivial(col_attr):\n            info[attr] = col_attr\n\n    # Find column attributes that have the same length as the column itself.\n    # These will be stored in the table as new columns (aka \"data attributes\").\n    # Examples include SkyCoord.ra (what is typically considered the data and is\n    # always an array) and Skycoord.obs_time (which can be a scalar or an\n    # array).\n    data_attrs = [\n        key\n        for key, value in obj_attrs.items()\n        if getattr(value, \"shape\", ())[:1] == col.shape[:1]\n    ]\n\n    for data_attr in data_attrs:\n        data = obj_attrs[data_attr]\n\n        # New column name combines the old name and attribute\n        # (e.g. skycoord.ra, skycoord.dec).unless it is the primary data\n        # attribute for the column (e.g. value for Quantity or data for\n        # MaskedColumn).  For primary data, we attempt to store any info on\n        # the format, etc., on the column, but not for ancillary data (e.g.,\n        # no sense to use a float format for a mask).\n        is_primary = data_attr == col.info._represent_as_dict_primary_data\n        if is_primary:\n            new_name = name\n            new_info = info\n        else:\n            new_name = name + \".\" + data_attr\n            new_info = {}\n\n        if not has_info_class(data, MixinInfo):\n            col_cls = (\n                MaskedColumn\n                if (\n                    hasattr(data, \"mask\")\n                    and np.any(data.mask != np.zeros((), data.mask.dtype))\n                )\n                else Column\n            )\n            data = col_cls(data, name=new_name, **new_info)\n            if is_primary:\n                # Don't store info in the __serialized_columns__ dict for this column\n                # since this is redundant with info stored on the new column.\n                info = {}\n\n        # Recurse. If this is anything that needs further serialization (i.e.,\n        # a Mixin column, a structured Column, a MaskedColumn for which mask is\n        # stored, etc.), it will define obj_attrs[new_name]. Otherwise, it will\n        # just add to new_cols and all we have to do is to link to the new name.\n        _represent_mixin_as_column(data, new_name, new_cols, obj_attrs)\n        obj_attrs[data_attr] = SerializedColumn(\n            obj_attrs.pop(new_name, {\"name\": new_name})\n        )\n\n    # Strip out from info any attributes defined by the parent,\n    # and store whatever remains.\n    for attr in col.info.attrs_from_parent:\n        if attr in info:\n            del info[attr]\n    if info:\n        obj_attrs[\"__info__\"] = info\n\n    # Store the fully qualified class name\n    if not isinstance(col, SerializedColumn):\n        obj_attrs.setdefault(\"__class__\", col.__module__ + \".\" + col.__class__.__name__)\n\n    mixin_cols[name] = obj_attrs\n", "type": "function"}, {"name": "test_info_preserved_pickle_copy_init", "is_method": false, "class_name": null, "parameters": ["mixin_cols"], "calls": ["pickle.loads", "c.__class__", "pickle.dumps", "func", "getattr", "getattr"], "code_location": {"file": "test_mixin.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table/tests", "start_line": 497, "end_line": 521}, "code_snippet": "def test_info_preserved_pickle_copy_init(mixin_cols):\n    \"\"\"\n    Test copy, pickle, and init from class roundtrip preserve info.  This\n    tests not only the mixin classes but a regular column as well.\n    \"\"\"\n\n    def pickle_roundtrip(c):\n        # protocol=5 matches the default for Python 3.14 and later\n        # and is needed to preserve byteorder (available since Python 3.8)\n        return pickle.loads(pickle.dumps(c, protocol=5))\n\n    def init_from_class(c):\n        return c.__class__(c)\n\n    attrs = (\"name\", \"unit\", \"dtype\", \"format\", \"description\", \"meta\")\n    for colname in (\"i\", \"m\"):\n        m = mixin_cols[colname]\n        m.info.name = colname\n        m.info.format = \"{0}\"\n        m.info.description = \"d\"\n        m.info.meta = {\"a\": 1}\n        for func in (copy.copy, copy.deepcopy, pickle_roundtrip, init_from_class):\n            m2 = func(m)\n            for attr in attrs:\n                assert getattr(m2.info, attr) == getattr(m.info, attr)\n", "type": "function"}, {"name": "__call__", "is_method": true, "class_name": "EcsvOutputter", "parameters": ["self", "cols", "meta"], "calls": ["__call__", "serialize._construct_mixins_from_columns", "super"], "code_location": {"file": "ecsv.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/ascii", "start_line": 254, "end_line": 263}, "code_snippet": "    def __call__(self, cols, meta):\n        # Convert to a Table with all plain Column subclass columns\n        out = super().__call__(cols, meta)\n\n        # If mixin columns exist (based on the special '__mixin_columns__'\n        # key in the table ``meta``), then use that information to construct\n        # appropriate mixin columns and remove the original data columns.\n        # If no __mixin_columns__ exists then this function just passes back\n        # the input table.\n        return serialize._construct_mixins_from_columns(out)\n", "type": "function"}, {"name": "_construct_mixin_from_obj_attrs_and_info", "is_method": false, "class_name": null, "parameters": ["obj_attrs", "info"], "calls": ["obj_attrs.pop", "cls_full_name.startswith", "cls_full_name.rpartition", "import_module", "getattr", "info.items", "cls.info._construct_from_dict", "info.items", "SerializedColumn", "ValueError", "setattr", "cls_full_name.startswith"], "code_location": {"file": "serialize.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 327, "end_line": 359}, "code_snippet": "def _construct_mixin_from_obj_attrs_and_info(obj_attrs, info):\n    # If this is a supported class then import the class and run\n    # the _construct_from_col method.  Prevent accidentally running\n    # untrusted code by only importing known astropy classes.\n    cls_full_name = obj_attrs.pop(\"__class__\", None)\n    if cls_full_name is None:\n        # We're dealing with a SerializedColumn holding columns, stored in\n        # obj_attrs. For this case, info holds the name (and nothing else).\n        mixin = SerializedColumn(obj_attrs)\n        mixin.info.name = info[\"name\"]\n        return mixin\n\n    # We translate locally created skyoffset frames and treat all\n    # built-in frames as known.\n    if cls_full_name.startswith(\"abc.SkyOffset\"):\n        cls_full_name = \"astropy.coordinates.SkyOffsetFrame\"\n    elif (\n        cls_full_name not in __construct_mixin_classes\n        and not cls_full_name.startswith(\"astropy.coordinates.builtin_frames\")\n    ):\n        raise ValueError(f\"unsupported class for construct {cls_full_name}\")\n\n    mod_name, _, cls_name = cls_full_name.rpartition(\".\")\n    module = import_module(mod_name)\n    cls = getattr(module, cls_name)\n    for attr, value in info.items():\n        if attr in cls.info.attrs_from_parent:\n            obj_attrs[attr] = value\n    mixin = cls.info._construct_from_dict(obj_attrs)\n    for attr, value in info.items():\n        if attr not in obj_attrs:\n            setattr(mixin.info, attr, value)\n    return mixin\n", "type": "function"}, {"name": "_construct_mixin_from_columns", "is_method": false, "class_name": null, "parameters": ["new_name", "obj_attrs", "out"], "calls": ["obj_attrs.items", "data_attrs_map.values", "sorted", "out.colnames.index", "obj_attrs.pop", "_construct_mixin_from_obj_attrs_and_info", "out.add_column", "isinstance", "len", "getattr", "nontrivial", "isinstance", "_construct_mixin_from_columns"], "code_location": {"file": "serialize.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 389, "end_line": 441}, "code_snippet": "def _construct_mixin_from_columns(new_name, obj_attrs, out):\n    data_attrs_map = {}\n    for name, val in obj_attrs.items():\n        if isinstance(val, SerializedColumn):\n            # A SerializedColumn can just link to a serialized column using a name\n            # (e.g., time.jd1), or itself be a mixin (e.g., coord.obstime).  Note\n            # that in principle a mixin could have include a column called 'name',\n            # hence we check whether the value is actually a string (see gh-13232).\n            if \"name\" in val and isinstance(val[\"name\"], str):\n                data_attrs_map[val[\"name\"]] = name\n            else:\n                out_name = f\"{new_name}.{name}\"\n                _construct_mixin_from_columns(out_name, val, out)\n                data_attrs_map[out_name] = name\n\n    for name in data_attrs_map.values():\n        del obj_attrs[name]\n\n    # The order of data_attrs_map may not match the actual order, as it is set\n    # by the yaml description.  So, sort names by position in the serialized table.\n    # Keep the index of the first column, so we can insert the new one there later.\n    names = sorted(data_attrs_map, key=out.colnames.index)\n    idx = out.colnames.index(names[0])\n\n    # Name is the column name in the table (e.g. \"coord.ra\") and\n    # data_attr is the object attribute name  (e.g. \"ra\").  A different\n    # example would be a formatted time object that would have (e.g.)\n    # \"time_col\" and \"value\", respectively.\n    for name in names:\n        obj_attrs[data_attrs_map[name]] = out[name]\n        del out[name]\n\n    info = obj_attrs.pop(\"__info__\", {})\n    if len(names) == 1:\n        # col is the first and only serialized column; in that case, use info\n        # stored on the column. First step is to get that first column which\n        # has been moved from `out` to `obj_attrs` above.\n        col = obj_attrs[data_attrs_map[name]]\n\n        # Now copy the relevant attributes\n        for attr, nontrivial in (\n            (\"unit\", lambda x: x not in (None, \"\")),\n            (\"format\", lambda x: x is not None),\n            (\"description\", lambda x: x is not None),\n            (\"meta\", lambda x: x),\n        ):\n            col_attr = getattr(col.info, attr)\n            if nontrivial(col_attr):\n                info[attr] = col_attr\n\n    info[\"name\"] = new_name\n    col = _construct_mixin_from_obj_attrs_and_info(obj_attrs, info)\n    out.add_column(col, index=idx)\n", "type": "function"}, {"name": "iter_str_vals", "is_method": true, "class_name": "BaseColumnInfo", "parameters": ["self"], "calls": ["_pformat_col_iter"], "code_location": {"file": "data_info.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils", "start_line": 582, "end_line": 593}, "code_snippet": "    def iter_str_vals(self):\n        \"\"\"\n        This is a mixin-safe version of Column.iter_str_vals.\n        \"\"\"\n        col = self._parent\n        if self.parent_table is None:\n            from astropy.table.column import FORMATTER as formatter\n        else:\n            formatter = self.parent_table.formatter\n\n        _pformat_col_iter = formatter._pformat_col_iter\n        yield from _pformat_col_iter(col, -1, False, False, {})\n", "type": "function"}, {"name": "__deepcopy__", "is_method": true, "class_name": "ColDefs", "parameters": ["self", "memo"], "calls": ["self.__class__", "copy.deepcopy"], "code_location": {"file": "column.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits", "start_line": 1635, "end_line": 1636}, "code_snippet": "    def __deepcopy__(self, memo):\n        return self.__class__([copy.deepcopy(c, memo) for c in self.columns])\n", "type": "function"}, {"name": "__setstate__", "is_method": true, "class_name": "FITS_rec", "parameters": ["self", "state"], "calls": ["__setstate__", "weakref.WeakSet", "zip", "setattr", "super"], "code_location": {"file": "fitsrec.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits", "start_line": 182, "end_line": 192}, "code_snippet": "    def __setstate__(self, state):\n        meta = state[-1]\n        column_state = state[-2]\n        state = state[:-2]\n\n        super().__setstate__(state)\n\n        self._col_weakrefs = weakref.WeakSet()\n\n        for attr, value in zip(meta, column_state):\n            setattr(self, attr, value)\n", "type": "function"}], "retrieved_count": 10, "cost_time": 4.138901233673096}
{"question": "How does the cache-clearing mechanism in the test verifying cache coherence between time objects and their array slices ensure consistency across views sharing underlying data arrays without creating circular references that prevent garbage collection?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_cache_coherence_with_views", "is_method": false, "class_name": null, "parameters": ["masked"], "calls": ["pytest.mark.parametrize", "Time", "np.may_share_memory", "assert_array_equal", "assert_array_equal", "assert_array_equal", "assert_array_equal", "assert_array_equal", "gc.collect", "t.flatten", "assert_array_equal", "np.may_share_memory", "set", "set", "np.all", "id", "id", "id"], "code_location": {"file": "test_basic.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 1847, "end_line": 1897}, "code_snippet": "def test_cache_coherence_with_views(masked):\n    # Create a time instance and a slice.\n    t = Time([\"2001:020\", \"2001:040\", \"2001:060\", \"2001:080\"], out_subfmt=\"date\")\n    if masked:\n        # Masked arrays do not own their data directly so worth testing.\n        t[1] = np.ma.masked\n    t01 = t[:2]\n    # These should share memory.\n    assert np.may_share_memory(t._time.jd1, t01._time.jd1)\n    # And have the same value, even though those are not shared,\n    # as they are calculated separately.\n    assert_array_equal(t01.value, t.value[:2])\n    assert not np.may_share_memory(t01.value, t.value)\n    # Check that we now have cached values.\n    assert \"format\" in t.cache\n    assert \"format\" in t01.cache\n    # This should still be the case if one or the other is set\n    # (regression test for gh-15452).\n    t[0] = \"1999:099\"\n    # Because the setting deletes all related caches.\n    assert not t.cache\n    assert not t01.cache\n    assert_array_equal(t01.jd1[:2], t.jd1[:2])\n    assert_array_equal(t01.value, t.value[:2])\n    # And also the other way around.\n    t01[1] = \"1999:100\"\n    assert not t.cache\n    assert not t01.cache\n    assert_array_equal(t01.jd1[:2], t.jd1[:2])\n    assert_array_equal(t01.value, t.value[:2])\n    # This works because they keep track of each other.\n    assert t01._id_cache is t._id_cache\n    assert set(t._id_cache) == {id(t), id(t01)}\n    # Check that our cache implementation does not keep objects alive\n    # unintentionally (i.e., that garbage collection works).\n    del t01\n    gc.collect()\n    assert set(t._id_cache) == {id(t)}\n    # Also check that deleting t01 did not remove the cache of t too.\n    assert \"format\" in t.cache\n    # If a copy was made, the cache is not shared.\n    tf = t.flatten()\n    assert \"format\" in t.cache\n    assert not tf.cache\n    assert_array_equal(tf.value, t.value)\n    assert \"format\" in tf.cache\n    t[0] = \"2000:001\"\n    assert not t.cache\n    assert \"format\" in tf.cache\n    assert not np.all(tf.value == t.value)\n    assert tf._id_cache is not t._id_cache\n", "type": "function"}, {"name": "test_cache_not_shared", "is_method": true, "class_name": "TestPickle", "parameters": ["self"], "calls": ["Time", "pickle.dumps", "pickle.loads"], "code_location": {"file": "test_pickle.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 29, "end_line": 41}, "code_snippet": "    def test_cache_not_shared(self):\n        t = Time([\"2001:020\", \"2001:040\", \"2001:060\", \"2001:080\"], out_subfmt=\"date\")\n        # Ensure something is in the cache.\n        t.value\n        assert \"format\" in t.cache\n        td = pickle.dumps(t)\n        assert \"format\" in t.cache\n        tl = pickle.loads(td)\n        assert \"format\" in t.cache\n        assert \"format\" not in tl.cache\n        t[0] = \"1999:099\"\n        assert t.value[0] == \"1999:099\"\n        assert tl.value[0] == \"2001:020\"\n", "type": "function"}, {"name": "test_inplace_array", "is_method": false, "class_name": null, "parameters": [], "calls": ["ICRS", "repr", "i.cache.clear", "assert_allclose", "assert_allclose", "len"], "code_location": {"file": "test_frames.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/tests", "start_line": 1395, "end_line": 1412}, "code_snippet": "def test_inplace_array():\n    i = ICRS([[1, 2], [3, 4]] * u.deg, [[10, 20], [30, 40]] * u.deg)\n\n    # Add an in frame units version of the rep to the cache.\n    repr(i)\n\n    # Check that repr() has added a rep to the cache\n    assert len(i.cache[\"representation\"]) == 2\n\n    # Modify the data\n    i.data.lon[:, 0] = [100, 200] * u.deg\n\n    # Clear the cache\n    i.cache.clear()\n\n    # This will use a second (potentially cached rep)\n    assert_allclose(i.ra, [[100, 2], [200, 4]] * u.deg)\n    assert_allclose(i.dec, [[10, 20], [30, 40]] * u.deg)\n", "type": "function"}, {"name": "test_cache_clearing", "is_method": true, "class_name": "TestFrame", "parameters": ["self"], "calls": ["self.fk5.copy", "assert_array_equal", "assert_array_equal", "assert_array_equal", "assert_array_equal", "repr", "np.zeros", "np.zeros", "repr", "repr"], "code_location": {"file": "test_masked.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/tests", "start_line": 234, "end_line": 244}, "code_snippet": "    def test_cache_clearing(self):\n        mfk5 = self.fk5.copy()\n        assert_array_equal(mfk5.ra.mask, self.mask_lon)\n        assert \"\" in repr(mfk5)\n        mfk5[...] = np.ma.nomask\n        assert_array_equal(mfk5.data.mask, np.zeros(mfk5.shape, bool))\n        assert_array_equal(mfk5.ra.mask, np.zeros(mfk5.shape, bool))\n        assert \"\" not in repr(mfk5)\n        mfk5[...] = self.fk5\n        assert_array_equal(mfk5.ra.mask, self.mask_lon)\n        assert \"\" in repr(mfk5)\n", "type": "function"}, {"name": "test_getitem", "is_method": true, "class_name": "TestBasic", "parameters": ["self"], "calls": ["np.arange", "Time", "Time", "np.all", "Time", "np.all", "np.all", "Time", "np.all", "Time", "np.all", "Time", "np.all", "np.arange", "np.all", "Time", "np.all", "np.array", "np.arange", "Time", "np.all", "np.all", "np.all", "np.all", "np.all", "np.all", "np.all", "np.all", "Time", "np.all", "np.all", "len", "t6.location.view", "t6.location.view", "t4.location.view", "t4.location.view", "np.arange", "np.arange", "np.arange", "np.arange", "len", "len", "len", "len"], "code_location": {"file": "test_basic.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 164, "end_line": 261}, "code_snippet": "    def test_getitem(self):\n        \"\"\"Test that Time objects holding arrays are properly subscriptable,\n        set isscalar as appropriate, and also subscript delta_ut1_utc, etc.\"\"\"\n\n        mjd = np.arange(50000, 50010)\n        t = Time(mjd, format=\"mjd\", scale=\"utc\", location=(\"45d\", \"50d\"))\n        t1 = t[3]\n        assert t1.isscalar is True\n        assert t1._time.jd1 == t._time.jd1[3]\n        assert t1.location is t.location\n        t1a = Time(mjd[3], format=\"mjd\", scale=\"utc\")\n        assert t1a.isscalar is True\n        assert np.all(t1._time.jd1 == t1a._time.jd1)\n        t1b = Time(t[3])\n        assert t1b.isscalar is True\n        assert np.all(t1._time.jd1 == t1b._time.jd1)\n        t2 = t[4:6]\n        assert t2.isscalar is False\n        assert np.all(t2._time.jd1 == t._time.jd1[4:6])\n        assert t2.location is t.location\n        t2a = Time(t[4:6])\n        assert t2a.isscalar is False\n        assert np.all(t2a._time.jd1 == t._time.jd1[4:6])\n        t2b = Time([t[4], t[5]])\n        assert t2b.isscalar is False\n        assert np.all(t2b._time.jd1 == t._time.jd1[4:6])\n        t2c = Time((t[4], t[5]))\n        assert t2c.isscalar is False\n        assert np.all(t2c._time.jd1 == t._time.jd1[4:6])\n        t.delta_tdb_tt = np.arange(len(t))  # Explicitly set (not testing .tdb)\n        t3 = t[4:6]\n        assert np.all(t3._delta_tdb_tt == t._delta_tdb_tt[4:6])\n        t4 = Time(\n            mjd,\n            format=\"mjd\",\n            scale=\"utc\",\n            location=(np.arange(len(mjd)), np.arange(len(mjd))),\n        )\n        t5a = t4[3]\n        assert t5a.location == t4.location[3]\n        assert t5a.location.shape == ()\n        t5b = t4[3:4]\n        assert t5b.location.shape == (1,)\n        # Check that indexing a size-1 array returns a scalar location as well;\n        # see gh-10113.\n        t5c = t5b[0]\n        assert t5c.location.shape == ()\n        t6 = t4[4:6]\n        assert np.all(t6.location == t4.location[4:6])\n        # check it is a view\n        # (via ndarray, since quantity setter problematic for structured array)\n        allzeros = np.array((0.0, 0.0, 0.0), dtype=t4.location.dtype)\n        assert t6.location.view(np.ndarray)[-1] != allzeros\n        assert t4.location.view(np.ndarray)[5] != allzeros\n        t6.location.view(np.ndarray)[-1] = allzeros\n        assert t4.location.view(np.ndarray)[5] == allzeros\n        # Test subscription also works for two-dimensional arrays.\n        frac = np.arange(0.0, 0.999, 0.2)\n        t7 = Time(\n            mjd[:, np.newaxis] + frac,\n            format=\"mjd\",\n            scale=\"utc\",\n            location=(\"45d\", \"50d\"),\n        )\n        assert t7[0, 0]._time.jd1 == t7._time.jd1[0, 0]\n        assert t7[0, 0].isscalar is True\n        assert np.all(t7[5]._time.jd1 == t7._time.jd1[5])\n        assert np.all(t7[5]._time.jd2 == t7._time.jd2[5])\n        assert np.all(t7[:, 2]._time.jd1 == t7._time.jd1[:, 2])\n        assert np.all(t7[:, 2]._time.jd2 == t7._time.jd2[:, 2])\n        assert np.all(t7[:, 0]._time.jd1 == t._time.jd1)\n        assert np.all(t7[:, 0]._time.jd2 == t._time.jd2)\n        # Get tdb to check that delta_tdb_tt attribute is sliced properly.\n        t7_tdb = t7.tdb\n        assert t7_tdb[0, 0].delta_tdb_tt == t7_tdb.delta_tdb_tt[0, 0]\n        assert np.all(t7_tdb[5].delta_tdb_tt == t7_tdb.delta_tdb_tt[5])\n        assert np.all(t7_tdb[:, 2].delta_tdb_tt == t7_tdb.delta_tdb_tt[:, 2])\n        # Explicitly set delta_tdb_tt attribute. Now it should not be sliced.\n        t7.delta_tdb_tt = 0.1\n        t7_tdb2 = t7.tdb\n        assert t7_tdb2[0, 0].delta_tdb_tt == 0.1\n        assert t7_tdb2[5].delta_tdb_tt == 0.1\n        assert t7_tdb2[:, 2].delta_tdb_tt == 0.1\n        # Check broadcasting of location.\n        t8 = Time(\n            mjd[:, np.newaxis] + frac,\n            format=\"mjd\",\n            scale=\"utc\",\n            location=(np.arange(len(frac)), np.arange(len(frac))),\n        )\n        assert t8[0, 0].location == t8.location[0, 0]\n        assert np.all(t8[5].location == t8.location[5])\n        assert np.all(t8[:, 2].location == t8.location[:, 2])\n        # Finally check empty array.\n        t9 = t[:0]\n        assert t9.isscalar is False\n        assert t9.shape == (0,)\n        assert t9.size == 0\n", "type": "function"}, {"name": "test_atleast_2d", "is_method": true, "class_name": "TestShapeFunctions", "parameters": ["self", "use_mask"], "calls": ["self.create_data", "self.t0.ravel", "np.atleast_2d", "assert_time_all_equal", "np.may_share_memory"], "code_location": {"file": "test_methods.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 400, "end_line": 408}, "code_snippet": "    def test_atleast_2d(self, use_mask):\n        self.create_data(use_mask)\n\n        t0r = self.t0.ravel()\n        assert t0r.ndim == 1\n        t0r_2d = np.atleast_2d(t0r)\n        assert t0r_2d.ndim == 2\n        assert_time_all_equal(t0r[np.newaxis], t0r_2d)\n        assert np.may_share_memory(t0r_2d.jd1, t0r.jd1)\n", "type": "function"}, {"name": "test_atleast_1d", "is_method": true, "class_name": "TestShapeFunctions", "parameters": ["self", "use_mask"], "calls": ["self.create_data", "np.atleast_1d", "assert_time_all_equal", "np.may_share_memory", "self.t0.ravel"], "code_location": {"file": "test_methods.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 388, "end_line": 397}, "code_snippet": "    def test_atleast_1d(self, use_mask):\n        self.create_data(use_mask)\n\n        t00 = self.t0.ravel()[0]\n        assert t00.ndim == 0\n        t00_1d = np.atleast_1d(t00)\n        assert t00_1d.ndim == 1\n        assert_time_all_equal(t00[np.newaxis], t00_1d)\n        # Actual jd1 will not share memory, as cast to scalar.\n        assert np.may_share_memory(t00_1d._time.jd1, t00._time.jd1)\n", "type": "function"}, {"name": "test_setitem_from_python_objects", "is_method": false, "class_name": null, "parameters": [], "calls": ["Time", "np.all", "allclose_sec", "np.all", "allclose_sec", "allclose_sec", "Time", "pytest.raises", "str"], "code_location": {"file": "test_basic.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/time/tests", "start_line": 2038, "end_line": 2078}, "code_snippet": "def test_setitem_from_python_objects():\n    t = Time([[1, 2], [3, 4]], format=\"cxcsec\")\n    assert t.cache == {}\n    t.iso\n    assert (\"iso\", \"*\", \"astropy\") in t.cache[\"format\"]\n    assert np.all(\n        t.iso\n        == [\n            [\"1998-01-01 00:00:01.000\", \"1998-01-01 00:00:02.000\"],\n            [\"1998-01-01 00:00:03.000\", \"1998-01-01 00:00:04.000\"],\n        ]\n    )\n\n    # Setting item clears cache\n    t[0, 1] = 100\n    assert t.cache == {}\n    assert allclose_sec(t.value, [[1, 100], [3, 4]])\n    assert np.all(\n        t.iso\n        == [\n            [\"1998-01-01 00:00:01.000\", \"1998-01-01 00:01:40.000\"],\n            [\"1998-01-01 00:00:03.000\", \"1998-01-01 00:00:04.000\"],\n        ]\n    )\n\n    # Set with a float value\n    t.iso\n    t[1, :] = 200\n    assert t.cache == {}\n    assert allclose_sec(t.value, [[1, 100], [200, 200]])\n\n    # Array of strings in yday format\n    t[:, 1] = [\"1998:002\", \"1998:003\"]\n    assert allclose_sec(t.value, [[1, 86400 * 1], [200, 86400 * 2]])\n\n    # Incompatible numeric value\n    t = Time([\"2000:001\", \"2000:002\"])\n    t[0] = \"2001:001\"\n    with pytest.raises(ValueError) as err:\n        t[0] = 100\n    assert \"cannot convert value to a compatible Time object\" in str(err.value)\n", "type": "function"}, {"name": "test_inplace_change", "is_method": false, "class_name": null, "parameters": [], "calls": ["ICRS", "repr", "i.cache.clear", "len"], "code_location": {"file": "test_frames.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/tests", "start_line": 1415, "end_line": 1432}, "code_snippet": "def test_inplace_change():\n    i = ICRS(1 * u.deg, 2 * u.deg)\n\n    # Add an in frame units version of the rep to the cache.\n    repr(i)\n\n    # Check that repr() has added a rep to the cache\n    assert len(i.cache[\"representation\"]) == 2\n\n    # Modify the data\n    i.data.lon[()] = 10 * u.deg\n\n    # Clear the cache\n    i.cache.clear()\n\n    # This will use a second (potentially cached rep)\n    assert i.ra == 10 * u.deg\n    assert i.dec == 2 * u.deg\n", "type": "function"}, {"name": "test_cache_clear_sc", "is_method": false, "class_name": null, "parameters": [], "calls": ["SkyCoord", "repr", "i.cache.clear", "len", "len"], "code_location": {"file": "test_sky_coord.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/tests", "start_line": 1779, "end_line": 1791}, "code_snippet": "def test_cache_clear_sc():\n    from astropy.coordinates import SkyCoord\n\n    i = SkyCoord(1 * u.deg, 2 * u.deg)\n\n    # Add an in frame units version of the rep to the cache.\n    repr(i)\n\n    assert len(i.cache[\"representation\"]) == 2\n\n    i.cache.clear()\n\n    assert len(i.cache[\"representation\"]) == 0\n", "type": "function"}], "retrieved_count": 10, "cost_time": 4.165750503540039}
{"question": "Where in the sorting method of the list subclass that reorders tuples for astropy table column attributes in the table metadata module does the control flow preserve column metadata ordering while maintaining additional key-value pairs after the predefined column keys?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "sort", "is_method": true, "class_name": "ColumnOrderList", "parameters": ["self"], "calls": ["sort", "dict", "self.extend", "super", "out_list.append", "out_list.append"], "code_location": {"file": "meta.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 18, "end_line": 35}, "code_snippet": "    def sort(self, *args, **kwargs):\n        super().sort()\n\n        column_keys = [\"name\", \"unit\", \"datatype\", \"format\", \"description\", \"meta\"]\n        in_dict = dict(self)\n        out_list = []\n\n        for key in column_keys:\n            if key in in_dict:\n                out_list.append((key, in_dict[key]))\n        for key, val in self:\n            if key not in column_keys:\n                out_list.append((key, val))\n\n        # Clear list in-place\n        del self[:]\n\n        self.extend(out_list)\n", "type": "function"}, {"name": "ColumnOrderList", "docstring": "List of tuples that sorts in a specific order that makes sense for\nastropy table column attributes.", "methods": ["sort"], "attributes": [], "code_location": {"file": "meta.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 12, "end_line": 35}, "type": "class"}, {"name": "items", "is_method": true, "class_name": "ColumnDict", "parameters": ["self"], "calls": ["ColumnOrderList", "items", "super"], "code_location": {"file": "meta.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 45, "end_line": 50}, "code_snippet": "    def items(self):\n        \"\"\"\n        Return items as a ColumnOrderList, which sorts in the preferred\n        way for column attributes.\n        \"\"\"\n        return ColumnOrderList(super().items())\n", "type": "function"}, {"name": "sort", "is_method": true, "class_name": "Table", "parameters": ["self", "keys"], "calls": ["isinstance", "self.argsort", "self.index_mode", "self.columns.values", "ValueError", "col.take"], "code_location": {"file": "table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 3594, "end_line": 3668}, "code_snippet": "    def sort(self, keys=None, *, kind=None, reverse=False):\n        \"\"\"\n        Sort the table according to one or more keys. This operates\n        on the existing table and does not return a new table.\n\n        Parameters\n        ----------\n        keys : str or list of str\n            The key(s) to order the table by. If None, use the\n            primary index of the Table.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n            Sorting algorithm used by ``numpy.argsort``.\n        reverse : bool\n            Sort in reverse order (default=False)\n\n        Examples\n        --------\n        Create a table with 3 columns::\n\n            >>> t = Table([['Max', 'Jo', 'John'], ['Miller', 'Miller', 'Jackson'],\n            ...            [12, 15, 18]], names=('firstname', 'name', 'tel'))\n            >>> print(t)\n            firstname   name  tel\n            --------- ------- ---\n                  Max  Miller  12\n                   Jo  Miller  15\n                 John Jackson  18\n\n        Sorting according to standard sorting rules, first 'name' then 'firstname'::\n\n            >>> t.sort(['name', 'firstname'])\n            >>> print(t)\n            firstname   name  tel\n            --------- ------- ---\n                 John Jackson  18\n                   Jo  Miller  15\n                  Max  Miller  12\n\n        Sorting according to standard sorting rules, first 'firstname' then 'tel',\n        in reverse order::\n\n            >>> t.sort(['firstname', 'tel'], reverse=True)\n            >>> print(t)\n            firstname   name  tel\n            --------- ------- ---\n                  Max  Miller  12\n                 John Jackson  18\n                   Jo  Miller  15\n        \"\"\"\n        if keys is None:\n            if not self.indices:\n                raise ValueError(\"Table sort requires input keys or a table index\")\n            keys = [x.info.name for x in self.indices[0].columns]\n\n        if isinstance(keys, str):\n            keys = [keys]\n\n        indexes = self.argsort(keys, kind=kind, reverse=reverse)\n\n        with self.index_mode(\"freeze\"):\n            for col in self.columns.values():\n                # Make a new sorted column.  This requires that take() also copies\n                # relevant info attributes for mixin columns.\n                new_col = col.take(indexes, axis=0)\n\n                # First statement in try: will succeed if the column supports an in-place\n                # update, and matches the legacy behavior of astropy Table.  However,\n                # some mixin classes may not support this, so in that case just drop\n                # in the entire new column. See #9553 and #9536 for discussion.\n                try:\n                    col[:] = new_col\n                except Exception:\n                    # In-place update failed for some reason, exception class not\n                    # predictable for arbitrary mixin.\n                    self[col.info.name] = new_col\n", "type": "function"}, {"name": "_repr_column_dict", "is_method": false, "class_name": null, "parameters": ["dumper", "data"], "calls": ["dumper.represent_mapping"], "code_location": {"file": "meta.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 142, "end_line": 150}, "code_snippet": "def _repr_column_dict(dumper, data):\n    \"\"\"\n    Represent ColumnDict in yaml dump.\n\n    This is the same as an ordinary mapping except that the keys\n    are written in a fixed order that makes sense for astropy table\n    columns.\n    \"\"\"\n    return dumper.represent_mapping(\"tag:yaml.org,2002:map\", data)\n", "type": "function"}, {"name": "TableColumns", "docstring": "OrderedDict subclass for a set of columns.\n\nThis class enhances item access to provide convenient access to columns\nby name or index, including slice access.  It also handles renaming\nof columns.\n\nThe initialization argument ``cols`` can be a list of ``Column`` objects\nor any structure that is valid for initializing a Python dict.  This\nincludes a dict, list of (key, val) tuples or [key, val] lists, etc.\n\nParameters\n----------\ncols : dict, list, tuple; optional\n    Column objects as data structure that can init dict (see above)", "methods": ["__init__", "__getitem__", "__setitem__", "__repr__", "_rename_column", "__delitem__", "isinstance", "not_isinstance", "setdefault", "update"], "attributes": [], "code_location": {"file": "table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 222, "end_line": 382}, "type": "class"}, {"name": "test_sort_with_mutable_skycoord", "is_method": false, "class_name": null, "parameters": [], "calls": ["Table", "t.sort", "np.all", "np.allclose", "np.allclose", "ra.to_value", "dec.to_value", "SkyCoord"], "code_location": {"file": "test_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table/tests", "start_line": 3217, "end_line": 3237}, "code_snippet": "def test_sort_with_mutable_skycoord():\n    \"\"\"Test sorting a table that has a mutable column such as SkyCoord.\n\n    In this case the sort is done in-place\n    \"\"\"\n    t = Table([[2, 1], SkyCoord([4, 3], [6, 5], unit=\"deg,deg\")], names=[\"a\", \"sc\"])\n    meta = {\"a\": [1, 2]}\n    ta = t[\"a\"]\n    tsc = t[\"sc\"]\n    t[\"sc\"].info.meta = meta\n    t.sort(\"a\")\n    assert np.all(t[\"a\"] == [1, 2])\n    assert np.allclose(t[\"sc\"].ra.to_value(u.deg), [3, 4])\n    assert np.allclose(t[\"sc\"].dec.to_value(u.deg), [5, 6])\n    assert t[\"a\"] is ta\n    assert t[\"sc\"] is tsc\n\n    # Prior to astropy 4.1 this was a deep copy of SkyCoord column; after 4.1\n    # it is a reference.\n    t[\"sc\"].info.meta[\"a\"][0] = 100\n    assert meta[\"a\"][0] == 100\n", "type": "function"}, {"name": "test_col_meta_merge", "is_method": true, "class_name": "TestJoin", "parameters": ["self", "operation_table_type"], "calls": ["self._setup", "t2.rename_column", "OrderedDict", "OrderedDict", "pytest.warns", "nullcontext", "table.join"], "code_location": {"file": "test_operations.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table/tests", "start_line": 525, "end_line": 575}, "code_snippet": "    def test_col_meta_merge(self, operation_table_type):\n        self._setup(operation_table_type)\n        t1 = self.t1\n        t2 = self.t2\n        t2.rename_column(\"d\", \"c\")  # force col conflict and renaming\n        meta1 = OrderedDict([(\"b\", [1, 2]), (\"c\", {\"a\": 1}), (\"d\", 1)])\n        meta2 = OrderedDict([(\"b\", [3, 4]), (\"c\", {\"b\": 1}), (\"a\", 1)])\n\n        # Key col 'a', should first value ('cm')\n        t1[\"a\"].unit = \"cm\"\n        t2[\"a\"].unit = \"m\"\n        # Key col 'b', take first value 't1_b'\n        t1[\"b\"].info.description = \"t1_b\"\n        # Key col 'b', take first non-empty value 't1_b'\n        t2[\"b\"].info.format = \"%6s\"\n        # Key col 'a', should be merged meta\n        t1[\"a\"].info.meta = meta1\n        t2[\"a\"].info.meta = meta2\n        # Key col 'b', should be meta2\n        t2[\"b\"].info.meta = meta2\n\n        # All these should pass through\n        t1[\"c\"].info.format = \"%3s\"\n        t1[\"c\"].info.description = \"t1_c\"\n\n        t2[\"c\"].info.format = \"%6s\"\n        t2[\"c\"].info.description = \"t2_c\"\n\n        if operation_table_type is Table:\n            ctx = pytest.warns(\n                metadata.MergeConflictWarning,\n                match=(\n                    r\"In merged column 'a' the 'unit' attribute does not match \\(cm\"\n                    r\" != m\\)\"\n                ),\n            )\n        else:\n            ctx = nullcontext()\n\n        with ctx:\n            t12 = table.join(t1, t2, keys=[\"a\", \"b\"])\n\n        assert t12[\"a\"].unit == \"m\"\n        assert t12[\"b\"].info.description == \"t1_b\"\n        assert t12[\"b\"].info.format == \"%6s\"\n        assert t12[\"a\"].info.meta == self.meta_merge\n        assert t12[\"b\"].info.meta == meta2\n        assert t12[\"c_1\"].info.format == \"%3s\"\n        assert t12[\"c_1\"].info.description == \"t1_c\"\n        assert t12[\"c_2\"].info.format == \"%6s\"\n        assert t12[\"c_2\"].info.description == \"t2_c\"\n", "type": "function"}, {"name": "sorted_data", "is_method": true, "class_name": "Index", "parameters": ["self"], "calls": ["self.data.sorted_data"], "code_location": {"file": "index.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 357, "end_line": 362}, "code_snippet": "    def sorted_data(self):\n        \"\"\"\n        Returns a list of rows in sorted order based on keys;\n        essentially acts as an argsort() on columns.\n        \"\"\"\n        return self.data.sorted_data()\n", "type": "function"}, {"name": "SortedArray", "docstring": "Implements a sorted array container using\na list of numpy arrays.\n\nParameters\n----------\ndata : Table\n    Sorted columns of the original table\nrow_index : Column object\n    Row numbers corresponding to data columns\nunique : bool\n    Whether the values of the index must be unique.\n    Defaults to False.", "methods": ["__init__", "cols", "add", "_get_key_slice", "find_pos", "find", "range", "remove", "shift_left", "shift_right", "replace_rows", "items", "sort", "sorted_data", "__getitem__", "__repr__"], "attributes": [], "code_location": {"file": "sorted_array.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 28, "end_line": 314}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.2421784400939941}
{"question": "Why does the test class that performs binary serialization roundtrips in the VOTable test suite incur performance overhead from repeatedly creating new in-memory binary stream objects during conversions between XML and binary representations of astronomical table data?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "_test_regression", "is_method": false, "class_name": null, "parameters": ["tmp_path", "_python_based", "binary_mode"], "calls": ["parse", "votable.get_first_table", "votable.to_xml", "assert_validate_schema", "assert_validate_schema", "votable2.to_xml", "assert_validate_schema", "sys.stdout.writelines", "votable2.to_xml", "get_pkg_data_filename", "str", "str", "open", "votable.to_xml", "str", "open", "parse", "votable2.get_first_table", "str", "str", "open", "fd.readlines", "open", "fd.readlines", "difflib.unified_diff", "str", "gzip.GzipFile", "gzfd.readlines", "rstrip", "x.rstrip", "list", "replace", "new_dtypes.append", "votable.get_first_table", "str", "str", "get_pkg_data_filename", "str", "str", "tuple", "votable.get_first_table", "x.decode", "votable.get_first_table"], "code_location": {"file": "test_vo.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable/tests", "start_line": 71, "end_line": 179}, "code_snippet": "def _test_regression(tmp_path, _python_based=False, binary_mode=1):\n    # Read the VOTABLE\n    votable = parse(\n        get_pkg_data_filename(\"data/regression.xml\"),\n        _debug_python_based_parser=_python_based,\n    )\n    table = votable.get_first_table()\n\n    dtypes = [\n        ((\"string test\", \"string_test\"), \"|O8\"),\n        ((\"fixed string test\", \"string_test_2\"), \"<U10\"),\n        (\"unicode_test\", \"|O8\"),\n        ((\"unicode test\", \"fixed_unicode_test\"), \"<U10\"),\n        ((\"string array test\", \"string_array_test\"), \"<U4\"),\n        (\"unsignedByte\", \"|u1\"),\n        (\"short\", \"<i2\"),\n        (\"int\", \"<i4\"),\n        (\"intNoNull\", \"<i4\"),\n        (\"long\", \"<i8\"),\n        (\"double\", \"<f8\"),\n        (\"float\", \"<f4\"),\n        (\"array\", \"|O8\"),\n        (\"bit\", \"|b1\"),\n        (\"bitarray\", \"|b1\", (3, 2)),\n        (\"bitvararray\", \"|O8\"),\n        (\"bitvararray2\", \"|O8\"),\n        (\"floatComplex\", \"<c8\"),\n        (\"doubleComplex\", \"<c16\"),\n        (\"doubleComplexArray\", \"|O8\"),\n        (\"doubleComplexArrayFixed\", \"<c16\", (2,)),\n        (\"boolean\", \"|b1\"),\n        (\"booleanArray\", \"|b1\", (4,)),\n        (\"nulls\", \"<i4\"),\n        (\"nulls_array\", \"<i4\", (2, 2)),\n        (\"precision1\", \"<f8\"),\n        (\"precision2\", \"<f8\"),\n        (\"doublearray\", \"|O8\"),\n        (\"bitarray2\", \"|b1\", (16,)),\n    ]\n    if sys.byteorder == \"big\":\n        new_dtypes = []\n        for dtype in dtypes:\n            dtype = list(dtype)\n            dtype[1] = dtype[1].replace(\"<\", \">\")\n            new_dtypes.append(tuple(dtype))\n        dtypes = new_dtypes\n    assert table.array.dtype == dtypes\n\n    votable.to_xml(\n        str(tmp_path / \"regression.tabledata.xml\"),\n        _debug_python_based_parser=_python_based,\n    )\n    assert_validate_schema(str(tmp_path / \"regression.tabledata.xml\"), votable.version)\n\n    if binary_mode == 1:\n        votable.get_first_table().format = \"binary\"\n        votable.version = \"1.1\"\n    elif binary_mode == 2:\n        votable.get_first_table()._config[\"version_1_3_or_later\"] = True\n        votable.get_first_table().format = \"binary2\"\n        votable.version = \"1.3\"\n\n    # Also try passing a file handle\n    with open(str(tmp_path / \"regression.binary.xml\"), \"wb\") as fd:\n        votable.to_xml(fd, _debug_python_based_parser=_python_based)\n    assert_validate_schema(str(tmp_path / \"regression.binary.xml\"), votable.version)\n    # Also try passing a file handle\n    with open(str(tmp_path / \"regression.binary.xml\"), \"rb\") as fd:\n        votable2 = parse(fd, _debug_python_based_parser=_python_based)\n    votable2.get_first_table().format = \"tabledata\"\n    votable2.to_xml(\n        str(tmp_path / \"regression.bin.tabledata.xml\"),\n        _astropy_version=\"testing\",\n        _debug_python_based_parser=_python_based,\n    )\n    assert_validate_schema(\n        str(tmp_path / \"regression.bin.tabledata.xml\"), votable.version\n    )\n\n    with open(\n        get_pkg_data_filename(\n            f\"data/regression.bin.tabledata.truth.{votable.version}.xml\"\n        ),\n        encoding=\"utf-8\",\n    ) as fd:\n        truth = fd.readlines()\n    with open(str(tmp_path / \"regression.bin.tabledata.xml\"), encoding=\"utf-8\") as fd:\n        output = fd.readlines()\n\n    # If the lines happen to be different, print a diff\n    # This is convenient for debugging\n    sys.stdout.writelines(\n        difflib.unified_diff(truth, output, fromfile=\"truth\", tofile=\"output\")\n    )\n\n    assert truth == output\n\n    # Test implicit gzip saving\n    votable2.to_xml(\n        str(tmp_path / \"regression.bin.tabledata.xml.gz\"),\n        _astropy_version=\"testing\",\n        _debug_python_based_parser=_python_based,\n    )\n    with gzip.GzipFile(str(tmp_path / \"regression.bin.tabledata.xml.gz\"), \"rb\") as gzfd:\n        output = gzfd.readlines()\n    output = [x.decode(\"utf-8\").rstrip() for x in output]\n    truth = [x.rstrip() for x in truth]\n\n    assert truth == output\n", "type": "function"}, {"name": "test_read_through_table_interface", "is_method": false, "class_name": null, "parameters": ["tmp_path"], "calls": ["np.errstate", "len", "pytest.warns", "t.write", "open", "Table.read", "len", "get_pkg_data_fileobj", "Table.read"], "code_location": {"file": "test_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable/tests", "start_line": 129, "end_line": 149}, "code_snippet": "def test_read_through_table_interface(tmp_path):\n    with np.errstate(over=\"ignore\"):\n        # https://github.com/astropy/astropy/issues/13341\n        with get_pkg_data_fileobj(\"data/regression.xml\", encoding=\"binary\") as fd:\n            t = Table.read(fd, format=\"votable\", table_id=\"main_table\")\n\n    assert len(t) == 5\n\n    # Issue 8354\n    assert t[\"float\"].format is None\n\n    fn = tmp_path / \"table_interface.xml\"\n\n    # W39: Bit values can not be masked\n    with pytest.warns(W39):\n        t.write(fn, table_id=\"FOO\", format=\"votable\")\n\n    with open(fn, \"rb\") as fd:\n        t2 = Table.read(fd, format=\"votable\", table_id=\"FOO\")\n\n    assert len(t2) == 5\n", "type": "function"}, {"name": "test_roundtrip", "is_method": false, "class_name": null, "parameters": [], "calls": ["parse", "io.BytesIO", "votable.to_xml", "bio.seek", "parse", "get_pkg_data_filename", "len", "len"], "code_location": {"file": "test_resource.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable/tests", "start_line": 27, "end_line": 49}, "code_snippet": "def test_roundtrip():\n    # Issue #16511 VOTable writer does not write out GROUPs within RESOURCEs\n\n    # Read the VOTABLE\n    votable = parse(get_pkg_data_filename(\"data/resource_groups.xml\"))\n\n    bio = io.BytesIO()\n    votable.to_xml(bio)\n    bio.seek(0)\n    votable = parse(bio)\n\n    resource = votable.resources[0]\n    groups = resource.groups\n    params = resource.params\n\n    # Test that params inside groups are not outside\n\n    assert len(groups[0].entries) == 1\n    assert groups[0].entries[0].name == \"ID\"\n\n    assert len(params) == 2\n    assert params[0].name == \"standardID\"\n    assert params[1].name == \"accessURL\"\n", "type": "function"}, {"name": "__generate_votable_test", "is_method": false, "class_name": null, "parameters": [], "calls": ["Table", "from_table", "Column", "Column", "astropy.io.votable.tree.VOTableFile"], "code_location": {"file": "test_dataorigin.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable/tests", "start_line": 10, "end_line": 17}, "code_snippet": "def __generate_votable_test():\n    table = Table(\n        [\n            Column(name=\"id\", data=[1, 2, 3, 4]),\n            Column(name=\"bmag\", unit=\"mag\", data=[5.6, 7.9, 12.4, 11.3]),\n        ]\n    )\n    return astropy.io.votable.tree.VOTableFile().from_table(table)\n", "type": "function"}, {"name": "test_write_to_fileobj", "is_method": true, "class_name": "TestSingleTable", "parameters": ["self"], "calls": ["Table", "BytesIO", "t.write", "buff.seek", "Table.read", "equal_data"], "code_location": {"file": "test_connect.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 78, "end_line": 85}, "code_snippet": "    def test_write_to_fileobj(self):\n        # regression test for https://github.com/astropy/astropy/issues/17703\n        t = Table(self.data)\n        buff = BytesIO()\n        t.write(buff, format=\"fits\")\n        buff.seek(0)\n        t2 = Table.read(buff)\n        assert equal_data(t2, t)\n", "type": "function"}, {"name": "test_table", "is_method": false, "class_name": null, "parameters": ["tmp_path"], "calls": ["votable.get_first_table", "table.to_table", "tree.VOTableFile.from_table", "votable2.get_first_table", "zip", "np.errstate", "parse", "np.all", "pytest.warns", "writeto", "get_pkg_data_filename", "str"], "code_location": {"file": "test_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable/tests", "start_line": 61, "end_line": 117}, "code_snippet": "def test_table(tmp_path):\n    # Read the VOTABLE\n    with np.errstate(over=\"ignore\"):\n        # https://github.com/astropy/astropy/issues/13341\n        votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n    table = votable.get_first_table()\n    astropy_table = table.to_table()\n\n    for name in table.array.dtype.names:\n        assert np.all(astropy_table.mask[name] == table.array.mask[name])\n\n    votable2 = tree.VOTableFile.from_table(astropy_table)\n    t = votable2.get_first_table()\n\n    field_types = [\n        (\"string_test\", {\"datatype\": \"char\", \"arraysize\": \"*\"}),\n        (\"string_test_2\", {\"datatype\": \"char\", \"arraysize\": \"10\"}),\n        (\"unicode_test\", {\"datatype\": \"unicodeChar\", \"arraysize\": \"*\"}),\n        (\"fixed_unicode_test\", {\"datatype\": \"unicodeChar\", \"arraysize\": \"10\"}),\n        (\"string_array_test\", {\"datatype\": \"char\", \"arraysize\": \"4*\"}),\n        (\"unsignedByte\", {\"datatype\": \"unsignedByte\"}),\n        (\"short\", {\"datatype\": \"short\"}),\n        (\"int\", {\"datatype\": \"int\"}),\n        (\"intNoNull\", {\"datatype\": \"int\"}),\n        (\"long\", {\"datatype\": \"long\"}),\n        (\"double\", {\"datatype\": \"double\"}),\n        (\"float\", {\"datatype\": \"float\"}),\n        (\"array\", {\"datatype\": \"long\", \"arraysize\": \"2*\"}),\n        (\"bit\", {\"datatype\": \"bit\"}),\n        (\"bitarray\", {\"datatype\": \"bit\", \"arraysize\": \"3x2\"}),\n        (\"bitvararray\", {\"datatype\": \"bit\", \"arraysize\": \"*\"}),\n        (\"bitvararray2\", {\"datatype\": \"bit\", \"arraysize\": \"3x2*\"}),\n        (\"floatComplex\", {\"datatype\": \"floatComplex\"}),\n        (\"doubleComplex\", {\"datatype\": \"doubleComplex\"}),\n        (\"doubleComplexArray\", {\"datatype\": \"doubleComplex\", \"arraysize\": \"*\"}),\n        (\"doubleComplexArrayFixed\", {\"datatype\": \"doubleComplex\", \"arraysize\": \"2\"}),\n        (\"boolean\", {\"datatype\": \"bit\"}),\n        (\"booleanArray\", {\"datatype\": \"bit\", \"arraysize\": \"4\"}),\n        (\"nulls\", {\"datatype\": \"int\"}),\n        (\"nulls_array\", {\"datatype\": \"int\", \"arraysize\": \"2x2\"}),\n        (\"precision1\", {\"datatype\": \"double\"}),\n        (\"precision2\", {\"datatype\": \"double\"}),\n        (\"doublearray\", {\"datatype\": \"double\", \"arraysize\": \"*\"}),\n        (\"bitarray2\", {\"datatype\": \"bit\", \"arraysize\": \"16\"}),\n    ]\n\n    for field, (name, d) in zip(t.fields, field_types):\n        assert field.ID == name\n        assert field.datatype == d[\"datatype\"], (\n            f\"{name} expected {d['datatype']} but get {field.datatype}\"\n        )\n        if \"arraysize\" in d:\n            assert field.arraysize == d[\"arraysize\"]\n\n    # W39: Bit values can not be masked\n    with pytest.warns(W39):\n        writeto(votable2, str(tmp_path / \"through_table.xml\"))\n", "type": "function"}, {"name": "test_select_columns_binary", "is_method": false, "class_name": null, "parameters": ["format_"], "calls": ["pytest.mark.parametrize", "io.BytesIO", "bio.seek", "parse", "to_table", "np.errstate", "parse", "votable.get_first_table", "pytest.warns", "votable.to_xml", "get_pkg_data_filename", "votable.get_first_table", "votable.get_first_table"], "code_location": {"file": "test_vo.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable/tests", "start_line": 829, "end_line": 845}, "code_snippet": "def test_select_columns_binary(format_):\n    with np.errstate(over=\"ignore\"):\n        # https://github.com/astropy/astropy/issues/13341\n        votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n    if format_ == \"binary2\":\n        votable.version = \"1.3\"\n        votable.get_first_table()._config[\"version_1_3_or_later\"] = True\n    votable.get_first_table().format = format_\n\n    bio = io.BytesIO()\n    # W39: Bit values can not be masked\n    with pytest.warns(W39):\n        votable.to_xml(bio)\n    bio.seek(0)\n    votable = parse(bio, columns=[0, 1, 2])\n    table = votable.get_first_table().to_table()\n    assert table.colnames == [\"string_test\", \"string_test_2\", \"unicode_test\"]\n", "type": "function"}, {"name": "test_from_table_without_mask", "is_method": false, "class_name": null, "parameters": [], "calls": ["Table", "Column", "t.add_column", "io.BytesIO", "t.write"], "code_location": {"file": "test_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable/tests", "start_line": 243, "end_line": 248}, "code_snippet": "def test_from_table_without_mask():\n    t = Table()\n    c = Column(data=[1, 2, 3], name=\"a\")\n    t.add_column(c)\n    output = io.BytesIO()\n    t.write(output, format=\"votable\")\n", "type": "function"}, {"name": "test_writeto", "is_method": false, "class_name": null, "parameters": ["path_format", "tmp_path", "home_is_tmpdir"], "calls": ["pytest.mark.parametrize", "Table", "from_table", "writeto", "str", "os.path.join", "open", "f.read", "os.path.exists", "os.path.expanduser"], "code_location": {"file": "test_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable/tests", "start_line": 421, "end_line": 442}, "code_snippet": "def test_writeto(path_format, tmp_path, home_is_tmpdir):\n    if path_format == \"plain\":\n        # pathlib.Path objects are not accepted by votable.writeto, so convert\n        # to a string\n        fname = str(tmp_path / \"writeto_test.vot\")\n    else:\n        fname = os.path.join(\"~\", \"writeto_test.vot\")\n\n    t = Table()\n    t[\"a\"] = [1, 2, 3]\n    vt = from_table(t)\n    writeto(vt, fname)\n\n    if path_format == \"tilde\":\n        # Ensure the tilde-prefixed path wasn't treated literally\n        assert not os.path.exists(fname)\n\n    with open(os.path.expanduser(fname)) as f:\n        obuff = f.read()\n    assert 'VOTABLE version=\"1.4\"' in obuff\n    assert \"BINARY\" not in obuff\n    assert \"TABLEDATA\" in obuff\n", "type": "function"}, {"name": "test_timesys_roundtrip", "is_method": false, "class_name": null, "parameters": [], "calls": ["parse", "io.BytesIO", "orig_votable.to_xml", "bio.seek", "parse", "_timesys_tests", "get_pkg_data_filename"], "code_location": {"file": "test_vo.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable/tests", "start_line": 1202, "end_line": 1208}, "code_snippet": "def test_timesys_roundtrip():\n    orig_votable = parse(get_pkg_data_filename(\"data/timesys.xml\"))\n    bio = io.BytesIO()\n    orig_votable.to_xml(bio)\n    bio.seek(0)\n    votable = parse(bio)\n    _timesys_tests(votable)\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.2022953033447266}
{"question": "At which level in the inheritance hierarchy does the class attribute specifying the function unit class control instantiation and conversion behavior inherited from the base class for logarithmic quantities?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "_quantity_class", "is_method": true, "class_name": "LogUnit", "parameters": ["self"], "calls": [], "code_location": {"file": "logarithmic.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/function", "start_line": 57, "end_line": 58}, "code_snippet": "    def _quantity_class(self):\n        return LogQuantity\n", "type": "function"}, {"name": "LogUnit", "docstring": "Logarithmic unit containing a physical one.\n\nUsually, logarithmic units are instantiated via specific subclasses\nsuch `~astropy.units.MagUnit`, `~astropy.units.DecibelUnit`, and\n`~astropy.units.DexUnit`.\n\nParameters\n----------\nphysical_unit : `~astropy.units.Unit` or `string`\n    Unit that is encapsulated within the logarithmic function unit.\n    If not given, dimensionless.\n\nfunction_unit :  `~astropy.units.Unit` or `string`\n    By default, the same as the logarithmic unit set by the subclass.", "methods": ["_default_function_unit", "_quantity_class", "from_physical", "to_physical", "_add_and_adjust_physical_unit", "__neg__", "__add__", "__radd__", "__sub__", "__rsub__"], "attributes": [], "code_location": {"file": "logarithmic.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/function", "start_line": 31, "end_line": 131}, "type": "class"}, {"name": "LogQuantity", "docstring": "A representation of a (scaled) logarithm of a number with a unit.\n\nParameters\n----------\nvalue : number, `~astropy.units.Quantity`, `~astropy.units.LogQuantity`, or sequence of quantity-like.\n    The numerical value of the logarithmic quantity. If a number or\n    a `~astropy.units.Quantity` with a logarithmic unit, it will be\n    converted to ``unit`` and the physical unit will be inferred from\n    ``unit``.  If a `~astropy.units.Quantity` with just a physical unit,\n    it will converted to the logarithmic unit, after, if necessary,\n    converting it to the physical unit inferred from ``unit``.\n\nunit : str, `~astropy.units.UnitBase`, or `~astropy.units.FunctionUnitBase`, optional\n    For an `~astropy.units.FunctionUnitBase` instance, the\n    physical unit will be taken from it; for other input, it will be\n    inferred from ``value``. By default, ``unit`` is set by the subclass.\n\ndtype : `~numpy.dtype`, optional\n    The ``dtype`` of the resulting Numpy array or scalar that will\n    hold the value.  If not provided, is is determined automatically\n    from the input value.\n\ncopy : bool, optional\n    If `True` (default), then the value is copied.  Otherwise, a copy will\n    only be made if ``__array__`` returns a copy, if value is a nested\n    sequence, or if a copy is needed to satisfy an explicitly given\n    ``dtype``.  (The `False` option is intended mostly for internal use,\n    to speed up initialization where a copy is known to have been made.\n    Use with care.)\n\nExamples\n--------\nTypically, use is made of an `~astropy.units.FunctionQuantity`\nsubclass, as in::\n\n    >>> import astropy.units as u\n    >>> u.Magnitude(-2.5)\n    <Magnitude -2.5 mag>\n    >>> u.Magnitude(10.*u.count/u.second)\n    <Magnitude -2.5 mag(ct / s)>\n    >>> u.Decibel(1.*u.W, u.DecibelUnit(u.mW))  # doctest: +FLOAT_CMP\n    <Decibel 30. dB(mW)>", "methods": ["__add__", "__radd__", "__iadd__", "__sub__", "__rsub__", "__isub__", "__mul__", "__rmul__", "__imul__", "__truediv__", "__itruediv__", "__pow__", "__ilshift__", "var", "std", "diff", "ediff1d"], "attributes": ["_unit_class", "_supported_functions"], "code_location": {"file": "logarithmic.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/function", "start_line": 218, "end_line": 428}, "type": "class"}, {"name": "FunctionUnitBase", "docstring": "Abstract base class for function units.\n\nFunction units are functions containing a physical unit, such as dB(mW).\nMost of the arithmetic operations on function units are defined in this\nbase class.\n\nWhile instantiation is defined, this class should not be used directly.\nRather, subclasses should be used that override the abstract properties\n`_default_function_unit` and `_quantity_class`, and the abstract methods\n`from_physical`, and `to_physical`.\n\nParameters\n----------\nphysical_unit : `~astropy.units.Unit` or `string`\n    Unit that is encapsulated within the function unit.\n    If not given, dimensionless.\n\nfunction_unit :  `~astropy.units.Unit` or `string`\n    By default, the same as the function unit set by the subclass.", "methods": ["_default_function_unit", "_quantity_class", "from_physical", "to_physical", "__init__", "_copy", "physical_unit", "function_unit", "equivalencies", "decompose", "si", "cgs", "_physical_type_id", "physical_type", "is_equivalent", "to", "is_unity", "__eq__", "__ne__", "__rlshift__", "__mul__", "__rmul__", "__truediv__", "__rtruediv__", "__pow__", "__pos__", "to_string", "__format__", "__str__", "__repr__", "_repr_latex_", "__hash__"], "attributes": ["__array_priority__"], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/function", "start_line": 66, "end_line": 488}, "type": "class"}, {"name": "DexUnit", "docstring": "Logarithmic physical units expressed in magnitudes.\n\nParameters\n----------\nphysical_unit : `~astropy.units.Unit` or `string`\n    Unit that is encapsulated within the magnitude function unit.\n    If not given, dimensionless.\n\nfunction_unit :  `~astropy.units.Unit` or `string`\n    By default, this is ``dex``, but this allows one to use an equivalent\n    unit such as ``0.5 dex``.", "methods": ["_default_function_unit", "_quantity_class", "to_string"], "attributes": [], "code_location": {"file": "logarithmic.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/function", "start_line": 159, "end_line": 190}, "type": "class"}, {"name": "FunctionQuantity", "docstring": "A representation of a (scaled) function of a number with a unit.\n\nFunction quantities are quantities whose units are functions containing a\nphysical unit, such as dB(mW).  Most of the arithmetic operations on\nfunction quantities are defined in this base class.\n\nWhile instantiation is also defined here, this class should not be\ninstantiated directly.  Rather, subclasses should be made which have\n``_unit_class`` pointing back to the corresponding function unit class.\n\nParameters\n----------\nvalue : number, quantity-like, or sequence thereof\n    The numerical value of the function quantity. If a number or\n    a `~astropy.units.Quantity` with a function unit, it will be converted\n    to ``unit`` and the physical unit will be inferred from ``unit``.\n    If a `~astropy.units.Quantity` with just a physical unit, it will\n    converted to the function unit, after, if necessary, converting it to\n    the physical unit inferred from ``unit``.\n\nunit : str, `~astropy.units.UnitBase`, or `~astropy.units.FunctionUnitBase`, optional\n    For an `~astropy.units.FunctionUnitBase` instance, the\n    physical unit will be taken from it; for other input, it will be\n    inferred from ``value``. By default, ``unit`` is set by the subclass.\n\ndtype : `~numpy.dtype`, optional\n    The dtype of the resulting Numpy array or scalar that will\n    hold the value.  If not provided, it is determined from the input,\n    except that any input that cannot represent float (integer and bool)\n    is converted to float.\n\ncopy : bool, optional\n    If `True` (default), then the value is copied.  Otherwise, a copy will\n    only be made if ``__array__`` returns a copy, if value is a nested\n    sequence, or if a copy is needed to satisfy an explicitly given\n    ``dtype``.  (The `False` option is intended mostly for internal use,\n    to speed up initialization where a copy is known to have been made.\n    Use with care.)\n\norder : {'C', 'F', 'A'}, optional\n    Specify the order of the array.  As in `~numpy.array`.  Ignored\n    if the input does not need to be converted and ``copy=False``.\n\nsubok : bool, optional\n    If `False` (default), the returned array will be forced to be of the\n    class used.  Otherwise, subclasses will be passed through.\n\nndmin : int, optional\n    Specifies the minimum number of dimensions that the resulting array\n    should have.  Ones will be prepended to the shape as needed to meet\n    this requirement.  This parameter is ignored if the input is a\n    `~astropy.units.Quantity` and ``copy=False``.\n\nRaises\n------\nTypeError\n    If the value provided is not a Python numeric type.\nTypeError\n    If the unit provided is not a `~astropy.units.FunctionUnitBase`\n    or `~astropy.units.Unit` object, or a parseable string unit.", "methods": ["__new__", "physical", "_function_view", "si", "cgs", "decompose", "__quantity_subclass__", "_set_unit", "__array_ufunc__", "_maybe_new_view", "__mul__", "__truediv__", "__rtruediv__", "_comparison", "__eq__", "__ne__", "__gt__", "__ge__", "__lt__", "__le__", "__lshift__", "_wrap_function", "max", "min", "sum", "cumsum", "clip"], "attributes": ["_unit_class", "__array_priority__", "_supported_ufuncs", "_supported_functions"], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/function", "start_line": 491, "end_line": 804}, "type": "class"}, {"name": "MagUnit", "docstring": "Logarithmic physical units expressed in magnitudes.\n\nParameters\n----------\nphysical_unit : `~astropy.units.Unit` or `string`\n    Unit that is encapsulated within the magnitude function unit.\n    If not given, dimensionless.\n\nfunction_unit :  `~astropy.units.Unit` or `string`\n    By default, this is ``mag``, but this allows one to use an equivalent\n    unit such as ``2 mag``.", "methods": ["_default_function_unit", "_quantity_class"], "attributes": [], "code_location": {"file": "logarithmic.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/function", "start_line": 134, "end_line": 156}, "type": "class"}, {"name": "TestLogUnitConversion", "docstring": "", "methods": ["test_physical_unit_conversion", "test_container_unit_conversion", "test_subclass_conversion", "test_unit_decomposition", "test_unit_multiple_possible_equivalencies", "test_magnitude_conversion_fails_message"], "attributes": [], "code_location": {"file": "test_logarithmic.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 212, "end_line": 310}, "type": "class"}, {"name": "DecibelUnit", "docstring": "Logarithmic physical units expressed in dB.\n\nParameters\n----------\nphysical_unit : `~astropy.units.Unit` or `string`\n    Unit that is encapsulated within the decibel function unit.\n    If not given, dimensionless.\n\nfunction_unit :  `~astropy.units.Unit` or `string`\n    By default, this is ``dB``, but this allows one to use an equivalent\n    unit such as ``2 dB``.", "methods": ["_default_function_unit", "_quantity_class"], "attributes": [], "code_location": {"file": "logarithmic.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/function", "start_line": 193, "end_line": 215}, "type": "class"}, {"name": "__quantity_subclass__", "is_method": true, "class_name": "FunctionQuantity", "parameters": ["self", "unit"], "calls": ["isinstance", "__quantity_subclass__", "super"], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/function", "start_line": 639, "end_line": 643}, "code_snippet": "    def __quantity_subclass__(self, unit):\n        if isinstance(unit, FunctionUnitBase):\n            return self.__class__, True\n        else:\n            return super().__quantity_subclass__(unit)[0], False\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1457104682922363}
{"question": "Where is the helper function that converts three angle unit inputs representing degree, arcminute, and arcsecond components to radian output units for the scipy special function that accepts three angle arguments?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "helper_s2p", "is_method": false, "class_name": null, "parameters": ["f", "unit1", "unit2", "unit3"], "calls": ["UnitTypeError", "get_converter", "get_converter"], "code_location": {"file": "erfa.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 72, "end_line": 80}, "code_snippet": "def helper_s2p(f, unit1, unit2, unit3):\n    from astropy.units.si import radian\n\n    try:\n        return [get_converter(unit1, radian), get_converter(unit2, radian), None], unit3\n    except UnitsError:\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to quantities with angle units\"\n        )\n", "type": "function"}, {"name": "helper_degree_minute_second_to_radian", "is_method": false, "class_name": null, "parameters": ["f", "unit1", "unit2", "unit3"], "calls": ["UnitTypeError", "get_converter", "get_converter", "get_converter"], "code_location": {"file": "scipy_special.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 53, "end_line": 65}, "code_snippet": "def helper_degree_minute_second_to_radian(f, unit1, unit2, unit3):\n    from astropy.units.si import arcmin, arcsec, degree, radian\n\n    try:\n        return [\n            get_converter(unit1, degree),\n            get_converter(unit2, arcmin),\n            get_converter(unit3, arcsec),\n        ], radian\n    except UnitsError:\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to quantities with angle units\"\n        )\n", "type": "function"}, {"name": "helper_atciqz_aticq", "is_method": false, "class_name": null, "parameters": ["f", "unit_rc", "unit_dc", "unit_astrom"], "calls": ["get_converter", "get_converter", "get_converter", "astrom_unit"], "code_location": {"file": "erfa.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 431, "end_line": 438}, "code_snippet": "def helper_atciqz_aticq(f, unit_rc, unit_dc, unit_astrom):\n    from astropy.units.si import radian\n\n    return [\n        get_converter(unit_rc, radian),\n        get_converter(unit_dc, radian),\n        get_converter(unit_astrom, astrom_unit()),\n    ], (radian, radian)\n", "type": "function"}, {"name": "helper_s2c", "is_method": false, "class_name": null, "parameters": ["f", "unit1", "unit2"], "calls": ["UnitTypeError", "get_converter", "get_converter"], "code_location": {"file": "erfa.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 58, "end_line": 69}, "code_snippet": "def helper_s2c(f, unit1, unit2):\n    from astropy.units.si import radian\n\n    try:\n        return [\n            get_converter(unit1, radian),\n            get_converter(unit2, radian),\n        ], dimensionless_unscaled\n    except UnitsError:\n        raise UnitTypeError(\n            f\"Can only apply '{f.__name__}' function to quantities with angle units\"\n        )\n", "type": "function"}, {"name": "helper_atoiq", "is_method": false, "class_name": null, "parameters": ["f", "unit_type", "unit_ri", "unit_di", "unit_astrom"], "calls": ["UnitTypeError", "get_converter", "get_converter", "get_converter", "astrom_unit"], "code_location": {"file": "erfa.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 462, "end_line": 473}, "code_snippet": "def helper_atoiq(f, unit_type, unit_ri, unit_di, unit_astrom):\n    from astropy.units.si import radian\n\n    if unit_type is not None:\n        raise UnitTypeError(\"argument 'type' should not have a unit\")\n\n    return [\n        None,\n        get_converter(unit_ri, radian),\n        get_converter(unit_di, radian),\n        get_converter(unit_astrom, astrom_unit()),\n    ], (radian, radian)\n", "type": "function"}, {"name": "helper_p2s", "is_method": false, "class_name": null, "parameters": ["f", "unit1"], "calls": [], "code_location": {"file": "erfa.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 89, "end_line": 92}, "code_snippet": "def helper_p2s(f, unit1):\n    from astropy.units.si import radian\n\n    return [None], (radian, radian, unit1)\n", "type": "function"}, {"name": "helper_aticqn", "is_method": false, "class_name": null, "parameters": ["f", "unit_rc", "unit_dc", "unit_astrom", "unit_b"], "calls": ["get_converter", "get_converter", "get_converter", "get_converter", "astrom_unit", "ldbody_unit"], "code_location": {"file": "erfa.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 441, "end_line": 449}, "code_snippet": "def helper_aticqn(f, unit_rc, unit_dc, unit_astrom, unit_b):\n    from astropy.units.si import radian\n\n    return [\n        get_converter(unit_rc, radian),\n        get_converter(unit_dc, radian),\n        get_converter(unit_astrom, astrom_unit()),\n        get_converter(unit_b, ldbody_unit()),\n    ], (radian, radian)\n", "type": "function"}, {"name": "helper_c2s", "is_method": false, "class_name": null, "parameters": ["f", "unit1"], "calls": [], "code_location": {"file": "erfa.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 83, "end_line": 86}, "code_snippet": "def helper_c2s(f, unit1):\n    from astropy.units.si import radian\n\n    return [None], (radian, radian)\n", "type": "function"}, {"name": "helper_atioq", "is_method": false, "class_name": null, "parameters": ["f", "unit_rc", "unit_dc", "unit_astrom"], "calls": ["get_converter", "get_converter", "get_converter", "astrom_unit"], "code_location": {"file": "erfa.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 452, "end_line": 459}, "code_snippet": "def helper_atioq(f, unit_rc, unit_dc, unit_astrom):\n    from astropy.units.si import radian\n\n    return [\n        get_converter(unit_rc, radian),\n        get_converter(unit_dc, radian),\n        get_converter(unit_astrom, astrom_unit()),\n    ], (radian,) * 5\n", "type": "function"}, {"name": "helper_twoarg_invtrig", "is_method": false, "class_name": null, "parameters": ["f", "unit1", "unit2"], "calls": ["get_converters_and_unit"], "code_location": {"file": "helpers.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 322, "end_line": 326}, "code_snippet": "def helper_twoarg_invtrig(f, unit1, unit2):\n    from astropy.units.si import radian\n\n    converters, _ = get_converters_and_unit(f, unit1, unit2)\n    return converters, radian\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.052154541015625}
{"question": "Where in the logging system are file output handlers assigned character encoding from configuration settings versus platform-preferred encoding?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_log_to_file_encoding", "is_method": false, "class_name": null, "parameters": ["tmp_path", "encoding"], "calls": ["pytest.mark.parametrize", "str", "local_path.resolve", "log.log_to_file", "isinstance", "locale.getpreferredencoding"], "code_location": {"file": "test_logger.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/tests", "start_line": 493, "end_line": 509}, "code_snippet": "def test_log_to_file_encoding(tmp_path, encoding):\n    local_path = tmp_path / \"test.log\"\n    log_path = str(local_path.resolve())\n\n    orig_encoding = conf.log_file_encoding\n\n    conf.log_file_encoding = encoding\n\n    with log.log_to_file(log_path):\n        for handler in log.handlers:\n            if isinstance(handler, logging.FileHandler):\n                if encoding:\n                    assert handler.stream.encoding == encoding\n                else:\n                    assert handler.stream.encoding == locale.getpreferredencoding()\n\n    conf.log_file_encoding = orig_encoding\n", "type": "function"}, {"name": "StreamHandler", "docstring": "A specialized StreamHandler that logs INFO and DEBUG messages to\nstdout, and all other messages to stderr.  Also provides coloring\nof the output, if enabled in the parent logger.", "methods": ["emit"], "attributes": [], "code_location": {"file": "logger.py", "path": "/data3/pwh/swebench-repos/astropy/astropy", "start_line": 554, "end_line": 591}, "type": "class"}, {"name": "_write_string", "is_method": false, "class_name": null, "parameters": ["f", "s"], "calls": ["fileobj_is_binary", "f.write", "isinstance", "encode_ascii", "decode_ascii", "isinstance"], "code_location": {"file": "util.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits", "start_line": 645, "end_line": 659}, "code_snippet": "def _write_string(f, s):\n    \"\"\"\n    Write a string to a file, encoding to ASCII if the file is open in binary\n    mode, or decoding if the file is open in text mode.\n    \"\"\"\n    # Assume if the file object doesn't have a specific mode, that the mode is\n    # binary\n    binmode = fileobj_is_binary(f)\n\n    if binmode and isinstance(s, str):\n        s = encode_ascii(s)\n    elif not binmode and not isinstance(f, str):\n        s = decode_ascii(s)\n\n    f.write(s)\n", "type": "function"}, {"name": "test_read_with_encoding", "is_method": false, "class_name": null, "parameters": ["tmp_path", "encoding"], "calls": ["pytest.mark.parametrize", "data.items", "ascii.read", "open", "f.write", "table.pformat", "ascii.read", "table.pformat"], "code_location": {"file": "test_read.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/ascii/tests", "start_line": 1729, "end_line": 1749}, "code_snippet": "def test_read_with_encoding(tmp_path, encoding):\n    data = {\"commented_header\": \"#  b  \\n 1 2 hllo\", \"csv\": \",b,\\n1,2,hllo\"}\n\n    testfile = tmp_path / \"test.txt\"\n    for fmt, content in data.items():\n        with open(testfile, \"w\", encoding=encoding) as f:\n            f.write(content)\n\n        table = ascii.read(testfile, encoding=encoding)\n        assert table.pformat() == [\"    b      \", \"--- --- -----\", \"  1   2 hllo\"]\n\n        for guess in (True, False):\n            table = ascii.read(\n                testfile, format=fmt, fast_reader=False, encoding=encoding, guess=guess\n            )\n            assert table[\"\"].dtype.kind == \"U\"\n            assert table.pformat() == [\n                \"    b      \",\n                \"--- --- -----\",\n                \"  1   2 hllo\",\n            ]\n", "type": "function"}, {"name": "test_log_to_file", "is_method": false, "class_name": null, "parameters": ["tmp_path", "level"], "calls": ["pytest.mark.parametrize", "local_path.open", "str", "local_path.open", "log_file.readlines", "log_file.close", "local_path.resolve", "log_file.close", "log.setLevel", "len", "len", "len", "log.setLevel", "log.log_to_file", "log.error", "log.warning", "log.info", "log.debug", "len", "eval", "len", "strip", "eval", "eval", "eval", "len", "strip", "strip", "strip", "len"], "code_location": {"file": "test_logger.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/tests", "start_line": 369, "end_line": 434}, "code_snippet": "def test_log_to_file(tmp_path, level):\n    local_path = tmp_path / \"test.log\"\n    log_file = local_path.open(\"wb\")\n    log_path = str(local_path.resolve())\n    orig_level = log.level\n\n    try:\n        if level is not None:\n            log.setLevel(level)\n\n        with log.log_to_file(log_path):\n            log.error(\"Error message\")\n            log.warning(\"Warning message\")\n            log.info(\"Information message\")\n            log.debug(\"Debug message\")\n\n        log_file.close()\n    finally:\n        log.setLevel(orig_level)\n\n    log_file = local_path.open(\"rb\")\n    log_entries = log_file.readlines()\n    log_file.close()\n\n    if level is None:\n        # The log level *should* be set to whatever it was in the config\n        level = conf.log_level\n\n    # Check list length\n    if level == \"DEBUG\":\n        assert len(log_entries) == 4\n    elif level == \"INFO\":\n        assert len(log_entries) == 3\n    elif level == \"WARN\":\n        assert len(log_entries) == 2\n    elif level == \"ERROR\":\n        assert len(log_entries) == 1\n\n    # Check list content\n\n    assert eval(log_entries[0].strip())[-3:] == (\n        \"astropy.tests.test_logger\",\n        \"ERROR\",\n        \"Error message\",\n    )\n\n    if len(log_entries) >= 2:\n        assert eval(log_entries[1].strip())[-3:] == (\n            \"astropy.tests.test_logger\",\n            \"WARNING\",\n            \"Warning message\",\n        )\n\n    if len(log_entries) >= 3:\n        assert eval(log_entries[2].strip())[-3:] == (\n            \"astropy.tests.test_logger\",\n            \"INFO\",\n            \"Information message\",\n        )\n\n    if len(log_entries) >= 4:\n        assert eval(log_entries[3].strip())[-3:] == (\n            \"astropy.tests.test_logger\",\n            \"DEBUG\",\n            \"Debug message\",\n        )\n", "type": "function"}, {"name": "test_empty_config_file", "is_method": false, "class_name": null, "parameters": [], "calls": ["get_content", "is_unedited_config_file", "get_content", "is_unedited_config_file", "open", "fd.read", "get_pkg_data_filename"], "code_location": {"file": "test_configs.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/config/tests", "start_line": 503, "end_line": 514}, "code_snippet": "def test_empty_config_file():\n    from astropy.config.configuration import is_unedited_config_file\n\n    def get_content(fn):\n        with open(get_pkg_data_filename(fn), encoding=\"latin-1\") as fd:\n            return fd.read()\n\n    content = get_content(\"data/empty.cfg\")\n    assert is_unedited_config_file(content)\n\n    content = get_content(\"data/not_empty.cfg\")\n    assert not is_unedited_config_file(content)\n", "type": "function"}, {"name": "test_fake_tty", "is_method": false, "class_name": null, "parameters": [], "calls": ["FakeTTY", "f1.isatty", "f1.write", "FakeTTY", "f2.isatty", "pytest.raises", "f1.getvalue", "f2.getvalue"], "code_location": {"file": "test_console.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 45, "end_line": 59}, "code_snippet": "def test_fake_tty():\n    # First test without a specified encoding; we should be able to write\n    # arbitrary unicode strings\n    f1 = FakeTTY()\n    assert f1.isatty()\n    f1.write(\"\")\n    assert f1.getvalue() == \"\"\n\n    # Now test an ASCII-only TTY--it should raise a UnicodeEncodeError when\n    # trying to write a string containing non-ASCII characters\n    f2 = FakeTTY(\"ascii\")\n    assert f2.isatty()\n    assert f2.__class__.__name__ == \"AsciiFakeTTY\"\n    assert pytest.raises(UnicodeEncodeError, f2.write, \"\")\n    assert f2.getvalue() == \"\"\n", "type": "function"}, {"name": "test_mode_normalization", "is_method": true, "class_name": "TestUtilMode", "parameters": ["self"], "calls": ["self.temp", "open", "util.fileobj_mode"], "code_location": {"file": "test_util.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 151, "end_line": 167}, "code_snippet": "    def test_mode_normalization(self):\n        # Use the normal python IO in append mode with all possible permutation\n        # of the \"mode\" letters.\n\n        # Tuple gives a file name suffix, the given mode and the functions\n        # return. The filenumber is only for consistency with the other\n        # test functions. Append can deal with existing and not existing files.\n        for num, mode, res in [\n            (0, \"a\", \"a\"),\n            (0, \"a+\", \"a+\"),\n            (0, \"ab\", \"ab\"),\n            (0, \"a+b\", \"ab+\"),\n            (0, \"ab+\", \"ab+\"),\n        ]:\n            filename = self.temp(f\"test3{num}.dat\")\n            with open(filename, mode) as fileobj:\n                assert util.fileobj_mode(fileobj) == res\n", "type": "function"}, {"name": "_write_with_fallback", "is_method": false, "class_name": null, "parameters": ["s", "write", "fileobj"], "calls": ["locale.getpreferredencoding", "Writer", "write", "write", "codecs.getwriter", "write", "codecs.getwriter", "codecs.getwriter", "Writer"], "code_location": {"file": "console.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils", "start_line": 191, "end_line": 223}, "code_snippet": "def _write_with_fallback(s, write, fileobj):\n    \"\"\"Write the supplied string with the given write function like\n    ``write(s)``, but use a writer for the locale's preferred encoding in case\n    of a UnicodeEncodeError.  Failing that attempt to write with 'utf-8' or\n    'latin-1'.\n    \"\"\"\n    try:\n        write(s)\n        return write\n    except UnicodeEncodeError:\n        # Let's try the next approach...\n        pass\n\n    enc = locale.getpreferredencoding()\n    try:\n        Writer = codecs.getwriter(enc)\n    except LookupError:\n        Writer = codecs.getwriter(\"utf-8\")\n\n    f = Writer(fileobj)\n    write = f.write\n\n    try:\n        write(s)\n        return write\n    except UnicodeEncodeError:\n        Writer = codecs.getwriter(\"latin-1\")\n        f = Writer(fileobj)\n        write = f.write\n\n    # If this doesn't work let the exception bubble up; I'm out of ideas\n    write(s)\n    return write\n", "type": "function"}, {"name": "get_xml_encoding", "is_method": false, "class_name": null, "parameters": ["source"], "calls": ["get_xml_iterator", "next", "data.get", "OSError"], "code_location": {"file": "iterparser.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/xml", "start_line": 164, "end_line": 185}, "code_snippet": "def get_xml_encoding(source):\n    \"\"\"\n    Determine the encoding of an XML file by reading its header.\n\n    Parameters\n    ----------\n    source : path-like, :term:`file-like (readable)`, or callable\n        Handle that contains the data or function that reads it.\n        If a function or callable object, it must directly read from a stream.\n        Non-callable objects must define a ``read`` method.\n\n    Returns\n    -------\n    encoding : str\n    \"\"\"\n    with get_xml_iterator(source) as iterator:\n        start, tag, data, pos = next(iterator)\n        if not start or tag != \"xml\":\n            raise OSError(\"Invalid XML file\")\n\n    # The XML spec says that no encoding === utf-8\n    return data.get(\"encoding\") or \"utf-8\"\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.027966022491455}
{"question": "What performance overhead does the unit validation decorator in the astropy units module introduce when validating unit equivalence through repeated attribute lookups and method calls in high-frequency function invocations?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_no_equivalent", "is_method": false, "class_name": null, "parameters": [], "calls": ["u.quantity_input", "test_unit", "pytest.raises", "myfunc_args", "test_quantity"], "code_location": {"file": "test_quantity_decorator.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 300, "end_line": 318}, "code_snippet": "def test_no_equivalent():\n    class test_unit:\n        pass\n\n    class test_quantity:\n        unit = test_unit()\n\n    @u.quantity_input(x=u.arcsec)\n    def myfunc_args(x):\n        return x\n\n    with pytest.raises(\n        TypeError,\n        match=(\n            \"Argument 'x' to function 'myfunc_args' has a 'unit' attribute without an\"\n            \" 'is_equivalent' method. You should pass in an astropy Quantity instead.\"\n        ),\n    ):\n        x, y = myfunc_args(test_quantity())\n", "type": "function"}, {"name": "test_equivalency_context", "is_method": false, "class_name": null, "parameters": [], "calls": ["u.GHz.find_equivalent_units", "all", "u.doppler_optical", "u.set_enabled_equivalencies", "u.Quantity", "assert_allclose", "assert_allclose", "u.Quantity", "assert_allclose", "view", "u.Quantity", "assert_allclose", "u.set_enabled_equivalencies", "u.GHz.to", "u.GHz.find_equivalent_units", "set", "set", "l1.to", "u.set_enabled_equivalencies", "u.dimensionless_angles", "np.exp", "np.exp", "pytest.raises", "phase.to", "u.cycle.to", "u.cycle.to", "u.spectral", "pytest.raises", "u.GHz.to", "abs", "np.array", "set", "to"], "code_location": {"file": "test_equivalencies.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 755, "end_line": 797}, "code_snippet": "def test_equivalency_context():\n    with u.set_enabled_equivalencies(u.dimensionless_angles()):\n        phase = u.Quantity(1.0, u.cycle)\n        assert_allclose(np.exp(1j * phase), 1.0)\n        Omega = u.cycle / (1.0 * u.minute)\n        assert_allclose(np.exp(1j * Omega * 60.0 * u.second), 1.0)\n        # ensure we can turn off equivalencies even within the scope\n        with pytest.raises(u.UnitsError):\n            phase.to(1, equivalencies=None)\n\n        # test the manager also works in the Quantity constructor.\n        q1 = u.Quantity(phase, u.dimensionless_unscaled)\n        assert_allclose(q1.value, u.cycle.to(u.radian))\n\n        # and also if we use a class that happens to have a unit attribute.\n        class MyQuantityLookalike(np.ndarray):\n            pass\n\n        mylookalike = np.array(1.0).view(MyQuantityLookalike)\n        mylookalike.unit = \"cycle\"\n        # test the manager also works in the Quantity constructor.\n        q2 = u.Quantity(mylookalike, u.dimensionless_unscaled)\n        assert_allclose(q2.value, u.cycle.to(u.radian))\n\n    with u.set_enabled_equivalencies(u.spectral()):\n        u.GHz.to(u.cm)\n        eq_on = u.GHz.find_equivalent_units()\n        with pytest.raises(u.UnitsError):\n            u.GHz.to(u.cm, equivalencies=None)\n\n    # without equivalencies, we should find a smaller (sub)set\n    eq_off = u.GHz.find_equivalent_units()\n    assert all(eq in set(eq_on) for eq in eq_off)\n    assert set(eq_off) < set(eq_on)\n\n    # Check the equivalency manager also works in ufunc evaluations,\n    # not just using (wrong) scaling. [#2496]\n    l2v = u.doppler_optical(6000 * u.angstrom)\n    l1 = 6010 * u.angstrom\n    assert l1.to(u.km / u.s, equivalencies=l2v) > 100.0 * u.km / u.s\n    with u.set_enabled_equivalencies(l2v):\n        assert l1 > 100.0 * u.km / u.s\n        assert abs((l1 - 500.0 * u.km / u.s).to(u.angstrom)) < 1.0 * u.km / u.s\n", "type": "function"}, {"name": "as_decorator", "is_method": true, "class_name": "QuantityInput", "parameters": ["cls", "func"], "calls": ["cls", "self"], "code_location": {"file": "decorators.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units", "start_line": 136, "end_line": 210}, "code_snippet": "    def as_decorator(cls, func=None, **kwargs):\n        r\"\"\"\n        A decorator for validating the units of arguments to functions.\n\n        Unit specifications can be provided as keyword arguments to the\n        decorator, or by using function annotation syntax. Arguments to the\n        decorator take precedence over any function annotations present.\n\n        A `~astropy.units.UnitsError` will be raised if the unit attribute of\n        the argument is not equivalent to the unit specified to the decorator or\n        in the annotation. If the argument has no unit attribute, i.e. it is not\n        a Quantity object, a `ValueError` will be raised unless the argument is\n        an annotation. This is to allow non Quantity annotations to pass\n        through.\n\n        Where an equivalency is specified in the decorator, the function will be\n        executed with that equivalency in force.\n\n        Notes\n        -----\n        The checking of arguments inside variable arguments to a function is not\n        supported (i.e. \\*arg or \\**kwargs).\n\n        The original function is accessible by the attributed ``__wrapped__``.\n        See :func:`functools.wraps` for details.\n\n        Examples\n        --------\n        .. code-block:: python\n\n            import astropy.units as u\n            @u.quantity_input(myangle=u.arcsec)\n            def myfunction(myangle):\n                return myangle**2\n\n\n        .. code-block:: python\n\n            import astropy.units as u\n            @u.quantity_input\n            def myfunction(myangle: u.arcsec):\n                return myangle**2\n\n        Or using a unit-aware Quantity annotation.\n\n        .. code-block:: python\n\n            @u.quantity_input\n            def myfunction(myangle: u.Quantity[u.arcsec]):\n                return myangle**2\n\n        Also you can specify a return value annotation, which will\n        cause the function to always return a `~astropy.units.Quantity` in that\n        unit.\n\n        .. code-block:: python\n\n            import astropy.units as u\n            @u.quantity_input\n            def myfunction(myangle: u.arcsec) -> u.deg**2:\n                return myangle**2\n\n        Using equivalencies::\n\n            import astropy.units as u\n            @u.quantity_input(myenergy=u.eV, equivalencies=u.mass_energy())\n            def myfunction(myenergy):\n                return myenergy**2\n\n        \"\"\"\n        self = cls(**kwargs)\n        if func is not None and not kwargs:\n            return self(func)\n        else:\n            return self\n", "type": "function"}, {"name": "test_only_ok_if_dimensionless", "is_method": true, "class_name": "TestLogQuantityMethods", "parameters": ["self", "method"], "calls": ["pytest.mark.parametrize", "np.all", "getattr", "pytest.raises", "getattr", "getattr"], "code_location": {"file": "test_logarithmic.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 1021, "end_line": 1026}, "code_snippet": "    def test_only_ok_if_dimensionless(self, method):\n        res = getattr(self.m1, method)()\n        assert np.all(res.value == getattr(self.m1._function_view, method)().value)\n        assert res.unit == self.m1.unit\n        with pytest.raises(TypeError):\n            getattr(self.mJy, method)()\n", "type": "function"}, {"name": "test_arg_equivalencies", "is_method": false, "class_name": null, "parameters": ["x_unit", "y_unit"], "calls": ["pytest.mark.parametrize", "u.quantity_input", "myfunc_args", "isinstance", "isinstance", "u.mass_energy"], "code_location": {"file": "test_quantity_decorator.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 271, "end_line": 282}, "code_snippet": "def test_arg_equivalencies(x_unit, y_unit):\n    @u.quantity_input(x=x_unit, y=y_unit, equivalencies=u.mass_energy())\n    def myfunc_args(x, y):\n        return x, y + (10 * u.J)  # Add an energy to check equiv is working\n\n    x, y = myfunc_args(1 * u.arcsec, 100 * u.gram)\n\n    assert isinstance(x, u.Quantity)\n    assert isinstance(y, u.Quantity)\n\n    assert x.unit == u.arcsec\n    assert y.unit == u.gram\n", "type": "function"}, {"name": "__call__", "is_method": true, "class_name": "QuantityInput", "parameters": ["self", "wrapped_function"], "calls": ["inspect.signature", "wraps", "wrapped_signature.bind", "wrapped_signature.parameters.values", "_validate_arg_value", "add_enabled_equivalencies", "contextlib.nullcontext", "wrapped_function", "_validate_arg_value", "_parse_annotation", "isinstance", "_parse_annotation", "isinstance", "len", "isinstance", "T.get_origin", "isinstance", "isinstance", "isinstance"], "code_location": {"file": "decorators.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units", "start_line": 217, "end_line": 344}, "code_snippet": "    def __call__(self, wrapped_function):\n        # Extract the function signature for the function we are wrapping.\n        wrapped_signature = inspect.signature(wrapped_function)\n\n        # Define a new function to return in place of the wrapped one\n        @wraps(wrapped_function)\n        def wrapper(*func_args, **func_kwargs):\n            # Bind the arguments to our new function to the signature of the original.\n            bound_args = wrapped_signature.bind(*func_args, **func_kwargs)\n\n            # Iterate through the parameters of the original signature\n            for param in wrapped_signature.parameters.values():\n                # We do not support variable arguments (*args, **kwargs)\n                if param.kind in (\n                    inspect.Parameter.VAR_KEYWORD,\n                    inspect.Parameter.VAR_POSITIONAL,\n                ):\n                    continue\n\n                # Catch the (never triggered) case where bind relied on a default value.\n                if (\n                    param.name not in bound_args.arguments\n                    and param.default is not param.empty\n                ):\n                    bound_args.arguments[param.name] = param.default\n\n                # Get the value of this parameter (argument to new function)\n                arg = bound_args.arguments[param.name]\n\n                # Get target unit or physical type, either from decorator kwargs\n                #   or annotations\n                if param.name in self.decorator_kwargs:\n                    targets = self.decorator_kwargs[param.name]\n                    is_annotation = False\n                else:\n                    targets = param.annotation\n                    is_annotation = True\n\n                    # parses to unit if it's an annotation (or list thereof)\n                    targets = _parse_annotation(targets)\n\n                # If the targets is empty, then no target units or physical\n                #   types were specified so we can continue to the next arg\n                if targets is inspect.Parameter.empty:\n                    continue\n\n                # If the argument value is None, and the default value is None,\n                #   pass through the None even if there is a target unit\n                if arg is None and param.default is None:\n                    continue\n\n                # Here, we check whether multiple target unit/physical type's\n                #   were specified in the decorator/annotation, or whether a\n                #   single string (unit or physical type) or a Unit object was\n                #   specified\n                if isinstance(targets, str) or not isinstance(targets, Sequence):\n                    valid_targets = [targets]\n\n                # Check for None in the supplied list of allowed units and, if\n                #   present and the passed value is also None, ignore.\n                elif None in targets or NoneType in targets:\n                    if arg is None:\n                        continue\n                    else:\n                        valid_targets = [t for t in targets if t is not None]\n\n                else:\n                    valid_targets = targets\n\n                # If we're dealing with an annotation, skip all the targets that\n                #    are not strings or subclasses of Unit. This is to allow\n                #    non unit related annotations to pass through\n                if is_annotation:\n                    valid_targets = [\n                        t\n                        for t in valid_targets\n                        if isinstance(t, (str, UnitBase, PhysicalType))\n                    ]\n\n                # Now we loop over the allowed units/physical types and validate\n                #   the value of the argument:\n                _validate_arg_value(\n                    param.name,\n                    wrapped_function.__name__,\n                    arg,\n                    valid_targets,\n                    self.equivalencies,\n                    self.strict_dimensionless,\n                )\n\n            if self.equivalencies:\n                equiv_context = add_enabled_equivalencies(self.equivalencies)\n            else:\n                # Avoid creating a duplicate registry if we don't have\n                # equivalencies to add. (If we're wrapping a short function,\n                # the time spent duplicating the registry is quite noticeable.)\n                equiv_context = contextlib.nullcontext()\n            # Call the original function with any equivalencies in force.\n            with equiv_context:\n                return_ = wrapped_function(*func_args, **func_kwargs)\n\n            # Return\n            ra = wrapped_signature.return_annotation\n            valid_empty = (inspect.Signature.empty, None, NoneType, T.NoReturn)\n            if ra not in valid_empty:\n                target = (\n                    ra\n                    if T.get_origin(ra) not in (T.Annotated, T.Union)\n                    else _parse_annotation(ra)\n                )\n                if isinstance(target, str) or not isinstance(target, Sequence):\n                    target = [target]\n                valid_targets = [\n                    t for t in target if isinstance(t, (str, UnitBase, PhysicalType))\n                ]\n                _validate_arg_value(\n                    \"return\",\n                    wrapped_function.__name__,\n                    return_,\n                    valid_targets,\n                    self.equivalencies,\n                    self.strict_dimensionless,\n                )\n                if len(valid_targets) > 0:\n                    return_ <<= valid_targets[0]\n            return return_\n\n        return wrapper\n", "type": "function"}, {"name": "test_arg_equivalencies3", "is_method": false, "class_name": null, "parameters": ["solarx_unit", "solary_unit"], "calls": ["pytest.mark.parametrize", "u.quantity_input", "myfunc_args", "isinstance", "isinstance", "u.mass_energy"], "code_location": {"file": "test_quantity_annotations.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 151, "end_line": 162}, "code_snippet": "def test_arg_equivalencies3(solarx_unit, solary_unit):\n    @u.quantity_input(equivalencies=u.mass_energy())\n    def myfunc_args(solarx: solarx_unit, solary: solary_unit):\n        return solarx, solary + (10 * u.J)  # Add an energy to check equiv is working\n\n    solarx, solary = myfunc_args(1 * u.arcsec, 100 * u.gram)\n\n    assert isinstance(solarx, Quantity)\n    assert isinstance(solary, Quantity)\n\n    assert solarx.unit == u.arcsec\n    assert solary.unit == u.gram\n", "type": "function"}, {"name": "test_quantity_conversion_equivalency_passed_on", "is_method": false, "class_name": null, "parameters": [], "calls": ["MySpectral", "q1.to", "q2.to", "assert_allclose", "MySpectral", "to", "assert_allclose", "u.spectral", "obj.view", "MySpectral", "q4.to"], "code_location": {"file": "test_quantity.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 819, "end_line": 838}, "code_snippet": "def test_quantity_conversion_equivalency_passed_on():\n    class MySpectral(u.Quantity):\n        _equivalencies = u.spectral()\n\n        def __quantity_view__(self, obj, unit):\n            return obj.view(MySpectral)\n\n        def __quantity_instance__(self, *args, **kwargs):\n            return MySpectral(*args, **kwargs)\n\n    q1 = MySpectral([1000, 2000], unit=u.Hz)\n    q2 = q1.to(u.nm)\n    assert q2.unit == u.nm\n    q3 = q2.to(u.Hz)\n    assert q3.unit == u.Hz\n    assert_allclose(q3.value, q1.value)\n    q4 = MySpectral([1000, 2000], unit=u.nm)\n    q5 = q4.to(u.Hz).to(u.nm)\n    assert q5.unit == u.nm\n    assert_allclose(q4.value, q5.value)\n", "type": "function"}, {"name": "test_units_decorator", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.mark.skipif", "quantity_support", "run_test", "Figure", "fig.add_subplot", "io.BytesIO", "ax.plot", "ax.plot", "ax.legend", "ax.fill_between", "fig.savefig", "ax.xaxis.get_units", "ax.yaxis.get_units"], "code_location": {"file": "test_units.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/visualization/tests", "start_line": 40, "end_line": 58}, "code_snippet": "def test_units_decorator():\n    @quantity_support()\n    def run_test():\n        fig = Figure()\n        ax = fig.add_subplot()\n\n        buff = io.BytesIO()\n\n        ax.plot([1, 2, 3] * u.m, [3, 4, 5] * u.kg, label=\"label\")\n        ax.plot([105, 210, 315] * u.cm, [3050, 3025, 3010] * u.g)\n        ax.legend()\n        # Also test fill_between, which requires actual conversion to ndarray.\n        ax.fill_between([1, 3] * u.m, [3, 5] * u.kg, [3050, 3010] * u.g)\n        fig.savefig(buff, format=\"svg\")\n\n        assert ax.xaxis.get_units() == u.m\n        assert ax.yaxis.get_units() == u.kg\n\n    run_test()\n", "type": "function"}, {"name": "test_equivalency_context_manager", "is_method": false, "class_name": null, "parameters": [], "calls": ["u.get_current_unit_registry", "len", "str"], "code_location": {"file": "test_units.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests", "start_line": 366, "end_line": 371}, "code_snippet": "def test_equivalency_context_manager():\n    base_registry = u.get_current_unit_registry()\n\n    # check starting with only the dimensionless_redshift equivalency.\n    assert len(base_registry.equivalencies) == 1\n    assert str(base_registry.equivalencies[0][0]) == \"redshift\"\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0068717002868652}
{"question": "Where in the control flow of the Simple Imaging Polynomial distortion correction method does the transformation of input coordinates pass through intermediate shifted values before polynomial distortion application?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "SIP", "docstring": "Simple Imaging Polynomial (SIP) model.\n\nThe SIP convention is used to represent distortions in FITS image headers.\nSee [1]_ for a description of the SIP convention.\n\nParameters\n----------\ncrpix : list or (2,) ndarray\n    CRPIX values\na_order : int\n    SIP polynomial order for first axis\nb_order : int\n    SIP order for second axis\na_coeff : dict\n    SIP coefficients for first axis\nb_coeff : dict\n    SIP coefficients for the second axis\nap_order : int\n    order for the inverse transformation (AP coefficients)\nbp_order : int\n    order for the inverse transformation (BP coefficients)\nap_coeff : dict\n    coefficients for the inverse transform\nbp_coeff : dict\n    coefficients for the inverse transform\n\nReferences\n----------\n.. [1] `David Shupe, et al, ADASS, ASP Conference Series, Vol. 347, 2005\n    <https://ui.adsabs.harvard.edu/abs/2005ASPC..347..491S>`_", "methods": ["__init__", "__repr__", "__str__", "inverse", "evaluate"], "attributes": ["n_inputs", "n_outputs", "_separable"], "code_location": {"file": "polynomial.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 1771, "end_line": 1885}, "type": "class"}, {"name": "_SIP1D", "docstring": "This implements the Simple Imaging Polynomial Model (SIP) in 1D.\n\nIt's unlikely it will be used in 1D so this class is private\nand SIP should be used instead.", "methods": ["__init__", "__repr__", "__str__", "evaluate", "get_num_coeff", "_generate_coeff_names", "_coeff_matrix", "_eval_sip"], "attributes": ["n_inputs", "n_outputs", "_separable"], "code_location": {"file": "polynomial.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 1658, "end_line": 1768}, "type": "class"}, {"name": "InverseSIP", "docstring": "Inverse Simple Imaging Polynomial.\n\nParameters\n----------\nap_order : int\n    order for the inverse transformation (AP coefficients)\nbp_order : int\n    order for the inverse transformation (BP coefficients)\nap_coeff : dict\n    coefficients for the inverse transform\nbp_coeff : dict\n    coefficients for the inverse transform", "methods": ["__init__", "__repr__", "__str__", "evaluate"], "attributes": ["n_inputs", "n_outputs", "_separable"], "code_location": {"file": "polynomial.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 1888, "end_line": 1957}, "type": "class"}, {"name": "sip_pix2foc", "is_method": true, "class_name": "WCS", "parameters": ["self"], "calls": ["self._array_converter", "len", "len", "TypeError"], "code_location": {"file": "wcs.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/wcs", "start_line": 2709, "end_line": 2717}, "code_snippet": "    def sip_pix2foc(self, *args):\n        if self.sip is None:\n            if len(args) == 2:\n                return args[0]\n            elif len(args) == 3:\n                return args[:2]\n            else:\n                raise TypeError(\"Wrong number of arguments\")\n        return self._array_converter(self.sip.pix2foc, None, *args)\n", "type": "function"}, {"name": "test_skycoord_to_pixel_distortions", "is_method": false, "class_name": null, "parameters": ["mode"], "calls": ["pytest.mark.parametrize", "get_pkg_data_filename", "SkyCoord", "skycoord_to_pixel", "transform_to", "assert_allclose", "assert_allclose", "pytest.warns", "WCS", "pixel_to_skycoord"], "code_location": {"file": "test_utils.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/wcs/tests", "start_line": 912, "end_line": 928}, "code_snippet": "def test_skycoord_to_pixel_distortions(mode):\n    # Import astropy.coordinates here to avoid circular imports\n    from astropy.coordinates import SkyCoord\n\n    header = get_pkg_data_filename(\"data/sip.fits\")\n    with pytest.warns(FITSFixedWarning):\n        wcs = WCS(header)\n\n    ref = SkyCoord(202.50 * u.deg, 47.19 * u.deg, frame=\"icrs\")\n\n    xp, yp = skycoord_to_pixel(ref, wcs, mode=mode)\n\n    # WCS is in FK5 so we need to transform back to ICRS\n    new = pixel_to_skycoord(xp, yp, wcs, mode=mode).transform_to(\"icrs\")\n\n    assert_allclose(new.ra.degree, ref.ra.degree)\n    assert_allclose(new.dec.degree, ref.dec.degree)\n", "type": "function"}, {"name": "_all_world2pix", "is_method": true, "class_name": "WCS", "parameters": ["self", "world", "origin", "tolerance", "maxiter", "adaptive", "detect_divergence", "quiet"], "calls": ["self.wcs_world2pix", "pix0.copy", "np.sum", "dn.copy", "np.seterr", "np.where", "np.seterr", "self.pix2foc", "np.geterr", "np.geterr", "np.all", "np.where", "np.sum", "np.where", "np.sum", "copy", "np.all", "np.isfinite", "NoConvergence", "NoConvergence", "np.nanmax", "self.pix2foc", "np.any", "all", "self.pix2foc", "np.square", "np.where", "np.where", "np.where", "np.isfinite", "np.where", "np.abs", "np.abs", "np.where", "np.where", "np.isfinite"], "code_location": {"file": "wcs.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/wcs", "start_line": 1803, "end_line": 2206}, "code_snippet": "    def _all_world2pix(\n        self, world, origin, tolerance, maxiter, adaptive, detect_divergence, quiet\n    ):\n        # ############################################################\n        # #          DESCRIPTION OF THE NUMERICAL METHOD            ##\n        # ############################################################\n        # In this section I will outline the method of solving\n        # the inverse problem of converting world coordinates to\n        # pixel coordinates (*inverse* of the direct transformation\n        # `all_pix2world`) and I will summarize some of the aspects\n        # of the method proposed here and some of the issues of the\n        # original `all_world2pix` (in relation to this method)\n        # discussed in https://github.com/astropy/astropy/issues/1977\n        # A more detailed discussion can be found here:\n        # https://github.com/astropy/astropy/pull/2373\n        #\n        #\n        #                  ### Background ###\n        #\n        #\n        # I will refer here to the [SIP Paper]\n        # (http://fits.gsfc.nasa.gov/registry/sip/SIP_distortion_v1_0.pdf).\n        # According to this paper, the effect of distortions as\n        # described in *their* equation (1) is:\n        #\n        # (1)   x = CD*(u+f(u)),\n        #\n        # where `x` is a *vector* of \"intermediate spherical\n        # coordinates\" (equivalent to (x,y) in the paper) and `u`\n        # is a *vector* of \"pixel coordinates\", and `f` is a vector\n        # function describing geometrical distortions\n        # (see equations 2 and 3 in SIP Paper.\n        # However, I prefer to use `w` for \"intermediate world\n        # coordinates\", `x` for pixel coordinates, and assume that\n        # transformation `W` performs the **linear**\n        # (CD matrix + projection onto celestial sphere) part of the\n        # conversion from pixel coordinates to world coordinates.\n        # Then we can re-write (1) as:\n        #\n        # (2)   w = W*(x+f(x)) = T(x)\n        #\n        # In `astropy.wcs.WCS` transformation `W` is represented by\n        # the `wcs_pix2world` member, while the combined (\"total\")\n        # transformation (linear part + distortions) is performed by\n        # `all_pix2world`. Below I summarize the notations and their\n        # equivalents in `astropy.wcs.WCS`:\n        #\n        # | Equation term | astropy.WCS/meaning          |\n        # | ------------- | ---------------------------- |\n        # | `x`           | pixel coordinates            |\n        # | `w`           | world coordinates            |\n        # | `W`           | `wcs_pix2world()`            |\n        # | `W^{-1}`      | `wcs_world2pix()`            |\n        # | `T`           | `all_pix2world()`            |\n        # | `x+f(x)`      | `pix2foc()`                  |\n        #\n        #\n        #      ### Direct Solving of Equation (2)  ###\n        #\n        #\n        # In order to find the pixel coordinates that correspond to\n        # given world coordinates `w`, it is necessary to invert\n        # equation (2): `x=T^{-1}(w)`, or solve equation `w==T(x)`\n        # for `x`. However, this approach has the following\n        # disadvantages:\n        #    1. It requires unnecessary transformations (see next\n        #       section).\n        #    2. It is prone to \"RA wrapping\" issues as described in\n        # https://github.com/astropy/astropy/issues/1977\n        # (essentially because `all_pix2world` may return points with\n        # a different phase than user's input `w`).\n        #\n        #\n        #      ### Description of the Method Used here ###\n        #\n        #\n        # By applying inverse linear WCS transformation (`W^{-1}`)\n        # to both sides of equation (2) and introducing notation `x'`\n        # (prime) for the pixels coordinates obtained from the world\n        # coordinates by applying inverse *linear* WCS transformation\n        # (\"focal plane coordinates\"):\n        #\n        # (3)   x' = W^{-1}(w)\n        #\n        # we obtain the following equation:\n        #\n        # (4)   x' = x+f(x),\n        #\n        # or,\n        #\n        # (5)   x = x'-f(x)\n        #\n        # This equation is well suited for solving using the method\n        # of fixed-point iterations\n        # (http://en.wikipedia.org/wiki/Fixed-point_iteration):\n        #\n        # (6)   x_{i+1} = x'-f(x_i)\n        #\n        # As an initial value of the pixel coordinate `x_0` we take\n        # \"focal plane coordinate\" `x'=W^{-1}(w)=wcs_world2pix(w)`.\n        # We stop iterations when `|x_{i+1}-x_i|<tolerance`. We also\n        # consider the process to be diverging if\n        # `|x_{i+1}-x_i|>|x_i-x_{i-1}|`\n        # **when** `|x_{i+1}-x_i|>=tolerance` (when current\n        # approximation is close to the true solution,\n        # `|x_{i+1}-x_i|>|x_i-x_{i-1}|` may be due to rounding errors\n        # and we ignore such \"divergences\" when\n        # `|x_{i+1}-x_i|<tolerance`). It may appear that checking for\n        # `|x_{i+1}-x_i|<tolerance` in order to ignore divergence is\n        # unnecessary since the iterative process should stop anyway,\n        # however, the proposed implementation of this iterative\n        # process is completely vectorized and, therefore, we may\n        # continue iterating over *some* points even though they have\n        # converged to within a specified tolerance (while iterating\n        # over other points that have not yet converged to\n        # a solution).\n        #\n        # In order to efficiently implement iterative process (6)\n        # using available methods in `astropy.wcs.WCS`, we add and\n        # subtract `x_i` from the right side of equation (6):\n        #\n        # (7)   x_{i+1} = x'-(x_i+f(x_i))+x_i = x'-pix2foc(x_i)+x_i,\n        #\n        # where `x'=wcs_world2pix(w)` and it is computed only *once*\n        # before the beginning of the iterative process (and we also\n        # set `x_0=x'`). By using `pix2foc` at each iteration instead\n        # of `all_pix2world` we get about 25% increase in performance\n        # (by not performing the linear `W` transformation at each\n        # step) and we also avoid the \"RA wrapping\" issue described\n        # above (by working in focal plane coordinates and avoiding\n        # pix->world transformations).\n        #\n        # As an added benefit, the process converges to the correct\n        # solution in just one iteration when distortions are not\n        # present (compare to\n        # https://github.com/astropy/astropy/issues/1977 and\n        # https://github.com/astropy/astropy/pull/2294): in this case\n        # `pix2foc` is the identical transformation\n        # `x_i=pix2foc(x_i)` and from equation (7) we get:\n        #\n        # x' = x_0 = wcs_world2pix(w)\n        # x_1 = x' - pix2foc(x_0) + x_0 = x' - pix2foc(x') + x' = x'\n        #     = wcs_world2pix(w) = x_0\n        # =>\n        # |x_1-x_0| = 0 < tolerance (with tolerance > 0)\n        #\n        # However, for performance reasons, it is still better to\n        # avoid iterations altogether and return the exact linear\n        # solution (`wcs_world2pix`) right-away when non-linear\n        # distortions are not present by checking that attributes\n        # `sip`, `cpdis1`, `cpdis2`, `det2im1`, and `det2im2` are\n        # *all* `None`.\n        #\n        #\n        #         ### Outline of the Algorithm ###\n        #\n        #\n        # While the proposed code is relatively long (considering\n        # the simplicity of the algorithm), this is due to: 1)\n        # checking if iterative solution is necessary at all; 2)\n        # checking for divergence; 3) re-implementation of the\n        # completely vectorized algorithm as an \"adaptive\" vectorized\n        # algorithm (for cases when some points diverge for which we\n        # want to stop iterations). In my tests, the adaptive version\n        # of the algorithm is about 50% slower than non-adaptive\n        # version for all HST images.\n        #\n        # The essential part of the vectorized non-adaptive algorithm\n        # (without divergence and other checks) can be described\n        # as follows:\n        #\n        #     pix0 = self.wcs_world2pix(world, origin)\n        #     pix  = pix0.copy() # 0-order solution\n        #\n        #     for k in range(maxiter):\n        #         # find correction to the previous solution:\n        #         dpix = self.pix2foc(pix, origin) - pix0\n        #\n        #         # compute norm (L2) of the correction:\n        #         dn = np.linalg.norm(dpix, axis=1)\n        #\n        #         # apply correction:\n        #         pix -= dpix\n        #\n        #         # check convergence:\n        #         if np.max(dn) < tolerance:\n        #             break\n        #\n        #    return pix\n        #\n        # Here, the input parameter `world` can be a `MxN` array\n        # where `M` is the number of coordinate axes in WCS and `N`\n        # is the number of points to be converted simultaneously to\n        # image coordinates.\n        #\n        #\n        #                ###  IMPORTANT NOTE:  ###\n        #\n        # If, in the future releases of the `~astropy.wcs`,\n        # `pix2foc` will not apply all the required distortion\n        # corrections then in the code below, calls to `pix2foc` will\n        # have to be replaced with\n        # wcs_world2pix(all_pix2world(pix_list, origin), origin)\n        #\n\n        # ############################################################\n        # #            INITIALIZE ITERATIVE PROCESS:                ##\n        # ############################################################\n\n        # initial approximation (linear WCS based only)\n        pix0 = self.wcs_world2pix(world, origin)\n\n        # Check that an iterative solution is required at all\n        # (when any of the non-CD-matrix-based corrections are\n        # present). If not required return the initial\n        # approximation (pix0).\n        if not self.has_distortion:\n            # No non-WCS corrections detected so\n            # simply return initial approximation:\n            return pix0\n\n        pix = pix0.copy()  # 0-order solution\n\n        # initial correction:\n        dpix = self.pix2foc(pix, origin) - pix0\n\n        # Update initial solution:\n        pix -= dpix\n\n        # Norm (L2) squared of the correction:\n        dn = np.sum(dpix * dpix, axis=1)\n        dnprev = dn.copy()  # if adaptive else dn\n        tol2 = tolerance**2\n\n        # Prepare for iterative process\n        k = 1\n        ind = None\n        inddiv = None\n\n        # Turn off numpy runtime warnings for 'invalid' and 'over':\n        old_invalid = np.geterr()[\"invalid\"]\n        old_over = np.geterr()[\"over\"]\n        np.seterr(invalid=\"ignore\", over=\"ignore\")\n\n        # ############################################################\n        # #                NON-ADAPTIVE ITERATIONS:                 ##\n        # ############################################################\n        if not adaptive:\n            # Fixed-point iterations:\n            while np.nanmax(dn) >= tol2 and k < maxiter:\n                # Find correction to the previous solution:\n                dpix = self.pix2foc(pix, origin) - pix0\n\n                # Compute norm (L2) squared of the correction:\n                dn = np.sum(dpix * dpix, axis=1)\n\n                # Check for divergence (we do this in two stages\n                # to optimize performance for the most common\n                # scenario when successive approximations converge):\n                if detect_divergence:\n                    divergent = dn >= dnprev\n                    if np.any(divergent):\n                        # Find solutions that have not yet converged:\n                        slowconv = dn >= tol2\n                        (inddiv,) = np.where(divergent & slowconv)\n\n                        if inddiv.shape[0] > 0:\n                            # Update indices of elements that\n                            # still need correction:\n                            conv = dn < dnprev\n                            iconv = np.where(conv)\n\n                            # Apply correction:\n                            dpixgood = dpix[iconv]\n                            pix[iconv] -= dpixgood\n                            dpix[iconv] = dpixgood\n\n                            # For the next iteration choose\n                            # non-divergent points that have not yet\n                            # converged to the requested accuracy:\n                            (ind,) = np.where(slowconv & conv)\n                            pix0 = pix0[ind]\n                            dnprev[ind] = dn[ind]\n                            k += 1\n\n                            # Switch to adaptive iterations:\n                            adaptive = True\n                            break\n                    # Save current correction magnitudes for later:\n                    dnprev = dn\n\n                # Apply correction:\n                pix -= dpix\n                k += 1\n\n        # ############################################################\n        # #                  ADAPTIVE ITERATIONS:                   ##\n        # ############################################################\n        if adaptive:\n            if ind is None:\n                (ind,) = np.where(np.isfinite(pix).all(axis=1))\n                pix0 = pix0[ind]\n\n            # \"Adaptive\" fixed-point iterations:\n            while ind.shape[0] > 0 and k < maxiter:\n                # Find correction to the previous solution:\n                dpixnew = self.pix2foc(pix[ind], origin) - pix0\n\n                # Compute norm (L2) of the correction:\n                dnnew = np.sum(np.square(dpixnew), axis=1)\n\n                # Bookkeeping of corrections:\n                dnprev[ind] = dn[ind].copy()\n                dn[ind] = dnnew\n\n                if detect_divergence:\n                    # Find indices of pixels that are converging:\n                    conv = dnnew < dnprev[ind]\n                    iconv = np.where(conv)\n                    iiconv = ind[iconv]\n\n                    # Apply correction:\n                    dpixgood = dpixnew[iconv]\n                    pix[iiconv] -= dpixgood\n                    dpix[iiconv] = dpixgood\n\n                    # Find indices of solutions that have not yet\n                    # converged to the requested accuracy\n                    # AND that do not diverge:\n                    (subind,) = np.where((dnnew >= tol2) & conv)\n\n                else:\n                    # Apply correction:\n                    pix[ind] -= dpixnew\n                    dpix[ind] = dpixnew\n\n                    # Find indices of solutions that have not yet\n                    # converged to the requested accuracy:\n                    (subind,) = np.where(dnnew >= tol2)\n\n                # Choose solutions that need more iterations:\n                ind = ind[subind]\n                pix0 = pix0[subind]\n\n                k += 1\n\n        # ############################################################\n        # #         FINAL DETECTION OF INVALID, DIVERGING,          ##\n        # #         AND FAILED-TO-CONVERGE POINTS                   ##\n        # ############################################################\n        # Identify diverging and/or invalid points:\n        invalid = (~np.all(np.isfinite(pix), axis=1)) & (\n            np.all(np.isfinite(world), axis=1)\n        )\n\n        # When detect_divergence==False, dnprev is outdated\n        # (it is the norm of the very first correction).\n        # Still better than nothing...\n        (inddiv,) = np.where(((dn >= tol2) & (dn >= dnprev)) | invalid)\n        if inddiv.shape[0] == 0:\n            inddiv = None\n\n        # Identify points that did not converge within 'maxiter'\n        # iterations:\n        if k >= maxiter:\n            (ind,) = np.where((dn >= tol2) & (dn < dnprev) & (~invalid))\n            if ind.shape[0] == 0:\n                ind = None\n        else:\n            ind = None\n\n        # Restore previous numpy error settings:\n        np.seterr(invalid=old_invalid, over=old_over)\n\n        # ############################################################\n        # #  RAISE EXCEPTION IF DIVERGING OR TOO SLOWLY CONVERGING  ##\n        # #  DATA POINTS HAVE BEEN DETECTED:                        ##\n        # ############################################################\n        if (ind is not None or inddiv is not None) and not quiet:\n            if inddiv is None:\n                raise NoConvergence(\n                    \"'WCS.all_world2pix' failed to \"\n                    f\"converge to the requested accuracy after {k:d} \"\n                    \"iterations.\",\n                    best_solution=pix,\n                    accuracy=np.abs(dpix),\n                    niter=k,\n                    slow_conv=ind,\n                    divergent=None,\n                )\n            else:\n                raise NoConvergence(\n                    \"'WCS.all_world2pix' failed to \"\n                    \"converge to the requested accuracy.\\n\"\n                    f\"After {k:d} iterations, the solution is diverging \"\n                    \"at least for one input point.\",\n                    best_solution=pix,\n                    accuracy=np.abs(dpix),\n                    niter=k,\n                    slow_conv=ind,\n                    divergent=inddiv,\n                )\n\n        return pix\n", "type": "function"}, {"name": "sip_foc2pix", "is_method": true, "class_name": "WCS", "parameters": ["self"], "calls": ["self._array_converter", "len", "len", "TypeError"], "code_location": {"file": "wcs.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/wcs", "start_line": 2750, "end_line": 2758}, "code_snippet": "    def sip_foc2pix(self, *args):\n        if self.sip is None:\n            if len(args) == 2:\n                return args[0]\n            elif len(args) == 3:\n                return args[:2]\n            else:\n                raise TypeError(\"Wrong number of arguments\")\n        return self._array_converter(self.sip.foc2pix, None, *args)\n", "type": "function"}, {"name": "_eval_sip", "is_method": true, "class_name": "_SIP1D", "parameters": ["self", "x", "y", "coef"], "calls": ["np.asarray", "np.asarray", "range", "np.zeros", "np.zeros", "range"], "code_location": {"file": "polynomial.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 1756, "end_line": 1768}, "code_snippet": "    def _eval_sip(self, x, y, coef):\n        x = np.asarray(x, dtype=np.float64)\n        y = np.asarray(y, dtype=np.float64)\n        if self.coeff_prefix == \"A\":\n            result = np.zeros(x.shape)\n        else:\n            result = np.zeros(y.shape)\n\n        for i in range(coef.shape[0]):\n            for j in range(coef.shape[1]):\n                if 1 < i + j < self.order + 1:\n                    result = result + coef[i, j] * x**i * y**j\n        return result\n", "type": "function"}, {"name": "fwd_eval", "is_method": true, "class_name": "SimModelTAB", "parameters": ["self", "xy"], "calls": ["np.array", "np.array", "np.array", "np.array", "np.atleast_2d", "np.logical_not", "astype", "np.dstack"], "code_location": {"file": "helper.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/wcs/tests", "start_line": 26, "end_line": 55}, "code_snippet": "    def fwd_eval(self, xy):\n        xb = 1 + self.nx // 3\n        px = np.array([1, xb, xb, self.nx + 1])\n        py = np.array([1, self.ny + 1])\n\n        xi = self.crval[0] + self.cdelt[0] * (px - self.crpix[0])\n        yi = self.crval[1] + self.cdelt[1] * (py - self.crpix[1])\n\n        cx = np.array([0.0, 0.26, 0.8, 1.0])\n        cy = np.array([-0.5, 0.5])\n\n        xy = np.atleast_2d(xy)\n        x = xy[:, 0]\n        y = xy[:, 1]\n\n        mbad = (x < px[0]) | (y < py[0]) | (x > px[-1]) | (y > py[-1])\n        mgood = np.logical_not(mbad)\n\n        i = 2 * (x > xb).astype(int)\n\n        psix = self.crval[0] + self.cdelt[0] * (x - self.crpix[0])\n        psiy = self.crval[1] + self.cdelt[1] * (y - self.crpix[1])\n\n        cfx = (psix - xi[i]) / (xi[i + 1] - xi[i])\n        cfy = (psiy - yi[0]) / (yi[1] - yi[0])\n\n        ra = cx[i] + cfx * (cx[i + 1] - cx[i])\n        dec = cy[0] + cfy * (cy[1] - cy[0])\n\n        return np.dstack([ra, dec])[0]\n", "type": "function"}, {"name": "imhorner", "is_method": true, "class_name": "OrthoPolynomialBase", "parameters": ["self", "x", "y", "coeff"], "calls": ["list", "_coeff.extend", "self._alpha", "len", "np.diff", "self._fcache", "range", "range", "range", "setattr", "range", "setattr", "range", "max", "setattr", "getattr", "str", "getattr", "str", "str", "str", "nonzero", "str"], "code_location": {"file": "polynomial.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 395, "end_line": 423}, "code_snippet": "    def imhorner(self, x, y, coeff):\n        _coeff = list(coeff)\n        _coeff.extend([0, 0, 0])\n        alpha = self._alpha()\n        r0 = _coeff[0]\n        nalpha = len(alpha)\n\n        karr = np.diff(alpha, axis=0)\n        kfunc = self._fcache(x, y)\n        x_terms = self.x_degree + 1\n        y_terms = self.y_degree + 1\n        nterms = x_terms + y_terms\n        for n in range(1, nterms + 1 + 3):\n            setattr(self, \"r\" + str(n), 0.0)\n\n        for n in range(1, nalpha):\n            k = karr[n - 1].nonzero()[0].max() + 1\n            rsum = 0\n            for i in range(1, k + 1):\n                rsum = rsum + getattr(self, \"r\" + str(i))\n            val = kfunc[k - 1] * (r0 + rsum)\n            setattr(self, \"r\" + str(k), val)\n            r0 = _coeff[n]\n            for i in range(1, k):\n                setattr(self, \"r\" + str(i), 0.0)\n        result = r0\n        for i in range(1, nterms + 1 + 3):\n            result = result + getattr(self, \"r\" + str(i))\n        return result\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.9856741428375244}
{"question": "Where in the control flow does the effective neutrino species parameter validation diverge when receiving a negative value versus a positive unitless quantity in the cosmology parameter test method?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test_Neff", "is_method": true, "class_name": "ParameterNeffTestMixin", "parameters": ["self", "cosmo_cls", "cosmo"], "calls": ["isinstance", "isinstance", "Neff.validate", "Neff.validate", "pytest.raises", "Neff.validate", "self.cls_kwargs.get"], "code_location": {"file": "test_parameters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 238, "end_line": 255}, "code_snippet": "    def test_Neff(self, cosmo_cls: type[Cosmology], cosmo: Cosmology):\n        \"\"\"Test Parameter ``Neff``.\"\"\"\n        # on the class\n        Neff = cosmo_cls.parameters[\"Neff\"]\n        assert isinstance(Neff, Parameter)\n        assert \"Number of effective neutrino species\" in Neff.__doc__\n        assert Neff.default == 3.04\n\n        # validation\n        assert Neff.validate(cosmo, 1) == 1\n        assert Neff.validate(cosmo, 10 * u.one) == 10\n        with pytest.raises(ValueError, match=\"Neff cannot be negative\"):\n            Neff.validate(cosmo, -1)\n\n        # on the instance\n        assert cosmo.Neff is cosmo.__dict__[\"Neff\"]\n        assert cosmo.Neff == self.cls_kwargs.get(\"Neff\", 3.04)\n        assert isinstance(cosmo.Neff, float)\n", "type": "function"}, {"name": "ParameterNeffTestMixin", "docstring": "Tests for `astropy.cosmology.Parameter` Neff on a Cosmology.\n\nNeff is a descriptor, which are tested by mixin, here with ``TestFLRW``.\nThese tests expect dicts ``_cls_args`` and ``cls_kwargs`` which give the\nargs and kwargs for the cosmology class, respectively. See ``TestFLRW``.", "methods": ["test_Neff", "test_init_Neff"], "attributes": [], "code_location": {"file": "test_parameters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 230, "end_line": 273}, "type": "class"}, {"name": "test_init_Neff", "is_method": true, "class_name": "ParameterNeffTestMixin", "parameters": ["self", "cosmo_cls", "ba"], "calls": ["cosmo_cls", "cosmo_cls", "pytest.raises", "cosmo_cls"], "code_location": {"file": "test_parameters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 257, "end_line": 273}, "code_snippet": "    def test_init_Neff(self, cosmo_cls: type[Cosmology], ba: BoundArguments):\n        \"\"\"Test initialization for values of ``Neff``.\"\"\"\n        # test that it works with units\n        ba.arguments[\"Neff\"] = (\n            cosmo_cls.parameters[\"Neff\"].default << u.one\n        )  # ensure units\n        cosmo = cosmo_cls(*ba.args, **ba.kwargs)\n        assert cosmo.Neff == ba.arguments[\"Neff\"]\n\n        # also without units\n        ba.arguments[\"Neff\"] = ba.arguments[\"Neff\"].value  # strip units\n        cosmo = cosmo_cls(*ba.args, **ba.kwargs)\n        assert cosmo.Neff == ba.arguments[\"Neff\"]\n\n        ba.arguments[\"Neff\"] = -1\n        with pytest.raises(ValueError):\n            cosmo_cls(*ba.args, **ba.kwargs)\n", "type": "function"}, {"name": "test_massivenu_density", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.mark.skipif", "np.array", "FlatLambdaCDM", "u.allclose", "u.allclose", "FlatLambdaCDM", "u.allclose", "np.array", "u.allclose", "FlatLambdaCDM", "u.allclose", "np.array", "u.allclose", "u.allclose", "u.allclose", "FlatLambdaCDM", "u.allclose", "np.array", "u.allclose", "ztest.astype", "u.allclose", "u.allclose", "np.array", "tcos.nu_relative_density", "tcos.efunc", "np.array", "tcos.nu_relative_density", "tcos.Onu", "np.array", "tcos.nu_relative_density", "tcos.Onu", "tcos.efunc", "tcos.inv_efunc", "np.array", "tcos.nu_relative_density", "tcos.Onu", "tcos.nu_relative_density", "tcos.Onu", "u.Quantity", "u.Quantity", "u.Quantity", "u.Quantity"], "code_location": {"file": "test_lambdacdm.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 542, "end_line": 602}, "code_snippet": "def test_massivenu_density():\n    # Testing neutrino density calculation\n\n    # Simple test cosmology, where we compare rho_nu and rho_gamma\n    # against the exact formula (eq 24/25 of Komatsu et al. 2011)\n    # computed using Mathematica.  The approximation we use for f(y)\n    # is only good to ~ 0.5% (with some redshift dependence), so that's\n    # what we test to.\n    ztest = np.array([0.0, 1.0, 2.0, 10.0, 1000.0])\n    nuprefac = 7.0 / 8.0 * (4.0 / 11.0) ** (4.0 / 3.0)\n    #  First try 3 massive neutrinos, all 100 eV -- note this is a universe\n    #  seriously dominated by neutrinos!\n    tcos = FlatLambdaCDM(75.0, 0.25, Tcmb0=3.0, Neff=3, m_nu=u.Quantity(100.0, u.eV))\n    assert tcos.has_massive_nu\n    assert tcos.Neff == 3\n    nurel_exp = (\n        nuprefac * tcos.Neff * np.array([171969, 85984.5, 57323, 15633.5, 171.801])\n    )\n    assert u.allclose(tcos.nu_relative_density(ztest), nurel_exp, rtol=5e-3)\n    assert u.allclose(tcos.efunc([0.0, 1.0]), [1.0, 7.46144727668], rtol=5e-3)\n\n    # Next, slightly less massive\n    tcos = FlatLambdaCDM(75.0, 0.25, Tcmb0=3.0, Neff=3, m_nu=u.Quantity(0.25, u.eV))\n    nurel_exp = (\n        nuprefac * tcos.Neff * np.array([429.924, 214.964, 143.312, 39.1005, 1.11086])\n    )\n    assert u.allclose(tcos.nu_relative_density(ztest), nurel_exp, rtol=5e-3)\n\n    # For this one also test Onu directly\n    onu_exp = np.array([0.01890217, 0.05244681, 0.0638236, 0.06999286, 0.1344951])\n    assert u.allclose(tcos.Onu(ztest), onu_exp, rtol=5e-3)\n\n    # And fairly light\n    tcos = FlatLambdaCDM(80.0, 0.30, Tcmb0=3.0, Neff=3, m_nu=u.Quantity(0.01, u.eV))\n\n    nurel_exp = (\n        nuprefac * tcos.Neff * np.array([17.2347, 8.67345, 5.84348, 1.90671, 1.00021])\n    )\n    assert u.allclose(tcos.nu_relative_density(ztest), nurel_exp, rtol=5e-3)\n    onu_exp = np.array([0.00066599, 0.00172677, 0.0020732, 0.00268404, 0.0978313])\n    assert u.allclose(tcos.Onu(ztest), onu_exp, rtol=5e-3)\n    assert u.allclose(tcos.efunc([1.0, 2.0]), [1.76225893, 2.97022048], rtol=1e-4)\n    assert u.allclose(tcos.inv_efunc([1.0, 2.0]), [0.5674535, 0.33667534], rtol=1e-4)\n\n    # Now a mixture of neutrino masses, with non-integer Neff\n    tcos = FlatLambdaCDM(\n        80.0, 0.30, Tcmb0=3.0, Neff=3.04, m_nu=u.Quantity([0.0, 0.01, 0.25], u.eV)\n    )\n    nurel_exp = (\n        nuprefac\n        * tcos.Neff\n        * np.array([149.386233, 74.87915, 50.0518, 14.002403, 1.03702333])\n    )\n    assert u.allclose(tcos.nu_relative_density(ztest), nurel_exp, rtol=5e-3)\n    onu_exp = np.array([0.00584959, 0.01493142, 0.01772291, 0.01963451, 0.10227728])\n    assert u.allclose(tcos.Onu(ztest), onu_exp, rtol=5e-3)\n\n    # Integer redshifts\n    ztest = ztest.astype(int)\n    assert u.allclose(tcos.nu_relative_density(ztest), nurel_exp, rtol=5e-3)\n    assert u.allclose(tcos.Onu(ztest), onu_exp, rtol=5e-3)\n", "type": "function"}, {"name": "test_m_nu", "is_method": true, "class_name": "Parameterm_nuTestMixin", "parameters": ["self", "cosmo_cls", "cosmo"], "calls": ["isinstance", "assert_quantity_allclose", "u.mass_energy", "assert_quantity_allclose", "assert_quantity_allclose", "assert_quantity_allclose"], "code_location": {"file": "test_parameters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 287, "end_line": 312}, "code_snippet": "    def test_m_nu(self, cosmo_cls: type[Cosmology], cosmo: Cosmology):\n        \"\"\"Test Parameter ``m_nu``.\"\"\"\n        # on the class\n        m_nu = cosmo_cls.parameters[\"m_nu\"]\n        assert isinstance(m_nu, Parameter)\n        assert \"Mass of neutrino species\" in m_nu.__doc__\n        assert m_nu.unit == u.eV\n        assert m_nu.equivalencies == u.mass_energy()\n        assert m_nu.default == 0.0 * u.eV\n\n        # on the instance\n        # assert cosmo.m_nu is cosmo._m_nu\n        assert_quantity_allclose(cosmo.m_nu, [0.0, 0.0, 0.0] * u.eV)\n\n        # set differently depending on the other inputs\n        if cosmo.Tnu0.value == 0:\n            assert cosmo.m_nu is None\n        elif not cosmo._massivenu:  # only massless\n            assert_quantity_allclose(cosmo.m_nu, 0 * u.eV)\n        elif self._nmasslessnu == 0:  # only massive\n            assert cosmo.m_nu == cosmo._massivenu_mass\n        else:  # a mix -- the most complicated case\n            assert_quantity_allclose(cosmo.m_nu[: self._nmasslessnu], 0 * u.eV)\n            assert_quantity_allclose(\n                cosmo.m_nu[self._nmasslessnu], cosmo._massivenu_mass\n            )\n", "type": "function"}, {"name": "test_Parameter_Ode0_validation", "is_method": true, "class_name": "ParameterOde0TestMixin", "parameters": ["self", "cosmo_cls", "cosmo"], "calls": ["cosmo_cls.parameters.get", "cosmo_cls._derived_parameters.get", "Ode0.validate", "Ode0.validate", "pytest.raises", "Ode0.validate"], "code_location": {"file": "test_parameters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 136, "end_line": 146}, "code_snippet": "    def test_Parameter_Ode0_validation(\n        self, cosmo_cls: type[Cosmology], cosmo: Cosmology\n    ):\n        \"\"\"Test Parameter ``Ode0`` validation.\"\"\"\n        Ode0 = cosmo_cls.parameters.get(\n            \"Ode0\", cosmo_cls._derived_parameters.get(\"Ode0\")\n        )\n        assert Ode0.validate(cosmo, 1.1) == 1.1\n        assert Ode0.validate(cosmo, 10 * u.one) == 10.0\n        with pytest.raises(TypeError, match=\"only dimensionless\"):\n            Ode0.validate(cosmo, 10 * u.km)\n", "type": "function"}, {"name": "Parameterm_nuTestMixin", "docstring": "Tests for `astropy.cosmology.Parameter` m_nu on a Cosmology.\n\nm_nu is a descriptor, which are tested by mixin, here with ``TestFLRW``.\nThese tests expect dicts ``_cls_args`` and ``cls_kwargs`` which give the\nargs and kwargs for the cosmology class, respectively. See ``TestFLRW``.", "methods": ["test_m_nu", "test_init_m_nu", "test_init_m_nu_and_Neff", "test_init_m_nu_override_by_Tcmb0"], "attributes": [], "code_location": {"file": "test_parameters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 279, "end_line": 392}, "type": "class"}, {"name": "test_init_m_nu_and_Neff", "is_method": true, "class_name": "Parameterm_nuTestMixin", "parameters": ["self", "cosmo_cls", "ba"], "calls": ["copy.copy", "u.Quantity", "cosmo_cls", "assert_quantity_allclose", "assert_quantity_allclose", "cosmo_cls", "assert_quantity_allclose", "pytest.raises", "cosmo_cls", "len", "cosmo.nu_relative_density", "len"], "code_location": {"file": "test_parameters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 339, "end_line": 370}, "code_snippet": "    def test_init_m_nu_and_Neff(self, cosmo_cls: type[Cosmology], ba: BoundArguments):\n        \"\"\"Test initialization for values of ``m_nu`` and ``Neff``.\n\n        Note this test requires ``Neff`` as constructor input, and a property\n        ``has_massive_nu``.\n        \"\"\"\n        # Mismatch with Neff = wrong number of neutrinos\n        tba = copy.copy(ba)\n        tba.arguments[\"Neff\"] = 4.05\n        tba.arguments[\"m_nu\"] = u.Quantity([0.15, 0.2, 0.1], u.eV)\n        with pytest.raises(ValueError, match=\"unexpected number of neutrino\"):\n            cosmo_cls(*tba.args, **tba.kwargs)\n\n        # No neutrinos, but Neff\n        tba.arguments[\"m_nu\"] = 0\n        cosmo = cosmo_cls(*tba.args, **tba.kwargs)\n        assert not cosmo.has_massive_nu\n        assert len(cosmo.m_nu) == 4\n        assert cosmo.m_nu.unit == u.eV\n        assert_quantity_allclose(cosmo.m_nu, 0 * u.eV)\n        # TODO! move this test when create ``test_nu_relative_density``\n        assert_quantity_allclose(\n            cosmo.nu_relative_density(1.0), 0.22710731766 * 4.05, rtol=1e-6\n        )\n\n        # All massive neutrinos case, len from Neff\n        tba.arguments[\"m_nu\"] = 0.1 * u.eV\n        cosmo = cosmo_cls(*tba.args, **tba.kwargs)\n        assert cosmo.has_massive_nu\n        assert len(cosmo.m_nu) == 4\n        assert cosmo.m_nu.unit == u.eV\n        assert_quantity_allclose(cosmo.m_nu, [0.1, 0.1, 0.1, 0.1] * u.eV)\n", "type": "function"}, {"name": "test_Ode0", "is_method": true, "class_name": "ParameterOde0TestMixin", "parameters": ["self", "cosmo"], "calls": ["isinstance"], "code_location": {"file": "test_parameters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 148, "end_line": 153}, "code_snippet": "    def test_Ode0(self, cosmo: Cosmology):\n        \"\"\"Test Parameter ``Ode0`` validation.\"\"\"\n        # if Ode0 is a parameter, test its value\n        assert cosmo.Ode0 is cosmo.__dict__[\"Ode0\"]\n        assert cosmo.Ode0 == self._cls_args[\"Ode0\"]\n        assert isinstance(cosmo.Ode0, float)\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "W1nu", "parameters": ["self"], "calls": ["__init__", "super"], "code_location": {"file": "test_w.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/cosmology/_src/tests/flrw", "start_line": 32, "end_line": 36}, "code_snippet": "    def __init__(self):\n        super().__init__(\n            70.0, 0.27, 0.73, Tcmb0=3.0, m_nu=0.1 * u.eV, name=\"test_cos_nu\"\n        )\n        self.__dict__[\"w0\"] = -0.8\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.960646390914917}
{"question": "Where in the data utilities module are the module-level functions responsible for raising the exception that indicates cache corruption during download cache consistency checks?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "check_download_cache", "is_method": false, "class_name": null, "parameters": ["pkgname"], "calls": ["set", "set", "_get_download_cache_loc", "os.scandir", "CacheDamaged", "os.path.abspath", "entry.name.startswith", "join", "os.path.join", "entry.is_dir", "bad_files.add", "messages.add", "os.listdir", "os.path.join", "bad_files.add", "messages.add", "os.path.join", "bad_files.add", "messages.add", "os.path.isfile", "bad_files.add", "messages.add", "get_file_contents", "os.path.isfile", "bad_files.add", "_is_url", "bad_files.add", "messages.add", "_url_to_dirname", "os.path.join", "messages.add", "messages.add", "bad_files.add", "messages.add"], "code_location": {"file": "data.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils", "start_line": 1956, "end_line": 2044}, "code_snippet": "def check_download_cache(pkgname=\"astropy\"):\n    \"\"\"Do a consistency check on the cache.\n\n    .. note::\n\n        Since v5.0, this function no longer returns anything.\n\n    Because the cache is shared by all versions of ``astropy`` in all virtualenvs\n    run by your user, possibly concurrently, it could accumulate problems.\n    This could lead to hard-to-debug problems or wasted space. This function\n    detects a number of incorrect conditions, including nonexistent files that\n    are indexed, files that are indexed but in the wrong place, and, if you\n    request it, files whose content does not match the hash that is indexed.\n\n    This function also returns a list of non-indexed files. A few will be\n    associated with the shelve object; their exact names depend on the backend\n    used but will probably be based on ``urlmap``. The presence of other files\n    probably indicates that something has gone wrong and inaccessible files\n    have accumulated in the cache. These can be removed with\n    :func:`clear_download_cache`, either passing the filename returned here, or\n    with no arguments to empty the entire cache and return it to a\n    reasonable, if empty, state.\n\n    Parameters\n    ----------\n    pkgname : str, optional\n        The package name to use to locate the download cache, i.e., for\n        ``pkgname='astropy'`` the default cache location is\n        ``~/.astropy/cache``.\n\n    Raises\n    ------\n    `~astropy.utils.data.CacheDamaged`\n        To indicate a problem with the cache contents; the exception contains\n        a ``.bad_files`` attribute containing a set of filenames to allow the\n        user to use :func:`clear_download_cache` to remove the offending items.\n    OSError, RuntimeError\n        To indicate some problem with the cache structure. This may need a full\n        :func:`clear_download_cache` to resolve, or may indicate some kind of\n        misconfiguration.\n    \"\"\"\n    bad_files = set()\n    messages = set()\n    dldir = _get_download_cache_loc(pkgname=pkgname)\n    with os.scandir(dldir) as it:\n        for entry in it:\n            f = os.path.abspath(os.path.join(dldir, entry.name))\n            if entry.name.startswith(\"rmtree-\"):\n                if f not in _tempfilestodel:\n                    bad_files.add(f)\n                    messages.add(f\"Cache entry {entry.name} not scheduled for deletion\")\n            elif entry.is_dir():\n                for sf in os.listdir(f):\n                    if sf in [\"url\", \"contents\"]:\n                        continue\n                    sf = os.path.join(f, sf)\n                    bad_files.add(sf)\n                    messages.add(f\"Unexpected file f{sf}\")\n                urlf = os.path.join(f, \"url\")\n                url = None\n                if not os.path.isfile(urlf):\n                    bad_files.add(urlf)\n                    messages.add(f\"Problem with URL file f{urlf}\")\n                else:\n                    url = get_file_contents(urlf, encoding=\"utf-8\")\n                    if not _is_url(url):\n                        bad_files.add(f)\n                        messages.add(f\"Malformed URL: {url}\")\n                    else:\n                        hashname = _url_to_dirname(url)\n                        if entry.name != hashname:\n                            bad_files.add(f)\n                            messages.add(\n                                f\"URL hashes to {hashname} but is stored in\"\n                                f\" {entry.name}\"\n                            )\n                if not os.path.isfile(os.path.join(f, \"contents\")):\n                    bad_files.add(f)\n                    if url is None:\n                        messages.add(f\"Hash {entry.name} is missing contents\")\n                    else:\n                        messages.add(\n                            f\"URL {url} with hash {entry.name} is missing contents\"\n                        )\n            else:\n                bad_files.add(f)\n                messages.add(f\"Left-over non-directory {f} in cache\")\n    if bad_files:\n        raise CacheDamaged(\"\\n\".join(messages), bad_files=bad_files)\n", "type": "function"}, {"name": "CacheDamaged", "docstring": "Record the URL or file that was a problem.\nUsing clear_download_cache on the .bad_file or .bad_url attribute,\nwhichever is not None, should resolve this particular problem.", "methods": ["__init__"], "attributes": [], "code_location": {"file": "data.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils", "start_line": 1944, "end_line": 1953}, "type": "class"}, {"name": "test_check_download_cache_finds_bogus_entries", "is_method": false, "class_name": null, "parameters": ["temp_cache", "valid_urls"], "calls": ["pytest.mark.filterwarnings", "next", "download_file", "_get_download_cache_loc", "os.path.abspath", "clear_download_cache", "os.path.join", "open", "f.write", "pytest.raises", "check_download_cache"], "code_location": {"file": "test_data.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 1543, "end_line": 1553}, "code_snippet": "def test_check_download_cache_finds_bogus_entries(temp_cache, valid_urls):\n    u, c = next(valid_urls)\n    download_file(u, cache=True)\n    dldir = _get_download_cache_loc()\n    bf = os.path.abspath(os.path.join(dldir, \"bogus\"))\n    with open(bf, \"w\") as f:\n        f.write(\"bogus file that exists\")\n    with pytest.raises(CacheDamaged) as e:\n        check_download_cache()\n    assert bf in e.value.bad_files\n    clear_download_cache()\n", "type": "function"}, {"name": "test_check_download_cache_cleanup", "is_method": false, "class_name": null, "parameters": ["temp_cache", "valid_urls"], "calls": ["pytest.mark.filterwarnings", "next", "download_file", "_get_download_cache_loc", "os.path.abspath", "os.path.abspath", "os.path.abspath", "next", "download_file", "os.unlink", "os.path.dirname", "os.path.join", "open", "f.write", "os.path.join", "open", "f.write", "os.path.join", "open", "f.write", "pytest.raises", "check_download_cache", "set", "clear_download_cache", "os.path.dirname"], "code_location": {"file": "test_data.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 1570, "end_line": 1596}, "code_snippet": "def test_check_download_cache_cleanup(temp_cache, valid_urls):\n    u, c = next(valid_urls)\n    fn = download_file(u, cache=True)\n    dldir = _get_download_cache_loc()\n\n    bf1 = os.path.abspath(os.path.join(dldir, \"bogus1\"))\n    with open(bf1, \"w\") as f:\n        f.write(\"bogus file that exists\")\n\n    bf2 = os.path.abspath(os.path.join(os.path.dirname(fn), \"bogus2\"))\n    with open(bf2, \"w\") as f:\n        f.write(\"other bogus file that exists\")\n\n    bf3 = os.path.abspath(os.path.join(dldir, \"contents\"))\n    with open(bf3, \"w\") as f:\n        f.write(\"awkwardly-named bogus file that exists\")\n\n    u2, c2 = next(valid_urls)\n    f2 = download_file(u, cache=True)\n    os.unlink(f2)\n    bf4 = os.path.dirname(f2)\n\n    with pytest.raises(CacheDamaged) as e:\n        check_download_cache()\n    assert set(e.value.bad_files) == {bf1, bf2, bf3, bf4}\n    for bf in e.value.bad_files:\n        clear_download_cache(bf)\n", "type": "function"}, {"name": "test_clear_download_cache_refuses_to_delete_outside_the_cache", "is_method": false, "class_name": null, "parameters": ["tmp_path"], "calls": ["pytest.mark.filterwarnings", "str", "os.path.exists", "os.path.exists", "open", "f.write", "pytest.raises", "clear_download_cache"], "code_location": {"file": "test_data.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 1532, "end_line": 1539}, "code_snippet": "def test_clear_download_cache_refuses_to_delete_outside_the_cache(tmp_path):\n    fn = str(tmp_path / \"file\")\n    with open(fn, \"w\") as f:\n        f.write(\"content\")\n    assert os.path.exists(fn)\n    with pytest.raises(RuntimeError):\n        clear_download_cache(fn)\n    assert os.path.exists(fn)\n", "type": "function"}, {"name": "test_check_download_cache_finds_bogus_subentries", "is_method": false, "class_name": null, "parameters": ["temp_cache", "valid_urls"], "calls": ["pytest.mark.filterwarnings", "next", "download_file", "os.path.abspath", "clear_download_cache", "os.path.join", "open", "f.write", "pytest.raises", "check_download_cache", "os.path.dirname"], "code_location": {"file": "test_data.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 1557, "end_line": 1566}, "code_snippet": "def test_check_download_cache_finds_bogus_subentries(temp_cache, valid_urls):\n    u, c = next(valid_urls)\n    f = download_file(u, cache=True)\n    bf = os.path.abspath(os.path.join(os.path.dirname(f), \"bogus\"))\n    with open(bf, \"w\") as f:\n        f.write(\"bogus file that exists\")\n    with pytest.raises(CacheDamaged) as e:\n        check_download_cache()\n    assert bf in e.value.bad_files\n    clear_download_cache()\n", "type": "function"}, {"name": "test_check_download_cache_works_if_readonly", "is_method": false, "class_name": null, "parameters": ["readonly_cache"], "calls": ["pytest.mark.filterwarnings", "check_download_cache"], "code_location": {"file": "test_data.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 1978, "end_line": 1979}, "code_snippet": "def test_check_download_cache_works_if_readonly(readonly_cache):\n    check_download_cache()\n", "type": "function"}, {"name": "test_clear_download_cache_raises_os_error", "is_method": false, "class_name": null, "parameters": ["temp_cache", "valid_urls", "monkeypatch"], "calls": ["pytest.mark.filterwarnings", "next", "download_file", "monkeypatch.setattr", "is_url_in_cache", "OSError", "pytest.warns", "clear_download_cache"], "code_location": {"file": "test_data.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 2318, "end_line": 2329}, "code_snippet": "def test_clear_download_cache_raises_os_error(temp_cache, valid_urls, monkeypatch):\n    def no_rename(path, mode=None):\n        raise OSError(errno.EBUSY, \"os.rename monkeypatched out\")\n\n    u, c = next(valid_urls)\n    download_file(u, cache=True)\n\n    monkeypatch.setattr(os, \"rename\", no_rename)\n\n    assert is_url_in_cache(u)\n    with pytest.warns(CacheMissingWarning, match=\"os.rename monkeypatched out\"):\n        clear_download_cache(u)\n", "type": "function"}, {"name": "test_download_cache_after_clear", "is_method": false, "class_name": null, "parameters": ["tmp_path", "temp_cache", "valid_urls"], "calls": ["pytest.mark.filterwarnings", "next", "_get_download_cache_loc", "download_file", "os.path.isfile", "clear_download_cache", "download_file", "os.path.isfile", "os.path.exists", "os.path.exists"], "code_location": {"file": "test_data.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/tests", "start_line": 680, "end_line": 694}, "code_snippet": "def test_download_cache_after_clear(tmp_path, temp_cache, valid_urls):\n    testurl, contents = next(valid_urls)\n    # Test issues raised in #4427 with clear_download_cache() without a URL,\n    # followed by subsequent download.\n    download_dir = _get_download_cache_loc()\n\n    fnout = download_file(testurl, cache=True)\n    assert os.path.isfile(fnout)\n\n    clear_download_cache()\n    assert not os.path.exists(fnout)\n    assert not os.path.exists(download_dir)\n\n    fnout = download_file(testurl, cache=True)\n    assert os.path.isfile(fnout)\n", "type": "function"}, {"name": "CacheMissingWarning", "docstring": "This warning indicates the standard cache directory is not accessible, with\nthe first argument providing the warning message. If args[1] is present, it\nis a filename indicating the path to a temporary file that was created to\nstore a remote data download in the absence of the cache.", "methods": [], "attributes": [], "code_location": {"file": "data.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils", "start_line": 149, "end_line": 155}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.9341139793395996}
{"question": "Where in the codebase is the classmethod that converts FITS keyword strings to uppercase before they are used as keys in the header's keyword-to-index mapping dictionary located?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "normalize_keyword", "is_method": true, "class_name": "Card", "parameters": ["cls", "keyword"], "calls": ["cls._rvkc_keyword_name_RE.match", "cls._keywd_FSC_RE.match", "join", "len", "upper", "upper", "upper", "match.group", "len", "upper", "strip", "keyword.strip", "strip", "match.group"], "code_location": {"file": "card.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits", "start_line": 575, "end_line": 605}, "code_snippet": "    def normalize_keyword(cls, keyword):\n        \"\"\"\n        `classmethod` to convert a keyword value that may contain a\n        field-specifier to uppercase.  The effect is to raise the key to\n        uppercase and leave the field specifier in its original case.\n\n        Parameters\n        ----------\n        keyword : or str\n            A keyword value or a ``keyword.field-specifier`` value\n        \"\"\"\n        # Test first for the most common case: a standard FITS keyword provided\n        # in standard all-caps\n        if len(keyword) <= KEYWORD_LENGTH and cls._keywd_FSC_RE.match(keyword):\n            return keyword\n\n        # Test if this is a record-valued keyword\n        match = cls._rvkc_keyword_name_RE.match(keyword)\n\n        if match:\n            return \".\".join(\n                (match.group(\"keyword\").strip().upper(), match.group(\"field_specifier\"))\n            )\n        elif len(keyword) > 9 and keyword[:9].upper() == \"HIERARCH \":\n            # Remove 'HIERARCH' from HIERARCH keywords; this could lead to\n            # ambiguity if there is actually a keyword card containing\n            # \"HIERARCH HIERARCH\", but shame on you if you do that.\n            return keyword[9:].strip().upper()\n        else:\n            # A normal FITS keyword, but provided in non-standard case\n            return keyword.strip().upper()\n", "type": "function"}, {"name": "Header", "docstring": "FITS header class.  This class exposes both a dict-like interface and a\nlist-like interface to FITS headers.\n\nThe header may be indexed by keyword and, like a dict, the associated value\nwill be returned.  When the header contains cards with duplicate keywords,\nonly the value of the first card with the given keyword will be returned.\nIt is also possible to use a 2-tuple as the index in the form (keyword,\nn)--this returns the n-th value with that keyword, in the case where there\nare duplicate keywords.\n\nFor example::\n\n    >>> header['NAXIS']\n    0\n    >>> header[('FOO', 1)]  # Return the value of the second FOO keyword\n    'foo'\n\nThe header may also be indexed by card number::\n\n    >>> header[0]  # Return the value of the first card in the header\n    'T'\n\nCommentary keywords such as HISTORY and COMMENT are special cases: When\nindexing the Header object with either 'HISTORY' or 'COMMENT' a list of all\nthe HISTORY/COMMENT values is returned::\n\n    >>> header['HISTORY']\n    This is the first history entry in this header.\n    This is the second history entry in this header.\n    ...\n\nSee the Astropy documentation for more details on working with headers.\n\nNotes\n-----\nAlthough FITS keywords must be exclusively upper case, retrieving an item\nin a `Header` object is case insensitive.", "methods": ["__init__", "__len__", "__iter__", "__contains__", "__getitem__", "__setitem__", "__delitem__", "__repr__", "__str__", "__eq__", "__add__", "__iadd__", "_ipython_key_completions_", "cards", "comments", "_modified", "_modified", "fromstring", "fromfile", "_fromcards", "_from_blocks", "_find_end_card", "tostring", "tofile", "fromtextfile", "totextfile", "clear", "copy", "__copy__", "__deepcopy__", "fromkeys", "get", "set", "items", "keys", "values", "pop", "popitem", "setdefault", "update", "append", "extend", "count", "index", "insert", "remove", "rename_keyword", "add_history", "add_comment", "add_blank", "strip", "data_size", "data_size_padded", "_update", "_cardindex", "_keyword_from_index", "_relativeinsert", "_updateindices", "_countblanks", "_useblanks", "_haswildcard", "_wildcardmatch", "_set_slice", "_splitcommentary", "_add_commentary"], "attributes": [], "code_location": {"file": "header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits", "start_line": 55, "end_line": 1913}, "type": "class"}, {"name": "test_header_method_keyword_normalization", "is_method": true, "class_name": "TestHeaderFunctions", "parameters": ["self"], "calls": ["fits.Header", "h.set", "h.set", "h.update", "h.remove", "list", "len", "len", "list", "len", "h.get", "list", "h.pop", "len", "h.setdefault", "len", "h.setdefault", "len", "list", "len", "list", "h.count", "h.index", "len", "list"], "code_location": {"file": "test_header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 2205, "end_line": 2260}, "code_snippet": "    def test_header_method_keyword_normalization(self):\n        \"\"\"\n        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/149\n\n        Basically ensures that all public Header methods are case-insensitive\n        w.r.t. keywords.\n\n        Provides a reasonably comprehensive test of several methods at once.\n        \"\"\"\n\n        h = fits.Header([(\"abC\", 1), (\"Def\", 2), (\"GeH\", 3)])\n        assert list(h) == [\"ABC\", \"DEF\", \"GEH\"]\n        assert \"abc\" in h\n        assert \"dEf\" in h\n\n        assert h[\"geh\"] == 3\n\n        # Case insensitivity of wildcards\n        assert len(h[\"g*\"]) == 1\n\n        h[\"aBc\"] = 2\n        assert h[\"abc\"] == 2\n        # ABC already existed so assigning to aBc should not have added any new\n        # cards\n        assert len(h) == 3\n\n        del h[\"gEh\"]\n        assert list(h) == [\"ABC\", \"DEF\"]\n        assert len(h) == 2\n        assert h.get(\"def\") == 2\n\n        h.set(\"Abc\", 3)\n        assert h[\"ABC\"] == 3\n        h.set(\"gEh\", 3, before=\"Abc\")\n        assert list(h) == [\"GEH\", \"ABC\", \"DEF\"]\n\n        assert h.pop(\"abC\") == 3\n        assert len(h) == 2\n\n        assert h.setdefault(\"def\", 3) == 2\n        assert len(h) == 2\n        assert h.setdefault(\"aBc\", 1) == 1\n        assert len(h) == 3\n        assert list(h) == [\"GEH\", \"DEF\", \"ABC\"]\n\n        h.update({\"GeH\": 1, \"iJk\": 4})\n        assert len(h) == 4\n        assert list(h) == [\"GEH\", \"DEF\", \"ABC\", \"IJK\"]\n        assert h[\"GEH\"] == 1\n\n        assert h.count(\"ijk\") == 1\n        assert h.index(\"ijk\") == 3\n\n        h.remove(\"Def\")\n        assert len(h) == 3\n        assert list(h) == [\"GEH\", \"ABC\", \"IJK\"]\n", "type": "function"}, {"name": "TableHeaderFormatter", "docstring": "Class to convert the header(s) of a FITS file into a Table object.\nThe table returned by the `parse` method will contain four columns:\nfilename, hdu, keyword, and value.\n\nSubclassed from HeaderFormatter, which contains the meat of the formatting.", "methods": ["_parse_internal"], "attributes": [], "code_location": {"file": "fitsheader.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/scripts", "start_line": 238, "end_line": 267}, "type": "class"}, {"name": "_BasicHeader", "docstring": "This class provides a fast header parsing, without all the additional\nfeatures of the Header class. Here only standard keywords are parsed, no\nsupport for CONTINUE, HIERARCH, COMMENT, HISTORY, or rvkc.\n\nThe raw card images are stored and parsed only if needed. The idea is that\nto create the HDU objects, only a small subset of standard cards is needed.\nOnce a card is parsed, which is deferred to the Card class, the Card object\nis kept in a cache. This is useful because a small subset of cards is used\na lot in the HDU creation process (NAXIS, XTENSION, ...).", "methods": ["__init__", "__getitem__", "__len__", "__iter__", "index", "data_size", "data_size_padded", "fromfile"], "attributes": [], "code_location": {"file": "header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits", "start_line": 1976, "end_line": 2054}, "type": "class"}, {"name": "test_indexing_case", "is_method": true, "class_name": "TestHeaderFunctions", "parameters": ["self", "key"], "calls": ["pytest.mark.parametrize", "fits.Header", "header.remove", "header.get", "header.index", "header.count"], "code_location": {"file": "test_header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 82, "end_line": 91}, "code_snippet": "    def test_indexing_case(self, key):\n        \"\"\"Check that indexing is case insensitive\"\"\"\n        header = fits.Header([(\"A\", \"B\", \"C\"), (\"D\", \"E\", \"F\")])\n        assert key in header\n        assert header[key] == \"B\"\n        assert header.get(key) == \"B\"\n        assert header.index(key) == 0\n        assert header.comments[key] == \"C\"\n        assert header.count(key) == 1\n        header.remove(key, ignore_missing=False)\n", "type": "function"}, {"name": "_parse_keyword", "is_method": true, "class_name": "Card", "parameters": ["self"], "calls": ["strip", "keyword.upper", "keyword.strip", "self._image.find", "self._check_if_rvkc_image", "warnings.warn", "self._image.split"], "code_location": {"file": "card.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits", "start_line": 705, "end_line": 748}, "code_snippet": "    def _parse_keyword(self):\n        keyword = self._image[:KEYWORD_LENGTH].strip()\n        keyword_upper = keyword.upper()\n\n        if keyword_upper in self._special_keywords:\n            return keyword_upper\n        elif (\n            keyword_upper == \"HIERARCH\"\n            and self._image[8] == \" \"\n            and HIERARCH_VALUE_INDICATOR in self._image\n        ):\n            # This is valid HIERARCH card as described by the HIERARCH keyword\n            # convention:\n            # http://fits.gsfc.nasa.gov/registry/hierarch_keyword.html\n            self._hierarch = True\n            self._value_indicator = HIERARCH_VALUE_INDICATOR\n            keyword = self._image.split(HIERARCH_VALUE_INDICATOR, 1)[0][9:]\n            return keyword.strip()\n        else:\n            val_ind_idx = self._image.find(VALUE_INDICATOR)\n            if 0 <= val_ind_idx <= KEYWORD_LENGTH:\n                # The value indicator should appear in byte 8, but we are\n                # flexible and allow this to be fixed\n                if val_ind_idx < KEYWORD_LENGTH:\n                    keyword = keyword[:val_ind_idx]\n                    keyword_upper = keyword_upper[:val_ind_idx]\n\n                rest = self._image[val_ind_idx + VALUE_INDICATOR_LEN :]\n\n                # So far this looks like a standard FITS keyword; check whether\n                # the value represents a RVKC; if so then we pass things off to\n                # the RVKC parser\n                if self._check_if_rvkc_image(keyword, rest):\n                    return self._keyword\n\n                return keyword_upper\n            else:\n                warnings.warn(\n                    \"The following header keyword is invalid or follows an \"\n                    f\"unrecognized non-standard convention:\\n{self._image}\",\n                    AstropyUserWarning,\n                )\n                self._invalid = True\n                return keyword\n", "type": "function"}, {"name": "FITSFixedWarning", "docstring": "The warning raised when the contents of the FITS header have been\nmodified to be standards compliant.", "methods": [], "attributes": [], "code_location": {"file": "wcs.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/wcs", "start_line": 352, "end_line": 356}, "type": "class"}, {"name": "test_hierarch_card_lookup", "is_method": true, "class_name": "TestHeaderFunctions", "parameters": ["self"], "calls": ["fits.Header"], "code_location": {"file": "test_header.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/tests", "start_line": 825, "end_line": 833}, "code_snippet": "    def test_hierarch_card_lookup(self):\n        header = fits.Header()\n        header[\"hierarch abcdefghi\"] = 10\n        assert \"abcdefghi\" in header\n        assert header[\"abcdefghi\"] == 10\n        # This used to be assert_false, but per ticket\n        # https://aeon.stsci.edu/ssb/trac/pyfits/ticket/155 hierarch keywords\n        # should be treated case-insensitively when performing lookups\n        assert \"ABCDEFGHI\" in header\n", "type": "function"}, {"name": "HeaderFormatter", "docstring": "Class to format the header(s) of a FITS file for display by the\n`fitsheader` tool; essentially a wrapper around a `HDUList` object.\n\nExample usage:\nfmt = HeaderFormatter('/path/to/file.fits')\nprint(fmt.parse(extensions=[0, 3], keywords=['NAXIS', 'BITPIX']))\n\nParameters\n----------\nfilename : str\n    Path to a single FITS file.\nverbose : bool\n    Verbose flag, to show more information about missing extensions,\n    keywords, etc.\n\nRaises\n------\nOSError\n    If `filename` does not exist or cannot be read.", "methods": ["__init__", "parse", "_parse_internal", "_get_cards", "close"], "attributes": [], "code_location": {"file": "fitsheader.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/fits/scripts", "start_line": 89, "end_line": 235}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.9137518405914307}
{"question": "Where is the base class for all fitters defined and which module implements constraint processing for the Sequential Least Squares Programming fitter?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "_NonLinearLSQFitter", "docstring": "Base class for Non-Linear least-squares fitters.\n\nParameters\n----------\ncalc_uncertainties : bool\n    If the covariance matrix should be computed and set in the fit_info.\n    Default: False\nuse_min_max_bounds : bool\n    If set, the parameter bounds for a model will be enforced for each given\n    parameter while fitting via a simple min/max condition.\n    Default: True", "methods": ["__init__", "objective_function", "_add_fitting_uncertainties", "_wrap_deriv", "_compute_param_cov", "_run_fitter", "_filter_non_finite", "__call__"], "attributes": ["supported_constraints"], "code_location": {"file": "fitting.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 1116, "end_line": 1438}, "type": "class"}, {"name": "SLSQPLSQFitter", "docstring": "Sequential Least Squares Programming (SLSQP) optimization algorithm and\nleast squares statistic.\n\nRaises\n------\nModelLinearityError\n    A linear model is passed to a nonlinear fitter\n\nNotes\n-----\nSee also the `~astropy.modeling.optimizers.SLSQP` optimizer.", "methods": ["__init__", "__call__"], "attributes": ["supported_constraints"], "code_location": {"file": "fitting.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 1745, "end_line": 1844}, "type": "class"}, {"name": "Fitter", "docstring": "Base class for all fitters.\n\nParameters\n----------\noptimizer : callable\n    A callable implementing an optimization algorithm\nstatistic : callable\n    Statistic function", "methods": ["__init_subclass__", "__init__", "objective_function", "_add_fitting_uncertainties", "__call__"], "attributes": ["_subclass_registry", "supported_constraints"], "code_location": {"file": "fitting.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 293, "end_line": 372}, "type": "class"}, {"name": "_NLLSQFitter", "docstring": "Wrapper class for `scipy.optimize.least_squares` method, which provides:\n    - Trust Region Reflective\n    - dogbox\n    - Levenberg-Marquardt\nalgorithms using the least squares statistic.\n\nParameters\n----------\nmethod : str\n    trf :  Trust Region Reflective algorithm, particularly suitable\n        for large sparse problems with bounds. Generally robust method.\n    dogbox : dogleg algorithm with rectangular trust regions, typical\n        use case is small problems with bounds. Not recommended for\n        problems with rank-deficient Jacobian.\n    lm : Levenberg-Marquardt algorithm as implemented in MINPACK.\n        Doesnt handle bounds and sparse Jacobians. Usually the most\n        efficient method for small unconstrained problems.\ncalc_uncertainties : bool\n    If the covariance matrix should be computed and set in the fit_info.\n    Default: False\nuse_min_max_bounds: bool\n    If set, the parameter bounds for a model will be enforced for each given\n    parameter while fitting via a simple min/max condition. A True setting\n    will replicate how LevMarLSQFitter enforces bounds.\n    Default: False\n\nAttributes\n----------\nfit_info :\n    A `scipy.optimize.OptimizeResult` class which contains all of\n    the most recent fit information", "methods": ["__init__", "_run_fitter"], "attributes": [], "code_location": {"file": "fitting.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 1530, "end_line": 1632}, "type": "class"}, {"name": "SimplexLSQFitter", "docstring": "Simplex algorithm and least squares statistic.\n\nRaises\n------\n`ModelLinearityError`\n    A linear model is passed to a nonlinear fitter", "methods": ["__init__", "__call__"], "attributes": ["supported_constraints"], "code_location": {"file": "fitting.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 1847, "end_line": 1936}, "type": "class"}, {"name": "TRFLSQFitter", "docstring": "Trust Region Reflective algorithm and least squares statistic.\n\nParameters\n----------\ncalc_uncertainties : bool\n    If the covariance matrix should be computed and set in the fit_info.\n    Default: False\n\nAttributes\n----------\nfit_info :\n    A `scipy.optimize.OptimizeResult` class which contains all of\n    the most recent fit information", "methods": ["__init__"], "attributes": [], "code_location": {"file": "fitting.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 1635, "end_line": 1654}, "type": "class"}, {"name": "UnsupportedConstraintError", "docstring": "Raised when a fitter does not support a type of constraint.", "methods": [], "attributes": [], "code_location": {"file": "fitting.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 177, "end_line": 180}, "type": "class"}, {"name": "LMLSQFitter", "docstring": "`scipy.optimize.least_squares` Levenberg-Marquardt algorithm and least squares statistic.\n\nParameters\n----------\ncalc_uncertainties : bool\n    If the covariance matrix should be computed and set in the fit_info.\n    Default: False\n\nAttributes\n----------\nfit_info :\n    A `scipy.optimize.OptimizeResult` class which contains all of\n    the most recent fit information", "methods": ["__init__", "__call__"], "attributes": [], "code_location": {"file": "fitting.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 1679, "end_line": 1742}, "type": "class"}, {"name": "test_LSQ_SLSQP_with_constraints", "is_method": true, "class_name": "TestNonLinearFitters", "parameters": ["self", "fitter"], "calls": ["pytest.mark.filterwarnings", "pytest.mark.parametrize", "fitter", "models.Gaussian1D", "SLSQPLSQFitter", "fslsqp", "fitter", "assert_allclose"], "code_location": {"file": "test_fitters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling/tests", "start_line": 515, "end_line": 527}, "code_snippet": "    def test_LSQ_SLSQP_with_constraints(self, fitter):\n        \"\"\"\n        Runs `LevMarLSQFitter`/`TRFLSQFitter` and `SLSQPLSQFitter` on a\n        model with constraints.\n        \"\"\"\n        fitter = fitter()\n\n        g1 = models.Gaussian1D(100, 5, stddev=1)\n        g1.mean.fixed = True\n        fslsqp = SLSQPLSQFitter()\n        slsqp_model = fslsqp(g1, self.xdata, self.ydata)\n        model = fitter(g1, self.xdata, self.ydata)\n        assert_allclose(model.parameters, slsqp_model.parameters, rtol=10 ** (-4))\n", "type": "function"}, {"name": "_validate_constraints", "is_method": false, "class_name": null, "parameters": ["supported_constraints", "model"], "calls": ["UnsupportedConstraintError", "UnsupportedConstraintError", "UnsupportedConstraintError", "UnsupportedConstraintError", "UnsupportedConstraintError", "message.format", "message.format", "message.format", "message.format", "message.format"], "code_location": {"file": "fitting.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 2283, "end_line": 2300}, "code_snippet": "def _validate_constraints(supported_constraints, model):\n    \"\"\"Make sure model constraints are supported by the current fitter.\"\"\"\n    message = \"Optimizer cannot handle {0} constraints.\"\n\n    if model.has_fixed and \"fixed\" not in supported_constraints:\n        raise UnsupportedConstraintError(message.format(\"fixed parameter\"))\n\n    if model.has_tied and \"tied\" not in supported_constraints:\n        raise UnsupportedConstraintError(message.format(\"tied parameter\"))\n\n    if model.has_bounds and \"bounds\" not in supported_constraints:\n        raise UnsupportedConstraintError(message.format(\"bound parameter\"))\n\n    if model.eqcons and \"eqcons\" not in supported_constraints:\n        raise UnsupportedConstraintError(message.format(\"equality\"))\n\n    if model.ineqcons and \"ineqcons\" not in supported_constraints:\n        raise UnsupportedConstraintError(message.format(\"inequality\"))\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.8968863487243652}
{"question": "Where during XML serialization in the VOTable format handling module is the warning condition that detects masked bit datatype values evaluated?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "W39", "docstring": "Bit values do not support masking.  This warning is raised upon\nsetting masked data in a bit column.\n\n**References**: `1.1\n<http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#sec:datatypes>`__,\n`1.2\n<http://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html#sec:datatypes>`__", "methods": [], "attributes": ["message_template"], "code_location": {"file": "exceptions.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 915, "end_line": 926}, "type": "class"}, {"name": "test_null_integer_binary", "is_method": true, "class_name": "TestThroughBinary", "parameters": ["self"], "calls": ["io.BytesIO", "pytest.warns", "self.votable.to_xml"], "code_location": {"file": "test_vo.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable/tests", "start_line": 778, "end_line": 788}, "code_snippet": "    def test_null_integer_binary(self):\n        # BINARY1 requires magic value to be specified\n\n        self.array.mask[\"intNoNull\"][0] = True\n\n        bio = io.BytesIO()\n\n        # W31: NaN's can not be represented in integer field\n        with pytest.warns(W31):\n            # https://github.com/astropy/astropy/issues/16090\n            self.votable.to_xml(bio)\n", "type": "function"}, {"name": "E04", "docstring": "A ``bit`` array should be a string of '0's and '1's.\n\n**References**: `1.1\n<http://www.ivoa.net/documents/VOTable/20040811/REC-VOTable-1.1-20040811.html#sec:datatypes>`__,\n`1.2\n<http://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html#sec:datatypes>`__", "methods": [], "attributes": ["message_template", "default_args"], "code_location": {"file": "exceptions.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 1223, "end_line": 1233}, "type": "class"}, {"name": "binoutput", "is_method": true, "class_name": "BitArray", "parameters": ["self", "value", "mask"], "calls": ["np.any", "bool_to_bitarray", "vo_warn"], "code_location": {"file": "converters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 1151, "end_line": 1155}, "code_snippet": "    def binoutput(self, value, mask):\n        if np.any(mask):\n            vo_warn(W39)\n\n        return bool_to_bitarray(value)\n", "type": "function"}, {"name": "test_table", "is_method": false, "class_name": null, "parameters": ["tmp_path"], "calls": ["votable.get_first_table", "table.to_table", "tree.VOTableFile.from_table", "votable2.get_first_table", "zip", "np.errstate", "parse", "np.all", "pytest.warns", "writeto", "get_pkg_data_filename", "str"], "code_location": {"file": "test_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable/tests", "start_line": 61, "end_line": 117}, "code_snippet": "def test_table(tmp_path):\n    # Read the VOTABLE\n    with np.errstate(over=\"ignore\"):\n        # https://github.com/astropy/astropy/issues/13341\n        votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n    table = votable.get_first_table()\n    astropy_table = table.to_table()\n\n    for name in table.array.dtype.names:\n        assert np.all(astropy_table.mask[name] == table.array.mask[name])\n\n    votable2 = tree.VOTableFile.from_table(astropy_table)\n    t = votable2.get_first_table()\n\n    field_types = [\n        (\"string_test\", {\"datatype\": \"char\", \"arraysize\": \"*\"}),\n        (\"string_test_2\", {\"datatype\": \"char\", \"arraysize\": \"10\"}),\n        (\"unicode_test\", {\"datatype\": \"unicodeChar\", \"arraysize\": \"*\"}),\n        (\"fixed_unicode_test\", {\"datatype\": \"unicodeChar\", \"arraysize\": \"10\"}),\n        (\"string_array_test\", {\"datatype\": \"char\", \"arraysize\": \"4*\"}),\n        (\"unsignedByte\", {\"datatype\": \"unsignedByte\"}),\n        (\"short\", {\"datatype\": \"short\"}),\n        (\"int\", {\"datatype\": \"int\"}),\n        (\"intNoNull\", {\"datatype\": \"int\"}),\n        (\"long\", {\"datatype\": \"long\"}),\n        (\"double\", {\"datatype\": \"double\"}),\n        (\"float\", {\"datatype\": \"float\"}),\n        (\"array\", {\"datatype\": \"long\", \"arraysize\": \"2*\"}),\n        (\"bit\", {\"datatype\": \"bit\"}),\n        (\"bitarray\", {\"datatype\": \"bit\", \"arraysize\": \"3x2\"}),\n        (\"bitvararray\", {\"datatype\": \"bit\", \"arraysize\": \"*\"}),\n        (\"bitvararray2\", {\"datatype\": \"bit\", \"arraysize\": \"3x2*\"}),\n        (\"floatComplex\", {\"datatype\": \"floatComplex\"}),\n        (\"doubleComplex\", {\"datatype\": \"doubleComplex\"}),\n        (\"doubleComplexArray\", {\"datatype\": \"doubleComplex\", \"arraysize\": \"*\"}),\n        (\"doubleComplexArrayFixed\", {\"datatype\": \"doubleComplex\", \"arraysize\": \"2\"}),\n        (\"boolean\", {\"datatype\": \"bit\"}),\n        (\"booleanArray\", {\"datatype\": \"bit\", \"arraysize\": \"4\"}),\n        (\"nulls\", {\"datatype\": \"int\"}),\n        (\"nulls_array\", {\"datatype\": \"int\", \"arraysize\": \"2x2\"}),\n        (\"precision1\", {\"datatype\": \"double\"}),\n        (\"precision2\", {\"datatype\": \"double\"}),\n        (\"doublearray\", {\"datatype\": \"double\", \"arraysize\": \"*\"}),\n        (\"bitarray2\", {\"datatype\": \"bit\", \"arraysize\": \"16\"}),\n    ]\n\n    for field, (name, d) in zip(t.fields, field_types):\n        assert field.ID == name\n        assert field.datatype == d[\"datatype\"], (\n            f\"{name} expected {d['datatype']} but get {field.datatype}\"\n        )\n        if \"arraysize\" in d:\n            assert field.arraysize == d[\"arraysize\"]\n\n    # W39: Bit values can not be masked\n    with pytest.warns(W39):\n        writeto(votable2, str(tmp_path / \"through_table.xml\"))\n", "type": "function"}, {"name": "output", "is_method": true, "class_name": "BitArray", "parameters": ["self", "value", "mask"], "calls": ["np.any", "np.asarray", "join", "vo_warn"], "code_location": {"file": "converters.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 1137, "end_line": 1142}, "code_snippet": "    def output(self, value, mask):\n        if np.any(mask):\n            vo_warn(W39)\n        value = np.asarray(value)\n        mapping = {False: \"0\", True: \"1\"}\n        return \"\".join(mapping[x] for x in value.flat)\n", "type": "function"}, {"name": "VOTableSpecWarning", "docstring": "The input XML file violates the spec, but there is an obvious workaround.", "methods": [], "attributes": [], "code_location": {"file": "exceptions.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 252, "end_line": 255}, "type": "class"}, {"name": "test_null_integer_binary2", "is_method": true, "class_name": "TestThroughBinary2", "parameters": ["self"], "calls": ["self.votable.get_first_table"], "code_location": {"file": "test_vo.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable/tests", "start_line": 815, "end_line": 825}, "code_snippet": "    def test_null_integer_binary2(self):\n        # Integers with no magic values should still be\n        # masked in BINARY2 format\n\n        self.array.mask[\"intNoNull\"][0] = True\n\n        table = self.votable.get_first_table()\n        array = table.array\n\n        assert array.mask[\"intNoNull\"][0]\n        assert array[\"intNoNull\"].mask[0]\n", "type": "function"}, {"name": "test_from_table_without_mask", "is_method": false, "class_name": null, "parameters": [], "calls": ["Table", "Column", "t.add_column", "io.BytesIO", "t.write"], "code_location": {"file": "test_table.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable/tests", "start_line": 243, "end_line": 248}, "code_snippet": "def test_from_table_without_mask():\n    t = Table()\n    c = Column(data=[1, 2, 3], name=\"a\")\n    t.add_column(c)\n    output = io.BytesIO()\n    t.write(output, format=\"votable\")\n", "type": "function"}, {"name": "VOTableChangeWarning", "docstring": "A change has been made to the input XML file.", "methods": [], "attributes": [], "code_location": {"file": "exceptions.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/io/votable", "start_line": 246, "end_line": 249}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.8794214725494385}
{"question": "What is the semantic interpretation of the three-dimensional cartesian velocity differential object returned by the function that updates coordinate differentials to match a velocity reference while preserving spatial position when the parameter controlling whether the result remains in the original coordinate frame is False?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "update_differentials_to_match", "is_method": false, "class_name": null, "parameters": ["original", "velocity_reference", "preserve_observer_frame"], "calls": ["original.transform_to", "velocity_reference.transform_to", "with_differentials", "original_icrs.realize_frame", "final.replicate", "ValueError", "hasattr", "velocity_reference.replicate", "ICRS", "ICRS", "velocity_reference_icrs.data.represent_as", "final_icrs.transform_to", "final_icrs.transform_to", "original_icrs.data.represent_as"], "code_location": {"file": "spectral_coordinate.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates", "start_line": 77, "end_line": 124}, "code_snippet": "def update_differentials_to_match(\n    original, velocity_reference, preserve_observer_frame=False\n):\n    \"\"\"\n    Given an original coordinate object, update the differentials so that\n    the final coordinate is at the same location as the original coordinate\n    but co-moving with the velocity reference object.\n\n    If preserve_original_frame is set to True, the resulting object will be in\n    the frame of the original coordinate, otherwise it will be in the frame of\n    the velocity reference.\n    \"\"\"\n    if not velocity_reference.data.differentials:\n        raise ValueError(\"Reference frame has no velocities\")\n\n    # If the reference has an obstime already defined, we should ignore\n    # it and stick with the original observer obstime.\n    if \"obstime\" in velocity_reference.frame_attributes and hasattr(\n        original, \"obstime\"\n    ):\n        velocity_reference = velocity_reference.replicate(obstime=original.obstime)\n\n    # We transform both coordinates to ICRS for simplicity and because we know\n    # it's a simple frame that is not time-dependent (it could be that both\n    # the original and velocity_reference frame are time-dependent)\n\n    original_icrs = original.transform_to(ICRS())\n    velocity_reference_icrs = velocity_reference.transform_to(ICRS())\n\n    differentials = velocity_reference_icrs.data.represent_as(\n        CartesianRepresentation, CartesianDifferential\n    ).differentials\n\n    data_with_differentials = original_icrs.data.represent_as(\n        CartesianRepresentation\n    ).with_differentials(differentials)\n\n    final_icrs = original_icrs.realize_frame(data_with_differentials)\n\n    if preserve_observer_frame:\n        final = final_icrs.transform_to(original)\n    else:\n        final = final_icrs.transform_to(velocity_reference)\n\n    return final.replicate(\n        representation_type=CartesianRepresentation,\n        differential_type=CartesianDifferential,\n    )\n", "type": "function"}, {"name": "with_observer_stationary_relative_to", "is_method": true, "class_name": "SpectralCoord", "parameters": ["self", "frame", "velocity", "preserve_observer_frame"], "calls": ["u.quantity_input", "isinstance", "isinstance", "isinstance", "update_differentials_to_match", "self._calculate_radial_velocity", "self._calculate_radial_velocity", "_apply_relativistic_doppler_shift", "self.replicate", "ValueError", "isinstance", "frame_cls", "frame.realize_frame", "frame.realize_frame", "isinstance", "CartesianRepresentation", "ValueError", "CartesianDifferential", "frame.data.with_differentials", "frame_transform_graph.lookup_name", "ValueError"], "code_location": {"file": "spectral_coordinate.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates", "start_line": 570, "end_line": 669}, "code_snippet": "    def with_observer_stationary_relative_to(\n        self, frame, velocity=None, preserve_observer_frame=False\n    ):\n        \"\"\"\n        A new  `SpectralCoord` with the velocity of the observer altered,\n        but not the position.\n\n        If a coordinate frame is specified, the observer velocities will be\n        modified to be stationary in the specified frame. If a coordinate\n        instance is specified, optionally with non-zero velocities, the\n        observer velocities will be updated so that the observer is co-moving\n        with the specified coordinates.\n\n        Parameters\n        ----------\n        frame : str, `~astropy.coordinates.BaseCoordinateFrame` or `~astropy.coordinates.SkyCoord`\n            The observation frame in which the observer will be stationary. This\n            can be the name of a frame (e.g. 'icrs'), a frame class, frame instance\n            with no data, or instance with data. This can optionally include\n            velocities.\n        velocity : `~astropy.units.Quantity` or `~astropy.coordinates.CartesianDifferential`, optional\n            If ``frame`` does not contain velocities, these can be specified as\n            a 3-element `~astropy.units.Quantity`. In the case where this is\n            also not specified, the velocities default to zero.\n        preserve_observer_frame : bool\n            If `True`, the final observer frame class will be the same as the\n            original one, and if `False` it will be the frame of the velocity\n            reference class.\n\n        Returns\n        -------\n        new_coord : `SpectralCoord`\n            The new coordinate object representing the spectral data\n            transformed based on the observer's new velocity frame.\n        \"\"\"\n        if self.observer is None or self.target is None:\n            raise ValueError(\n                \"This method can only be used if both observer \"\n                \"and target are defined on the SpectralCoord.\"\n            )\n\n        # Start off by extracting frame if a SkyCoord was passed in\n        if isinstance(frame, SkyCoord):\n            frame = frame.frame\n\n        if isinstance(frame, BaseCoordinateFrame):\n            if not frame.has_data:\n                frame = frame.realize_frame(\n                    CartesianRepresentation(0 * u.km, 0 * u.km, 0 * u.km)\n                )\n\n            if frame.data.differentials:\n                if velocity is not None:\n                    raise ValueError(\n                        \"frame already has differentials, cannot also specify velocity\"\n                    )\n                # otherwise frame is ready to go\n            else:\n                if velocity is None:\n                    differentials = ZERO_VELOCITIES\n                else:\n                    differentials = CartesianDifferential(velocity)\n                frame = frame.realize_frame(\n                    frame.data.with_differentials(differentials)\n                )\n\n        if isinstance(frame, (type, str)):\n            if isinstance(frame, type):\n                frame_cls = frame\n            elif isinstance(frame, str):\n                frame_cls = frame_transform_graph.lookup_name(frame)\n            if velocity is None:\n                velocity = 0 * u.m / u.s, 0 * u.m / u.s, 0 * u.m / u.s\n            elif velocity.shape != (3,):\n                raise ValueError(\"velocity should be a Quantity vector with 3 elements\")\n            frame = frame_cls(\n                0 * u.m,\n                0 * u.m,\n                0 * u.m,\n                *velocity,\n                representation_type=\"cartesian\",\n                differential_type=\"cartesian\",\n            )\n\n        observer = update_differentials_to_match(\n            self.observer, frame, preserve_observer_frame=preserve_observer_frame\n        )\n\n        # Calculate the initial and final los velocity\n        init_obs_vel = self._calculate_radial_velocity(\n            self.observer, self.target, as_scalar=True\n        )\n        fin_obs_vel = self._calculate_radial_velocity(\n            observer, self.target, as_scalar=True\n        )\n\n        # Apply transformation to data\n        new_data = _apply_relativistic_doppler_shift(self, fin_obs_vel - init_obs_vel)\n\n        return self.replicate(value=new_data, observer=observer)\n", "type": "function"}, {"name": "__call__", "is_method": true, "class_name": "FunctionTransformWithFiniteDifference", "parameters": ["self", "fromcoord", "toframe"], "calls": ["callable", "fromcoord.realize_frame", "supcall", "CartesianDifferential", "with_differentials", "reprwithoutdiff.realize_frame", "supcall", "self.finite_difference_dt", "fromcoord.data.without_differentials", "supcall", "supcall", "supcall", "fromcoord.realize_frame", "fromcoord.realize_frame", "fromcoord.realize_frame", "supcall", "supcall", "supcall", "reprwithoutdiff.data.to_cartesian", "CartesianRepresentation", "CartesianRepresentation", "CartesianRepresentation", "from_diffless.replicate", "toframe.replicate_without_data", "from_diffless.replicate", "toframe.replicate_without_data", "from_diffless.replicate", "toframe.replicate_without_data", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "code_location": {"file": "function.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/transformations", "start_line": 167, "end_line": 257}, "code_snippet": "    def __call__(self, fromcoord, toframe):\n        from astropy.coordinates.representation import (\n            CartesianDifferential,\n            CartesianRepresentation,\n        )\n\n        supcall = self.func\n        if not fromcoord.data.differentials:\n            return supcall(fromcoord, toframe)\n        # this is the finite difference case\n\n        if callable(self.finite_difference_dt):\n            dt = self.finite_difference_dt(fromcoord, toframe)\n        else:\n            dt = self.finite_difference_dt\n        halfdt = dt / 2\n\n        from_diffless = fromcoord.realize_frame(fromcoord.data.without_differentials())\n        reprwithoutdiff = supcall(from_diffless, toframe)\n\n        # first we use the existing differential to compute an offset due to\n        # the already-existing velocity, but in the new frame\n        fromcoord_cart = fromcoord.cartesian\n        if self.symmetric_finite_difference:\n            fwdxyz = (\n                fromcoord_cart.xyz + fromcoord_cart.differentials[\"s\"].d_xyz * halfdt\n            )\n            fwd = supcall(\n                fromcoord.realize_frame(CartesianRepresentation(fwdxyz)), toframe\n            )\n            backxyz = (\n                fromcoord_cart.xyz - fromcoord_cart.differentials[\"s\"].d_xyz * halfdt\n            )\n            back = supcall(\n                fromcoord.realize_frame(CartesianRepresentation(backxyz)), toframe\n            )\n        else:\n            fwdxyz = fromcoord_cart.xyz + fromcoord_cart.differentials[\"s\"].d_xyz * dt\n            fwd = supcall(\n                fromcoord.realize_frame(CartesianRepresentation(fwdxyz)), toframe\n            )\n            back = reprwithoutdiff\n        diffxyz = (fwd.cartesian - back.cartesian).xyz / dt\n\n        # now we compute the \"induced\" velocities due to any movement in\n        # the frame itself over time\n        attrname = self.finite_difference_frameattr_name\n        if attrname is not None:\n            if self.symmetric_finite_difference:\n                if self._diff_attr_in_fromsys:\n                    kws = {attrname: getattr(from_diffless, attrname) + halfdt}\n                    from_diffless_fwd = from_diffless.replicate(**kws)\n                else:\n                    from_diffless_fwd = from_diffless\n                if self._diff_attr_in_tosys:\n                    kws = {attrname: getattr(toframe, attrname) + halfdt}\n                    fwd_frame = toframe.replicate_without_data(**kws)\n                else:\n                    fwd_frame = toframe\n                fwd = supcall(from_diffless_fwd, fwd_frame)\n\n                if self._diff_attr_in_fromsys:\n                    kws = {attrname: getattr(from_diffless, attrname) - halfdt}\n                    from_diffless_back = from_diffless.replicate(**kws)\n                else:\n                    from_diffless_back = from_diffless\n                if self._diff_attr_in_tosys:\n                    kws = {attrname: getattr(toframe, attrname) - halfdt}\n                    back_frame = toframe.replicate_without_data(**kws)\n                else:\n                    back_frame = toframe\n                back = supcall(from_diffless_back, back_frame)\n            else:\n                if self._diff_attr_in_fromsys:\n                    kws = {attrname: getattr(from_diffless, attrname) + dt}\n                    from_diffless_fwd = from_diffless.replicate(**kws)\n                else:\n                    from_diffless_fwd = from_diffless\n                if self._diff_attr_in_tosys:\n                    kws = {attrname: getattr(toframe, attrname) + dt}\n                    fwd_frame = toframe.replicate_without_data(**kws)\n                else:\n                    fwd_frame = toframe\n                fwd = supcall(from_diffless_fwd, fwd_frame)\n                back = reprwithoutdiff\n\n            diffxyz += (fwd.cartesian - back.cartesian).xyz / dt\n\n        newdiff = CartesianDifferential(diffxyz)\n        reprwithdiff = reprwithoutdiff.data.to_cartesian().with_differentials(newdiff)\n        return reprwithoutdiff.realize_frame(reprwithdiff)\n", "type": "function"}, {"name": "velocity", "is_method": true, "class_name": "BaseCoordinateFrame", "parameters": ["self"], "calls": ["ValueError"], "code_location": {"file": "baseframe.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates", "start_line": 2163, "end_line": 2175}, "code_snippet": "    def velocity(self):\n        \"\"\"\n        Shorthand for retrieving the Cartesian space-motion as a\n        `~astropy.coordinates.CartesianDifferential` object.\n\n        This is equivalent to calling ``self.cartesian.differentials['s']``.\n        \"\"\"\n        if \"s\" not in self.data.differentials:\n            raise ValueError(\n                \"Frame has no associated velocity (Differential) data information.\"\n            )\n\n        return self.cartesian.differentials[\"s\"]\n", "type": "function"}, {"name": "CartesianDifferential", "docstring": "Differentials in of points in 3D cartesian coordinates.\n\nParameters\n----------\nd_x, d_y, d_z : `~astropy.units.Quantity` or array\n    The x, y, and z coordinates of the differentials. If ``d_x``, ``d_y``,\n    and ``d_z`` have different shapes, they should be broadcastable. If not\n    quantities, ``unit`` should be set.  If only ``d_x`` is given, it is\n    assumed that it contains an array with the 3 coordinates stored along\n    ``xyz_axis``.\nunit : `~astropy.units.Unit` or str\n    If given, the differentials will be converted to this unit (or taken to\n    be in this unit if not given.\nxyz_axis : int, optional\n    The axis along which the coordinates are stored when a single array is\n    provided instead of distinct ``d_x``, ``d_y``, and ``d_z`` (default: 0).\ncopy : bool, optional\n    If `True` (default), arrays will be copied. If `False`, arrays will\n    be references, though possibly broadcast to ensure matching shapes.", "methods": ["__init__", "to_cartesian", "from_cartesian", "transform", "get_d_xyz"], "attributes": ["base_representation", "_d_xyz", "d_xyz"], "code_location": {"file": "cartesian.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/representation", "start_line": 313, "end_line": 433}, "type": "class"}, {"name": "attach_zero_velocities", "is_method": false, "class_name": null, "parameters": ["coord"], "calls": ["coord.cartesian.with_differentials", "coord.realize_frame"], "code_location": {"file": "spectral_coordinate.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates", "start_line": 127, "end_line": 132}, "code_snippet": "def attach_zero_velocities(coord):\n    \"\"\"\n    Set the differentials to be stationary on a coordinate object.\n    \"\"\"\n    new_data = coord.cartesian.with_differentials(ZERO_VELOCITIES)\n    return coord.realize_frame(new_data)\n", "type": "function"}, {"name": "_apply_transform", "is_method": true, "class_name": "BaseAffineTransform", "parameters": ["self", "fromcoord", "matrix", "offset"], "calls": ["data.to_cartesian", "rep.with_differentials", "rep.without_differentials", "isinstance", "isinstance", "isinstance", "isinstance", "TypeError", "TypeError", "len", "ValueError", "isinstance", "represent_as", "data.with_differentials", "diff.represent_as", "rep.transform", "offset.without_differentials", "newrep.with_differentials", "with_differentials", "newrep.represent_as", "newrep.with_differentials", "data.without_differentials", "data.differentials.items", "newdiff.represent_as", "represent_as", "getattr", "represent_as", "newrep.represent_as", "type", "type", "type", "newrep.represent_as", "type"], "code_location": {"file": "affine.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/transformations", "start_line": 34, "end_line": 202}, "code_snippet": "    def _apply_transform(self, fromcoord, matrix, offset):\n        from astropy.coordinates.representation import (\n            CartesianDifferential,\n            RadialDifferential,\n            SphericalCosLatDifferential,\n            SphericalDifferential,\n            UnitSphericalRepresentation,\n        )\n\n        data = fromcoord.data\n        has_velocity = \"s\" in data.differentials\n\n        # Bail out if no transform is actually requested\n        if matrix is None and offset is None:\n            return data\n\n        # list of unit differentials\n        _unit_diffs = (\n            SphericalDifferential._unit_differential,\n            SphericalCosLatDifferential._unit_differential,\n        )\n        unit_vel_diff = has_velocity and isinstance(\n            data.differentials[\"s\"], _unit_diffs\n        )\n        rad_vel_diff = has_velocity and isinstance(\n            data.differentials[\"s\"], RadialDifferential\n        )\n\n        # Some initial checking to short-circuit doing any re-representation if\n        # we're going to fail anyways:\n        if isinstance(data, UnitSphericalRepresentation) and offset is not None:\n            raise TypeError(\n                \"Position information stored on coordinate frame \"\n                \"is insufficient to do a full-space position \"\n                \"transformation (representation class: {data.__class__})\"\n            )\n\n        if (\n            has_velocity\n            and (unit_vel_diff or rad_vel_diff)\n            and offset is not None\n            and \"s\" in offset.differentials\n        ):\n            # Coordinate has a velocity, but it is not a full-space velocity\n            # that we need to do a velocity offset\n            raise TypeError(\n                \"Velocity information stored on coordinate frame is insufficient to do\"\n                \" a full-space velocity transformation (differential class:\"\n                f\" {data.differentials['s'].__class__})\"\n            )\n\n        if len(data.differentials) > 1:\n            # We should never get here because the frame initializer shouldn't\n            # allow more differentials, but this just adds protection for\n            # subclasses that somehow skip the checks\n            raise ValueError(\n                \"Representation passed to AffineTransform contains multiple associated\"\n                \" differentials. Only a single differential with velocity units is\"\n                f\" presently supported (differentials: {data.differentials}).\"\n            )\n\n        # If the representation is a UnitSphericalRepresentation, and this is\n        # just a MatrixTransform, we have to try to turn the differential into a\n        # Unit version of the differential (if no radial velocity) or a\n        # sphericaldifferential with zero proper motion (if only a radial\n        # velocity) so that the matrix operation works\n        if (\n            has_velocity\n            and isinstance(data, UnitSphericalRepresentation)\n            and not (unit_vel_diff or rad_vel_diff)\n        ):\n            # retrieve just velocity differential\n            unit_diff = data.differentials[\"s\"].represent_as(\n                data.differentials[\"s\"]._unit_differential, data\n            )\n            data = data.with_differentials({\"s\": unit_diff})  # updates key\n\n        # If it's a RadialDifferential, we flat-out ignore the differentials\n        # This is because, by this point (past the validation above), we can\n        # only possibly be doing a rotation-only transformation, and that\n        # won't change the radial differential. We later add it back in\n        elif rad_vel_diff:\n            data = data.without_differentials()\n\n        # Convert the representation and differentials to cartesian without\n        # having them attached to a frame\n        rep = data.to_cartesian()\n        diffs = {\n            k: diff.represent_as(CartesianDifferential, data)\n            for k, diff in data.differentials.items()\n        }\n        rep = rep.with_differentials(diffs)\n\n        # Only do transform if matrix is specified. This is for speed in\n        # transformations that only specify an offset (e.g., LSR)\n        if matrix is not None:\n            # Note: this applies to both representation and differentials\n            rep = rep.transform(matrix)\n\n        # TODO: if we decide to allow arithmetic between representations that\n        # contain differentials, this can be tidied up\n        newrep = rep.without_differentials()\n        if offset is not None:\n            newrep += offset.without_differentials()\n\n        # We need a velocity (time derivative) and, for now, are strict: the\n        # representation can only contain a velocity differential and no others.\n        if has_velocity and not rad_vel_diff:\n            veldiff = rep.differentials[\"s\"]  # already in Cartesian form\n\n            if offset is not None and \"s\" in offset.differentials:\n                veldiff += offset.differentials[\"s\"]\n\n            newrep = newrep.with_differentials({\"s\": veldiff})\n\n        if isinstance(fromcoord.data, UnitSphericalRepresentation):\n            # Special-case this because otherwise the return object will think\n            # it has a valid distance with the default return (a\n            # CartesianRepresentation instance)\n\n            if has_velocity and not unit_vel_diff and not rad_vel_diff:\n                # We have to first represent as the Unit types we converted to,\n                # then put the d_distance information back in to the\n                # differentials and re-represent as their original forms\n                newdiff = newrep.differentials[\"s\"]\n                _unit_cls = fromcoord.data.differentials[\"s\"]._unit_differential\n                newdiff = newdiff.represent_as(_unit_cls, newrep)\n\n                kwargs = {comp: getattr(newdiff, comp) for comp in newdiff.components}\n                kwargs[\"d_distance\"] = fromcoord.data.differentials[\"s\"].d_distance\n                diffs = {\n                    \"s\": type(fromcoord.data.differentials[\"s\"])(copy=False, **kwargs)\n                }\n\n            elif has_velocity and unit_vel_diff:\n                newdiff = newrep.differentials[\"s\"].represent_as(\n                    fromcoord.data.differentials[\"s\"].__class__, newrep\n                )\n                diffs = {\"s\": newdiff}\n\n            else:\n                diffs = newrep.differentials\n\n            newrep = newrep.represent_as(type(fromcoord.data)).with_differentials(diffs)\n\n        elif has_velocity and unit_vel_diff:\n            # Here, we're in the case where the representation is not\n            # UnitSpherical, but the differential *is* one of the UnitSpherical\n            # types. We have to convert back to that differential class or the\n            # resulting frame will think it has a valid radial_velocity. This\n            # can probably be cleaned up: we currently have to go through the\n            # dimensional version of the differential before representing as the\n            # unit differential so that the units work out (the distance length\n            # unit shouldn't appear in the resulting proper motions)\n\n            diff_cls = fromcoord.data.differentials[\"s\"].__class__\n            newrep = newrep.represent_as(\n                type(fromcoord.data), diff_cls._dimensional_differential\n            ).represent_as(type(fromcoord.data), diff_cls)\n\n        # We pulled the radial differential off of the representation\n        # earlier, so now we need to put it back. But, in order to do that, we\n        # have to turn the representation into a repr that is compatible with\n        # having a RadialDifferential\n        if has_velocity and rad_vel_diff:\n            newrep = newrep.represent_as(fromcoord.data.__class__)\n            newrep = newrep.with_differentials({\"s\": fromcoord.data.differentials[\"s\"]})\n\n        return newrep\n", "type": "function"}, {"name": "_get_velocities", "is_method": false, "class_name": null, "parameters": ["coord"], "calls": [], "code_location": {"file": "spectral_coordinate.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates", "start_line": 135, "end_line": 139}, "code_snippet": "def _get_velocities(coord):\n    if \"s\" in coord.data.differentials:\n        return coord.velocity\n    else:\n        return ZERO_VELOCITIES\n", "type": "function"}, {"name": "test_with_observer_stationary_relative_to", "is_method": false, "class_name": null, "parameters": [], "calls": ["SpectralCoord", "SpectralCoord", "assert_quantity_allclose", "sc2.with_observer_stationary_relative_to", "assert_quantity_allclose", "sc2.with_observer_stationary_relative_to", "assert_quantity_allclose", "sc2.with_observer_stationary_relative_to", "assert_quantity_allclose", "sc2.with_observer_stationary_relative_to", "assert_quantity_allclose", "sc2.with_observer_stationary_relative_to", "assert_quantity_allclose", "sc2.with_observer_stationary_relative_to", "assert_quantity_allclose", "sc2.with_observer_stationary_relative_to", "assert_quantity_allclose", "sc2.with_observer_stationary_relative_to", "assert_quantity_allclose", "sc2.with_observer_stationary_relative_to", "assert_quantity_allclose", "sc2.with_observer_stationary_relative_to", "sc2.with_observer_stationary_relative_to", "isinstance", "isinstance", "pytest.raises", "sc1.with_observer_stationary_relative_to", "ICRS", "ICRS", "ICRS", "SkyCoord", "SkyCoord", "pytest.raises", "sc2.with_observer_stationary_relative_to", "pytest.raises", "sc2.with_observer_stationary_relative_to", "transform_to", "ICRS", "ICRS", "ICRS", "ICRS", "ICRS", "SkyCoord", "ICRS"], "code_location": {"file": "test_spectral_coordinate.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/tests", "start_line": 461, "end_line": 631}, "code_snippet": "def test_with_observer_stationary_relative_to():\n    # Simple tests of with_observer_stationary_relative_to to cover different\n    # ways of calling it\n\n    # The replicate method makes a new object with attributes updated, but doesn't\n    # do any conversion\n\n    sc1 = SpectralCoord([4000, 5000] * u.AA)\n    with pytest.raises(\n        ValueError,\n        match=(\n            \"This method can only be used if both observer and target are defined on\"\n            \" the SpectralCoord\"\n        ),\n    ):\n        sc1.with_observer_stationary_relative_to(\"icrs\")\n\n    sc2 = SpectralCoord(\n        [4000, 5000] * u.AA,\n        observer=ICRS(\n            0 * u.km,\n            0 * u.km,\n            0 * u.km,\n            -1 * u.km / u.s,\n            0 * u.km / u.s,\n            -1 * u.km / u.s,\n            representation_type=\"cartesian\",\n            differential_type=\"cartesian\",\n        ),\n        target=ICRS(\n            0 * u.deg, 45 * u.deg, distance=1 * u.kpc, radial_velocity=2 * u.km / u.s\n        ),\n    )\n\n    # Motion of observer is in opposite direction to target\n    assert_quantity_allclose(sc2.radial_velocity, (2 + 2**0.5) * u.km / u.s)\n\n    # Change to observer that is stationary in ICRS\n    sc3 = sc2.with_observer_stationary_relative_to(\"icrs\")\n\n    # Velocity difference is now pure radial velocity of target\n    assert_quantity_allclose(sc3.radial_velocity, 2 * u.km / u.s)\n\n    # Check setting the velocity in with_observer_stationary_relative_to\n    sc4 = sc2.with_observer_stationary_relative_to(\n        \"icrs\", velocity=[-(2**0.5), 0, -(2**0.5)] * u.km / u.s\n    )\n\n    # Observer once again moving away from target but faster\n    assert_quantity_allclose(sc4.radial_velocity, 4 * u.km / u.s)\n\n    # Check that we can also pass frame classes instead of names\n\n    sc5 = sc2.with_observer_stationary_relative_to(\n        ICRS, velocity=[-(2**0.5), 0, -(2**0.5)] * u.km / u.s\n    )\n    assert_quantity_allclose(sc5.radial_velocity, 4 * u.km / u.s)\n\n    # And make sure we can also pass instances of classes without data\n\n    sc6 = sc2.with_observer_stationary_relative_to(\n        ICRS(), velocity=[-(2**0.5), 0, -(2**0.5)] * u.km / u.s\n    )\n    assert_quantity_allclose(sc6.radial_velocity, 4 * u.km / u.s)\n\n    # And with data provided no velocities are present\n\n    sc7 = sc2.with_observer_stationary_relative_to(\n        ICRS(0 * u.km, 0 * u.km, 0 * u.km, representation_type=\"cartesian\"),\n        velocity=[-(2**0.5), 0, -(2**0.5)] * u.km / u.s,\n    )\n    assert_quantity_allclose(sc7.radial_velocity, 4 * u.km / u.s)\n\n    # And also have the ability to pass frames with velocities already defined\n\n    sc8 = sc2.with_observer_stationary_relative_to(\n        ICRS(\n            0 * u.km,\n            0 * u.km,\n            0 * u.km,\n            2**0.5 * u.km / u.s,\n            0 * u.km / u.s,\n            2**0.5 * u.km / u.s,\n            representation_type=\"cartesian\",\n            differential_type=\"cartesian\",\n        )\n    )\n    assert_quantity_allclose(\n        sc8.radial_velocity, 0 * u.km / u.s, atol=1e-10 * u.km / u.s\n    )\n\n    # Make sure that things work properly if passing a SkyCoord\n\n    sc9 = sc2.with_observer_stationary_relative_to(\n        SkyCoord(ICRS(0 * u.km, 0 * u.km, 0 * u.km, representation_type=\"cartesian\")),\n        velocity=[-(2**0.5), 0, -(2**0.5)] * u.km / u.s,\n    )\n    assert_quantity_allclose(sc9.radial_velocity, 4 * u.km / u.s)\n\n    sc10 = sc2.with_observer_stationary_relative_to(\n        SkyCoord(\n            ICRS(\n                0 * u.km,\n                0 * u.km,\n                0 * u.km,\n                2**0.5 * u.km / u.s,\n                0 * u.km / u.s,\n                2**0.5 * u.km / u.s,\n                representation_type=\"cartesian\",\n                differential_type=\"cartesian\",\n            )\n        )\n    )\n    assert_quantity_allclose(\n        sc10.radial_velocity, 0 * u.km / u.s, atol=1e-10 * u.km / u.s\n    )\n\n    # But we shouldn't be able to pass both a frame with velocities, and explicit velocities\n\n    with pytest.raises(\n        ValueError,\n        match=\"frame already has differentials, cannot also specify velocity\",\n    ):\n        sc2.with_observer_stationary_relative_to(\n            ICRS(\n                0 * u.km,\n                0 * u.km,\n                0 * u.km,\n                2**0.5 * u.km / u.s,\n                0 * u.km / u.s,\n                2**0.5 * u.km / u.s,\n                representation_type=\"cartesian\",\n                differential_type=\"cartesian\",\n            ),\n            velocity=[-(2**0.5), 0, -(2**0.5)] * u.km / u.s,\n        )\n\n    # And velocities should have three elements\n\n    with pytest.raises(\n        ValueError, match=\"velocity should be a Quantity vector with 3 elements\"\n    ):\n        sc2.with_observer_stationary_relative_to(\n            ICRS, velocity=[-(2**0.5), 0, -(2**0.5), -3] * u.km / u.s\n        )\n\n    # Make sure things don't change depending on what frame class is used for reference\n    sc11 = sc2.with_observer_stationary_relative_to(\n        SkyCoord(\n            ICRS(\n                0 * u.km,\n                0 * u.km,\n                0 * u.km,\n                2**0.5 * u.km / u.s,\n                0 * u.km / u.s,\n                2**0.5 * u.km / u.s,\n                representation_type=\"cartesian\",\n                differential_type=\"cartesian\",\n            )\n        ).transform_to(Galactic)\n    )\n    assert_quantity_allclose(\n        sc11.radial_velocity, 0 * u.km / u.s, atol=1e-10 * u.km / u.s\n    )\n\n    # Check that it is possible to preserve the observer frame\n    sc12 = sc2.with_observer_stationary_relative_to(LSRD)\n    sc13 = sc2.with_observer_stationary_relative_to(LSRD, preserve_observer_frame=True)\n\n    assert isinstance(sc12.observer, Galactic)\n    assert isinstance(sc13.observer, ICRS)\n", "type": "function"}, {"name": "to_cartesian", "is_method": true, "class_name": "BaseDifferential", "parameters": ["self", "base"], "calls": ["self._get_base_vectors", "functools.reduce", "zip", "getattr"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/representation", "start_line": 1440, "end_line": 1462}, "code_snippet": "    def to_cartesian(self, base):\n        \"\"\"Convert the differential to 3D rectangular cartesian coordinates.\n\n        Parameters\n        ----------\n        base : instance of ``self.base_representation``\n            The points for which the differentials are to be converted: each of\n            the components is multiplied by its unit vectors and scale factors.\n\n        Returns\n        -------\n        `~astropy.coordinates.CartesianDifferential`\n            This object, converted.\n\n        \"\"\"\n        base_e, base_sf = self._get_base_vectors(base)\n        return functools.reduce(\n            operator.add,\n            (\n                getattr(self, d_c) * base_sf[c] * base_e[c]\n                for d_c, c in zip(self.components, base.components)\n            ),\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.8638641834259033}
{"question": "Where are the lower-level mathematical transformation functions that the sky-to-pixel zenithal equal area projection class delegates to for converting the zenithal angle theta into the radial distance R_theta during the sky-to-pixel projection?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "Sky2Pix_ZenithalEqualArea", "docstring": "Zenithal equidistant projection - sky to pixel.\n\nCorresponds to the ``ZEA`` projection in FITS WCS.\n\nSee `Zenithal` for a definition of the full transformation.\n\n.. math::\n    R_\\theta &= \\frac{180^\\circ}{\\pi} \\sqrt{2(1 - \\sin\\theta)} \\\\\n             &= \\frac{360^\\circ}{\\pi} \\sin\\left(\\frac{90^\\circ - \\theta}{2}\\right)", "methods": [], "attributes": [], "code_location": {"file": "projections.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 626, "end_line": 637}, "type": "class"}, {"name": "Pix2Sky_ZenithalEqualArea", "docstring": "Zenithal equidistant projection - pixel to sky.\n\nCorresponds to the ``ZEA`` projection in FITS WCS.\n\nSee `Zenithal` for a definition of the full transformation.\n\n.. math::\n    \\theta = 90^\\circ - 2 \\sin^{-1} \\left(\\frac{\\pi R_\\theta}{360^\\circ}\\right)", "methods": [], "attributes": [], "code_location": {"file": "projections.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 613, "end_line": 623}, "type": "class"}, {"name": "Zenithal", "docstring": "Base class for all Zenithal projections.\n\nZenithal (or azimuthal) projections map the sphere directly onto a\nplane.  All zenithal projections are specified by defining the\nradius as a function of native latitude, :math:`R_\\theta`.\n\nThe pixel-to-sky transformation is defined as:\n\n.. math::\n    \\phi &= \\arg(-y, x) \\\\\n    R_\\theta &= \\sqrt{x^2 + y^2}\n\nand the inverse (sky-to-pixel) is defined as:\n\n.. math::\n    x &= R_\\theta \\sin \\phi \\\\\n    y &= R_\\theta \\cos \\phi", "methods": [], "attributes": [], "code_location": {"file": "projections.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 265, "end_line": 283}, "type": "class"}, {"name": "Sky2Pix_ZenithalEquidistant", "docstring": "Zenithal equidistant projection - sky to pixel.\n\nCorresponds to the ``ARC`` projection in FITS WCS.\n\nSee `Zenithal` for a definition of the full transformation.\n\n.. math::\n    R_\\theta = 90^\\circ - \\theta", "methods": [], "attributes": [], "code_location": {"file": "projections.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 600, "end_line": 610}, "type": "class"}, {"name": "Pix2Sky_ZenithalEquidistant", "docstring": "Zenithal equidistant projection - pixel to sky.\n\nCorresponds to the ``ARC`` projection in FITS WCS.\n\nSee `Zenithal` for a definition of the full transformation.\n\n.. math::\n    \\theta = 90^\\circ - R_\\theta", "methods": [], "attributes": [], "code_location": {"file": "projections.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 587, "end_line": 597}, "type": "class"}, {"name": "Sky2Pix_CylindricalEqualArea", "docstring": "Cylindrical equal area projection - sky to pixel.\n\nCorresponds to the ``CEA`` projection in FITS WCS.\n\n.. math::\n    x &= \\phi \\\\\n    y &= \\frac{180^{\\circ}}{\\pi}\\frac{\\sin \\theta}{\\lambda}\n\nParameters\n----------\nlam : float\n    Radius of the cylinder in spherical radii, .  Default is 0.", "methods": [], "attributes": ["lam"], "code_location": {"file": "projections.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 802, "end_line": 818}, "type": "class"}, {"name": "Sky2Pix_Gnomonic", "docstring": "Gnomonic Projection - sky to pixel.\n\nCorresponds to the ``TAN`` projection in FITS WCS.\n\nSee `Zenithal` for a definition of the full transformation.\n\n.. math::\n    R_\\theta = \\frac{180^{\\circ}}{\\pi}\\cot \\theta", "methods": [], "attributes": [], "code_location": {"file": "projections.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 487, "end_line": 497}, "type": "class"}, {"name": "Pix2Sky_CylindricalEqualArea", "docstring": "Cylindrical equal area projection - pixel to sky.\n\nCorresponds to the ``CEA`` projection in FITS WCS.\n\n.. math::\n    \\phi &= x \\\\\n    \\theta &= \\sin^{-1}\\left(\\frac{\\pi}{180^{\\circ}}\\lambda y\\right)\n\nParameters\n----------\nlam : float\n    Radius of the cylinder in spherical radii, .  Default is 1.", "methods": [], "attributes": ["lam"], "code_location": {"file": "projections.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 783, "end_line": 799}, "type": "class"}, {"name": "Pix2Sky_Gnomonic", "docstring": "Gnomonic projection - pixel to sky.\n\nCorresponds to the ``TAN`` projection in FITS WCS.\n\nSee `Zenithal` for a definition of the full transformation.\n\n.. math::\n    \\theta = \\tan^{-1}\\left(\\frac{180^{\\circ}}{\\pi R_\\theta}\\right)", "methods": [], "attributes": [], "code_location": {"file": "projections.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 474, "end_line": 484}, "type": "class"}, {"name": "Sky2Pix_Stereographic", "docstring": "Stereographic Projection - sky to pixel.\n\nCorresponds to the ``STG`` projection in FITS WCS.\n\nSee `Zenithal` for a definition of the full transformation.\n\n.. math::\n    R_\\theta = \\frac{180^{\\circ}}{\\pi}\\frac{2 \\cos \\theta}{1 + \\sin \\theta}", "methods": [], "attributes": [], "code_location": {"file": "projections.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/modeling", "start_line": 513, "end_line": 523}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.8466792106628418}
{"question": "Where in the astropy codebase is the mechanism that propagates boolean exclusion flags during array shape expansion implemented, coordinating between array wrapper objects that track excluded elements and the coordinate grid generation routine from the numerical computing library?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "__array_wrap__", "is_method": true, "class_name": "MaskedNDArray", "parameters": ["self", "obj", "context", "return_scalar"], "calls": ["NotImplementedError", "self.from_unmasked", "get_data_and_mask"], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked", "start_line": 1096, "end_line": 1106}, "code_snippet": "    def __array_wrap__(self, obj, context=None, return_scalar=False):\n        if context is None:\n            # Functions like np.ediff1d call __array_wrap__ to turn the array\n            # into self's subclass.\n            return self.from_unmasked(*get_data_and_mask(obj))\n\n        raise NotImplementedError(\n            \"__array_wrap__ should not be used with a context any more since all use \"\n            \"should go through array_function. Please raise an issue on \"\n            \"https://github.com/astropy/astropy\"\n        )\n", "type": "function"}, {"name": "block", "is_method": false, "class_name": null, "parameters": ["arrays"], "calls": ["np_core.shape_base._block_setup", "np_core.shape_base._block_info_recursion", "np.result_type", "all", "all", "Masked", "zip", "np.empty"], "code_location": {"file": "function_helpers.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked", "start_line": 637, "end_line": 658}, "code_snippet": "def block(arrays):\n    # We need to override block since the numpy implementation can take two\n    # different paths, one for concatenation, one for creating a large empty\n    # result array in which parts are set.  Each assumes array input and\n    # cannot be used directly.  Since it would be very costly to inspect all\n    # arrays and then turn them back into a nested list, we just copy here the\n    # second implementation, np.core.shape_base._block_slicing, since it is\n    # shortest and easiest.\n    from astropy.utils.masked import Masked\n\n    arrays, list_ndim, result_ndim, final_size = np_core.shape_base._block_setup(arrays)\n    shape, slices, arrays = np_core.shape_base._block_info_recursion(\n        arrays, list_ndim, result_ndim\n    )\n    dtype = np.result_type(*[arr.dtype for arr in arrays])\n    F_order = all(arr.flags[\"F_CONTIGUOUS\"] for arr in arrays)\n    C_order = all(arr.flags[\"C_CONTIGUOUS\"] for arr in arrays)\n    order = \"F\" if F_order and not C_order else \"C\"\n    result = Masked(np.empty(shape=shape, dtype=dtype, order=order))\n    for the_slice, arr in zip(slices, arrays):\n        result[(Ellipsis,) + the_slice] = arr\n    return result, None, None\n", "type": "function"}, {"name": "FalseArray", "docstring": "Boolean mask array that is always False.\n\nThis is used to create a stub ``mask`` property which is a boolean array of\n``False`` used by default for mixin columns and corresponding to the mixin\ncolumn data shape.  The ``mask`` looks like a normal numpy array but an\nexception will be raised if ``True`` is assigned to any element.  The\nconsequences of the limitation are most obvious in the high-level table\noperations.\n\nParameters\n----------\nshape : tuple\n    Data shape", "methods": ["__new__", "__setitem__"], "attributes": [], "code_location": {"file": "column.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/table", "start_line": 99, "end_line": 125}, "type": "class"}, {"name": "test_broadcast_to_via_apply", "is_method": true, "class_name": "TestManipulation", "parameters": ["self"], "calls": ["self.s0._apply", "np.all", "np.all", "np.all", "np.may_share_memory", "np.may_share_memory", "np.may_share_memory", "type", "type"], "code_location": {"file": "test_representation_methods.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/tests", "start_line": 238, "end_line": 249}, "code_snippet": "    def test_broadcast_to_via_apply(self):\n        s0_broadcast = self.s0._apply(np.broadcast_to, (3, 6, 7), subok=True)\n        s0_diff = s0_broadcast.differentials[\"s\"]\n        assert type(s0_broadcast) is type(self.s0)\n        assert s0_broadcast.shape == (3, 6, 7)\n        assert s0_diff.shape == s0_broadcast.shape\n        assert np.all(s0_broadcast.lon == self.s0.lon)\n        assert np.all(s0_broadcast.lat == self.s0.lat)\n        assert np.all(s0_broadcast.distance == self.s0.distance)\n        assert np.may_share_memory(s0_broadcast.lon, self.s0.lon)\n        assert np.may_share_memory(s0_broadcast.lat, self.s0.lat)\n        assert np.may_share_memory(s0_broadcast.distance, self.s0.distance)\n", "type": "function"}, {"name": "test_broadcast_to_using_apply", "is_method": true, "class_name": "TestMaskedArrayBroadcast", "parameters": ["self"], "calls": ["self.mb._apply", "Masked", "assert_masked_equal", "np.broadcast_to", "np.broadcast_to"], "code_location": {"file": "test_functions.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked/tests", "start_line": 413, "end_line": 423}, "code_snippet": "    def test_broadcast_to_using_apply(self):\n        # Partially just to ensure we cover the relevant part of _apply.\n        shape = self.ma.shape\n        ba = self.mb._apply(np.broadcast_to, shape=shape, subok=True)\n        assert ba.shape == shape\n        assert ba.mask.shape == shape\n        expected = Masked(\n            np.broadcast_to(self.mb.unmasked, shape, subok=True),\n            np.broadcast_to(self.mb.mask, shape, subok=True),\n        )\n        assert_masked_equal(ba, expected)\n", "type": "function"}, {"name": "__array_wrap__", "is_method": true, "class_name": "Quantity", "parameters": ["self", "obj", "context", "return_scalar"], "calls": ["NotImplementedError", "self._new_view"], "code_location": {"file": "quantity.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units", "start_line": 591, "end_line": 601}, "code_snippet": "    def __array_wrap__(self, obj, context=None, return_scalar=False):\n        if context is None:\n            # Methods like .squeeze() created a new `ndarray` and then call\n            # __array_wrap__ to turn the array into self's subclass.\n            return self._new_view(obj)\n\n        raise NotImplementedError(\n            \"__array_wrap__ should not be used with a context any more since all \"\n            \"use should go through array_function. Please raise an issue on \"\n            \"https://github.com/astropy/astropy\"\n        )\n", "type": "function"}, {"name": "test_meshgrid", "is_method": true, "class_name": "TestMeshGrid", "parameters": ["self"], "calls": ["np.arange", "np.array", "Masked", "np.array", "np.array", "Masked", "np.meshgrid", "np.broadcast_arrays", "np.broadcast_arrays", "assert_array_equal", "assert_array_equal"], "code_location": {"file": "test_function_helpers.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked/tests", "start_line": 1390, "end_line": 1402}, "code_snippet": "    def test_meshgrid(self):\n        a = np.arange(1.0, 4.0)\n        mask_a = np.array([True, False, False])\n        ma = Masked(a, mask=mask_a)\n        b = np.array([2.5, 10.0, 3.0, 4.0])\n        mask_b = np.array([False, True, False, True])\n        mb = Masked(b, mask=mask_b)\n        oa, ob = np.meshgrid(ma, mb)\n        xa, xb = np.broadcast_arrays(a, b[:, np.newaxis])\n        ma, mb = np.broadcast_arrays(mask_a, mask_b[:, np.newaxis])\n        for o, x, m in ((oa, xa, ma), (ob, xb, mb)):\n            assert_array_equal(o.unmasked, x)\n            assert_array_equal(o.mask, m)\n", "type": "function"}, {"name": "test_erfa_rxp", "is_method": true, "class_name": "MaskedUfuncTests", "parameters": ["self"], "calls": ["Masked", "Masked", "erfa_ufunc.rxp", "erfa_ufunc.rxp", "assert_array_equal", "assert_array_equal", "erfa_ufunc.rxp", "assert_array_equal", "assert_array_equal", "erfa_ufunc.rxp", "assert_array_equal", "assert_array_equal", "np.eye", "reshape", "np.arange"], "code_location": {"file": "test_functions.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked/tests", "start_line": 216, "end_line": 232}, "code_snippet": "    def test_erfa_rxp(self):\n        # Regression tests for gh-16116\n        m = Masked(np.eye(3))\n        v = Masked(np.arange(6).reshape(2, 3))\n        rxp1 = erfa_ufunc.rxp(m, v)\n        exp = erfa_ufunc.rxp(m.unmasked, v.unmasked)\n        assert_array_equal(rxp1.unmasked, exp)\n        assert_array_equal(rxp1.mask, False)\n        v.mask[0, 0] = True\n        rxp2 = erfa_ufunc.rxp(m, v)\n        assert_array_equal(rxp2.unmasked, exp)\n        assert_array_equal(rxp2.mask, [[True] * 3, [False] * 3])\n        m.mask[1, 1] = True\n        v.mask[...] = False\n        rxp3 = erfa_ufunc.rxp(m, v)\n        assert_array_equal(rxp3.unmasked, exp)\n        assert_array_equal(rxp3.mask, True)\n", "type": "function"}, {"name": "__array_ufunc__", "is_method": true, "class_name": "MaskedNDArray", "parameters": ["self", "ufunc", "method"], "calls": ["self._get_data_and_masks", "kwargs.get", "kwargs.get", "self._masked_result", "self._get_data_and_masks", "zip", "get_data_and_mask", "getattr", "zip", "ufunc", "np.isnan", "kwargs.get", "kwargs.get", "zip", "combine_masks", "np.zeros", "nan_masks.append", "np.lib.function_base._parse_gufunc_signature", "np.lib._function_base_impl._parse_gufunc_signature", "combine_masks", "combine_masks", "zip", "combine_masks", "TypeError", "kwargs.items", "ufunc.signature.replace", "ufunc.signature.replace", "len", "in_masks.append", "result_masks.append", "combine_masks", "kwargs.get", "np.logical_or.reduce", "np.expand_dims", "NotImplementedError", "tuple", "tuple", "np.broadcast_to", "kwargs.get", "kwargs.get", "np.logical_or.reduce", "kwargs.get", "np.logical_or.accumulate", "len", "range", "range", "np.ndim", "np.logical_or.reduce", "len", "len"], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/utils/masked", "start_line": 817, "end_line": 1009}, "code_snippet": "    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\n        # Get inputs and there masks.\n        unmasked, masks = self._get_data_and_masks(inputs)\n\n        # Deal with possible outputs and their masks.\n        out = kwargs.get(\"out\")\n        out_mask = None\n        if out is None:\n            out_masks = [None] * ufunc.nout\n        else:\n            out_unmasked, out_masks = self._get_data_and_masks(out)\n            kwargs[\"out\"] = out_unmasked\n            for d, m in zip(out_unmasked, out_masks):\n                if m is None:\n                    # TODO: allow writing to unmasked output if nothing is masked?\n                    if d is not None:\n                        raise TypeError(\"cannot write to unmasked output\")\n                elif out_mask is None:\n                    out_mask = m\n\n        # TODO: where is only needed for __call__ and reduce;\n        # this is very fast, but still worth separating out?\n        where = kwargs.get(\"where\", True)\n        if where is True:\n            where_unmasked = True\n            where_mask = None\n        else:\n            where_unmasked, where_mask = get_data_and_mask(where)\n            kwargs[\"where\"] = where_unmasked\n\n        # First calculate the unmasked result. This will also verify kwargs.\n        # It will raise if the arguments do not know how to deal with each other.\n        result = getattr(ufunc, method)(*unmasked, **kwargs)\n\n        if ufunc.signature:\n            # We're dealing with a gufunc. For now, only deal with\n            # np.matmul and gufuncs for which the mask of any output always\n            # depends on all core dimension values of all inputs.\n            # TODO: in principle, it should be possible to generate the mask\n            # purely based on the signature.\n            if ufunc is np.matmul:\n                # np.matmul is tricky and its signature cannot be parsed by\n                # _parse_gufunc_signature.  But we can calculate the mask\n                # with matmul by using that nan will propagate correctly.\n                # We use float16 to minimize the memory requirements.\n                nan_masks = []\n                for a, m in zip(unmasked, masks):\n                    nan_mask = np.zeros(a.shape, dtype=np.float16)\n                    if m is not None:\n                        nan_mask[m] = np.nan\n                    nan_masks.append(nan_mask)\n                m_kwargs = {\n                    k: v for k, v in kwargs.items() if k not in (\"out\", \"where\")\n                }\n                t = ufunc(*nan_masks, **m_kwargs)\n                mask = np.isnan(t, out=out_mask)\n\n            else:\n                # Parse signature with private numpy function. Note it\n                # cannot handle spaces in tuples, so remove those.\n                if NUMPY_LT_2_0:\n                    in_sig, out_sig = np.lib.function_base._parse_gufunc_signature(\n                        ufunc.signature.replace(\" \", \"\")\n                    )\n                else:\n                    (\n                        in_sig,\n                        out_sig,\n                    ) = np.lib._function_base_impl._parse_gufunc_signature(\n                        ufunc.signature.replace(\" \", \"\")\n                    )\n                axes = kwargs.get(\"axes\")\n                if axes is None:\n                    # Maybe axis was given? (Note: ufunc will not take both.)\n                    axes = [kwargs.get(\"axis\")] * ufunc.nargs\n                elif len(axes) < ufunc.nargs:\n                    # All outputs have no core dimensions, which means axes\n                    # is not needed, but add None's for the zip below.\n                    axes = axes + [None] * (ufunc.nargs - len(axes))  # not inplace!\n                keepdims = kwargs.get(\"keepdims\", False)\n                in_masks = []\n                for sig, mask, axis in zip(in_sig, masks, axes[: ufunc.nin]):\n                    if mask is not None:\n                        if sig:\n                            if axis is None:\n                                axis = tuple(range(-1, -1 - len(sig), -1))\n                            # Input has core dimensions.  Assume that if any\n                            # value in those is masked, the output will be\n                            # masked too (TODO: for multiple core dimensions\n                            # this may be too strong).\n                            mask = np.logical_or.reduce(\n                                mask, axis=axis, keepdims=keepdims\n                            )\n                        in_masks.append(mask)\n\n                if ufunc.nout == 1 and out_sig[0] == ():\n                    # Special-case where possible in-place is easy.\n                    mask = combine_masks(in_masks, out=out_mask, copy=False)\n                else:\n                    # Here, some masks may need expansion, so we forego in-place.\n                    mask = combine_masks(in_masks, copy=False)\n                    result_masks = []\n                    for os, omask, axis in zip(out_sig, out_masks, axes[ufunc.nin :]):\n                        if os:\n                            # Output has core dimensions.  Assume all those\n                            # get the same mask.\n                            if axis is None:\n                                axis = tuple(range(-1, -1 - len(os), -1))\n                            result_mask = np.expand_dims(mask, axis)\n                        else:\n                            result_mask = mask\n                        if omask is not None:\n                            omask[...] = result_mask\n                        result_masks.append(result_mask)\n\n                    mask = result_masks if ufunc.nout > 1 else result_masks[0]\n\n        elif method == \"__call__\":\n            # Regular ufunc call.\n            # Combine the masks from the input, possibly selecting elements.\n            mask = combine_masks(masks, out=out_mask, where=where_unmasked)\n            # If relevant, also mask output elements for which where was masked.\n            if where_mask is not None:\n                mask |= where_mask\n            if out_mask is not None:\n                # Check for any additional explicitly given outputs.\n                for m in out_masks[1:]:\n                    if m is not None and m is not out_mask:\n                        m[...] = mask\n\n        elif method == \"outer\":\n            # Must have two inputs and one output, so also only one output mask.\n            # Adjust masks as will be done for data.\n            m0, m1 = masks\n            if m0 is not None and m0.ndim > 0:\n                m0 = m0[(...,) + (np.newaxis,) * np.ndim(unmasked[1])]\n            mask = combine_masks((m0, m1), out=out_mask)\n\n        elif method in {\"reduce\", \"accumulate\"}:\n            # Reductions like np.add.reduce (sum).\n            # Treat any masked where as if the input element was masked.\n            mask = combine_masks((masks[0], where_mask), copy=False)\n            if mask is False and out_mask is not None:\n                if where_unmasked is True:\n                    out_mask[...] = False\n                else:\n                    # This is too complicated, just fall through to below.\n                    mask = np.broadcast_to(False, inputs[0].shape)\n\n            if mask is not False:\n                # By default, we simply propagate masks, since for\n                # things like np.sum, it makes no sense to do otherwise.\n                # Individual methods need to override as needed.\n                if method == \"reduce\":\n                    axis = kwargs.get(\"axis\")\n                    keepdims = kwargs.get(\"keepdims\", False)\n                    mask = np.logical_or.reduce(\n                        mask,\n                        where=where_unmasked,\n                        axis=axis,\n                        keepdims=keepdims,\n                        out=out_mask,\n                    )\n                    if where_unmasked is not True:\n                        # Mask also whole rows in which no elements were selected;\n                        # those will have been left as unmasked above.\n                        mask |= ~np.logical_or.reduce(\n                            where_unmasked, axis=axis, keepdims=keepdims\n                        )\n\n                else:\n                    # Accumulate\n                    axis = kwargs.get(\"axis\", 0)\n                    mask = np.logical_or.accumulate(mask, axis=axis, out=out_mask)\n\n            elif out is None:\n                # Can only get here if neither input nor output was masked, but\n                # perhaps where was masked (possible in \"not NUMPY_LT_1_25\").\n                # We don't support this.\n                return NotImplemented\n\n        elif method in {\"reduceat\", \"at\"}:  # pragma: no cover\n            raise NotImplementedError(\n                \"masked instances cannot yet deal with 'reduceat' or 'at'.\"\n            )\n\n        if result is None:  # pragma: no cover\n            # This happens for the \"at\" method.\n            return result\n\n        if out is not None and ufunc.nout == 1:\n            out = out[0]\n        return self._masked_result(result, mask, out)\n", "type": "function"}, {"name": "test_broadcast_to", "is_method": true, "class_name": "TestShapeFunctions", "parameters": ["self"], "calls": ["np.broadcast_to", "np.all", "np.all", "np.all", "np.may_share_memory", "np.may_share_memory", "np.may_share_memory", "np.broadcast_to", "np.all", "np.all", "np.all", "np.may_share_memory", "np.may_share_memory", "np.may_share_memory", "self.s0.copy", "np.broadcast_to", "np.may_share_memory", "np.all", "type", "type", "np.may_share_memory", "np.may_share_memory"], "code_location": {"file": "test_representation_methods.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/tests", "start_line": 305, "end_line": 339}, "code_snippet": "    def test_broadcast_to(self):\n        s0_broadcast = np.broadcast_to(self.s0, (3, 6, 7))\n        s0_diff = s0_broadcast.differentials[\"s\"]\n        assert type(s0_broadcast) is type(self.s0)\n        assert s0_broadcast.shape == (3, 6, 7)\n        assert s0_diff.shape == s0_broadcast.shape\n        assert np.all(s0_broadcast.lon == self.s0.lon)\n        assert np.all(s0_broadcast.lat == self.s0.lat)\n        assert np.all(s0_broadcast.distance == self.s0.distance)\n        assert np.may_share_memory(s0_broadcast.lon, self.s0.lon)\n        assert np.may_share_memory(s0_broadcast.lat, self.s0.lat)\n        assert np.may_share_memory(s0_broadcast.distance, self.s0.distance)\n\n        s1_broadcast = np.broadcast_to(self.s1, shape=(3, 6, 7))\n        s1_diff = s1_broadcast.differentials[\"s\"]\n        assert s1_broadcast.shape == (3, 6, 7)\n        assert s1_diff.shape == s1_broadcast.shape\n        assert np.all(s1_broadcast.lat == self.s1.lat)\n        assert np.all(s1_broadcast.lon == self.s1.lon)\n        assert np.all(s1_broadcast.distance == self.s1.distance)\n        assert s1_broadcast.distance.shape == (3, 6, 7)\n        assert np.may_share_memory(s1_broadcast.lat, self.s1.lat)\n        assert np.may_share_memory(s1_broadcast.lon, self.s1.lon)\n        assert np.may_share_memory(s1_broadcast.distance, self.s1.distance)\n\n        # A final test that \"may_share_memory\" equals \"does_share_memory\"\n        # Do this on a copy, to keep self.s0 unchanged.\n        sc = self.s0.copy()\n        assert not np.may_share_memory(sc.lon, self.s0.lon)\n        assert not np.may_share_memory(sc.lat, self.s0.lat)\n        sc_broadcast = np.broadcast_to(sc, (3, 6, 7))\n        assert np.may_share_memory(sc_broadcast.lon, sc.lon)\n        # Can only write to copy, not to broadcast version.\n        sc.lon[0, 0] = 22.0 * u.hourangle\n        assert np.all(sc_broadcast.lon[:, 0, 0] == 22.0 * u.hourangle)\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.8293237686157227}
{"question": "What modules imported in the helper module that provides Quantity-specific implementations for numpy function overrides would be affected if the numpy functions that examine array dimensions and structure were removed from the dictionary mapping numpy functions to their custom implementations?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "apply_over_axes", "is_method": false, "class_name": null, "parameters": ["func", "a", "axes"], "calls": ["func", "np.array", "np.expand_dims", "ValueError"], "code_location": {"file": "function_helpers.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 1211, "end_line": 1237}, "code_snippet": "def apply_over_axes(func, a, axes):\n    # Copied straight from numpy/lib/shape_base, just to omit its\n    # val = asarray(a); if only it had been asanyarray, or just not there\n    # since a is assumed to an an array in the next line...\n    # Which is what we do here - we can only get here if it is a Quantity.\n    val = a\n    N = a.ndim\n    if np.array(axes).ndim == 0:\n        axes = (axes,)\n    for axis in axes:\n        if axis < 0:\n            axis = N + axis\n        args = (val, axis)\n        res = func(*args)\n        if res.ndim == val.ndim:\n            val = res\n        else:\n            res = np.expand_dims(res, axis)\n            if res.ndim == val.ndim:\n                val = res\n            else:\n                raise ValueError(\n                    \"function is not returning an array of the correct shape\"\n                )\n    # Returning unit is None to signal nothing should happen to\n    # the output.\n    return val, None, None\n", "type": "function"}, {"name": "__array_ufunc__", "is_method": true, "class_name": "FunctionQuantity", "parameters": ["self", "function", "method"], "calls": ["__array_ufunc__", "UnitTypeError", "super"], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/function", "start_line": 660, "end_line": 670}, "code_snippet": "    def __array_ufunc__(self, function, method, *inputs, **kwargs):\n        # TODO: it would be more logical to have this in Quantity already,\n        # instead of in UFUNC_HELPERS, where it cannot be overridden.\n        # And really it should just return NotImplemented, since possibly\n        # another argument might know what to do.\n        if function not in self._supported_ufuncs:\n            raise UnitTypeError(\n                f\"Cannot use ufunc '{function.__name__}' with function quantities\"\n            )\n\n        return super().__array_ufunc__(function, method, *inputs, **kwargs)\n", "type": "function"}, {"name": "array2string", "is_method": false, "class_name": null, "parameters": ["a"], "calls": ["kwargs.get", "len", "np.printoptions", "_make_options_dict", "_get_format_function"], "code_location": {"file": "function_helpers.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 1269, "end_line": 1302}, "code_snippet": "def array2string(a, *args, **kwargs):\n    # array2string breaks on quantities as it tries to turn individual\n    # items into float, which works only for dimensionless.  Since the\n    # defaults would not keep any unit anyway, this is rather pointless -\n    # we're better off just passing on the array view.  However, one can\n    # also work around this by passing on a formatter (as is done in Angle).\n    # So, we do nothing if the formatter argument is present and has the\n    # relevant formatter for our dtype.\n    formatter = args[6] if len(args) >= 7 else kwargs.get(\"formatter\")\n\n    if formatter is None:\n        a = a.value\n    else:\n        # See whether it covers our dtype.\n        if NUMPY_LT_2_0:\n            from numpy.core.arrayprint import _get_format_function, _make_options_dict\n        else:\n            from numpy._core.arrayprint import _get_format_function, _make_options_dict\n\n        with np.printoptions(formatter=formatter) as options:\n            options = _make_options_dict(**options)\n            try:\n                ff = _get_format_function(a.value, **options)\n            except Exception:\n                # Shouldn't happen, but possibly we're just not being smart\n                # enough, so let's pass things on as is.\n                pass\n            else:\n                # If the selected format function is that of numpy, we know\n                # things will fail if we pass in the Quantity, so use .value.\n                if \"numpy\" in ff.__module__:\n                    a = a.value\n\n    return (a,) + args, kwargs, None, None\n", "type": "function"}, {"name": "TestQuantityReshapeFuncs", "docstring": "Test different ndarray methods that alter the array shape\n\ntests: reshape, squeeze, ravel, flatten, transpose, swapaxes", "methods": ["test_reshape", "test_squeeze", "test_ravel", "test_flatten", "test_transpose", "test_swapaxes", "test_flat_attributes"], "attributes": [], "code_location": {"file": "test_quantity_array_methods.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/tests", "start_line": 78, "end_line": 148}, "type": "class"}, {"name": "__array_function__", "is_method": true, "class_name": "Quantity", "parameters": ["self", "function", "types", "args", "kwargs"], "calls": ["self._result_as_quantity", "__array_function__", "super", "function_helper", "__array_function__", "self._not_implemented_or_raise", "function", "dispatched_function", "warnings.warn", "__array_function__", "super", "str", "self._not_implemented_or_raise", "super"], "code_location": {"file": "quantity.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units", "start_line": 1846, "end_line": 1944}, "code_snippet": "    def __array_function__(self, function, types, args, kwargs):\n        \"\"\"Wrap numpy functions, taking care of units.\n\n        Parameters\n        ----------\n        function : callable\n            Numpy function to wrap\n        types : iterable of classes\n            Classes that provide an ``__array_function__`` override. Can\n            in principle be used to interact with other classes. Below,\n            mostly passed on to `~numpy.ndarray`, which can only interact\n            with subclasses.\n        args : tuple\n            Positional arguments provided in the function call.\n        kwargs : dict\n            Keyword arguments provided in the function call.\n\n        Returns\n        -------\n        result: `~astropy.units.Quantity`, `~numpy.ndarray`\n            As appropriate for the function.  If the function is not\n            supported, `NotImplemented` is returned, which will lead to\n            a `TypeError` unless another argument overrode the function.\n\n        Raises\n        ------\n        ~astropy.units.UnitsError\n            If operands have incompatible units.\n        \"\"\"\n        # A function should be in one of the following sets or dicts:\n        # 1. SUBCLASS_SAFE_FUNCTIONS (set), if the numpy implementation\n        #    supports Quantity; we pass on to ndarray.__array_function__.\n        # 2. FUNCTION_HELPERS (dict), if the numpy implementation is usable\n        #    after converting quantities to arrays with suitable units,\n        #    and possibly setting units on the result.\n        # 3. DISPATCHED_FUNCTIONS (dict), if the function makes sense but\n        #    requires a Quantity-specific implementation.\n        # 4. UNSUPPORTED_FUNCTIONS (set), if the function does not make sense.\n        # For now, since we may not yet have complete coverage, if a\n        # function is in none of the above, we simply call the numpy\n        # implementation.\n        if function in SUBCLASS_SAFE_FUNCTIONS:\n            return super().__array_function__(function, types, args, kwargs)\n\n        elif function in FUNCTION_HELPERS:\n            function_helper = FUNCTION_HELPERS[function]\n            try:\n                args, kwargs, unit, out = function_helper(*args, **kwargs)\n            except NotImplementedError:\n                return self._not_implemented_or_raise(function, types)\n\n            try:\n                result = super().__array_function__(function, types, args, kwargs)\n            except AttributeError as e:\n                # this exception handling becomes unneeded in numpy 2.2 (not NUMPY_LT_2_2)\n                # see https://github.com/numpy/numpy/issues/27500\n                if \"_implementation\" not in str(e):\n                    raise\n                result = function(*args, **kwargs)\n\n            # Fall through to return section\n\n        elif function in DISPATCHED_FUNCTIONS:\n            dispatched_function = DISPATCHED_FUNCTIONS[function]\n            try:\n                result, unit, out = dispatched_function(*args, **kwargs)\n            except NotImplementedError:\n                return self._not_implemented_or_raise(function, types)\n\n            # Fall through to return section\n\n        elif function in UNSUPPORTED_FUNCTIONS:\n            return NotImplemented\n\n        else:\n            warnings.warn(\n                f\"function '{function.__name__}' is not known to astropy's Quantity.\"\n                \" Will run it anyway, hoping it will treat ndarray subclasses\"\n                \" correctly. Please raise an issue at\"\n                \" https://github.com/astropy/astropy/issues.\",\n                AstropyWarning,\n            )\n            return super().__array_function__(function, types, args, kwargs)\n\n        if unit is UNIT_FROM_LIKE_ARG:\n            # fallback mechanism for NEP 35 functions that dispatch on the 'like'\n            # argument (i.e. self, in this context), in cases where no other\n            # argument provides a unit\n            unit = self.unit\n\n        # If unit is None, a plain array is expected (e.g., boolean), which\n        # means we're done.\n        # We're also done if the result was NotImplemented, which can happen\n        # if other inputs/outputs override __array_function__;\n        # hopefully, they can then deal with us.\n        if unit is None or result is NotImplemented:\n            return result\n\n        return self._result_as_quantity(result, unit, out=out)\n", "type": "function"}, {"name": "__array_ufunc__", "is_method": true, "class_name": "SpectralQuantity", "parameters": ["self", "function", "method"], "calls": ["__array_ufunc__", "result.view", "result.__array_finalize__", "super", "TypeError", "result.view", "result.view"], "code_location": {"file": "spectral_quantity.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates", "start_line": 85, "end_line": 110}, "code_snippet": "    def __array_ufunc__(self, function, method, *inputs, **kwargs):\n        # We always return Quantity except in a few specific cases\n        result = super().__array_ufunc__(function, method, *inputs, **kwargs)\n        if (\n            (\n                function is np.multiply\n                or (function is np.true_divide and inputs[0] is self)\n            )\n            and result.unit == self.unit\n        ) or (\n            function in (np.minimum, np.maximum, np.fmax, np.fmin)\n            and method in (\"reduce\", \"reduceat\")\n        ):\n            result = result.view(self.__class__)\n            result.__array_finalize__(self)\n        else:\n            if result is self:\n                raise TypeError(\n                    \"Cannot store the result of this operation in\"\n                    f\" {self.__class__.__name__}\"\n                )\n            if result.dtype.kind == \"b\":\n                result = result.view(np.ndarray)\n            else:\n                result = result.view(Quantity)\n        return result\n", "type": "function"}, {"name": "test_functions", "is_method": true, "class_name": "TestSpectralQuantity", "parameters": ["self"], "calls": ["SpectralQuantity", "func", "isinstance", "func", "func", "isinstance", "func", "isinstance"], "code_location": {"file": "test_spectral_quantity.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/coordinates/tests", "start_line": 240, "end_line": 259}, "code_snippet": "    def test_functions(self):\n        # Checks for other functions - some operations should return SpectralQuantity,\n        # while some should just return plain Quantity\n\n        # First, operations that should return SpectralQuantity\n\n        sq1 = SpectralQuantity([10, 20, 30] * u.AA)\n        for func in (np.nanmin, np.nanmax):\n            sq2 = func(sq1)\n            assert isinstance(sq2, SpectralQuantity)\n            assert sq2.value == func(sq1.value)\n            assert sq2.unit == u.AA\n\n        # Next, operations that should return Quantity\n\n        for func in (np.sum,):\n            q3 = func(sq1)\n            assert isinstance(q3, u.Quantity) and not isinstance(q3, SpectralQuantity)\n            assert q3.value == func(sq1.value)\n            assert q3.unit == u.AA\n", "type": "function"}, {"name": "_not_implemented_or_raise", "is_method": true, "class_name": "Quantity", "parameters": ["self", "function", "types"], "calls": ["any", "TypeError", "issubclass", "issubclass"], "code_location": {"file": "quantity.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units", "start_line": 1946, "end_line": 1960}, "code_snippet": "    def _not_implemented_or_raise(self, function, types):\n        # Our function helper or dispatcher found that the function does not\n        # work with Quantity.  In principle, there may be another class that\n        # knows what to do with us, for which we should return NotImplemented.\n        # But if there is ndarray (or a non-Quantity subclass of it) around,\n        # it quite likely coerces, so we should just break.\n        if any(\n            issubclass(t, np.ndarray) and not issubclass(t, Quantity) for t in types\n        ):\n            raise TypeError(\n                f\"the Quantity implementation cannot handle {function} \"\n                \"with the given arguments.\"\n            ) from None\n        else:\n            return NotImplemented\n", "type": "function"}, {"name": "_wrap_function", "is_method": true, "class_name": "FunctionQuantity", "parameters": ["self", "function"], "calls": ["all", "TypeError", "_wrap_function", "tuple", "self._function_view._wrap_function", "super", "getattr", "hasattr", "hasattr"], "code_location": {"file": "core.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/function", "start_line": 769, "end_line": 785}, "code_snippet": "    def _wrap_function(self, function, *args, **kwargs):\n        if function in self._supported_functions:\n            return super()._wrap_function(function, *args, **kwargs)\n\n        # For dimensionless, we can convert to regular quantities.\n        if all(\n            arg.unit.physical_unit == dimensionless_unscaled\n            for arg in (self,) + args\n            if (hasattr(arg, \"unit\") and hasattr(arg.unit, \"physical_unit\"))\n        ):\n            args = tuple(getattr(arg, \"_function_view\", arg) for arg in args)\n            return self._function_view._wrap_function(function, *args, **kwargs)\n\n        raise TypeError(\n            f\"Cannot use method that uses function '{function.__name__}' with \"\n            \"function quantities that are not dimensionless.\"\n        )\n", "type": "function"}, {"name": "get_scipy_special_helpers", "is_method": false, "class_name": null, "parameters": [], "calls": ["getattr", "getattr", "getattr"], "code_location": {"file": "scipy_special.py", "path": "/data3/pwh/swebench-repos/astropy/astropy/units/quantity_helper", "start_line": 68, "end_line": 85}, "code_snippet": "def get_scipy_special_helpers():\n    import scipy.special as sps\n\n    SCIPY_HELPERS = {}\n    for name in dimensionless_to_dimensionless_sps_ufuncs:\n        ufunc = getattr(sps, name, None)\n        SCIPY_HELPERS[ufunc] = helper_dimensionless_to_dimensionless\n\n    for ufunc in degree_to_dimensionless_sps_ufuncs:\n        SCIPY_HELPERS[getattr(sps, ufunc)] = helper_degree_to_dimensionless\n\n    for ufunc in two_arg_dimensionless_sps_ufuncs:\n        SCIPY_HELPERS[getattr(sps, ufunc)] = helper_two_arg_dimensionless\n\n    # ufuncs handled as special cases\n    SCIPY_HELPERS[sps.cbrt] = helper_cbrt\n    SCIPY_HELPERS[sps.radian] = helper_degree_minute_second_to_radian\n    return SCIPY_HELPERS\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.8167688846588135}
