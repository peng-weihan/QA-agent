{"question": "What is the architectural role of the functional-style wrapper classes that convert rule context segments into tuple-based collections with query methods in decoupling the layout rule that enforces select target line placement from direct parse tree structure access?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "_get_indexes", "is_method": true, "class_name": "Rule_LT09", "parameters": ["context"], "calls": ["segment.children", "children.select", "children.find", "children.select", "children.select", "get", "siblings_post.select", "SelectTargetsInfo", "sp.is_type", "select_targets.get", "sp.is_keyword", "children.find", "sp.is_type", "children.find", "children.select", "children.select", "children.find", "FunctionalContext", "sp.is_type", "list", "selects.get", "newlines.get", "sp.is_type", "sp.is_type", "segments_after_first_line.get", "first", "FunctionalContext", "selects.get", "newlines.get", "sp.or_", "children.find", "sp.is_type", "sp.is_type", "sp.is_meta", "comment_after_select.get", "siblings_post.first", "sp.is_type"], "code_location": {"file": "LT09.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 109, "end_line": 157}, "code_snippet": "    def _get_indexes(context: RuleContext) -> SelectTargetsInfo:\n        children = FunctionalContext(context).segment.children()\n        select_targets = children.select(sp.is_type(\"select_clause_element\"))\n        first_select_target_idx = children.find(select_targets.get())\n        selects = children.select(sp.is_keyword(\"select\"))\n        select_idx = children.find(selects.get()) if selects else -1\n        newlines = children.select(sp.is_type(\"newline\"))\n        first_new_line_idx = children.find(newlines.get()) if newlines else -1\n        comment_after_select_idx = -1\n        if newlines:\n            comment_after_select = children.select(\n                sp.is_type(\"comment\"),\n                start_seg=selects.get(),\n                stop_seg=newlines.get(),\n                loop_while=sp.or_(\n                    sp.is_type(\"comment\"), sp.is_type(\"whitespace\"), sp.is_meta()\n                ),\n            )\n            if comment_after_select:\n                comment_after_select_idx = (\n                    children.find(comment_after_select.get())\n                    if comment_after_select\n                    else -1\n                )\n        first_whitespace_idx = -1\n        if first_new_line_idx != -1:\n            # TRICKY: Ignore whitespace prior to the first newline, e.g. if\n            # the line with \"SELECT\" (before any select targets) has trailing\n            # whitespace.\n            segments_after_first_line = children.select(\n                sp.is_type(\"whitespace\"), start_seg=children[first_new_line_idx]\n            )\n            first_whitespace_idx = children.find(segments_after_first_line.get())\n\n        siblings_post = FunctionalContext(context).siblings_post\n        from_segment = siblings_post.first(sp.is_type(\"from_clause\")).first().get()\n        pre_from_whitespace = siblings_post.select(\n            sp.is_type(\"whitespace\"), stop_seg=from_segment\n        )\n        return SelectTargetsInfo(\n            select_idx,\n            first_new_line_idx,\n            first_select_target_idx,\n            first_whitespace_idx,\n            comment_after_select_idx,\n            select_targets,\n            from_segment,\n            list(pre_from_whitespace),\n        )\n", "type": "function"}, {"name": "Segments", "docstring": "Encapsulates a sequence of one or more BaseSegments.\n\nThe segments may or may not be contiguous in a parse tree.\nProvides useful operations on a sequence of segments to simplify rule creation.", "methods": ["__new__", "__init__", "__add__", "__radd__", "find", "all", "any", "reversed", "raw_slices", "raw_segments", "recursive_crawl_all", "recursive_crawl", "children", "first", "last", "__iter__", "__getitem__", "__getitem__", "__getitem__", "get", "apply", "select", "iterate_segments"], "attributes": [], "code_location": {"file": "segments.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/functional", "start_line": 20, "end_line": 224}, "type": "class"}, {"name": "SelectClauseSegment", "docstring": "A group of elements in a select target statement.", "methods": [], "attributes": [], "code_location": {"file": "dialect_mysql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2074, "end_line": 2079}, "type": "class"}, {"name": "SelectClauseSegment", "docstring": "A group of elements in a select target statement.", "methods": [], "attributes": ["type", "match_grammar", "match_grammar", "match_grammar", "match_grammar", "type", "match_grammar", "type", "match_grammar", "type", "match_grammar"], "code_location": {"file": "dialect_exasol.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 363, "end_line": 392}, "type": "class"}, {"name": "SelectClauseSegment", "docstring": "A group of elements in a select target statement.", "methods": [], "attributes": [], "code_location": {"file": "dialect_ansi.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1908, "end_line": 1923}, "type": "class"}, {"name": "_eval_single_select_target_element", "is_method": true, "class_name": "Rule_LT09", "parameters": ["self", "select_targets_info", "context"], "calls": ["select_clause.children", "is_type", "select_children.first", "LintResult", "FunctionalContext", "self.logger.info", "self.logger.info", "LintResult", "WhitespaceSegment", "initial_deletes.append", "sp.is_type", "select_children.index", "initial_deletes.append", "LintFix.replace", "is_type", "select_stmt.segments.index", "select_children.index", "modifier.get", "initial_deletes.append", "select_clause.get", "len", "next_segment.is_type", "select_clause.get", "select_clause.get", "modifier.get", "WhitespaceSegment", "len", "LintFix.delete", "select", "next_segment.is_type", "select_children.select", "is_type", "fixes.append", "fixes.append", "select_children.reversed", "sp.is_type", "fixes.append", "LintFix.delete", "fixes.append", "all_deletes.add", "LintFix.create_after", "LintFix.delete", "LintFix.delete", "list", "len", "NewlineSegment"], "code_location": {"file": "LT09.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 242, "end_line": 414}, "code_snippet": "    def _eval_single_select_target_element(\n        self, select_targets_info, context: RuleContext\n    ):\n        select_clause = FunctionalContext(context).segment\n        parent_stack = context.parent_stack\n        target_idx = select_targets_info.first_select_target_idx\n        select_children = select_clause.children()\n        target_seg = select_children[target_idx]\n\n        # If it's all on one line, then there's no issue.\n        if not (\n            select_targets_info.select_idx\n            < select_targets_info.first_new_line_idx\n            < target_idx\n        ):\n            self.logger.info(\n                \"Target at index %s is already on a single line.\",\n                target_idx,\n            )\n            return None\n\n        # Does the target contain a newline?\n        # i.e. even if it's a single element, does it already span more than\n        # one line?\n        if \"newline\" in target_seg.descendant_type_set:\n            self.logger.info(\n                \"Target at index %s spans multiple lines so ignoring.\",\n                target_idx,\n            )\n            return None\n\n        if select_targets_info.comment_after_select_idx != -1:\n            # The SELECT is followed by a comment on the same line. In order\n            # to autofix this, we'd need to move the select target between\n            # SELECT and the comment and potentially delete the entire line\n            # where the select target was (if it is now empty). This is\n            # *fairly tricky and complex*, in part because the newline on\n            # the select target's line is several levels higher in the\n            # parser tree. Hence, we currently don't autofix this. Could be\n            # autofixed in the future if/when we have the time.\n            return LintResult(anchor=select_clause.get())\n\n        # Prepare the select clause which will be inserted\n        insert_buff = [WhitespaceSegment(), target_seg]\n        # Delete the first select target from its original location.\n        # We'll add it to the right section at the end, once we know\n        # what to add.\n        initial_deletes = [target_seg]\n        # If there's whitespace before it, delete that too.\n        if select_children[target_idx - 1].is_type(\"whitespace\"):\n            initial_deletes.append(select_children[target_idx - 1])\n\n        # Do we have a modifier?\n        modifier: Optional[Segments]\n        modifier = select_children.first(sp.is_type(\"select_clause_modifier\"))\n\n        if (\n            # Check if the modifier is one we care about\n            modifier\n            # We only care if it's not already on the first line.\n            and select_children.index(modifier.get())\n            >= select_targets_info.first_new_line_idx\n        ):\n            # Prepend it to the insert buffer\n            insert_buff = [WhitespaceSegment(), modifier[0]] + insert_buff\n\n            modifier_idx = select_children.index(modifier.get())\n            # Delete the whitespace after it (which is two after, thanks to indent)\n            if (\n                len(select_children) > modifier_idx + 1\n                and select_children[modifier_idx + 2].is_whitespace\n            ):\n                initial_deletes.append(select_children[modifier_idx + 2])\n\n            # Delete the modifier itself\n            initial_deletes.append(modifier[0])\n\n            # Set the position marker for removing the preceding\n            # whitespace and newline, which we'll use below.\n            start_idx = modifier_idx\n            start_seg = modifier[0]\n        else:\n            # Set the position marker for removing the preceding\n            # whitespace and newline, which we'll use below.\n            start_idx = target_idx\n            start_seg = select_children[select_targets_info.first_new_line_idx]\n\n        fixes = [\n            # Insert the select_clause in place of the first newline in the\n            # Select statement\n            LintFix.replace(\n                select_children[select_targets_info.first_new_line_idx],\n                insert_buff,\n            ),\n            # Materialise any deletes so far...\n            *(LintFix.delete(seg) for seg in initial_deletes),\n        ]\n\n        if parent_stack and parent_stack[-1].is_type(\"select_statement\"):\n            select_stmt = parent_stack[-1]\n            select_clause_idx = select_stmt.segments.index(select_clause.get())\n            after_select_clause_idx = select_clause_idx + 1\n\n            if len(select_stmt.segments) > after_select_clause_idx:\n                add_newline = True\n                to_delete: Sequence[BaseSegment] = [target_seg]\n                next_segment = select_stmt.segments[after_select_clause_idx]\n\n                if next_segment.is_type(\"newline\"):\n                    # Since we're deleting the newline, we should also delete all\n                    # whitespace before it or it will add random whitespace to\n                    # following statements. So walk back through the segment\n                    # deleting whitespace until you get the previous newline, or\n                    # something else.\n                    to_delete = select_children.reversed().select(\n                        loop_while=sp.is_type(\"whitespace\"),\n                        start_seg=select_children[start_idx],\n                    )\n                    if to_delete:\n                        # The select_clause is immediately followed by a\n                        # newline. Delete the newline in order to avoid leaving\n                        # behind an empty line after fix, *unless* we stopped\n                        # due to something other than a newline.\n                        delete_last_newline = select_children[\n                            start_idx - len(to_delete) - 1\n                        ].is_type(\"newline\")\n\n                        # Delete the newline if we decided to.\n                        if delete_last_newline:\n                            fixes.append(LintFix.delete(next_segment))\n\n                elif next_segment.is_type(\"whitespace\"):\n                    # The select_clause has stuff after (most likely a comment)\n                    # Delete the whitespace immediately after the select clause\n                    # so the other stuff aligns nicely based on where the select\n                    # clause started.\n                    fixes.append(LintFix.delete(next_segment))\n\n                if to_delete:\n                    # Clean up by moving leftover select_clause segments.\n\n                    # Context: Some of the other fixes we make in\n                    # _eval_single_select_target_element() leave leftover\n                    # child segments that need to be moved to become\n                    # *siblings* of the select_clause.\n                    move_after_select_clause = select_children.select(\n                        start_seg=start_seg,\n                        stop_seg=to_delete[-1],\n                    )\n                    # :TRICKY: Below, we have a couple places where we\n                    # filter to guard against deleting the same segment\n                    # multiple times -- this is illegal.\n                    all_deletes = {\n                        fix.anchor for fix in fixes if fix.edit_type == \"delete\"\n                    }\n                    for seg in (*to_delete, *move_after_select_clause):\n                        if seg not in all_deletes:\n                            fixes.append(LintFix.delete(seg))\n                            all_deletes.add(seg)\n\n                    if move_after_select_clause or add_newline:\n                        fixes.append(\n                            LintFix.create_after(\n                                select_clause[0],\n                                ([NewlineSegment()] if add_newline else [])\n                                + list(move_after_select_clause),\n                            )\n                        )\n\n        return LintResult(\n            anchor=select_clause.get(),\n            fixes=fixes,\n        )\n", "type": "function"}, {"name": "crawl", "is_method": true, "class_name": "BaseCrawler", "parameters": ["self", "context"], "calls": [], "code_location": {"file": "crawlers.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 32, "end_line": 33}, "code_snippet": "    def crawl(self, context: RuleContext) -> Iterator[RuleContext]:\n        \"\"\"Yields a RuleContext for each segment the rule should process.\"\"\"\n", "type": "function"}, {"name": "SelectClauseSegment", "docstring": "A group of elements in a select target statement.\n\nOverriding ANSI to remove greedy logic which assumes statements have been\ndelimited", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 861, "end_line": 877}, "type": "class"}, {"name": "SelectClauseElementSegment", "docstring": "An element in the targets of a select statement.\n\nOverriding ANSI to remove greedy logic which assumes statements have been\ndelimited", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 798, "end_line": 817}, "type": "class"}, {"name": "SelectClauseElementSegment", "docstring": "An element in the targets of a select statement.", "methods": ["get_alias"], "attributes": ["type", "match_grammar", "match_grammar", "type", "match_grammar", "match_grammar"], "code_location": {"file": "dialect_duckdb.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 485, "end_line": 500}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.0055475234985352}
{"question": "What is the relationship between the standalone header formatting function's use of StringIO text buffers and the output stream formatter class's responsibility for managing formatted output streams?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "format_linting_result_header", "is_method": false, "class_name": null, "parameters": [], "calls": ["StringIO", "text_buffer.write", "text_buffer.getvalue"], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 62, "end_line": 66}, "code_snippet": "def format_linting_result_header() -> str:\n    \"\"\"Format the header of a linting result output.\"\"\"\n    text_buffer = StringIO()\n    text_buffer.write(\"==== readout ====\\n\")\n    return text_buffer.getvalue()\n", "type": "function"}, {"name": "OutputStreamFormatter", "docstring": "Formatter which writes to an OutputStream.\n\nOn instantiation, this formatter accepts a function to\ndispatch messages. Each public method accepts an object\nor data in a common format, with this class handling the\nformatting and output.\n\nThis class is designed to be subclassed if we eventually\nwant to provide other methods of surfacing output.\n\n\nArgs:\n    output_stream: Output is sent here\n    verbosity: Specifies how verbose output should be\n    filter_empty: If True, empty messages will not be dispatched\n    output_line_length: Maximum line length", "methods": ["__init__", "should_produce_plain_output", "_dispatch", "_format_config", "dispatch_config", "dispatch_persist_filename", "_format_path", "dispatch_path", "dispatch_template_header", "dispatch_parse_header", "dispatch_lint_header", "dispatch_compilation_header", "dispatch_processing_header", "dispatch_dialect_warning", "_format_file_violations", "dispatch_file_violations", "colorize", "colorize_helper", "cli_table_row", "cli_table", "format_filename", "format_violation", "format_linting_stats", "format_config_vals", "_format_rule_description", "format_rules", "format_dialects", "format_dialect_warning", "print_out_residual_error_counts", "print_out_violations_and_timing", "completion_message"], "attributes": [], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 69, "end_line": 714}, "type": "class"}, {"name": "FormatterInterface", "docstring": "Generic formatter interface.", "methods": ["dispatch_persist_filename", "dispatch_lint_header", "dispatch_file_violations", "dispatch_dialect_warning", "dispatch_template_header", "dispatch_parse_header", "dispatch_processing_header", "dispatch_path", "colorize"], "attributes": [], "code_location": {"file": "formatter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 23, "end_line": 80}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "PathAndUserErrorHandler", "parameters": ["self", "formatter"], "calls": [], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 128, "end_line": 129}, "code_snippet": "    def __init__(self, formatter: OutputStreamFormatter) -> None:\n        self.formatter = formatter\n", "type": "function"}, {"name": "OutputStream", "docstring": "Base class for linter output stream.", "methods": ["__init__", "write", "close"], "attributes": [], "code_location": {"file": "outputstream.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 14, "end_line": 26}, "type": "class"}, {"name": "dispatch_template_header", "is_method": true, "class_name": "FormatterInterface", "parameters": ["self", "fname", "linter_config", "file_config"], "calls": [], "code_location": {"file": "formatter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 53, "end_line": 60}, "code_snippet": "    def dispatch_template_header(\n        self,\n        fname: str,\n        linter_config: \"FluffConfig\",\n        file_config: Optional[\"FluffConfig\"],\n    ) -> None:\n        \"\"\"Dispatch the header displayed before templating.\"\"\"\n        ...\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "OutputStreamFormatter", "parameters": ["self", "output_stream", "nocolor", "verbosity", "filter_empty", "output_line_length", "show_lint_violations"], "calls": ["self.should_produce_plain_output"], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 88, "end_line": 102}, "code_snippet": "    def __init__(\n        self,\n        output_stream: OutputStream,\n        nocolor: bool,\n        verbosity: int = 0,\n        filter_empty: bool = True,\n        output_line_length: int = 80,\n        show_lint_violations: bool = False,\n    ):\n        self._output_stream = output_stream\n        self.plain_output = self.should_produce_plain_output(nocolor)\n        self.verbosity = verbosity\n        self._filter_empty = filter_empty\n        self.output_line_length = output_line_length\n        self.show_lint_violations = show_lint_violations\n", "type": "function"}, {"name": "handle_signature", "is_method": true, "class_name": "SQLFluffRule", "parameters": ["self", "sig", "signode"], "calls": ["addnodes.desc_type", "addnodes.desc_name", "raw_obj_type.capitalize", "len"], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 32, "end_line": 46}, "code_snippet": "    def handle_signature(self, sig, signode):\n        \"\"\"Handle the initial signature of the node.\n\n        This formats the header of the section.\n        \"\"\"\n        raw_obj_type = \"code\" if len(sig) == 4 else \"rule\"\n        obj_type = raw_obj_type.capitalize() + \" \"\n        signode += addnodes.desc_type(obj_type, obj_type)\n        signode += addnodes.desc_name(sig, sig)\n\n        fullname = obj_type + sig\n        signode[\"type\"] = raw_obj_type\n        signode[\"sig\"] = sig\n        signode[\"fullname\"] = fullname\n        return (fullname, raw_obj_type, sig)\n", "type": "function"}, {"name": "text", "is_method": true, "class_name": "FluffLogHandler", "parameters": ["self"], "calls": ["_remove_ansi_escape_sequences", "self.stream.getvalue"], "code_location": {"file": "logging.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/testing", "start_line": 34, "end_line": 36}, "code_snippet": "    def text(self) -> str:\n        \"\"\"The formatted log text.\"\"\"\n        return _remove_ansi_escape_sequences(self.stream.getvalue())\n", "type": "function"}, {"name": "dispatch_parse_header", "is_method": true, "class_name": "FormatterInterface", "parameters": ["self", "fname"], "calls": [], "code_location": {"file": "formatter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 63, "end_line": 65}, "code_snippet": "    def dispatch_parse_header(self, fname: str) -> None:\n        \"\"\"Dispatch the header displayed before parsing.\"\"\"\n        ...\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0139796733856201}
{"question": "What is the longest-match priority algorithm in the grammar matching attribute of the top-level statement parsing segment that resolves ambiguity when multiple statement type patterns match the same input token sequence?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "MultiStatementSegment", "docstring": "Overriding StatementSegment to allow for additional segment parsing.", "methods": [], "attributes": ["type"], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 582, "end_line": 594}, "type": "class"}, {"name": "StatementSegment", "docstring": "Overriding StatementSegment to allow for additional segment parsing.", "methods": [], "attributes": [], "code_location": {"file": "dialect_databricks.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1147, "end_line": 1171}, "type": "class"}, {"name": "StatementSegment", "docstring": "Overriding StatementSegment to allow for additional segment parsing.", "methods": [], "attributes": [], "code_location": {"file": "dialect_clickhouse.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2126, "end_line": 2139}, "type": "class"}, {"name": "StatementSegment", "docstring": "Overriding StatementSegment to allow for additional segment parsing.", "methods": [], "attributes": [], "code_location": {"file": "dialect_sparksql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2823, "end_line": 2879}, "type": "class"}, {"name": "StatementSegment", "docstring": "Overriding StatementSegment to allow for additional segment parsing.", "methods": [], "attributes": [], "code_location": {"file": "dialect_sqlite.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1124, "end_line": 1146}, "type": "class"}, {"name": "StatementSegment", "docstring": "Overriding StatementSegment to allow for additional segment parsing.", "methods": [], "attributes": [], "code_location": {"file": "dialect_athena.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 450, "end_line": 468}, "type": "class"}, {"name": "StatementSegment", "docstring": "Overriding StatementSegment to allow for additional segment parsing.", "methods": [], "attributes": [], "code_location": {"file": "dialect_starrocks.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 395, "end_line": 405}, "type": "class"}, {"name": "StatementSegment", "docstring": "Overriding StatementSegment to allow for additional segment parsing.", "methods": [], "attributes": [], "code_location": {"file": "dialect_mysql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1406, "end_line": 1458}, "type": "class"}, {"name": "StatementSegment", "docstring": "Overriding StatementSegment to allow for additional segment parsing.", "methods": [], "attributes": [], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 625, "end_line": 673}, "type": "class"}, {"name": "StatementSegment", "docstring": "Overriding StatementSegment to allow for additional segment parsing.", "methods": [], "attributes": [], "code_location": {"file": "dialect_hive.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 715, "end_line": 733}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.0363678932189941}
{"question": "How should the logging adapter that prepends rule codes to messages in the SQL linting framework be refactored to separate code extraction from formatting while maintaining logging adapter compatibility?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "prep_violations", "is_method": false, "class_name": null, "parameters": ["rule", "violations"], "calls": [], "code_location": {"file": "rules.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/testing", "start_line": 234, "end_line": 241}, "code_snippet": "def prep_violations(\n    rule: str, violations: Collection[ViolationDictType]\n) -> Collection[ViolationDictType]:\n    \"\"\"Default to test rule if code is omitted.\"\"\"\n    for v in violations:\n        if \"code\" not in v:\n            v[\"code\"] = rule\n    return violations\n", "type": "function"}, {"name": "_warn_unfixable", "is_method": true, "class_name": "Linter", "parameters": ["code"], "calls": ["linter_logger.warning"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 313, "end_line": 316}, "code_snippet": "    def _warn_unfixable(code: str) -> None:\n        linter_logger.warning(\n            f\"One fix for {code} not applied, it would re-cause the same error.\"\n        )\n", "type": "function"}, {"name": "_log_apply_fixes_check_issue", "is_method": true, "class_name": "BaseSegment", "parameters": ["message"], "calls": ["linter_logger.critical"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 1221, "end_line": 1224}, "code_snippet": "    def _log_apply_fixes_check_issue(\n        message: str, *args: Any\n    ) -> None:  # pragma: no cover\n        linter_logger.critical(message, exc_info=True, *args)\n", "type": "function"}, {"name": "SQLLintError", "docstring": "An error which occurred during linting.\n\nIn particular we reference the rule here to do extended logging based on\nthe rule in question which caused the fail.\n\nArgs:\n    segment (:obj:`BaseSegment`, optional): The segment which is relevant\n        for the failure in parsing. This is likely to be a subclass of\n        `BaseSegment` rather than the parent class itself. This is mostly\n        used for logging and for referencing position.", "methods": ["__init__", "__reduce__", "to_dict", "fixable", "rule_code", "rule_name", "source_signature", "__repr__"], "attributes": ["_identifier"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 249, "end_line": 383}, "type": "class"}, {"name": "rule_code", "is_method": true, "class_name": "SQLBaseError", "parameters": ["self"], "calls": [], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 90, "end_line": 92}, "code_snippet": "    def rule_code(self) -> str:\n        \"\"\"Fetch the code of the rule which cause this error.\"\"\"\n        return self._code or \"????\"\n", "type": "function"}, {"name": "rule_code", "is_method": true, "class_name": "SQLLintError", "parameters": ["self"], "calls": [], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 341, "end_line": 343}, "code_snippet": "    def rule_code(self) -> str:\n        \"\"\"Fetch the code of the rule which cause this error.\"\"\"\n        return self.rule.code\n", "type": "function"}, {"name": "assert_rule_fail_in_sql", "is_method": false, "class_name": null, "parameters": ["code", "sql", "configs", "line_numbers"], "calls": ["print", "_setup_config", "lint_string", "linted.get_violations", "print", "linted.fix_string", "print", "startswith", "pytest.fail", "any", "print", "pytest.fail", "any", "Linter", "pytest.fail", "isinstance", "isinstance", "pytest.fail", "repr", "e.desc", "format", "split_comma_separated_string", "linted.tree.stringify", "get_rule_from_set", "split_comma_separated_string", "e.desc"], "code_location": {"file": "rules.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/testing", "start_line": 101, "end_line": 172}, "code_snippet": "def assert_rule_fail_in_sql(\n    code: str,\n    sql: str,\n    configs: Optional[ConfigMappingType] = None,\n    line_numbers: Optional[list[int]] = None,\n) -> tuple[str, list[SQLBaseError]]:\n    \"\"\"Assert that a given rule does fail on the given sql.\n\n    Args:\n        code (str): The code of the rule to test.\n        sql (str): The SQL text to check against.\n        configs (:obj:`ConfigMappingType`, optional): A config dict\n            object containing any overrides.\n        line_numbers (list of int, optional): The line numbers which\n            we want to test that errors occurred on.\n\n    Returns:\n        Tuple: values(fixed_sql (str), violations (list))\n            fixed_sql (str): The fixed string after linting. Note that for\n                testing purposes, `.lint_string()` is always called with\n                `fix` set to `True`.\n            violations (list of SQLBaseError): the violations found during\n                linting.\n    \"\"\"\n    print(\"# Asserting Rule Fail in SQL\")\n    # Set up the config to only use the rule we are testing.\n    cfg = _setup_config(code, configs)\n    # Lint it using the current config (while in fix mode)\n    linted = Linter(config=cfg).lint_string(sql, fix=True)\n    all_violations = linted.get_violations()\n    print(\"Errors Found:\")\n    for e in all_violations:\n        print(\"    \" + repr(e))\n        if e.desc().startswith(\"Unexpected exception\"):\n            pytest.fail(f\"Linter failed with {e.desc()}\")  # pragma: no cover\n    parse_errors = [\n        v for v in all_violations if isinstance(v, (SQLParseError, SQLTemplaterError))\n    ]\n    if parse_errors:\n        pytest.fail(f\"Found the following parse errors in test case: {parse_errors}\")\n    lint_errors: list[SQLLintError] = [\n        v for v in all_violations if isinstance(v, SQLLintError)\n    ]\n    if not any(v.rule.code in split_comma_separated_string(code) for v in lint_errors):\n        assert linted.tree\n        print(f\"Parsed File:\\n{linted.tree.stringify()}\")\n        pytest.fail(\n            f\"No {code} failures found in query which should fail.\",\n            pytrace=False,\n        )\n    if line_numbers:\n        actual_line_numbers = [e.line_no for e in lint_errors]\n        if line_numbers != actual_line_numbers:  # pragma: no cover\n            pytest.fail(\n                \"Expected errors on lines {}, but got errors on lines {}\".format(\n                    line_numbers, actual_line_numbers\n                )\n            )\n    fixed_sql, _ = linted.fix_string()\n\n    # Check that if it has made changes that this rule has set\n    # `is_fix_compatible` appropriately.\n    if fixed_sql != sql:\n        assert any(\n            get_rule_from_set(rule, config=cfg).is_fix_compatible\n            for rule in split_comma_separated_string(code)\n        ), (\n            f\"Rule {code} returned fixes but does not specify \"\n            \"'is_fix_compatible = True'.\"\n        )\n\n    return fixed_sql, linted.violations\n", "type": "function"}, {"name": "test__cli__command_fix_stdin_logging_to_stderr", "is_method": false, "class_name": null, "parameters": ["monkeypatch"], "calls": ["monkeypatch.setattr", "invoke_assert_code", "cls._warn_unfixable", "lint_fix_parsed", "super"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 1328, "end_line": 1345}, "code_snippet": "def test__cli__command_fix_stdin_logging_to_stderr(monkeypatch):\n    \"\"\"Check that logging goes to stderr when stdin is passed to fix.\"\"\"\n    perfect_sql = \"select col from table\"\n\n    class MockLinter(sqlfluff.core.Linter):\n        @classmethod\n        def lint_fix_parsed(cls, *args, **kwargs):\n            cls._warn_unfixable(\"<FAKE CODE>\")\n            return super().lint_fix_parsed(*args, **kwargs)\n\n    monkeypatch.setattr(sqlfluff.cli.commands, \"Linter\", MockLinter)\n    result = invoke_assert_code(\n        args=[fix, (\"-\", \"--rules=LT02\", \"--dialect=ansi\")],\n        cli_input=perfect_sql,\n    )\n\n    assert result.stdout == perfect_sql\n    assert \"<FAKE CODE>\" in result.stderr\n", "type": "function"}, {"name": "SQLFluffRule", "docstring": "SQLFluff rule directive for sphinx.\n\nRule directives can be used as shown below.\n\n.. code-block:: rst\n\n    .. sqlfluff:rule:: AM01\n                       ambiguous.distinct\n\n        Write the documentation for the rule here.\n\nTo cross reference (i.e. refer to) objects defined like this\nboth the code and name reference is available:\n\n.. code-block:: rst\n\n    :sqlfluff:ref:`CP02`\n    :sqlfluff:ref:`capitalisation.identifiers`", "methods": ["handle_signature", "add_target_and_index", "_object_hierarchy_parts", "_toc_entry_name"], "attributes": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 10, "end_line": 79}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "BaseRule", "parameters": ["self", "code", "description"], "calls": ["kwargs.items", "RuleLoggingAdapter", "kwargs.keys", "ValueError", "format"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 421, "end_line": 441}, "code_snippet": "    def __init__(self, code: str, description: str, **kwargs: Any) -> None:\n        self.description = description\n        self.code = code\n        # kwargs represents the config passed to the rule. Add all kwargs as class\n        # attributes so they can be accessed in rules which inherit from this class\n        for key, value in kwargs.items():\n            self.__dict__[key] = value\n\n        # We also define a custom logger here, which also includes the code\n        # of the rule in the logging.\n        self.logger = RuleLoggingAdapter(rules_logger, {\"code\": code})\n        # Validate that declared configuration options exist\n        for keyword in self.config_keywords:\n            if keyword not in kwargs.keys():\n                raise ValueError(\n                    (\n                        \"Unrecognized config '{}' for Rule {}. If this \"\n                        \"is a new option, please add it to \"\n                        \"`default_config.cfg` or plugin specific config.\"\n                    ).format(keyword, code)\n                )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0233423709869385}
{"question": "How does the class method that creates indentation line structures determine the initial balance by checking the last point's line break index to distinguish first from subsequent lines?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "from_points", "is_method": true, "class_name": "_IndentLine", "parameters": ["cls", "indent_points"], "calls": ["cls"], "code_location": {"file": "reindent.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 116, "end_line": 123}, "code_snippet": "    def from_points(cls, indent_points: list[_IndentPoint]) -> \"_IndentLine\":\n        # Catch edge case for first line where we'll start with a\n        # block if no initial indent.\n        if indent_points[-1].last_line_break_idx:\n            starting_balance = indent_points[0].closing_indent_balance\n        else:\n            starting_balance = 0\n        return cls(starting_balance, indent_points)\n", "type": "function"}, {"name": "opening_balance", "is_method": true, "class_name": "_IndentLine", "parameters": ["self"], "calls": [], "code_location": {"file": "reindent.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 212, "end_line": 223}, "code_snippet": "    def opening_balance(self) -> int:\n        \"\"\"The opening indent balance of the line.\n\n        NOTE: We use the first point for the starting balance rather than\n        the line starting balance because we're using this to detect missing\n        lines and if the line has been corrected then we don't want to do\n        that.\n        \"\"\"\n        # Edge case for first line of a file (where starting indent must be zero).\n        if self.indent_points[-1].last_line_break_idx is None:\n            return 0\n        return self.indent_points[0].closing_indent_balance\n", "type": "function"}, {"name": "_IndentLine", "docstring": "Temporary structure for handing a line of indent points.\n\nMutable so that we can adjust the initial indent balance\nfor things like comments and templated elements, after\nconstructing all the metadata for the points on the line.", "methods": ["__repr__", "from_points", "iter_elements", "iter_blocks", "iter_block_segments", "is_all_comments", "is_all_templates", "desired_indent_units", "closing_balance", "opening_balance"], "attributes": [], "code_location": {"file": "reindent.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 91, "end_line": 223}, "type": "class"}, {"name": "closing_balance", "is_method": true, "class_name": "_IndentLine", "parameters": ["self"], "calls": [], "code_location": {"file": "reindent.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 208, "end_line": 210}, "code_snippet": "    def closing_balance(self) -> int:\n        \"\"\"The closing indent balance of the line.\"\"\"\n        return self.indent_points[-1].closing_indent_balance\n", "type": "function"}, {"name": "_IndentPoint", "docstring": "Temporary structure for holding metadata about an indented ReflowPoint.\n\nWe only evaluate point which either *are* line breaks or\ncontain Indent/Dedent segments.", "methods": ["closing_indent_balance"], "attributes": [], "code_location": {"file": "reindent.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 65, "end_line": 87}, "type": "class"}, {"name": "_map_line_buffers", "is_method": false, "class_name": null, "parameters": ["elements", "allow_implicit_indents"], "calls": ["_crawl_indent_points", "point_buffer.append", "lines.append", "list", "len", "lines.append", "get_indent_impulse", "_IndentLine.from_points", "list", "reversed", "untaken_indent_locs.keys", "_IndentLine.from_points", "range", "any", "range", "all", "imbalanced_locs.append", "cast", "_previous_points.get", "range"], "code_location": {"file": "reindent.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 986, "end_line": 1139}, "code_snippet": "def _map_line_buffers(\n    elements: ReflowSequenceType, allow_implicit_indents: bool = False\n) -> tuple[list[_IndentLine], list[int]]:\n    \"\"\"Map the existing elements, building up a list of _IndentLine.\n\n    Returns:\n        :obj:`tuple` of a :obj:`list` of :obj:`_IndentLine` and a\n            :obj:`list` of :obj:`int`. The first is the main output\n            and is designed to be used in assessing indents and\n            their effect through a SQL file. The latter is a list of\n            \"imbalanced\" indent locations, where the positive indent\n            is untaken, but its corresponding negative indent *is*\n            taken.\n\n    \"\"\"\n    # First build up the buffer of lines.\n    lines = []\n    point_buffer = []\n    _previous_points = {}\n    # Buffers to keep track of indents which are untaken on the way\n    # up but taken on the way down. We track them explicitly so we\n    # can force them later.\n\n    #: dict of ints: maps indentation balance values to the last\n    #: index location where they were seen. This is a working buffer\n    #: and not directly returned by the function.\n    untaken_indent_locs = {}\n    #: list of ints: a list of element indices which contain untaken\n    #: positive indents, that should be forced later because their\n    #: corresponding negative indent _was_ taken. Several edge cases\n    #: are excluded from this list and so not included. See code below.\n    imbalanced_locs = []\n\n    for indent_point in _crawl_indent_points(\n        elements, allow_implicit_indents=allow_implicit_indents\n    ):\n        # We evaluate all the points in a line at the same time, so\n        # we first build up a buffer.\n        point_buffer.append(indent_point)\n        _previous_points[indent_point.idx] = indent_point\n\n        if not indent_point.is_line_break:\n            # If it's not a line break, we should still check whether it's\n            # a positive untaken to keep track of them.\n            # ...unless it's implicit.\n            indent_stats = cast(\n                ReflowPoint, elements[indent_point.idx]\n            ).get_indent_impulse()\n            if indent_point.indent_impulse > indent_point.indent_trough and not (\n                allow_implicit_indents and indent_stats.implicit_indents\n            ):\n                untaken_indent_locs[\n                    indent_point.initial_indent_balance + indent_point.indent_impulse\n                ] = indent_point.idx\n            continue\n\n        # If it *is* a line break, then store it.\n        lines.append(_IndentLine.from_points(point_buffer))\n\n        # We should also evaluate whether this point inserts a newline at the close\n        # of an indent which was untaken on the way up.\n        # https://github.com/sqlfluff/sqlfluff/issues/4234\n        # Special case 1:\n        # If we're at the end of the file we shouldn't interpret it as a line break\n        # for problem indents, they're a bit of a special case.\n        # Special case 2:\n        # Bracketed expressions are a bit odd here.\n        # e.g.\n        #   WHERE (\n        #       foo = bar\n        #   )\n        #   LIMIT 1\n        #\n        # Technically there's an untaken indent before the opening bracket\n        # but this layout is common practice so we're not going to force\n        # one there even though there _is_ a line break after the closing\n        # bracket.\n        following_class_types = elements[indent_point.idx + 1].class_types\n        if (\n            indent_point.indent_trough\n            # End of file ends case. (Special case 1)\n            and \"end_of_file\" not in following_class_types\n        ):\n            passing_indents = list(\n                range(\n                    indent_point.initial_indent_balance,\n                    indent_point.initial_indent_balance + indent_point.indent_trough,\n                    -1,\n                )\n            )\n            # There might be many indents at this point, but if any match, then\n            # we should still force an indent\n\n            # NOTE: We work _inward_ to check which have been taken.\n            for i in reversed(passing_indents):\n                # Was this outer one untaken?\n                if i not in untaken_indent_locs:\n                    # No? Stop the loop. If we've a corresponding indent for\n                    # this dedent, we shouldn't use the same location to force\n                    # untaken indents at inner levels.\n                    break\n\n                loc = untaken_indent_locs[i]\n\n                # First check for bracket special case. It's less about whether\n                # the section _ends_ with a lone bracket, and more about whether\n                # the _starting point_ is a bracket which closes a line. If it\n                # is, then skip this location. (Special case 2).\n                # NOTE: We can safely \"look ahead\" here because we know all files\n                # end with an IndentBlock, and we know here that `loc` refers to\n                # an IndentPoint.\n                if \"start_bracket\" in elements[loc + 1].class_types:\n                    continue\n\n                # If the location was in the line we're just closing. That's\n                # not a problem because it's an untaken indent which is closed\n                # on the same line.\n                if any(ip.idx == loc for ip in point_buffer):\n                    continue\n\n                # If the only elements between current point and the end of the\n                # reference line are comments, then don't trigger, it's a misplaced\n                # indent.\n                # First find the end of the reference line.\n                for j in range(loc, indent_point.idx):\n                    _pt = _previous_points.get(j, None)\n                    if not _pt:\n                        continue\n                    if _pt.is_line_break:\n                        break\n                assert _pt\n                # Then check if all comments.\n                if all(\n                    \"comment\" in elements[k].class_types\n                    for k in range(_pt.idx + 1, indent_point.idx, 2)\n                ):\n                    # It is all comments. Ignore it.\n                    continue\n\n                imbalanced_locs.append(loc)\n\n        # Remove any which are now no longer relevant from the working buffer.\n        for k in list(untaken_indent_locs.keys()):\n            if k > indent_point.initial_indent_balance + indent_point.indent_trough:\n                del untaken_indent_locs[k]\n\n        # Reset the buffer\n        point_buffer = [indent_point]\n\n    # Handle potential final line\n    if len(point_buffer) > 1:\n        lines.append(_IndentLine.from_points(point_buffer))\n\n    return lines, imbalanced_locs\n", "type": "function"}, {"name": "closing_indent_balance", "is_method": true, "class_name": "_IndentPoint", "parameters": ["self"], "calls": [], "code_location": {"file": "reindent.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 86, "end_line": 87}, "code_snippet": "    def closing_indent_balance(self) -> int:\n        return self.initial_indent_balance + self.indent_impulse\n", "type": "function"}, {"name": "_match_indents", "is_method": false, "class_name": null, "parameters": ["line_elements", "rebreak_priorities", "newline_idx", "allow_implicit_indents"], "calls": ["defaultdict", "enumerate", "list", "e.get_indent_impulse", "_increment_balance", "nmi.items", "matched_indents.keys", "list", "isinstance", "extend", "append", "matched_indents.pop", "reflow_logger.debug", "matched_indents.keys", "difference", "implicit_indents.keys", "matched_indents.pop", "reflow_logger.debug", "len", "set"], "code_location": {"file": "reindent.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 1864, "end_line": 1945}, "code_snippet": "def _match_indents(\n    line_elements: ReflowSequenceType,\n    rebreak_priorities: dict[int, int],\n    newline_idx: int,\n    allow_implicit_indents: bool = False,\n) -> MatchedIndentsType:\n    \"\"\"Identify indent points, taking into account rebreak_priorities.\n\n    Expect fractional keys, because of the half values for\n    rebreak points.\n    \"\"\"\n    balance = 0\n    matched_indents: MatchedIndentsType = defaultdict(list)\n    implicit_indents: dict[int, tuple[int, ...]] = {}\n    for idx, e in enumerate(line_elements):\n        # We only care about points, because only they contain indents.\n        if not isinstance(e, ReflowPoint):\n            continue\n\n        # As usual, indents are referred to by their \"uphill\" side\n        # so what number we store the point against depends on whether\n        # it's positive or negative.\n        # NOTE: Here we don't actually pass in the forward types because\n        # we don't need them for the output. It doesn't make a difference.\n        indent_stats = e.get_indent_impulse()\n        e_idx = newline_idx - len(line_elements) + idx + 1\n        # Save any implicit indents.\n        if indent_stats.implicit_indents:\n            implicit_indents[e_idx] = indent_stats.implicit_indents\n        balance, nmi = _increment_balance(balance, indent_stats, e_idx)\n        # Incorporate nmi into matched_indents\n        for b, indices in nmi.items():\n            matched_indents[b].extend(indices)\n\n        # Something can be both an indent point AND a rebreak point.\n        if idx in rebreak_priorities:\n            # For potential rebreak options (i.e. ones without an indent)\n            # we add 0.5 so that they sit *between* the varying indent\n            # options. that means we split them before any of their\n            # content, but don't necessarily split them when their\n            # container is split.\n\n            # Also to spread out the breaks within an indent, we further\n            # add hints to distinguish between them. This is where operator\n            # precedence (as defined above) actually comes into effect.\n            priority = rebreak_priorities[idx]\n            # Assume `priority` in range 0 - 50. So / 100 to add to 0.5.\n            matched_indents[balance + 0.5 + (priority / 100)].append(e_idx)\n        else:\n            continue\n\n    # Before working out the lowest option, we purge any which contain\n    # ONLY the final point. That's because adding indents there won't\n    # actually help the line length. There's *already* a newline there.\n    for indent_level in list(matched_indents.keys()):\n        if matched_indents[indent_level] == [newline_idx]:\n            matched_indents.pop(indent_level)\n            reflow_logger.debug(\n                \"    purging balance of %s, it references only the final element.\",\n                indent_level,\n            )\n\n    # ADDITIONALLY - if implicit indents are allowed we should\n    # only use them if they match another untaken point (which isn't\n    # implicit, or the end of the line).\n    # NOTE: This logic might be best suited to be sited elsewhere\n    # when (and if) we introduce smarter choices on where to add\n    # indents.\n    if allow_implicit_indents:\n        for indent_level in list(matched_indents.keys()):\n            major_points = set(matched_indents[indent_level]).difference(\n                [newline_idx], implicit_indents.keys()\n            )\n            if not major_points:\n                matched_indents.pop(indent_level)\n                reflow_logger.debug(\n                    \"    purging balance of %s, it references implicit indents \"\n                    \"or the final indent.\",\n                    indent_level,\n                )\n\n    return matched_indents\n", "type": "function"}, {"name": "_generate_indent_stats", "is_method": true, "class_name": "ReflowPoint", "parameters": ["segments"], "calls": ["IndentStats", "seg.is_type", "tuple", "cast", "implicit_indents.append"], "code_location": {"file": "elements.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 384, "end_line": 407}, "code_snippet": "    def _generate_indent_stats(\n        segments: Sequence[RawSegment],\n    ) -> IndentStats:\n        \"\"\"Generate the change in intended indent balance.\n\n        This is the main logic which powers .get_indent_impulse()\n        \"\"\"\n        trough = 0\n        running_sum = 0\n        implicit_indents = []\n        for seg in segments:\n            if seg.is_type(\"indent\"):\n                indent_seg = cast(Indent, seg)\n                running_sum += indent_seg.indent_val\n                # Do we need to add a new implicit indent?\n                if indent_seg.is_implicit:\n                    implicit_indents.append(running_sum)\n                # NOTE: We don't check for removal of implicit indents\n                # because it's unlikely that one would be opened, and then\n                # closed within the same point. That would probably be the\n                # sign of a bug in the dialect.\n            if running_sum < trough:\n                trough = running_sum\n        return IndentStats(running_sum, trough, tuple(implicit_indents))\n", "type": "function"}, {"name": "_crawl_indent_points", "is_method": false, "class_name": null, "parameters": ["elements", "allow_implicit_indents"], "calls": ["enumerate", "isinstance", "IndentStats.from_combination", "_IndentPoint", "_update_crawl_balances", "elem.get_indent_impulse", "has_untemplated_newline", "cast", "IndentStats", "_update_crawl_balances", "IndentStats", "is_type", "isinstance", "_IndentPoint", "_IndentPoint", "cast", "elem_j.num_newlines"], "code_location": {"file": "reindent.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 804, "end_line": 983}, "code_snippet": "def _crawl_indent_points(\n    elements: ReflowSequenceType, allow_implicit_indents: bool = False\n) -> Iterator[_IndentPoint]:\n    \"\"\"Crawl through a reflow sequence, mapping existing indents.\n\n    This is where *most* of the logic for smart indentation\n    happens. The values returned here have a large impact on\n    exactly how indentation is treated.\n\n    NOTE: If a line ends with a comment, indent impulses are pushed\n    to the point _after_ the comment rather than before to aid with\n    indentation. This saves searching for them later.\n\n    TODO: Once this function *works*, there's definitely headroom\n    for simplification and optimisation. We should do that.\n    \"\"\"\n    last_line_break_idx: int | None = None\n    indent_balance = 0\n    untaken_indents: tuple[int, ...] = ()\n    cached_indent_stats: Optional[IndentStats] = None\n    cached_point: Optional[_IndentPoint] = None\n    for idx, elem in enumerate(elements):\n        if isinstance(elem, ReflowPoint):\n            # NOTE: The following line should never lead to an index error\n            # because files should always have a trailing IndentBlock containing\n            # an \"end_of_file\" marker, and so the final IndentPoint should always\n            # have _something_ after it.\n            indent_stats = IndentStats.from_combination(\n                cached_indent_stats,\n                elem.get_indent_impulse(),\n            )\n            # If don't allow implicit indents we should remove them here.\n            # Also, if we do - we should check for brackets.\n            # NOTE: The reason we check following class_types is because\n            # bracketed expressions behave a little differently and are an\n            # exception to the normal implicit indent rules. For implicit\n            # indents which precede bracketed expressions, the implicit indent\n            # is treated as a normal indent. In this case the start_bracket\n            # must be the start of the bracketed section which isn't closed\n            # on the same line - if it _is_ closed then we keep the implicit\n            # indents.\n            if indent_stats.implicit_indents:\n                unclosed_bracket = False\n                if (\n                    allow_implicit_indents\n                    and \"start_bracket\" in elements[idx + 1].class_types\n                ):\n                    # Is it closed in the line? Iterate forward to find out.\n                    # get the stack depth\n                    next_elem = cast(ReflowBlock, elements[idx + 1])\n                    depth = next_elem.depth_info.stack_depth\n                    for elem_j in elements[idx + 1 :]:\n                        if isinstance(elem_j, ReflowPoint):\n                            if elem_j.num_newlines() > 0:\n                                unclosed_bracket = True\n                                break\n                        elif (\n                            \"end_bracket\" in elem_j.class_types\n                            and elem_j.depth_info.stack_depth == depth\n                        ):\n                            break\n                    else:  # pragma: no cover\n                        unclosed_bracket = True\n\n                if unclosed_bracket or not allow_implicit_indents:\n                    # Blank indent stats if not using them\n                    indent_stats = IndentStats(\n                        indent_stats.impulse, indent_stats.trough, ()\n                    )\n\n            # Was there a cache?\n            if cached_indent_stats:\n                # If there was we can safely assume there is a cached point.\n                assert cached_point\n                # If there was, this is a signal that we need to yield two points.\n                # The content of those points depends on the newlines that surround the\n                # last segments (which will be comment block).\n                # _leading_ comments (i.e. those preceded by a newline): Yield _before_\n                # _trailing_ comments (or rare \"mid\" comments): Yield _after_\n                # TODO: We might want to reconsider the treatment of comments in the\n                # middle of lines eventually, but they're fairly unusual so not well\n                # covered in tests as of writing.\n\n                # We yield the first of those points here, and then manipulate the\n                # indent_stats object to allow the following code to yield the other.\n\n                # We can refer back to the cached point as a framework. In both\n                # cases we use the combined impulse and trough, but we use the\n                # current indent balance and untaken indents.\n                if cached_point.is_line_break:\n                    # It's a leading comment. Yield all the info in that point.\n                    yield _IndentPoint(\n                        cached_point.idx,\n                        indent_stats.impulse,\n                        indent_stats.trough,\n                        indent_balance,\n                        cached_point.last_line_break_idx,\n                        True,\n                        untaken_indents,\n                    )\n                    # Before zeroing, crystallise any effect on overall balances.\n                    indent_balance, untaken_indents = _update_crawl_balances(\n                        untaken_indents, indent_balance, indent_stats, True\n                    )\n                    # Set indent stats to zero because we've already yielded.\n                    indent_stats = IndentStats(0, 0, indent_stats.implicit_indents)\n                else:\n                    # It's a trailing (or mid) comment. Yield it in the next.\n                    yield _IndentPoint(\n                        cached_point.idx,\n                        0,\n                        0,\n                        indent_balance,\n                        cached_point.last_line_break_idx,\n                        False,\n                        untaken_indents,\n                    )\n                    # No need to reset indent stats. It's already good.\n\n            # Reset caches.\n            cached_indent_stats = None\n            has_newline = False\n            cached_point = None\n\n            # Do we have a newline?\n            has_newline = has_untemplated_newline(elem) and idx != last_line_break_idx\n\n            # Construct the point we may yield\n            indent_point = _IndentPoint(\n                idx,\n                indent_stats.impulse,\n                indent_stats.trough,\n                indent_balance,\n                last_line_break_idx,\n                has_newline,\n                untaken_indents,\n            )\n\n            # Update the last newline index if this is a newline.\n            # NOTE: We used the previous value in the construction of the\n            # _IndentPoint above and we only reset after that construction.\n            if has_newline:\n                last_line_break_idx = idx\n\n            # Is the next element a comment? If so - delay the decision until we've\n            # got any indents from after the comment too.\n            #\n            # Also, some templaters might insert custom marker slices that are of zero\n            # source string length as a way of marking locations in the middle of\n            # templated output.  These don't correspond to real source code, so we\n            # can't meaningfully indent before them.  We can safely handle them similar\n            # to the comment case.\n            if \"comment\" in elements[idx + 1].class_types or (\n                \"placeholder\" in elements[idx + 1].class_types\n                and cast(TemplateSegment, elements[idx + 1].segments[0]).source_str\n                == \"\"\n            ):\n                cached_indent_stats = indent_stats\n                # Create parts of a point to use later.\n                cached_point = indent_point\n                # We loop around so that we don't do the untaken indent calcs yet.\n                continue\n            # Is it meaningful as an indent point?\n            # i.e. Is it a line break? AND not a templated one.\n            # NOTE: a point at idx zero is meaningful because it's like an indent.\n            # NOTE: Last edge case. If we haven't yielded yet, but the\n            # next element is the end of the file. Yield.\n            elif (\n                has_newline\n                or indent_stats.impulse\n                or indent_stats.trough\n                or idx == 0\n                or elements[idx + 1].segments[0].is_type(\"end_of_file\")\n            ):\n                yield indent_point\n\n            # Update balances\n            indent_balance, untaken_indents = _update_crawl_balances(\n                untaken_indents, indent_balance, indent_stats, has_newline\n            )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.036872386932373}
{"question": "How does the grammar matching attribute of the segment class defining REJECT clauses in import or export statements depend on the ordered-element grammar construct and the alternative-choice grammar construct from the parser module to enforce that REJECT appears before LIMIT?\n</start_of_rewritten_question>", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "RejectClauseSegment", "docstring": "`REJECT` clause within an import / export statement.", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_exasol.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2039, "end_line": 2051}, "type": "class"}, {"name": "SelectClauseElementSegment", "docstring": "An element in the targets of a select statement.\n\nOverriding ANSI to remove greedy logic which assumes statements have been\ndelimited", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 798, "end_line": 817}, "type": "class"}, {"name": "OrderByClauseSegment", "docstring": "A `ORDER BY` clause like in `SELECT`.\n\nOverriding ANSI to remove Greedy logic which assumes statements have been\ndelimited", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 4554, "end_line": 4585}, "type": "class"}, {"name": "ImportErrorsClauseSegment", "docstring": "`ERRORS` clause.", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_exasol.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1994, "end_line": 2012}, "type": "class"}, {"name": "ExceptClauseSegment", "docstring": "SELECT EXCEPT clause.", "methods": [], "attributes": [], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1260, "end_line": 1267}, "type": "class"}, {"name": "MatchRecognizeClauseSegment", "docstring": "A `MATCH_RECOGNIZE` clause.\n\nhttps://docs.snowflake.com/en/sql-reference/constructs/match_recognize.html", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_snowflake.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1564, "end_line": 1663}, "type": "class"}, {"name": "LimitClauseSegment", "docstring": "Overriding LimitClauseSegment to allow for additional segment parsing.", "methods": [], "attributes": [], "code_location": {"file": "dialect_clickhouse.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2160, "end_line": 2192}, "type": "class"}, {"name": "SelectClauseSegment", "docstring": "A group of elements in a select target statement.\n\nOverriding ANSI to remove greedy logic which assumes statements have been\ndelimited", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 861, "end_line": 877}, "type": "class"}, {"name": "ExcludeClauseSegment", "docstring": "A Redshift SELECT EXCLUDE clause.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_EXCLUDE_list.html", "methods": [], "attributes": ["type", "match_grammar", "type", "match_grammar"], "code_location": {"file": "dialect_redshift.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2930, "end_line": 2943}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "A snowflake unordered `SELECT` statement including optional Qualify.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_QUALIFY_clause.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_redshift.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2905, "end_line": 2916}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.0477712154388428}
{"question": "How does the SELECT statement segment in Vertica resolve the grammar composition conflict when the timeseries clause segment is inserted into the unordered SELECT statement segment's match grammar?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "SelectStatementSegment", "docstring": "A `SELECT` statement.\n\nCopy of ansi class except additional TimeseriesClauseSegment grammar", "methods": [], "attributes": [], "code_location": {"file": "dialect_vertica.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1948, "end_line": 1969}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "A `SELECT` statement without any ORDER clauses or later.\n\nCopy of ansi class except additional terminator TimeseriesClauseSegment", "methods": [], "attributes": [], "code_location": {"file": "dialect_vertica.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1922, "end_line": 1945}, "type": "class"}, {"name": "TimeseriesClauseSegment", "docstring": "A vertica `TIMESERIES` clause.\n\nhttps://docs.vertica.com/latest/en/sql-reference/statements/select/timeseries-clause/", "methods": [], "attributes": ["type"], "code_location": {"file": "dialect_vertica.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1902, "end_line": 1919}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "Enhance unordered `SELECT` statement for valid SparkSQL clauses.\n\nThis is designed for use in the context of set operations,\nfor other use cases, we should use the main\nSelectStatementSegment.", "methods": [], "attributes": [], "code_location": {"file": "dialect_sparksql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2131, "end_line": 2148}, "type": "class"}, {"name": "SelectStatementSegment", "docstring": "Overrides ANSI as the parse grammar copy needs to be reapplied.\n\nAs per https://www.postgresql.org/docs/current/sql-select.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_postgres.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1799, "end_line": 1825}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "Overrides ANSI Statement, to allow for SELECT INTO statements.", "methods": [], "attributes": [], "code_location": {"file": "dialect_postgres.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1781, "end_line": 1796}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "A snowflake unordered `SELECT` statement including optional Qualify.\n\nhttps://docs.snowflake.com/en/sql-reference/constructs/qualify.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_snowflake.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2901, "end_line": 2917}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "A snowflake unordered `SELECT` statement including optional Qualify.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_QUALIFY_clause.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_redshift.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2905, "end_line": 2916}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "Enhance unordered `SELECT` statement to include QUALIFY.", "methods": [], "attributes": [], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 570, "end_line": 579}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "Enhance unordered `SELECT` statement to include QUALIFY.", "methods": [], "attributes": [], "code_location": {"file": "dialect_clickhouse.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 456, "end_line": 462}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.038492202758789}
{"question": "How does the hook-based discovery mechanism in the function that aggregates rule configuration information from plugins enable separation between rule package configuration registration and the core framework?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "get_config_info", "is_method": false, "class_name": null, "parameters": [], "calls": ["get_plugin_manager", "plugin_manager.hook.get_configs_info", "config_info_dict.items"], "code_location": {"file": "config_info.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 88, "end_line": 99}, "code_snippet": "def get_config_info() -> dict[str, ConfigInfo]:\n    \"\"\"Get the config from core sqlfluff and sqlfluff plugins and merges them.\n\n    NOTE: This should be the entry point into getting config info rather than\n    importing the default set above, as many values are defined only in rule\n    packages.\n    \"\"\"\n    plugin_manager = get_plugin_manager()\n    configs_info = plugin_manager.hook.get_configs_info()\n    return {\n        k: v for config_info_dict in configs_info for k, v in config_info_dict.items()\n    }\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": ["get_rules_from_path"], "code_location": {"file": "lib.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/plugin", "start_line": 14, "end_line": 21}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: All standard rules will eventually be loaded as\n    plugins and so before 2.0.0, once all legacy plugin definitions\n    are migrated, this function will be amended to return no rules.\n    \"\"\"\n    return get_rules_from_path()\n", "type": "function"}, {"name": "get_objects", "is_method": true, "class_name": "SQLFluffDomain", "parameters": ["self"], "calls": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 108, "end_line": 110}, "code_snippet": "    def get_objects(self):\n        \"\"\"Hook to get all the rules.\"\"\"\n        yield from self.data[\"rules\"]\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "RuleSet", "parameters": ["self", "name", "config_info"], "calls": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 904, "end_line": 907}, "code_snippet": "    def __init__(self, name: str, config_info: dict[str, ConfigInfo]) -> None:\n        self.name = name\n        self.config_info = config_info\n        self._register: dict[str, RuleManifest] = {}\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/ambiguous", "start_line": 29, "end_line": 53}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.ambiguous.AM01 import Rule_AM01\n    from sqlfluff.rules.ambiguous.AM02 import Rule_AM02\n    from sqlfluff.rules.ambiguous.AM03 import Rule_AM03\n    from sqlfluff.rules.ambiguous.AM04 import Rule_AM04\n    from sqlfluff.rules.ambiguous.AM05 import Rule_AM05\n    from sqlfluff.rules.ambiguous.AM06 import Rule_AM06\n    from sqlfluff.rules.ambiguous.AM07 import Rule_AM07\n    from sqlfluff.rules.ambiguous.AM08 import Rule_AM08\n\n    return [\n        Rule_AM01,\n        Rule_AM02,\n        Rule_AM03,\n        Rule_AM04,\n        Rule_AM05,\n        Rule_AM06,\n        Rule_AM07,\n        Rule_AM08,\n    ]\n", "type": "function"}, {"name": "get_rulepack", "is_method": true, "class_name": "RuleSet", "parameters": ["self", "config"], "calls": ["self._validate_config_options", "config.get_section", "set", "self.rule_reference_map", "any", "any", "sorted", "self._expand_rule_refs", "self._expand_rule_refs", "RulePack", "self._register.keys", "manifest.rule_class.get_config_ref", "rules_logger.warning", "config.get", "list", "config.get", "rules_logger.warning", "rules_logger.warning", "self._register.keys", "rule_class.get_config_ref", "config.get_section", "description.format", "instantiated_rules.append", "self._register.values", "rules_config.items", "isinstance", "format", "format", "rules_config.items", "kwargs.update", "self._validate_config_options", "kwargs.update", "rule_class", "len", "rule_class.get_config_ref", "rules_logger.warning", "fnmatch.filter", "fnmatch.filter", "isinstance", "list", "rules_logger.warning", "reference_map.keys", "reference_map.keys"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 1078, "end_line": 1207}, "code_snippet": "    def get_rulepack(self, config: \"FluffConfig\") -> RulePack:\n        \"\"\"Use the config to return the appropriate rules.\n\n        We use the config both for allowlisting and denylisting, but also\n        for configuring the rules given the given config.\n        \"\"\"\n        # Validate all generic rule configs\n        self._validate_config_options(config)\n\n        # Fetch config section:\n        rules_config = config.get_section(\"rules\")\n\n        # Generate the master reference map. The priority order is:\n        # codes > names > groups > aliases\n        # (i.e. if there's a collision between a name and an\n        # alias - we assume the alias is wrong.)\n        valid_codes: set[str] = set(self._register.keys())\n        reference_map = self.rule_reference_map()\n        valid_config_lookups = {\n            manifest.rule_class.get_config_ref() for manifest in self._register.values()\n        }\n\n        # Validate config doesn't try to specify values for unknown rules.\n        # NOTE: We _warn_ here rather than error.\n        for unexpected_ref in [\n            # Filtering to dicts gives us the sections.\n            k\n            for k, v in rules_config.items()\n            if isinstance(v, dict)\n            # Only keeping ones we don't expect\n            if k not in valid_config_lookups\n        ]:\n            rules_logger.warning(\n                \"Rule configuration contain a section for unexpected \"\n                f\"rule {unexpected_ref!r}. These values will be ignored.\"\n            )\n            # For convenience (and migration), if we do find a potential match\n            # for the reference - add that as a warning.\n            # NOTE: We don't actually accept config in these cases, even though\n            # we could potentially match - because how to resolve _multiple_\n            # matching config sections is ambiguous.\n            if unexpected_ref in reference_map:\n                referenced_codes = reference_map[unexpected_ref]\n                if len(referenced_codes) == 1:\n                    referenced_code = list(referenced_codes)[0]\n                    referenced_name = self._register[referenced_code].name\n                    config_ref = self._register[\n                        referenced_code\n                    ].rule_class.get_config_ref()\n                    rules_logger.warning(\n                        \"The reference was however found as a match for rule \"\n                        f\"{referenced_code} with name {referenced_name!r}. \"\n                        \"SQLFluff assumes configuration for this rule will \"\n                        f\"be specified in 'sqlfluff:rules:{config_ref}'.\"\n                    )\n                elif referenced_codes:\n                    rules_logger.warning(\n                        \"The reference was found as a match for multiple rules: \"\n                        f\"{referenced_codes}. Config should be specified by the \"\n                        \"name of the relevant rule e.g. \"\n                        \"'sqlfluff:rules:capitalisation.keywords'.\"\n                    )\n\n        # The lists here are lists of references, which might be codes,\n        # names, aliases or groups.\n        # We default the allowlist to all the rules if not set (i.e. not specifying\n        # any rules, just means \"all the rules\").\n        allowlist = config.get(\"rule_allowlist\") or list(valid_codes)\n        denylist = config.get(\"rule_denylist\") or []\n\n        allowlisted_unknown_rule_codes = [\n            r\n            for r in allowlist\n            # Add valid groups to the register when searching for invalid rules _only_\n            if not fnmatch.filter(reference_map.keys(), r)\n        ]\n        if any(allowlisted_unknown_rule_codes):\n            rules_logger.warning(\n                \"Tried to allowlist unknown rule references: {!r}\".format(\n                    allowlisted_unknown_rule_codes\n                )\n            )\n\n        denylisted_unknown_rule_codes = [\n            r for r in denylist if not fnmatch.filter(reference_map.keys(), r)\n        ]\n        if any(denylisted_unknown_rule_codes):  # pragma: no cover\n            rules_logger.warning(\n                \"Tried to denylist unknown rules references: {!r}\".format(\n                    denylisted_unknown_rule_codes\n                )\n            )\n\n        keylist = sorted(self._register.keys())\n\n        # First we expand the allowlist and denylist globs\n        expanded_allowlist = self._expand_rule_refs(allowlist, reference_map)\n        expanded_denylist = self._expand_rule_refs(denylist, reference_map)\n\n        # Then we filter the rules\n        keylist = [\n            r for r in keylist if r in expanded_allowlist and r not in expanded_denylist\n        ]\n\n        # Construct the kwargs for each rule and instantiate in turn.\n        instantiated_rules = []\n        # Keep only config which isn't a section (for specific rule) (i.e. isn't a dict)\n        # We'll handle those directly in the specific rule config section below.\n        generic_rule_config = {\n            k: v for k, v in rules_config.items() if not isinstance(v, dict)\n        }\n        for code in keylist:\n            kwargs = {}\n            rule_class = self._register[code].rule_class\n            # Fetch the lookup code for the rule.\n            rule_config_ref = rule_class.get_config_ref()\n            specific_rule_config = config.get_section((\"rules\", rule_config_ref))\n            if generic_rule_config:\n                kwargs.update(generic_rule_config)\n            if specific_rule_config:\n                # Validate specific rule config before adding\n                self._validate_config_options(config, rule_config_ref)\n                kwargs.update(specific_rule_config)\n            kwargs[\"code\"] = code\n            # Allow variable substitution in making the description\n            kwargs[\"description\"] = self._register[code].description.format(**kwargs)\n            # Instantiate when ready\n            instantiated_rules.append(rule_class(**kwargs))\n\n        return RulePack(instantiated_rules, reference_map)\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-plugin-example/src/sqlfluff_plugin_example", "start_line": 23, "end_line": 38}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: It is much better that we only import the rule on demand.\n    The root module of the plugin (i.e. this file which contains\n    all of the hook implementations) should have fully loaded before\n    we try and import the rules. This is partly for performance\n    reasons - but more because the definition of a BaseRule requires\n    that all of the get_configs_info() methods have both been\n    defined _and have run_ before so all the validation information\n    is available for the validation steps in the meta class.\n    \"\"\"\n    # i.e. we DO recommend importing here:\n    from sqlfluff_plugin_example.rules import Rule_Example_L001  # noqa: F811\n\n    return [Rule_Example_L001]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 56, "end_line": 69}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.references.RF01 import Rule_RF01\n    from sqlfluff.rules.references.RF02 import Rule_RF02\n    from sqlfluff.rules.references.RF03 import Rule_RF03\n    from sqlfluff.rules.references.RF04 import Rule_RF04\n    from sqlfluff.rules.references.RF05 import Rule_RF05\n    from sqlfluff.rules.references.RF06 import Rule_RF06\n\n    return [Rule_RF01, Rule_RF02, Rule_RF03, Rule_RF04, Rule_RF05, Rule_RF06]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 47, "end_line": 85}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.layout.LT01 import Rule_LT01\n    from sqlfluff.rules.layout.LT02 import Rule_LT02\n    from sqlfluff.rules.layout.LT03 import Rule_LT03\n    from sqlfluff.rules.layout.LT04 import Rule_LT04\n    from sqlfluff.rules.layout.LT05 import Rule_LT05\n    from sqlfluff.rules.layout.LT06 import Rule_LT06\n    from sqlfluff.rules.layout.LT07 import Rule_LT07\n    from sqlfluff.rules.layout.LT08 import Rule_LT08\n    from sqlfluff.rules.layout.LT09 import Rule_LT09\n    from sqlfluff.rules.layout.LT10 import Rule_LT10\n    from sqlfluff.rules.layout.LT11 import Rule_LT11\n    from sqlfluff.rules.layout.LT12 import Rule_LT12\n    from sqlfluff.rules.layout.LT13 import Rule_LT13\n    from sqlfluff.rules.layout.LT14 import Rule_LT14\n    from sqlfluff.rules.layout.LT15 import Rule_LT15\n\n    return [\n        Rule_LT01,\n        Rule_LT02,\n        Rule_LT03,\n        Rule_LT04,\n        Rule_LT05,\n        Rule_LT06,\n        Rule_LT07,\n        Rule_LT08,\n        Rule_LT09,\n        Rule_LT10,\n        Rule_LT11,\n        Rule_LT12,\n        Rule_LT13,\n        Rule_LT14,\n        Rule_LT15,\n    ]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "start_line": 63, "end_line": 95}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.convention.CV01 import Rule_CV01\n    from sqlfluff.rules.convention.CV02 import Rule_CV02\n    from sqlfluff.rules.convention.CV03 import Rule_CV03\n    from sqlfluff.rules.convention.CV04 import Rule_CV04\n    from sqlfluff.rules.convention.CV05 import Rule_CV05\n    from sqlfluff.rules.convention.CV06 import Rule_CV06\n    from sqlfluff.rules.convention.CV07 import Rule_CV07\n    from sqlfluff.rules.convention.CV08 import Rule_CV08\n    from sqlfluff.rules.convention.CV09 import Rule_CV09\n    from sqlfluff.rules.convention.CV10 import Rule_CV10\n    from sqlfluff.rules.convention.CV11 import Rule_CV11\n    from sqlfluff.rules.convention.CV12 import Rule_CV12\n\n    return [\n        Rule_CV01,\n        Rule_CV02,\n        Rule_CV03,\n        Rule_CV04,\n        Rule_CV05,\n        Rule_CV06,\n        Rule_CV07,\n        Rule_CV08,\n        Rule_CV09,\n        Rule_CV10,\n        Rule_CV11,\n        Rule_CV12,\n    ]\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0544238090515137}
{"question": "What would be the propagation path through the whitespace reformatting utility's indentation checking workflow if the indent unit string generator were modified to accept an additional parameter for custom indent strings?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "construct_single_indent", "is_method": false, "class_name": null, "parameters": ["indent_unit", "tab_space_size"], "calls": ["SQLFluffUserError"], "code_location": {"file": "reindent.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 734, "end_line": 743}, "code_snippet": "def construct_single_indent(indent_unit: str, tab_space_size: int) -> str:\n    \"\"\"Construct a single indent unit.\"\"\"\n    if indent_unit == \"tab\":\n        return \"\\t\"\n    elif indent_unit == \"space\":\n        return \" \" * tab_space_size\n    else:  # pragma: no cover\n        raise SQLFluffUserError(\n            f\"Expected indent_unit of 'tab' or 'space', instead got {indent_unit}\"\n        )\n", "type": "function"}, {"name": "_get_indentation", "is_method": true, "class_name": "Rule_ST04", "parameters": ["self", "parent_segments", "segment", "tab_space_size", "indent_unit"], "calls": ["first", "last", "sp.is_type", "sp.is_type", "isinstance", "join", "reversed", "parent_segments.select", "seg_indent.get", "construct_single_indent", "leading_whitespace.get", "len", "parent_segments.select"], "code_location": {"file": "ST04.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 161, "end_line": 194}, "code_snippet": "    def _get_indentation(\n        self,\n        parent_segments: Segments,\n        segment: BaseSegment,\n        tab_space_size: int,\n        indent_unit: str,\n    ) -> str:\n        \"\"\"Calculate the indentation level for rebuilding nested struct.\n\n        This is only a best attempt as the input may not be equally indented. The layout\n        rules, if run, would resolve this.\n        \"\"\"\n        leading_whitespace = (\n            parent_segments.select(stop_seg=segment)\n            .reversed()\n            .first(sp.is_type(\"whitespace\"))\n        )\n        seg_indent = parent_segments.select(stop_seg=segment).last(sp.is_type(\"indent\"))\n        indent_level = 1\n        if (\n            seg_indent\n            and (segment_indent := seg_indent.get())\n            and isinstance(segment_indent, Indent)\n        ):\n            indent_level = segment_indent.indent_val + 1\n        indent_str = (\n            \"\".join(seg.raw for seg in leading_whitespace)\n            if leading_whitespace\n            and (whitespace_seg := leading_whitespace.get())\n            and len(whitespace_seg.raw) > 1\n            else construct_single_indent(indent_unit, tab_space_size) * indent_level\n        )\n\n        return indent_str\n", "type": "function"}, {"name": "_deduce_line_current_indent", "is_method": false, "class_name": null, "parameters": ["elements", "last_line_break_idx"], "calls": ["indent_seg.is_type", "_get_indent_segment", "NotImplementedError", "isinstance", "is_type", "source_str.split", "reflow_logger.warning", "cast", "reflow_logger.debug", "cast", "seg.source_str.startswith", "reflow_logger.debug", "indent_seg.is_type", "indent_seg.is_type", "cast"], "code_location": {"file": "reindent.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 1142, "end_line": 1208}, "code_snippet": "def _deduce_line_current_indent(\n    elements: ReflowSequenceType, last_line_break_idx: Optional[int] = None\n) -> str:\n    \"\"\"Deduce the current indent string.\n\n    This method accounts for both literal indents and indents\n    consumed from the source as by potential templating tags.\n    \"\"\"\n    indent_seg = None\n    if not elements[0].segments:\n        return \"\"\n    elif last_line_break_idx:\n        indent_seg = cast(\n            ReflowPoint, elements[last_line_break_idx]\n        )._get_indent_segment()\n    elif isinstance(elements[0], ReflowPoint) and elements[0].segments[\n        0\n    ].pos_marker.working_loc == (1, 1):\n        # No last_line_break_idx, but this is a point. It's the first line.\n\n        # First check whether this is a first line with a leading\n        # placeholder.\n        if elements[0].segments[0].is_type(\"placeholder\"):\n            reflow_logger.debug(\"    Handling as initial leading placeholder\")\n            seg = cast(TemplateSegment, elements[0].segments[0])\n            # Is the placeholder a consumed whitespace?\n            if seg.source_str.startswith((\" \", \"\\t\")):\n                indent_seg = seg\n        # Otherwise it's an initial leading literal whitespace.\n        else:\n            reflow_logger.debug(\"    Handling as initial leading whitespace\")\n            for indent_seg in elements[0].segments[::-1]:\n                if indent_seg.is_type(\"whitespace\") and not indent_seg.is_templated:\n                    break\n            # Handle edge case of no whitespace, but with newline.\n            if indent_seg and not indent_seg.is_type(\"whitespace\"):\n                indent_seg = None\n\n    if not indent_seg:\n        return \"\"\n\n    # We have to check pos marker before checking is templated.\n    # Insertions don't have pos_markers - so aren't templated,\n    # but also don't support calling is_templated.\n    if indent_seg.is_type(\"placeholder\"):\n        # It's a consumed indent.\n        return cast(TemplateSegment, indent_seg).source_str.split(\"\\n\")[-1] or \"\"\n    elif not indent_seg.pos_marker or not indent_seg.is_templated:\n        # It's a literal\n        assert \"\\n\" not in indent_seg.raw, f\"Found newline in indent: {indent_seg}\"\n        return indent_seg.raw\n    else:  # pragma: no cover\n        # It's templated. This shouldn't happen. Segments returned by\n        # _get_indent_segment, should be valid indents (i.e. whitespace\n        # or placeholders for consumed whitespace). This is a bug.\n        if indent_seg.pos_marker:\n            reflow_logger.warning(\n                \"Segment position marker: %s: [SRC: %s, TMP:%s]\",\n                indent_seg.pos_marker,\n                indent_seg.pos_marker.source_slice,\n                indent_seg.pos_marker.templated_slice,\n            )\n        raise NotImplementedError(\n            \"Unexpected templated indent. Report this as a bug on \"\n            f\"GitHub. Segment: {indent_seg}\\n\"\n            \"https://github.com/sqlfluff/sqlfluff/issues/new/choose\"\n        )\n", "type": "function"}, {"name": "test_reflow__desired_indent_units", "is_method": false, "class_name": null, "parameters": ["indent_line", "forced_indents", "expected_units"], "calls": ["pytest.mark.parametrize", "indent_line.desired_indent_units", "_IndentLine", "_IndentLine", "_IndentLine", "_IndentLine", "_IndentLine", "_IndentLine", "_IndentLine", "_IndentLine", "_IndentLine", "_IndentLine", "_IndentPoint", "_IndentPoint", "_IndentPoint", "_IndentPoint", "_IndentPoint", "_IndentPoint", "_IndentPoint", "_IndentPoint", "_IndentPoint", "_IndentPoint"], "code_location": {"file": "reindent_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/utils/reflow", "start_line": 830, "end_line": 832}, "code_snippet": "def test_reflow__desired_indent_units(indent_line, forced_indents, expected_units):\n    \"\"\"Test _IndentLine.desired_indent_units() directly.\"\"\"\n    assert indent_line.desired_indent_units(forced_indents) == expected_units\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_LT02", "parameters": ["self", "context"], "calls": ["get_results", "reindent", "ReflowSequence.from_root"], "code_location": {"file": "LT02.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 52, "end_line": 64}, "code_snippet": "    def _eval(self, context: RuleContext) -> list[LintResult]:\n        \"\"\"Indentation not consistent with previous lines.\n\n        To set the default tab size, set the `tab_space_size` value\n        in the appropriate configuration. To correct indents to tabs\n        use the `indent_unit` value set to `tab`.\n\n        \"\"\"\n        return (\n            ReflowSequence.from_root(context.segment, context.config)\n            .reindent()\n            .get_results()\n        )\n", "type": "function"}, {"name": "_indent_description", "is_method": false, "class_name": null, "parameters": ["indent"], "calls": ["all", "all", "NotImplementedError", "len", "len"], "code_location": {"file": "elements.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 229, "end_line": 252}, "code_snippet": "def _indent_description(indent: str) -> str:\n    \"\"\"Construct a human readable description of the indent.\n\n    NOTE: We operate assuming that the \"correct\" indent is\n    never a mix of tabs and spaces. That means if the provided\n    indent *does* contain both that this description is likely\n    a case where we are matching a pre-existing indent, and can\n    assume that the *description* of that indent is non-critical.\n    To handle that situation gracefully we just return \"Mixed Indent\".\n\n    See: https://github.com/sqlfluff/sqlfluff/issues/4255\n    \"\"\"\n    if indent == \"\":\n        return \"no indent\"\n    elif \" \" in indent and \"\\t\" in indent:\n        return \"mixed indent\"\n    elif indent[0] == \" \":\n        assert all(c == \" \" for c in indent)\n        return f\"indent of {len(indent)} spaces\"\n    elif indent[0] == \"\\t\":  # pragma: no cover\n        assert all(c == \"\\t\" for c in indent)\n        return f\"indent of {len(indent)} tabs\"\n    else:  # pragma: no cover\n        raise NotImplementedError(f\"Invalid indent construction: {indent!r}\")\n", "type": "function"}, {"name": "desired_indent_units", "is_method": true, "class_name": "_IndentLine", "parameters": ["self", "forced_indents"], "calls": ["reflow_logger.debug", "list", "len", "len"], "code_location": {"file": "reindent.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 155, "end_line": 206}, "code_snippet": "    def desired_indent_units(self, forced_indents: list[int]) -> int:\n        \"\"\"Calculate the desired indent units.\n\n        This is the heart of the indentation calculations.\n\n        First we work out how many previous indents are untaken.\n        In the easy case, we just use the number of untaken\n        indents from previous points. The more complicated example\n        is where *this point* has both dedents *and* indents. In\n        this case we use the `indent_trough` to prune any\n        previous untaken indents which were above the trough at\n        this point.\n\n        After that we calculate the indent from the incoming\n        balance, minus any relevant untaken events *plus* any\n        previously untaken indents which have been forced (i.e.\n        inserted by the same operation).\n        \"\"\"\n        if self.indent_points[0].indent_trough:\n            # This says - purge any untaken indents which happened before\n            # the trough (or at least only _keep_ any which would have remained).\n            # NOTE: Minus signs are really hard to get wrong here.\n            relevant_untaken_indents = [\n                i\n                for i in self.indent_points[0].untaken_indents\n                if i\n                <= self.initial_indent_balance\n                - (\n                    self.indent_points[0].indent_impulse\n                    - self.indent_points[0].indent_trough\n                )\n            ]\n        else:\n            relevant_untaken_indents = list(self.indent_points[0].untaken_indents)\n\n        desired_indent = (\n            self.initial_indent_balance\n            - len(relevant_untaken_indents)\n            + len(forced_indents)\n        )\n\n        reflow_logger.debug(\n            \"    Desired Indent Calculation: IB: %s, RUI: %s, UIL: %s, \"\n            \"iII: %s, iIT: %s. = %s\",\n            self.initial_indent_balance,\n            relevant_untaken_indents,\n            self.indent_points[0].untaken_indents,\n            self.indent_points[0].indent_impulse,\n            self.indent_points[0].indent_trough,\n            desired_indent,\n        )\n        return desired_indent\n", "type": "function"}, {"name": "indent_to", "is_method": true, "class_name": "ReflowPoint", "parameters": ["self", "desired_indent", "after", "before", "description", "source"], "calls": ["self._get_indent_segment", "reflow_logger.debug", "self.num_newlines", "indent_seg.is_type", "cast", "slice", "SourceFix", "self.num_newlines", "indent_seg.source_str.split", "slice_overlaps", "reflow_logger.warning", "indent_seg.edit", "tuple", "ReflowPoint", "NewlineSegment", "len", "reflow_logger.warning", "indent_seg.pos_marker.source_str", "LintFix.replace", "LintResult", "indent_seg.edit", "self.segments.index", "range", "WhitespaceSegment", "seg.is_type", "ReflowPoint", "self.segments.index", "LintFix.replace", "ReflowPoint", "ReflowPoint", "is_type", "ReflowPoint", "NotImplementedError", "tuple", "LintResult", "self.segments.index", "LintResult", "len", "pos_marker.is_literal", "LintResult", "LintFix.create_before", "LintFix.create_after", "ws_seg.edit", "ReflowPoint", "WhitespaceSegment", "before.is_type", "after.is_type", "tuple", "len", "LintResult", "LintFix.replace", "LintFix.replace", "cast", "cast", "_indent_description", "_indent_description", "_indent_description", "_indent_description", "_indent_description", "_indent_description", "LintFix.delete", "_indent_description", "_indent_description"], "code_location": {"file": "elements.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 413, "end_line": 686}, "code_snippet": "    def indent_to(\n        self,\n        desired_indent: str,\n        after: Optional[BaseSegment] = None,\n        before: Optional[BaseSegment] = None,\n        description: Optional[str] = None,\n        source: Optional[str] = None,\n    ) -> tuple[list[LintResult], \"ReflowPoint\"]:\n        \"\"\"Coerce a point to have a particular indent.\n\n        If the point currently contains no newlines, one will\n        be introduced and any trailing whitespace will be effectively\n        removed.\n\n        More specifically, the newline is *inserted before* the existing\n        whitespace, with the new indent being a *replacement* for that\n        same whitespace.\n\n        For placeholder newlines or indents we generate appropriate\n        source fixes.\n        \"\"\"\n        assert \"\\n\" not in desired_indent, \"Newline found in desired indent.\"\n        # Get the indent (or in the case of no newline, the last whitespace)\n        indent_seg = self._get_indent_segment()\n        reflow_logger.debug(\n            \"Coercing indent %s to %r. (newlines: %s)\",\n            indent_seg,\n            desired_indent,\n            self.num_newlines(),\n        )\n\n        if indent_seg and indent_seg.is_type(\"placeholder\"):\n            # Handle the placeholder case.\n            indent_seg = cast(TemplateSegment, indent_seg)\n            # There should always be a newline, so assert that.\n            assert \"\\n\" in indent_seg.source_str\n            # We should always replace the section _containing_ the\n            # newline, rather than just bluntly inserting. This\n            # makes slicing later easier.\n            current_indent = indent_seg.source_str.split(\"\\n\")[-1]\n            source_slice = slice(\n                indent_seg.pos_marker.source_slice.stop - len(current_indent),\n                indent_seg.pos_marker.source_slice.stop,\n            )\n            for existing_source_fix in indent_seg.source_fixes:  # pragma: no cover\n                if slice_overlaps(existing_source_fix.source_slice, source_slice):\n                    reflow_logger.warning(\n                        \"Creating overlapping source fix. Results may be \"\n                        \"unpredictable and this might be a sign of a bug. \"\n                        \"Please report this along with your query.\\n\"\n                        f\"({existing_source_fix.source_slice} overlaps \"\n                        f\"{source_slice})\"\n                    )\n\n            new_source_fix = SourceFix(\n                desired_indent,\n                source_slice,\n                # The templated slice is going to be a zero slice _anyway_.\n                indent_seg.pos_marker.templated_slice,\n            )\n\n            if new_source_fix in indent_seg.source_fixes:  # pragma: no cover\n                # NOTE: If we're trying to reapply the same fix, don't.\n                # Just return an error without the fixes. This is probably\n                # a bug if we're taking this route, but this clause will help\n                # catch bugs faster if they occur.\n                reflow_logger.warning(\n                    \"Attempted to apply a duplicate source fix to %r. \"\n                    \"Returning this time without fix.\",\n                    indent_seg.pos_marker.source_str(),\n                )\n                fixes = []\n                new_segments = self.segments\n            else:\n                if current_indent:\n                    new_source_str = (\n                        indent_seg.source_str[: -len(current_indent)] + desired_indent\n                    )\n                else:\n                    new_source_str = indent_seg.source_str + desired_indent\n                assert \"\\n\" in new_source_str\n                new_placeholder = indent_seg.edit(\n                    source_fixes=[new_source_fix],\n                    source_str=new_source_str,\n                )\n                fixes = [LintFix.replace(indent_seg, [new_placeholder])]\n                new_segments = tuple(\n                    new_placeholder if seg is indent_seg else seg\n                    for seg in self.segments\n                )\n\n            return [\n                LintResult(\n                    indent_seg,\n                    fixes,\n                    description=description\n                    or f\"Expected {_indent_description(desired_indent)}.\",\n                    source=source,\n                )\n            ], ReflowPoint(new_segments)\n\n        elif self.num_newlines():\n            # There is already a newline. Is there an indent?\n            if indent_seg:\n                # Coerce existing indent to desired.\n                if indent_seg.raw == desired_indent:\n                    # Trivial case. Indent already correct\n                    return [], self\n                elif desired_indent == \"\":\n                    idx = self.segments.index(indent_seg)\n                    return [\n                        LintResult(\n                            indent_seg,\n                            # Coerce to no indent. We don't want the indent. Delete it.\n                            [LintFix.delete(indent_seg)],\n                            description=description or \"Line should not be indented.\",\n                            source=source,\n                        )\n                    ], ReflowPoint(self.segments[:idx] + self.segments[idx + 1 :])\n\n                # Standard case of an indent change.\n                new_indent = indent_seg.edit(desired_indent)\n                idx = self.segments.index(indent_seg)\n                return [\n                    LintResult(\n                        indent_seg,\n                        [LintFix.replace(indent_seg, [new_indent])],\n                        description=description\n                        or f\"Expected {_indent_description(desired_indent)}.\",\n                        source=source,\n                    )\n                ], ReflowPoint(\n                    self.segments[:idx] + (new_indent,) + self.segments[idx + 1 :]\n                )\n\n            else:\n                # There is a newline, but no indent. Make one after the newline\n                # Find the index of the last newline (there _will_ be one because\n                # we checked self.num_newlines() above).\n\n                # Before going further, check we have a non-zero indent.\n                if not desired_indent:\n                    # We're trying to coerce a non-existent indent to zero. This\n                    # means we're already ok.\n                    return [], self\n\n                for idx in range(len(self.segments) - 1, -1, -1):\n                    # NOTE: Must be a _literal_ newline, not a templated one.\n                    # https://github.com/sqlfluff/sqlfluff/issues/4367\n                    if self.segments[idx].is_type(\"newline\"):\n                        if self.segments[idx].pos_marker.is_literal():\n                            break\n\n                new_indent = WhitespaceSegment(desired_indent)\n                return [\n                    LintResult(\n                        # The anchor for the *result* should be the segment\n                        # *after* the newline, otherwise the location of the fix\n                        # is confusing.\n                        # For this method, `before` is optional, but normally\n                        # passed. If it is there, use that as the anchor\n                        # instead. We fall back to the last newline if not.\n                        before if before else self.segments[idx],\n                        # Rather than doing a `create_after` here, we're\n                        # going to do a replace. This is effectively to give a hint\n                        # to the linter that this is safe to do before a templated\n                        # placeholder. This solves some potential bugs - although\n                        # it feels a bit like a workaround.\n                        [\n                            LintFix.replace(\n                                self.segments[idx], [self.segments[idx], new_indent]\n                            )\n                        ],\n                        description=description\n                        or f\"Expected {_indent_description(desired_indent)}.\",\n                        source=source,\n                    )\n                ], ReflowPoint(\n                    self.segments[: idx + 1] + (new_indent,) + self.segments[idx + 1 :]\n                )\n\n        else:\n            # There isn't currently a newline.\n            new_newline = NewlineSegment()\n            new_segs: list[RawSegment]\n            # Check for whitespace\n            ws_seg = None\n            for seg in self.segments[::-1]:\n                if seg.is_type(\"whitespace\"):\n                    ws_seg = seg\n            if not ws_seg:\n                # Work out the new segments. Always a newline, only whitespace if\n                # there's a non zero indent.\n                new_segs = [new_newline] + (\n                    [WhitespaceSegment(desired_indent)] if desired_indent else []\n                )\n                # There isn't a whitespace segment either. We need to insert one.\n                # Do we have an anchor?\n                if not before and not after:  # pragma: no cover\n                    raise NotImplementedError(\n                        \"Not set up to handle empty points in this \"\n                        \"scenario without provided before/after \"\n                        f\"anchor: {self.segments}\"\n                    )\n                # Otherwise make a new indent, attached to the relevant anchor.\n                # Prefer anchoring before because it makes the labelling better.\n                elif before:\n                    before_raw = (\n                        cast(TemplateSegment, before).source_str\n                        if before.is_type(\"placeholder\")\n                        else before.raw\n                    )\n                    fix = LintFix.create_before(before, new_segs)\n                    description = description or (\n                        \"Expected line break and \"\n                        f\"{_indent_description(desired_indent)} \"\n                        f\"before {before_raw!r}.\"\n                    )\n                else:\n                    assert after  # mypy hint\n                    after_raw = (\n                        cast(TemplateSegment, after).source_str\n                        if after.is_type(\"placeholder\")\n                        else after.raw\n                    )\n                    fix = LintFix.create_after(after, new_segs)\n                    description = description or (\n                        \"Expected line break and \"\n                        f\"{_indent_description(desired_indent)} \"\n                        f\"after {after_raw!r}.\"\n                    )\n                new_point = ReflowPoint(tuple(new_segs))\n                anchor = before\n            else:\n                # There is whitespace. Coerce it to the right indent and add\n                # a newline _before_. In the edge case that we're coercing to\n                # _no indent_, edit existing indent to be the newline and leave\n                # it there.\n                if desired_indent == \"\":\n                    new_segs = [new_newline]\n                else:\n                    new_segs = [new_newline, ws_seg.edit(desired_indent)]\n                idx = self.segments.index(ws_seg)\n                if not description:\n                    # Prefer before, because it makes the anchoring better.\n                    if before:\n                        description = (\n                            \"Expected line break and \"\n                            f\"{_indent_description(desired_indent)} \"\n                            f\"before {before.raw!r}.\"\n                        )\n                    elif after:\n                        description = (\n                            \"Expected line break and \"\n                            f\"{_indent_description(desired_indent)} \"\n                            f\"after {after.raw!r}.\"\n                        )\n                    else:  # pragma: no cover\n                        # NOTE: Doesn't have test coverage because there's\n                        # normally an `after` or `before` value, so this\n                        # clause is unused.\n                        description = (\n                            \"Expected line break and \"\n                            f\"{_indent_description(desired_indent)}.\"\n                        )\n                fix = LintFix.replace(ws_seg, new_segs)\n                new_point = ReflowPoint(\n                    self.segments[:idx] + tuple(new_segs) + self.segments[idx + 1 :]\n                )\n                anchor = ws_seg\n\n            return [\n                LintResult(anchor, fixes=[fix], description=description, source=source)\n            ], new_point\n", "type": "function"}, {"name": "_generate_indent_stats", "is_method": true, "class_name": "ReflowPoint", "parameters": ["segments"], "calls": ["IndentStats", "seg.is_type", "tuple", "cast", "implicit_indents.append"], "code_location": {"file": "elements.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 384, "end_line": 407}, "code_snippet": "    def _generate_indent_stats(\n        segments: Sequence[RawSegment],\n    ) -> IndentStats:\n        \"\"\"Generate the change in intended indent balance.\n\n        This is the main logic which powers .get_indent_impulse()\n        \"\"\"\n        trough = 0\n        running_sum = 0\n        implicit_indents = []\n        for seg in segments:\n            if seg.is_type(\"indent\"):\n                indent_seg = cast(Indent, seg)\n                running_sum += indent_seg.indent_val\n                # Do we need to add a new implicit indent?\n                if indent_seg.is_implicit:\n                    implicit_indents.append(running_sum)\n                # NOTE: We don't check for removal of implicit indents\n                # because it's unlikely that one would be opened, and then\n                # closed within the same point. That would probably be the\n                # sign of a bug in the dialect.\n            if running_sum < trough:\n                trough = running_sum\n        return IndentStats(running_sum, trough, tuple(implicit_indents))\n", "type": "function"}, {"name": "test__rules__std_LT02_LT11_union_all_in_subquery_fix", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.fix"], "code_location": {"file": "std_LT02_LT11_combo_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/rules", "start_line": 27, "end_line": 46}, "code_snippet": "def test__rules__std_LT02_LT11_union_all_in_subquery_fix():\n    \"\"\"Verify combination of rules LT02 and LT11 produces a correct indentation.\"\"\"\n    sql = (\n        \"SELECT c FROM (\\n\"\n        \"    SELECT 'g' UNION ALL\\n\"\n        \"    SELECT 'h'\\n\"\n        \"    UNION ALL SELECT 'j'\\n\"\n        \")\\n\"\n    )\n    fixed_sql = (\n        \"SELECT c FROM (\\n\"\n        \"    SELECT 'g'\\n\"\n        \"    UNION ALL\\n\"\n        \"    SELECT 'h'\\n\"\n        \"    UNION ALL\\n\"\n        \"    SELECT 'j'\\n\"\n        \")\\n\"\n    )\n    result = sqlfluff.fix(sql)\n    assert result == fixed_sql\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0735929012298584}
{"question": "What coordination mechanism between the method that scans raw SQL files for inline configuration directives and the rule reference expansion process ensures that inline file directives override both string-based rule specifications and their corresponding internal list representations?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test__process_raw_file_for_config", "is_method": false, "class_name": null, "parameters": ["raw_sql"], "calls": ["pytest.mark.parametrize", "FluffConfig", "cfg.process_raw_file_for_config", "cfg.get", "cfg.get", "cfg.get", "cfg.get", "cfg.get", "cfg.get", "cfg.get", "cfg.get", "cfg.get", "cfg.get"], "code_location": {"file": "fluffconfig_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/config", "start_line": 362, "end_line": 384}, "code_snippet": "def test__process_raw_file_for_config(raw_sql):\n    \"\"\"Test the processing of a file inline directives.\"\"\"\n    cfg = FluffConfig(config_b)\n\n    # verify initial attributes based on the preloaded configuration\n    assert cfg.get(\"max_line_length\") == 80\n    assert cfg.get(\"rules\") == \"LT03\"\n    assert cfg.get(\"exclude_rules\") is None\n\n    # internal list attributes should have corresponding exploded list values\n    assert cfg.get(\"rule_allowlist\") == [\"LT03\"]\n    assert cfg.get(\"rule_denylist\") == []\n\n    cfg.process_raw_file_for_config(raw_sql, \"test.sql\")\n\n    # verify overrides based on the file inline directives\n    assert cfg.get(\"max_line_length\") == 25\n    assert cfg.get(\"rules\") == \"LT05,LT06\"\n    assert cfg.get(\"exclude_rules\") == \"LT01,LT02\"\n\n    # internal list attributes should have overridden exploded list values\n    assert cfg.get(\"rule_allowlist\") == [\"LT05\", \"LT06\"]\n    assert cfg.get(\"rule_denylist\") == [\"LT01\", \"LT02\"]\n", "type": "function"}, {"name": "test__process_inline_config", "is_method": false, "class_name": null, "parameters": [], "calls": ["FluffConfig", "cfg.process_inline_config", "cfg.process_inline_config", "cfg.process_inline_config", "cfg.process_inline_config", "cfg.process_inline_config", "cfg.process_inline_config", "cfg.process_inline_config", "cfg.get", "cfg.get", "cfg.get", "cfg.get", "cfg.get", "cfg.get", "cfg.get", "cfg.get", "cfg.get", "cfg.get", "cfg.get", "cfg.get", "cfg.get"], "code_location": {"file": "fluffconfig_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/config", "start_line": 316, "end_line": 348}, "code_snippet": "def test__process_inline_config():\n    \"\"\"Test the processing of inline in-file configuration directives.\"\"\"\n    cfg = FluffConfig(config_b)\n    assert cfg.get(\"rules\") == \"LT03\"\n\n    cfg.process_inline_config(\"-- sqlfluff:rules:LT02\", \"test.sql\")\n    assert cfg.get(\"rules\") == \"LT02\"\n\n    assert cfg.get(\"tab_space_size\", section=\"indentation\") == 4\n    cfg.process_inline_config(\"-- sqlfluff:indentation:tab_space_size:20\", \"test.sql\")\n    assert cfg.get(\"tab_space_size\", section=\"indentation\") == 20\n\n    assert cfg.get(\"dialect\") == \"ansi\"\n    assert cfg.get(\"dialect_obj\").name == \"ansi\"\n    cfg.process_inline_config(\"-- sqlfluff:dialect:postgres\", \"test.sql\")\n    assert cfg.get(\"dialect\") == \"postgres\"\n    assert cfg.get(\"dialect_obj\").name == \"postgres\"\n\n    assert cfg.get(\"rulez\") is None\n    cfg.process_inline_config(\"-- sqlfluff:rulez:LT06\", \"test.sql\")\n    assert cfg.get(\"rulez\") == \"LT06\"\n\n    # Check that Windows paths don't get mangled\n    cfg.process_inline_config(\"-- sqlfluff:jinja:my_path:c:\\\\foo\", \"test.sql\")\n    assert cfg.get(\"my_path\", section=\"jinja\") == \"c:\\\\foo\"\n\n    # Check that JSON objects are not mangled\n    cfg.process_inline_config('-- sqlfluff:jinja:my_dict:{\"k\":\"v\"}', \"test.sql\")\n    assert cfg.get(\"my_dict\", section=\"jinja\") == '{\"k\":\"v\"}'\n\n    # Check that JSON arrays are not mangled\n    cfg.process_inline_config('-- sqlfluff:jinja:my_dict:[{\"k\":\"v\"}]', \"test.sql\")\n    assert cfg.get(\"my_dict\", section=\"jinja\") == '[{\"k\":\"v\"}]'\n", "type": "function"}, {"name": "process_raw_file_for_config", "is_method": true, "class_name": "FluffConfig", "parameters": ["self", "raw_str", "fname"], "calls": ["raw_str.splitlines", "self._handle_comma_separated_values", "raw_line.startswith", "self.process_inline_config"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 707, "end_line": 732}, "code_snippet": "    def process_raw_file_for_config(self, raw_str: str, fname: str) -> None:\n        \"\"\"Process a full raw file for inline config and update self.\n\n        Args:\n            raw_str (str): The full SQL script to evaluate for inline configs.\n            fname (str): The name of the current file being processed. This\n                is used purely for logging purposes in the case that an\n                invalid config string is provided so that any error messages\n                can reference the file with the issue.\n\n        >>> cfg = FluffConfig(overrides={\"dialect\": \"ansi\"})\n        >>> cfg.process_raw_file_for_config(\n        ...     \"-- sqlfluff:dialect:postgres\",\n        ...     \"test.sql\"\n        ... )\n        >>> cfg.get(\"dialect\")\n        'postgres'\n        \"\"\"\n        # Scan the raw file for config commands.\n        for raw_line in raw_str.splitlines():\n            # With or without a space.\n            if raw_line.startswith((\"-- sqlfluff\", \"--sqlfluff\")):\n                # Found a in-file config command\n                self.process_inline_config(raw_line, fname)\n        # Deal with potential list-like inputs.\n        self._handle_comma_separated_values()\n", "type": "function"}, {"name": "test__config__validate_configs_inline_layout", "is_method": false, "class_name": null, "parameters": ["raw_sql"], "calls": ["pytest.mark.parametrize", "FluffConfig", "pytest.raises", "cfg.process_raw_file_for_config"], "code_location": {"file": "fluffconfig_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/config", "start_line": 274, "end_line": 285}, "code_snippet": "def test__config__validate_configs_inline_layout(raw_sql):\n    \"\"\"Test _validate_configs method of FluffConfig when used on a file.\n\n    This test covers both the validation of inline config\n    directives but also the validation of layout configs.\n    \"\"\"\n    # Instantiate config object.\n    cfg = FluffConfig(configs={\"core\": {\"dialect\": \"ansi\"}})\n\n    # Try to process an invalid inline config. Make sure we get an error.\n    with pytest.raises(SQLFluffUserError):\n        cfg.process_raw_file_for_config(raw_sql, \"test.sql\")\n", "type": "function"}, {"name": "process_inline_config", "is_method": true, "class_name": "FluffConfig", "parameters": ["self", "config_line", "fname"], "calls": ["config_line.startswith", "strip", "split_colon_separated_string", "records_to_nested_dict", "validate_config_dict", "self.set_value", "strip", "config_line.startswith", "config_logger.warning", "len", "coerce_value", "list", "isinstance", "self._initialise_dialect", "iter_records_from_nested_dict"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 659, "end_line": 705}, "code_snippet": "    def process_inline_config(self, config_line: str, fname: str) -> None:\n        \"\"\"Process an inline config command and update self.\n\n        Args:\n            config_line (str): The inline config section to be processed.\n                This should usually begin with ``-- sqlfluff:``.\n            fname (str): The name of the current file being processed. This\n                is used purely for logging purposes in the case that an\n                invalid config string is provided so that any error messages\n                can reference the file with the issue.\n\n        >>> cfg = FluffConfig(overrides={\"dialect\": \"ansi\"})\n        >>> cfg.process_inline_config(\n        ...     \"-- sqlfluff:dialect:postgres\",\n        ...     \"test.sql\"\n        ... )\n        >>> cfg.get(\"dialect\")\n        'postgres'\n        \"\"\"\n        # Strip preceding comment marks\n        if config_line.startswith(\"--\"):\n            config_line = config_line[2:].strip()\n        # Strip preceding sqlfluff line.\n        if not config_line.startswith(\"sqlfluff:\"):  # pragma: no cover\n            config_logger.warning(\n                \"Unable to process inline config statement: %r\", config_line\n            )\n            return\n        config_line = config_line[9:].strip()\n        config_key, config_value = split_colon_separated_string(config_line)\n        # Move to core section if appropriate\n        if len(config_key) == 1:\n            config_key = (\"core\",) + config_key\n        # Coerce data types\n        config_record = (config_key, coerce_value(config_value))\n        # Convert to dict & validate\n        config_dict: ConfigMappingType = records_to_nested_dict([config_record])\n        validate_config_dict(config_dict, f\"inline config in {fname}\")\n        config_val = list(iter_records_from_nested_dict(config_dict))[0]\n\n        # Set the value\n        self.set_value(config_key, config_value)\n        # If the config is for dialect, initialise the dialect.\n        if config_val[0] == (\"core\", \"dialect\"):\n            dialect_value = config_val[1]\n            assert isinstance(dialect_value, str)\n            self._initialise_dialect(dialect_value)\n", "type": "function"}, {"name": "_handle_comma_separated_values", "is_method": true, "class_name": "FluffConfig", "parameters": ["self"], "calls": ["get", "split_comma_separated_string", "isinstance"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 139, "end_line": 151}, "code_snippet": "    def _handle_comma_separated_values(self) -> None:\n        for in_key, out_key in [\n            (\"ignore\", \"ignore\"),\n            (\"warnings\", \"warnings\"),\n            (\"rules\", \"rule_allowlist\"),\n            (\"exclude_rules\", \"rule_denylist\"),\n        ]:\n            in_value = self._configs[\"core\"].get(in_key, None)\n            if in_value:\n                assert not isinstance(in_value, dict)\n                self._configs[\"core\"][out_key] = split_comma_separated_string(in_value)\n            else:\n                self._configs[\"core\"][out_key] = []\n", "type": "function"}, {"name": "from_source", "is_method": true, "class_name": "IgnoreMask", "parameters": ["cls", "source", "inline_comment_regex", "reference_map"], "calls": ["enumerate", "source.split", "linter_logger.info", "cls", "inline_comment_regex.search", "cls._parse_noqa", "isinstance", "violations.append", "ignore_buff.append"], "code_location": {"file": "noqa.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 190, "end_line": 215}, "code_snippet": "    def from_source(\n        cls,\n        source: str,\n        inline_comment_regex: RegexLexer,\n        reference_map: dict[str, set[str]],\n    ) -> tuple[\"IgnoreMask\", list[SQLBaseError]]:\n        \"\"\"Look for inline ignore comments and return NoQaDirectives.\n\n        Very similar to .from_tree(), but can be run on raw source\n        (i.e. does not require the code to have parsed successfully).\n        \"\"\"\n        ignore_buff: list[NoQaDirective] = []\n        violations: list[SQLBaseError] = []\n        for idx, line in enumerate(source.split(\"\\n\")):\n            match = inline_comment_regex.search(line) if line else None\n            if match:\n                ignore_entry = cls._parse_noqa(\n                    line[match[0] : match[1]], idx + 1, match[0], reference_map\n                )\n                if isinstance(ignore_entry, SQLParseError):\n                    violations.append(ignore_entry)  # pragma: no cover\n                elif ignore_entry:\n                    ignore_buff.append(ignore_entry)\n        if ignore_buff:\n            linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n        return cls(ignore_buff), violations\n", "type": "function"}, {"name": "test__cli__command_lint_warning_name_rule", "is_method": false, "class_name": null, "parameters": [], "calls": ["CliRunner", "runner.invoke", "result.stdout.strip", "result.stdout.strip"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 896, "end_line": 920}, "code_snippet": "def test__cli__command_lint_warning_name_rule():\n    \"\"\"Test that configuring warnings works.\n\n    For this test the warnings are configured using\n    inline config in the file. That's more for simplicity\n    however the code paths should be the same if it's\n    configured in a file.\n    \"\"\"\n    runner = CliRunner()\n    result = runner.invoke(\n        lint,\n        [\n            \"test/fixtures/cli/warning_name_a.sql\",\n        ],\n    )\n    # Because we're only warning. The command should pass.\n    assert result.exit_code == 0\n    # The output should still say PASS.\n    assert \"PASS\" in result.stdout.strip()\n    # But should also contain the warnings.\n    # NOTE: Not including the whole description because it's too long.\n    assert (\n        \"L:   4 | P:   9 | LT01 | WARNING: Expected single whitespace\"\n        in result.stdout.strip()\n    )\n", "type": "function"}, {"name": "test__api__config_override", "is_method": false, "class_name": null, "parameters": ["kwargs", "expected", "tmpdir"], "calls": ["pytest.mark.parametrize", "sqlfluff.lint", "intersection", "set", "dict"], "code_location": {"file": "simple_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 643, "end_line": 650}, "code_snippet": "def test__api__config_override(kwargs, expected, tmpdir):\n    \"\"\"Test that parameters to lint() override .sqlfluff correctly (or not).\"\"\"\n    config_path = \"test/fixtures/api/config_override/.sqlfluff\"\n    sql = \"SELECT TRIM(name) AS name FROM some_table\"\n    lint_results = sqlfluff.lint(sql, config_path=config_path, **kwargs)\n    assert expected == {\"RF02\", \"RF04\"}.intersection(\n        {lr[\"code\"] for lr in lint_results}\n    )\n", "type": "function"}, {"name": "test__cli__command_lint_warning", "is_method": false, "class_name": null, "parameters": [], "calls": ["CliRunner", "runner.invoke", "result.stdout.strip", "result.stdout.strip"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 869, "end_line": 893}, "code_snippet": "def test__cli__command_lint_warning():\n    \"\"\"Test that configuring warnings works.\n\n    For this test the warnings are configured using\n    inline config in the file. That's more for simplicity\n    however the code paths should be the same if it's\n    configured in a file.\n    \"\"\"\n    runner = CliRunner()\n    result = runner.invoke(\n        lint,\n        [\n            \"test/fixtures/cli/warning_a.sql\",\n        ],\n    )\n    # Because we're only warning. The command should pass.\n    assert result.exit_code == 0\n    # The output should still say PASS.\n    assert \"PASS\" in result.stdout.strip()\n    # But should also contain the warnings.\n    # NOTE: Not including the whole description because it's too long.\n    assert (\n        \"L:   4 | P:   9 | LT01 | WARNING: Expected single whitespace\"\n        in result.stdout.strip()\n    )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0881824493408203}
{"question": "What is the architectural responsibility distribution between the static method that extracts select clause structural information as a pure data extraction layer and the methods that determine layout violations and generate fixes?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "_get_indexes", "is_method": true, "class_name": "Rule_LT09", "parameters": ["context"], "calls": ["segment.children", "children.select", "children.find", "children.select", "children.select", "get", "siblings_post.select", "SelectTargetsInfo", "sp.is_type", "select_targets.get", "sp.is_keyword", "children.find", "sp.is_type", "children.find", "children.select", "children.select", "children.find", "FunctionalContext", "sp.is_type", "list", "selects.get", "newlines.get", "sp.is_type", "sp.is_type", "segments_after_first_line.get", "first", "FunctionalContext", "selects.get", "newlines.get", "sp.or_", "children.find", "sp.is_type", "sp.is_type", "sp.is_meta", "comment_after_select.get", "siblings_post.first", "sp.is_type"], "code_location": {"file": "LT09.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 109, "end_line": 157}, "code_snippet": "    def _get_indexes(context: RuleContext) -> SelectTargetsInfo:\n        children = FunctionalContext(context).segment.children()\n        select_targets = children.select(sp.is_type(\"select_clause_element\"))\n        first_select_target_idx = children.find(select_targets.get())\n        selects = children.select(sp.is_keyword(\"select\"))\n        select_idx = children.find(selects.get()) if selects else -1\n        newlines = children.select(sp.is_type(\"newline\"))\n        first_new_line_idx = children.find(newlines.get()) if newlines else -1\n        comment_after_select_idx = -1\n        if newlines:\n            comment_after_select = children.select(\n                sp.is_type(\"comment\"),\n                start_seg=selects.get(),\n                stop_seg=newlines.get(),\n                loop_while=sp.or_(\n                    sp.is_type(\"comment\"), sp.is_type(\"whitespace\"), sp.is_meta()\n                ),\n            )\n            if comment_after_select:\n                comment_after_select_idx = (\n                    children.find(comment_after_select.get())\n                    if comment_after_select\n                    else -1\n                )\n        first_whitespace_idx = -1\n        if first_new_line_idx != -1:\n            # TRICKY: Ignore whitespace prior to the first newline, e.g. if\n            # the line with \"SELECT\" (before any select targets) has trailing\n            # whitespace.\n            segments_after_first_line = children.select(\n                sp.is_type(\"whitespace\"), start_seg=children[first_new_line_idx]\n            )\n            first_whitespace_idx = children.find(segments_after_first_line.get())\n\n        siblings_post = FunctionalContext(context).siblings_post\n        from_segment = siblings_post.first(sp.is_type(\"from_clause\")).first().get()\n        pre_from_whitespace = siblings_post.select(\n            sp.is_type(\"whitespace\"), stop_seg=from_segment\n        )\n        return SelectTargetsInfo(\n            select_idx,\n            first_new_line_idx,\n            first_select_target_idx,\n            first_whitespace_idx,\n            comment_after_select_idx,\n            select_targets,\n            from_segment,\n            list(pre_from_whitespace),\n        )\n", "type": "function"}, {"name": "_eval_single_select_target_element", "is_method": true, "class_name": "Rule_LT09", "parameters": ["self", "select_targets_info", "context"], "calls": ["select_clause.children", "is_type", "select_children.first", "LintResult", "FunctionalContext", "self.logger.info", "self.logger.info", "LintResult", "WhitespaceSegment", "initial_deletes.append", "sp.is_type", "select_children.index", "initial_deletes.append", "LintFix.replace", "is_type", "select_stmt.segments.index", "select_children.index", "modifier.get", "initial_deletes.append", "select_clause.get", "len", "next_segment.is_type", "select_clause.get", "select_clause.get", "modifier.get", "WhitespaceSegment", "len", "LintFix.delete", "select", "next_segment.is_type", "select_children.select", "is_type", "fixes.append", "fixes.append", "select_children.reversed", "sp.is_type", "fixes.append", "LintFix.delete", "fixes.append", "all_deletes.add", "LintFix.create_after", "LintFix.delete", "LintFix.delete", "list", "len", "NewlineSegment"], "code_location": {"file": "LT09.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 242, "end_line": 414}, "code_snippet": "    def _eval_single_select_target_element(\n        self, select_targets_info, context: RuleContext\n    ):\n        select_clause = FunctionalContext(context).segment\n        parent_stack = context.parent_stack\n        target_idx = select_targets_info.first_select_target_idx\n        select_children = select_clause.children()\n        target_seg = select_children[target_idx]\n\n        # If it's all on one line, then there's no issue.\n        if not (\n            select_targets_info.select_idx\n            < select_targets_info.first_new_line_idx\n            < target_idx\n        ):\n            self.logger.info(\n                \"Target at index %s is already on a single line.\",\n                target_idx,\n            )\n            return None\n\n        # Does the target contain a newline?\n        # i.e. even if it's a single element, does it already span more than\n        # one line?\n        if \"newline\" in target_seg.descendant_type_set:\n            self.logger.info(\n                \"Target at index %s spans multiple lines so ignoring.\",\n                target_idx,\n            )\n            return None\n\n        if select_targets_info.comment_after_select_idx != -1:\n            # The SELECT is followed by a comment on the same line. In order\n            # to autofix this, we'd need to move the select target between\n            # SELECT and the comment and potentially delete the entire line\n            # where the select target was (if it is now empty). This is\n            # *fairly tricky and complex*, in part because the newline on\n            # the select target's line is several levels higher in the\n            # parser tree. Hence, we currently don't autofix this. Could be\n            # autofixed in the future if/when we have the time.\n            return LintResult(anchor=select_clause.get())\n\n        # Prepare the select clause which will be inserted\n        insert_buff = [WhitespaceSegment(), target_seg]\n        # Delete the first select target from its original location.\n        # We'll add it to the right section at the end, once we know\n        # what to add.\n        initial_deletes = [target_seg]\n        # If there's whitespace before it, delete that too.\n        if select_children[target_idx - 1].is_type(\"whitespace\"):\n            initial_deletes.append(select_children[target_idx - 1])\n\n        # Do we have a modifier?\n        modifier: Optional[Segments]\n        modifier = select_children.first(sp.is_type(\"select_clause_modifier\"))\n\n        if (\n            # Check if the modifier is one we care about\n            modifier\n            # We only care if it's not already on the first line.\n            and select_children.index(modifier.get())\n            >= select_targets_info.first_new_line_idx\n        ):\n            # Prepend it to the insert buffer\n            insert_buff = [WhitespaceSegment(), modifier[0]] + insert_buff\n\n            modifier_idx = select_children.index(modifier.get())\n            # Delete the whitespace after it (which is two after, thanks to indent)\n            if (\n                len(select_children) > modifier_idx + 1\n                and select_children[modifier_idx + 2].is_whitespace\n            ):\n                initial_deletes.append(select_children[modifier_idx + 2])\n\n            # Delete the modifier itself\n            initial_deletes.append(modifier[0])\n\n            # Set the position marker for removing the preceding\n            # whitespace and newline, which we'll use below.\n            start_idx = modifier_idx\n            start_seg = modifier[0]\n        else:\n            # Set the position marker for removing the preceding\n            # whitespace and newline, which we'll use below.\n            start_idx = target_idx\n            start_seg = select_children[select_targets_info.first_new_line_idx]\n\n        fixes = [\n            # Insert the select_clause in place of the first newline in the\n            # Select statement\n            LintFix.replace(\n                select_children[select_targets_info.first_new_line_idx],\n                insert_buff,\n            ),\n            # Materialise any deletes so far...\n            *(LintFix.delete(seg) for seg in initial_deletes),\n        ]\n\n        if parent_stack and parent_stack[-1].is_type(\"select_statement\"):\n            select_stmt = parent_stack[-1]\n            select_clause_idx = select_stmt.segments.index(select_clause.get())\n            after_select_clause_idx = select_clause_idx + 1\n\n            if len(select_stmt.segments) > after_select_clause_idx:\n                add_newline = True\n                to_delete: Sequence[BaseSegment] = [target_seg]\n                next_segment = select_stmt.segments[after_select_clause_idx]\n\n                if next_segment.is_type(\"newline\"):\n                    # Since we're deleting the newline, we should also delete all\n                    # whitespace before it or it will add random whitespace to\n                    # following statements. So walk back through the segment\n                    # deleting whitespace until you get the previous newline, or\n                    # something else.\n                    to_delete = select_children.reversed().select(\n                        loop_while=sp.is_type(\"whitespace\"),\n                        start_seg=select_children[start_idx],\n                    )\n                    if to_delete:\n                        # The select_clause is immediately followed by a\n                        # newline. Delete the newline in order to avoid leaving\n                        # behind an empty line after fix, *unless* we stopped\n                        # due to something other than a newline.\n                        delete_last_newline = select_children[\n                            start_idx - len(to_delete) - 1\n                        ].is_type(\"newline\")\n\n                        # Delete the newline if we decided to.\n                        if delete_last_newline:\n                            fixes.append(LintFix.delete(next_segment))\n\n                elif next_segment.is_type(\"whitespace\"):\n                    # The select_clause has stuff after (most likely a comment)\n                    # Delete the whitespace immediately after the select clause\n                    # so the other stuff aligns nicely based on where the select\n                    # clause started.\n                    fixes.append(LintFix.delete(next_segment))\n\n                if to_delete:\n                    # Clean up by moving leftover select_clause segments.\n\n                    # Context: Some of the other fixes we make in\n                    # _eval_single_select_target_element() leave leftover\n                    # child segments that need to be moved to become\n                    # *siblings* of the select_clause.\n                    move_after_select_clause = select_children.select(\n                        start_seg=start_seg,\n                        stop_seg=to_delete[-1],\n                    )\n                    # :TRICKY: Below, we have a couple places where we\n                    # filter to guard against deleting the same segment\n                    # multiple times -- this is illegal.\n                    all_deletes = {\n                        fix.anchor for fix in fixes if fix.edit_type == \"delete\"\n                    }\n                    for seg in (*to_delete, *move_after_select_clause):\n                        if seg not in all_deletes:\n                            fixes.append(LintFix.delete(seg))\n                            all_deletes.add(seg)\n\n                    if move_after_select_clause or add_newline:\n                        fixes.append(\n                            LintFix.create_after(\n                                select_clause[0],\n                                ([NewlineSegment()] if add_newline else [])\n                                + list(move_after_select_clause),\n                            )\n                        )\n\n        return LintResult(\n            anchor=select_clause.get(),\n            fixes=fixes,\n        )\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_LT10", "parameters": ["self", "context"], "calls": ["context.segment.is_type", "segment.children", "child_segments.first", "child_segments.select", "child_segments.select", "child_segments.select", "fixes.append", "fixes.append", "child_segments.select", "LintResult", "sp.is_type", "WhitespaceSegment", "edit_segments.append", "LintFix.create_after", "fixes.extend", "fixes.extend", "LintFix.delete", "fixes.extend", "sp.is_type", "sp.or_", "sp.is_type", "sp.or_", "sp.is_type", "sp.or_", "NewlineSegment", "sp.is_whitespace", "sp.or_", "FunctionalContext", "sp.is_whitespace", "sp.is_meta", "sp.is_whitespace", "sp.is_meta", "sp.is_whitespace", "sp.is_meta", "LintFix.delete", "LintFix.delete", "sp.is_type", "sp.is_meta", "LintFix.delete"], "code_location": {"file": "LT10.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 41, "end_line": 139}, "code_snippet": "    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Select clause modifiers must appear on same line as SELECT.\"\"\"\n        # We only care about select_clause.\n        assert context.segment.is_type(\"select_clause\")\n\n        # Get children of select_clause and the corresponding select keyword.\n        child_segments = FunctionalContext(context).segment.children()\n        select_keyword = child_segments[0]\n\n        # See if we have a select_clause_modifier.\n        select_clause_modifier_seg = child_segments.first(\n            sp.is_type(\"select_clause_modifier\")\n        )\n\n        # Rule doesn't apply if there's no select clause modifier.\n        if not select_clause_modifier_seg:\n            return None\n\n        select_clause_modifier = select_clause_modifier_seg[0]\n\n        # Are there any newlines between the select keyword\n        # and the select clause modifier.\n        leading_newline_segments = child_segments.select(\n            select_if=sp.is_type(\"newline\"),\n            loop_while=sp.or_(sp.is_whitespace(), sp.is_meta()),\n            start_seg=select_keyword,\n        )\n\n        # Rule doesn't apply if select clause modifier\n        # is already on the same line as the select keyword.\n        if not leading_newline_segments:\n            return None\n\n        # We should check if there is whitespace before the select clause modifier\n        # and remove this during the lint fix.\n        leading_whitespace_segments = child_segments.select(\n            select_if=sp.is_type(\"whitespace\"),\n            loop_while=sp.or_(sp.is_whitespace(), sp.is_meta()),\n            start_seg=select_keyword,\n        )\n\n        # We should also check if the following select clause element\n        # is on the same line as the select clause modifier.\n        trailing_newline_segments = child_segments.select(\n            select_if=sp.is_type(\"newline\"),\n            loop_while=sp.or_(sp.is_whitespace(), sp.is_meta()),\n            start_seg=select_clause_modifier,\n        )\n\n        # We will insert these segments directly after the select keyword.\n        edit_segments = [\n            WhitespaceSegment(),\n            select_clause_modifier,\n        ]\n        if not trailing_newline_segments:\n            # if the first select clause element is on the same line\n            # as the select clause modifier then also insert a newline.\n            edit_segments.append(NewlineSegment())\n\n        fixes = []\n        # Move select clause modifier after select keyword.\n        fixes.append(\n            LintFix.create_after(\n                anchor_segment=select_keyword,\n                edit_segments=edit_segments,\n            )\n        )\n\n        # Delete original newlines and whitespace between select keyword\n        # and select clause modifier.\n\n        # If there is not a newline after the select clause modifier then delete\n        # newlines between the select keyword and select clause modifier.\n        if not trailing_newline_segments:\n            fixes.extend(LintFix.delete(s) for s in leading_newline_segments)\n        # If there is a newline after the select clause modifier then delete both the\n        # newlines and whitespace between the select keyword and select clause modifier.\n        else:\n            fixes.extend(\n                LintFix.delete(s)\n                for s in leading_newline_segments + leading_whitespace_segments\n            )\n        # Delete the original select clause modifier.\n        fixes.append(LintFix.delete(select_clause_modifier))\n\n        # If there is whitespace (on the same line) after the select clause modifier\n        # then also delete this.\n        trailing_whitespace_segments = child_segments.select(\n            select_if=sp.is_whitespace(),\n            loop_while=sp.or_(sp.is_type(\"whitespace\"), sp.is_meta()),\n            start_seg=select_clause_modifier,\n        )\n        if trailing_whitespace_segments:\n            fixes.extend(LintFix.delete(s) for s in trailing_whitespace_segments)\n\n        return LintResult(\n            anchor=context.segment,\n            fixes=fixes,\n        )\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_ST06", "parameters": ["self", "context"], "calls": ["context.segment.is_type", "reversed", "reversed", "context.segment.get_children", "seg.is_type", "seg.is_type", "enumerate", "LintResult", "seg.get_child", "seg.get_parent", "with_compound_statement.recursive_crawl", "append", "len", "any", "LintResult", "LintFix.replace", "self._implicit_column_references", "zip", "with_compound_statement.path_to", "any", "isinstance", "segment.get_child", "self._validate", "any", "isinstance", "path_step.segment.is_type", "segment.get_child", "_function.get_child", "isinstance", "path_step.segment.is_type", "self._validate", "segment.get_child", "_expression.get_child", "self._validate", "len", "len"], "code_location": {"file": "ST06.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 64, "end_line": 232}, "code_snippet": "    def _eval(self, context: RuleContext) -> EvalResultType:\n        self.violation_exists = False\n        # Bands of select targets in order to be enforced\n        select_element_order_preference: tuple[\n            tuple[Union[str, tuple[str, ...]], ...], ...\n        ] = (\n            (\"wildcard_expression\",),\n            (\n                \"object_reference\",\n                \"literal\",\n                \"cast_expression\",\n                (\"function\", \"cast\"),\n                (\"expression\", \"cast_expression\"),\n            ),\n        )\n\n        # Track which bands have been seen, with additional empty list for the\n        # non-matching elements. If we find a matching target element, we append the\n        # element to the corresponding index.\n        self.seen_band_elements: list[list[BaseSegment]] = [\n            [] for _ in select_element_order_preference\n        ] + [\n            []\n        ]  # type: ignore\n\n        assert context.segment.is_type(\"select_clause\")\n\n        # insert, merge, create table, union are order-sensitive\n        for seg in reversed(context.parent_stack):\n            if seg.is_type(\n                \"insert_statement\",\n                \"set_expression\",\n                \"create_table_statement\",\n                \"merge_statement\",\n            ):\n                return None\n\n        # CTE is order-sensitive only if CTE is referenced as SELECT * in set expression\n        for seg in reversed(context.parent_stack):\n            if seg.is_type(\"common_table_expression\"):\n                cte_identifier = seg.get_child(\"identifier\")\n                assert cte_identifier is not None\n                maybe_with_compound_statement = seg.get_parent()\n                if maybe_with_compound_statement is None:\n                    break  # pragma: no cover\n                with_compound_statement, _ = maybe_with_compound_statement\n                for ref in with_compound_statement.recursive_crawl(\"table_reference\"):\n                    if ref.raw_upper == cte_identifier.raw_upper:\n                        path = with_compound_statement.path_to(ref)\n                        if any(\n                            path_step.segment.is_type(\"set_expression\")\n                            for path_step in path\n                        ):\n                            select_statements = [\n                                path_step.segment\n                                for path_step in path\n                                if path_step.segment.is_type(\n                                    \"select_statement\",\n                                    \"unordered_select_statement_segment\",\n                                )\n                            ]\n                            if any(\n                                \"wildcard_expression\"\n                                in select_statement.descendant_type_set\n                                for select_statement in select_statements\n                            ):\n                                return None\n\n        select_clause_segment = context.segment\n        select_target_elements = context.segment.get_children(\"select_clause_element\")\n        if not select_target_elements:\n            return None\n\n        # Iterate through all the select targets to find any order violations\n        for segment in select_target_elements:\n            # The band index of the current segment in\n            # select_element_order_preference\n            self.current_element_band = None\n\n            # Compare the segment to the bands in select_element_order_preference\n            for i, band in enumerate(select_element_order_preference):\n                for e in band:\n                    # Identify simple select target\n                    if isinstance(e, str) and segment.get_child(e):\n                        self._validate(i, segment)\n\n                    # Identify function\n                    elif isinstance(e, tuple) and e[0] == \"function\":\n                        try:\n                            _function = segment.get_child(\"function\")\n                            assert _function\n                            _function_name = _function.get_child(\"function_name\")\n                            assert _function_name\n                            if _function_name.raw == e[1]:\n                                self._validate(i, segment)\n                        except (AttributeError, AssertionError):\n                            # If the segment doesn't match\n                            pass\n\n                    # Identify simple expression\n                    elif isinstance(e, tuple) and e[0] == \"expression\":\n                        try:\n                            _expression = segment.get_child(\"expression\")\n                            assert _expression\n\n                            if (\n                                _expression.get_child(e[1])\n                                and _expression.segments[0].type\n                                in (\n                                    \"column_reference\",\n                                    \"object_reference\",\n                                    \"literal\",\n                                    \"cast_expression\",\n                                )\n                                # len == 2 to ensure the expression is 'simple'\n                                and (\n                                    len(_expression.segments) == 2\n                                    # cast_expression is one length\n                                    or len(_expression.segments) == 1\n                                )\n                            ):\n                                self._validate(i, segment)\n                        except (AttributeError, AssertionError):\n                            # If the segment doesn't match\n                            pass\n\n            # If the target doesn't exist in select_element_order_preference then it\n            # is 'complex' and must go last\n            if self.current_element_band is None:\n                self.seen_band_elements[-1].append(segment)\n\n        if self.violation_exists:\n            if len(context.parent_stack) and any(\n                self._implicit_column_references(context.parent_stack[-1])\n            ):\n                # If there are implicit column references (i.e. column\n                # numbers), warn but don't fix, because it's much more\n                # complicated to autofix.\n                return LintResult(anchor=select_clause_segment)\n            # Create a list of all the edit fixes\n            # We have to do this at the end of iterating through all the\n            # select_target_elements to get the order correct. This means we can't\n            # add a lint fix to each individual LintResult as we go\n            ordered_select_target_elements = [\n                segment for band in self.seen_band_elements for segment in band\n            ]\n            # TODO: The \"if\" in the loop below compares corresponding items\n            # to avoid creating \"do-nothing\" edits. A potentially better\n            # approach would leverage difflib.SequenceMatcher.get_opcodes(),\n            # which generates a list of edit actions (similar to the\n            # command-line \"diff\" tool in Linux). This is more complex to\n            # implement, but minimizing the number of LintFixes makes the\n            # final application of patches (in \"sqlfluff fix\") more robust.\n            fixes = [\n                LintFix.replace(\n                    initial_select_target_element,\n                    [replace_select_target_element],\n                )\n                for initial_select_target_element, replace_select_target_element in zip(  # noqa: E501\n                    select_target_elements, ordered_select_target_elements\n                )\n                if initial_select_target_element is not replace_select_target_element\n            ]\n            # Anchoring on the select statement segment ensures that\n            # select statements which include macro targets are ignored\n            # when ignore_templated_areas is set\n            return LintResult(anchor=select_clause_segment, fixes=fixes)\n\n        return None\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_AL09", "parameters": ["self", "context"], "calls": ["context.segment.is_type", "segment.children", "children.select", "sp.is_type", "clause_element.get_raw_segments", "clause_element.get_child", "clause_element.get_child", "column.get_children", "clause_element.get_child", "alias_expression.get_child", "getattr", "alias_expression.get_child", "fixes.append", "violations.append", "FunctionalContext", "self.logger.warning", "fixes.append", "LintFix.delete", "LintResult", "column_identifier.is_type", "alias_identifier.is_type", "violations.append", "LintFix.delete", "LintResult"], "code_location": {"file": "AL09.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/aliasing", "start_line": 94, "end_line": 215}, "code_snippet": "    def _eval(self, context: RuleContext) -> EvalResultType:\n        \"\"\"Find self-aliased columns and fix them.\n\n        Checks the alias in the `SELECT` clause and see if the\n        alias identifier is same as the column identifier (self-alias).\n\n        If the column is self-aliased, then the `AS` keyword,\n        whitespaces and alias identifier is removed as part of the fix.\n        For example: `col_a as col_a,` is fixed to `col_a,`\n        \"\"\"\n        assert context.segment.is_type(\"select_clause\")\n\n        violations = []\n\n        children: Segments = FunctionalContext(context).segment.children()\n\n        for clause_element in children.select(sp.is_type(\"select_clause_element\")):\n            clause_element_raw_segments = (\n                clause_element.get_raw_segments()\n            )  # col_a as col_a\n\n            column = clause_element.get_child(\"column_reference\")  # `col_a`\n            alias_expression = clause_element.get_child(\n                \"alias_expression\"\n            )  # `as col_a`\n\n            # We're only interested in direct aliasing of columns (i.e. not\n            # and expression), so if that isn't the case, move on.\n            if not (alias_expression and column):\n                continue\n\n            # The column needs to be a naked_identifier or quoted_identifier\n            # (not positional identifier like $n in snowflake).\n            # Move on if not. Some column references have multiple elements\n            # (e.g. my_table.my_column), so only fetch the last available.\n            _column_elements = column.get_children(\n                \"naked_identifier\", \"quoted_identifier\"\n            )\n            if not _column_elements:  # pragma: no cover\n                continue\n            column_identifier = _column_elements[-1]\n\n            # Fetch the whitespace between the reference and the alias.\n            whitespace = clause_element.get_child(\"whitespace\")  # ` `\n\n            # The alias can be the naked_identifier or the quoted_identifier\n            alias_identifier = alias_expression.get_child(\n                \"naked_identifier\", \"quoted_identifier\"\n            )\n\n            # if we do not have an alias identifier we can continue\n            if not alias_identifier:  # pragma: no cover\n                continue\n\n            alias_keyword_raw = getattr(\n                alias_expression.get_child(\"alias_operator\"), \"raw\", None\n            )\n            # If the alias keyword is '=', then no whitespace have to be present\n            # between the alias_keyword and the alias_identifier\n            if alias_keyword_raw != \"=\":\n                if not (whitespace and alias_identifier):  # pragma: no cover\n                    # We *should* expect all of these to be non-null, but some bug\n                    # reports suggest that that isn't always the case for some\n                    # dialects. In those cases, log a warning here, but don't\n                    # flag it as a linting issue. Hopefully this will help\n                    # better bug reports in future.\n                    self.logger.warning(\n                        \"AL09 found an unexpected syntax in an alias expression. \"\n                        \"Unable to determine if this is a self-alias. Please \"\n                        \"report this as a bug on GitHub.\\n\\n\"\n                        f\"Debug details: dialect: {context.dialect.name}, \"\n                        f\"whitespace: {whitespace is not None}, \"\n                        f\"alias_identifier: {alias_identifier is not None}, \"\n                        f\"alias_expression: {clause_element.raw!r}.\"\n                    )\n                    continue\n\n            case_sensitive_dialects = [\"clickhouse\"]\n\n            # We compare the _exact_ raw value of the column identifier\n            # and the alias identifier (i.e. including quoting and casing).\n            # Resolving aliases & references with differing quoting and casing\n            # should be done in conjunction with RF06 & CP02 (see docstring).\n            if column_identifier.raw == alias_identifier.raw:\n                fixes: list[LintFix] = []\n\n                if whitespace is not None:\n                    fixes.append(LintFix.delete(whitespace))\n                fixes.append(LintFix.delete(alias_expression))\n\n                violations.append(\n                    LintResult(\n                        anchor=clause_element_raw_segments[0],\n                        description=\"Column should not be self-aliased.\",\n                        fixes=fixes,\n                    )\n                )\n            # If *both* are unquoted, and we're in a dialect which isn't case\n            # sensitive for unquoted identifiers, then flag an error but don't\n            # suggest a fix. It's ambiguous about what the users intent was:\n            # i.e. did they mean to change the case (and so the correct\n            # resolution is quoting), or did they mistakenly add an unnecessary\n            # alias?\n            elif (\n                context.dialect.name not in case_sensitive_dialects\n                and column_identifier.is_type(\"naked_identifier\")\n                and alias_identifier is not None\n                and alias_identifier.is_type(\"naked_identifier\")\n                and column_identifier.raw_upper == alias_identifier.raw_upper\n            ):\n                violations.append(\n                    LintResult(\n                        anchor=clause_element_raw_segments[0],\n                        description=(\n                            \"Ambiguous self alias. Either remove unnecessary \"\n                            \"alias, or quote alias/reference to make case \"\n                            \"change explicit.\"\n                        ),\n                    )\n                )\n\n        return violations or None\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_ST09", "parameters": ["self", "context"], "calls": ["context.segment.is_type", "segment.children", "children.recursive_crawl", "recursive_crawl", "next", "table_aliases.append", "recursive_crawl", "len", "get_eventual_alias", "from_expression_alias_info.segment.raw_normalized", "alias.upper", "children", "conditions.append", "subconditions.append", "comparison_operator.get_children", "first_column_reference.get_child", "second_column_reference.get_child", "upper", "upper", "LintResult", "join_clauses.children", "alias_info.segment.raw_normalized", "join_on_conditions.children", "self._split_list_by_segment_type", "self._is_qualified_column_operator_qualified_column_sequence", "FunctionalContext", "cast", "get_eventual_aliases", "Segments", "expression_group.append", "first_table_seg.raw_normalized", "second_table_seg.raw_normalized", "first_column_reference.pos_marker.is_literal", "table_aliases.index", "table_aliases.index", "table_aliases.index", "table_aliases.index", "children.recursive_crawl", "cast", "LintFix.replace", "LintFix.replace", "LintFix.replace", "SymbolSegment"], "code_location": {"file": "ST09.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 76, "end_line": 283}, "code_snippet": "    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Find rule violations and provide fixes.\n\n        0. Grab all table aliases into a table_aliases list.\n        1. Grab all conditions from the different join_on_condition segments.\n        2. Break conditions down into subconditions using the \"and\" and \"or\"\n        binary operators.\n        3. Keep subconditions that are made up of a qualified column_reference,\n        a comparison_operator and another qualified column_reference segments.\n        4. Check whether the table associated with the first column_reference segment\n        has a greater index in table_aliases than the second column_reference segment.\n        If so, populate the fixes list (lower index instead of greater index\n        if preferred_first_table_in_join_clause == \"later\").\n        5.a. If fixes is empty the rule passes.\n        5.b. If fixes isn't empty we return a LintResult object with fixable violations.\n        \"\"\"\n        self.preferred_first_table_in_join_clause: str\n\n        assert context.segment.is_type(\"from_expression\")\n\n        # STEP 0.\n        table_aliases: list[str] = []\n\n        children = FunctionalContext(context).segment.children()\n\n        # we use recursive_crawl to deal with brackets\n        join_clauses = children.recursive_crawl(\"join_clause\")\n\n        join_on_conditions = join_clauses.children().recursive_crawl(\n            \"join_on_condition\"\n        )\n\n        # we only care about join_on_condition segments\n        if len(join_on_conditions) == 0:\n            return None\n\n        # the first alias comes from the from clause\n        from_expression_alias_info = next(\n            cast(\n                FromExpressionElementSegment,\n                children.recursive_crawl(\"from_expression_element\")[0],\n            ).get_eventual_alias()\n        )\n        from_expression_alias: str = (\n            from_expression_alias_info.segment.raw_normalized(False)\n            if from_expression_alias_info.segment\n            else from_expression_alias_info.ref_str\n        )\n\n        table_aliases.append(from_expression_alias)\n\n        # the rest of the aliases come from the different join clauses\n        join_clause_alias_infos: list[AliasInfo] = [\n            cast(JoinClauseSegment, join_clause).get_eventual_aliases()[0][1]\n            for join_clause in [clause for clause in join_clauses]\n        ]\n\n        join_clause_aliases = [\n            (\n                alias_info.segment.raw_normalized(False)\n                if alias_info.segment\n                else alias_info.ref_str\n            )\n            for alias_info in join_clause_alias_infos\n        ]\n\n        table_aliases += join_clause_aliases\n\n        table_aliases = [alias.upper() for alias in table_aliases]\n\n        # STEP 1.\n        conditions: list[list[BaseSegment]] = []\n\n        join_on_condition__expressions = join_on_conditions.children().recursive_crawl(\n            \"expression\"\n        )\n\n        for expression in join_on_condition__expressions:\n            expression_group = []\n            for element in Segments(expression).children():\n                if element.type not in (\"whitespace\", \"newline\"):\n                    expression_group.append(element)\n            conditions.append(expression_group)\n\n        # STEP 2.\n        subconditions: list[list[list[BaseSegment]]] = []\n\n        for expression_group in conditions:\n            subconditions.append(\n                self._split_list_by_segment_type(\n                    segment_list=expression_group,\n                    delimiter_type=\"binary_operator\",\n                    delimiters=[\"and\", \"or\"],\n                )\n            )\n\n        subconditions_flattened: list[list[BaseSegment]] = [\n            item for sublist in subconditions for item in sublist\n        ]\n\n        # STEP 3.\n        column_operator_column_subconditions: list[list[BaseSegment]] = [\n            subcondition\n            for subcondition in subconditions_flattened\n            if self._is_qualified_column_operator_qualified_column_sequence(\n                subcondition\n            )\n        ]\n\n        # STEP 4.\n        fixes: list[LintFix] = []\n        anchor_segment = context.segment  # Default anchor\n\n        for subcondition in column_operator_column_subconditions:\n            comparison_operator = subcondition[1]\n            first_column_reference = subcondition[0]\n            second_column_reference = subcondition[2]\n            raw_comparison_operators = comparison_operator.get_children(\n                \"raw_comparison_operator\"\n            )\n\n            first_table_seg = first_column_reference.get_child(\n                \"naked_identifier\", \"quoted_identifier\"\n            )\n            second_table_seg = second_column_reference.get_child(\n                \"naked_identifier\", \"quoted_identifier\"\n            )\n            assert first_table_seg and second_table_seg\n            first_table = first_table_seg.raw_normalized(False).upper()\n            second_table = second_table_seg.raw_normalized(False).upper()\n\n            # if we swap the two column references around the comparison operator\n            # we might have to replace the comparison operator with a different one\n            raw_comparison_operator_opposites = {\"<\": \">\", \">\": \"<\"}\n\n            # there seem to be edge cases where either the first table or the second\n            # table is not in table_aliases, in which case we cannot provide any fix\n            if first_table not in table_aliases or second_table not in table_aliases:\n                continue\n\n            if (\n                table_aliases.index(first_table) > table_aliases.index(second_table)\n                and self.preferred_first_table_in_join_clause == \"earlier\"\n            ) or (\n                table_aliases.index(first_table) < table_aliases.index(second_table)\n                and self.preferred_first_table_in_join_clause == \"later\"\n            ):\n                # Use the first column reference as anchor if it has a literal\n                # position marker. This ensures the violation is anchored to\n                # a literal segment which won't be filtered out in templated\n                # code.\n                if (\n                    not fixes\n                    and first_column_reference.pos_marker\n                    and first_column_reference.pos_marker.is_literal()\n                ):\n                    anchor_segment = first_column_reference\n\n                fixes = (\n                    fixes\n                    + [\n                        LintFix.replace(\n                            first_column_reference,\n                            [second_column_reference],\n                        )\n                    ]\n                    + [\n                        LintFix.replace(\n                            second_column_reference,\n                            [first_column_reference],\n                        )\n                    ]\n                    + (\n                        [\n                            LintFix.replace(\n                                raw_comparison_operators[0],\n                                [\n                                    SymbolSegment(\n                                        raw=raw_comparison_operator_opposites[\n                                            raw_comparison_operators[0].raw\n                                        ],\n                                        type=\"raw_comparison_operator\",\n                                    )\n                                ],\n                            )\n                        ]\n                        if raw_comparison_operators\n                        and raw_comparison_operators[0].raw\n                        in raw_comparison_operator_opposites\n                        and [r.raw for r in raw_comparison_operators] != [\"<\", \">\"]\n                        else []\n                    )\n                )\n\n        # STEP 5.a.\n        if not fixes:\n            return None\n\n        # STEP 5.b.\n        else:\n            return LintResult(\n                anchor=anchor_segment,\n                fixes=fixes,\n                description=(\n                    \"Joins should list the table referenced \"\n                    f\"{self.preferred_first_table_in_join_clause} first.\"\n                ),\n            )\n", "type": "function"}, {"name": "_eval_gen", "is_method": true, "class_name": "Rule_CV12", "parameters": ["self", "context"], "calls": ["select_statement.is_type", "select_statement.get_child", "self._is_where_clause_simplifable", "set", "select_statement.recursive_crawl", "collections.deque", "enumerate", "where_clause.get_child", "self._get_subexpression_chunks", "self._get_from_expression_element_alias", "next", "encountered_references.add", "any", "join_clause.get_child", "is_type", "where_clause_fix_segments.popleft", "is_type", "where_clause_fix_segments.pop", "where_clause.get_child", "is_type", "is_type", "select_statement.recursive_crawl", "join_clause.recursive_crawl", "self._get_from_expression_element_alias", "set", "enumerate", "where_clause_fix_segments.extend", "where_clause_fix_segments.append", "LintResult", "LintResult", "LintResult", "collections.deque", "enumerate", "ExpressionSegment", "JoinOnConditionSegment", "JoinClauseSegment", "BinaryOperatorSegment", "all", "this_join_clause_subexpressions.add", "consumed_subexpressions.add", "LintResult", "is_type", "join_clause_fix_segments.popleft", "is_type", "join_clause_fix_segments.pop", "tuple", "LintResult", "seg.recursive_crawl", "len", "join_clause_fix_segments.extend", "join_clause_fix_segments.append", "KeywordSegment", "WhitespaceSegment", "WhitespaceSegment", "LintFix.replace", "LintFix.delete", "LintFix.delete", "col_ref.raw_upper.startswith", "BinaryOperatorSegment", "tuple", "LintFix.replace"], "code_location": {"file": "CV12.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "start_line": 67, "end_line": 250}, "code_snippet": "    def _eval_gen(self, context: RuleContext) -> Iterator[LintResult]:\n        # We are only interested in SELECT statement.\n        select_statement = context.segment\n        assert select_statement.is_type(\"select_statement\")\n\n        maybe_where_clause = select_statement.get_child(\"where_clause\")\n        if not maybe_where_clause:\n            return\n\n        where_clause = maybe_where_clause\n        where_clause_simplifable = self._is_where_clause_simplifable(where_clause)\n\n        if where_clause_simplifable:\n            expr = where_clause.get_child(\"expression\")\n            assert expr is not None\n            subexpressions = self._get_subexpression_chunks(expr)\n        else:\n            subexpressions = []\n        consumed_subexpressions = set()\n\n        # get references in from clause\n        select_table_references = [\n            *select_statement.recursive_crawl(\n                \"from_expression_element\",\n                no_recursive_seg_type=[\"join_clause\", \"select_statement\"],\n            )\n        ]\n\n        # track all seen references (from clause + all previous joins)\n        encountered_references = {\n            self._get_from_expression_element_alias(table_ref)\n            for table_ref in select_table_references\n        }\n\n        for join_clause in select_statement.recursive_crawl(\n            \"join_clause\", no_recursive_seg_type=[\"select_statement\"]\n        ):\n            # mark table reference as seen\n            join_table_reference = next(\n                join_clause.recursive_crawl(\n                    \"from_expression_element\",\n                    no_recursive_seg_type=[\"select_statement\"],\n                )\n            )\n            encountered_references.add(\n                self._get_from_expression_element_alias(join_table_reference)\n            )\n            join_clause_keywords = [\n                seg for seg in join_clause.segments if seg.type == \"keyword\"\n            ]\n\n            if any(\n                kw.raw_upper in (\"CROSS\", \"POSITIONAL\", \"USING\", \"APPLY\")\n                for kw in join_clause_keywords\n            ):\n                # If explicit CROSS JOIN is used, disregard lack of condition\n                # If explicit POSITIONAL JOIN is used, disregard lack of condition\n                # If explicit JOIN USING is used, disregard lack of condition\n                # If explicit CROSS/OUTER APPLY is used, disregard lack of condition\n                continue\n\n            this_join_condition = join_clause.get_child(\"join_on_condition\")\n            if this_join_condition:\n                # Join condition is present, no error reported.\n                continue\n\n            if not where_clause_simplifable:\n                yield LintResult(anchor=join_clause)\n            else:\n                this_join_clause_subexpressions = set()\n                for subexpr_idx, subexpr_segments in enumerate(subexpressions):\n                    if subexpr_idx in consumed_subexpressions:\n                        continue\n                    qualified_column_references = [\n                        col_ref\n                        for seg in subexpr_segments\n                        for col_ref in seg.recursive_crawl(\n                            \"column_reference\",\n                            no_recursive_seg_type=\"select_statement\",\n                        )\n                        if \"dot\" in col_ref.descendant_type_set\n                    ]\n                    if len(qualified_column_references) > 1 and all(\n                        col_ref.raw_upper.startswith(\n                            tuple(\n                                f\"{table_ref}.\" for table_ref in encountered_references\n                            )\n                        )\n                        for col_ref in qualified_column_references\n                    ):\n                        this_join_clause_subexpressions.add(subexpr_idx)\n                        consumed_subexpressions.add(subexpr_idx)\n\n                if not this_join_clause_subexpressions:\n                    yield LintResult(join_clause)\n                else:\n                    join_clause_fix_segments: Deque[BaseSegment] = collections.deque()\n                    for subexpr_idx, subexpr_segments in enumerate(subexpressions):\n                        if subexpr_idx in this_join_clause_subexpressions:\n                            join_clause_fix_segments.extend(subexpr_segments)\n                            join_clause_fix_segments.append(\n                                BinaryOperatorSegment(\"AND\")\n                            )\n\n                    while join_clause_fix_segments and join_clause_fix_segments[\n                        0\n                    ].is_type(\"whitespace\", \"binary_operator\"):\n                        join_clause_fix_segments.popleft()\n                    while join_clause_fix_segments and join_clause_fix_segments[\n                        -1\n                    ].is_type(\"whitespace\", \"binary_operator\"):\n                        join_clause_fix_segments.pop()\n\n                    join_on_expression = ExpressionSegment(\n                        tuple(join_clause_fix_segments),\n                    )\n                    join_on = JoinOnConditionSegment(\n                        (\n                            KeywordSegment(\"ON\"),\n                            WhitespaceSegment(),\n                            join_on_expression,\n                        )\n                    )\n                    join_clause_segment = JoinClauseSegment(\n                        (\n                            *join_clause.segments,\n                            WhitespaceSegment(),\n                            join_on,\n                        )\n                    )\n\n                    yield LintResult(\n                        anchor=join_clause,\n                        fixes=[\n                            LintFix.replace(\n                                join_clause,\n                                edit_segments=[join_clause_segment],\n                            )\n                        ],\n                    )\n\n        if not where_clause_simplifable:\n            return\n\n        if not consumed_subexpressions:\n            return\n\n        # Rewrite WHERE to keep conditions not moved to ON clauses\n        where_clause_fix_segments: Deque[BaseSegment] = collections.deque()\n        for subexpr_idx, subexpr_segments in enumerate(subexpressions):\n            if subexpr_idx not in consumed_subexpressions:\n                where_clause_fix_segments.extend(subexpr_segments)\n                where_clause_fix_segments.append(BinaryOperatorSegment(\"AND\"))\n\n        while where_clause_fix_segments and where_clause_fix_segments[0].is_type(\n            \"whitespace\", \"binary_operator\"\n        ):\n            where_clause_fix_segments.popleft()\n        while where_clause_fix_segments and where_clause_fix_segments[-1].is_type(\n            \"whitespace\", \"binary_operator\"\n        ):\n            where_clause_fix_segments.pop()\n\n        if where_clause_fix_segments:\n            where_clause_expr = where_clause.get_child(\"expression\")\n            assert where_clause_expr is not None\n            yield LintResult(\n                anchor=where_clause_expr,\n                fixes=[\n                    LintFix.replace(\n                        where_clause_expr, edit_segments=[*where_clause_fix_segments]\n                    )\n                ],\n            )\n        else:\n            assert select_statement.segments[-1].is_type(\"where_clause\")\n            assert select_statement.segments[-2].is_type(\"whitespace\", \"newline\")\n            yield LintResult(\n                anchor=where_clause,\n                fixes=[\n                    LintFix.delete(select_statement.segments[-2]),\n                    LintFix.delete(select_statement.segments[-1]),\n                ],\n            )\n", "type": "function"}, {"name": "_eval_multiple_select_target_elements", "is_method": true, "class_name": "Rule_LT09", "parameters": ["self", "select_targets_info", "segment"], "calls": ["enumerate", "Segments", "get", "get", "self.logger.debug", "LintResult", "segment.get_child", "segment.select_children", "fixes.append", "raw_segments.first", "last", "segment.segments.index", "LintFix.delete", "LintFix.create_before", "fixes.extend", "fixes.append", "sp.is_code", "len", "LintFix.create_before", "select_clause_raws.select", "s.is_type", "NewlineSegment", "LintFix.delete", "Segments", "s.is_type", "NewlineSegment", "sp.and_", "sp.is_code", "sp.not_", "sp.raw_is"], "code_location": {"file": "LT09.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 159, "end_line": 240}, "code_snippet": "    def _eval_multiple_select_target_elements(\n        self, select_targets_info, segment\n    ) -> Optional[LintResult]:\n        \"\"\"Multiple select targets. Ensure each is on a separate line.\"\"\"\n        fixes = []\n        previous_code = None\n        select_clause_raws = Segments(segment).raw_segments\n        for i, select_target in enumerate(select_targets_info.select_targets):\n            assert select_target.pos_marker\n            target_start_line = select_target.pos_marker.working_line_no\n            target_initial_code = (\n                Segments(select_target).raw_segments.first(sp.is_code()).get()\n            )\n            assert target_initial_code\n            previous_code = (\n                select_clause_raws.select(\n                    # Get the first code that isn't a comma.\n                    select_if=sp.and_(sp.is_code(), sp.not_(sp.raw_is(\",\"))),\n                    start_seg=previous_code,\n                    stop_seg=target_initial_code,\n                )\n                .last()\n                .get()\n            )\n            assert previous_code\n            assert previous_code.pos_marker\n            previous_end_line = previous_code.pos_marker.working_line_no\n            self.logger.debug(\n                \"- Evaluating %s [%s, %s]: Prev ends with: %s\",\n                select_target,\n                previous_end_line,\n                target_start_line,\n                previous_code,\n            )\n\n            # Check whether this target *starts* on the same line that the\n            # previous one *ends* on. If they are on the same line, insert a newline.\n            if target_start_line == previous_end_line:\n                # Find and delete any whitespace before the select target.\n                start_seg = select_targets_info.select_idx\n                # If any select modifier (e.g. distinct ) is present, start\n                # there rather than at the beginning.\n                modifier = segment.get_child(\"select_clause_modifier\")\n                if modifier:\n                    start_seg = segment.segments.index(modifier)\n\n                ws_to_delete = segment.select_children(\n                    start_seg=(\n                        segment.segments[start_seg]\n                        if not i\n                        else select_targets_info.select_targets[i - 1]\n                    ),\n                    select_if=lambda s: s.is_type(\"whitespace\"),\n                    loop_while=lambda s: s.is_type(\"whitespace\", \"comma\") or s.is_meta,\n                )\n                fixes += [LintFix.delete(ws) for ws in ws_to_delete]\n                fixes.append(LintFix.create_before(select_target, [NewlineSegment()]))\n\n            # If we are at the last select target check if the FROM clause\n            # is on the same line, and if so move it to its own line.\n            if select_targets_info.from_segment:\n                if (i + 1 == len(select_targets_info.select_targets)) and (\n                    select_target.pos_marker.working_line_no\n                    == select_targets_info.from_segment.pos_marker.working_line_no\n                ):\n                    fixes.extend(\n                        [\n                            LintFix.delete(ws)\n                            for ws in select_targets_info.pre_from_whitespace\n                        ]\n                    )\n                    fixes.append(\n                        LintFix.create_before(\n                            select_targets_info.from_segment,\n                            [NewlineSegment()],\n                        )\n                    )\n\n        if fixes:\n            return LintResult(anchor=segment, fixes=fixes)\n\n        return None\n", "type": "function"}, {"name": "get_select_statement_info", "is_method": false, "class_name": null, "parameters": ["segment", "dialect", "early_exit"], "calls": ["segment.is_type", "get_aliases_from_select", "segment.get_child", "_get_object_references", "segment.get_child", "cast", "segment.get_child", "SelectStatementColumnsAndTables", "segment.get_child", "_select_clause.get_children", "s.get_alias", "fc.recursive_crawl", "fc.recursive_crawl", "_get_object_references", "table_expression.iter_segments", "join_clause.iter_segments", "seg.is_type", "_get_object_references", "is_qualified", "seg.is_type", "seg.is_type", "_get_object_references", "cast", "on_seg.is_type", "seg.is_type", "_get_object_references", "subseg.is_type", "using_cols.append"], "code_location": {"file": "select.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/analysis", "start_line": 39, "end_line": 128}, "code_snippet": "def get_select_statement_info(\n    segment: BaseSegment, dialect: Optional[Dialect], early_exit: bool = True\n) -> Optional[SelectStatementColumnsAndTables]:\n    \"\"\"Analyze a select statement: targets, aliases, etc. Return info.\"\"\"\n    assert segment.is_type(\"select_statement\")\n    table_aliases, standalone_aliases = get_aliases_from_select(segment, dialect)\n    if early_exit and not table_aliases and not standalone_aliases:\n        return None\n\n    # Iterate through all the references, both in the select clause, but also\n    # potential others.\n    sc = segment.get_child(\"select_clause\")\n    # Sometimes there is no select clause (e.g. \"SELECT *\" is a select_clause_element)\n    if not sc:  # pragma: no cover\n        # TODO: Review whether this clause should be removed. It might only\n        # have existed for an old way of structuring the Exasol dialect.\n        return None\n    # NOTE: In this first crawl, don't crawl inside any sub-selects, that's very\n    # important for both isolation and performance reasons.\n    reference_buffer = _get_object_references(sc)\n    table_reference_buffer = []\n    for potential_clause in (\n        \"where_clause\",\n        \"groupby_clause\",\n        \"having_clause\",\n        \"orderby_clause\",\n        \"qualify_clause\",\n    ):\n        clause = segment.get_child(potential_clause)\n        if clause:\n            reference_buffer += _get_object_references(clause)\n\n    # Get all select targets.\n    _select_clause = segment.get_child(\"select_clause\")\n    assert _select_clause, \"Select statement found without select clause.\"\n    select_targets = cast(\n        list[SelectClauseElementSegment],\n        _select_clause.get_children(\"select_clause_element\"),\n    )\n\n    # Get all column aliases. NOTE: In two steps so mypy can follow.\n    _pre_aliases = [s.get_alias() for s in select_targets]\n    col_aliases = [_alias for _alias in _pre_aliases if _alias is not None]\n\n    # Get any columns referred to in a using clause, and extract anything\n    # from ON clauses.\n    using_cols = []\n    fc = segment.get_child(\"from_clause\")\n    if fc:\n        for table_expression in fc.recursive_crawl(\n            \"table_expression\", no_recursive_seg_type=\"select_statement\"\n        ):\n            for seg in table_expression.iter_segments():\n                # table references can get tricky with what is a schema, table,\n                # project, or column. It may be best for now to use the redshift\n                # unnest logic for dialects that support arrays or objects/structs\n                # in AL05. However, this solves finding other types of references\n                # in functions such as LATERAL FLATTEN.\n                if not seg.is_type(\"table_reference\"):\n                    reference_buffer += _get_object_references(seg)\n                elif cast(ObjectReferenceSegment, seg).is_qualified():\n                    table_reference_buffer += _get_object_references(seg)\n        for join_clause in fc.recursive_crawl(\n            \"join_clause\", no_recursive_seg_type=\"select_statement\"\n        ):\n            seen_using = False\n            for seg in join_clause.iter_segments():\n                if seg.is_type(\"keyword\") and seg.raw_upper == \"USING\":\n                    seen_using = True\n                elif seg.is_type(\"join_on_condition\"):\n                    for on_seg in seg.segments:\n                        if on_seg.is_type(\"bracketed\", \"expression\"):\n                            # Deal with expressions\n                            reference_buffer += _get_object_references(seg)\n                elif seen_using and seg.is_type(\"bracketed\"):\n                    for subseg in seg.segments:\n                        if subseg.is_type(\"identifier\"):\n                            using_cols.append(subseg)\n                    seen_using = False\n\n    return SelectStatementColumnsAndTables(\n        select_statement=segment,\n        table_aliases=table_aliases or [],\n        standalone_aliases=standalone_aliases or [],\n        reference_buffer=reference_buffer,\n        select_targets=select_targets,\n        col_aliases=col_aliases,\n        using_cols=using_cols,\n        table_reference_buffer=table_reference_buffer,\n    )\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_LT09", "parameters": ["self", "context"], "calls": ["context.segment.is_type", "self._get_indexes", "children", "bool", "FunctionalContext", "sp.is_type", "self._eval_single_select_target_element", "len", "select_clause.children", "len", "self._eval_multiple_select_target_elements", "sp.is_type"], "code_location": {"file": "LT09.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 86, "end_line": 106}, "code_snippet": "    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        self.wildcard_policy: str\n        assert context.segment.is_type(\"select_clause\")\n        select_targets_info = self._get_indexes(context)\n        select_clause = FunctionalContext(context).segment\n        wildcards = select_clause.children(\n            sp.is_type(\"select_clause_element\")\n        ).children(sp.is_type(\"wildcard_expression\"))\n        has_wildcard = bool(wildcards)\n        if len(select_targets_info.select_targets) == 1 and (\n            not has_wildcard or self.wildcard_policy == \"single\"\n        ):\n            return self._eval_single_select_target_element(\n                select_targets_info,\n                context,\n            )\n        elif len(select_targets_info.select_targets):\n            return self._eval_multiple_select_target_elements(\n                select_targets_info, context.segment\n            )\n        return None\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.084418535232544}
{"question": "What method in the filtering logic that extracts aliases from SELECT FROM clauses differentiates between regular table reference aliases and column-returning function aliases?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "get_aliases_from_select", "is_method": false, "class_name": null, "parameters": ["segment", "dialect"], "calls": ["segment.get_child", "isinstance", "fc.get_eventual_aliases", "_get_pivot_table_aliases", "_get_lambda_argument_columns", "_has_value_table_function", "standalone_aliases.append", "table_aliases.append"], "code_location": {"file": "select.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/analysis", "start_line": 131, "end_line": 161}, "code_snippet": "def get_aliases_from_select(\n    segment: BaseSegment, dialect: Optional[Dialect] = None\n) -> tuple[Optional[list[AliasInfo]], Optional[list[BaseSegment]]]:\n    \"\"\"Gets the aliases referred to in the FROM clause.\n\n    Returns a tuple of two lists:\n    - Table aliases\n    - Value table function aliases\n    \"\"\"\n    fc = segment.get_child(\"from_clause\")\n    if not fc:\n        # If there's no from clause then just abort.\n        return None, None\n    assert isinstance(fc, (FromClauseSegment, JoinClauseSegment))\n    aliases = fc.get_eventual_aliases()\n\n    # We only want table aliases, so filter out aliases for value table\n    # functions, lambda parameters and pivot columns.\n    standalone_aliases: list[BaseSegment] = []\n    standalone_aliases += _get_pivot_table_aliases(segment, dialect)\n    standalone_aliases += _get_lambda_argument_columns(segment, dialect)\n\n    table_aliases = []\n    for table_expr, alias_info in aliases:\n        if _has_value_table_function(table_expr, dialect):\n            if alias_info.segment and alias_info.segment not in standalone_aliases:\n                standalone_aliases.append(alias_info.segment)\n        elif alias_info not in table_aliases:\n            table_aliases.append(alias_info)\n\n    return table_aliases, standalone_aliases\n", "type": "function"}, {"name": "_filter_table_expressions", "is_method": true, "class_name": "Rule_AL07", "parameters": ["cls", "base_table", "from_expression_elements"], "calls": ["from_expression.get_child", "table_expression.get_child", "from_expression.get_child", "from_expression.get_child", "alias_exp_ref.get_child", "TableAliasInfo"], "code_location": {"file": "AL07.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/aliasing", "start_line": 154, "end_line": 187}, "code_snippet": "    def _filter_table_expressions(\n        cls, base_table, from_expression_elements\n    ) -> Generator[TableAliasInfo, None, None]:\n        for from_expression in from_expression_elements:\n            table_expression = from_expression.get_child(\"table_expression\")\n            if not table_expression:\n                continue  # pragma: no cover\n            table_ref = table_expression.get_child(\"object_reference\")\n\n            # If the from_expression_element has no object_references - skip it\n            # An example case is a lateral flatten, where we have a function segment\n            # instead of a table_reference segment.\n            if not table_ref:\n                continue\n\n            # If this is self-join - skip it\n            if (\n                base_table\n                and base_table.raw == table_ref.raw\n                and base_table != table_ref\n            ):\n                continue\n\n            whitespace_ref = from_expression.get_child(\"whitespace\")\n\n            # If there's no alias expression - skip it\n            alias_exp_ref = from_expression.get_child(\"alias_expression\")\n            if alias_exp_ref is None:\n                continue\n\n            alias_identifier_ref = alias_exp_ref.get_child(\"identifier\")\n            yield TableAliasInfo(\n                table_ref, whitespace_ref, alias_exp_ref, alias_identifier_ref\n            )\n", "type": "function"}, {"name": "_followed_by_qualify", "is_method": true, "class_name": "Rule_AL05", "parameters": ["self", "context", "alias"], "calls": ["alias.alias_expression.get_end_loc", "seg.get_end_loc", "seg.is_type"], "code_location": {"file": "AL05.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/aliasing", "start_line": 198, "end_line": 210}, "code_snippet": "    def _followed_by_qualify(self, context: RuleContext, alias: AliasInfo) -> bool:\n        curr_from_seen = False\n        assert alias.alias_expression\n        for seg in context.segment.segments:\n            if alias.alias_expression.get_end_loc() == seg.get_end_loc():\n                curr_from_seen = True\n            elif curr_from_seen and not seg.is_code:\n                continue\n            elif curr_from_seen and seg.is_type(\"qualify_clause\"):\n                return True\n            elif curr_from_seen:\n                return False\n        return False\n", "type": "function"}, {"name": "_is_aliased_select_clause_element", "is_method": true, "class_name": "Rule_RF05", "parameters": ["context"], "calls": ["reversed", "seg.is_type", "seg.is_type", "seg.get_child"], "code_location": {"file": "RF05.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 240, "end_line": 247}, "code_snippet": "    def _is_aliased_select_clause_element(context: RuleContext) -> bool:\n        for seg in reversed(context.parent_stack):\n            if seg.is_type(\"alias_expression\"):\n                return False\n            if seg.is_type(\"select_clause_element\"):\n                return seg.get_child(\"alias_expression\") is not None\n\n        return False\n", "type": "function"}, {"name": "_get_from_expression_element_alias", "is_method": true, "class_name": "Rule_CV12", "parameters": ["from_expr_element"], "calls": ["from_expr_element.get_child", "alias_seg.get_child"], "code_location": {"file": "CV12.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "start_line": 253, "end_line": 263}, "code_snippet": "    def _get_from_expression_element_alias(from_expr_element: BaseSegment) -> str:\n        if \"alias_expression\" in from_expr_element.direct_descendant_type_set:\n            alias_seg = from_expr_element.get_child(\"alias_expression\")\n            assert alias_seg is not None\n            identifier_seg = alias_seg.get_child(\"identifier\")\n            assert identifier_seg is not None\n            alias_str = identifier_seg.raw_upper\n        else:\n            alias_str = from_expr_element.raw_upper\n\n        return alias_str\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_AL06", "parameters": ["self", "context"], "calls": ["context.segment.is_type", "segment.children", "children.recursive_crawl", "self._lint_aliases", "FunctionalContext"], "code_location": {"file": "AL06.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/aliasing", "start_line": 52, "end_line": 65}, "code_snippet": "    def _eval(self, context: RuleContext) -> Optional[list[LintResult]]:\n        \"\"\"Identify aliases in from clause and join conditions.\n\n        Find base table, table expressions in join, and other expressions in select\n        clause and decide if it's needed to report them.\n        \"\"\"\n        self.min_alias_length: Optional[int]\n        self.max_alias_length: Optional[int]\n\n        assert context.segment.is_type(\"select_statement\")\n        children = FunctionalContext(context).segment.children()\n        from_expression_elements = children.recursive_crawl(\"from_expression_element\")\n\n        return self._lint_aliases(from_expression_elements) or None\n", "type": "function"}, {"name": "_extract_references_from_expression", "is_method": true, "class_name": "Rule_ST11", "parameters": ["self", "segment"], "calls": ["segment.is_type", "segment.get_child", "segment.recursive_crawl", "alias_expression.get_child", "upper", "upper", "raw_normalized", "alias_identifier.raw_normalized"], "code_location": {"file": "ST11.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 89, "end_line": 108}, "code_snippet": "    def _extract_references_from_expression(self, segment: BaseSegment) -> str:\n        assert segment.is_type(\"from_expression_element\")\n        # If there's an alias, we care more about that.\n        alias_expression = segment.get_child(\"alias_expression\")\n        if alias_expression:\n            alias_identifier = alias_expression.get_child(\"identifier\")\n            if alias_identifier:\n                # Append the raw representation and the from expression.\n                return alias_identifier.raw_normalized(casefold=False).upper()\n        # Otherwise if no alias, we need the name of the object we're\n        # referencing.\n        for table_reference in segment.recursive_crawl(\n            \"table_reference\", no_recursive_seg_type=\"select_statement\"\n        ):\n            return table_reference.segments[-1].raw_normalized(casefold=False).upper()\n        # If we can't find a reference, just return an empty string\n        # to signal that there isn't one. This could be a case of a\n        # VALUES clause, or anything else selectable which hasn't\n        # been given an alias.\n        return \"\"\n", "type": "function"}, {"name": "_get_pivot_table_aliases", "is_method": false, "class_name": null, "parameters": ["segment", "dialect"], "calls": ["segment.recursive_crawl", "fc.recursive_crawl", "pivot_table_aliases.append"], "code_location": {"file": "select.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/analysis", "start_line": 182, "end_line": 198}, "code_snippet": "def _get_pivot_table_aliases(\n    segment: BaseSegment, dialect: Optional[Dialect]\n) -> list[BaseSegment]:\n    if not dialect:\n        # We need the dialect to get the pivot table column names. If\n        # we don't have it, assume the clause does not have a pivot table\n        return []  # pragma: no cover\n\n    pivot_table_aliases: list[BaseSegment] = []\n    for fc in segment.recursive_crawl(\"from_pivot_expression\"):\n        for pivot_table_alias in fc.recursive_crawl(\n            \"pivot_column_reference\", \"table_reference\"\n        ):\n            if pivot_table_alias.raw not in [a.raw for a in pivot_table_aliases]:\n                pivot_table_aliases.append(pivot_table_alias)\n\n    return pivot_table_aliases\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_AL07", "parameters": ["self", "context"], "calls": ["context.segment.is_type", "segment.children", "first", "first", "children.select", "clause.recursive_crawl", "clause.recursive_crawl", "self._lint_aliases_in_join", "children.select", "children", "from_expression_elements.append", "column_reference_segments.append", "FunctionalContext", "sp.is_type", "sp.is_type", "first", "children", "sp.is_type", "first", "children", "sp.is_type", "first", "from_clause_segment.children", "sp.is_type"], "code_location": {"file": "AL07.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/aliasing", "start_line": 92, "end_line": 151}, "code_snippet": "    def _eval(self, context: RuleContext) -> Optional[list[LintResult]]:\n        \"\"\"Identify aliases in from clause and join conditions.\n\n        Find base table, table expressions in join, and other expressions in select\n        clause and decide if it's needed to report them.\n        \"\"\"\n        # Config type hints\n        self.force_enable: bool\n\n        # Issue 2810: BigQuery has some tricky expectations (apparently not\n        # documented, but subject to change, e.g.:\n        # https://www.reddit.com/r/bigquery/comments/fgk31y/new_in_bigquery_no_more_backticks_around_table/)\n        # about whether backticks are required (and whether the query is valid\n        # or not, even with them), depending on whether the GCP project name is\n        # present, or just the dataset name. Since SQLFluff doesn't have access\n        # to BigQuery when it is looking at the query, it would be complex for\n        # this rule to do the right thing. For now, the rule simply disables\n        # itself.\n        if not self.force_enable:\n            return None\n\n        assert context.segment.is_type(\"select_statement\")\n\n        children = FunctionalContext(context).segment.children()\n        from_clause_segment = children.select(sp.is_type(\"from_clause\")).first()\n        base_table = (\n            from_clause_segment.children(sp.is_type(\"from_expression\"))\n            .first()\n            .children(sp.is_type(\"from_expression_element\"))\n            .first()\n            .children(sp.is_type(\"table_expression\"))\n            .first()\n            .children(sp.is_type(\"object_reference\"))\n            .first()\n        )\n        if not base_table:\n            return None\n\n        # A buffer for all table expressions in join conditions\n        from_expression_elements = []\n        column_reference_segments = []\n\n        after_from_clause = children.select(start_seg=from_clause_segment[0])\n        for clause in from_clause_segment + after_from_clause:\n            for from_expression_element in clause.recursive_crawl(\n                \"from_expression_element\"\n            ):\n                from_expression_elements.append(from_expression_element)\n            for column_reference in clause.recursive_crawl(\"column_reference\"):\n                column_reference_segments.append(column_reference)\n\n        return (\n            self._lint_aliases_in_join(\n                base_table[0] if base_table else None,\n                from_expression_elements,\n                column_reference_segments,\n                context.segment,\n            )\n            or None\n        )\n", "type": "function"}, {"name": "TableAliasInfo", "docstring": "Structure yielded by_filter_table_expressions().", "methods": [], "attributes": [], "code_location": {"file": "AL07.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/aliasing", "start_line": 13, "end_line": 19}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.0910401344299316}
{"question": "How does the SELECT REPLACE clause segment's parsing grammar handle failures when the delimiter-based parser encounters malformed expression-alias pairs that violate expression or identifier grammar constraints?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "ReplaceClauseSegment", "docstring": "SELECT REPLACE clause.", "methods": [], "attributes": ["type", "match_grammar", "type", "match_grammar"], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1331, "end_line": 1346}, "type": "class"}, {"name": "ReplaceClauseSegment", "docstring": "A snowflake SELECT REPLACE clause.\n\nhttps://docs.snowflake.com/en/sql-reference/sql/select.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_snowflake.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1976, "end_line": 1994}, "type": "class"}, {"name": "AliasExpressionSegment", "docstring": "Modified to allow UDTF in SELECT clause to return multiple columns aliases.\n\nFull Apache Hive `Built-in Table-Generating Functions (UDTF)` reference here:\nhttps://cwiki.apache.org/confluence/display/hive/languagemanual+udf#LanguageManualUDF-Built-inTable-GeneratingFunctions(UDTF)", "methods": [], "attributes": [], "code_location": {"file": "dialect_hive.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 469, "end_line": 487}, "type": "class"}, {"name": "WildcardReplaceExpressionSegment", "docstring": "A `REPLACE` clause within a wildcard expression.", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_duckdb.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 404, "end_line": 424}, "type": "class"}, {"name": "SelectClauseSegment", "docstring": "Overriding SelectClauseSegment to allow for additional segment parsing.", "methods": [], "attributes": [], "code_location": {"file": "dialect_hive.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1004, "end_line": 1014}, "type": "class"}, {"name": "SelectClauseElementSegment", "docstring": "An element in the targets of a select statement.\n\nOverriding ANSI to remove greedy logic which assumes statements have been\ndelimited", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 798, "end_line": 817}, "type": "class"}, {"name": "SelectClauseSegment", "docstring": "A group of elements in a select target statement.\n\nOverriding ANSI to remove greedy logic which assumes statements have been\ndelimited", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 861, "end_line": 877}, "type": "class"}, {"name": "RejectClauseSegment", "docstring": "`REJECT` clause within an import / export statement.", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_exasol.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2039, "end_line": 2051}, "type": "class"}, {"name": "WildcardRenameExpressionSegment", "docstring": "A `RENAME` clause within a wildcard expression.", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_duckdb.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 427, "end_line": 447}, "type": "class"}, {"name": "SelectClauseSegment", "docstring": "A group of elements in a select target statement.\n\nRemove OVERLAPS as a terminator as this can be part of SelectClauseModifierSegment", "methods": [], "attributes": [], "code_location": {"file": "dialect_teradata.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 875, "end_line": 894}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.0807864665985107}
{"question": "How does the DROP VIEW statement grammar's pattern matching attribute handle optional IF EXISTS and drop behavior clauses across SQL dialects?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "DropViewStatementSegment", "docstring": "A `DROP VIEW` statement.\n\nOverriding ANSI to add optional delimiter.", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 4634, "end_line": 4644}, "type": "class"}, {"name": "DropViewStatementSegment", "docstring": "A `DROP VIEW` statement with CASCADE and RESTRICT option.\n\nhttps://docs.exasol.com/sql/drop_view.htm", "methods": [], "attributes": [], "code_location": {"file": "dialect_exasol.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 811, "end_line": 830}, "type": "class"}, {"name": "DropViewStatementSegment", "docstring": "A `DROP VIEW` statement.\n\nhttps://www.postgresql.org/docs/15/sql-dropview.html\nhttps://github.com/postgres/postgres/blob/4380c2509d51febad34e1fac0cfaeb98aaa716c5/src/backend/parser/gram.y#L6698-L6719", "methods": [], "attributes": [], "code_location": {"file": "dialect_postgres.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3389, "end_line": 3402}, "type": "class"}, {"name": "DropViewStatementSegment", "docstring": "A `DROP VIEW` statement.\n\nAs specified in\nhttps://clickhouse.com/docs/en/sql-reference/statements/drop/", "methods": [], "attributes": ["type", "match_grammar", "type", "is_ddl", "is_dml", "is_dql", "is_dcl", "match_grammar", "type", "match_grammar"], "code_location": {"file": "dialect_clickhouse.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1482, "end_line": 1498}, "type": "class"}, {"name": "DropViewStatementSegment", "docstring": "A `DROP VIEW` statement.", "methods": [], "attributes": [], "code_location": {"file": "dialect_ansi.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3511, "end_line": 3522}, "type": "class"}, {"name": "DropTableStatementSegment", "docstring": "A `DROP TABLE` statement.\n\nDoris-specific version that supports:\n- IF EXISTS clause\n- Database-qualified table names\n- FORCE option", "methods": [], "attributes": ["type", "match_grammar", "type", "match_grammar", "type", "match_grammar", "type", "is_ddl", "is_dml", "is_dql", "is_dcl", "match_grammar", "type", "type", "match_grammar"], "code_location": {"file": "dialect_doris.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 424, "end_line": 440}, "type": "class"}, {"name": "CreateViewStatementSegment", "docstring": "A `CREATE VIEW` statement.\n\nAdjusted to allow CREATE OR ALTER instead of CREATE OR REPLACE.\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-view-transact-sql#examples\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/alter-view-transact-sql#examples", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3033, "end_line": 3061}, "type": "class"}, {"name": "DropMaterializedViewStatementSegment", "docstring": "A `DROP MATERIALIZED VIEW` statement.\n\nAs specified in https://www.postgresql.org/docs/14/sql-dropmaterializedview.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_postgres.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3218, "end_line": 3233}, "type": "class"}, {"name": "DropMaterializedViewStatementSegment", "docstring": "A `DROP MATERIALIZED VIEW` statement.\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_materialized_view_statement", "methods": [], "attributes": ["type", "match_grammar", "type", "match_grammar", "type", "match_grammar"], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2447, "end_line": 2461}, "type": "class"}, {"name": "DropMaterializedViewStatementSegment", "docstring": "A snowflake `DROP MATERIALIZED VIEW ...` statement.\n\nhttps://docs.snowflake.com/en/sql-reference/sql/drop-materialized-view.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_snowflake.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 8826, "end_line": 8839}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.0915477275848389}
{"question": "How should the function that dynamically imports user-provided Python modules from configuration in the Jinja templater be redesigned to enforce security boundaries and prevent arbitrary code execution?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "JinjaTemplater", "docstring": "A templater using the jinja2 library.\n\nSee: https://jinja.palletsprojects.com/", "methods": ["_extract_macros_from_template", "_extract_macros_from_path", "_extract_macros_from_config", "_extract_libraries_from_config", "_crawl_tree", "_get_jinja_env", "_get_macros_path", "_get_loader_search_path", "_get_jinja_analyzer", "_apply_dbt_builtins", "_get_env_context", "construct_render_func", "_generate_violations_for_undefined_variables", "_init_undefined_tracking", "process", "slice_file", "_rectify_templated_slices", "_calculate_variant_score", "_handle_unreached_code", "process_with_variants", "_exclude_macros"], "attributes": ["name"], "code_location": {"file": "jinja.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 95, "end_line": 1079}, "type": "class"}, {"name": "_get_jinja_env", "is_method": true, "class_name": "JinjaTemplater", "parameters": ["self", "config"], "calls": ["self._get_macros_path", "self._get_loader_search_path", "self._apply_dbt_builtins", "SandboxedEnvironment", "SafeFileSystemLoader", "extensions.append", "config.get", "FileSystemLoader", "TemplateNotFound", "isinstance", "get_source", "str", "templater_logger.debug", "os.path.splitext", "super", "os.path.basename", "str"], "code_location": {"file": "jinja.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 337, "end_line": 397}, "code_snippet": "    def _get_jinja_env(self, config: Optional[FluffConfig] = None) -> Environment:\n        \"\"\"Get a properly configured jinja environment.\n\n        This method returns a properly configured jinja environment. It\n        first checks if the 'ignore' key is present in the config dictionary and\n        if it contains the value 'templating'. If so, it creates a subclass of\n        FileSystemLoader called SafeFileSystemLoader that overrides the\n        get_source method to handle missing templates when templating is ignored.\n        If 'ignore' is not present or does not contain 'templating', it uses the\n        regular FileSystemLoader. It then sets the extensions to ['jinja2.ext.do']\n        and adds the DBTTestExtension if the _apply_dbt_builtins method returns\n        True. Finally, it returns a SandboxedEnvironment object with the\n        specified settings.\n\n        Args:\n            config (dict, optional): A dictionary containing configuration settings.\n\n        Returns:\n            jinja2.Environment: A properly configured jinja environment.\n        \"\"\"\n        loader: Optional[FileSystemLoader]\n        macros_path = self._get_macros_path(config, \"load_macros_from_path\")\n        loader_search_path = self._get_loader_search_path(config)\n        final_search_path = (loader_search_path or []) + (macros_path or [])\n\n        ignore_templating = config and \"templating\" in config.get(\"ignore\")\n        if ignore_templating:\n\n            class SafeFileSystemLoader(FileSystemLoader):\n                def get_source(\n                    self, environment: Environment, name: str\n                ) -> tuple[str, str, Callable[..., Any]]:\n                    try:\n                        if not isinstance(name, DummyUndefined):\n                            return super().get_source(environment, name)\n                        raise TemplateNotFound(str(name))\n                    except TemplateNotFound:\n                        # When ignore=templating is set, treat missing files\n                        # or attempts to load an \"Undefined\" file as the first\n                        # 'base' part of the name / filename rather than failing.\n                        templater_logger.debug(\n                            \"Providing dummy contents for Jinja macro file: %s\", name\n                        )\n                        value = os.path.splitext(os.path.basename(str(name)))[0]\n                        return value, f\"{value}.sql\", lambda: False\n\n            loader = SafeFileSystemLoader(final_search_path or [])\n        else:\n            loader = FileSystemLoader(final_search_path) if final_search_path else None\n        extensions: list[Union[str, type[Extension]]] = [\"jinja2.ext.do\"]\n        if self._apply_dbt_builtins(config):\n            extensions.append(DBTTestExtension)\n\n        return SandboxedEnvironment(\n            # We explicitly want to preserve newlines.\n            keep_trailing_newline=True,\n            # The do extension allows the \"do\" directive\n            autoescape=False,\n            extensions=extensions,\n            loader=loader,\n        )\n", "type": "function"}, {"name": "_extract_libraries_from_config", "is_method": true, "class_name": "JinjaTemplater", "parameters": ["self", "config"], "calls": ["JinjaTemplater.Libraries", "os.path.exists", "os.path.basename", "pkgutil.walk_packages", "config.get", "config.get_section", "os.path.join", "os.path.join", "module_finder.find_spec", "importlib.util.module_from_spec", "spec.loader.exec_module", "getattr", "module_name.split", "reduce", "setattr", "setattr", "libraries.__dict__.items", "module_name.startswith", "k.startswith", "getattr"], "code_location": {"file": "jinja.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 242, "end_line": 312}, "code_snippet": "    def _extract_libraries_from_config(self, config: FluffConfig) -> dict[str, Any]:\n        \"\"\"Extracts libraries from the given configuration.\n\n        This function iterates over the modules in the library path and\n        imports them dynamically. The imported modules are then added to a 'Libraries'\n        object, which is returned as a dictionary excluding magic methods.\n\n        Args:\n            config: The configuration object.\n\n        Returns:\n            dict: A dictionary containing the extracted libraries.\n        \"\"\"\n        # If a more global library_path is set, let that take precedence.\n        library_path = config.get(\"library_path\") or config.get_section(\n            (self.templater_selector, self.name, \"library_path\")\n        )\n        if not library_path:\n            return {}\n\n        libraries = JinjaTemplater.Libraries()\n\n        # If library_path has __init__.py we parse it as one module, else we parse it\n        # a set of modules\n        is_library_module = os.path.exists(os.path.join(library_path, \"__init__.py\"))\n        library_module_name = os.path.basename(library_path)\n\n        # Need to go one level up to parse as a module correctly\n        walk_path = (\n            os.path.join(library_path, \"..\") if is_library_module else library_path\n        )\n\n        for module_finder, module_name, _ in pkgutil.walk_packages([walk_path]):\n            # skip other modules that can be near module_dir\n            if is_library_module and not module_name.startswith(library_module_name):\n                continue\n\n            # import_module is deprecated as of python 3.4. This follows roughly\n            # the guidance of the python docs:\n            # https://docs.python.org/3/library/importlib.html#approximating-importlib-import-module\n            spec = module_finder.find_spec(module_name, None)\n            assert (\n                spec\n            ), f\"Module {module_name} failed to be found despite being listed.\"\n            module = importlib.util.module_from_spec(spec)\n            sys.modules[module_name] = module\n            assert spec.loader, f\"Module {module_name} missing expected loader.\"\n            spec.loader.exec_module(module)\n\n            if \".\" in module_name:  # nested modules have `.` in module_name\n                *module_path, last_module_name = module_name.split(\".\")\n                # find parent module recursively\n                parent_module = reduce(\n                    lambda res, path_part: getattr(res, path_part),\n                    module_path,\n                    libraries,\n                )\n\n                # set attribute on module object to make jinja working correctly\n                setattr(parent_module, last_module_name, module)\n            else:\n                # set attr on `libraries` obj to make it work in jinja nicely\n                setattr(libraries, module_name, module)\n\n        if is_library_module:\n            # when library is module we have one more root module in hierarchy and we\n            # remove it\n            libraries = getattr(libraries, library_module_name)\n\n        # remove magic methods from result\n        return {k: v for k, v in libraries.__dict__.items() if not k.startswith(\"__\")}\n", "type": "function"}, {"name": "_get_env_context", "is_method": true, "class_name": "JinjaTemplater", "parameters": ["self", "fname", "config", "env"], "calls": ["self.get_context", "self._extract_libraries_from_config", "live_context.update", "libraries.get", "self._apply_dbt_builtins", "self._get_macros_path", "self._get_macros_path", "live_context.update", "env.filters.update", "live_context.update", "self._extract_macros_from_config", "self._extract_macros_from_path"], "code_location": {"file": "jinja.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 496, "end_line": 558}, "code_snippet": "    def _get_env_context(\n        self,\n        fname: Optional[str],\n        config: Optional[FluffConfig],\n        env: Environment,\n    ) -> dict[str, Any]:\n        \"\"\"Get the templating context from the config.\n\n        NOTE: This closely mirrors the `get_context` method which we inherit from the\n        python templater, but extends the signature. For that reason we define a new\n        method here, which internally refers to `get_context`.\n\n        Args:\n            fname (str, optional): The name of the file.\n            config (dict, optional): The configuration.\n            env: The Jinja Environment.\n\n        Returns:\n            dict: The templating context.\n        \"\"\"\n        # Load the context\n        live_context = self.get_context(fname, config)\n        # Apply dbt builtin functions if we're allowed.\n        if config:\n            # first make libraries available in the context\n            # so they can be used by the macros too\n            libraries = self._extract_libraries_from_config(config=config)\n            live_context.update(libraries)\n\n            jinja_filters = libraries.get(\"SQLFLUFF_JINJA_FILTERS\")\n            if jinja_filters:\n                env.filters.update(jinja_filters)\n\n            if self._apply_dbt_builtins(config):\n                for name in DBT_BUILTINS:\n                    # Only apply if it hasn't already been set at this stage.\n                    if name not in live_context:\n                        live_context[name] = DBT_BUILTINS[name]\n\n        # Load macros from path (if applicable)\n        if config:\n            macros_path = self._get_macros_path(config, \"load_macros_from_path\")\n            exclude_macros_path = self._get_macros_path(\n                config, \"exclude_macros_from_path\"\n            )\n            if macros_path:\n                live_context.update(\n                    self._extract_macros_from_path(\n                        macros_path,\n                        env=env,\n                        ctx=live_context,\n                        exclude_paths=exclude_macros_path,\n                    )\n                )\n\n            # Load config macros, these will take precedence over macros from the path\n            live_context.update(\n                self._extract_macros_from_config(\n                    config=config, env=env, ctx=live_context\n                )\n            )\n\n        return live_context\n", "type": "function"}, {"name": "PythonTemplater", "docstring": "A templater using python format strings.\n\nSee: https://docs.python.org/3/library/string.html#format-string-syntax\n\nFor the python templater we don't allow functions or macros because there isn't\na good way of doing it securely. Use the jinja templater for this.\n\nThe python templater also defines a lot of the logic for how\nto allow fixing and translation in a templated file.", "methods": ["__init__", "infer_type", "get_context", "process", "slice_file", "_check_for_wrapped", "_substring_occurrences", "_sorted_occurrence_tuples", "_slice_template", "_split_invariants", "_filter_occurrences", "_coalesce_types", "_split_uniques_coalesce_rest"], "attributes": ["name"], "code_location": {"file": "python.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 152, "end_line": 1120}, "type": "class"}, {"name": "DerivedJinjaTemplater", "docstring": "A templater that includes some custom Jinja tags.\n\nThis is used for tests that show the templater can be extended for custom plugin\ntemplaters that support custom tags.", "methods": ["_get_jinja_env", "_get_jinja_analyzer"], "attributes": ["name"], "code_location": {"file": "jinja_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/templaters", "start_line": 1041, "end_line": 1056}, "type": "class"}, {"name": "test__templater_jinja_error_catastrophic", "is_method": false, "class_name": null, "parameters": [], "calls": ["JinjaTemplater", "pytest.raises", "t.process", "templater_exception.rule_code", "str", "dict", "FluffConfig"], "code_location": {"file": "jinja_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/templaters", "start_line": 619, "end_line": 632}, "code_snippet": "def test__templater_jinja_error_catastrophic():\n    \"\"\"Test error handling in the jinja templater.\"\"\"\n    t = JinjaTemplater(override_context=dict(blah=7))\n    instr = JINJA_STRING\n    with pytest.raises(SQLTemplaterError) as excinfo:\n        t.process(\n            in_str=instr,\n            fname=\"test\",\n            config=FluffConfig(overrides={\"dialect\": \"ansi\"}),\n        )\n    templater_exception = excinfo.value\n    assert templater_exception.rule_code() == \"TMP\"\n    assert templater_exception.line_no == 1\n    assert \"Unrecoverable failure in Jinja templating\" in str(templater_exception)\n", "type": "function"}, {"name": "_get_jinja_env", "is_method": true, "class_name": "DerivedJinjaTemplater", "parameters": ["self", "config"], "calls": ["_get_jinja_env", "env.add_extension", "super"], "code_location": {"file": "jinja_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/templaters", "start_line": 1050, "end_line": 1053}, "code_snippet": "    def _get_jinja_env(self, config=None):\n        env = super()._get_jinja_env(config)\n        env.add_extension(DBMigrationExtension)\n        return env\n", "type": "function"}, {"name": "_get_tag_configuration", "is_method": true, "class_name": "DerivedJinjaAnalyzer", "parameters": ["cls", "tag"], "calls": ["tag_map.get", "JinjaTagConfiguration", "JinjaTagConfiguration", "JinjaTagConfiguration", "_get_tag_configuration", "super"], "code_location": {"file": "jinja_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/templaters", "start_line": 832, "end_line": 847}, "code_snippet": "    def _get_tag_configuration(cls, tag: str) -> JinjaTagConfiguration:\n        tag_map = {\n            \"up\": JinjaTagConfiguration(\n                block_type=\"block_start\",\n                block_tracking=True,\n            ),\n            \"down\": JinjaTagConfiguration(\n                block_type=\"block_mid\",\n                block_tracking=True,\n            ),\n            \"end\": JinjaTagConfiguration(\n                block_type=\"block_end\",\n                block_tracking=True,\n            ),\n        }\n        return tag_map.get(tag, super()._get_tag_configuration(tag))\n", "type": "function"}, {"name": "JinjaAnalyzer", "docstring": "Analyzes a Jinja template to prepare for tracing.", "methods": ["__init__", "_get_tag_configuration", "_get_jinja_tracer", "next_slice_id", "slice_info_for_literal", "update_inside_set_call_macro_or_block", "make_raw_slice_info", "analyze", "track_templated", "track_call", "track_literal", "extract_tag_contents", "track_block_end", "update_next_slice_indices", "handle_left_whitespace_stripping"], "attributes": ["re_open_tag", "re_close_tag", "block_types"], "code_location": {"file": "tracer.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters/slicers", "start_line": 274, "end_line": 895}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.089082956314087}
{"question": "Why does the persist filename dispatcher conditionally suppress skipped file messages based on verbosity rather than delegating filtering to the formatter or a separate layer?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "dispatch_persist_filename", "is_method": true, "class_name": "OutputStreamFormatter", "parameters": ["self", "filename", "result"], "calls": ["self._dispatch", "self.format_filename"], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 155, "end_line": 159}, "code_snippet": "    def dispatch_persist_filename(self, filename: str, result: str) -> None:\n        \"\"\"Dispatch filenames during a persist operation.\"\"\"\n        # Only show the skip records at higher levels of verbosity\n        if self.verbosity >= 2 or result != \"SKIP\":\n            self._dispatch(self.format_filename(filename=filename, success=result))\n", "type": "function"}, {"name": "dispatch_persist_filename", "is_method": true, "class_name": "FormatterInterface", "parameters": ["self", "filename", "result"], "calls": [], "code_location": {"file": "formatter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 27, "end_line": 29}, "code_snippet": "    def dispatch_persist_filename(self, filename: str, result: str) -> None:\n        \"\"\"Called after a formatted file as been persisted to disk.\"\"\"\n        ...\n", "type": "function"}, {"name": "OutputStreamFormatter", "docstring": "Formatter which writes to an OutputStream.\n\nOn instantiation, this formatter accepts a function to\ndispatch messages. Each public method accepts an object\nor data in a common format, with this class handling the\nformatting and output.\n\nThis class is designed to be subclassed if we eventually\nwant to provide other methods of surfacing output.\n\n\nArgs:\n    output_stream: Output is sent here\n    verbosity: Specifies how verbose output should be\n    filter_empty: If True, empty messages will not be dispatched\n    output_line_length: Maximum line length", "methods": ["__init__", "should_produce_plain_output", "_dispatch", "_format_config", "dispatch_config", "dispatch_persist_filename", "_format_path", "dispatch_path", "dispatch_template_header", "dispatch_parse_header", "dispatch_lint_header", "dispatch_compilation_header", "dispatch_processing_header", "dispatch_dialect_warning", "_format_file_violations", "dispatch_file_violations", "colorize", "colorize_helper", "cli_table_row", "cli_table", "format_filename", "format_violation", "format_linting_stats", "format_config_vals", "_format_rule_description", "format_rules", "format_dialects", "format_dialect_warning", "print_out_residual_error_counts", "print_out_violations_and_timing", "completion_message"], "attributes": [], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 69, "end_line": 714}, "type": "class"}, {"name": "dispatch_file_violations", "is_method": true, "class_name": "OutputStreamFormatter", "parameters": ["self", "fname", "linted_file", "only_fixable", "warn_unused_ignores"], "calls": ["self._format_file_violations", "self._dispatch", "linted_file.get_violations", "bool"], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 257, "end_line": 279}, "code_snippet": "    def dispatch_file_violations(\n        self,\n        fname: str,\n        linted_file: LintedFile,\n        only_fixable: bool,\n        warn_unused_ignores: bool,\n    ) -> None:\n        \"\"\"Dispatch any violations found in a file.\"\"\"\n        if self.verbosity < 0:\n            return\n        s = self._format_file_violations(\n            fname,\n            linted_file.get_violations(\n                fixable=(\n                    True\n                    if bool(only_fixable and not self.show_lint_violations)\n                    else None\n                ),\n                filter_warning=False,\n                warn_unused_ignores=warn_unused_ignores,\n            ),\n        )\n        self._dispatch(s)\n", "type": "function"}, {"name": "dispatch_parse_header", "is_method": true, "class_name": "OutputStreamFormatter", "parameters": ["self", "fname"], "calls": ["self._dispatch", "self.format_filename"], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 188, "end_line": 191}, "code_snippet": "    def dispatch_parse_header(self, fname: str) -> None:\n        \"\"\"Dispatch the header displayed before parsing.\"\"\"\n        if self.verbosity > 1:\n            self._dispatch(self.format_filename(filename=fname, success=\"PARSING\"))\n", "type": "function"}, {"name": "test__templater_dbt_skips_file", "is_method": false, "class_name": null, "parameters": ["path", "reason", "dbt_templater", "project_dir", "dbt_fluff_config"], "calls": ["pytest.mark.parametrize", "pytest.raises", "dbt_templater.process", "os.path.join", "FluffConfig"], "code_location": {"file": "templater_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/test", "start_line": 327, "end_line": 340}, "code_snippet": "def test__templater_dbt_skips_file(\n    path,\n    reason,\n    dbt_templater,\n    project_dir,\n    dbt_fluff_config,\n):\n    \"\"\"A disabled dbt model should be skipped.\"\"\"\n    with pytest.raises(SQLFluffSkipFile, match=reason):\n        dbt_templater.process(\n            in_str=\"\",\n            fname=os.path.join(project_dir, path),\n            config=FluffConfig(configs=dbt_fluff_config),\n        )\n", "type": "function"}, {"name": "SQLFluffSkipFile", "docstring": "An error returned from a templater to skip a file.", "methods": [], "attributes": [], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 166, "end_line": 169}, "type": "class"}, {"name": "persist_changes", "is_method": true, "class_name": "LintedDir", "parameters": ["self", "formatter", "fixed_file_suffix"], "calls": ["file.persist_tree"], "code_location": {"file": "linted_dir.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 210, "end_line": 226}, "code_snippet": "    def persist_changes(\n        self,\n        formatter: Optional[FormatterInterface] = None,\n        fixed_file_suffix: str = \"\",\n    ) -> dict[str, Union[bool, str]]:\n        \"\"\"Persist changes to files in the given path.\n\n        This also logs the output as we go using the formatter if present.\n        \"\"\"\n        assert self.retain_files, \"cannot `persist_changes()` without `retain_files`\"\n        # Run all the fixes for all the files and return a dict\n        buffer: dict[str, Union[bool, str]] = {}\n        for file in self.files:\n            buffer[file.path] = file.persist_tree(\n                suffix=fixed_file_suffix, formatter=formatter\n            )\n        return buffer\n", "type": "function"}, {"name": "dispatch_file_violations", "is_method": true, "class_name": "FormatterInterface", "parameters": ["self", "fname", "linted_file", "only_fixable", "warn_unused_ignores"], "calls": [], "code_location": {"file": "formatter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 37, "end_line": 45}, "code_snippet": "    def dispatch_file_violations(\n        self,\n        fname: str,\n        linted_file: \"LintedFile\",\n        only_fixable: bool,\n        warn_unused_ignores: bool,\n    ) -> None:\n        \"\"\"Dispatch any violations found in a file.\"\"\"\n        ...\n", "type": "function"}, {"name": "dispatch_template_header", "is_method": true, "class_name": "OutputStreamFormatter", "parameters": ["self", "fname", "linter_config", "file_config"], "calls": ["self._dispatch", "self.format_filename", "file_config.diff_to", "self._dispatch", "self._dispatch", "self.format_config_vals", "linter_config.iter_vals"], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 170, "end_line": 186}, "code_snippet": "    def dispatch_template_header(\n        self, fname: str, linter_config: FluffConfig, file_config: Optional[FluffConfig]\n    ) -> None:\n        \"\"\"Dispatch the header displayed before templating.\"\"\"\n        if self.verbosity > 1:\n            self._dispatch(self.format_filename(filename=fname, success=\"TEMPLATING\"))\n            # This is where we output config diffs if they exist.\n            if file_config:\n                # Only output config diffs if there is a config to diff to.\n                config_diff = file_config.diff_to(linter_config)\n                if config_diff:  # pragma: no cover\n                    self._dispatch(\"   Config Diff:\")\n                    self._dispatch(\n                        self.format_config_vals(\n                            linter_config.iter_vals(cfg=config_diff)\n                        )\n                    )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0789284706115723}
{"question": "Why would repeatedly instantiating the NamedTuple that stores select clause target information in the layout rule during SQL linting cause performance degradation?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "_eval_single_select_target_element", "is_method": true, "class_name": "Rule_LT09", "parameters": ["self", "select_targets_info", "context"], "calls": ["select_clause.children", "is_type", "select_children.first", "LintResult", "FunctionalContext", "self.logger.info", "self.logger.info", "LintResult", "WhitespaceSegment", "initial_deletes.append", "sp.is_type", "select_children.index", "initial_deletes.append", "LintFix.replace", "is_type", "select_stmt.segments.index", "select_children.index", "modifier.get", "initial_deletes.append", "select_clause.get", "len", "next_segment.is_type", "select_clause.get", "select_clause.get", "modifier.get", "WhitespaceSegment", "len", "LintFix.delete", "select", "next_segment.is_type", "select_children.select", "is_type", "fixes.append", "fixes.append", "select_children.reversed", "sp.is_type", "fixes.append", "LintFix.delete", "fixes.append", "all_deletes.add", "LintFix.create_after", "LintFix.delete", "LintFix.delete", "list", "len", "NewlineSegment"], "code_location": {"file": "LT09.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 242, "end_line": 414}, "code_snippet": "    def _eval_single_select_target_element(\n        self, select_targets_info, context: RuleContext\n    ):\n        select_clause = FunctionalContext(context).segment\n        parent_stack = context.parent_stack\n        target_idx = select_targets_info.first_select_target_idx\n        select_children = select_clause.children()\n        target_seg = select_children[target_idx]\n\n        # If it's all on one line, then there's no issue.\n        if not (\n            select_targets_info.select_idx\n            < select_targets_info.first_new_line_idx\n            < target_idx\n        ):\n            self.logger.info(\n                \"Target at index %s is already on a single line.\",\n                target_idx,\n            )\n            return None\n\n        # Does the target contain a newline?\n        # i.e. even if it's a single element, does it already span more than\n        # one line?\n        if \"newline\" in target_seg.descendant_type_set:\n            self.logger.info(\n                \"Target at index %s spans multiple lines so ignoring.\",\n                target_idx,\n            )\n            return None\n\n        if select_targets_info.comment_after_select_idx != -1:\n            # The SELECT is followed by a comment on the same line. In order\n            # to autofix this, we'd need to move the select target between\n            # SELECT and the comment and potentially delete the entire line\n            # where the select target was (if it is now empty). This is\n            # *fairly tricky and complex*, in part because the newline on\n            # the select target's line is several levels higher in the\n            # parser tree. Hence, we currently don't autofix this. Could be\n            # autofixed in the future if/when we have the time.\n            return LintResult(anchor=select_clause.get())\n\n        # Prepare the select clause which will be inserted\n        insert_buff = [WhitespaceSegment(), target_seg]\n        # Delete the first select target from its original location.\n        # We'll add it to the right section at the end, once we know\n        # what to add.\n        initial_deletes = [target_seg]\n        # If there's whitespace before it, delete that too.\n        if select_children[target_idx - 1].is_type(\"whitespace\"):\n            initial_deletes.append(select_children[target_idx - 1])\n\n        # Do we have a modifier?\n        modifier: Optional[Segments]\n        modifier = select_children.first(sp.is_type(\"select_clause_modifier\"))\n\n        if (\n            # Check if the modifier is one we care about\n            modifier\n            # We only care if it's not already on the first line.\n            and select_children.index(modifier.get())\n            >= select_targets_info.first_new_line_idx\n        ):\n            # Prepend it to the insert buffer\n            insert_buff = [WhitespaceSegment(), modifier[0]] + insert_buff\n\n            modifier_idx = select_children.index(modifier.get())\n            # Delete the whitespace after it (which is two after, thanks to indent)\n            if (\n                len(select_children) > modifier_idx + 1\n                and select_children[modifier_idx + 2].is_whitespace\n            ):\n                initial_deletes.append(select_children[modifier_idx + 2])\n\n            # Delete the modifier itself\n            initial_deletes.append(modifier[0])\n\n            # Set the position marker for removing the preceding\n            # whitespace and newline, which we'll use below.\n            start_idx = modifier_idx\n            start_seg = modifier[0]\n        else:\n            # Set the position marker for removing the preceding\n            # whitespace and newline, which we'll use below.\n            start_idx = target_idx\n            start_seg = select_children[select_targets_info.first_new_line_idx]\n\n        fixes = [\n            # Insert the select_clause in place of the first newline in the\n            # Select statement\n            LintFix.replace(\n                select_children[select_targets_info.first_new_line_idx],\n                insert_buff,\n            ),\n            # Materialise any deletes so far...\n            *(LintFix.delete(seg) for seg in initial_deletes),\n        ]\n\n        if parent_stack and parent_stack[-1].is_type(\"select_statement\"):\n            select_stmt = parent_stack[-1]\n            select_clause_idx = select_stmt.segments.index(select_clause.get())\n            after_select_clause_idx = select_clause_idx + 1\n\n            if len(select_stmt.segments) > after_select_clause_idx:\n                add_newline = True\n                to_delete: Sequence[BaseSegment] = [target_seg]\n                next_segment = select_stmt.segments[after_select_clause_idx]\n\n                if next_segment.is_type(\"newline\"):\n                    # Since we're deleting the newline, we should also delete all\n                    # whitespace before it or it will add random whitespace to\n                    # following statements. So walk back through the segment\n                    # deleting whitespace until you get the previous newline, or\n                    # something else.\n                    to_delete = select_children.reversed().select(\n                        loop_while=sp.is_type(\"whitespace\"),\n                        start_seg=select_children[start_idx],\n                    )\n                    if to_delete:\n                        # The select_clause is immediately followed by a\n                        # newline. Delete the newline in order to avoid leaving\n                        # behind an empty line after fix, *unless* we stopped\n                        # due to something other than a newline.\n                        delete_last_newline = select_children[\n                            start_idx - len(to_delete) - 1\n                        ].is_type(\"newline\")\n\n                        # Delete the newline if we decided to.\n                        if delete_last_newline:\n                            fixes.append(LintFix.delete(next_segment))\n\n                elif next_segment.is_type(\"whitespace\"):\n                    # The select_clause has stuff after (most likely a comment)\n                    # Delete the whitespace immediately after the select clause\n                    # so the other stuff aligns nicely based on where the select\n                    # clause started.\n                    fixes.append(LintFix.delete(next_segment))\n\n                if to_delete:\n                    # Clean up by moving leftover select_clause segments.\n\n                    # Context: Some of the other fixes we make in\n                    # _eval_single_select_target_element() leave leftover\n                    # child segments that need to be moved to become\n                    # *siblings* of the select_clause.\n                    move_after_select_clause = select_children.select(\n                        start_seg=start_seg,\n                        stop_seg=to_delete[-1],\n                    )\n                    # :TRICKY: Below, we have a couple places where we\n                    # filter to guard against deleting the same segment\n                    # multiple times -- this is illegal.\n                    all_deletes = {\n                        fix.anchor for fix in fixes if fix.edit_type == \"delete\"\n                    }\n                    for seg in (*to_delete, *move_after_select_clause):\n                        if seg not in all_deletes:\n                            fixes.append(LintFix.delete(seg))\n                            all_deletes.add(seg)\n\n                    if move_after_select_clause or add_newline:\n                        fixes.append(\n                            LintFix.create_after(\n                                select_clause[0],\n                                ([NewlineSegment()] if add_newline else [])\n                                + list(move_after_select_clause),\n                            )\n                        )\n\n        return LintResult(\n            anchor=select_clause.get(),\n            fixes=fixes,\n        )\n", "type": "function"}, {"name": "_get_indexes", "is_method": true, "class_name": "Rule_LT09", "parameters": ["context"], "calls": ["segment.children", "children.select", "children.find", "children.select", "children.select", "get", "siblings_post.select", "SelectTargetsInfo", "sp.is_type", "select_targets.get", "sp.is_keyword", "children.find", "sp.is_type", "children.find", "children.select", "children.select", "children.find", "FunctionalContext", "sp.is_type", "list", "selects.get", "newlines.get", "sp.is_type", "sp.is_type", "segments_after_first_line.get", "first", "FunctionalContext", "selects.get", "newlines.get", "sp.or_", "children.find", "sp.is_type", "sp.is_type", "sp.is_meta", "comment_after_select.get", "siblings_post.first", "sp.is_type"], "code_location": {"file": "LT09.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 109, "end_line": 157}, "code_snippet": "    def _get_indexes(context: RuleContext) -> SelectTargetsInfo:\n        children = FunctionalContext(context).segment.children()\n        select_targets = children.select(sp.is_type(\"select_clause_element\"))\n        first_select_target_idx = children.find(select_targets.get())\n        selects = children.select(sp.is_keyword(\"select\"))\n        select_idx = children.find(selects.get()) if selects else -1\n        newlines = children.select(sp.is_type(\"newline\"))\n        first_new_line_idx = children.find(newlines.get()) if newlines else -1\n        comment_after_select_idx = -1\n        if newlines:\n            comment_after_select = children.select(\n                sp.is_type(\"comment\"),\n                start_seg=selects.get(),\n                stop_seg=newlines.get(),\n                loop_while=sp.or_(\n                    sp.is_type(\"comment\"), sp.is_type(\"whitespace\"), sp.is_meta()\n                ),\n            )\n            if comment_after_select:\n                comment_after_select_idx = (\n                    children.find(comment_after_select.get())\n                    if comment_after_select\n                    else -1\n                )\n        first_whitespace_idx = -1\n        if first_new_line_idx != -1:\n            # TRICKY: Ignore whitespace prior to the first newline, e.g. if\n            # the line with \"SELECT\" (before any select targets) has trailing\n            # whitespace.\n            segments_after_first_line = children.select(\n                sp.is_type(\"whitespace\"), start_seg=children[first_new_line_idx]\n            )\n            first_whitespace_idx = children.find(segments_after_first_line.get())\n\n        siblings_post = FunctionalContext(context).siblings_post\n        from_segment = siblings_post.first(sp.is_type(\"from_clause\")).first().get()\n        pre_from_whitespace = siblings_post.select(\n            sp.is_type(\"whitespace\"), stop_seg=from_segment\n        )\n        return SelectTargetsInfo(\n            select_idx,\n            first_new_line_idx,\n            first_select_target_idx,\n            first_whitespace_idx,\n            comment_after_select_idx,\n            select_targets,\n            from_segment,\n            list(pre_from_whitespace),\n        )\n", "type": "function"}, {"name": "Rule_LT09", "docstring": "Select targets should be on a new line unless there is only one select target.\n\n.. note::\n   By default, a wildcard (e.g. ``SELECT *``) is considered a single select target.\n   If you want it to be treated as multiple select targets, configure\n   ``wildcard_policy = multiple``.\n\n**Anti-pattern**\n\nMultiple select targets on the same line.\n\n.. code-block:: sql\n\n    select a, b\n    from foo;\n\n    -- Single select target on its own line.\n\n    SELECT\n        a\n    FROM foo;\n\n\n**Best practice**\n\nMultiple select targets each on their own line.\n\n.. code-block:: sql\n\n    select\n        a,\n        b\n    from foo;\n\n    -- Single select target on the same line as the ``SELECT``\n    -- keyword.\n\n    SELECT a\n    FROM foo;\n\n    -- When select targets span multiple lines, however they\n    -- can still be on a new line.\n\n    SELECT\n        SUM(\n            1 + SUM(\n                2 + 3\n            )\n        ) AS col\n    FROM test_table;", "methods": ["_eval", "_get_indexes", "_eval_multiple_select_target_elements", "_eval_single_select_target_element"], "attributes": ["name", "aliases", "groups", "config_keywords", "crawl_behaviour", "is_fix_compatible"], "code_location": {"file": "LT09.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 25, "end_line": 414}, "type": "class"}, {"name": "_eval", "is_method": true, "class_name": "Rule_LT09", "parameters": ["self", "context"], "calls": ["context.segment.is_type", "self._get_indexes", "children", "bool", "FunctionalContext", "sp.is_type", "self._eval_single_select_target_element", "len", "select_clause.children", "len", "self._eval_multiple_select_target_elements", "sp.is_type"], "code_location": {"file": "LT09.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 86, "end_line": 106}, "code_snippet": "    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        self.wildcard_policy: str\n        assert context.segment.is_type(\"select_clause\")\n        select_targets_info = self._get_indexes(context)\n        select_clause = FunctionalContext(context).segment\n        wildcards = select_clause.children(\n            sp.is_type(\"select_clause_element\")\n        ).children(sp.is_type(\"wildcard_expression\"))\n        has_wildcard = bool(wildcards)\n        if len(select_targets_info.select_targets) == 1 and (\n            not has_wildcard or self.wildcard_policy == \"single\"\n        ):\n            return self._eval_single_select_target_element(\n                select_targets_info,\n                context,\n            )\n        elif len(select_targets_info.select_targets):\n            return self._eval_multiple_select_target_elements(\n                select_targets_info, context.segment\n            )\n        return None\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_ST06", "parameters": ["self", "context"], "calls": ["context.segment.is_type", "reversed", "reversed", "context.segment.get_children", "seg.is_type", "seg.is_type", "enumerate", "LintResult", "seg.get_child", "seg.get_parent", "with_compound_statement.recursive_crawl", "append", "len", "any", "LintResult", "LintFix.replace", "self._implicit_column_references", "zip", "with_compound_statement.path_to", "any", "isinstance", "segment.get_child", "self._validate", "any", "isinstance", "path_step.segment.is_type", "segment.get_child", "_function.get_child", "isinstance", "path_step.segment.is_type", "self._validate", "segment.get_child", "_expression.get_child", "self._validate", "len", "len"], "code_location": {"file": "ST06.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 64, "end_line": 232}, "code_snippet": "    def _eval(self, context: RuleContext) -> EvalResultType:\n        self.violation_exists = False\n        # Bands of select targets in order to be enforced\n        select_element_order_preference: tuple[\n            tuple[Union[str, tuple[str, ...]], ...], ...\n        ] = (\n            (\"wildcard_expression\",),\n            (\n                \"object_reference\",\n                \"literal\",\n                \"cast_expression\",\n                (\"function\", \"cast\"),\n                (\"expression\", \"cast_expression\"),\n            ),\n        )\n\n        # Track which bands have been seen, with additional empty list for the\n        # non-matching elements. If we find a matching target element, we append the\n        # element to the corresponding index.\n        self.seen_band_elements: list[list[BaseSegment]] = [\n            [] for _ in select_element_order_preference\n        ] + [\n            []\n        ]  # type: ignore\n\n        assert context.segment.is_type(\"select_clause\")\n\n        # insert, merge, create table, union are order-sensitive\n        for seg in reversed(context.parent_stack):\n            if seg.is_type(\n                \"insert_statement\",\n                \"set_expression\",\n                \"create_table_statement\",\n                \"merge_statement\",\n            ):\n                return None\n\n        # CTE is order-sensitive only if CTE is referenced as SELECT * in set expression\n        for seg in reversed(context.parent_stack):\n            if seg.is_type(\"common_table_expression\"):\n                cte_identifier = seg.get_child(\"identifier\")\n                assert cte_identifier is not None\n                maybe_with_compound_statement = seg.get_parent()\n                if maybe_with_compound_statement is None:\n                    break  # pragma: no cover\n                with_compound_statement, _ = maybe_with_compound_statement\n                for ref in with_compound_statement.recursive_crawl(\"table_reference\"):\n                    if ref.raw_upper == cte_identifier.raw_upper:\n                        path = with_compound_statement.path_to(ref)\n                        if any(\n                            path_step.segment.is_type(\"set_expression\")\n                            for path_step in path\n                        ):\n                            select_statements = [\n                                path_step.segment\n                                for path_step in path\n                                if path_step.segment.is_type(\n                                    \"select_statement\",\n                                    \"unordered_select_statement_segment\",\n                                )\n                            ]\n                            if any(\n                                \"wildcard_expression\"\n                                in select_statement.descendant_type_set\n                                for select_statement in select_statements\n                            ):\n                                return None\n\n        select_clause_segment = context.segment\n        select_target_elements = context.segment.get_children(\"select_clause_element\")\n        if not select_target_elements:\n            return None\n\n        # Iterate through all the select targets to find any order violations\n        for segment in select_target_elements:\n            # The band index of the current segment in\n            # select_element_order_preference\n            self.current_element_band = None\n\n            # Compare the segment to the bands in select_element_order_preference\n            for i, band in enumerate(select_element_order_preference):\n                for e in band:\n                    # Identify simple select target\n                    if isinstance(e, str) and segment.get_child(e):\n                        self._validate(i, segment)\n\n                    # Identify function\n                    elif isinstance(e, tuple) and e[0] == \"function\":\n                        try:\n                            _function = segment.get_child(\"function\")\n                            assert _function\n                            _function_name = _function.get_child(\"function_name\")\n                            assert _function_name\n                            if _function_name.raw == e[1]:\n                                self._validate(i, segment)\n                        except (AttributeError, AssertionError):\n                            # If the segment doesn't match\n                            pass\n\n                    # Identify simple expression\n                    elif isinstance(e, tuple) and e[0] == \"expression\":\n                        try:\n                            _expression = segment.get_child(\"expression\")\n                            assert _expression\n\n                            if (\n                                _expression.get_child(e[1])\n                                and _expression.segments[0].type\n                                in (\n                                    \"column_reference\",\n                                    \"object_reference\",\n                                    \"literal\",\n                                    \"cast_expression\",\n                                )\n                                # len == 2 to ensure the expression is 'simple'\n                                and (\n                                    len(_expression.segments) == 2\n                                    # cast_expression is one length\n                                    or len(_expression.segments) == 1\n                                )\n                            ):\n                                self._validate(i, segment)\n                        except (AttributeError, AssertionError):\n                            # If the segment doesn't match\n                            pass\n\n            # If the target doesn't exist in select_element_order_preference then it\n            # is 'complex' and must go last\n            if self.current_element_band is None:\n                self.seen_band_elements[-1].append(segment)\n\n        if self.violation_exists:\n            if len(context.parent_stack) and any(\n                self._implicit_column_references(context.parent_stack[-1])\n            ):\n                # If there are implicit column references (i.e. column\n                # numbers), warn but don't fix, because it's much more\n                # complicated to autofix.\n                return LintResult(anchor=select_clause_segment)\n            # Create a list of all the edit fixes\n            # We have to do this at the end of iterating through all the\n            # select_target_elements to get the order correct. This means we can't\n            # add a lint fix to each individual LintResult as we go\n            ordered_select_target_elements = [\n                segment for band in self.seen_band_elements for segment in band\n            ]\n            # TODO: The \"if\" in the loop below compares corresponding items\n            # to avoid creating \"do-nothing\" edits. A potentially better\n            # approach would leverage difflib.SequenceMatcher.get_opcodes(),\n            # which generates a list of edit actions (similar to the\n            # command-line \"diff\" tool in Linux). This is more complex to\n            # implement, but minimizing the number of LintFixes makes the\n            # final application of patches (in \"sqlfluff fix\") more robust.\n            fixes = [\n                LintFix.replace(\n                    initial_select_target_element,\n                    [replace_select_target_element],\n                )\n                for initial_select_target_element, replace_select_target_element in zip(  # noqa: E501\n                    select_target_elements, ordered_select_target_elements\n                )\n                if initial_select_target_element is not replace_select_target_element\n            ]\n            # Anchoring on the select statement segment ensures that\n            # select statements which include macro targets are ignored\n            # when ignore_templated_areas is set\n            return LintResult(anchor=select_clause_segment, fixes=fixes)\n\n        return None\n", "type": "function"}, {"name": "Rule_ST06", "docstring": "Select wildcards then simple targets before calculations and aggregates.\n\n**Anti-pattern**\n\n.. code-block:: sql\n\n    select\n        a,\n        *,\n        row_number() over (partition by id order by date) as y,\n        b\n    from x\n\n\n**Best practice**\n\nOrder ``select`` targets in ascending complexity\n\n.. code-block:: sql\n\n    select\n        *,\n        a,\n        b,\n        row_number() over (partition by id order by date) as y\n    from x", "methods": ["_validate", "_eval", "_implicit_column_references"], "attributes": ["name", "aliases", "groups", "crawl_behaviour", "is_fix_compatible"], "code_location": {"file": "ST06.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 17, "end_line": 248}, "type": "class"}, {"name": "StatementSegment", "docstring": "An element in the targets of a select statement.", "methods": [], "attributes": [], "code_location": {"file": "dialect_db2.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 578, "end_line": 586}, "type": "class"}, {"name": "StatementSegment", "docstring": "An element in the targets of a select statement.", "methods": [], "attributes": [], "code_location": {"file": "dialect_duckdb.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 715, "end_line": 723}, "type": "class"}, {"name": "_eval_multiple_select_target_elements", "is_method": true, "class_name": "Rule_LT09", "parameters": ["self", "select_targets_info", "segment"], "calls": ["enumerate", "Segments", "get", "get", "self.logger.debug", "LintResult", "segment.get_child", "segment.select_children", "fixes.append", "raw_segments.first", "last", "segment.segments.index", "LintFix.delete", "LintFix.create_before", "fixes.extend", "fixes.append", "sp.is_code", "len", "LintFix.create_before", "select_clause_raws.select", "s.is_type", "NewlineSegment", "LintFix.delete", "Segments", "s.is_type", "NewlineSegment", "sp.and_", "sp.is_code", "sp.not_", "sp.raw_is"], "code_location": {"file": "LT09.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 159, "end_line": 240}, "code_snippet": "    def _eval_multiple_select_target_elements(\n        self, select_targets_info, segment\n    ) -> Optional[LintResult]:\n        \"\"\"Multiple select targets. Ensure each is on a separate line.\"\"\"\n        fixes = []\n        previous_code = None\n        select_clause_raws = Segments(segment).raw_segments\n        for i, select_target in enumerate(select_targets_info.select_targets):\n            assert select_target.pos_marker\n            target_start_line = select_target.pos_marker.working_line_no\n            target_initial_code = (\n                Segments(select_target).raw_segments.first(sp.is_code()).get()\n            )\n            assert target_initial_code\n            previous_code = (\n                select_clause_raws.select(\n                    # Get the first code that isn't a comma.\n                    select_if=sp.and_(sp.is_code(), sp.not_(sp.raw_is(\",\"))),\n                    start_seg=previous_code,\n                    stop_seg=target_initial_code,\n                )\n                .last()\n                .get()\n            )\n            assert previous_code\n            assert previous_code.pos_marker\n            previous_end_line = previous_code.pos_marker.working_line_no\n            self.logger.debug(\n                \"- Evaluating %s [%s, %s]: Prev ends with: %s\",\n                select_target,\n                previous_end_line,\n                target_start_line,\n                previous_code,\n            )\n\n            # Check whether this target *starts* on the same line that the\n            # previous one *ends* on. If they are on the same line, insert a newline.\n            if target_start_line == previous_end_line:\n                # Find and delete any whitespace before the select target.\n                start_seg = select_targets_info.select_idx\n                # If any select modifier (e.g. distinct ) is present, start\n                # there rather than at the beginning.\n                modifier = segment.get_child(\"select_clause_modifier\")\n                if modifier:\n                    start_seg = segment.segments.index(modifier)\n\n                ws_to_delete = segment.select_children(\n                    start_seg=(\n                        segment.segments[start_seg]\n                        if not i\n                        else select_targets_info.select_targets[i - 1]\n                    ),\n                    select_if=lambda s: s.is_type(\"whitespace\"),\n                    loop_while=lambda s: s.is_type(\"whitespace\", \"comma\") or s.is_meta,\n                )\n                fixes += [LintFix.delete(ws) for ws in ws_to_delete]\n                fixes.append(LintFix.create_before(select_target, [NewlineSegment()]))\n\n            # If we are at the last select target check if the FROM clause\n            # is on the same line, and if so move it to its own line.\n            if select_targets_info.from_segment:\n                if (i + 1 == len(select_targets_info.select_targets)) and (\n                    select_target.pos_marker.working_line_no\n                    == select_targets_info.from_segment.pos_marker.working_line_no\n                ):\n                    fixes.extend(\n                        [\n                            LintFix.delete(ws)\n                            for ws in select_targets_info.pre_from_whitespace\n                        ]\n                    )\n                    fixes.append(\n                        LintFix.create_before(\n                            select_targets_info.from_segment,\n                            [NewlineSegment()],\n                        )\n                    )\n\n        if fixes:\n            return LintResult(anchor=segment, fixes=fixes)\n\n        return None\n", "type": "function"}, {"name": "SelectClauseSegment", "docstring": "A group of elements in a select target statement.\n\nIt's very similar to `SelectClauseSegment` from `dialect_ansi` except does not\nhave set `SetOperatorSegment` as possible terminator - this is to avoid issues\nwith wrongly recognized `EXCEPT`.", "methods": [], "attributes": [], "code_location": {"file": "dialect_sparksql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3486, "end_line": 3514}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.0965285301208496}
{"question": "How does the linting error class manage edit recommendation objects through dictionary conversion, duplicate detection, and optional field promotion to ensure template-aware and original-file edits remain consistent during state preservation, duplicate removal, or API output generation?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "LintFix", "docstring": "A class to hold a potential fix to a linting violation.\n\nArgs:\n    edit_type (:obj:`str`): One of `create_before`, `create_after`,\n        `replace`, `delete` to indicate the kind of fix this represents.\n    anchor (:obj:`BaseSegment`): A segment which represents\n        the *position* that this fix should be applied at. For deletions\n        it represents the segment to delete, for creations it implies the\n        position to create at (with the existing element at this position\n        to be moved *after* the edit), for a `replace` it implies the\n        segment to be replaced.\n    edit (iterable of :obj:`BaseSegment`, optional): For `replace` and\n        `create` fixes, this holds the iterable of segments to create\n        or replace at the given `anchor` point.\n    source (iterable of :obj:`BaseSegment`, optional): For `replace` and\n        `create` fixes, this holds iterable of segments that provided\n        code. IMPORTANT: The linter uses this to prevent copying material\n        from templated areas.", "methods": ["__init__", "is_just_source_edit", "__repr__", "to_dict", "__eq__", "delete", "replace", "create_before", "create_after", "get_fix_slices", "has_template_conflicts", "_raw_slices_from_templated_slices"], "attributes": [], "code_location": {"file": "fix.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 14, "end_line": 413}, "type": "class"}, {"name": "to_dict", "is_method": true, "class_name": "SQLLintError", "parameters": ["self"], "calls": ["to_dict", "_base_dict.update", "cast", "_base_dict.get", "super", "_extract_position", "len", "fix.to_dict"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 300, "end_line": 332}, "code_snippet": "    def to_dict(self) -> SerializedObject:\n        \"\"\"Return a dict of properties.\n\n        This is useful in the API for outputting violations.\n\n        For linting errors we additionally add details of any fixes.\n        \"\"\"\n        _base_dict = super().to_dict()\n        _base_dict.update(\n            fixes=[fix.to_dict() for fix in self.fixes],\n            **_extract_position(self.segment),\n        )\n        # Edge case: If the base error doesn't have an end position\n        # but we only have one fix and it _does_. Then use use that in the\n        # overall fix.\n        _fixes = cast(list[SerializedObject], _base_dict.get(\"fixes\", []))\n        if \"end_line_pos\" not in _base_dict and len(_fixes) == 1:\n            _fix = _fixes[0]\n            # If the mandatory keys match...\n            if (\n                _fix[\"start_line_no\"] == _base_dict[\"start_line_no\"]\n                and _fix[\"start_line_pos\"] == _base_dict[\"start_line_pos\"]\n            ):\n                # ...then hoist all the optional ones from the fix.\n                for key in [\n                    \"start_file_pos\",\n                    \"end_line_no\",\n                    \"end_line_pos\",\n                    \"end_file_pos\",\n                ]:\n                    _base_dict[key] = _fix[key]\n\n        return _base_dict\n", "type": "function"}, {"name": "source_signature", "is_method": true, "class_name": "SQLLintError", "parameters": ["self"], "calls": ["tuple", "self.check_tuple", "tuple", "tuple", "_source_fixes.append"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 349, "end_line": 375}, "code_snippet": "    def source_signature(self) -> tuple[Any, ...]:\n        \"\"\"Return hashable source signature for deduplication.\n\n        For linting errors we need to dedupe on more than just location and\n        description, we also need to check the edits potentially made, both\n        in the templated file but also in the source.\n        \"\"\"\n        fix_raws = tuple(\n            tuple(e.raw for e in f.edit) if f.edit else None for f in self.fixes\n        )\n        _source_fixes: list[tuple[str, int, int]] = []\n        for fix in self.fixes:\n            if not fix.edit:\n                continue\n            for edit in fix.edit:\n                for source_edit in edit.source_fixes:\n                    # NOTE: It's important that we don't dedupe on the\n                    # templated slice for the source fix, because that will\n                    # be different for different locations in any loop.\n                    _source_fixes.append(\n                        (\n                            source_edit.edit,\n                            source_edit.source_slice.start,\n                            source_edit.source_slice.stop,\n                        )\n                    )\n        return (self.check_tuple(), self.description, fix_raws, tuple(_source_fixes))\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "LintFix", "parameters": ["self", "edit_type", "anchor", "edit", "source"], "calls": ["ValueError", "ValueError", "all", "s.copy", "rules_logger.debug"], "code_location": {"file": "fix.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 35, "end_line": 88}, "code_snippet": "    def __init__(\n        self,\n        edit_type: str,\n        anchor: BaseSegment,\n        edit: Optional[Iterable[BaseSegment]] = None,\n        source: Optional[Iterable[BaseSegment]] = None,\n    ) -> None:\n        if edit_type not in (\n            \"create_before\",\n            \"create_after\",\n            \"replace\",\n            \"delete\",\n        ):  # pragma: no cover\n            raise ValueError(f\"Unexpected edit_type: {edit_type}\")\n        self.edit_type = edit_type\n        if not anchor:  # pragma: no cover\n            raise ValueError(\"Fixes must provide an anchor.\")\n        self.anchor = anchor\n        self.edit: Optional[list[BaseSegment]] = None\n        if edit is not None:\n            # Copy all the elements of edit to stop contamination.\n            # We're about to start stripping the position markers\n            # off some of the elements and we don't want to end up\n            # stripping the positions of the original elements of\n            # the parsed structure.\n            self.edit = [s.copy() for s in edit]\n            # Check that any edits don't have a position marker set.\n            # We should rely on realignment to make position markers.\n            # Strip position markers of anything enriched, otherwise things can get\n            # blurry\n            for seg in self.edit:\n                if seg.pos_marker:\n                    # Developer warning.\n                    rules_logger.debug(\n                        \"Developer Note: Edit segment found with preset position \"\n                        \"marker. These should be unset and calculated later.\"\n                    )\n                    seg.pos_marker = None\n            # Once stripped, we shouldn't replace any markers because\n            # later code may rely on them being accurate, which we\n            # can't guarantee with edits.\n        self.source = [seg for seg in source if seg.pos_marker] if source else []\n\n        # On creation of the fix we'll also validate the edits are non-trivial.\n        if self.edit_type in (\"create_before\", \"create_after\"):\n            assert self.edit, \"A create fix must have an edit.\"\n            # They should all have a non-zero raw.\n            assert all(\n                seg.raw for seg in self.edit\n            ), f\"Invalid edit found: {self.edit}.\"\n        elif self.edit_type == \"replace\":\n            assert (\n                self.edit != self.anchor\n            ), \"Fix created which replaces segment with itself.\"\n", "type": "function"}, {"name": "SourceFix", "docstring": "A stored reference to a fix in the non-templated file.", "methods": ["__hash__"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 53, "end_line": 68}, "type": "class"}, {"name": "deduplicate_in_source_space", "is_method": true, "class_name": "LintedFile", "parameters": ["violations"], "calls": ["set", "sorted", "v.source_signature", "new_violations.append", "dedupe_buffer.add", "linter_logger.debug"], "code_location": {"file": "linted_file.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 95, "end_line": 119}, "code_snippet": "    def deduplicate_in_source_space(\n        violations: list[SQLBaseError],\n    ) -> list[SQLBaseError]:\n        \"\"\"Removes duplicates in the source space.\n\n        This is useful for templated files with loops, where we'll\n        get a violation for each pass around the loop, but the user\n        only cares about it once and we're only going to fix it once.\n\n        By filtering them early we get a more a more helpful CLI\n        output *and* and more efficient fixing routine (by handling\n        fewer fixes).\n        \"\"\"\n        new_violations = []\n        dedupe_buffer = set()\n        for v in violations:\n            signature = v.source_signature()\n            if signature not in dedupe_buffer:\n                new_violations.append(v)\n                dedupe_buffer.add(signature)\n            else:\n                linter_logger.debug(\"Removing duplicate source violation: %r\", v)\n        # Sort on return so that if any are out of order, they're now ordered\n        # appropriately. This happens most often when linting multiple variants.\n        return sorted(new_violations, key=lambda v: (v.line_no, v.line_pos))\n", "type": "function"}, {"name": "to_dict", "is_method": true, "class_name": "LintFix", "parameters": ["self"], "calls": ["_position.to_source_dict", "cast", "join", "self.is_just_source_edit", "len", "len", "_position.templated_file.source_position_dict_from_slice"], "code_location": {"file": "fix.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 129, "end_line": 177}, "code_snippet": "    def to_dict(self) -> dict[str, Any]:\n        \"\"\"Serialise this LintFix as a dict.\"\"\"\n        assert self.anchor\n        _position = self.anchor.pos_marker\n        assert _position\n        _src_loc = _position.to_source_dict()\n        if self.edit_type == \"delete\":\n            return {\n                \"type\": self.edit_type,\n                \"edit\": \"\",\n                **_src_loc,\n            }\n        elif self.edit_type == \"replace\" and self.is_just_source_edit(\n            single_source_fix=True\n        ):\n            assert self.edit is not None\n            assert len(self.edit) == 1\n            assert len(self.edit[0].source_fixes) == 1\n            _source_fix = self.edit[0].source_fixes[0]\n            return {\n                \"type\": self.edit_type,\n                \"edit\": _source_fix.edit,\n                **_position.templated_file.source_position_dict_from_slice(\n                    _source_fix.source_slice\n                ),\n            }\n\n        # Otherwise it's a standard creation or a replace.\n        seg_list = cast(list[BaseSegment], self.edit)\n        _edit = \"\".join(s.raw for s in seg_list)\n\n        if self.edit_type == \"create_before\":\n            # If we're creating _before_, the end point isn't relevant.\n            # Make it the same as the start.\n            _src_loc[\"end_line_no\"] = _src_loc[\"start_line_no\"]\n            _src_loc[\"end_line_pos\"] = _src_loc[\"start_line_pos\"]\n            _src_loc[\"end_file_pos\"] = _src_loc[\"start_file_pos\"]\n        elif self.edit_type == \"create_after\":\n            # If we're creating _after_, the start point isn't relevant.\n            # Make it the same as the end.\n            _src_loc[\"start_line_no\"] = _src_loc[\"end_line_no\"]\n            _src_loc[\"start_line_pos\"] = _src_loc[\"end_line_pos\"]\n            _src_loc[\"start_file_pos\"] = _src_loc[\"end_file_pos\"]\n\n        return {\n            \"type\": self.edit_type,\n            \"edit\": _edit,\n            **_src_loc,\n        }\n", "type": "function"}, {"name": "add", "is_method": true, "class_name": "AnchorEditInfo", "parameters": ["self", "fix"], "calls": ["fix.is_just_source_edit", "self.fixes.append", "setattr", "linter_logger.info", "edit", "linter_logger.info", "getattr"], "code_location": {"file": "fix.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 31, "end_line": 67}, "code_snippet": "    def add(self, fix: \"LintFix\") -> None:\n        \"\"\"Adds the fix and updates stats.\n\n        We also allow potentially multiple source fixes on the same\n        anchor by condensing them together here.\n        \"\"\"\n        if fix in self.fixes:\n            # Deduplicate fixes in case it's already in there.\n            return\n\n        if fix.is_just_source_edit():\n            assert fix.edit\n            # is_just_source_edit confirms there will be a list\n            # so we can hint that to mypy.\n            self.source_fixes += fix.edit[0].source_fixes\n            # is there already a replace?\n            if self._first_replace:\n                assert self._first_replace.edit\n                # is_just_source_edit confirms there will be a list\n                # and that's the only way to get into _first_replace\n                # if it's populated so we can hint that to mypy.\n                linter_logger.info(\n                    \"Multiple edits detected, condensing %s onto %s\",\n                    fix,\n                    self._first_replace,\n                )\n                self._first_replace.edit[0] = self._first_replace.edit[0].edit(\n                    source_fixes=self.source_fixes\n                )\n                linter_logger.info(\"Condensed fix: %s\", self._first_replace)\n                # Return without otherwise adding in this fix.\n                return\n\n        self.fixes.append(fix)\n        if fix.edit_type == \"replace\" and not self._first_replace:\n            self._first_replace = fix\n        setattr(self, fix.edit_type, getattr(self, fix.edit_type) + 1)\n", "type": "function"}, {"name": "apply_fixes", "is_method": false, "class_name": null, "parameters": ["segment", "dialect", "rule_code", "fixes", "fix_even_unparsable"], "calls": ["segment.invalidate_caches", "segment.is_raw", "fixes.pop", "list", "apply_fixes", "range", "len", "range", "segment.__class__", "seg_buffer.append", "seg_fixes.reverse", "fixes_applied.append", "linter_logger.debug", "segment._position_segments", "len", "segment._is_code_or_meta", "len", "segment._is_code_or_meta", "hasattr", "hasattr", "len", "seg_buffer.append", "seg_buffer.append", "seg_buffer.append", "tuple", "segment._position_segments", "err.add_note", "new_seg.validate_segment_with_reparse", "len", "tuple", "getattr", "len"], "code_location": {"file": "fix.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 107, "end_line": 335}, "code_snippet": "def apply_fixes(\n    segment: BaseSegment,\n    dialect: \"Dialect\",\n    rule_code: str,\n    fixes: dict[int, AnchorEditInfo],\n    fix_even_unparsable: bool = False,\n) -> tuple[\"BaseSegment\", list[\"BaseSegment\"], list[\"BaseSegment\"], bool]:\n    \"\"\"Apply a dictionary of fixes to this segment.\n\n    Used in to apply fixes found in linting. If a segment remains unchanged\n    then the original is returned, but if any changes are made to it, or any\n    of it's child segments, then it returns a copy rather than mutating the\n    original.\n\n    Most fixes are usually applied when this method is called on their parent\n    segment, this is because that's where we can insert or move segments relative\n    to the anchor specified in the fix. This has the implication that if the\n    method is called on a `RawSegment`, then no changes will be applied, because\n    a `RawSegment` never has child segments.\n\n    After fixing, it calls `validate_segment_with_reparse` on the segment to\n    check that the segment still parses after any changes are made. The result\n    of this is returned as a boolean in the last element of the return tuple.\n    As the function recurses, if an inner element doesn't parse after fixing,\n    then the outer segment will also be checked, and if found to parse successfully\n    then the method returns `True` as valid. This is because sometimes the fixes\n    change the structure enough that a wider reparse is necessary.\n\n    Because of this validity checking, any unparsable sections are assumed\n    unfixable (because we won't know if we're corrupting the SQL). The method\n    will therefore return early without applying any fixes if the segment it's\n    called on is unparsable (because we already know that validation check will\n    fail already).\n\n    If `fix_even_unparsable` is True, then we will still apply fixes to unparsable\n    sections, but will do so *without validation*. That means that the final\n    element of the return value will always return `True`, so that we don't interrupt\n    the validity checking of any outer (parsable) sections.\n    \"\"\"\n    if not fixes or segment.is_raw():\n        return segment, [], [], True\n\n    seg_buffer = []\n    before = []\n    after = []\n    fixes_applied: list[LintFix] = []\n    requires_validate = False\n\n    for seg in segment.segments:\n        # Look for uuid match.\n        # This handles potential positioning ambiguity.\n        anchor_info: Optional[AnchorEditInfo] = fixes.pop(seg.uuid, None)\n\n        if anchor_info is None:\n            # No fix matches here, just add the segment and move on.\n            seg_buffer.append(seg)\n            continue\n\n        # Otherwise there is a fix match.\n        seg_fixes = anchor_info.fixes\n        if (\n            len(seg_fixes) == 2 and seg_fixes[0].edit_type == \"create_after\"\n        ):  # pragma: no cover\n            # Must be create_before & create_after. Swap so the\n            # \"before\" comes first.\n            seg_fixes.reverse()\n\n        for f in anchor_info.fixes:\n            assert f.anchor.uuid == seg.uuid\n            fixes_applied.append(f)\n            linter_logger.debug(\n                \"Matched fix for %s against segment: %s -> %s\",\n                rule_code,\n                f,\n                seg,\n            )\n\n            # Deletes are easy.\n            if f.edit_type == \"delete\":\n                # We're just getting rid of this segment.\n                requires_validate = True\n                # NOTE: We don't add the segment in this case.\n                continue\n\n            # Otherwise it must be a replace or a create.\n            assert f.edit_type in (\n                \"replace\",\n                \"create_before\",\n                \"create_after\",\n            ), f\"Unexpected edit_type: {f.edit_type!r} in {f!r}\"\n\n            if f.edit_type == \"create_after\" and len(anchor_info.fixes) == 1:\n                # in the case of a creation after that is not part\n                # of a create_before/create_after pair, also add\n                # this segment before the edit.\n                seg_buffer.append(seg)\n\n            # We're doing a replacement (it could be a single\n            # segment or an iterable)\n            assert f.edit, f\"Edit {f.edit_type!r} requires `edit`.\"\n            consumed_pos = False\n            for s in f.edit:\n                seg_buffer.append(s)\n                # If one of them has the same raw representation\n                # then the first that matches gets to take the\n                # original position marker.\n                if f.edit_type == \"replace\" and s.raw == seg.raw and not consumed_pos:\n                    seg_buffer[-1].pos_marker = seg.pos_marker\n                    consumed_pos = True\n\n            # If we're just editing a segment AND keeping the type the\n            # same then no need to validate. Otherwise we should\n            # trigger a validation (e.g. for creations or\n            # multi-replace).\n            if not (\n                f.edit_type == \"replace\"\n                and len(f.edit) == 1\n                and f.edit[0].class_types == seg.class_types\n            ):\n                requires_validate = True\n\n            if f.edit_type == \"create_before\":\n                # in the case of a creation before, also add this\n                # segment on the end\n                seg_buffer.append(seg)\n\n    # Invalidate any caches\n    segment.invalidate_caches()\n\n    # If any fixes applied, do an intermediate reposition. When applying\n    # fixes to children and then trying to reposition them, that recursion\n    # may rely on the parent having already populated positions for any\n    # of the fixes applied there first. This ensures those segments have\n    # working positions to work with.\n    if fixes_applied:\n        assert segment.pos_marker\n        seg_buffer = list(\n            segment._position_segments(tuple(seg_buffer), parent_pos=segment.pos_marker)\n        )\n\n    # Then recurse (i.e. deal with the children) (Requeueing)\n    seg_queue = seg_buffer\n    seg_buffer = []\n    for seg in seg_queue:\n        s, pre, post, validated = apply_fixes(seg, dialect, rule_code, fixes)\n        # 'before' and 'after' will usually be empty. Only used when\n        # lower-level fixes left 'seg' with non-code (usually\n        # whitespace) segments as the first or last children. This is\n        # generally not allowed (see the can_start_end_non_code field),\n        # and these segments need to be \"bubbled up\" the tree.\n        seg_buffer += pre + [s] + post\n        # If we fail to validate a child segment, make sure to validate this\n        # segment.\n        if not validated:\n            requires_validate = True\n\n    # Most correct whitespace positioning will have already been handled\n    # _however_, the exception is `replace` edits which match start or\n    # end with whitespace. We also need to handle any leading or trailing\n    # whitespace ejected from the any fixes applied to child segments.\n    # Here we handle those by checking the start and end of the resulting\n    # segment sequence for whitespace.\n    # If we're left with any non-code at the end, trim them off and pass them\n    # up to the parent segment for handling.\n    if not segment.can_start_end_non_code:\n        _idx = 0\n        for _idx in range(0, len(seg_buffer)):\n            if segment._is_code_or_meta(seg_buffer[_idx]):\n                break\n        before = seg_buffer[:_idx]\n        seg_buffer = seg_buffer[_idx:]\n\n        _idx = len(seg_buffer)\n        for _idx in range(len(seg_buffer), 0, -1):\n            if segment._is_code_or_meta(seg_buffer[_idx - 1]):\n                break\n        after = seg_buffer[_idx:]\n        seg_buffer = seg_buffer[:_idx]\n\n    # Reform into a new segment\n    assert segment.pos_marker\n    try:\n        new_seg = segment.__class__(\n            # Realign the segments within\n            segments=segment._position_segments(\n                tuple(seg_buffer), parent_pos=segment.pos_marker\n            ),\n            pos_marker=segment.pos_marker,\n            # Pass through any additional kwargs\n            **{k: getattr(segment, k) for k in segment.additional_kwargs},\n        )\n    except AssertionError as err:  # pragma: no cover\n        # An AssertionError on creating a new segment is likely a whitespace\n        # check fail. If possible add information about the fixes we tried to\n        # apply, before re-raising.\n        # NOTE: only available in python 3.11+.\n        if hasattr(err, \"add_note\"):\n            err.add_note(f\" After applying fixes: {fixes_applied}.\")\n        raise err\n\n    # Handle any necessary validation.\n    if requires_validate:\n        # Was it already unparsable?\n        if \"unparsable\" in segment.descendant_type_set | segment.class_types:\n            if fix_even_unparsable:\n                # If we're fixing even unparsable sections, there's no point trying\n                # to validate, it will always fail. We may still want to validate\n                # other sections of the file though, so we should just declare *this*\n                # part of the file to be all good.\n                validated = True\n            else:\n                # It was already unparsable, but we're being asked to validate.\n                # Don't any apply fixes from within this region and just return the\n                # original segment.\n                return segment, [], [], True\n        # Otherwise only validate if there's a match_grammar. Otherwise we may get\n        # strange results (for example with the BracketedSegment).\n        elif hasattr(new_seg, \"match_grammar\"):\n            validated = new_seg.validate_segment_with_reparse(dialect)\n    else:\n        validated = not requires_validate\n    # Return the new segment and any non-code that needs to bubble up\n    # the tree.\n    # NOTE: We pass on whether this segment has been validated. It's\n    # very possible that our parsing here may fail depending on the\n    # type of segment that has been replaced, but if not we rely on\n    # a parent segment still being valid. If we get all the way up\n    # to the root and it's still not valid - that's a problem.\n    return new_seg, before, after, validated\n", "type": "function"}, {"name": "AnchorEditInfo", "docstring": "For a given fix anchor, count of the fix edit types and fixes for it.", "methods": ["add", "total", "is_valid"], "attributes": [], "code_location": {"file": "fix.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 19, "end_line": 93}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.1085693836212158}
{"question": "Why does the early exit check using the cached subtree type set intersection prevent performance degradation in the recursive segment search method when traversing deeply nested trees with irrelevant branches?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "recursive_crawl", "is_method": true, "class_name": "BaseSegment", "parameters": ["self"], "calls": ["isinstance", "self.is_type", "self.descendant_type_set.intersection", "seg.is_type", "seg.recursive_crawl"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 1005, "end_line": 1054}, "code_snippet": "    def recursive_crawl(\n        self,\n        *seg_type: str,\n        recurse_into: bool = True,\n        no_recursive_seg_type: Optional[Union[str, list[str]]] = None,\n        allow_self: bool = True,\n    ) -> Iterator[BaseSegment]:\n        \"\"\"Recursively crawl for segments of a given type.\n\n        Args:\n            seg_type: :obj:`str`: one or more type of segment\n                to look for.\n            recurse_into: :obj:`bool`: When an element of type \"seg_type\" is\n                found, whether to recurse into it.\n            no_recursive_seg_type: :obj:`Union[str, list[str]]`: a type of segment\n                not to recurse further into. It is highly recommended\n                to set this argument where possible, as it can significantly\n                narrow the search pattern.\n            allow_self: :obj:`bool`: Whether to allow the initial segment this\n                is called on to be one of the results.\n        \"\"\"\n        if isinstance(no_recursive_seg_type, str):\n            no_recursive_seg_type = [no_recursive_seg_type]\n\n        # Assuming there is a segment to be found, first check self (if allowed):\n        if allow_self and self.is_type(*seg_type):\n            match = True\n            yield self\n        else:\n            match = False\n\n        # Check whether the types we're looking for are in this segment\n        # at all. If not, exit early.\n        if not self.descendant_type_set.intersection(seg_type):\n            # Terminate iteration.\n            return None\n\n        # Then handle any recursion.\n        if recurse_into or not match:\n            for seg in self.segments:\n                # Don't recurse if the segment is of a type we shouldn't\n                # recurse into.\n                # NOTE: Setting no_recursive_seg_type can significantly\n                # improve performance in many cases.\n                if not no_recursive_seg_type or not seg.is_type(*no_recursive_seg_type):\n                    yield from seg.recursive_crawl(\n                        *seg_type,\n                        recurse_into=recurse_into,\n                        no_recursive_seg_type=no_recursive_seg_type,\n                    )\n", "type": "function"}, {"name": "recursive_crawl", "is_method": true, "class_name": "Segments", "parameters": ["self"], "calls": ["Segments", "s.recursive_crawl", "segments.append"], "code_location": {"file": "segments.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/functional", "start_line": 112, "end_line": 118}, "code_snippet": "    def recursive_crawl(self, *seg_type: str, recurse_into: bool = True) -> \"Segments\":\n        \"\"\"Recursively crawl for segments of a given type.\"\"\"\n        segments: list[BaseSegment] = []\n        for s in self:\n            for i in s.recursive_crawl(*seg_type, recurse_into=recurse_into):\n                segments.append(i)\n        return Segments(*segments, templated_file=self.templated_file)\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "SegmentSeekerCrawler", "parameters": ["self", "types", "provide_raw_stack", "allow_recurse"], "calls": ["__init__", "super"], "code_location": {"file": "crawlers.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 54, "end_line": 71}, "code_snippet": "    def __init__(\n        self,\n        types: set[str],\n        provide_raw_stack: bool = False,\n        allow_recurse: bool = True,\n        **kwargs: Any,\n    ) -> None:\n        self.types = types\n        # Tracking a raw stack involves a lot of tuple manipulation, so we\n        # only do it when required - otherwise we skip it. Rules can explicitly\n        # request it when defining their crawler.\n        self.provide_raw_stack = provide_raw_stack\n        # If allow_recurse is false, then once a segment matches, none of it's\n        # children will be returned. This is useful in cases where we might have\n        # many start points, but one root segment will check any matching sub-\n        # segments in the same evaluation.\n        self.allow_recurse = allow_recurse\n        super().__init__(**kwargs)\n", "type": "function"}, {"name": "get_parent_of", "is_method": true, "class_name": "BaseRule", "parameters": ["cls", "segment", "root_segment"], "calls": ["cls.get_parent_of"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 672, "end_line": 696}, "code_snippet": "    def get_parent_of(\n        cls, segment: BaseSegment, root_segment: BaseSegment\n    ) -> Optional[BaseSegment]:  # pragma: no cover TODO?\n        \"\"\"Return the segment immediately containing segment.\n\n        NB: This is recursive.\n\n        Args:\n            segment: The segment to look for.\n            root_segment: Some known parent of the segment\n                we're looking for (although likely not the\n                direct parent in question).\n\n        \"\"\"\n        if segment in root_segment.segments:\n            return root_segment\n        elif root_segment.segments:\n            # try each of the subsegments\n            for sub in root_segment.segments:\n                p = cls.get_parent_of(segment, sub)\n                if p:\n                    return p\n        # Not directly in the segment and\n        # no subsegments to check. Return None.\n        return None\n", "type": "function"}, {"name": "is_self_match", "is_method": true, "class_name": "ParentOfSegmentCrawler", "parameters": ["self", "segment"], "calls": ["bool"], "code_location": {"file": "crawlers.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 139, "end_line": 147}, "code_snippet": "    def is_self_match(self, segment: BaseSegment) -> bool:\n        \"\"\"Does this segment match the relevant criteria.\n\n        We use the _direct_ child set here so that if any of the\n        direct child segments match any of the types we're looking\n        for, then we know that this segment is a parent of that\n        kind of segment.\n        \"\"\"\n        return bool(self.types & segment.direct_descendant_type_set)\n", "type": "function"}, {"name": "SegmentSeekerCrawler", "docstring": "A crawler that efficiently searches for specific segment types.\n\nThe segment type(s) are specified on creation.", "methods": ["__init__", "is_self_match", "crawl"], "attributes": [], "code_location": {"file": "crawlers.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 48, "end_line": 130}, "type": "class"}, {"name": "descendant_type_set", "is_method": true, "class_name": "BaseSegment", "parameters": ["self"], "calls": ["frozenset", "chain.from_iterable"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 330, "end_line": 341}, "code_snippet": "    def descendant_type_set(self) -> frozenset[str]:\n        \"\"\"The set of all contained types.\n\n        This is used for rule crawling.\n\n        NOTE: Does not include the types of the parent segment itself.\n        \"\"\"\n        return frozenset(\n            chain.from_iterable(\n                seg.descendant_type_set | seg.class_types for seg in self.segments\n            )\n        )\n", "type": "function"}, {"name": "crawl", "is_method": true, "class_name": "SegmentSeekerCrawler", "parameters": ["self", "context"], "calls": ["self.is_self_match", "enumerate", "self.passes_filter", "tuple", "tuple", "self.crawl", "cast"], "code_location": {"file": "crawlers.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 77, "end_line": 130}, "code_snippet": "    def crawl(self, context: RuleContext) -> Iterator[RuleContext]:\n        \"\"\"Yields a RuleContext for each segment the rule should process.\n\n        We assume that segments are yielded by their parent.\n        \"\"\"\n        # Check whether we should consider this segment _or it's children_\n        # at all.\n        self_match = False\n        if not self.passes_filter(context.segment):\n            if self.provide_raw_stack:  # pragma: no cover\n                context.raw_stack += tuple(context.segment.raw_segments)\n            return\n\n        # Then check the segment itself, yield if it's a match.\n        if self.is_self_match(context.segment):\n            self_match = True\n            yield context\n\n        # Check whether any children?\n        # Abort if not - we've already yielded self.\n        # NOTE: This same clause also works if we did match but aren't\n        # allowed to recurse.\n        if not context.segment.segments or (self_match and not self.allow_recurse):\n            # Add self to raw stack first if so.\n            if self.provide_raw_stack:\n                context.raw_stack += (cast(RawSegment, context.segment),)\n            return\n\n        # Check whether one of the targets is present (set intersection)\n        if not self.types & context.segment.descendant_type_set:\n            # None present. Don't look further.\n            # This aggressive pruning helps performance.\n            # Track raw stack if required.\n            if self.provide_raw_stack:\n                context.raw_stack += tuple(context.segment.raw_segments)\n            return\n\n        # NOTE: Full context is not implemented yet. More dev work required\n        # before everything will be available here.\n\n        # Given we know that one is present in here somewhere, search for it.\n        new_parent_stack = context.parent_stack + (context.segment,)\n        for idx, child in enumerate(context.segment.segments):\n            # For performance reasons, don't create a new RuleContext for\n            # each segment; just modify the existing one in place. This\n            # requires some careful bookkeeping, but it avoids creating a\n            # HUGE number of short-lived RuleContext objects\n            # (#linter loops x #rules x #segments).\n            # Importantly, we're resetting values here, because they\n            # may have been modified deeper in the recursion.\n            context.segment = child\n            context.parent_stack = new_parent_stack\n            context.segment_idx = idx\n            yield from self.crawl(context)\n", "type": "function"}, {"name": "copy", "is_method": true, "class_name": "BaseSegment", "parameters": ["self", "segments", "parent", "parent_idx"], "calls": ["cls.__new__", "new_segment.__dict__.update", "new_segment.set_parent", "self.__dict__.get", "tuple", "seg.copy", "enumerate"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 860, "end_line": 912}, "code_snippet": "    def copy(\n        self,\n        segments: Optional[tuple[BaseSegment, ...]] = None,\n        parent: Optional[BaseSegment] = None,\n        parent_idx: Optional[int] = None,\n    ) -> BaseSegment:\n        \"\"\"Copy the segment recursively, with appropriate copying of references.\n\n        Optionally provide child segments which have already been dealt\n        with to avoid another copy operation.\n\n        NOTE: In the copy operation it's really important that we get\n        a clean segregation so that we can't go backward and mutate the\n        source object, but at the same time we should be mindful of what\n        _needs_ to be copied to avoid a deep copy where one isn't required.\n        \"\"\"\n        cls = self.__class__\n        new_segment = cls.__new__(cls)\n        # Position markers are immutable, and it's important that we keep\n        # a reference to the same TemplatedFile, so keep the same position\n        # marker. By updating from the source dict, we achieve that.\n        # By using the __dict__ object we also transfer the _cache_ too\n        # which is stored there by @cached_property.\n        new_segment.__dict__.update(self.__dict__)\n\n        # Reset the parent if provided.\n        if parent:\n            assert parent_idx is not None, \"parent_idx must be provided it parent is.\"\n            new_segment.set_parent(parent, parent_idx)\n\n        # If the segment doesn't have a segments property, we're done.\n        # NOTE: This is a proxy way of understanding whether it's a RawSegment\n        # of not. Typically will _have_ a `segments` attribute, but it's an\n        # empty tuple.\n        if not self.__dict__.get(\"segments\", None):\n            assert (\n                not segments\n            ), f\"Cannot provide `segments` argument to {cls.__name__} `.copy()`\\n\"\n        # If segments were provided, use them.\n        elif segments:\n            new_segment.segments = segments\n        # Otherwise we should handle recursive segment coping.\n        # We use the native .copy() method (this method!) appropriately\n        # so that the same logic is applied in recursion.\n        # We set the parent for children directly on the copy method\n        # to ensure those line up properly.\n        else:\n            new_segment.segments = tuple(\n                seg.copy(parent=new_segment, parent_idx=idx)\n                for idx, seg in enumerate(self.segments)\n            )\n\n        return new_segment\n", "type": "function"}, {"name": "from_segment", "is_method": true, "class_name": "Query", "parameters": ["cls", "segment", "dialect", "parent"], "calls": ["segment.is_type", "segment.is_type", "cls", "segment.is_type", "list", "upper", "cls.from_segment", "Selectable", "segment.recursive_crawl", "segment.is_type", "segment.recursive_crawl", "segment.recursive_crawl", "cls._extract_subqueries", "next", "selectables.append", "selectables.append", "cte_defs.append", "name_seg.raw_normalized", "cte.recursive_crawl", "analysis_logger.info", "Selectable", "Selectable"], "code_location": {"file": "query.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/analysis", "start_line": 306, "end_line": 411}, "code_snippet": "    def from_segment(\n        cls: type[T],\n        segment: BaseSegment,\n        dialect: Dialect,\n        parent: Optional[T] = None,\n    ) -> T:\n        \"\"\"Recursively generate a query from an appropriate segment.\"\"\"\n        assert segment.is_type(\n            *SELECTABLE_TYPES, *SUBSELECT_TYPES\n        ), f\"Invalid segment for `from_segment`: {segment}\"\n\n        selectables = []\n        subqueries = []\n        cte_defs: list[BaseSegment] = []\n        query_type = QueryType.Simple\n\n        if segment.is_type(\"select_statement\", *SUBSELECT_TYPES):\n            # It's a select. Instantiate a Query.\n            selectables = [Selectable(segment, dialect=dialect)]\n        elif segment.is_type(\"set_expression\"):\n            # It's a set expression. There may be multiple selectables.\n            for _seg in segment.recursive_crawl(\"select_statement\", recurse_into=False):\n                selectables.append(Selectable(_seg, dialect=dialect))\n        else:\n            # Otherwise it's a WITH statement.\n            assert segment.is_type(\"with_compound_statement\")\n            query_type = QueryType.WithCompound\n            for _seg in segment.recursive_crawl(\n                # NOTE: We don't _specify_ set expressions here, because\n                # all set expressions are made of selects, and we\n                # want to look straight through to those child\n                # expressions.\n                \"select_statement\",\n                recurse_into=False,\n                no_recursive_seg_type=\"common_table_expression\",\n            ):\n                selectables.append(Selectable(_seg, dialect=dialect))\n\n            # We also need to handle CTEs\n            for _seg in segment.recursive_crawl(\n                \"common_table_expression\",\n                recurse_into=False,\n                # Don't recurse into any other WITH statements.\n                no_recursive_seg_type=\"with_compound_statement\",\n            ):\n                # Just store the segments for now.\n                cte_defs.append(_seg)\n\n        # Extract subqueries from any selectables.\n        for selectable in selectables:\n            # NOTE: If any VALUES clauses are present, they pass through here\n            # safely without Exception. They won't yield any subqueries.\n            subqueries += list(cls._extract_subqueries(selectable, dialect))\n\n        # Instantiate the query\n        outer_query = cls(\n            query_type,\n            dialect,\n            selectables,\n            parent=parent,\n            subqueries=subqueries,\n        )\n\n        # If we don't have any CTEs, we can stop now.\n        if not cte_defs:\n            return outer_query\n\n        # Otherwise build up the CTE map.\n        ctes = {}\n        for cte in cte_defs:\n            # NOTE: This feels a little risky to just assume the first segment\n            # is the name, but it's the same functionality we've run with for\n            # a while.\n            name_seg = cte.segments[0]\n            name = name_seg.raw_normalized(False).upper()\n            # Get the query out of it, just stop on the first one we find.\n            try:\n                inner_qry = next(\n                    cte.recursive_crawl(\n                        *SELECTABLE_TYPES,\n                        \"values_clause\",\n                        # Very rarely, we might find things like update\n                        # clauses in here, handle them accordingly.\n                        *SUBSELECT_TYPES,\n                    ),\n                )\n            # If this fails it's because we didn't find anything \"selectable\"\n            # in the CTE. Flag this up, but then carry on. It's likely something\n            # strange (w.g. a Clickhouse WITH clause setting a with).\n            except StopIteration:  # pragma: no cover\n                # Log it as an issue, but otherwise skip this one.\n                analysis_logger.info(f\"Skipping unexpected CTE structure: {cte.raw!r}\")\n                continue\n            qry = cls.from_segment(inner_qry, dialect=dialect, parent=outer_query)\n            assert qry\n            # Populate the CTE specific args.\n            qry.cte_definition_segment = cte\n            qry.cte_name_segment = name_seg\n            # File it in the dictionary.\n            ctes[name] = qry\n\n        # Set the CTEs attribute on the outer.\n        # NOTE: Because we're setting this after instantiation, it's important\n        # that we've already set the `parent` value of the cte queries.\n        outer_query.ctes = ctes\n        return outer_query\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0966548919677734}
{"question": "Why does the list comprehension in the plugin hook that converts the generator-based templater discovery into a list impact memory allocation efficiency compared to directly returning the generator?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "core_templaters", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 13, "end_line": 20}, "code_snippet": "def core_templaters() -> Iterator[type[RawTemplater]]:\n    \"\"\"Returns the templater tuples for the core templaters.\"\"\"\n    yield from [\n        RawTemplater,\n        JinjaTemplater,\n        PythonTemplater,\n        PlaceholderTemplater,\n    ]\n", "type": "function"}, {"name": "get_templaters", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt", "start_line": 8, "end_line": 10}, "code_snippet": "def get_templaters():\n    \"\"\"Get templaters.\"\"\"\n    return [DbtTemplater]\n", "type": "function"}, {"name": "get_templaters", "is_method": false, "class_name": null, "parameters": [], "calls": ["list", "core_templaters"], "code_location": {"file": "lib.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/plugin", "start_line": 25, "end_line": 28}, "code_snippet": "def get_templaters() -> list[type[RawTemplater]]:\n    \"\"\"Get templaters.\"\"\"\n    templaters = list(t for t in core_templaters())\n    return templaters\n", "type": "function"}, {"name": "DerivedJinjaTemplater", "docstring": "A templater that includes some custom Jinja tags.\n\nThis is used for tests that show the templater can be extended for custom plugin\ntemplaters that support custom tags.", "methods": ["_get_jinja_env", "_get_jinja_analyzer"], "attributes": ["name"], "code_location": {"file": "jinja_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/templaters", "start_line": 1041, "end_line": 1056}, "type": "class"}, {"name": "JinjaTemplater", "docstring": "A templater using the jinja2 library.\n\nSee: https://jinja.palletsprojects.com/", "methods": ["_extract_macros_from_template", "_extract_macros_from_path", "_extract_macros_from_config", "_extract_libraries_from_config", "_crawl_tree", "_get_jinja_env", "_get_macros_path", "_get_loader_search_path", "_get_jinja_analyzer", "_apply_dbt_builtins", "_get_env_context", "construct_render_func", "_generate_violations_for_undefined_variables", "_init_undefined_tracking", "process", "slice_file", "_rectify_templated_slices", "_calculate_variant_score", "_handle_unreached_code", "process_with_variants", "_exclude_macros"], "attributes": ["name"], "code_location": {"file": "jinja.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 95, "end_line": 1079}, "type": "class"}, {"name": "_discover_plugins", "is_method": false, "class_name": null, "parameters": [], "calls": ["list", "importlib.metadata.distributions"], "code_location": {"file": "host.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/plugin", "start_line": 42, "end_line": 53}, "code_snippet": "def _discover_plugins() -> Iterator[tuple[importlib.metadata.EntryPoint, str, str]]:\n    \"\"\"Uses the same mechanism as pluggy to introspect available plugins.\n\n    This method is then intended to allow loading of plugins individually,\n    for better error handling.\n    \"\"\"\n    for dist in list(importlib.metadata.distributions()):\n        for ep in dist.entry_points:\n            # Check it's a SQLFluff one\n            if ep.group != project_name:\n                continue\n            yield ep, ep.name, dist.version\n", "type": "function"}, {"name": "get_templater", "is_method": true, "class_name": "FluffConfig", "parameters": ["self"], "calls": ["self.get_templater_class"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 470, "end_line": 472}, "code_snippet": "    def get_templater(self, **kwargs: Any) -> RawTemplater:\n        \"\"\"Instantiate the configured templater.\"\"\"\n        return self.get_templater_class()(**kwargs)\n", "type": "function"}, {"name": "PythonTemplater", "docstring": "A templater using python format strings.\n\nSee: https://docs.python.org/3/library/string.html#format-string-syntax\n\nFor the python templater we don't allow functions or macros because there isn't\na good way of doing it securely. Use the jinja templater for this.\n\nThe python templater also defines a lot of the logic for how\nto allow fixing and translation in a templated file.", "methods": ["__init__", "infer_type", "get_context", "process", "slice_file", "_check_for_wrapped", "_substring_occurrences", "_sorted_occurrence_tuples", "_slice_template", "_split_invariants", "_filter_occurrences", "_coalesce_types", "_split_uniques_coalesce_rest"], "attributes": ["name"], "code_location": {"file": "python.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 152, "end_line": 1120}, "type": "class"}, {"name": "get_templaters", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "reindent_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/utils/reflow", "start_line": 76, "end_line": 78}, "code_snippet": "def get_templaters() -> list[type[RawTemplater]]:\n    \"\"\"Return templaters provided by this test module.\"\"\"\n    return [SpecialMarkerInserter]\n", "type": "function"}, {"name": "get_templater_class", "is_method": true, "class_name": "FluffConfig", "parameters": ["self"], "calls": ["get", "isinstance", "chain.from_iterable", "SQLFluffUserError", "self._plugin_manager.hook.get_templaters", "config_logger.warning", "format", "join", "templater_lookup.keys"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 432, "end_line": 468}, "code_snippet": "    def get_templater_class(self) -> type[RawTemplater]:\n        \"\"\"Get the configured templater class.\n\n        .. note::\n           This is mostly useful to call directly when rules want to determine\n           the *type* of a templater without (in particular to work out if it's a\n           derivative of the jinja templater), without needing to instantiate a\n           full templater. Instantiated templaters don't pickle well, so aren't\n           automatically passed around between threads/processes.\n        \"\"\"\n        templater_lookup: dict[str, type[RawTemplater]] = {\n            templater.name: templater\n            for templater in chain.from_iterable(\n                self._plugin_manager.hook.get_templaters()\n            )\n        }\n        # Fetch the config value.\n        templater_name = self._configs[\"core\"].get(\"templater\", \"<no value set>\")\n        assert isinstance(\n            templater_name, str\n        ), f\"Config value `templater` expected to be a string. Not: {templater_name!r}\"\n        try:\n            cls = templater_lookup[templater_name]\n            # Return class. Do not instantiate yet. That happens in `get_templater()`\n            # for situations which require it.\n            return cls\n        except KeyError:\n            if templater_name == \"dbt\":  # pragma: no cover\n                config_logger.warning(\n                    \"Starting in sqlfluff version 0.7.0 the dbt templater is \"\n                    \"distributed as a separate python package. Please pip install \"\n                    \"sqlfluff-templater-dbt to use it.\"\n                )\n            raise SQLFluffUserError(\n                \"Requested templater {!r} which is not currently available. Try one of \"\n                \"{}\".format(templater_name, \", \".join(templater_lookup.keys()))\n            )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1070351600646973}
{"question": "Why does the statement segment class that defines the grammar pattern for parsing DROP SEARCH INDEX statements enable BigQuery dialect-specific SQL parsing and validation in the sqlfluff framework?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "DropSearchIndexStatementSegment", "docstring": "A `DROP SEARCH INDEX` statement.\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_search_index", "methods": [], "attributes": ["type"], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3091, "end_line": 3107}, "type": "class"}, {"name": "CreateSearchIndexStatementSegment", "docstring": "A `CREATE SEARCH INDEX` statement.\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_search_index_statement", "methods": [], "attributes": ["type"], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3064, "end_line": 3088}, "type": "class"}, {"name": "DropVectorIndexStatementSegment", "docstring": "A `DROP VECTOR INDEX` statement.\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_vector_index", "methods": [], "attributes": ["type"], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3135, "end_line": 3151}, "type": "class"}, {"name": "DropIndexStatementSegment", "docstring": "A `DROP INDEX` statement.\n\nOverriding ANSI to include required ON clause.", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2051, "end_line": 2065}, "type": "class"}, {"name": "DropIndexStatementSegment", "docstring": "A `DROP INDEX` statement.", "methods": [], "attributes": [], "code_location": {"file": "dialect_ansi.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3550, "end_line": 3561}, "type": "class"}, {"name": "DropIndexStatementSegment", "docstring": "A `DROP INDEX` statement.\n\nhttps://www.postgresql.org/docs/15/sql-dropindex.html\nhttps://github.com/postgres/postgres/blob/4380c2509d51febad34e1fac0cfaeb98aaa716c5/src/backend/parser/gram.y#L6698-L6719\nhttps://github.com/postgres/postgres/blob/4380c2509d51febad34e1fac0cfaeb98aaa716c5/src/backend/parser/gram.y#L6808-L6829", "methods": [], "attributes": [], "code_location": {"file": "dialect_postgres.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 4580, "end_line": 4595}, "type": "class"}, {"name": "DropIndexStatementSegment", "docstring": "A `DROP INDEX` statement.\n\nhttps://dev.mysql.com/doc/refman/8.0/en/drop-index.html", "methods": [], "attributes": ["match_grammar", "match_grammar", "type"], "code_location": {"file": "dialect_mysql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2464, "end_line": 2492}, "type": "class"}, {"name": "CreateVectorIndexStatementSegment", "docstring": "A `CREATE VECTOR INDEX` statement.\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_vector_index_statement", "methods": [], "attributes": ["type"], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3110, "end_line": 3132}, "type": "class"}, {"name": "DropMaterializedViewStatementSegment", "docstring": "A `DROP MATERIALIZED VIEW` statement.\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_materialized_view_statement", "methods": [], "attributes": ["type", "match_grammar", "type", "match_grammar", "type", "match_grammar"], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2447, "end_line": 2461}, "type": "class"}, {"name": "DropReservationStatementSegment", "docstring": "A `DROP RESERVATION` statement.\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_reservation_statement", "methods": [], "attributes": ["type"], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3300, "end_line": 3313}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.1228077411651611}
{"question": "How should the factory method that constructs depth maps from raw segments and a root segment be refactored to leverage caching optimizations similar to the factory method that uses cached ancestor information from a parent segment?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "from_raws_and_root", "is_method": true, "class_name": "DepthMap", "parameters": ["cls", "raw_segments", "root_segment"], "calls": ["cls", "root_segment.path_to", "buff.append"], "code_location": {"file": "depthmap.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 141, "end_line": 156}, "code_snippet": "    def from_raws_and_root(\n        cls: type[\"DepthMap\"],\n        raw_segments: Sequence[RawSegment],\n        root_segment: BaseSegment,\n    ) -> \"DepthMap\":\n        \"\"\"Generate a DepthMap a sequence of raws and a root.\n\n        NOTE: This is the less efficient way to construct a DepthMap\n        as it doesn't take advantage of caching in the same way as\n        `from_parent`.\n        \"\"\"\n        buff = []\n        for raw in raw_segments:\n            stack = root_segment.path_to(raw)\n            buff.append((raw, stack))\n        return cls(raws_with_stack=buff)\n", "type": "function"}, {"name": "from_parent", "is_method": true, "class_name": "DepthMap", "parameters": ["cls", "parent"], "calls": ["cls"], "code_location": {"file": "depthmap.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 132, "end_line": 138}, "code_snippet": "    def from_parent(cls: type[\"DepthMap\"], parent: BaseSegment) -> \"DepthMap\":\n        \"\"\"Generate a DepthMap from all the children of a segment.\n\n        NOTE: This is the most efficient way to construct a DepthMap\n        due to caching in the BaseSegment.\n        \"\"\"\n        return cls(raws_with_stack=parent.raw_segments_with_ancestors)\n", "type": "function"}, {"name": "from_raw_segments", "is_method": true, "class_name": "ReflowSequence", "parameters": ["cls", "segments", "root_segment", "config", "depth_map"], "calls": ["ReflowConfig.from_fluff_config", "cls", "DepthMap.from_raws_and_root", "cls._elements_from_raw_segments"], "code_location": {"file": "sequence.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 175, "end_line": 206}, "code_snippet": "    def from_raw_segments(\n        cls: type[\"ReflowSequence\"],\n        segments: Sequence[RawSegment],\n        root_segment: BaseSegment,\n        config: FluffConfig,\n        depth_map: Optional[DepthMap] = None,\n    ) -> \"ReflowSequence\":\n        \"\"\"Construct a ReflowSequence from a sequence of raw segments.\n\n        This is intended as a base constructor, which others can use.\n        In particular, if no `depth_map` argument is provided, this\n        method will generate one in a potentially inefficient way.\n        If the calling method has access to a better way of inferring\n        a depth map (for example because it has access to a common root\n        segment for all the content), it should do that instead and pass\n        it in.\n        \"\"\"\n        reflow_config = ReflowConfig.from_fluff_config(config)\n        if depth_map is None:\n            depth_map = DepthMap.from_raws_and_root(segments, root_segment)\n        return cls(\n            elements=cls._elements_from_raw_segments(\n                segments,\n                reflow_config=reflow_config,\n                # NOTE: This pathway is inefficient. Ideally the depth\n                # map should be constructed elsewhere and then passed in.\n                depth_map=depth_map,\n            ),\n            root_segment=root_segment,\n            reflow_config=reflow_config,\n            depth_map=depth_map,\n        )\n", "type": "function"}, {"name": "test_reflow_depthmap_from_parent", "is_method": false, "class_name": null, "parameters": ["default_config"], "calls": ["parse_ansi_string", "DepthMap.from_parent", "all", "all", "len", "select_keyword_di.common_with"], "code_location": {"file": "depthmap_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/utils/reflow", "start_line": 13, "end_line": 49}, "code_snippet": "def test_reflow_depthmap_from_parent(default_config):\n    \"\"\"Test map construction from a root segment.\"\"\"\n    sql = \"SELECT 1\"\n    root = parse_ansi_string(sql, default_config)\n\n    dm = DepthMap.from_parent(root)\n\n    # We use UUIDS in the depth map so we can't assert their value.\n    # What we can do is use them.\n\n    # Check that we get the right depths.\n    assert [dm.depth_info[seg.uuid].stack_depth for seg in root.raw_segments] == [\n        4,\n        4,\n        4,\n        5,\n        4,\n        1,\n    ]\n    # Check they all share the same first three hash and\n    # class type elements (except the end of file marker at the end).\n    # These should be the file, statement and select statement.\n    expected = ({\"file\", \"base\"}, {\"statement\", \"base\"}, {\"select_statement\", \"base\"})\n    assert all(\n        dm.depth_info[seg.uuid].stack_class_types[:3] == expected\n        for seg in root.raw_segments[:-1]\n    )\n    first_hashes = dm.depth_info[root.raw_segments[0].uuid].stack_hashes[:3]\n    assert all(\n        dm.depth_info[seg.uuid].stack_hashes[:3] == first_hashes\n        for seg in root.raw_segments[:-1]\n    )\n\n    # While we're here, test the DepthInfo.common_with method\n    select_keyword_di = dm.depth_info[root.raw_segments[0].uuid]\n    numeric_one_di = dm.depth_info[root.raw_segments[3].uuid]\n    assert len(select_keyword_di.common_with(numeric_one_di)) == 4\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "DepthMap", "parameters": ["self", "raws_with_stack"], "calls": ["DepthInfo.from_raw_and_stack"], "code_location": {"file": "depthmap.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 126, "end_line": 129}, "code_snippet": "    def __init__(self, raws_with_stack: Sequence[tuple[RawSegment, list[PathStep]]]):\n        self.depth_info = {}\n        for raw, stack in raws_with_stack:\n            self.depth_info[raw.uuid] = DepthInfo.from_raw_and_stack(raw, stack)\n", "type": "function"}, {"name": "DepthMap", "docstring": "A mapping of raw segments to depth and parent information.\n\nThis class addresses two needs:\n- To understand configuration of segments with no whitespace\n  within them - so the config is related to the parent and\n  not the segment)\n- To map the depth of an indent points to apply some precedence\n  for where to insert line breaks.\n\nThe internals are structured around a list to do lookups\nand a dict (keyed with the raw segment UUID) to hold the rest.", "methods": ["__init__", "from_parent", "from_raws_and_root", "get_depth_info", "copy_depth_info"], "attributes": [], "code_location": {"file": "depthmap.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 111, "end_line": 182}, "type": "class"}, {"name": "from_root", "is_method": true, "class_name": "ReflowSequence", "parameters": ["cls", "root_segment", "config"], "calls": ["cls.from_raw_segments", "DepthMap.from_parent"], "code_location": {"file": "sequence.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 209, "end_line": 226}, "code_snippet": "    def from_root(\n        cls: type[\"ReflowSequence\"], root_segment: BaseSegment, config: FluffConfig\n    ) -> \"ReflowSequence\":\n        \"\"\"Generate a sequence from a root segment.\n\n        Args:\n            root_segment (:obj:`BaseSegment`): The relevant root\n                segment (usually the base :obj:`FileSegment`).\n            config (:obj:`FluffConfig`): A config object from which\n                to load the spacing behaviours of different segments.\n        \"\"\"\n        return cls.from_raw_segments(\n            root_segment.raw_segments,\n            root_segment,\n            config=config,\n            # This is the efficient route. We use it here because we can.\n            depth_map=DepthMap.from_parent(root_segment),\n        )\n", "type": "function"}, {"name": "DepthInfo", "docstring": "An object to hold the depth information for a specific raw segment.", "methods": ["from_raw_and_stack", "common_with", "trim"], "attributes": [], "code_location": {"file": "depthmap.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 54, "end_line": 108}, "type": "class"}, {"name": "copy_depth_info", "is_method": true, "class_name": "DepthMap", "parameters": ["self", "anchor", "new_segment", "trim"], "calls": ["trim", "self.get_depth_info"], "code_location": {"file": "depthmap.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 169, "end_line": 182}, "code_snippet": "    def copy_depth_info(\n        self, anchor: RawSegment, new_segment: RawSegment, trim: int = 0\n    ) -> None:\n        \"\"\"Copy the depth info for one segment and apply to another.\n\n        This mutates the existing depth map. That's ok because it's\n        an idempotent operation and uuids should be unique.\n\n        This is used in edits to a reflow sequence when new segments are\n        inserted and can't infer their own depth info.\n\n        NOTE: we don't remove the old one because it causes no harm.\n        \"\"\"\n        self.depth_info[new_segment.uuid] = self.get_depth_info(anchor).trim(trim)\n", "type": "function"}, {"name": "test__parser__raw_segments_with_ancestors", "is_method": false, "class_name": null, "parameters": ["raw_segments", "DummySegment", "DummyAuxSegment"], "calls": ["DummySegment", "DummyAuxSegment", "PathStep", "PathStep", "PathStep"], "code_location": {"file": "segments_raw_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser/segments", "start_line": 12, "end_line": 30}, "code_snippet": "def test__parser__raw_segments_with_ancestors(\n    raw_segments, DummySegment, DummyAuxSegment\n):\n    \"\"\"Test raw_segments_with_ancestors.\n\n    This is used in the reflow module to assess parse depth.\n    \"\"\"\n    test_seg = DummySegment([DummyAuxSegment(raw_segments[:1]), raw_segments[1]])\n    # Result should be the same raw segment, but with appropriate parents\n    assert test_seg.raw_segments_with_ancestors == [\n        (\n            raw_segments[0],\n            [\n                PathStep(test_seg, 0, 2, (0, 1)),\n                PathStep(test_seg.segments[0], 0, 1, (0,)),\n            ],\n        ),\n        (raw_segments[1], [PathStep(test_seg, 1, 2, (0, 1))]),\n    ]\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1346888542175293}
{"question": "Why does the BigQuery dialect's function name segment class override the ANSI base class's match_grammar attribute with allow_gaps set to True?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "FunctionNameSegment", "docstring": "Function name, including any prefix bits, e.g. project or schema.\n\nOverriding FunctionNameSegment to support Snowflake's IDENTIFIER pseudo-function.", "methods": [], "attributes": ["type", "type", "type", "type", "type"], "code_location": {"file": "dialect_snowflake.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 921, "end_line": 954}, "type": "class"}, {"name": "BareFunctionSegment", "docstring": "A function that can be called without parenthesis per ANSI specification.\n\nDB2 extends this to include `special registers`.", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_db2.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 257, "end_line": 264}, "type": "class"}, {"name": "FunctionNameSegment", "docstring": "Describes the name of a function.\n\nThis includes any prefix bits, e.g. project, schema or the SAFE keyword.", "methods": [], "attributes": [], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1067, "end_line": 1095}, "type": "class"}, {"name": "ColumnReferenceSegment", "docstring": "A reference to column, field or alias.\n\nWe override this for BigQuery to allow keywords in structures\n(using Full segments) and to properly return references for objects.\n\nRef: https://cloud.google.com/bigquery/docs/reference/standard-sql/lexical\n\"A reserved keyword must be a quoted identifier if it is a standalone\nkeyword or the first component of a path expression. It may be unquoted\nas the second or later component of a path expression.\"", "methods": [], "attributes": [], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1483, "end_line": 1567}, "type": "class"}, {"name": "SplittableObjectReferenceGrammar", "docstring": "An extended object reference grammar for BigQuery.\n\nThis class customizes the splitting of object references (such as table or column\nnames) to handle BigQuery's syntax, where object names may be quoted and\ncan refer to columns, tables, datasets, or projects. In BigQuery, object references\ncan be multi-part (e.g., `project.dataset.table.column`) and may include quoted\nidentifiers that contain keywords or special characters.", "methods": ["_iter_reference_parts"], "attributes": [], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1463, "end_line": 1480}, "type": "class"}, {"name": "CreateFunctionStatementSegment", "docstring": "A `CREATE FUNCTION` statement.\n\nThis version in the ANSI dialect should be a \"common subset\" of the\nstructure of the code for those dialects.\npostgres: https://www.postgresql.org/docs/9.1/sql-createfunction.html\nsnowflake: https://docs.snowflake.com/en/sql-reference/sql/create-function.html\nbigquery:\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions", "methods": [], "attributes": [], "code_location": {"file": "dialect_ansi.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3946, "end_line": 3973}, "type": "class"}, {"name": "FunctionSegment", "docstring": "A scalar or aggregate function.\n\nExtended version of `ansi` to add support of row typecasting\nhttps://prestodb.io/docs/current/language/types.html#row\n```\ncast(row(val1, val2) as row(a integer, b integer))\n```", "methods": [], "attributes": ["type", "match_grammar", "match_grammar", "type", "type", "type", "match_grammar"], "code_location": {"file": "dialect_hive.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 885, "end_line": 935}, "type": "class"}, {"name": "CubeFunctionNameSegment", "docstring": "ROLLUP function name segment.\n\nNeed to be able to specify this as type `function_name_identifier`\nwithin a `function_name` so that linting rules identify it properly.", "methods": [], "attributes": ["type"], "code_location": {"file": "dialect_ansi.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2548, "end_line": 2560}, "type": "class"}, {"name": "RankFunctionNameSegment", "docstring": "Rank function name segment.\n\nNeed to be able to specify this as type function_name\nso that linting rules identify it properly", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3123, "end_line": 3131}, "type": "class"}, {"name": "RollupFunctionNameSegment", "docstring": "ROLLUP function name segment.\n\nNeed to be able to specify this as type `function_name_identifier`\nwithin a `function_name` so that linting rules identify it properly.", "methods": [], "attributes": ["type"], "code_location": {"file": "dialect_ansi.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2533, "end_line": 2545}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.129540205001831}
{"question": "What architectural mechanism in the main linting class decouples SQL dialect-specific grammar definitions from the rule-based validation process?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "SQLLintError", "docstring": "An error which occurred during linting.\n\nIn particular we reference the rule here to do extended logging based on\nthe rule in question which caused the fail.\n\nArgs:\n    segment (:obj:`BaseSegment`, optional): The segment which is relevant\n        for the failure in parsing. This is likely to be a subclass of\n        `BaseSegment` rather than the parent class itself. This is mostly\n        used for logging and for referencing position.", "methods": ["__init__", "__reduce__", "to_dict", "fixable", "rule_code", "rule_name", "source_signature", "__repr__"], "attributes": ["_identifier"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 249, "end_line": 383}, "type": "class"}, {"name": "test__api__info_rules", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.list_rules", "isinstance", "RuleTuple"], "code_location": {"file": "info_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 29, "end_line": 52}, "code_snippet": "def test__api__info_rules():\n    \"\"\"Basic linting of dialects.\"\"\"\n    rules = sqlfluff.list_rules()\n    assert isinstance(rules, list)\n    assert (\n        RuleTuple(\n            code=\"LT01\",\n            name=\"layout.spacing\",\n            description=\"Inappropriate Spacing.\",\n            groups=(\"all\", \"core\", \"layout\"),\n            aliases=(\n                \"L001\",\n                \"L005\",\n                \"L006\",\n                \"L008\",\n                \"L023\",\n                \"L024\",\n                \"L039\",\n                \"L048\",\n                \"L071\",\n            ),\n        )\n        in rules\n    )\n", "type": "function"}, {"name": "Dialect", "docstring": "Serves as the basis for runtime resolution of Grammar.\n\nArgs:\n    name (:obj:`str`): The name of the dialect, used for lookup.\n    lexer_matchers (iterable of :obj:`StringLexer`): A structure defining\n        the lexing config for this dialect.", "methods": ["__init__", "__repr__", "expand", "sets", "bracket_sets", "update_keywords_set_from_multiline_string", "copy_as", "add", "replace", "add_update_segments", "get_grammar", "get_segment", "ref", "set_lexer_matchers", "get_lexer_matchers", "patch_lexer_matchers", "insert_lexer_matchers", "get_root_segment"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "start_line": 18, "end_line": 397}, "type": "class"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "SQLFluffDriver", "parameters": ["self"], "calls": ["__init__", "super", "s.encode", "sys.getfilesystemencoding"], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 25, "end_line": 34}, "code_snippet": "    def __init__(self) -> None:\n        super().__init__(\n            [sys.executable, \"-m\", \"sqlfluff.cli.commands\"],\n            [\".sql\"],\n            [\n                s.encode(sys.getfilesystemencoding())\n                for s in [\"sqlfluff\", \"lint\", \"--format=json\"]\n            ],\n            exit_codes=[0, 1],\n        )\n", "type": "function"}, {"name": "test__api__info_dialects", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.list_dialects", "isinstance"], "code_location": {"file": "info_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 7, "end_line": 26}, "code_snippet": "def test__api__info_dialects():\n    \"\"\"Basic linting of dialects.\"\"\"\n    dialects = sqlfluff.list_dialects()\n    assert isinstance(dialects, list)\n    # Turn it into a dict so we can look for items in there.\n    dialect_dict = {dialect.label: dialect for dialect in dialects}\n    # Check the ansi dialect works\n    assert \"ansi\" in dialect_dict\n    ansi = dialect_dict[\"ansi\"]\n    assert ansi.label == \"ansi\"\n    assert ansi.name == \"ANSI\"\n    assert ansi.inherits_from == \"nothing\"\n    assert \"This is the base dialect\" in ansi.docstring\n    # Check one other works\n    assert \"postgres\" in dialect_dict\n    postgres = dialect_dict[\"postgres\"]\n    assert postgres.label == \"postgres\"\n    assert postgres.name == \"PostgreSQL\"\n    assert postgres.inherits_from == \"ansi\"\n    assert \"this is often the dialect to use\" in postgres.docstring\n", "type": "function"}, {"name": "DummyLintError", "docstring": "Fake lint error used by tests, similar to SQLLintError.", "methods": [], "attributes": [], "code_location": {"file": "noqa_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 15, "end_line": 20}, "type": "class"}, {"name": "DummyLintError", "docstring": "Fake lint error used by tests, similar to SQLLintError.", "methods": ["__init__", "__init__"], "attributes": [], "code_location": {"file": "linter_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 26, "end_line": 31}, "type": "class"}, {"name": "TestFlinkSQLDialect", "docstring": "Test FlinkSQL dialect parsing.", "methods": ["test_flink_dialect_basic", "test_flink_create_table_basic", "test_flink_row_data_type", "test_flink_timestamp_with_precision", "test_flink_watermark_definition", "test_flink_computed_column", "test_flink_metadata_column", "test_flink_show_statements", "test_flink_use_statements", "test_flink_describe_statement", "test_flink_explain_statement", "test_flink_create_catalog", "test_flink_create_database", "test_flink_alternative_with_syntax"], "attributes": [], "code_location": {"file": "flink_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 6, "end_line": 231}, "type": "class"}, {"name": "_parse_tokens", "is_method": true, "class_name": "Linter", "parameters": ["tokens", "config", "fname", "parse_statistics"], "calls": ["Parser", "linter_logger.info", "linter_logger.info", "parsed.iter_unparsables", "parser.parse", "format", "violations.append", "linter_logger.info", "linter_logger.info", "tuple", "linter_logger.info", "violations.append", "parsed.stringify", "SQLParseError", "unparsable.stringify", "format", "len"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 231, "end_line": 281}, "code_snippet": "    def _parse_tokens(\n        tokens: Sequence[BaseSegment],\n        config: FluffConfig,\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> tuple[Optional[BaseSegment], list[SQLParseError]]:\n        parser = Parser(config=config)\n        violations = []\n        # Parse the file and log any problems\n        try:\n            parsed: Optional[BaseSegment] = parser.parse(\n                # Regardless of how the sequence was passed in, we should\n                # coerce it to a tuple here, before we head deeper into\n                # the parsing process.\n                tuple(tokens),\n                fname=fname,\n                parse_statistics=parse_statistics,\n            )\n        except SQLParseError as err:\n            linter_logger.info(\"PARSING FAILED! : %s\", err)\n            violations.append(err)\n            return None, violations\n\n        if parsed is None:  # pragma: no cover\n            return None, violations\n\n        linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n        linter_logger.info(\"\\n\" + parsed.stringify())\n        # We may succeed parsing, but still have unparsable segments. Extract them\n        # here.\n        for unparsable in parsed.iter_unparsables():\n            # No exception has been raised explicitly, but we still create one here\n            # so that we can use the common interface\n            assert unparsable.pos_marker\n            violations.append(\n                SQLParseError(\n                    \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n                    \"{1!r}\".format(\n                        unparsable.pos_marker.working_loc,\n                        (\n                            unparsable.raw\n                            if len(unparsable.raw) < 40\n                            else unparsable.raw[:40] + \"...\"\n                        ),\n                    ),\n                    segment=unparsable,\n                )\n            )\n            linter_logger.info(\"Found unparsable segment...\")\n            linter_logger.info(unparsable.stringify())\n        return parsed, violations\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1676099300384521}
{"question": "How does the method that returns column and table analysis information handle treating Data Manipulation Language statements and values clause segments as SELECT statements?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "AnalyzeTableSegment", "docstring": "An `ANALYZE {TABLE | TABLES}` statement.\n\nhttps://spark.apache.org/docs/latest/sql-ref-syntax-aux-analyze-table.html", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_sparksql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2565, "end_line": 2621}, "type": "class"}, {"name": "get_select_statement_info", "is_method": false, "class_name": null, "parameters": ["segment", "dialect", "early_exit"], "calls": ["segment.is_type", "get_aliases_from_select", "segment.get_child", "_get_object_references", "segment.get_child", "cast", "segment.get_child", "SelectStatementColumnsAndTables", "segment.get_child", "_select_clause.get_children", "s.get_alias", "fc.recursive_crawl", "fc.recursive_crawl", "_get_object_references", "table_expression.iter_segments", "join_clause.iter_segments", "seg.is_type", "_get_object_references", "is_qualified", "seg.is_type", "seg.is_type", "_get_object_references", "cast", "on_seg.is_type", "seg.is_type", "_get_object_references", "subseg.is_type", "using_cols.append"], "code_location": {"file": "select.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/analysis", "start_line": 39, "end_line": 128}, "code_snippet": "def get_select_statement_info(\n    segment: BaseSegment, dialect: Optional[Dialect], early_exit: bool = True\n) -> Optional[SelectStatementColumnsAndTables]:\n    \"\"\"Analyze a select statement: targets, aliases, etc. Return info.\"\"\"\n    assert segment.is_type(\"select_statement\")\n    table_aliases, standalone_aliases = get_aliases_from_select(segment, dialect)\n    if early_exit and not table_aliases and not standalone_aliases:\n        return None\n\n    # Iterate through all the references, both in the select clause, but also\n    # potential others.\n    sc = segment.get_child(\"select_clause\")\n    # Sometimes there is no select clause (e.g. \"SELECT *\" is a select_clause_element)\n    if not sc:  # pragma: no cover\n        # TODO: Review whether this clause should be removed. It might only\n        # have existed for an old way of structuring the Exasol dialect.\n        return None\n    # NOTE: In this first crawl, don't crawl inside any sub-selects, that's very\n    # important for both isolation and performance reasons.\n    reference_buffer = _get_object_references(sc)\n    table_reference_buffer = []\n    for potential_clause in (\n        \"where_clause\",\n        \"groupby_clause\",\n        \"having_clause\",\n        \"orderby_clause\",\n        \"qualify_clause\",\n    ):\n        clause = segment.get_child(potential_clause)\n        if clause:\n            reference_buffer += _get_object_references(clause)\n\n    # Get all select targets.\n    _select_clause = segment.get_child(\"select_clause\")\n    assert _select_clause, \"Select statement found without select clause.\"\n    select_targets = cast(\n        list[SelectClauseElementSegment],\n        _select_clause.get_children(\"select_clause_element\"),\n    )\n\n    # Get all column aliases. NOTE: In two steps so mypy can follow.\n    _pre_aliases = [s.get_alias() for s in select_targets]\n    col_aliases = [_alias for _alias in _pre_aliases if _alias is not None]\n\n    # Get any columns referred to in a using clause, and extract anything\n    # from ON clauses.\n    using_cols = []\n    fc = segment.get_child(\"from_clause\")\n    if fc:\n        for table_expression in fc.recursive_crawl(\n            \"table_expression\", no_recursive_seg_type=\"select_statement\"\n        ):\n            for seg in table_expression.iter_segments():\n                # table references can get tricky with what is a schema, table,\n                # project, or column. It may be best for now to use the redshift\n                # unnest logic for dialects that support arrays or objects/structs\n                # in AL05. However, this solves finding other types of references\n                # in functions such as LATERAL FLATTEN.\n                if not seg.is_type(\"table_reference\"):\n                    reference_buffer += _get_object_references(seg)\n                elif cast(ObjectReferenceSegment, seg).is_qualified():\n                    table_reference_buffer += _get_object_references(seg)\n        for join_clause in fc.recursive_crawl(\n            \"join_clause\", no_recursive_seg_type=\"select_statement\"\n        ):\n            seen_using = False\n            for seg in join_clause.iter_segments():\n                if seg.is_type(\"keyword\") and seg.raw_upper == \"USING\":\n                    seen_using = True\n                elif seg.is_type(\"join_on_condition\"):\n                    for on_seg in seg.segments:\n                        if on_seg.is_type(\"bracketed\", \"expression\"):\n                            # Deal with expressions\n                            reference_buffer += _get_object_references(seg)\n                elif seen_using and seg.is_type(\"bracketed\"):\n                    for subseg in seg.segments:\n                        if subseg.is_type(\"identifier\"):\n                            using_cols.append(subseg)\n                    seen_using = False\n\n    return SelectStatementColumnsAndTables(\n        select_statement=segment,\n        table_aliases=table_aliases or [],\n        standalone_aliases=standalone_aliases or [],\n        reference_buffer=reference_buffer,\n        select_targets=select_targets,\n        col_aliases=col_aliases,\n        using_cols=using_cols,\n        table_reference_buffer=table_reference_buffer,\n    )\n", "type": "function"}, {"name": "AnalyzeStatementSegment", "docstring": "Analyze Statement Segment.\n\nAs specified in https://www.postgresql.org/docs/13/sql-analyze.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_postgres.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 4875, "end_line": 4896}, "type": "class"}, {"name": "AnalyzeTableStatementSegment", "docstring": "An `ANALYZE TABLE` statement.\n\nhttps://dev.mysql.com/doc/refman/8.0/en/analyze-table.html", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_mysql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2650, "end_line": 2696}, "type": "class"}, {"name": "AnalyzeStatementSegment", "docstring": "An 'ANALYZE' statement.\n\nAs per docs https://trino.io/docs/current/sql/analyze.html", "methods": [], "attributes": ["type", "match_grammar", "type", "_option", "_tables_and_columns", "match_grammar"], "code_location": {"file": "dialect_trino.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 513, "end_line": 536}, "type": "class"}, {"name": "AnalizeSegment", "docstring": "ANALYZE statement.\n\nhttps://docs.vmware.com/en/VMware-Greenplum/6/greenplum-database/ref_guide-sql_commands-ANALYZE.html", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_greenplum.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 351, "end_line": 377}, "type": "class"}, {"name": "_should_ignore_reference", "is_method": true, "class_name": "Rule_RF01", "parameters": ["self", "reference", "selectable"], "calls": ["selectable.selectable.path_to", "any", "ps.segment.is_type"], "code_location": {"file": "RF01.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 160, "end_line": 172}, "code_snippet": "    def _should_ignore_reference(\n        self, reference: ObjectReferenceSegment, selectable: Selectable\n    ) -> bool:\n        ref_path = selectable.selectable.path_to(reference)\n        # Ignore references occurring in an \"INTO\" clause:\n        # - They are table references, not column references.\n        # - They are the target table, similar to an INSERT or UPDATE\n        #   statement, thus not expected to match a table in the FROM\n        #   clause.\n        if ref_path:\n            return any(ps.segment.is_type(\"into_table_clause\") for ps in ref_path)\n        else:\n            return False  # pragma: no cover\n", "type": "function"}, {"name": "select_info", "is_method": true, "class_name": "Selectable", "parameters": ["self"], "calls": ["self.selectable.is_type", "get_select_statement_info", "Segments", "first", "first", "AliasInfo", "SelectStatementColumnsAndTables", "sp.is_type", "sp.is_type", "bool", "values.children", "alias_expression.children"], "code_location": {"file": "query.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/analysis", "start_line": 69, "end_line": 109}, "code_snippet": "    def select_info(self) -> Optional[SelectStatementColumnsAndTables]:\n        \"\"\"Returns SelectStatementColumnsAndTables on the SELECT.\"\"\"\n        if self.selectable.is_type(\"select_statement\"):\n            return get_select_statement_info(\n                self.selectable, self.dialect, early_exit=False\n            )\n        else:  # DML or values_clause\n            # This is a bit dodgy, but a very useful abstraction. Here, we\n            # interpret a DML or values_clause segment as if it were a SELECT.\n            # Someday, we may need to tweak this, e.g. perhaps add a separate\n            # QueryType for this (depending on the needs of the rules that use\n            # it.\n            #\n            # For more info on the syntax and behavior of VALUES and its\n            # similarity to a SELECT statement with literal values (no table\n            # source), see the \"Examples\" section of the Postgres docs page:\n            # (https://www.postgresql.org/docs/8.2/sql-values.html).\n            values = Segments(self.selectable)\n            alias_expression = values.children().first(sp.is_type(\"alias_expression\"))\n            name = alias_expression.children().first(\n                sp.is_type(\"naked_identifier\", \"quoted_identifier\")\n            )\n            alias_info = AliasInfo(\n                name[0].raw if name else \"\",\n                name[0] if name else None,\n                bool(name),\n                self.selectable,\n                alias_expression[0] if alias_expression else None,\n                None,\n            )\n\n            return SelectStatementColumnsAndTables(\n                select_statement=self.selectable,\n                table_aliases=[alias_info],\n                standalone_aliases=[],\n                reference_buffer=[],\n                select_targets=[],\n                col_aliases=[],\n                using_cols=[],\n                table_reference_buffer=[],\n            )\n", "type": "function"}, {"name": "SelectStatementColumnsAndTables", "docstring": "Structure returned by get_select_statement_info().", "methods": [], "attributes": [], "code_location": {"file": "select.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/analysis", "start_line": 16, "end_line": 26}, "type": "class"}, {"name": "SelectClauseSegment", "docstring": "Overrides ANSI to allow INTO as a terminator.", "methods": [], "attributes": [], "code_location": {"file": "dialect_postgres.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1828, "end_line": 1857}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.1556715965270996}
{"question": "Why is the occurrence sorting method implemented as a static method within the Python format string templating class rather than as a standalone utility function?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test__templater_python_sorted_occurrence_tuples", "is_method": false, "class_name": null, "parameters": ["test", "result"], "calls": ["pytest.mark.parametrize", "PythonTemplater._sorted_occurrence_tuples"], "code_location": {"file": "python_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/templaters", "start_line": 169, "end_line": 171}, "code_snippet": "def test__templater_python_sorted_occurrence_tuples(test, result):\n    \"\"\"Test _sorted_occurrence_tuples.\"\"\"\n    assert PythonTemplater._sorted_occurrence_tuples(test) == result\n", "type": "function"}, {"name": "_sorted_occurrence_tuples", "is_method": true, "class_name": "PythonTemplater", "parameters": ["occurrences"], "calls": ["sorted", "occurrences.keys"], "code_location": {"file": "python.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 454, "end_line": 462}, "code_snippet": "    def _sorted_occurrence_tuples(\n        occurrences: dict[str, list[int]],\n    ) -> list[tuple[str, int]]:\n        \"\"\"Sort a dict of occurrences into a sorted list of tuples.\"\"\"\n        return sorted(\n            ((raw, idx) for raw in occurrences.keys() for idx in occurrences[raw]),\n            # Sort first by position, then by lexical (for stability)\n            key=lambda x: (x[1], x[0]),\n        )\n", "type": "function"}, {"name": "test__templater_python_substring_occurrences", "is_method": false, "class_name": null, "parameters": ["mainstr", "substrings", "positions"], "calls": ["pytest.mark.parametrize", "PythonTemplater._substring_occurrences", "isinstance"], "code_location": {"file": "python_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/templaters", "start_line": 150, "end_line": 155}, "code_snippet": "def test__templater_python_substring_occurrences(mainstr, substrings, positions):\n    \"\"\"Test _substring_occurrences.\"\"\"\n    occurrences = PythonTemplater._substring_occurrences(mainstr, substrings)\n    assert isinstance(occurrences, dict)\n    pos_test = [occurrences[substring] for substring in substrings]\n    assert pos_test == positions\n", "type": "function"}, {"name": "_slice_template", "is_method": true, "class_name": "PythonTemplater", "parameters": ["cls", "in_str"], "calls": ["Formatter", "fmt.parse", "cls._sorted_occurrence_tuples", "format", "len", "cls._substring_occurrences", "escape_chars.pop", "RawFileSlice", "len", "RawFileSlice", "RawFileSlice", "len", "RawFileSlice"], "code_location": {"file": "python.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 465, "end_line": 509}, "code_snippet": "    def _slice_template(cls, in_str: str) -> Iterator[RawFileSlice]:\n        \"\"\"Slice a templated python string into token tuples.\n\n        This uses Formatter() as per:\n        https://docs.python.org/3/library/string.html#string.Formatter\n        \"\"\"\n        fmt = Formatter()\n        in_idx = 0\n        for literal_text, field_name, format_spec, conversion in fmt.parse(in_str):\n            if literal_text:\n                escape_chars = cls._sorted_occurrence_tuples(\n                    cls._substring_occurrences(literal_text, [\"}\", \"{\"])\n                )\n                idx = 0\n                while escape_chars:\n                    first_char = escape_chars.pop()\n                    # Is there a literal first?\n                    if first_char[1] > idx:\n                        yield RawFileSlice(\n                            literal_text[idx : first_char[1]], \"literal\", in_idx\n                        )\n                        in_idx += first_char[1] - idx\n                    # Add the escaped\n                    idx = first_char[1] + len(first_char[0])\n                    # We double them here to make the raw\n                    yield RawFileSlice(\n                        literal_text[first_char[1] : idx] * 2, \"escaped\", in_idx\n                    )\n                    # Will always be 2 in this case.\n                    # This is because ALL escape sequences in the python formatter\n                    # are two characters which reduce to one.\n                    in_idx += 2\n                # Deal with last one (if present)\n                if literal_text[idx:]:\n                    yield RawFileSlice(literal_text[idx:], \"literal\", in_idx)\n                    in_idx += len(literal_text) - idx\n            # Deal with fields\n            if field_name:\n                constructed_token = \"{{{field_name}{conv}{spec}}}\".format(\n                    field_name=field_name,\n                    conv=f\"!{conversion}\" if conversion else \"\",\n                    spec=f\":{format_spec}\" if format_spec else \"\",\n                )\n                yield RawFileSlice(constructed_token, \"templated\", in_idx)\n                in_idx += len(constructed_token)\n", "type": "function"}, {"name": "PythonTemplater", "docstring": "A templater using python format strings.\n\nSee: https://docs.python.org/3/library/string.html#format-string-syntax\n\nFor the python templater we don't allow functions or macros because there isn't\na good way of doing it securely. Use the jinja templater for this.\n\nThe python templater also defines a lot of the logic for how\nto allow fixing and translation in a templated file.", "methods": ["__init__", "infer_type", "get_context", "process", "slice_file", "_check_for_wrapped", "_substring_occurrences", "_sorted_occurrence_tuples", "_slice_template", "_split_invariants", "_filter_occurrences", "_coalesce_types", "_split_uniques_coalesce_rest"], "attributes": ["name"], "code_location": {"file": "python.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 152, "end_line": 1120}, "type": "class"}, {"name": "test__templater_python_split_uniques_coalesce_rest", "is_method": false, "class_name": null, "parameters": ["split_file", "raw_occurrences", "templated_occurrences", "templated_str", "result", "caplog"], "calls": ["pytest.mark.parametrize", "caplog.at_level", "list", "PythonTemplater._split_uniques_coalesce_rest", "IntermediateFileSlice", "TemplatedFileSlice", "IntermediateFileSlice", "IntermediateFileSlice", "IntermediateFileSlice", "IntermediateFileSlice", "IntermediateFileSlice", "IntermediateFileSlice", "IntermediateFileSlice", "TemplatedFileSlice", "TemplatedFileSlice", "TemplatedFileSlice", "TemplatedFileSlice", "TemplatedFileSlice", "TemplatedFileSlice", "TemplatedFileSlice", "TemplatedFileSlice", "TemplatedFileSlice", "IntermediateFileSlice", "TemplatedFileSlice", "TemplatedFileSlice", "TemplatedFileSlice", "TemplatedFileSlice", "TemplatedFileSlice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "RawFileSlice", "RawFileSlice", "RawFileSlice", "RawFileSlice", "RawFileSlice", "RawFileSlice", "RawFileSlice", "RawFileSlice", "RawFileSlice", "RawFileSlice", "RawFileSlice", "RawFileSlice", "RawFileSlice", "RawFileSlice"], "code_location": {"file": "python_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/templaters", "start_line": 378, "end_line": 399}, "code_snippet": "def test__templater_python_split_uniques_coalesce_rest(\n    split_file, raw_occurrences, templated_occurrences, templated_str, result, caplog\n):\n    \"\"\"Test _split_uniques_coalesce_rest.\"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.templater\"):\n        resp = list(\n            PythonTemplater._split_uniques_coalesce_rest(\n                split_file,\n                raw_occurrences,\n                templated_occurrences,\n                templated_str,\n            )\n        )\n    # Check contiguous\n    prev_slice = None\n    for elem in result:\n        if prev_slice:\n            assert elem[1].start == prev_slice[0].stop\n            assert elem[2].start == prev_slice[1].stop\n        prev_slice = (elem[1], elem[2])\n    # check result\n    assert resp == result\n", "type": "function"}, {"name": "_split_invariants", "is_method": true, "class_name": "PythonTemplater", "parameters": ["cls", "raw_sliced", "literals", "raw_occurrences", "templated_occurrences", "templated_str"], "calls": ["sorted", "invariants.copy", "buffer.append", "IntermediateFileSlice", "IntermediateFileSlice", "len", "RawFileSlice", "slice", "slice", "len", "len", "templater_logger.debug", "invariants.remove", "IntermediateFileSlice", "offset_slice", "offset_slice", "len", "slice", "slice", "len", "len", "RawFileSlice", "sum", "len"], "code_location": {"file": "python.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 512, "end_line": 611}, "code_snippet": "    def _split_invariants(\n        cls,\n        raw_sliced: list[RawFileSlice],\n        literals: list[str],\n        raw_occurrences: dict[str, list[int]],\n        templated_occurrences: dict[str, list[int]],\n        templated_str: str,\n    ) -> Iterator[IntermediateFileSlice]:\n        \"\"\"Split a sliced file on its invariant literals.\n\n        We prioritise the _longest_ invariants first as they\n        are more likely to the the anchors.\n        \"\"\"\n        # Calculate invariants\n        invariants = [\n            literal\n            for literal in literals\n            if len(raw_occurrences[literal]) == 1\n            and len(templated_occurrences[literal]) == 1\n        ]\n        # Work through the invariants and make sure they appear\n        # in order.\n        for linv in sorted(invariants, key=len, reverse=True):\n            # Any invariants which have templated positions, relative\n            # to source positions, which aren't in order, should be\n            # ignored.\n\n            # Is this one still relevant?\n            if linv not in invariants:\n                continue  # pragma: no cover\n\n            source_pos, templ_pos = raw_occurrences[linv], templated_occurrences[linv]\n            # Copy the list before iterating because we're going to edit it.\n            for tinv in invariants.copy():\n                if tinv != linv:\n                    src_dir = source_pos > raw_occurrences[tinv]\n                    tmp_dir = templ_pos > templated_occurrences[tinv]\n                    # If it's not in the same direction in the source and template\n                    # remove it.\n                    if src_dir != tmp_dir:  # pragma: no cover\n                        templater_logger.debug(\n                            \"          Invariant found out of order: %r\", tinv\n                        )\n                        invariants.remove(tinv)\n\n        # Set up some buffers\n        buffer: list[RawFileSlice] = []\n        idx: Optional[int] = None\n        templ_idx = 0\n        # Loop through\n        for raw_file_slice in raw_sliced:\n            if raw_file_slice.raw in invariants:\n                if buffer:\n                    yield IntermediateFileSlice(\n                        \"compound\",\n                        slice(idx, raw_file_slice.source_idx),\n                        slice(templ_idx, templated_occurrences[raw_file_slice.raw][0]),\n                        buffer,\n                    )\n                buffer = []\n                idx = None\n                yield IntermediateFileSlice(\n                    \"invariant\",\n                    offset_slice(\n                        raw_file_slice.source_idx,\n                        len(raw_file_slice.raw),\n                    ),\n                    offset_slice(\n                        templated_occurrences[raw_file_slice.raw][0],\n                        len(raw_file_slice.raw),\n                    ),\n                    [\n                        RawFileSlice(\n                            raw_file_slice.raw,\n                            raw_file_slice.slice_type,\n                            templated_occurrences[raw_file_slice.raw][0],\n                        )\n                    ],\n                )\n                templ_idx = templated_occurrences[raw_file_slice.raw][0] + len(\n                    raw_file_slice.raw\n                )\n            else:\n                buffer.append(\n                    RawFileSlice(\n                        raw_file_slice.raw,\n                        raw_file_slice.slice_type,\n                        raw_file_slice.source_idx,\n                    )\n                )\n                if idx is None:\n                    idx = raw_file_slice.source_idx\n        # If we have a final buffer, yield it\n        if buffer:\n            yield IntermediateFileSlice(\n                \"compound\",\n                slice((idx or 0), (idx or 0) + sum(len(slc.raw) for slc in buffer)),\n                slice(templ_idx, len(templated_str)),\n                buffer,\n            )\n", "type": "function"}, {"name": "__hash__", "is_method": true, "class_name": "SourceFix", "parameters": ["self"], "calls": ["hash"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 65, "end_line": 68}, "code_snippet": "    def __hash__(self) -> int:\n        # Only hash based on the source slice, not the\n        # templated slice (which might change)\n        return hash((self.edit, self.source_slice.start, self.source_slice.stop))\n", "type": "function"}, {"name": "__post_init__", "is_method": true, "class_name": "PositionMarker", "parameters": ["self"], "calls": ["self.templated_position", "object.__setattr__", "object.__setattr__"], "code_location": {"file": "markers.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 39, "end_line": 48}, "code_snippet": "    def __post_init__(self) -> None:\n        # If the working position has not been explicitly set\n        # then infer it from the position in the templated file.\n        # This is accurate up until the point that any fixes have\n        # been applied.\n        if self.working_line_no == -1 or self.working_line_pos == -1:\n            line_no, line_pos = self.templated_position()\n            # Use the base method because we're working with a frozen class\n            object.__setattr__(self, \"working_line_no\", line_no)\n            object.__setattr__(self, \"working_line_pos\", line_pos)\n", "type": "function"}, {"name": "format_linting_stats", "is_method": true, "class_name": "OutputStreamFormatter", "parameters": ["self", "result", "verbose"], "calls": ["StringIO", "result.stats", "text_buffer.write", "text_buffer.write", "text_buffer.getvalue", "self.cli_table", "format"], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 486, "end_line": 519}, "code_snippet": "    def format_linting_stats(self, result, verbose=0) -> str:\n        \"\"\"Format a set of stats given a `LintingResult`.\"\"\"\n        text_buffer = StringIO()\n        all_stats = result.stats(EXIT_FAIL, EXIT_SUCCESS)\n        text_buffer.write(\"==== summary ====\\n\")\n        if verbose >= 2:\n            output_fields = [\n                \"files\",\n                \"violations\",\n                \"clean files\",\n                \"unclean files\",\n                \"avg per file\",\n                \"unclean rate\",\n                \"status\",\n            ]\n            special_formats = {\"unclean rate\": \"{0:.0%}\"}\n        else:\n            output_fields = [\"violations\", \"status\"]\n            special_formats = {}\n        # Generate content tuples, applying special formats for some fields\n        summary_content = [\n            (\n                key,\n                (\n                    special_formats[key].format(all_stats[key])\n                    if key in special_formats\n                    else all_stats[key]\n                ),\n            )\n            for key in output_fields\n        ]\n        # Render it all as a table\n        text_buffer.write(self.cli_table(summary_content, max_label_width=14))\n        return text_buffer.getvalue()\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1522328853607178}
{"question": "How does the method that merges configuration values in the spacing configuration dataclass handle precedence when multiple configuration sources provide conflicting values for spacing attributes?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "get_block_config", "is_method": true, "class_name": "ReflowConfig", "parameters": ["self", "block_class_types", "depth_info"], "calls": ["self.config_types.intersection", "BlockConfig", "enumerate", "block_config.incorporate", "self.config_types.intersection", "block_config.incorporate", "block_config.incorporate", "get", "get"], "code_location": {"file": "config.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 124, "end_line": 179}, "code_snippet": "    def get_block_config(\n        self,\n        block_class_types: AbstractSet[str],\n        depth_info: Optional[DepthInfo] = None,\n    ) -> BlockConfig:\n        \"\"\"Given the class types of a ReflowBlock return spacing config.\n\n        When fetching the config for a single class type for a simple block\n        we should just get an appropriate simple config back.\n        >>> cfg = ReflowConfig.from_dict({\"comma\": {\"spacing_before\": \"touch\"}})\n        >>> cfg.get_block_config({\"comma\"})  # doctest: +ELLIPSIS\n        BlockConfig(spacing_before='touch', spacing_after='single', ...)\n        \"\"\"\n        # set intersection to get the class types which matter\n        configured_types = self.config_types.intersection(block_class_types)\n        # Start with a default config.\n        block_config = BlockConfig()\n\n        # Update with the config from any specific classes.\n\n        # First: With the types of any parent segments where\n        # we're at one end (if depth info provided).\n        if depth_info:\n            parent_start, parent_end = True, True\n            for idx, key in enumerate(depth_info.stack_hashes[::-1]):\n                # Work out if we're allowed to claim the parent.\n                if depth_info.stack_positions[key].type not in (\"solo\", \"start\"):\n                    parent_start = False\n                if depth_info.stack_positions[key].type not in (\"solo\", \"end\"):\n                    parent_end = False\n                if not (parent_start or parent_end):\n                    break\n                # Get corresponding classes.\n                parent_classes = depth_info.stack_class_types[-1 - idx]\n                configured_parent_types = self.config_types.intersection(parent_classes)\n                # Claim the _before_ config if at the start.\n                if parent_start:\n                    for seg_type in configured_parent_types:\n                        block_config.incorporate(\n                            before=self._config_dict[seg_type].get(\"spacing_before\")\n                        )\n                # Claim the _after_ config if at the end.\n                if parent_end:\n                    for seg_type in configured_parent_types:\n                        block_config.incorporate(\n                            after=self._config_dict[seg_type].get(\"spacing_after\")\n                        )\n\n        # Second: With the types of the raw segment itself.\n        # Unless someone is doing something complicated with their configuration\n        # there should only be one.\n        # TODO: Extend (or at least harden) this code to handle multiple\n        # configured (and matched) types much better.\n        for seg_type in configured_types:\n            block_config.incorporate(config=self._config_dict[seg_type])\n        return block_config\n", "type": "function"}, {"name": "from_config", "is_method": true, "class_name": "ReflowBlock", "parameters": ["cls", "segments", "config", "depth_info"], "calls": ["config.get_block_config", "zip", "cls", "cls._class_types", "config.get_block_config"], "code_location": {"file": "elements.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 178, "end_line": 226}, "code_snippet": "    def from_config(\n        cls: type[\"ReflowBlock\"],\n        segments: tuple[RawSegment, ...],\n        config: ReflowConfig,\n        depth_info: DepthInfo,\n    ) -> \"ReflowBlock\":\n        \"\"\"Construct a ReflowBlock while extracting relevant configuration.\n\n        This is the primary route to construct a ReflowBlock, as\n        is allows all of the inference of the spacing and position\n        configuration from the segments it contains and the\n        appropriate config objects.\n        \"\"\"\n        block_config = config.get_block_config(cls._class_types(segments), depth_info)\n        stack_spacing_configs = {}\n        line_position_configs = {}\n        keyword_line_position_configs = {}\n        keyword_line_position_exclusions_configs = {}\n        for hash, class_types in zip(\n            depth_info.stack_hashes, depth_info.stack_class_types\n        ):\n            cfg = config.get_block_config(class_types)\n            if cfg.spacing_within:\n                stack_spacing_configs[hash] = cfg.spacing_within\n            if cfg.line_position:\n                line_position_configs[hash] = cfg.line_position\n            if cfg.keyword_line_position:\n                keyword_line_position_configs[hash] = cfg.keyword_line_position\n            if cfg.keyword_line_position_exclusions:\n                keyword_line_position_exclusions_configs[hash] = (\n                    cfg.keyword_line_position_exclusions\n                )\n        return cls(\n            segments=segments,\n            spacing_before=block_config.spacing_before,\n            spacing_after=block_config.spacing_after,\n            line_position=block_config.line_position,\n            depth_info=depth_info,\n            stack_spacing_configs=stack_spacing_configs,\n            line_position_configs=line_position_configs,\n            keyword_line_position=block_config.keyword_line_position,\n            keyword_line_position_configs=keyword_line_position_configs,\n            keyword_line_position_exclusions=(\n                block_config.keyword_line_position_exclusions\n            ),\n            keyword_line_position_exclusions_configs=(\n                keyword_line_position_exclusions_configs\n            ),\n        )\n", "type": "function"}, {"name": "test__validate_configs_precedence_same_file", "is_method": false, "class_name": null, "parameters": [], "calls": ["any", "records_to_nested_dict", "validate_config_dict_for_removed"], "code_location": {"file": "validate_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/config", "start_line": 43, "end_line": 64}, "code_snippet": "def test__validate_configs_precedence_same_file():\n    \"\"\"Test _validate_configs method of FluffConfig where there's a conflict.\"\"\"\n    # Check with a known conflicted value\n    old_key = (\"rules\", \"LT03\", \"operator_new_lines\")\n    new_key = (\"layout\", \"type\", \"binary_operator\", \"line_position\")\n    # Check it's still conflicted.\n    assert any(\n        k.old_path == old_key and k.new_path == new_key for k in REMOVED_CONFIGS\n    ), (\n        \"This test depends on this key still being removed. Update the test to \"\n        \"one that is if this one isn't.\"\n    )\n    # Test config\n    config = records_to_nested_dict([(new_key, \"foo\"), (old_key, \"foo\")])\n    # Before validation\n    assert config == {\n        \"rules\": {\"LT03\": {\"operator_new_lines\": \"foo\"}},\n        \"layout\": {\"type\": {\"binary_operator\": {\"line_position\": \"foo\"}}},\n    }\n    validate_config_dict_for_removed(config, \"<test>\")\n    # Check we only get the new key after validation\n    assert config == {\"layout\": {\"type\": {\"binary_operator\": {\"line_position\": \"foo\"}}}}\n", "type": "function"}, {"name": "incorporate", "is_method": true, "class_name": "BlockConfig", "parameters": ["self", "before", "after", "within", "line_position", "config", "keyword_line_position", "keyword_line_position_exclusions"], "calls": ["split_comma_separated_string", "config.get", "config.get", "config.get", "config.get", "config.get", "config.get"], "code_location": {"file": "config.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 27, "end_line": 61}, "code_snippet": "    def incorporate(\n        self,\n        before: Optional[str] = None,\n        after: Optional[str] = None,\n        within: Optional[str] = None,\n        line_position: Optional[str] = None,\n        config: Optional[ConfigElementType] = None,\n        keyword_line_position: Optional[str] = None,\n        keyword_line_position_exclusions: Union[str, list[str], None] = None,\n    ) -> None:\n        \"\"\"Mutate the config based on additional information.\"\"\"\n        config = config or {}\n        self.spacing_before = (\n            before or config.get(\"spacing_before\", None) or self.spacing_before\n        )\n        self.spacing_after = (\n            after or config.get(\"spacing_after\", None) or self.spacing_after\n        )\n        self.spacing_within = (\n            within or config.get(\"spacing_within\", None) or self.spacing_within\n        )\n        self.line_position = (\n            line_position or config.get(\"line_position\", None) or self.line_position\n        )\n        self.keyword_line_position = (\n            keyword_line_position\n            or config.get(\"keyword_line_position\", None)\n            or self.keyword_line_position\n        )\n        self.keyword_line_position_exclusions = split_comma_separated_string(\n            keyword_line_position_exclusions\n            or config.get(\"keyword_line_position_exclusions\", None)\n            or self.keyword_line_position_exclusions\n            or []\n        )\n", "type": "function"}, {"name": "_rebuild_spacing", "is_method": true, "class_name": "Rule_ST04", "parameters": ["self", "indent_str", "nested_clauses"], "calls": ["any", "sp.is_comment", "nested_clauses.first", "seg.is_type", "seg.is_type", "sp.not_", "NewlineSegment", "WhitespaceSegment", "sp.is_whitespace", "WhitespaceSegment"], "code_location": {"file": "ST04.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 216, "end_line": 242}, "code_snippet": "    def _rebuild_spacing(\n        self, indent_str: str, nested_clauses: Segments\n    ) -> list[BaseSegment]:\n        buff = []\n        # If the first segment is a comment, add a newline\n        prior_newline = nested_clauses.first(sp.not_(sp.is_whitespace())).any(\n            sp.is_comment()\n        )\n        prior_whitespace = \"\"\n        for seg in nested_clauses:\n            if seg.is_type(\"when_clause\", \"else_clause\") or (\n                prior_newline and seg.is_comment\n            ):\n                buff += [NewlineSegment(), WhitespaceSegment(indent_str), seg]\n                prior_newline = False\n                prior_whitespace = \"\"\n            elif seg.is_type(\"newline\"):\n                prior_newline = True\n                prior_whitespace = \"\"\n            elif not prior_newline and seg.is_comment:\n                buff += [WhitespaceSegment(prior_whitespace), seg]\n                prior_newline = False\n                prior_whitespace = \"\"\n            elif seg.is_whitespace:\n                # Don't reset newline\n                prior_whitespace = seg.raw\n        return buff\n", "type": "function"}, {"name": "get_config_info", "is_method": false, "class_name": null, "parameters": [], "calls": ["get_plugin_manager", "plugin_manager.hook.get_configs_info", "config_info_dict.items"], "code_location": {"file": "config_info.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 88, "end_line": 99}, "code_snippet": "def get_config_info() -> dict[str, ConfigInfo]:\n    \"\"\"Get the config from core sqlfluff and sqlfluff plugins and merges them.\n\n    NOTE: This should be the entry point into getting config info rather than\n    importing the default set above, as many values are defined only in rule\n    packages.\n    \"\"\"\n    plugin_manager = get_plugin_manager()\n    configs_info = plugin_manager.hook.get_configs_info()\n    return {\n        k: v for config_info_dict in configs_info for k, v in config_info_dict.items()\n    }\n", "type": "function"}, {"name": "handle_respace__inline_without_space", "is_method": false, "class_name": null, "parameters": ["pre_constraint", "post_constraint", "prev_block", "next_block", "segment_buffer", "existing_results", "anchor_on"], "calls": ["intersection", "segment_buffer.append", "reflow_logger.debug", "post_constraint.startswith", "reflow_logger.debug", "LintResult", "reflow_logger.debug", "WhitespaceSegment", "ValueError", "reflow_logger.warning", "ValueError", "LintResult", "NotImplementedError", "NotImplementedError", "reflow_logger.debug", "WhitespaceSegment", "pretty_segment_name", "pretty_segment_name", "cast", "LintFix", "cast", "LintFix", "WhitespaceSegment", "WhitespaceSegment"], "code_location": {"file": "respace.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 484, "end_line": 632}, "code_snippet": "def handle_respace__inline_without_space(\n    pre_constraint: str,\n    post_constraint: str,\n    prev_block: Optional[\"ReflowBlock\"],\n    next_block: Optional[\"ReflowBlock\"],\n    segment_buffer: list[RawSegment],\n    existing_results: list[LintResult],\n    anchor_on: str = \"before\",\n) -> tuple[list[RawSegment], list[LintResult], bool]:\n    \"\"\"Ensure spacing is the right size.\n\n    This forms one of the cases handled by .respace_point().\n\n    This code assumes:\n    - a ReflowPoint with no newlines.\n    - a ReflowPoint which _no_ whitespace.\n\n    Given this we apply constraints to either confirm no\n    spacing is required or create some of the right size.\n    \"\"\"\n    # Do we have either side set to \"touch\" or \"any\"\n    if {\"touch\", \"any\"}.intersection([pre_constraint, post_constraint]):\n        # In this instance - no whitespace is correct.\n        # Either because there shouldn't be, or because \"any\"\n        # means we shouldn't check.\n        return segment_buffer, existing_results, False\n    # Are we supposed to be aligning?\n    elif post_constraint.startswith(\"align\"):\n        reflow_logger.debug(\"    Inserting Aligned Whitespace.\")\n        # TODO: We currently rely on a second pass to align\n        # insertions. This is where we could devise alignment\n        # in advance, but most of the alignment code relies on\n        # having existing position markers for those insertions.\n        # https://github.com/sqlfluff/sqlfluff/issues/4492\n        desired_space = \" \"\n        added_whitespace = WhitespaceSegment(desired_space)\n    # Is it anything other than the default case?\n    elif not (pre_constraint == post_constraint == \"single\"):  # pragma: no cover\n        # TODO: This will get test coverage when configuration routines\n        # are in properly.\n        raise NotImplementedError(\n            f\"Unexpected Constraints: {pre_constraint}, {post_constraint}\"\n        )\n    else:\n        # Default to a single whitespace\n        reflow_logger.debug(\"    Inserting Single Whitespace.\")\n        added_whitespace = WhitespaceSegment()\n\n    # Add it to the buffer first (the easy bit). The hard bit\n    # is to then determine how to generate the appropriate LintFix\n    # objects.\n    segment_buffer.append(added_whitespace)\n\n    # So special handling here. If segments either side\n    # already exist then we don't care which we anchor on\n    # but if one is already an insertion (as shown by a lack)\n    # of pos_marker, then we should piggy back on that pre-existing\n    # fix.\n    existing_fix = None\n    insertion = None\n    if prev_block and not prev_block.segments[-1].pos_marker:\n        existing_fix = \"after\"\n        insertion = prev_block.segments[-1]\n    elif next_block and not next_block.segments[0].pos_marker:\n        existing_fix = \"before\"\n        insertion = next_block.segments[0]\n\n    if existing_fix:\n        reflow_logger.debug(\"    Detected existing fix %s\", existing_fix)\n        if not existing_results:  # pragma: no cover\n            raise ValueError(\n                \"Fixes detected, but none passed to .respace(). \"\n                \"This will cause conflicts.\"\n            )\n        # Find the fix\n        assert insertion\n        for res in existing_results:\n            # Does it contain the insertion?\n            # TODO: This feels ugly - eq for BaseSegment is different\n            # to uuid matching for RawSegment. Perhaps this should be\n            # more aligned. There might be a better way of doing this.\n            for fix in res.fixes or []:\n                if fix.edit and insertion.uuid in [elem.uuid for elem in fix.edit]:\n                    break\n            else:  # pragma: no cover\n                continue\n            break\n        else:  # pragma: no cover\n            reflow_logger.warning(\"Results %s\", existing_results)\n            raise ValueError(f\"Couldn't find insertion for {insertion}\")\n        # Mutate the existing fix\n        assert res\n        assert fix\n        assert fix in res.fixes\n        assert fix.edit  # It's going to be an edit if we've picked it up.\n        # Mutate the fix, it's still in the same result, and that result\n        # is still in the existing_results.\n        if existing_fix == \"before\":\n            fix.edit = [cast(BaseSegment, added_whitespace)] + fix.edit\n        elif existing_fix == \"after\":\n            fix.edit = fix.edit + [cast(BaseSegment, added_whitespace)]\n\n        # No need to add new results, because we mutated the existing.\n        return segment_buffer, existing_results, True\n\n    # Otherwise...\n    reflow_logger.debug(\"    Not Detected existing fix. Creating new\")\n    if prev_block and next_block:\n        desc = (\n            \"Expected single whitespace between \"\n            f\"{pretty_segment_name(prev_block.segments[-1])} \"\n            f\"and {pretty_segment_name(next_block.segments[0])}.\"\n        )\n    else:  # pragma: no cover\n        # Something to fall back on if prev_block and next_block not provided.\n        desc = \"Expected single whitespace.\"\n    # Take into account hint on where to anchor if given.\n    if prev_block and anchor_on != \"after\":\n        new_result = LintResult(\n            # We do this shuffle, because for the CLI it's clearer if the\n            # anchor for the error is at the point that the insertion will\n            # happen which is the *start* of the next segment, even if\n            # we're anchoring the fix on the previous.\n            next_block.segments[0] if next_block else prev_block.segments[-1],\n            fixes=[\n                LintFix(\n                    \"create_after\",\n                    anchor=prev_block.segments[-1],\n                    edit=[WhitespaceSegment()],\n                )\n            ],\n            description=desc,\n        )\n    elif next_block:\n        new_result = LintResult(\n            next_block.segments[0],\n            fixes=[\n                LintFix(\n                    \"create_before\",\n                    anchor=next_block.segments[0],\n                    edit=[WhitespaceSegment()],\n                )\n            ],\n            description=desc,\n        )\n    else:  # pragma: no cover\n        NotImplementedError(\"Not set up to handle a missing _after_ and _before_.\")\n\n    return segment_buffer, existing_results + [new_result], True\n", "type": "function"}, {"name": "FluffConfig", "docstring": "The persistent object for internal methods to access configuration.\n\nThis class is designed to be instantiated once for each file and then be\nreused by each part of the process. For multiple files in the same path, a\nparent object will be created for the each path and then variants of it\nare created *for each file*. The object itself contains the references\nto any long lived objects which might be used by multiple parts of the\ncodebase such as the dialect and the templater (both of which can be\nresource intensive to load & instantiate), which allows (for example),\nmultiple files to reuse the same instance of the relevant dialect.\n\nIt is also designed to pickle well for use in parallel operations.\n\nArgs:\n    configs (ConfigMappingType, optional): A nested dict of config\n        values from which to construct the config.\n    extra_config_path (str, optional): An optional additional path\n        to load config files from. These are loaded last if found\n        and take precedence over any pre-existing config values.\n        Note that when provided directly to the class, this path\n        is not loaded for the class in question (it's assumed that\n        has already been done, and the results are incorporated in\n        the `configs` argument), but it *is* passed onward to child\n        config instances, which will use it.\n    ignore_local_config (bool, optional, defaults to False): If set to\n        True, this skips loading configuration from the user home\n        directory (``~``) or ``appdir`` path.\n    overrides (ConfigMappingType, optional): A additional set of\n        configs to merge into the ``core`` section of the config\n        object at the end. These values take precedence over all\n        other provided values and are inherited by child configs.\n        For example, override values provided in the CLI use this\n        method to apply to all files in a linting operation. Note\n        that this mapping dict *only* applies to the ``core``\n        section and so cannot be used for all values.\n    plugin_manager (PluginManager, optional): Optional pre-loaded\n        config manager. Generally users should not need to provide\n        this, as the class will fetch it's own if not provided.\n        This argument is used when creating new class instances to\n        avoid reloading the manager.\n\n.. note::\n   Methods for accessing internal properties on the config are not particularly\n   standardised as the project currently assumes that few other tools are using\n   this interface directly. If you or your project would like more formally\n   supported methods for access to the config object, raise an issue on GitHub\n   with the kind of things you'd like to achieve.", "methods": ["__init__", "_handle_comma_separated_values", "_initialise_dialect", "verify_dialect_specified", "__getstate__", "__setstate__", "copy", "from_root", "from_string", "from_strings", "from_path", "from_kwargs", "get_templater_class", "get_templater", "make_child_from_path", "diff_to", "get", "get_section", "set_value", "iter_vals", "process_inline_config", "process_raw_file_for_config"], "attributes": ["private_vals"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 37, "end_line": 732}, "type": "class"}, {"name": "handle_respace__inline_with_space", "is_method": false, "class_name": null, "parameters": ["pre_constraint", "post_constraint", "prev_block", "next_block", "root_segment", "segment_buffer", "last_whitespace"], "calls": ["segment_buffer.index", "NotImplementedError", "segment_buffer.pop", "post_constraint.startswith", "post_constraint.startswith", "_extract_alignment_config", "last_whitespace.edit", "new_results.append", "LintResult", "_determine_aligned_inline_spacing", "LintResult", "pretty_segment_name", "last_whitespace.pos_marker.end_point_marker", "LintFix.delete", "pos_marker.end_point_marker", "reflow_logger.info", "pretty_segment_name", "pretty_segment_name", "LintFix"], "code_location": {"file": "respace.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 348, "end_line": 481}, "code_snippet": "def handle_respace__inline_with_space(\n    pre_constraint: str,\n    post_constraint: str,\n    prev_block: Optional[\"ReflowBlock\"],\n    next_block: Optional[\"ReflowBlock\"],\n    root_segment: BaseSegment,\n    segment_buffer: list[RawSegment],\n    last_whitespace: RawSegment,\n) -> tuple[list[RawSegment], list[LintResult]]:\n    \"\"\"Check inline spacing is the right size.\n\n    This forms one of the cases handled by .respace_point().\n\n    This code assumes:\n    - a ReflowPoint with no newlines.\n    - a ReflowPoint which has _some_ whitespace.\n\n    Given this we apply constraints to ensure the whitespace\n    is of an appropriate size.\n    \"\"\"\n    # Get some indices so that we can reference around them\n    ws_idx = segment_buffer.index(last_whitespace)\n\n    # Do we have either side set to \"any\"\n    if \"any\" in [pre_constraint, post_constraint]:\n        # In this instance - don't change anything.\n        # e.g. this could mean there is a comment on one side.\n        return segment_buffer, []\n\n    # Do we have either side set to \"touch\"?\n    if \"touch\" in [pre_constraint, post_constraint]:\n        # In this instance - no whitespace is correct, This\n        # means we should delete it.\n        segment_buffer.pop(ws_idx)\n        if next_block:\n            description = (\n                \"Unexpected whitespace before \"\n                f\"{pretty_segment_name(next_block.segments[0])}.\"\n            )\n        else:  # pragma: no cover\n            # This clause has no test coverage because next_block is\n            # normally provided.\n            description = \"Unexpected whitespace\"\n\n        return segment_buffer, [\n            LintResult(\n                last_whitespace,\n                [LintFix.delete(last_whitespace)],\n                # Should make description from constraints.\n                description=description,\n            ),\n        ]\n\n    # Handle left alignment & singles\n    if (\n        post_constraint.startswith(\"align\") and next_block\n    ) or pre_constraint == post_constraint == \"single\":\n        # Determine the desired spacing, either as alignment or as a single.\n        if post_constraint.startswith(\"align\") and next_block:\n            seg_type, align_within, align_scope = _extract_alignment_config(\n                post_constraint\n            )\n\n            next_pos: Optional[PositionMarker]\n            if next_block.segments[0].pos_marker:\n                next_pos = next_block.segments[0].pos_marker\n            # Excluded from coverage: no longer triggered since AL01 rule was refactored\n            elif last_whitespace.pos_marker:  # pragma: no cover\n                next_pos = last_whitespace.pos_marker.end_point_marker()\n            # These second clauses are much less likely and so are excluded from\n            # coverage. If we find a way of covering them, that would be great\n            # but for now they exist as backups.\n            elif prev_block and prev_block.segments[-1].pos_marker:  # pragma: no cover\n                next_pos = prev_block.segments[-1].pos_marker.end_point_marker()\n            else:  # pragma: no cover\n                reflow_logger.info(\"Unable to find position marker for alignment.\")\n                next_pos = None\n                desired_space = \" \"\n                desc = f\"Expected only single space. Found {last_whitespace.raw!r}.\"\n\n            if next_pos:\n                desired_space = _determine_aligned_inline_spacing(\n                    root_segment,\n                    last_whitespace,\n                    next_block.segments[0],\n                    next_pos,\n                    seg_type,\n                    align_within,\n                    align_scope,\n                )\n\n                desc = (\n                    f\"{seg_type!r} elements are expected to be aligned. Found \"\n                    \"incorrect whitespace before \"\n                    f\"{pretty_segment_name(next_block.segments[0])}: \"\n                    f\"{last_whitespace.raw!r}.\"\n                )\n        else:\n            if next_block:\n                desc = (\n                    \"Expected only single space before \"\n                    f\"{pretty_segment_name(next_block.segments[0])}. Found \"\n                    f\"{last_whitespace.raw!r}.\"\n                )\n            else:  # pragma: no cover\n                # This clause isn't has no test coverage because next_block is\n                # normally provided.\n                desc = f\"Expected only single space. Found {last_whitespace.raw!r}.\"\n            desired_space = \" \"\n\n        new_results: list[LintResult] = []\n\n        if last_whitespace.raw != desired_space:\n            new_seg = last_whitespace.edit(desired_space)\n            new_results.append(\n                LintResult(\n                    last_whitespace,\n                    [\n                        LintFix(\n                            \"replace\",\n                            anchor=last_whitespace,\n                            edit=[new_seg],\n                        )\n                    ],\n                    description=desc,\n                )\n            )\n            segment_buffer[ws_idx] = new_seg\n\n        return segment_buffer, new_results\n\n    raise NotImplementedError(  # pragma: no cover\n        f\"Unexpected Constraints: {pre_constraint}, {post_constraint}\"\n    )\n", "type": "function"}, {"name": "_unpack_constraint", "is_method": false, "class_name": null, "parameters": ["constraint", "strip_newlines"], "calls": ["constraint.startswith", "reflow_logger.warning", "constraint.partition", "SQLFluffUserError"], "code_location": {"file": "respace.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 27, "end_line": 55}, "code_snippet": "def _unpack_constraint(constraint: str, strip_newlines: bool) -> tuple[str, bool]:\n    \"\"\"Unpack a spacing constraint.\n\n    Used as a helper function in `determine_constraints`.\n    \"\"\"\n    # Check for deprecated options.\n    if constraint == \"inline\":  # pragma: no cover\n        reflow_logger.warning(\n            \"Found 'inline' specified as a 'spacing_within' constraint. \"\n            \"This setting is deprecated and has been replaced by the more \"\n            \"explicit 'touch:inline'. Upgrade your configuration to \"\n            \"remove this warning.\"\n        )\n        constraint = \"touch:inline\"\n\n    # Unless align, split.\n    if constraint.startswith(\"align\"):\n        modifier = \"\"\n    else:\n        constraint, _, modifier = constraint.partition(\":\")\n\n    if not modifier:\n        pass\n    elif modifier == \"inline\":\n        strip_newlines = True\n    else:  # pragma: no cover\n        raise SQLFluffUserError(f\"Unexpected constraint modifier: {constraint!r}\")\n\n    return constraint, strip_newlines\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1932671070098877}
{"question": "Why does the test function that validates SQL rule fixes use a three-phase sequence of linting, fixing, and linting again instead of testing components independently?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "generic_roundtrip_test", "is_method": false, "class_name": null, "parameters": ["source_file", "rulestring"], "calls": ["isinstance", "tempfile.mkdtemp", "os.path.join", "CliRunner", "runner.invoke", "runner.invoke", "runner.invoke", "shutil.rmtree", "open", "open", "StringIO", "dest_file.write", "f.read"], "code_location": {"file": "std_roundtrip_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/rules", "start_line": 15, "end_line": 46}, "code_snippet": "def generic_roundtrip_test(source_file, rulestring):\n    \"\"\"Run a roundtrip test given a sql file and a rule.\n\n    We take a file buffer, lint, fix and lint, finally checking that\n    the file fails initially but not after fixing.\n    \"\"\"\n    if isinstance(source_file, str):\n        # If it's a string, treat it as a path so lets load it.\n        with open(source_file) as f:\n            source_file = StringIO(f.read())\n\n    filename = \"testing.sql\"\n    # Lets get the path of a file to use\n    tempdir_path = tempfile.mkdtemp()\n    filepath = os.path.join(tempdir_path, filename)\n    # Open the example file and write the content to it\n    with open(filepath, mode=\"w\") as dest_file:\n        for line in source_file:\n            dest_file.write(line)\n    runner = CliRunner()\n    # Check that we first detect the issue\n    result = runner.invoke(lint, [\"--rules\", rulestring, \"--dialect=ansi\", filepath])\n    assert result.exit_code == 1\n    # Fix the file (in force mode)\n    result = runner.invoke(\n        fix, [\"--rules\", rulestring, \"--dialect=ansi\", \"-f\", filepath]\n    )\n    assert result.exit_code == 0\n    # Now lint the file and check for exceptions\n    result = runner.invoke(lint, [\"--rules\", rulestring, \"--dialect=ansi\", filepath])\n    assert result.exit_code == 0\n    shutil.rmtree(tempdir_path)\n", "type": "function"}, {"name": "test__dialect__base_broad_fix", "is_method": false, "class_name": null, "parameters": ["dialect", "file", "raise_critical_errors_after_fix", "caplog"], "calls": ["pytest.mark.parametrize", "load_file", "dict", "lex_and_parse", "print", "FluffConfig", "Linter", "linter.get_rulepack", "linter.lint_parsed", "parsed.tree.stringify"], "code_location": {"file": "dialects_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 85, "end_line": 120}, "code_snippet": "def test__dialect__base_broad_fix(\n    dialect, file, raise_critical_errors_after_fix, caplog\n):\n    \"\"\"Run a full fix with all rules, in search of critical errors.\n\n    NOTE: This suite does all of the same things as the above test\n    suite (the `parse_suite`), but also runs fix. In CI, we run\n    the above tests _with_ coverage tracking, but these we run\n    _without_.\n\n    The purpose of this test is as a more stretching run through\n    a wide range of test sql examples, and the full range of rules\n    to find any potential critical errors raised by any interactions\n    between different dialects and rules.\n\n    We also do not use DEBUG logging here because it gets _very_\n    noisy.\n    \"\"\"\n    raw = load_file(dialect, file)\n    config_overrides = dict(dialect=dialect)\n\n    parsed: Optional[ParsedString] = lex_and_parse(config_overrides, raw)\n    if not parsed:  # Empty file case\n        return\n    print(parsed.tree.stringify())\n\n    config = FluffConfig(overrides=config_overrides)\n    linter = Linter(config=config)\n    rule_pack = linter.get_rulepack()\n    # Due to \"raise_critical_errors_after_fix\" fixture \"fix\",\n    # will now throw.\n    linter.lint_parsed(\n        parsed,\n        rule_pack,\n        fix=True,\n    )\n", "type": "function"}, {"name": "test__api__fix_string_specific", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.fix"], "code_location": {"file": "simple_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 434, "end_line": 438}, "code_snippet": "def test__api__fix_string_specific():\n    \"\"\"Basic checking of lint functionality with a specific rule.\"\"\"\n    result = sqlfluff.fix(my_bad_query, rules=[\"CP01\"])\n    # Check actual result\n    assert result == \"SELECT  *, 1, blah AS  fOO  FROM myTable\"\n", "type": "function"}, {"name": "assert_rule_fail_in_sql", "is_method": false, "class_name": null, "parameters": ["code", "sql", "configs", "line_numbers"], "calls": ["print", "_setup_config", "lint_string", "linted.get_violations", "print", "linted.fix_string", "print", "startswith", "pytest.fail", "any", "print", "pytest.fail", "any", "Linter", "pytest.fail", "isinstance", "isinstance", "pytest.fail", "repr", "e.desc", "format", "split_comma_separated_string", "linted.tree.stringify", "get_rule_from_set", "split_comma_separated_string", "e.desc"], "code_location": {"file": "rules.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/testing", "start_line": 101, "end_line": 172}, "code_snippet": "def assert_rule_fail_in_sql(\n    code: str,\n    sql: str,\n    configs: Optional[ConfigMappingType] = None,\n    line_numbers: Optional[list[int]] = None,\n) -> tuple[str, list[SQLBaseError]]:\n    \"\"\"Assert that a given rule does fail on the given sql.\n\n    Args:\n        code (str): The code of the rule to test.\n        sql (str): The SQL text to check against.\n        configs (:obj:`ConfigMappingType`, optional): A config dict\n            object containing any overrides.\n        line_numbers (list of int, optional): The line numbers which\n            we want to test that errors occurred on.\n\n    Returns:\n        Tuple: values(fixed_sql (str), violations (list))\n            fixed_sql (str): The fixed string after linting. Note that for\n                testing purposes, `.lint_string()` is always called with\n                `fix` set to `True`.\n            violations (list of SQLBaseError): the violations found during\n                linting.\n    \"\"\"\n    print(\"# Asserting Rule Fail in SQL\")\n    # Set up the config to only use the rule we are testing.\n    cfg = _setup_config(code, configs)\n    # Lint it using the current config (while in fix mode)\n    linted = Linter(config=cfg).lint_string(sql, fix=True)\n    all_violations = linted.get_violations()\n    print(\"Errors Found:\")\n    for e in all_violations:\n        print(\"    \" + repr(e))\n        if e.desc().startswith(\"Unexpected exception\"):\n            pytest.fail(f\"Linter failed with {e.desc()}\")  # pragma: no cover\n    parse_errors = [\n        v for v in all_violations if isinstance(v, (SQLParseError, SQLTemplaterError))\n    ]\n    if parse_errors:\n        pytest.fail(f\"Found the following parse errors in test case: {parse_errors}\")\n    lint_errors: list[SQLLintError] = [\n        v for v in all_violations if isinstance(v, SQLLintError)\n    ]\n    if not any(v.rule.code in split_comma_separated_string(code) for v in lint_errors):\n        assert linted.tree\n        print(f\"Parsed File:\\n{linted.tree.stringify()}\")\n        pytest.fail(\n            f\"No {code} failures found in query which should fail.\",\n            pytrace=False,\n        )\n    if line_numbers:\n        actual_line_numbers = [e.line_no for e in lint_errors]\n        if line_numbers != actual_line_numbers:  # pragma: no cover\n            pytest.fail(\n                \"Expected errors on lines {}, but got errors on lines {}\".format(\n                    line_numbers, actual_line_numbers\n                )\n            )\n    fixed_sql, _ = linted.fix_string()\n\n    # Check that if it has made changes that this rule has set\n    # `is_fix_compatible` appropriately.\n    if fixed_sql != sql:\n        assert any(\n            get_rule_from_set(rule, config=cfg).is_fix_compatible\n            for rule in split_comma_separated_string(code)\n        ), (\n            f\"Rule {code} returned fixes but does not specify \"\n            \"'is_fix_compatible = True'.\"\n        )\n\n    return fixed_sql, linted.violations\n", "type": "function"}, {"name": "lint_fix_parsed", "is_method": true, "class_name": "Linter", "parameters": ["cls", "tree", "config", "rule_pack", "fix", "fname", "templated_file", "formatter"], "calls": ["config.get", "config.get", "linter_logger.info", "linter_logger.info", "config.get", "formatter.dispatch_lint_header", "cls.allowed_rule_ref_map", "IgnoreMask.from_tree", "phases.append", "range", "cls.remove_templated_errors", "format", "sorted", "config.get", "len", "linter_logger.info", "is_first_linter_pass", "tqdm", "tree.stringify", "rule_pack.codes", "progress_bar_crawler.set_description", "time.monotonic", "crawler.crawl", "is_first_linter_pass", "rule_timings.append", "linter_logger.info", "linter_logger.warning", "linter_logger.info", "compute_anchor_edit_info", "any", "isinstance", "is_first_linter_pass", "config.get", "anchor_info.items", "cls._report_conflicting_fixes_same_anchor", "linter_logger.debug", "apply_fixes", "time.monotonic", "anchor_info.values", "config.get", "tuple", "linter_logger.debug", "config.get", "tuple", "linter_logger.warning", "previous_versions.add", "cls._warn_unfixable"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 380, "end_line": 628}, "code_snippet": "    def lint_fix_parsed(\n        cls,\n        tree: BaseSegment,\n        config: FluffConfig,\n        rule_pack: RulePack,\n        fix: bool = False,\n        fname: Optional[str] = None,\n        templated_file: Optional[\"TemplatedFile\"] = None,\n        formatter: Optional[FormatterInterface] = None,\n    ) -> tuple[BaseSegment, list[SQLBaseError], Optional[IgnoreMask], RuleTimingsType]:\n        \"\"\"Lint and optionally fix a tree object.\"\"\"\n        # Keep track of the linting errors on the very first linter pass. The\n        # list of issues output by \"lint\" and \"fix\" only includes issues present\n        # in the initial SQL code, EXCLUDING any issues that may be created by\n        # the fixes themselves.\n        initial_linting_errors = []\n        # A placeholder for the fixes we had on the previous loop\n        last_fixes: Optional[list[LintFix]] = None\n        # Keep a set of previous versions to catch infinite loops.\n        previous_versions: set[tuple[str, tuple[\"SourceFix\", ...]]] = {(tree.raw, ())}\n        # Keep a buffer for recording rule timings.\n        rule_timings: RuleTimingsType = []\n\n        # If we are fixing then we want to loop up to the runaway_limit, otherwise just\n        # once for linting.\n        loop_limit = config.get(\"runaway_limit\") if fix else 1\n\n        # Dispatch the output for the lint header\n        if formatter:\n            formatter.dispatch_lint_header(\n                fname or \"<filename>\", sorted(rule_pack.codes())\n            )\n\n        # Look for comment segments which might indicate lines to ignore.\n        disable_noqa_except: Optional[str] = config.get(\"disable_noqa_except\")\n        if not config.get(\"disable_noqa\") or disable_noqa_except:\n            allowed_rules_ref_map = cls.allowed_rule_ref_map(\n                rule_pack.reference_map, disable_noqa_except\n            )\n            ignore_mask, ivs = IgnoreMask.from_tree(tree, allowed_rules_ref_map)\n            initial_linting_errors += ivs\n        else:\n            ignore_mask = None\n\n        save_tree = tree\n        # There are two phases of rule running.\n        # 1. The main loop is for most rules. These rules are assumed to\n        # interact and cause a cascade of fixes requiring multiple passes.\n        # These are run the `runaway_limit` number of times (default 10).\n        # 2. The post loop is for post-processing rules, not expected to trigger\n        # any downstream rules, e.g. capitalization fixes. They are run on the\n        # first loop and then twice at the end (once to fix, and once again to\n        # check result of fixes), but not in the intervening loops.\n        phases = [\"main\"]\n        if fix:\n            phases.append(\"post\")\n        for phase in phases:\n            if len(phases) > 1:\n                rules_this_phase = [\n                    rule for rule in rule_pack.rules if rule.lint_phase == phase\n                ]\n            else:\n                rules_this_phase = rule_pack.rules\n            for loop in range(loop_limit if phase == \"main\" else 2):\n\n                def is_first_linter_pass() -> bool:\n                    return phase == phases[0] and loop == 0\n\n                # Additional newlines are to assist in scanning linting loops\n                # during debugging.\n                linter_logger.info(\n                    f\"\\n\\nEntering linter phase {phase}, loop {loop + 1}/{loop_limit}\\n\"\n                )\n                changed = False\n\n                if is_first_linter_pass():\n                    # In order to compute initial_linting_errors correctly, need\n                    # to run all rules on the first loop of the main phase.\n                    rules_this_phase = rule_pack.rules\n                progress_bar_crawler = tqdm(\n                    rules_this_phase,\n                    desc=\"lint by rules\",\n                    leave=False,\n                    disable=progress_bar_configuration.disable_progress_bar,\n                )\n\n                for crawler in progress_bar_crawler:\n                    # Performance: After first loop pass, skip rules that don't\n                    # do fixes. Any results returned won't be seen by the user\n                    # anyway (linting errors ADDED by rules changing SQL, are\n                    # not reported back to the user - only initial linting errors),\n                    # so there's absolutely no reason to run them.\n                    if (\n                        fix\n                        and not is_first_linter_pass()\n                        and not crawler.is_fix_compatible\n                    ):\n                        continue\n\n                    progress_bar_crawler.set_description(f\"rule {crawler.code}\")\n                    t0 = time.monotonic()\n\n                    # fixes should be a dict {} with keys edit, delete, create\n                    # delete is just a list of segments to delete\n                    # edit and create are list of tuples. The first element is\n                    # the \"anchor\", the segment to look for either to edit or to\n                    # insert BEFORE. The second is the element to insert or create.\n                    linting_errors, _, fixes, _ = crawler.crawl(\n                        tree,\n                        dialect=config.get(\"dialect_obj\"),\n                        fix=fix,\n                        templated_file=templated_file,\n                        ignore_mask=ignore_mask,\n                        fname=fname,\n                        config=config,\n                    )\n                    if is_first_linter_pass():\n                        initial_linting_errors += linting_errors\n\n                    if fix and fixes:\n                        linter_logger.info(f\"Applying Fixes [{crawler.code}]: {fixes}\")\n                        # Do some sanity checks on the fixes before applying.\n                        anchor_info = compute_anchor_edit_info(fixes)\n                        if any(\n                            not info.is_valid for info in anchor_info.values()\n                        ):  # pragma: no cover\n                            message = (\n                                f\"Rule {crawler.code} returned conflicting \"\n                                \"fixes with the same anchor. This is only \"\n                                \"supported for create_before+create_after, so \"\n                                \"the fixes will not be applied. \"\n                            )\n                            for uuid, info in anchor_info.items():\n                                if not info.is_valid:\n                                    message += f\"\\n{uuid}:\"\n                                    for _fix in info.fixes:\n                                        message += f\"\\n    {_fix}\"\n                            cls._report_conflicting_fixes_same_anchor(message)\n                            for lint_result in linting_errors:\n                                lint_result.fixes = []\n                        elif fixes == last_fixes:\n                            # If we generate the same fixes two times in a row,\n                            # that means we're in a loop, and we want to stop.\n                            # (Fixes should address issues, hence different\n                            # and/or fewer fixes next time.)\n                            # This is most likely because fixes could not be safely\n                            # applied last time, so we should stop gracefully.\n                            linter_logger.debug(\n                                f\"Fixes generated for {crawler.code} are the same as \"\n                                \"the previous pass. Assuming that we cannot apply them \"\n                                \"safely. Passing gracefully.\"\n                            )\n                        else:\n                            # This is the happy path. We have fixes, now we want to\n                            # apply them.\n                            last_fixes = fixes\n                            new_tree, _, _, _valid = apply_fixes(\n                                tree,\n                                config.get(\"dialect_obj\"),\n                                crawler.code,\n                                anchor_info,\n                                fix_even_unparsable=config.get(\"fix_even_unparsable\"),\n                            )\n\n                            # Check for infinite loops. We use a combination of the\n                            # fixed templated file and the list of source fixes to\n                            # apply.\n                            loop_check_tuple = (\n                                new_tree.raw,\n                                tuple(new_tree.source_fixes),\n                            )\n                            # Was anything actually applied? If not, then the fixes we\n                            # had cannot be safely applied and we should stop trying.\n                            if loop_check_tuple == (tree.raw, tuple(tree.source_fixes)):\n                                linter_logger.debug(\n                                    f\"Fixes for {crawler.code} could not be safely be \"\n                                    \"applied. Likely due to initially unparsable file.\"\n                                )\n                            elif not _valid:\n                                # The fixes result in an invalid file. Don't apply\n                                # the fix and skip onward. Show a warning.\n                                linter_logger.warning(\n                                    f\"Fixes for {crawler.code} not applied, as it \"\n                                    \"would result in an unparsable file. Please \"\n                                    \"report this as a bug with a minimal query \"\n                                    \"which demonstrates this warning.\"\n                                )\n                            elif loop_check_tuple not in previous_versions:\n                                # We've not seen this version of the file so\n                                # far. Continue.\n                                tree = new_tree\n                                previous_versions.add(loop_check_tuple)\n                                changed = True\n                                continue\n                            else:\n                                # Applying these fixes took us back to a state\n                                # which we've seen before. We're in a loop, so\n                                # we want to stop.\n                                cls._warn_unfixable(crawler.code)\n\n                    # Record rule timing\n                    rule_timings.append(\n                        (crawler.code, crawler.name, time.monotonic() - t0)\n                    )\n\n                if fix and not changed:\n                    # We did not change the file. Either the file is clean (no\n                    # fixes), or any fixes which are present will take us back\n                    # to a previous state.\n                    linter_logger.info(\n                        f\"Fix loop complete for {phase} phase. Stability \"\n                        f\"achieved after {loop}/{loop_limit} loops.\"\n                    )\n                    break\n            else:\n                if fix:\n                    # The linter loop hit the limit before reaching a stable point\n                    # (i.e. free of lint errors). If this happens, it's usually\n                    # because one or more rules produced fixes which did not address\n                    # the original issue **or** created new issues.\n                    linter_logger.warning(\n                        f\"Loop limit on fixes reached [{loop_limit}].\"\n                    )\n\n                    # Discard any fixes for the linting errors, since they caused a\n                    # loop. IMPORTANT: By doing this, we are telling SQLFluff that\n                    # these linting errors are \"unfixable\". This is important,\n                    # because when \"sqlfluff fix\" encounters unfixable lint errors,\n                    # it exits with a \"failure\" exit code, which is exactly what we\n                    # want in this situation. (Reason: Although this is more of an\n                    # internal SQLFluff issue, users deserve to know about it,\n                    # because it means their file(s) weren't fixed.\n                    for violation in initial_linting_errors:\n                        if isinstance(violation, SQLLintError):\n                            violation.fixes = []\n\n                    # Return the original parse tree, before any fixes were applied.\n                    # Reason: When the linter hits the loop limit, the file is often\n                    # messy, e.g. some of the fixes were applied repeatedly, possibly\n                    # other weird things. We don't want the user to see this junk!\n                    return save_tree, initial_linting_errors, ignore_mask, rule_timings\n\n        if config.get(\"ignore_templated_areas\", default=True):\n            initial_linting_errors = cls.remove_templated_errors(initial_linting_errors)\n\n        linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Fixed Tree:\"))\n        linter_logger.info(\"\\n\" + tree.stringify())\n\n        return tree, initial_linting_errors, ignore_mask, rule_timings\n", "type": "function"}, {"name": "test__api__fix_string", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.fix", "isinstance"], "code_location": {"file": "simple_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 417, "end_line": 431}, "code_snippet": "def test__api__fix_string():\n    \"\"\"Basic checking of lint functionality.\"\"\"\n    result = sqlfluff.fix(my_bad_query)\n    # Check return types.\n    assert isinstance(result, str)\n    # Check actual result\n    assert (\n        result\n        == \"\"\"SELECT\n    *,\n    1,\n    blah AS foo\nFROM mytable\n\"\"\"\n    )\n", "type": "function"}, {"name": "auto_fix_test", "is_method": false, "class_name": null, "parameters": ["dialect", "folder", "caplog"], "calls": ["caplog.set_level", "caplog.set_level", "tempfile.mkdtemp", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "print", "upper", "get", "print", "clear_config_caches", "FluffConfig.from_root", "Linter", "lnt.lint_path", "print", "set", "res.persist_changes", "shutil.rmtree", "clear_config_caches", "open", "yaml.safe_load", "open", "ValueError", "res.check_tuples", "set", "open", "fixed_file.read", "open", "comp_file.read", "join", "open", "open", "print", "open", "json.load", "get", "dest_file.write", "open", "print", "expected_vs.add", "dest_file.write"], "code_location": {"file": "std_fix_auto_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/rules", "start_line": 47, "end_line": 163}, "code_snippet": "def auto_fix_test(dialect, folder, caplog):\n    \"\"\"A test for roundtrip testing, take a file buffer, lint, fix and lint.\n\n    This is explicitly different from the linter version of this, in that\n    it uses the command line rather than the direct api.\n    \"\"\"\n    # Log just the rules logger for this test.\n    # NOTE: In debugging it may be instructive to enable some of\n    # the other loggers listed here to debug particular issues.\n    # Enabling all of them results in very long logs so use\n    # wisely.\n    # caplog.set_level(logging.DEBUG, logger=\"sqlfluff.templater\")\n    # caplog.set_level(logging.DEBUG, logger=\"sqlfluff.lexer\")\n    caplog.set_level(logging.DEBUG, logger=\"sqlfluff.linter\")\n    caplog.set_level(logging.DEBUG, logger=\"sqlfluff.rules\")\n\n    filename = \"testing.sql\"\n    # Lets get the path of a file to use\n    tempdir_path = tempfile.mkdtemp()\n    filepath = os.path.join(tempdir_path, filename)\n    cfgpath = os.path.join(tempdir_path, \".sqlfluff\")\n    src_filepath = os.path.join(*base_auto_fix_path, dialect, folder, \"before.sql\")\n    cmp_filepath = os.path.join(*base_auto_fix_path, dialect, folder, \"after.sql\")\n    vio_filepath = os.path.join(*base_auto_fix_path, dialect, folder, \"violations.json\")\n    cfg_filepath = os.path.join(*base_auto_fix_path, dialect, folder, \".sqlfluff\")\n    test_conf_filepath = os.path.join(\n        *base_auto_fix_path, dialect, folder, \"test-config.yml\"\n    )\n\n    # Load the config file for the test:\n    with open(test_conf_filepath) as cfg_file:\n        cfg = yaml.safe_load(cfg_file)\n    print(\"## Config: \", cfg)\n    rules: Optional[str] = \",\".join(cfg[\"test-config\"].get(\"rules\")).upper()\n    if \"ALL\" in rules:\n        rules = None\n    raise_on_non_linting_violations = cfg[\"test-config\"].get(\n        \"raise_on_non_linting_violations\", True\n    )\n\n    # Open the example file and write the content to it\n    print_buff = \"\"\n    with open(filepath, mode=\"w\") as dest_file:\n        with open(src_filepath) as source_file:\n            for line in source_file:\n                dest_file.write(line)\n                print_buff += line\n    # Copy the config file too\n    try:\n        with open(cfgpath, mode=\"w\") as dest_file:\n            with open(cfg_filepath) as source_file:\n                print(\"## Config File Found.\")\n                for line in source_file:\n                    dest_file.write(line)\n    except FileNotFoundError:\n        # No config file? No big deal\n        print(\"## No Config File Found.\")\n        pass\n    print(f\"## Input file:\\n{print_buff}\")\n    # Do we need to do a violations check?\n    try:\n        with open(vio_filepath) as vio_file:\n            violations = json.load(vio_file)\n    except FileNotFoundError:\n        # No violations file. Let's not worry\n        violations = None\n\n    # Run the fix command\n    overrides = {\"dialect\": dialect}\n    if rules:\n        overrides[\"rules\"] = rules\n\n    # Clear config caches before loading. The way we move files around\n    # makes the filepath based caching inaccurate, which leads to unstable\n    # test cases unless we regularly clear the cache.\n    clear_config_caches()\n    cfg = FluffConfig.from_root(overrides=overrides)\n    lnt = Linter(config=cfg)\n    res = lnt.lint_path(filepath, fix=True)\n\n    if not res.files:\n        raise ValueError(\"LintedDir empty: Parsing likely failed.\")\n    print(f\"## Templated file:\\n{res.tree.raw}\")\n\n    # We call the check_tuples here, even to makes sure any non-linting\n    # violations are raised, and the test fails.\n    vs = set(\n        res.check_tuples(\n            raise_on_non_linting_violations=raise_on_non_linting_violations\n        )\n    )\n    # If we have a violations structure, let's enforce it.\n    if violations:\n        # Format the violations file\n        expected_vs = set()\n        for rule_key in violations[\"violations\"][\"linting\"]:\n            for elem in violations[\"violations\"][\"linting\"][rule_key]:\n                expected_vs.add((rule_key, *elem))\n        assert expected_vs == vs\n\n    # Actually do the fixes\n    res = res.persist_changes()\n    # Read the fixed file\n    with open(filepath) as fixed_file:\n        fixed_buff = fixed_file.read()\n    # Clear up once read\n    shutil.rmtree(tempdir_path)\n    # Also clear the config cache again so it's not polluted for later tests.\n    clear_config_caches()\n    # Read the comparison file\n    with open(cmp_filepath) as comp_file:\n        comp_buff = comp_file.read()\n\n    # Make sure we were successful\n    assert res\n    # Assert that we fixed as expected\n    assert fixed_buff == comp_buff\n", "type": "function"}, {"name": "test__cli__command__fix", "is_method": false, "class_name": null, "parameters": ["rule", "fname"], "calls": ["pytest.mark.parametrize", "open", "generic_roundtrip_test"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 1033, "end_line": 1036}, "code_snippet": "def test__cli__command__fix(rule, fname):\n    \"\"\"Test the round trip of detecting, fixing and then not detecting the rule.\"\"\"\n    with open(fname) as test_file:\n        generic_roundtrip_test(test_file, rule)\n", "type": "function"}, {"name": "test__cli__command__fix", "is_method": false, "class_name": null, "parameters": ["rule", "path"], "calls": ["pytest.mark.parametrize", "generic_roundtrip_test"], "code_location": {"file": "std_roundtrip_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/rules", "start_line": 124, "end_line": 126}, "code_snippet": "def test__cli__command__fix(rule, path):\n    \"\"\"Test the round trip of detecting, fixing and then not detecting given rule.\"\"\"\n    generic_roundtrip_test(path, rule)\n", "type": "function"}, {"name": "assert_violations_after_fix", "is_method": false, "class_name": null, "parameters": ["test_case"], "calls": ["print", "assert_rule_fail_in_sql", "e.to_dict", "prep_violations", "print", "yaml.dump"], "code_location": {"file": "rules.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/testing", "start_line": 264, "end_line": 291}, "code_snippet": "def assert_violations_after_fix(test_case: RuleTestCase) -> None:\n    \"\"\"Assert that the given violations are found in the fixed sql.\"\"\"\n    print(\"# Asserting Violations After Fix\")\n    assert (\n        test_case.fix_str\n    ), \"Test case must have `fix_str` to call `assert_violations_after_fix()`\"\n    assert test_case.violations_after_fix, (\n        \"Test case must have `violations_after_fix` to call \"\n        \"`assert_violations_after_fix()`\"\n    )\n    _, violations_after_fix = assert_rule_fail_in_sql(\n        test_case.rule,\n        test_case.fix_str,\n        configs=test_case.configs,\n        line_numbers=test_case.line_numbers,\n    )\n    violation_info = [e.to_dict() for e in violations_after_fix]\n    try:\n        assert violation_info == prep_violations(\n            test_case.rule, test_case.violations_after_fix\n        )\n    except AssertionError:  # pragma: no cover\n        print(\n            \"Actual violations_after_fix:\\n\",\n            yaml.dump(violation_info, allow_unicode=True),\n            sep=\"\",\n        )\n        raise\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1742405891418457}
{"question": "Why does repeatedly instantiating the segment class that parses structured data type definitions during parsing of deeply nested structured data type definitions impact memory allocation and garbage collection overhead compared to reusing the shared grammar definition object?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "StructTypeSegment", "docstring": "Expression to construct a STRUCT datatype.", "methods": [], "attributes": [], "code_location": {"file": "dialect_athena.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 355, "end_line": 361}, "type": "class"}, {"name": "StructTypeSegment", "docstring": "Expression to construct a STRUCT datatype.", "methods": [], "attributes": [], "code_location": {"file": "dialect_hive.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 281, "end_line": 287}, "type": "class"}, {"name": "DatatypeSegment", "docstring": "A data type segment.\n\nIn particular here, this enabled the support for\nthe STRUCT datatypes.", "methods": [], "attributes": [], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1349, "end_line": 1365}, "type": "class"}, {"name": "StructTypeSegment", "docstring": "Expression to construct a STRUCT datatype.", "methods": [], "attributes": [], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1368, "end_line": 1374}, "type": "class"}, {"name": "StructTypeSegment", "docstring": "Expression to construct a STRUCT datatype.", "methods": [], "attributes": ["match_grammar", "match_grammar", "match_grammar", "type", "match_grammar"], "code_location": {"file": "dialect_duckdb.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 198, "end_line": 204}, "type": "class"}, {"name": "StructTypeSegment", "docstring": "Expression to construct a STRUCT datatype.\n\n(Used in BigQuery for example)", "methods": [], "attributes": [], "code_location": {"file": "dialect_ansi.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 901, "end_line": 908}, "type": "class"}, {"name": "StructTypeSegment", "docstring": "STRUCT type as per hive.", "methods": [], "attributes": [], "code_location": {"file": "dialect_sparksql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1168, "end_line": 1171}, "type": "class"}, {"name": "StructTypeSchemaSegment", "docstring": "Expression to construct the schema of a STRUCT datatype.", "methods": [], "attributes": [], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1377, "end_line": 1400}, "type": "class"}, {"name": "StructTypeSchemaSegment", "docstring": "Expression to construct the schema of a STRUCT datatype.", "methods": [], "attributes": [], "code_location": {"file": "dialect_hive.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 290, "end_line": 306}, "type": "class"}, {"name": "StructTypeSchemaSegment", "docstring": "Expression to construct the schema of a STRUCT datatype.", "methods": [], "attributes": [], "code_location": {"file": "dialect_athena.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 364, "end_line": 380}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.1938469409942627}
{"question": "Why does the method that dispatches dialect warnings integrate with the CLI formatter's output routing mechanism to ensure warnings are properly formatted and routed?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "dispatch_dialect_warning", "is_method": true, "class_name": "OutputStreamFormatter", "parameters": ["self", "dialect"], "calls": ["self._dispatch", "self.format_dialect_warning"], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 216, "end_line": 218}, "code_snippet": "    def dispatch_dialect_warning(self, dialect: str) -> None:\n        \"\"\"Dispatch a warning for dialects.\"\"\"\n        self._dispatch(self.format_dialect_warning(dialect))  # pragma: no cover\n", "type": "function"}, {"name": "dispatch_dialect_warning", "is_method": true, "class_name": "FormatterInterface", "parameters": ["self", "dialect"], "calls": [], "code_location": {"file": "formatter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 48, "end_line": 50}, "code_snippet": "    def dispatch_dialect_warning(self, dialect: str) -> None:\n        \"\"\"Dispatch a warning for dialects.\"\"\"\n        ...\n", "type": "function"}, {"name": "format_dialects", "is_method": true, "class_name": "OutputStreamFormatter", "parameters": ["self", "dialect_readout", "verbose"], "calls": ["StringIO", "text_buffer.write", "text_buffer.write", "text_buffer.getvalue", "self.cli_table", "dialect_readout"], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 574, "end_line": 594}, "code_snippet": "    def format_dialects(self, dialect_readout, verbose=0) -> str:\n        \"\"\"Format the dialects yielded by `dialect_readout`.\"\"\"\n        text_buffer = StringIO()\n        text_buffer.write(\"==== sqlfluff - dialects ====\\n\")\n        readouts = [\n            (\n                dialect.label,\n                f\"{dialect.name} dialect [inherits from '{dialect.inherits_from}']\",\n            )\n            for dialect in dialect_readout()\n        ]\n        text_buffer.write(\n            self.cli_table(\n                readouts,\n                col_width=60,\n                cols=1,\n                label_color=Color.blue,\n                val_align=\"right\",\n            )\n        )\n        return text_buffer.getvalue()\n", "type": "function"}, {"name": "format_dialect_warning", "is_method": true, "class_name": "OutputStreamFormatter", "parameters": ["self", "dialect"], "calls": ["self.colorize"], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 596, "end_line": 604}, "code_snippet": "    def format_dialect_warning(self, dialect: str) -> str:\n        \"\"\"Output a warning for parsing errors.\"\"\"\n        return self.colorize(\n            (\n                \"WARNING: Parsing errors found and dialect is set to \"\n                f\"'{dialect}'. Have you configured your dialect correctly?\"\n            ),\n            Color.light,\n        )\n", "type": "function"}, {"name": "test__cli__command_parse_error_dialect_explicit_warning", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 180, "end_line": 200}, "code_snippet": "def test__cli__command_parse_error_dialect_explicit_warning():\n    \"\"\"Check parsing error raises the right warning.\"\"\"\n    # For any parsing error there should be a non-zero exit code\n    # and a human-readable warning should be displayed.\n    # Dialect specified as commandline option.\n    invoke_assert_code(\n        ret_code=1,\n        args=[\n            parse,\n            [\n                \"-n\",\n                \"--dialect\",\n                \"postgres\",\n                \"test/fixtures/cli/fail_many.sql\",\n            ],\n        ],\n        assert_stdout_contains=(\n            \"WARNING: Parsing errors found and dialect is set to 'postgres'. \"\n            \"Have you configured your dialect correctly?\"\n        ),\n    )\n", "type": "function"}, {"name": "test__cli__command_parse_error_dialect_implicit_warning", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 203, "end_line": 224}, "code_snippet": "def test__cli__command_parse_error_dialect_implicit_warning():\n    \"\"\"Check parsing error raises the right warning.\"\"\"\n    # For any parsing error there should be a non-zero exit code\n    # and a human-readable warning should be displayed.\n    # Dialect specified in .sqlfluff config.\n    invoke_assert_code(\n        ret_code=1,\n        args=[\n            # Config sets dialect to tsql\n            parse,\n            [\n                \"-n\",\n                \"--config\",\n                \"test/fixtures/cli/extra_configs/.sqlfluff\",\n                \"test/fixtures/cli/fail_many.sql\",\n            ],\n        ],\n        assert_stdout_contains=(\n            \"WARNING: Parsing errors found and dialect is set to 'tsql'. \"\n            \"Have you configured your dialect correctly?\"\n        ),\n    )\n", "type": "function"}, {"name": "dispatch_config", "is_method": true, "class_name": "OutputStreamFormatter", "parameters": ["self", "linter"], "calls": ["self._dispatch", "self._format_config"], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 151, "end_line": 153}, "code_snippet": "    def dispatch_config(self, linter: Linter) -> None:\n        \"\"\"Dispatch configuration output appropriately.\"\"\"\n        self._dispatch(self._format_config(linter))\n", "type": "function"}, {"name": "test__cli__command_dialect_legacy", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 227, "end_line": 241}, "code_snippet": "def test__cli__command_dialect_legacy():\n    \"\"\"Check the script raises the right exception on a legacy dialect.\"\"\"\n    invoke_assert_code(\n        ret_code=2,\n        args=[\n            lint,\n            [\n                \"-n\",\n                \"--dialect\",\n                \"exasol_fs\",\n                \"test/fixtures/linter/indentation_error_simple.sql\",\n            ],\n        ],\n        assert_stdout_contains=\"Please use the 'exasol' dialect instead.\",\n    )\n", "type": "function"}, {"name": "test__cli__command_dialects", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 960, "end_line": 962}, "code_snippet": "def test__cli__command_dialects():\n    \"\"\"Check dialects command for exceptions.\"\"\"\n    invoke_assert_code(args=[dialects])\n", "type": "function"}, {"name": "test__cli__command_dialect", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 110, "end_line": 124}, "code_snippet": "def test__cli__command_dialect():\n    \"\"\"Check the script raises the right exception on an unknown dialect.\"\"\"\n    # The dialect is unknown should be a non-zero exit code\n    invoke_assert_code(\n        ret_code=2,\n        args=[\n            lint,\n            [\n                \"-n\",\n                \"--dialect\",\n                \"faslkjh\",\n                \"test/fixtures/linter/indentation_error_simple.sql\",\n            ],\n        ],\n    )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.2032532691955566}
{"question": "What is the sequence of state mutations in the private method that computes additional allowed characters with dialect-specific adjustments for the identifier special characters validation rule?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "_get_additional_allowed_characters", "is_method": true, "class_name": "Rule_RF05", "parameters": ["self", "dialect_name"], "calls": ["set", "join", "result.update", "result.update", "result.update"], "code_location": {"file": "RF05.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 58, "end_line": 69}, "code_snippet": "    def _get_additional_allowed_characters(self, dialect_name: str) -> str:\n        \"\"\"Returns additional allowed characters, with adjustments for dialect.\"\"\"\n        result: set[str] = set()\n        if self.additional_allowed_characters:\n            result.update(self.additional_allowed_characters)\n        if dialect_name == \"bigquery\":\n            # In BigQuery, also allow hyphens.\n            result.update(\"-\")\n        if dialect_name == \"snowflake\":\n            # In Snowflake, external stage metadata uses $.\n            result.update(\"$\")\n        return \"\".join(result)\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_RF05", "parameters": ["self", "context"], "calls": ["context.segment.is_type", "self._is_aliased_select_clause_element", "context.segment.is_type", "identifier.replace", "self._get_additional_allowed_characters", "regex.search", "LintResult", "context.segment.raw_normalized", "is_type", "identifier.translate", "identifiers_policy_applicable", "LintResult", "self._init_ignore_words_list", "identifier.lower", "regex.search", "LintResult", "is_type", "is_type", "identifier.replace", "is_type", "is_type", "identifier.replace", "str.maketrans", "identifier.isalnum", "identifier.lower", "identifier.replace"], "code_location": {"file": "RF05.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 71, "end_line": 225}, "code_snippet": "    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Do not use special characters in object names.\"\"\"\n        # Config type hints\n        self.quoted_identifiers_policy: str\n        self.unquoted_identifiers_policy: str\n        self.allow_space_in_identifier: bool\n        self.additional_allowed_characters: str\n        self.ignore_words: str\n        self.ignore_words_regex: str\n\n        # Confirm it's a single identifier.\n        assert context.segment.is_type(\"naked_identifier\", \"quoted_identifier\")\n\n        # Get the ignore_words_list configuration.\n        try:\n            ignore_words_list = self.ignore_words_list\n        except AttributeError:\n            # First-time only, read the settings from configuration. This is\n            # very slow.\n            ignore_words_list = self._init_ignore_words_list()\n\n        # Assume unquoted (we'll update if quoted)\n        policy = self.unquoted_identifiers_policy\n\n        identifier = context.segment.raw\n\n        # Skip if in ignore list\n        if ignore_words_list and identifier.lower() in ignore_words_list:\n            return None\n\n        # Skip if matches ignore regex\n        if self.ignore_words_regex and regex.search(\n            self.ignore_words_regex, identifier\n        ):\n            return LintResult(memory=context.memory)\n\n        if self._is_aliased_select_clause_element(context):\n            # If selects are aliased, ignore unaliased column reference\n            return None\n\n        # Do some extra processing for quoted identifiers.\n        if context.segment.is_type(\"quoted_identifier\"):\n            # Update the default policy to quoted\n            policy = self.quoted_identifiers_policy\n\n            # Strip the quotes first\n            identifier = context.segment.raw_normalized(casefold=False)\n\n            # Skip if in ignore list - repeat check now we've strip the quotes\n            if ignore_words_list and identifier.lower() in ignore_words_list:\n                return None\n\n            # Skip if matches ignore regex - repeat check now we've strip the quotes\n            if self.ignore_words_regex and regex.search(\n                self.ignore_words_regex, identifier\n            ):\n                return LintResult(memory=context.memory)\n\n            # PostgreSQL Extension allows the use of extensions.\n            #\n            # These extensions are often qutoed identifiers.\n            # (https://www.postgresql.org/docs/current/contrib.html)\n            #\n            # Allow quoted identifiers in extension references\n            if (\n                context.dialect.name in [\"postgres\"]\n                and context.parent_stack\n                and context.parent_stack[-1].is_type(\"extension_reference\")\n            ):\n                return None\n\n            # BigQuery table references are quoted in back ticks so allow dots\n            #\n            # It also allows a star at the end of table_references for wildcards\n            # (https://cloud.google.com/bigquery/docs/querying-wildcard-tables)\n            #\n            # Strip both out before testing the identifier\n            if (\n                context.dialect.name in [\"bigquery\"]\n                and context.parent_stack\n                and context.parent_stack[-1].is_type(\"table_reference\")\n            ):\n                if identifier and identifier[-1] == \"*\":\n                    identifier = identifier[:-1]\n                identifier = identifier.replace(\".\", \"\")\n\n            # Databricks & SparkSQL file references for direct file query\n            # are quoted in back ticks to allow for identifiers common\n            # in file paths and regex patterns for path globbing\n            # https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-file.html\n            #\n            # Path Glob Filters (done inline for SQL direct file query)\n            # https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html#path-global-filter\n            #\n\n            if (\n                context.dialect.name in [\"databricks\", \"sparksql\"]\n                and context.parent_stack\n            ):\n                # Databricks & SparkSQL file references for direct file query\n                # are quoted in back ticks to allow for identifiers common\n                # in file paths and regex patterns for path globbing\n                # https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-file.html\n                #\n                # Path Glob Filters (done inline for SQL direct file query)\n                # https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html#path-global-filter\n                #\n                if context.parent_stack[-1].is_type(\"file_reference\"):\n                    return None\n\n                # Databricks & SparkSQL properties keys\n                # used for setting table and runtime\n                # configurations denote namespace using dots, so these are\n                # removed before testing L057 to not trigger false positives\n                # Runtime configurations:\n                # https://spark.apache.org/docs/latest/configuration.html#application-properties\n                # Example configurations for table:\n                # https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#configuration\n                #\n                if context.parent_stack[-1].is_type(\"property_name_identifier\"):\n                    identifier = identifier.replace(\".\", \"\")\n\n            # Strip spaces if allowed (note a separate config as only valid for quoted\n            # identifiers)\n            if self.allow_space_in_identifier:\n                identifier = identifier.replace(\" \", \"\")\n\n        # We always allow underscores so strip them out\n        identifier = identifier.replace(\"_\", \"\")\n\n        # redshift allows a # at the beginning of temporary table names\n        if (\n            context.dialect.name == \"redshift\"\n            and identifier[0] == \"#\"\n            and context.parent_stack\n            and context.parent_stack[-1].is_type(\"table_reference\")\n        ):\n            identifier = identifier[1:]\n\n        # Set the identified minus the allowed characters\n        additional_allowed_characters = self._get_additional_allowed_characters(\n            context.dialect.name\n        )\n        if additional_allowed_characters:\n            identifier = identifier.translate(\n                str.maketrans(\"\", \"\", additional_allowed_characters)\n            )\n\n        # Finally test if the remaining identifier is only made up of alphanumerics\n        if identifiers_policy_applicable(policy, context.parent_stack) and not (\n            identifier.isalnum()\n        ):\n            return LintResult(anchor=context.segment)\n\n        return None\n", "type": "function"}, {"name": "Rule_RF05", "docstring": "Do not use special characters in identifiers.\n\n**Anti-pattern**\n\nUsing special characters within identifiers when creating or aliasing objects.\n\n.. code-block:: sql\n\n    CREATE TABLE DBO.ColumnNames\n    (\n        [Internal Space] INT,\n        [Greater>Than] INT,\n        [Less<Than] INT,\n        Number# INT\n    )\n\n**Best practice**\n\nIdentifiers should include only alphanumerics and underscores.\n\n.. code-block:: sql\n\n    CREATE TABLE DBO.ColumnNames\n    (\n        [Internal_Space] INT,\n        [GreaterThan] INT,\n        [LessThan] INT,\n        NumberVal INT\n    )", "methods": ["_get_additional_allowed_characters", "_eval", "_init_ignore_words_list", "_is_aliased_select_clause_element"], "attributes": ["name", "aliases", "groups", "config_keywords", "crawl_behaviour"], "code_location": {"file": "RF05.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 12, "end_line": 247}, "type": "class"}, {"name": "_eval", "is_method": true, "class_name": "Rule_CP02", "parameters": ["self", "context"], "calls": ["identifiers_policy_applicable", "_eval", "LintResult", "super"], "code_location": {"file": "CP02.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/capitalisation", "start_line": 98, "end_line": 116}, "code_snippet": "    def _eval(self, context: RuleContext) -> Optional[list[LintResult]]:\n        # Return None if identifier is case-sensitive property to enable Change\n        # Data Feed\n        # https://docs.delta.io/2.0.0/delta-change-data-feed.html#enable-change-data-feed\n        if (\n            context.dialect.name in [\"databricks\", \"sparksql\"]\n            and context.parent_stack\n            and context.parent_stack[-1].type == \"property_name_identifier\"\n            and context.segment.raw == \"enableChangeDataFeed\"\n        ):\n            return None\n\n        if identifiers_policy_applicable(\n            self.unquoted_identifiers_policy,  # type: ignore\n            context.parent_stack,\n        ):\n            return super()._eval(context=context)\n        else:\n            return [LintResult(memory=context.memory)]\n", "type": "function"}, {"name": "TdColumnConstraintSegment", "docstring": "Teradata specific column attributes.\n\ne.g. CHARACTER SET LATIN | [NOT] (CASESPECIFIC|CS) | (UPPERCASE|UC)", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_teradata.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 487, "end_line": 514}, "type": "class"}, {"name": "Rule_RF06", "docstring": "Unnecessary quoted identifier.\n\nThis rule will fail if the quotes used to quote an identifier are (un)necessary\ndepending on the ``force_quote_identifier`` configuration. This rule applies to\nboth column *references* and their *aliases*. The *default* (safe) behaviour is\ndesigned not to unexpectedly corrupt SQL. That means the circumstances in which\nquotes can be safely removed depends on the current dialect would resolve the\nunquoted variant of the identifier (see below for examples).\n\nAdditionally this rule may be configured to a more aggressive setting by setting\n:code:`case_sensitive` to :code:`False`, in which case quotes will be removed\nregardless of the casing of the contained identifier. Any identifiers which contain\nspecial characters, spaces or keywords will still be left quoted. This setting is\nmore appropriate for projects or teams where there is more control over the inputs\nand outputs of queries, and where it's more viable to institute rules such\nas enforcing that all identifiers are the default casing (and therefore meaning\nthat using quotes to change the case of identifiers is unnecessary).\n\n.. list-table::\n   :widths: 26 26 48\n   :header-rows: 1\n\n   * - Dialect group\n     -  Example where quotes are safe to remove.\n     -  Examples where quotes are not safe to remove.\n   * - Natively :code:`UPPERCASE` dialects e.g. Snowflake, BigQuery,\n       TSQL & Oracle.\n     - Identifiers which, without quotes, would resolve to the default\n       casing of :code:`FOO` i.e. :code:`\"FOO\"`.\n     - Identifiers where the quotes are necessary to preserve case\n       (e.g. :code:`\"Foo\"` or :code:`\"foo\"`), or where the identifier\n       contains something invalid without the quotes such as keywords\n       or special characters e.g. :code:`\"SELECT\"`, :code:`\"With Space\"`\n       or :code:`\"Special&Characters\"`.\n   * - Natively :code:`lowercase` dialects e.g. Athena,\n       Hive & Postgres\n     - Identifiers which, without quotes, would resolve to the default\n       casing of :code:`foo` i.e. :code:`\"foo\"`.\n     - Identifiers where the quotes are necessary to preserve case\n       (e.g. :code:`\"Foo\"` or :code:`\"foo\"`), or where the identifier\n       contains something invalid without the quotes such as keywords\n       or special characters e.g. :code:`\"SELECT\"`, :code:`\"With Space\"`\n       or :code:`\"Special&Characters\"`.\n   * - Case insensitive dialects e.g. :ref:`duckdb_dialect_ref` or\n       :ref:`sparksql_dialect_ref`\n     - Any identifiers which are valid without quotes: e.g. :code:`\"FOO\"`,\n       :code:`\"foo\"`, :code:`\"Foo\"`, :code:`\"fOo\"`, :code:`FOO` and\n       :code:`foo` would all resolve to the same object.\n     - Identifiers which contain something invalid without the quotes\n       such as keywords or special characters e.g. :code:`\"SELECT\"`,\n       :code:`\"With Space\"` or :code:`\"Special&Characters\"`.\n\nThis rule is closely associated with (and constrained by the same above\nfactors) as :sqlfluff:ref:`aliasing.self_alias.column` (:sqlfluff:ref:`AL09`).\n\nWhen ``prefer_quoted_identifiers = False`` (default behaviour), the quotes are\nunnecessary, except for reserved keywords and special characters in identifiers.\n\n**Anti-pattern**\n\nIn this example, valid unquoted identifiers,\nthat are not also reserved keywords, are needlessly quoted.\n\n.. code-block:: sql\n\n    SELECT \"foo\" as \"bar\";  -- For lowercase dialects like Postgres\n    SELECT \"FOO\" as \"BAR\";  -- For uppercase dialects like Snowflake\n\n**Best practice**\n\nUse unquoted identifiers where possible.\n\n.. code-block:: sql\n\n    SELECT foo as bar;  -- For lowercase dialects like Postgres\n    SELECT FOO as BAR;  -- For uppercase dialects like Snowflake\n\n    -- Note that where the case of the quoted identifier requires\n    -- the quotes to remain, or where the identifier cannot be\n    -- unquoted because it would be invalid to do so, the quotes\n    -- may remain. For example:\n    SELECT\n        \"Case_Sensitive_Identifier\" as is_allowed,\n        \"Identifier with spaces or speci@l characters\" as this_too,\n        \"SELECT\" as also_reserved_words\n    FROM \"My Table With Spaces\"\n\nWhen ``prefer_quoted_identifiers = True``, the quotes are always necessary, no\nmatter if the identifier is valid, a reserved keyword, or contains special\ncharacters.\n\n.. note::\n   Note due to different quotes being used by different dialects supported by\n   `SQLFluff`, and those quotes meaning different things in different contexts,\n   this mode is not ``sqlfluff fix`` compatible.\n\n**Anti-pattern**\n\nIn this example, a valid unquoted identifier, that is also not a reserved keyword,\nis required to be quoted.\n\n.. code-block:: sql\n\n    SELECT 123 as foo\n\n**Best practice**\nUse quoted identifiers.\n\n.. code-block:: sql\n\n    SELECT 123 as \"foo\" -- For ANSI, ...\n    -- or\n    SELECT 123 as `foo` -- For BigQuery, MySql, ...", "methods": ["_eval", "ignore_words_list"], "attributes": ["name", "aliases", "groups", "config_keywords", "crawl_behaviour", "is_fix_compatible"], "code_location": {"file": "RF06.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 17, "end_line": 294}, "type": "class"}, {"name": "AlterSequenceOptionsSegment", "docstring": "Dialect-specific options for ALTER SEQUENCE statement.\n\nAs specified in https://www.postgresql.org/docs/13/sql-altersequence.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_postgres.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 4679, "end_line": 4727}, "type": "class"}, {"name": "SecretsEqualsSegment", "docstring": "SECRETS clause.\n\nhttps://docs.snowflake.com/en/sql-reference/sql/alter-function\nhttps://docs.snowflake.com/en/sql-reference/sql/create-external-access-integration", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_snowflake.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2850, "end_line": 2879}, "type": "class"}, {"name": "_create_base_is_null_sequence", "is_method": false, "class_name": null, "parameters": ["is_upper", "operator_raw"], "calls": ["KeywordSegment", "KeywordSegment", "WhitespaceSegment"], "code_location": {"file": "CV05.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "start_line": 111, "end_line": 124}, "code_snippet": "def _create_base_is_null_sequence(\n    is_upper: bool,\n    operator_raw: str,\n) -> CorrectionListType:\n    is_seg = KeywordSegment(\"IS\" if is_upper else \"is\")\n    not_seg = KeywordSegment(\"NOT\" if is_upper else \"not\")\n    if operator_raw == \"=\":\n        return [is_seg]\n\n    return [\n        is_seg,\n        WhitespaceSegment(),\n        not_seg,\n    ]\n", "type": "function"}, {"name": "_cross_join_supported", "is_method": true, "class_name": "Rule_AM08", "parameters": ["context"], "calls": ["context.dialect.sets", "context.dialect.sets"], "code_location": {"file": "AM08.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/ambiguous", "start_line": 108, "end_line": 113}, "code_snippet": "    def _cross_join_supported(context: RuleContext) -> bool:\n        return (\n            False\n            or \"CROSS\" in context.dialect.sets(\"reserved_keywords\")\n            or \"CROSS\" in context.dialect.sets(\"unreserved_keywords\")\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.2074036598205566}
{"question": "Why does the segment class that represents operator class references integrate with the dialect's type system to enable proper parsing and validation in index creation and constraint definitions?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "OperatorClassReferenceSegment", "docstring": "A reference to an operator class.", "methods": [], "attributes": ["type"], "code_location": {"file": "dialect_postgres.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1040, "end_line": 1043}, "type": "class"}, {"name": "_get_seg", "is_method": false, "class_name": null, "parameters": ["class_def", "dialect"], "calls": ["cast", "dialect.get_segment"], "code_location": {"file": "ST05.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 529, "end_line": 530}, "code_snippet": "def _get_seg(class_def: S, dialect: Dialect) -> S:\n    return cast(S, dialect.get_segment(class_def.__name__))\n", "type": "function"}, {"name": "IndexTypeReferenceSegment", "docstring": "A reference to an indextype.", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_oracle.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1113, "end_line": 1118}, "type": "class"}, {"name": "IndexElementSegment", "docstring": "Index element segment.\n\nAs found in https://www.postgresql.org/docs/15/sql-altertable.html.\nhttps://github.com/postgres/postgres/blob/4380c2509d51febad34e1fac0cfaeb98aaa716c5/src/backend/parser/gram.y#L8089", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_postgres.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 4072, "end_line": 4090}, "type": "class"}, {"name": "ColumnTypeReferenceSegment", "docstring": "A column type reference segment (e.g. `table_name.column_name%type`).\n\nhttps://www.postgresql.org/docs/current/sql-createfunction.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_postgres.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 6664, "end_line": 6674}, "type": "class"}, {"name": "IndexElementOptionsSegment", "docstring": "Index element options segment.\n\nhttps://github.com/postgres/postgres/blob/4380c2509d51febad34e1fac0cfaeb98aaa716c5/src/backend/parser/gram.y#L8057", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_postgres.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 4049, "end_line": 4069}, "type": "class"}, {"name": "ColumnConstraintSegment", "docstring": "A column option; each CREATE TABLE column can have 0 or more.\n\nOverriding ColumnConstraintSegment to allow for additional segment parsing\nand to support on conflict clauses.", "methods": [], "attributes": [], "code_location": {"file": "dialect_sqlite.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 730, "end_line": 777}, "type": "class"}, {"name": "IndexAccessMethodSegment", "docstring": "Index access method (e.g. `USING gist`).", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_postgres.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1033, "end_line": 1037}, "type": "class"}, {"name": "DatatypeSegment", "docstring": "A data type segment.\n\nSupports timestamp with(out) time zone. Doesn't currently support intervals.", "methods": [], "attributes": [], "code_location": {"file": "dialect_ansi.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1018, "end_line": 1059}, "type": "class"}, {"name": "IndexReferenceSegment", "docstring": "A reference to an index.\n\nOverriding to capture TSQL's override of ObjectReferenceSegment", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2199, "end_line": 2205}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.2329192161560059}
{"question": "Where does the return value of the substring location method on the remaining unprocessed string buffer in the exact string matcher class determine whether the position finder method returns a tuple or None?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "search", "is_method": true, "class_name": "RegexLexer", "parameters": ["self", "forward_string"], "calls": ["self._compiled_regex.search", "match.group", "match.span", "lexer_logger.warning"], "code_location": {"file": "lexer.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 334, "end_line": 346}, "code_snippet": "    def search(self, forward_string: str) -> Optional[tuple[int, int]]:\n        \"\"\"Use regex to find a substring.\"\"\"\n        match = self._compiled_regex.search(forward_string)\n        if match:\n            # We can only match strings with length\n            if match.group(0):\n                return match.span()\n            else:  # pragma: no cover\n                lexer_logger.warning(\n                    f\"Zero length Lex item returned from {self.name!r}. Report this as \"\n                    \"a bug.\"\n                )\n        return None\n", "type": "function"}, {"name": "working_loc_after", "is_method": true, "class_name": "PositionMarker", "parameters": ["self", "raw"], "calls": ["self.infer_next_position"], "code_location": {"file": "markers.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 75, "end_line": 81}, "code_snippet": "    def working_loc_after(self, raw: str) -> tuple[int, int]:\n        \"\"\"Location tuple for the working position.\"\"\"\n        return self.infer_next_position(\n            raw,\n            self.working_line_no,\n            self.working_line_pos,\n        )\n", "type": "function"}, {"name": "working_loc", "is_method": true, "class_name": "PositionMarker", "parameters": ["self"], "calls": [], "code_location": {"file": "markers.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 71, "end_line": 73}, "code_snippet": "    def working_loc(self) -> tuple[int, int]:\n        \"\"\"Location tuple for the working position.\"\"\"\n        return self.working_line_no, self.working_line_pos\n", "type": "function"}, {"name": "assert_matches", "is_method": false, "class_name": null, "parameters": ["instring", "matcher", "matchstring"], "calls": ["matcher.match", "isinstance", "len", "len"], "code_location": {"file": "lexer_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser", "start_line": 16, "end_line": 33}, "code_snippet": "def assert_matches(instring, matcher, matchstring):\n    \"\"\"Assert that a matcher does or doesn't work on a string.\n\n    The optional `matchstring` argument, which can optionally\n    be None, allows to either test positive matching of a\n    particular string or negative matching (that it explicitly)\n    doesn't match.\n    \"\"\"\n    res = matcher.match(instring)\n    # Check we've got the right type\n    assert isinstance(res, LexMatch)\n    if matchstring is None:\n        assert res.forward_string == instring\n        assert res.elements == []\n    else:\n        assert res.forward_string == instring[len(matchstring) :]\n        assert len(res.elements) == 1\n        assert res.elements[0].raw == matchstring\n", "type": "function"}, {"name": "get_end_loc", "is_method": true, "class_name": "BaseSegment", "parameters": ["self"], "calls": ["self.pos_marker.working_loc_after"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 773, "end_line": 778}, "code_snippet": "    def get_end_loc(self) -> tuple[int, int]:\n        \"\"\"Get a location tuple at the end of this segment.\"\"\"\n        assert self.pos_marker, f\"{self} has no PositionMarker\"\n        return self.pos_marker.working_loc_after(\n            self.raw,\n        )\n", "type": "function"}, {"name": "_match", "is_method": true, "class_name": "StringLexer", "parameters": ["self", "forward_string"], "calls": ["forward_string.startswith", "LexedElement"], "code_location": {"file": "lexer.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 165, "end_line": 170}, "code_snippet": "    def _match(self, forward_string: str) -> Optional[LexedElement]:\n        \"\"\"The private match function. Just look for a literal string.\"\"\"\n        if forward_string.startswith(self.template):\n            return LexedElement(self.template, self)\n        else:\n            return None\n", "type": "function"}, {"name": "check_parse_cache", "is_method": true, "class_name": "ParseContext", "parameters": ["self", "loc_key", "matcher_key"], "calls": ["self._parse_cache.get"], "code_location": {"file": "context.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 299, "end_line": 306}, "code_snippet": "    def check_parse_cache(\n        self, loc_key: tuple[Any, ...], matcher_key: str\n    ) -> Optional[\"MatchResult\"]:\n        \"\"\"Check against the parse cache for a pre-existing match.\n\n        If no match is found in the cache, this returns None.\n        \"\"\"\n        return self._parse_cache.get((loc_key, matcher_key))\n", "type": "function"}, {"name": "findall", "is_method": false, "class_name": null, "parameters": ["substr", "in_str"], "calls": ["in_str.find", "in_str.find"], "code_location": {"file": "string.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/helpers", "start_line": 15, "end_line": 26}, "code_snippet": "def findall(substr: str, in_str: str) -> Iterator[int]:\n    \"\"\"Yields all the positions sbstr within in_str.\n\n    https://stackoverflow.com/questions/4664850/how-to-find-all-occurrences-of-a-substring\n    \"\"\"\n    # Return nothing if one of the inputs is trivial\n    if not substr or not in_str:\n        return\n    idx = in_str.find(substr)\n    while idx != -1:\n        yield idx\n        idx = in_str.find(substr, idx + 1)\n", "type": "function"}, {"name": "match", "is_method": true, "class_name": "StringLexer", "parameters": ["self", "forward_string"], "calls": ["self._match", "len", "ValueError", "self._subdivide", "LexMatch", "LexMatch", "len"], "code_location": {"file": "lexer.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 264, "end_line": 284}, "code_snippet": "    def match(self, forward_string: str) -> LexMatch:\n        \"\"\"Given a string, match what we can and return the rest.\n\n        Returns:\n            :obj:`LexMatch`\n\n        \"\"\"\n        if len(forward_string) == 0:  # pragma: no cover\n            raise ValueError(\"Unexpected empty string!\")\n        matched = self._match(forward_string)\n\n        if matched:\n            # Handle potential subdivision elsewhere.\n            new_elements = self._subdivide(matched)\n\n            return LexMatch(\n                forward_string[len(matched.raw) :],\n                new_elements,\n            )\n        else:\n            return LexMatch(forward_string, [])\n", "type": "function"}, {"name": "templated_position", "is_method": true, "class_name": "PositionMarker", "parameters": ["self"], "calls": ["self.templated_file.get_line_pos_of_char_pos"], "code_location": {"file": "markers.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 150, "end_line": 154}, "code_snippet": "    def templated_position(self) -> tuple[int, int]:\n        \"\"\"Return the line and position of this marker in the source.\"\"\"\n        return self.templated_file.get_line_pos_of_char_pos(\n            self.templated_slice.start, source=False\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3471536636352539}
{"question": "Where does the table reference segment parsed from the table-like clause flow through the bracketed delimited identifier grammar to determine which columns are included in the table creation statement?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "TableReferenceSegment", "docstring": "A reference to an table, CTE, subquery or alias.\n\nOverload for DuckDB as only tables can be single quoted identifiers\nwhen used by the httpfs extension.", "methods": ["iter_raw_references"], "attributes": ["match_grammar", "match_grammar", "type", "type", "type", "type", "type"], "code_location": {"file": "dialect_duckdb.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 621, "end_line": 630}, "type": "class"}, {"name": "_create_col_reference", "is_method": false, "class_name": null, "parameters": ["table_ref", "column_name"], "calls": ["ColumnReferenceSegment", "IdentifierSegment", "SymbolSegment", "IdentifierSegment"], "code_location": {"file": "ST07.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 217, "end_line": 223}, "code_snippet": "def _create_col_reference(table_ref: str, column_name: str) -> ColumnReferenceSegment:\n    segments = (\n        IdentifierSegment(raw=table_ref, type=\"naked_identifier\"),\n        SymbolSegment(raw=\".\", type=\"symbol\"),\n        IdentifierSegment(raw=column_name, type=\"naked_identifier\"),\n    )\n    return ColumnReferenceSegment(segments=segments, pos_marker=None)\n", "type": "function"}, {"name": "TableExpressionSegment", "docstring": "The main table expression e.g. within a FROM clause.\n\nIn SQL standard, as well as T-SQL, table expressions (`table reference` in SQL\nstandard) can also be join tables, optionally bracketed, allowing for nested joins.", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 4459, "end_line": 4491}, "type": "class"}, {"name": "_create_table_ref", "is_method": false, "class_name": null, "parameters": ["table_name", "dialect"], "calls": ["partial", "Seg", "Seg", "cast", "TableExpressionSeg", "dialect.get_segment", "TableReferenceSeg", "IdentifierSegment"], "code_location": {"file": "ST05.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 556, "end_line": 574}, "code_snippet": "def _create_table_ref(table_name: str, dialect: Dialect) -> TableExpressionSegment:\n    Seg = partial(_get_seg, dialect=dialect)\n    TableExpressionSeg = Seg(TableExpressionSegment)\n    TableReferenceSeg = Seg(TableReferenceSegment)\n    IdentifierSegment = cast(\n        type[CodeSegment], dialect.get_segment(\"IdentifierSegment\")\n    )\n    return TableExpressionSeg(\n        segments=(\n            TableReferenceSeg(\n                segments=(\n                    IdentifierSegment(\n                        raw=table_name,\n                        type=\"naked_identifier\",\n                    ),\n                ),\n            ),\n        ),\n    )\n", "type": "function"}, {"name": "TableReferenceSegment", "docstring": "A reference to an table, CTE, subquery or alias.\n\nExtended from ANSI to allow Database Link syntax using AtSignSegment", "methods": [], "attributes": [], "code_location": {"file": "dialect_oracle.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1234, "end_line": 1263}, "type": "class"}, {"name": "TableReferenceSegment", "docstring": "A reference to an table, CTE, subquery or alias.\n\nOverriding to capture TSQL's override of ObjectReferenceSegment", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2145, "end_line": 2178}, "type": "class"}, {"name": "TableReferenceSegment", "docstring": "A reference to an table, CTE, subquery or alias.", "methods": [], "attributes": [], "code_location": {"file": "dialect_databricks.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 463, "end_line": 466}, "type": "class"}, {"name": "TableReferenceSegment", "docstring": "A reference to an table, CTE, subquery or alias.", "methods": [], "attributes": [], "code_location": {"file": "dialect_ansi.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1169, "end_line": 1172}, "type": "class"}, {"name": "TableReferenceSegment", "docstring": "A reference to an object that may contain embedded hyphens.", "methods": [], "attributes": [], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1570, "end_line": 1650}, "type": "class"}, {"name": "TableReferenceSegment", "docstring": "A reference to a table.\n\nAlso allows `table->path` and `table->>path` for JSON values.\nhttps://www.sqlite.org/json1.html#jptr", "methods": [], "attributes": [], "code_location": {"file": "dialect_sqlite.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 491, "end_line": 511}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.34294915199279785}
{"question": "Where in the sequence matching method are optional clause references checked when evaluating the match grammar attribute of the CREATE TABLE statement segment class?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "TableClausesSegment", "docstring": "Clauses for a table specification.\n\nhttps://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table-spec.html", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_databricks.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1499, "end_line": 1518}, "type": "class"}, {"name": "CreateTableStatementSegment", "docstring": "Create table segment.\n\nhttps://dev.mysql.com/doc/refman/8.0/en/create-table.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_mysql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 430, "end_line": 671}, "type": "class"}, {"name": "TableEndClauseSegment", "docstring": "Support Table Options at end of tables.\n\nhttps://www.sqlite.org/syntax/table-options.html", "methods": [], "attributes": ["type", "type"], "code_location": {"file": "dialect_sqlite.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 545, "end_line": 552}, "type": "class"}, {"name": "TableOptionSegment", "docstring": "TABLE option in `CREATE TABLE` statement.\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-transact-sql", "methods": [], "attributes": ["_ledger_view_option", "_on_partitions", "type", "match_grammar"], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1664, "end_line": 1872}, "type": "class"}, {"name": "ReferencesConstraintGrammar", "docstring": "REFERENCES constraint option in `CREATE TABLE` statement.\n\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-transact-sql", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1875, "end_line": 1893}, "type": "class"}, {"name": "MatchRecognizeClauseSegment", "docstring": "A `MATCH_RECOGNIZE` clause.\n\nhttps://docs.snowflake.com/en/sql-reference/constructs/match_recognize.html", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_snowflake.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1564, "end_line": 1663}, "type": "class"}, {"name": "CreateTableLikeClauseSegment", "docstring": "`CREATE TABLE` LIKE clause.", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_exasol.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1155, "end_line": 1174}, "type": "class"}, {"name": "CreateSequenceOptionsSegment", "docstring": "Options for Create Sequence statement.\n\nAs specified in https://www.postgresql.org/docs/13/sql-createsequence.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_postgres.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 4619, "end_line": 4658}, "type": "class"}, {"name": "CreateTableStatementSegment", "docstring": "`CREATE TABLE` segment.\n\nhttps://mariadb.com/kb/en/create-table/", "methods": [], "attributes": [], "code_location": {"file": "dialect_mariadb.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 74, "end_line": 315}, "type": "class"}, {"name": "CreateTableStatementSegment", "docstring": "`CREATE TABLE` statement.\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_table_statement", "methods": [], "attributes": [], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1959, "end_line": 2001}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.3358006477355957}
{"question": "Where is the testing utility function that invokes CLI commands and asserts return codes, which the dialect validation test function delegates to, located?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "test__cli__command_dialects", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 960, "end_line": 962}, "code_snippet": "def test__cli__command_dialects():\n    \"\"\"Check dialects command for exceptions.\"\"\"\n    invoke_assert_code(args=[dialects])\n", "type": "function"}, {"name": "invoke_assert_code", "is_method": false, "class_name": null, "parameters": ["ret_code", "args", "kwargs", "cli_input", "assert_stdout_contains", "assert_stderr_contains", "raise_exceptions"], "calls": ["runner.invoke", "print", "CliRunner", "CliRunner", "inspect.signature", "result.stdout.replace", "result.stderr.replace", "isinstance"], "code_location": {"file": "cli.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/testing", "start_line": 9, "end_line": 43}, "code_snippet": "def invoke_assert_code(\n    ret_code: int = 0,\n    args: Optional[list[Any]] = None,\n    kwargs: Optional[dict[str, Any]] = None,\n    cli_input: Optional[str] = None,\n    assert_stdout_contains: str = \"\",\n    assert_stderr_contains: str = \"\",\n    raise_exceptions: bool = True,\n) -> Result:\n    \"\"\"Invoke a command and check return code.\"\"\"\n    args = args or []\n    kwargs = kwargs or {}\n    if cli_input:\n        kwargs[\"input\"] = cli_input\n    if \"mix_stderr\" in inspect.signature(CliRunner).parameters:  # pragma: no cover\n        runner = CliRunner(mix_stderr=False)  # type: ignore[call-arg,unused-ignore]\n    else:  # pragma: no cover\n        runner = CliRunner()\n    result = runner.invoke(*args, **kwargs)\n    # Output the CLI code for debugging\n    print(result.output)\n    if assert_stdout_contains != \"\":\n        # The replace command just accounts for cross platform testing.\n        assert assert_stdout_contains in result.stdout.replace(\"\\\\\", \"/\")\n    if assert_stderr_contains != \"\":\n        # The replace command just accounts for cross platform testing.\n        assert assert_stderr_contains in result.stderr.replace(\"\\\\\", \"/\")\n    # Check return codes, and unless we specifically want to pass back exceptions,\n    # we should raise any exceptions which aren't `SystemExit` ones (i.e. ones\n    # raised by `sys.exit()`)\n    if raise_exceptions and result.exception:\n        if not isinstance(result.exception, SystemExit):\n            raise result.exception  # pragma: no cover\n    assert ret_code == result.exit_code\n    return result\n", "type": "function"}, {"name": "test__cli__command_dialect", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 110, "end_line": 124}, "code_snippet": "def test__cli__command_dialect():\n    \"\"\"Check the script raises the right exception on an unknown dialect.\"\"\"\n    # The dialect is unknown should be a non-zero exit code\n    invoke_assert_code(\n        ret_code=2,\n        args=[\n            lint,\n            [\n                \"-n\",\n                \"--dialect\",\n                \"faslkjh\",\n                \"test/fixtures/linter/indentation_error_simple.sql\",\n            ],\n        ],\n    )\n", "type": "function"}, {"name": "test__cli__command_fix_stdin_error_exit_code", "is_method": false, "class_name": null, "parameters": ["sql", "exit_code", "params", "assert_stderr_contains"], "calls": ["pytest.mark.parametrize", "invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 1380, "end_line": 1389}, "code_snippet": "def test__cli__command_fix_stdin_error_exit_code(\n    sql, exit_code, params, assert_stderr_contains\n):\n    \"\"\"Check that the CLI fails nicely if fixing a templated stdin.\"\"\"\n    invoke_assert_code(\n        ret_code=exit_code,\n        args=[fix, ((params,) if params else ()) + (\"--dialect=ansi\", \"-\")],\n        cli_input=sql,\n        assert_stderr_contains=assert_stderr_contains,\n    )\n", "type": "function"}, {"name": "test__cli__command_no_dialect", "is_method": false, "class_name": null, "parameters": ["command"], "calls": ["pytest.mark.parametrize", "invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 137, "end_line": 151}, "code_snippet": "def test__cli__command_no_dialect(command):\n    \"\"\"Check the script raises the right exception no dialect.\"\"\"\n    # The dialect is unknown should be a non-zero exit code\n    result = invoke_assert_code(\n        ret_code=2,\n        args=[\n            command,\n            [\"-\"],\n        ],\n        cli_input=\"SELECT 1\",\n    )\n    assert \"User Error\" in result.stderr\n    assert \"No dialect was specified\" in result.stderr\n    # No traceback should be in the output\n    assert \"Traceback (most recent call last)\" not in result.stderr\n", "type": "function"}, {"name": "test__cli__command_no_dialect_stdin_filename_inline_dialect", "is_method": false, "class_name": null, "parameters": ["command"], "calls": ["pytest.mark.parametrize", "invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 163, "end_line": 177}, "code_snippet": "def test__cli__command_no_dialect_stdin_filename_inline_dialect(command):\n    \"\"\"Check the script runs with no dialect but has an inline configuration.\"\"\"\n    # The dialect is unknown should be a non-zero exit code\n    result = invoke_assert_code(\n        ret_code=0,\n        args=[\n            command,\n            [\"--stdin-filename\", \"test.sql\", \"-\"],\n        ],\n        cli_input=\"-- sqlfluff:dialect:ansi\\nSELECT 1\\n\",\n    )\n    assert \"User Error\" not in result.stderr\n    assert \"No dialect was specified\" not in result.stderr\n    # No traceback should be in the output\n    assert \"Traceback (most recent call last)\" not in result.stderr\n", "type": "function"}, {"name": "test__cli__command_dialect_legacy", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 227, "end_line": 241}, "code_snippet": "def test__cli__command_dialect_legacy():\n    \"\"\"Check the script raises the right exception on a legacy dialect.\"\"\"\n    invoke_assert_code(\n        ret_code=2,\n        args=[\n            lint,\n            [\n                \"-n\",\n                \"--dialect\",\n                \"exasol_fs\",\n                \"test/fixtures/linter/indentation_error_simple.sql\",\n            ],\n        ],\n        assert_stdout_contains=\"Please use the 'exasol' dialect instead.\",\n    )\n", "type": "function"}, {"name": "test__cli__command_lint_parse_with_retcode", "is_method": false, "class_name": null, "parameters": ["command", "ret_code"], "calls": ["pytest.mark.parametrize", "invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 787, "end_line": 789}, "code_snippet": "def test__cli__command_lint_parse_with_retcode(command, ret_code):\n    \"\"\"Check commands expecting a non-zero ret code.\"\"\"\n    invoke_assert_code(ret_code=ret_code, args=command)\n", "type": "function"}, {"name": "test__cli__command_parse_error_dialect_implicit_warning", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 203, "end_line": 224}, "code_snippet": "def test__cli__command_parse_error_dialect_implicit_warning():\n    \"\"\"Check parsing error raises the right warning.\"\"\"\n    # For any parsing error there should be a non-zero exit code\n    # and a human-readable warning should be displayed.\n    # Dialect specified in .sqlfluff config.\n    invoke_assert_code(\n        ret_code=1,\n        args=[\n            # Config sets dialect to tsql\n            parse,\n            [\n                \"-n\",\n                \"--config\",\n                \"test/fixtures/cli/extra_configs/.sqlfluff\",\n                \"test/fixtures/cli/fail_many.sql\",\n            ],\n        ],\n        assert_stdout_contains=(\n            \"WARNING: Parsing errors found and dialect is set to 'tsql'. \"\n            \"Have you configured your dialect correctly?\"\n        ),\n    )\n", "type": "function"}, {"name": "test___main___help", "is_method": false, "class_name": null, "parameters": [], "calls": ["subprocess.check_output"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 2005, "end_line": 2010}, "code_snippet": "def test___main___help():\n    \"\"\"Test that the CLI can be access via __main__.\"\"\"\n    # nonzero exit is good enough\n    subprocess.check_output(\n        [sys.executable, \"-m\", \"sqlfluff\", \"--help\"], env=os.environ\n    )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3370208740234375}
{"question": "Where in the Hive dialect function parsing class do grammar rules prevent ambiguity between the row keyword used as a function name versus type constructor?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "FunctionSegment", "docstring": "A scalar or aggregate function.\n\nExtended version of `ansi` to add support of row typecasting\nhttps://prestodb.io/docs/current/language/types.html#row\n```\ncast(row(val1, val2) as row(a integer, b integer))\n```", "methods": [], "attributes": ["type", "match_grammar", "match_grammar", "type", "type", "type", "match_grammar"], "code_location": {"file": "dialect_hive.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 885, "end_line": 935}, "type": "class"}, {"name": "HintFunctionSegment", "docstring": "A Function within a SparkSQL Hint.\n\nhttps://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-hints.html", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_sparksql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2018, "end_line": 2029}, "type": "class"}, {"name": "AliasExpressionSegment", "docstring": "Modified to allow UDTF in SELECT clause to return multiple columns aliases.\n\nFull Apache Hive `Built-in Table-Generating Functions (UDTF)` reference here:\nhttps://cwiki.apache.org/confluence/display/hive/languagemanual+udf#LanguageManualUDF-Built-inTable-GeneratingFunctions(UDTF)", "methods": [], "attributes": [], "code_location": {"file": "dialect_hive.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 469, "end_line": 487}, "type": "class"}, {"name": "RollupFunctionNameSegment", "docstring": "ROLLUP function name segment.\n\nNeed to be able to specify this as type `function_name_identifier`\nwithin a `function_name` so that linting rules identify it properly.", "methods": [], "attributes": ["type"], "code_location": {"file": "dialect_ansi.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2533, "end_line": 2545}, "type": "class"}, {"name": "CubeFunctionNameSegment", "docstring": "ROLLUP function name segment.\n\nNeed to be able to specify this as type `function_name_identifier`\nwithin a `function_name` so that linting rules identify it properly.", "methods": [], "attributes": ["type"], "code_location": {"file": "dialect_ansi.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2548, "end_line": 2560}, "type": "class"}, {"name": "FrameClauseSegment", "docstring": "A frame clause for window functions.\n\nThis overrides the ansi dialect frame clause segment as the sparksql\nframe clause allows for a more expressive frame syntax.\nhttps://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-window.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_sparksql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3573, "end_line": 3597}, "type": "class"}, {"name": "BareFunctionSegment", "docstring": "A function that can be called without parenthesis per ANSI specification.\n\nDB2 extends this to include `special registers`.", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_db2.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 257, "end_line": 264}, "type": "class"}, {"name": "WithinGroupFunctionNameSegment", "docstring": "WITHIN GROUP function name segment.\n\nFor aggregation functions that use the WITHIN GROUP clause.\nhttps://docs.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql\nhttps://docs.microsoft.com/en-us/sql/t-sql/functions/percentile-cont-transact-sql\nhttps://docs.microsoft.com/en-us/sql/t-sql/functions/percentile-disc-transact-sql\n\nNeed to be able to specify this as type function_name\nso that linting rules identify it properly", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3166, "end_line": 3183}, "type": "class"}, {"name": "CastFunctionNameSegment", "docstring": "CAST function name segment.\n\nNeed to be able to specify this as type function_name\nso that linting rules identify it properly", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3086, "end_line": 3094}, "type": "class"}, {"name": "FromExpressionElementSegment", "docstring": "Modified from ANSI to allow for `LATERAL VIEW` clause.", "methods": ["get_eventual_alias"], "attributes": ["match_grammar", "type", "type", "_base_from_expression_element", "type", "match_grammar", "match_grammar"], "code_location": {"file": "dialect_hive.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 456, "end_line": 466}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.33519625663757324}
{"question": "Where is the match_grammar definition for the JSON format clause class located within the CREATE EXTERNAL FILE FORMAT statement parsing structure?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "ExternalFileFormatJsonClause", "docstring": "`CREATE EXTERNAL FILE FORMAT` *JSON* format clause.\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-file-format-transact-sql&tabs=json#syntax", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 6326, "end_line": 6346}, "type": "class"}, {"name": "JsonFileFormatTypeParameters", "docstring": "A Snowflake File Format Type Options segment for JSON.\n\nhttps://docs.snowflake.com/en/sql-reference/sql/create-file-format.html", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_snowflake.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 5961, "end_line": 6021}, "type": "class"}, {"name": "ExternalFileFormatDeltaClause", "docstring": "`CREATE EXTERNAL FILE FORMAT` *Delta Lake* format clause.\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-file-format-transact-sql&tabs=delta#syntax", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 6349, "end_line": 6361}, "type": "class"}, {"name": "ExternalFileFormatParquetClause", "docstring": "`CREATE EXTERNAL FILE FORMAT` *PARQUET* format clause.\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-file-format-transact-sql&tabs=parquet#syntax", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 6303, "end_line": 6323}, "type": "class"}, {"name": "ExternalFileFormatRcClause", "docstring": "`CREATE EXTERNAL FILE FORMAT` *Record Columnar file format (RcFile)* clause.\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-file-format-transact-sql&tabs=rc#syntax", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 6252, "end_line": 6277}, "type": "class"}, {"name": "ExternalFileFormatDelimitedTextClause", "docstring": "`CREATE EXTERNAL FILE FORMAT` *Delimited text* clause.\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-file-format-transact-sql&tabs=delimited#syntax", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 6220, "end_line": 6249}, "type": "class"}, {"name": "ExternalFileFormatOrcClause", "docstring": "`CREATE EXTERNAL FILE FORMAT` *Optimized Row Columnar (ORC)* format clause.\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-file-format-transact-sql&tabs=orc#syntax", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 6280, "end_line": 6300}, "type": "class"}, {"name": "CreateExternalFileFormat", "docstring": "A statement to create an `EXTERNAL FILE FORMAT` object.\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-file-format-transact-sql&tabs=delta#syntax", "methods": [], "attributes": ["type"], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 6364, "end_line": 6389}, "type": "class"}, {"name": "ExternalFileFormatDelimitedTextFormatOptionClause", "docstring": "`CREATE EXTERNAL FILE FORMAT` Delimited text `FORMAT_OPTIONS` clause.", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 6189, "end_line": 6217}, "type": "class"}, {"name": "RowFormatClauseSegment", "docstring": "`ROW FORMAT` clause in a CREATE statement.", "methods": [], "attributes": ["type", "match_grammar", "type", "match_grammar"], "code_location": {"file": "dialect_athena.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 573, "end_line": 606}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.33867812156677246}
{"question": "Where is the logic that determines which segments halt greedy matching implemented within the grammar class that matches any content?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "match", "is_method": true, "class_name": "Anything", "parameters": ["self", "segments", "idx", "parse_context"], "calls": ["greedy_match", "terminators.extend", "MatchResult", "slice", "len"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/grammar", "start_line": 453, "end_line": 485}, "code_snippet": "    def match(\n        self,\n        segments: Sequence[\"BaseSegment\"],\n        idx: int,\n        parse_context: \"ParseContext\",\n    ) -> MatchResult:\n        \"\"\"Matches... Anything.\n\n        Most useful in match grammars, where a later parse grammar\n        will work out what's inside.\n\n        NOTE: This grammar does still only match as far as any inherited\n        terminators if they exist.\n        \"\"\"\n        terminators = [*self.terminators]\n        if not self.reset_terminators:\n            # Only add context terminators if we're not resetting.\n            terminators.extend(parse_context.terminators)\n        if not terminators:\n            return MatchResult(slice(idx, len(segments)))\n\n        return greedy_match(\n            segments,\n            idx,\n            parse_context,\n            terminators,\n            # Using the nested match option means that we can match\n            # any bracketed sections we find to persist the structure\n            # even if this grammar is permissive on the meaning.\n            # This preserves backward compatibility with older\n            # parsing behaviour.\n            nested_match=True,\n        )\n", "type": "function"}, {"name": "greedy_match", "is_method": false, "class_name": null, "parameters": ["segments", "idx", "parse_context", "matchers", "include_terminator", "nested_match"], "calls": ["skip_stop_index_backward_to_code", "MatchResult", "matcher.simple", "MatchResult", "MatchResult", "slice", "parse_context.deeper_match", "next_ex_bracket_match", "MatchResult", "all", "range", "slice", "slice", "slice", "len", "_s.isalpha", "is_type"], "code_location": {"file": "match_algorithms.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 552, "end_line": 653}, "code_snippet": "def greedy_match(\n    segments: Sequence[BaseSegment],\n    idx: int,\n    parse_context: ParseContext,\n    matchers: Sequence[Matchable],\n    include_terminator: bool = False,\n    nested_match: bool = False,\n) -> MatchResult:\n    \"\"\"Match anything up to some defined terminator.\"\"\"\n    working_idx = idx\n    # NOTE: _stop_idx is always reset below after matching before reference\n    # but mypy is unhappy unless we set a default value here.\n    _stop_idx = idx\n    # NOTE: child_matches is always tracked, but it will only ever have\n    # _content_ if `nested_match` is True. It otherwise remains an empty tuple.\n    child_matches: tuple[MatchResult, ...] = ()\n\n    while True:\n        with parse_context.deeper_match(name=\"GreedyUntil\") as ctx:\n            match, matcher, inner_matches = next_ex_bracket_match(\n                segments,\n                idx=working_idx,\n                matchers=matchers,\n                parse_context=ctx,\n            )\n\n        if nested_match:\n            child_matches += inner_matches\n\n        # No match? That means we've not found any terminators.\n        if not match:\n            # Claim everything left.\n            return MatchResult(slice(idx, len(segments)), child_matches=child_matches)\n\n        _start_idx = match.matched_slice.start\n        _stop_idx = match.matched_slice.stop\n        # NOTE: For some terminators we only count them if they're preceded\n        # by whitespace, and others we don't. In principle, we aim that for\n        # _keywords_ we require whitespace, and for symbols we don't.\n        # We do this by looking at the `simple` method of the returned\n        # matcher, and if it's entirely alphabetical (as defined by\n        # str.isalpha()) then we infer that it's a keyword, and therefore\n        # _does_ require whitespace before it.\n        assert matcher, f\"Match without matcher: {match}\"\n        _simple = matcher.simple(parse_context)\n        assert _simple, f\"Terminators require a simple method: {matcher}\"\n        _strings, _types = _simple\n        # NOTE: Typed matchers aren't common here, but we assume that they\n        # _don't_ require preceding whitespace.\n        # Do we need to enforce whitespace preceding?\n        if all(_s.isalpha() for _s in _strings) and not _types:\n            allowable_match = False\n            # NOTE: Edge case - if we're matching the _first_ element (i.e. that\n            # there are no `pre` segments) then we _do_ allow it.\n            # TODO: Review whether this is as designed, but it is consistent\n            # with past behaviour.\n            if _start_idx == working_idx:\n                allowable_match = True\n            # Work backward through previous segments looking for whitespace.\n            for _idx in range(_start_idx, working_idx, -1):\n                if segments[_idx - 1].is_meta:\n                    continue\n                elif segments[_idx - 1].is_type(\"whitespace\", \"newline\"):\n                    allowable_match = True\n                    break\n                else:\n                    # Found something other than metas and whitespace.\n                    break\n\n            # If this match isn't preceded by whitespace and that is\n            # a requirement, then we can't use it. Carry on...\n            if not allowable_match:\n                working_idx = _stop_idx\n                # Loop around, don't return yet\n                continue\n\n        # Otherwise, it's allowable!\n        break\n\n    # Return without any child matches or inserts. Greedy Matching\n    # shouldn't be used for mutation.\n    if include_terminator:\n        return MatchResult(slice(idx, _stop_idx), child_matches=child_matches)\n\n    # If we're _not_ including the terminator, we need to work back a little.\n    # If it's preceded by any non-code, we can't claim that.\n    # Work backwards so we don't include it.\n    _stop_idx = skip_stop_index_backward_to_code(\n        segments, match.matched_slice.start, idx\n    )\n\n    # If we went all the way back to `idx`, then ignore the _stop_idx.\n    # There isn't any code in the gap _anyway_ - so there's no point trimming.\n    if idx == _stop_idx:\n        # TODO: I don't really like this rule, it feels like a hack.\n        # Review whether it should be here.\n        return MatchResult(\n            slice(idx, match.matched_slice.start), child_matches=child_matches\n        )\n\n    # Otherwise return the trimmed version.\n    return MatchResult(slice(idx, _stop_idx), child_matches=child_matches)\n", "type": "function"}, {"name": "test__parser__grammar_anything_match", "is_method": false, "class_name": null, "parameters": ["terminators", "match_length", "test_segments", "fresh_ansi_dialect"], "calls": ["pytest.mark.parametrize", "ParseContext", "match", "StringParser", "slice", "Anything"], "code_location": {"file": "grammar_other_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser/grammar", "start_line": 211, "end_line": 223}, "code_snippet": "def test__parser__grammar_anything_match(\n    terminators, match_length, test_segments, fresh_ansi_dialect\n):\n    \"\"\"Test the Anything grammar.\n\n    NOTE: Anything combined with terminators implements the semantics\n    which used to be implemented by `GreedyUntil`.\n    \"\"\"\n    ctx = ParseContext(dialect=fresh_ansi_dialect)\n    terms = [StringParser(kw, KeywordSegment) for kw in terminators]\n    result = Anything(terminators=terms).match(test_segments, 0, parse_context=ctx)\n    assert result.matched_slice == slice(0, match_length)\n    assert result.matched_class is None  # We shouldn't have set a class\n", "type": "function"}, {"name": "BaseGrammar", "docstring": "Grammars are a way of composing match statements.\n\nAny grammar must implement the `match` function. Segments can also be\npassed to most grammars. Segments implement `match` as a classmethod. Grammars\nimplement it as an instance method.", "methods": ["_resolve_ref", "__init__", "cache_key", "is_optional", "simple", "__str__", "__repr__", "__eq__", "__ne__", "copy"], "attributes": ["is_meta"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/grammar", "start_line": 70, "end_line": 319}, "type": "class"}, {"name": "match", "is_method": true, "class_name": "Ref", "parameters": ["self", "segments", "idx", "parse_context"], "calls": ["self._get_elem", "parse_context.deeper_match", "elem.match", "parse_context.deeper_match", "self.exclude.match", "MatchResult.empty_at"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/grammar", "start_line": 386, "end_line": 427}, "code_snippet": "    def match(\n        self,\n        segments: Sequence[\"BaseSegment\"],\n        idx: int,\n        parse_context: \"ParseContext\",\n    ) -> MatchResult:\n        \"\"\"Match a list of segments against this segment.\n\n        Matching can be done from either the raw or the segments.\n        This raw function can be overridden, or a grammar defined\n        on the underlying class.\n\n        Args:\n            segments (tuple[BaseSegment, ...]): The sequence of segments\n                to match against.\n            idx (int): Index of the element in the sequence.\n            parse_context (ParseContext): The parse context.\n\n        Returns:\n            MatchResult: The result of the matching process.\n        \"\"\"\n        elem = self._get_elem(dialect=parse_context.dialect)\n\n        # First if we have an *exclude* option, we should check that\n        # which would prevent the rest of this grammar from matching.\n        if self.exclude:\n            with parse_context.deeper_match(\n                name=self._ref + \"-Exclude\",\n                clear_terminators=self.reset_terminators,\n                push_terminators=self.terminators,\n            ) as ctx:\n                if self.exclude.match(segments, idx, ctx):\n                    return MatchResult.empty_at(idx)\n\n        # Match against that. NB We're not incrementing the match_depth here.\n        # References shouldn't really count as a depth of match.\n        with parse_context.deeper_match(\n            name=self._ref,\n            clear_terminators=self.reset_terminators,\n            push_terminators=self.terminators,\n        ) as ctx:\n            return elem.match(segments, idx, parse_context)\n", "type": "function"}, {"name": "Segments", "docstring": "Encapsulates a sequence of one or more BaseSegments.\n\nThe segments may or may not be contiguous in a parse tree.\nProvides useful operations on a sequence of segments to simplify rule creation.", "methods": ["__new__", "__init__", "__add__", "__radd__", "find", "all", "any", "reversed", "raw_slices", "raw_segments", "recursive_crawl_all", "recursive_crawl", "children", "first", "last", "__iter__", "__getitem__", "__getitem__", "__getitem__", "get", "apply", "select", "iterate_segments"], "attributes": [], "code_location": {"file": "segments.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/functional", "start_line": 20, "end_line": 224}, "type": "class"}, {"name": "test__parser__algorithms__greedy_match", "is_method": false, "class_name": null, "parameters": ["raw_segments", "target_words", "inc_term", "result_slice", "generate_test_segments", "test_dialect"], "calls": ["pytest.mark.parametrize", "generate_test_segments", "ParseContext", "greedy_match", "StringParser", "slice", "slice", "slice", "slice", "slice"], "code_location": {"file": "match_algorithms_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser", "start_line": 200, "end_line": 222}, "code_snippet": "def test__parser__algorithms__greedy_match(\n    raw_segments,\n    target_words,\n    inc_term,\n    result_slice,\n    generate_test_segments,\n    test_dialect,\n):\n    \"\"\"Test the `greedy_match()` method.\"\"\"\n    test_segments = generate_test_segments(raw_segments)\n    matchers = [StringParser(word, KeywordSegment) for word in target_words]\n    ctx = ParseContext(dialect=test_dialect)\n\n    match = greedy_match(\n        segments=test_segments,\n        idx=0,\n        parse_context=ctx,\n        matchers=matchers,\n        include_terminator=inc_term,\n    )\n\n    assert match\n    assert match.matched_slice == result_slice\n", "type": "function"}, {"name": "trim_to_terminator", "is_method": false, "class_name": null, "parameters": ["segments", "idx", "terminators", "parse_context"], "calls": ["skip_stop_index_backward_to_code", "len", "len", "parse_context.deeper_match", "prune_options", "parse_context.deeper_match", "greedy_match", "term.match"], "code_location": {"file": "match_algorithms.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 656, "end_line": 710}, "code_snippet": "def trim_to_terminator(\n    segments: Sequence[BaseSegment],\n    idx: int,\n    terminators: Sequence[Matchable],\n    parse_context: ParseContext,\n) -> int:\n    \"\"\"Trim forward segments based on terminators.\n\n    Given a forward set of segments, trim elements from `segments` to\n    `tail` by using a `greedy_match()` to identify terminators.\n\n    If no terminators are found, no change is made.\n\n    NOTE: This method is designed replace a `max_idx`:\n\n    .. code-block:: python\n\n        max_idx = _trim_to_terminator(segments[:max_idx], idx, ...)\n\n    \"\"\"\n    # Is there anything left to match on.\n    if idx >= len(segments):\n        # Nope. No need to trim.\n        return len(segments)\n\n    # NOTE: If there is a terminator _immediately_, then greedy\n    # match will appear to not match (because there's \"nothing\" before\n    # the terminator). To resolve that case, we first match immediately\n    # on the terminators and handle that case explicitly if it occurs.\n    with parse_context.deeper_match(name=\"Trim-GreedyA-@0\") as ctx:\n        pruned_terms = prune_options(\n            terminators, segments, start_idx=idx, parse_context=ctx\n        )\n        for term in pruned_terms:\n            if term.match(segments, idx, ctx):\n                # One matched immediately. Claim everything to the tail.\n                return idx\n\n    # If the above case didn't match then we proceed as expected.\n    with parse_context.deeper_match(\n        name=\"Trim-GreedyB-@0\", track_progress=False\n    ) as ctx:\n        term_match = greedy_match(\n            segments,\n            idx,\n            parse_context=ctx,\n            matchers=terminators,\n        )\n\n    # Greedy match always returns.\n    # Skip backward from wherever it got to (either a terminator, or\n    # the end of the sequence).\n    return skip_stop_index_backward_to_code(\n        segments, term_match.matched_slice.stop, idx\n    )\n", "type": "function"}, {"name": "match", "is_method": true, "class_name": "Sequence", "parameters": ["self", "segments", "idx", "parse_context"], "calls": ["len", "tuple", "MatchResult", "trim_to_terminator", "isinstance", "_flush_metas", "parse_context.update_progress", "elem.match", "skip_start_index_forward_to_code", "elem.is_optional", "tuple", "wrap", "parse_context.deeper_match", "elem.match", "elem.is_optional", "skip_start_index_forward_to_code", "MatchResult", "trim_to_terminator", "skip_start_index_forward_to_code", "skip_stop_index_backward_to_code", "slice", "meta_buffer.append", "isinstance", "issubclass", "meta_buffer.append", "MatchResult.empty_at", "MatchResult.empty_at", "MatchResult.empty_at", "MatchResult", "MatchResult", "slice", "MatchResult", "slice", "slice", "MatchResult", "slice", "slice"], "code_location": {"file": "sequence.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/grammar", "start_line": 117, "end_line": 371}, "code_snippet": "    def match(\n        self,\n        segments: SequenceType[\"BaseSegment\"],\n        idx: int,\n        parse_context: \"ParseContext\",\n    ) -> MatchResult:\n        \"\"\"Match a specific sequence of elements.\n\n        When returning incomplete matches in one of the greedy parse\n        modes, we don't return any new meta segments (whether from conditionals\n        or otherwise). This is because we meta segments (typically indents)\n        may only make sense in the context of a full sequence, as their\n        corresponding pair may be later (and yet unrendered).\n\n        Partial matches should however still return the matched (mutated)\n        versions of any segments which _have_ been processed to provide\n        better feedback to the user.\n        \"\"\"\n        start_idx = idx  # Where did we start\n        matched_idx = idx  # Where have we got to\n        max_idx = len(segments)  # What is the limit\n        insert_segments: tuple[tuple[int, type[MetaSegment]], ...] = ()\n        child_matches: tuple[MatchResult, ...] = ()\n        first_match = True\n        # Metas with a negative indent value come AFTER\n        # the whitespace. Positive or neutral come BEFORE.\n        # HOWEVER: If one is already there, we must preserve\n        # the order. This forced ordering is fine if there's\n        # a positive followed by a negative in the sequence,\n        # but if by design a positive arrives *after* a\n        # negative then we should insert it after the positive\n        # instead.\n        # https://github.com/sqlfluff/sqlfluff/issues/3836\n        meta_buffer = []\n\n        if self.parse_mode == ParseMode.GREEDY:\n            # In the GREEDY mode, we first look ahead to find a terminator\n            # before matching any code.\n            max_idx = trim_to_terminator(\n                segments,\n                idx,\n                terminators=[*self.terminators, *parse_context.terminators],\n                parse_context=parse_context,\n            )\n\n        # Iterate elements\n        for elem in self._elements:\n            # 1. Handle any metas or conditionals.\n            # We do this first so that it's the same whether we've run\n            # out of segments or not.\n            # If it's a conditional, evaluate it.\n            # In both cases, we don't actually add them as inserts yet\n            # because their position will depend on what types we accrue.\n            if isinstance(elem, Conditional):\n                # A conditional grammar will only ever return insertions.\n                # If it's not enabled it returns an empty match.\n                # NOTE: No deeper match here, it seemed unnecessary.\n                _match = elem.match(segments, matched_idx, parse_context)\n                # Rather than taking them as a match at this location, we\n                # requeue them for addition later.\n                for _, submatch in _match.insert_segments:\n                    meta_buffer.append(submatch)\n                continue\n            # If it's a raw meta, just add it to our list.\n            elif isinstance(elem, type) and issubclass(elem, Indent):\n                meta_buffer.append(elem)\n                continue\n\n            # 2. Match Segments.\n            # At this point we know there are segments left to match\n            # on and that the current element isn't a meta or conditional.\n            _idx = matched_idx\n            # TODO: Need test cases to cover overmatching non code properly\n            # especially around optional elements.\n            if self.allow_gaps:\n                # First, if we're allowing gaps, consume any non-code.\n                # NOTE: This won't consume from the end of a sequence\n                # because this happens only in the run up to matching\n                # another element. This is as designed.\n                _idx = skip_start_index_forward_to_code(segments, matched_idx, max_idx)\n\n            # Have we prematurely run out of segments?\n            if _idx >= max_idx:\n                # If the current element is optional, carry on.\n                if elem.is_optional():\n                    continue\n                # Otherwise we have a problem. We've already consumed\n                # any metas, optionals and conditionals.\n                # This is a failed match because we couldn't complete\n                # the sequence.\n\n                if (\n                    # In a strict mode, running out a segments to match\n                    # on means that we don't match anything.\n                    self.parse_mode == ParseMode.STRICT\n                    # If nothing has been matched _anyway_ then just bail out.\n                    or matched_idx == start_idx\n                ):\n                    return MatchResult.empty_at(idx)\n\n                # On any of the other modes (GREEDY or GREEDY_ONCE_STARTED)\n                # we've effectively already claimed the segments, we've\n                # just failed to match. In which case it's unparsable.\n                insert_segments += tuple((matched_idx, meta) for meta in meta_buffer)\n                return MatchResult(\n                    matched_slice=slice(start_idx, matched_idx),\n                    insert_segments=insert_segments,\n                    child_matches=child_matches,\n                ).wrap(\n                    UnparsableSegment,\n                    segment_kwargs={\n                        \"expected\": (\n                            f\"{elem} after {segments[matched_idx - 1]}. Found nothing.\"\n                        )\n                    },\n                )\n\n            # Match the current element against the current position.\n            with parse_context.deeper_match(name=f\"Sequence-@{idx}\") as ctx:\n                # HACK: Segment slicing hack to limit\n                elem_match = elem.match(segments[:max_idx], _idx, ctx)\n\n            # Did we fail to match? (totally or un-cleanly)\n            if not elem_match:\n                # If we can't match an element, we should ascertain whether it's\n                # required. If so then fine, move on, but otherwise we should\n                # crash out without a match. We have not matched the sequence.\n                if elem.is_optional():\n                    # Pass this one and move onto the next element.\n                    continue\n\n                if self.parse_mode == ParseMode.STRICT:\n                    # In a strict mode, failing to match an element means that\n                    # we don't match anything.\n                    return MatchResult.empty_at(idx)\n\n                if (\n                    self.parse_mode == ParseMode.GREEDY_ONCE_STARTED\n                    and matched_idx == start_idx\n                ):\n                    # If it's only greedy once started, and we haven't matched\n                    # anything yet, then we also don't match anything.\n                    return MatchResult.empty_at(idx)\n\n                # On any of the other modes (GREEDY or GREEDY_ONCE_STARTED)\n                # we've effectively already claimed the segments, we've\n                # just failed to match. In which case it's unparsable.\n\n                # Handle the simple case where we haven't even started the\n                # sequence yet first:\n                if matched_idx == start_idx:\n                    return MatchResult(\n                        matched_slice=slice(start_idx, max_idx),\n                        matched_class=UnparsableSegment,\n                        segment_kwargs={\n                            \"expected\": (\n                                f\"{elem} to start sequence. Found {segments[_idx]}\"\n                            )\n                        },\n                    )\n\n                # Then handle the case of a partial match.\n                _start_idx = skip_start_index_forward_to_code(\n                    segments, matched_idx, max_idx\n                )\n                return MatchResult(\n                    # NOTE: We use the already matched segments in the\n                    # return value so that if any have already been\n                    # matched, the user can see that. Those are not\n                    # part of the unparsable section.\n                    # NOTE: The unparsable section is _included_ in the span\n                    # of the parent match.\n                    # TODO: Make tests to assert that child matches sit within\n                    # the parent!!!\n                    matched_slice=slice(start_idx, max_idx),\n                    insert_segments=insert_segments,\n                    child_matches=child_matches\n                    + (\n                        MatchResult(\n                            # The unparsable section is just the remaining\n                            # segments we were unable to match from the\n                            # sequence.\n                            matched_slice=slice(_start_idx, max_idx),\n                            matched_class=UnparsableSegment,\n                            segment_kwargs={\n                                \"expected\": (\n                                    f\"{elem} after {segments[matched_idx - 1]}. \"\n                                    f\"Found {segments[_idx]}\"\n                                )\n                            },\n                        ),\n                    ),\n                )\n\n            # Flush any metas...\n            insert_segments += _flush_metas(matched_idx, _idx, meta_buffer, segments)\n            meta_buffer = []\n\n            # Otherwise we _do_ have a match. Update the position.\n            matched_idx = elem_match.matched_slice.stop\n            parse_context.update_progress(matched_idx)\n\n            if first_match and self.parse_mode == ParseMode.GREEDY_ONCE_STARTED:\n                # In the GREEDY_ONCE_STARTED mode, we first look ahead to find a\n                # terminator after the first match (and only the first match).\n                max_idx = trim_to_terminator(\n                    segments,\n                    matched_idx,\n                    terminators=[*self.terminators, *parse_context.terminators],\n                    parse_context=parse_context,\n                )\n                first_match = False\n\n            # How we deal with child segments depends on whether it had a matched\n            # class or not.\n            # If it did, then just add it as a child match and we're done. Move on.\n            if elem_match.matched_class:\n                child_matches += (elem_match,)\n                continue\n            # Otherwise, we un-nest the returned structure, adding any inserts and\n            # children into the inserts and children of this sequence.\n            child_matches += elem_match.child_matches\n            insert_segments += elem_match.insert_segments\n\n        # If we get to here, we've matched all of the elements (or skipped them).\n        insert_segments += tuple((matched_idx, meta) for meta in meta_buffer)\n\n        # Finally if we're in one of the greedy modes, and there's anything\n        # left as unclaimed, mark it as unparsable.\n        if self.parse_mode in (ParseMode.GREEDY, ParseMode.GREEDY_ONCE_STARTED):\n            if max_idx > matched_idx:\n                _idx = skip_start_index_forward_to_code(segments, matched_idx, max_idx)\n                _stop_idx = skip_stop_index_backward_to_code(segments, max_idx, _idx)\n\n                if _stop_idx > _idx:\n                    child_matches += (\n                        MatchResult(\n                            # The unparsable section is just the remaining\n                            # segments we were unable to match from the\n                            # sequence.\n                            matched_slice=slice(_idx, _stop_idx),\n                            matched_class=UnparsableSegment,\n                            # TODO: We should come up with a better \"expected\" string\n                            # than this\n                            segment_kwargs={\"expected\": \"Nothing here.\"},\n                        ),\n                    )\n                    # Match up to the end.\n                    matched_idx = _stop_idx\n\n        return MatchResult(\n            matched_slice=slice(start_idx, matched_idx),\n            insert_segments=insert_segments,\n            child_matches=child_matches,\n        )\n", "type": "function"}, {"name": "test__parser__grammar_anysetof", "is_method": false, "class_name": null, "parameters": ["generate_test_segments"], "calls": ["generate_test_segments", "StringParser", "StringParser", "AnySetOf", "ParseContext", "g.match", "g.match", "MatchResult", "slice", "MatchResult", "MatchResult", "slice", "slice"], "code_location": {"file": "grammar_anyof_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser/grammar", "start_line": 246, "end_line": 277}, "code_snippet": "def test__parser__grammar_anysetof(generate_test_segments):\n    \"\"\"Test the AnySetOf grammar.\"\"\"\n    token_list = [\"bar\", \"  \\t \", \"foo\", \"  \\t \", \"bar\"]\n    segments = generate_test_segments(token_list)\n\n    bar = StringParser(\"bar\", KeywordSegment)\n    foo = StringParser(\"foo\", KeywordSegment)\n    g = AnySetOf(foo, bar)\n    ctx = ParseContext(dialect=None)\n\n    # Check it doesn't match if the start is whitespace.\n    assert not g.match(segments, 1, ctx)\n\n    # Check structure if we start with a match.\n    result = g.match(segments, 0, ctx)\n    assert result == MatchResult(\n        matched_slice=slice(0, 3),\n        child_matches=(\n            MatchResult(\n                slice(0, 1),\n                KeywordSegment,\n                segment_kwargs={\"instance_types\": (\"keyword\",)},\n            ),\n            MatchResult(\n                slice(2, 3),\n                KeywordSegment,\n                segment_kwargs={\"instance_types\": (\"keyword\",)},\n            ),\n            # NOTE: The second \"bar\" isn't included because this\n            # is any *set* of and we've already have \"bar\" once.\n        ),\n    )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3550302982330322}
{"question": "Where does the BigQuery FOR-IN-DO statement segment's grammar pattern control the parsing sequence from the query statement grammar through the nested loop body segment?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "ForInStatementSegment", "docstring": "FOR..IN...DO...END FOR statement.\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/procedural-language#for-in", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 714, "end_line": 734}, "type": "class"}, {"name": "ForInStatementsSegment", "docstring": "Statements within a FOR..IN...DO...END FOR statement.\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/procedural-language#for-in", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 694, "end_line": 711}, "type": "class"}, {"name": "ForInLoopSegment", "docstring": "FOR...IN...DO...END FOR statement.\n\nhttps://docs.snowflake.com/en/developer-guide/snowflake-scripting/loops#for-loop", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_snowflake.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 9172, "end_line": 9203}, "type": "class"}, {"name": "ForClauseSegment", "docstring": "This is the body of a `FOR` clause.", "methods": [], "attributes": ["type", "match_grammar", "type", "match_grammar", "type", "_common_directives_for_xml", "_elements", "match_grammar"], "code_location": {"file": "dialect_mysql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2107, "end_line": 2123}, "type": "class"}, {"name": "ForLoopStatementSegment", "docstring": "A `FOR LOOP` statement.\n\nhttps://docs.oracle.com/en/database/oracle/oracle-database/23/lnpls/FOR-LOOP-statement.html", "methods": [], "attributes": ["type"], "code_location": {"file": "dialect_oracle.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2688, "end_line": 2725}, "type": "class"}, {"name": "LoopStatementSegment", "docstring": "LOOP...END LOOP statement.\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/procedural-language#loop", "methods": [], "attributes": ["type", "match_grammar", "type", "match_grammar", "type"], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 856, "end_line": 870}, "type": "class"}, {"name": "LoopStatementsSegment", "docstring": "Statements within a LOOP... END LOOP statement.\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/procedural-language#loop", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 836, "end_line": 853}, "type": "class"}, {"name": "LoopStatementSegment", "docstring": "A `LOOP` statement.\n\nhttps://docs.oracle.com/en/database/oracle/oracle-database/23/lnpls/loop-statements.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_oracle.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2743, "end_line": 2760}, "type": "class"}, {"name": "WhileStatementSegment", "docstring": "WHILE...END WHILE statement.\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/procedural-language#while", "methods": [], "attributes": ["type", "match_grammar", "type", "match_grammar"], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 890, "end_line": 906}, "type": "class"}, {"name": "FunctionForLoopSegment", "docstring": "The definition of a for loop within a function body.", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_exasol.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3130, "end_line": 3162}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.33292126655578613}
{"question": "Where does the method in the jinja padding rule that partitions tag whitespace determine boundaries between opening markers and content when modifier characters are present?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "_get_whitespace_ends", "is_method": true, "class_name": "Rule_JJ01", "parameters": ["s"], "calls": ["main.strip", "main.find", "len"], "code_location": {"file": "JJ01.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/jinja", "start_line": 47, "end_line": 84}, "code_snippet": "    def _get_whitespace_ends(s: str) -> tuple[str, str, str, str, str]:\n        \"\"\"Remove tag ends and partition off any whitespace ends.\n\n        This function assumes that we've already trimmed the string\n        to just the tag, and will raise an AssertionError if not.\n        >>> Rule_JJ01._get_whitespace_ends('  {{not_trimmed}}   ')\n        Traceback (most recent call last):\n            ...\n        AssertionError\n\n        In essence it divides up a tag into the end tokens, any\n        leading or trailing whitespace and the inner content\n        >>> Rule_JJ01._get_whitespace_ends('{{ my_content }}')\n        ('{{', ' ', 'my_content', ' ', '}}')\n\n        It also works with block tags and more complicated content\n        and end markers.\n        >>> Rule_JJ01._get_whitespace_ends('{%+if a + b is True     -%}')\n        ('{%+', '', 'if a + b is True', '     ', '-%}')\n        \"\"\"\n        assert s[0] == \"{\" and s[-1] == \"}\"\n        # Jinja tags all have a length of two. We can use slicing\n        # to remove them easily.\n        main = s[2:-2]\n        pre = s[:2]\n        post = s[-2:]\n        # Optionally Jinja tags may also have plus of minus notation\n        # https://jinja2docs.readthedocs.io/en/stable/templates.html#whitespace-control\n        modifier_chars = [\"+\", \"-\"]\n        if main and main[0] in modifier_chars:\n            main = main[1:]\n            pre = s[:3]\n        if main and main[-1] in modifier_chars:\n            main = main[:-1]\n            post = s[-3:]\n        inner = main.strip()\n        pos = main.find(inner)\n        return pre, main[:pos], inner, main[pos + len(inner) :], post\n", "type": "function"}, {"name": "handle_left_whitespace_stripping", "is_method": true, "class_name": "JinjaAnalyzer", "parameters": ["self", "token", "block_idx"], "calls": ["self.raw_sliced.append", "self.slice_info_for_literal", "self.raw_str.index", "skipped_str.isspace", "templater_logger.warning", "RawFileSlice"], "code_location": {"file": "tracer.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters/slicers", "start_line": 851, "end_line": 895}, "code_snippet": "    def handle_left_whitespace_stripping(self, token: str, block_idx: int) -> None:\n        \"\"\"If block open uses whitespace stripping, record it.\n\n        When a \"begin\" tag (whether block, comment, or data) uses whitespace\n        stripping\n        (https://jinja.palletsprojects.com/en/3.0.x/templates/#whitespace-control)\n        the Jinja lex() function handles this by discarding adjacent whitespace\n        from 'raw_str'. For more insight, see the tokeniter() function in this file:\n        https://github.com/pallets/jinja/blob/main/src/jinja2/lexer.py\n\n        We want to detect and correct for this in order to:\n        - Correctly update \"idx\" (if this is wrong, that's a potential\n          DISASTER because lint fixes use this info to update the source file,\n          and incorrect values often result in CORRUPTING the user's file so\n          it's no longer valid SQL. :-O\n        - Guarantee that the slices we return fully \"cover\" the contents of\n          'in_str'.\n\n        We detect skipped characters by looking ahead in in_str for the token\n        just returned from lex(). The token text will either be at the current\n        'idx_raw' position (if whitespace stripping did not occur) OR it'll be\n        farther along in 'raw_str', but we're GUARANTEED that lex() only skips\n        over WHITESPACE; nothing else.\n        \"\"\"\n        # Find the token returned. Did lex() skip over any characters?\n        num_chars_skipped = self.raw_str.index(token, self.idx_raw) - self.idx_raw\n        if not num_chars_skipped:\n            return\n\n        # Yes. It skipped over some characters. Compute a string\n        # containing the skipped characters.\n        skipped_str = self.raw_str[self.idx_raw : self.idx_raw + num_chars_skipped]\n\n        # Sanity check: Verify that Jinja only skips over\n        # WHITESPACE, never anything else.\n        if not skipped_str.isspace():  # pragma: no cover\n            templater_logger.warning(\n                \"Jinja lex() skipped non-whitespace: %s\", skipped_str\n            )\n        # Treat the skipped whitespace as a literal.\n        self.raw_sliced.append(\n            RawFileSlice(skipped_str, \"literal\", self.idx_raw, block_idx)\n        )\n        self.raw_slice_info[self.raw_sliced[-1]] = self.slice_info_for_literal(0)\n        self.idx_raw += num_chars_skipped\n", "type": "function"}, {"name": "extract_tag_contents", "is_method": true, "class_name": "JinjaAnalyzer", "parameters": ["str_parts", "m_close", "m_open", "str_buff"], "calls": ["len", "isspace", "isspace", "trimmed_content.split", "len", "m_open.group", "len", "m_close.group"], "code_location": {"file": "tracer.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters/slicers", "start_line": 757, "end_line": 787}, "code_snippet": "    def extract_tag_contents(\n        str_parts: list[str],\n        m_close: regex.Match[str],\n        m_open: regex.Match[str],\n        str_buff: str,\n    ) -> list[str]:\n        \"\"\"Given Jinja tag info, return the stuff inside the braces.\n\n        I.e. Trim off the brackets and the whitespace.\n\n        Args:\n            str_parts (list[str]): A list of string parts.\n            m_close (regex.Match[str]): The regex match for the closing tag.\n            m_open (regex.Match[str]): The regex match for the opening tag.\n            str_buff (str): The string buffer.\n\n        Returns:\n            list[str]: The trimmed parts inside the Jinja tag.\n        \"\"\"\n        if len(str_parts) >= 3:\n            # Handle a tag received as individual parts.\n            trimmed_parts = str_parts[1:-1]\n            if trimmed_parts[0].isspace():\n                del trimmed_parts[0]\n            if trimmed_parts[-1].isspace():\n                del trimmed_parts[-1]\n        else:\n            # Handle a tag received in one go.\n            trimmed_content = str_buff[len(m_open.group(0)) : -len(m_close.group(0))]\n            trimmed_parts = trimmed_content.split()\n        return trimmed_parts\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_JJ01", "parameters": ["self", "context"], "calls": ["context.segment.pos_marker.is_literal", "context.config.get_templater_class", "issubclass", "self.logger.debug", "raw_slice.raw.strip", "self.logger.debug", "self._get_whitespace_ends", "raw_slice.raw.find", "self.logger.debug", "self._find_raw_at_src_idx", "results.append", "SourceFix", "LintResult", "slice", "len", "LintFix.replace", "raw_seg.edit"], "code_location": {"file": "JJ01.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/jinja", "start_line": 109, "end_line": 224}, "code_snippet": "    def _eval(self, context: RuleContext) -> list[LintResult]:\n        \"\"\"Look for non-literal segments.\n\n        NOTE: The existing crawlers don't filter very well for only templated\n        code, and so we process the whole file from the root here.\n        \"\"\"\n        # If the position maker for the root segment is literal then there's\n        # no templated code. So we can return early.\n        assert context.segment.pos_marker\n        if context.segment.pos_marker.is_literal():\n            return []\n\n        # We'll need the templated file. If for whatever reason it's\n        # not present, abort.\n        if not context.templated_file:  # pragma: no cover\n            return []\n\n        # We also only work with setups which use the jinja templater\n        # or a derivative of that. Otherwise return empty.\n        # NOTE: The `templater_obj` is not available in parallel operations\n        # and we don't really want to rehydrate a templater just to check\n        # what type it is, so use `get_templater_class()`.\n        _templater_class = context.config.get_templater_class()\n        if not issubclass(_templater_class, JinjaTemplater):\n            self.logger.debug(f\"Detected non-jinja templater: {_templater_class.name}\")\n            return []\n\n        results = []\n        # Work through the templated slices\n        for raw_slice in context.templated_file.raw_sliced:\n            # We only want templated slices.\n            if raw_slice.slice_type not in (\"templated\", \"block_start\", \"block_end\"):\n                continue\n\n            stripped = raw_slice.raw.strip()\n            if not stripped or stripped[0] != \"{\" or stripped[-1] != \"}\":\n                continue  # pragma: no cover\n\n            self.logger.debug(\n                \"Tag found @ source index %s: %r \", raw_slice.source_idx, stripped\n            )\n\n            # Partition and Position\n            src_idx = raw_slice.source_idx\n            tag_pre, ws_pre, inner, ws_post, tag_post = self._get_whitespace_ends(\n                stripped\n            )\n            position = raw_slice.raw.find(stripped[0])\n\n            self.logger.debug(\n                \"Tag string segments: %r | %r | %r | %r | %r @ %s + %s\",\n                tag_pre,\n                ws_pre,\n                inner,\n                ws_post,\n                tag_post,\n                src_idx,\n                position,\n            )\n\n            # For the following section, whitespace should be a single\n            # whitespace OR it should contain a newline.\n\n            pre_fix = None\n            post_fix = None\n            # Check the initial whitespace.\n            if not ws_pre or (ws_pre != \" \" and \"\\n\" not in ws_pre):\n                pre_fix = \" \"\n            # Check latter whitespace.\n            if not ws_post or (ws_post != \" \" and \"\\n\" not in ws_post):\n                post_fix = \" \"\n\n            # If no fixes, continue\n            if pre_fix is None and post_fix is None:\n                continue\n\n            fixed = (\n                tag_pre + (pre_fix or ws_pre) + inner + (post_fix or ws_post) + tag_post\n            )\n\n            # We need to identify a raw segment to attach to fix to.\n            raw_seg = self._find_raw_at_src_idx(context.segment, src_idx)\n\n            # If that raw segment already has fixes, don't apply it again.\n            # We're likely on a second pass.\n            if raw_seg.source_fixes:\n                continue\n\n            source_fixes = [\n                SourceFix(\n                    fixed,\n                    slice(\n                        src_idx + position,\n                        src_idx + position + len(stripped),\n                    ),\n                    # This position in the templated file is rough, but\n                    # close enough for sequencing.\n                    raw_seg.pos_marker.templated_slice,\n                )\n            ]\n\n            results.append(\n                LintResult(\n                    anchor=raw_seg,\n                    description=f\"Jinja tags should have a single \"\n                    f\"whitespace on either side: {stripped}\",\n                    fixes=[\n                        LintFix.replace(\n                            raw_seg,\n                            [raw_seg.edit(source_fixes=source_fixes)],\n                        )\n                    ],\n                )\n            )\n\n        return results\n", "type": "function"}, {"name": "update_inside_set_call_macro_or_block", "is_method": true, "class_name": "JinjaAnalyzer", "parameters": ["self", "block_type", "trimmed_parts", "m_open", "m_close", "tag_contents"], "calls": ["self.env.from_string", "isinstance", "join", "self.track_call"], "code_location": {"file": "tracer.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters/slicers", "start_line": 450, "end_line": 513}, "code_snippet": "    def update_inside_set_call_macro_or_block(\n        self,\n        block_type: str,\n        trimmed_parts: list[str],\n        m_open: Optional[regex.Match[str]],\n        m_close: Optional[regex.Match[str]],\n        tag_contents: list[str],\n    ) -> tuple[Optional[RawSliceInfo], str]:\n        \"\"\"Based on block tag, update whether in a set/call/macro/block section.\"\"\"\n        if block_type == \"block_start\" and trimmed_parts[0] in (\n            \"block\",\n            \"call\",\n            \"macro\",\n            \"set\",\n        ):\n            # Jinja supports two forms of {% set %}:\n            # - {% set variable = value %}\n            # - {% set variable %}value{% endset %}\n            # https://jinja.palletsprojects.com/en/2.10.x/templates/#block-assignments\n            # When the second format is used, set one of the fields\n            # 'inside_set_or_macro' or 'inside_block' to True. This info is\n            # used elsewhere, as other code inside these regions require\n            # special handling. (Generally speaking, JinjaAnalyzer ignores\n            # the contents of these blocks, treating them like opaque templated\n            # regions.)\n            try:\n                # Entering a set/macro block. Build a source string consisting\n                # of just this one Jinja command and see if it parses. If so,\n                # it's a standalone command. OTOH, if it fails with \"Unexpected\n                # end of template\", it was the opening command for a block.\n                self.env.from_string(\n                    f\"{self.env.block_start_string} {' '.join(trimmed_parts)} \"\n                    f\"{self.env.block_end_string}\"\n                )\n                # Here we should mutate the block type to just templated\n                # so we don't treat it as a block.\n                # https://github.com/sqlfluff/sqlfluff/issues/3750\n                block_type = \"templated\"\n            except TemplateSyntaxError as e:\n                if (\n                    isinstance(e.message, str)\n                    and \"Unexpected end of template\" in e.message\n                ):\n                    # It was opening a block, thus we're inside a set, macro, or\n                    # block.\n                    if trimmed_parts[0] == \"block\":\n                        self.inside_block = True\n                    else:\n                        result = None\n                        if trimmed_parts[0] == \"call\":\n                            assert m_open and m_close\n                            result = self.track_call(m_open, m_close, tag_contents)\n                        self.inside_set_macro_or_call = True\n                        return result, block_type\n                else:\n                    raise  # pragma: no cover\n        elif block_type == \"block_end\":\n            if trimmed_parts[0] in (\"endcall\", \"endmacro\", \"endset\"):\n                # Exiting a set or macro or block.\n                self.inside_set_macro_or_call = False\n            elif trimmed_parts[0] == \"endblock\":\n                # Exiting a {% block %} block.\n                self.inside_block = False\n        return None, block_type\n", "type": "function"}, {"name": "has_templated_newline", "is_method": true, "class_name": "_RebreakLocation", "parameters": ["self", "elements"], "calls": ["seg.is_type", "seg.is_type", "seg.pos_marker.is_literal", "seg.pos_marker.is_literal"], "code_location": {"file": "rebreak.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/reflow", "start_line": 96, "end_line": 117}, "code_snippet": "    def has_templated_newline(self, elements: ReflowSequenceType) -> bool:\n        \"\"\"Is either side a templated newline?\n\n        If either side has a templated newline, then that's ok too.\n        The intent here is that if the next newline is a _templated_\n        one, then in the source there will be a tag ({{ tag }}), which\n        acts like _not having a newline_.\n        \"\"\"\n        # Check the _last_ newline of the previous point.\n        # Slice backward to search in reverse.\n        for seg in elements[self.prev.newline_pt_idx].segments[::-1]:\n            if seg.is_type(\"newline\"):\n                if not seg.pos_marker.is_literal():\n                    return True\n                break\n        # Check the _first_ newline of the next point.\n        for seg in elements[self.next.newline_pt_idx].segments:\n            if seg.is_type(\"newline\"):\n                if not seg.pos_marker.is_literal():\n                    return True\n                break\n        return False\n", "type": "function"}, {"name": "track_templated", "is_method": true, "class_name": "JinjaAnalyzer", "parameters": ["self", "m_open", "m_close", "tag_contents"], "calls": ["self.next_slice_id", "m_open.group", "m_close.group", "self.make_raw_slice_info", "join"], "code_location": {"file": "tracer.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters/slicers", "start_line": 682, "end_line": 709}, "code_snippet": "    def track_templated(\n        self,\n        m_open: regex.Match[str],\n        m_close: regex.Match[str],\n        tag_contents: list[str],\n    ) -> RawSliceInfo:\n        \"\"\"Compute tracking info for Jinja templated region, e.g. {{ foo }}.\n\n        Args:\n            m_open (regex.Match): A regex match object representing the opening tag.\n            m_close (regex.Match): A regex match object representing the closing tag.\n            tag_contents (list[str]): A list of strings representing the contents of the\n                tag.\n\n        Returns:\n            RawSliceInfo: A RawSliceInfo object containing the computed\n            tracking info.\n        \"\"\"\n        unique_alternate_id = self.next_slice_id()\n        open_ = m_open.group(1)\n        close_ = m_close.group(1)\n        # Here, we still need to evaluate the original tag contents, e.g. in\n        # case it has intentional side effects, but also return a slice ID\n        # for tracking.\n        alternate_code = (\n            f\"\\0{unique_alternate_id} {open_} {''.join(tag_contents)} {close_}\"\n        )\n        return self.make_raw_slice_info(unique_alternate_id, alternate_code)\n", "type": "function"}, {"name": "track_call", "is_method": true, "class_name": "JinjaAnalyzer", "parameters": ["self", "m_open", "m_close", "tag_contents"], "calls": ["self.next_slice_id", "m_open.group", "m_close.group", "self.make_raw_slice_info", "join"], "code_location": {"file": "tracer.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters/slicers", "start_line": 711, "end_line": 738}, "code_snippet": "    def track_call(\n        self,\n        m_open: regex.Match[str],\n        m_close: regex.Match[str],\n        tag_contents: list[str],\n    ) -> RawSliceInfo:\n        \"\"\"Set up tracking for \"{% call ... %}\".\n\n        Args:\n            m_open (regex.Match): A regex match object representing the opening tag.\n            m_close (regex.Match): A regex match object representing the closing tag.\n            tag_contents (list[str]): A list of strings representing the contents of the\n                tag.\n\n        Returns:\n            RawSliceInfo: A RawSliceInfo object containing the computed\n            tracking info.\n        \"\"\"\n        unique_alternate_id = self.next_slice_id()\n        open_ = m_open.group(1)\n        close_ = m_close.group(1)\n        # Here, we still need to evaluate the original tag contents, e.g. in\n        # case it has intentional side effects, but also return a slice ID\n        # for tracking.\n        alternate_code = (\n            f\"\\0{unique_alternate_id} {open_} {''.join(tag_contents)} {close_}\"\n        )\n        return self.make_raw_slice_info(unique_alternate_id, alternate_code)\n", "type": "function"}, {"name": "first_trimmed_raw", "is_method": false, "class_name": null, "parameters": ["seg"], "calls": ["seg.raw_upper.split"], "code_location": {"file": "match_algorithms.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 44, "end_line": 63}, "code_snippet": "def first_trimmed_raw(seg: BaseSegment) -> str:\n    \"\"\"Trim whitespace off a whole element raw.\n\n    Used as a helper function in BaseGrammar._look_ahead_match.\n\n    For existing compound segments, we should assume that within\n    that segment, things are internally consistent, that means\n    rather than enumerating all the individual segments of a longer\n    one we just dump out the whole segment, but splitting off the\n    first element separated by whitespace. This is a) faster and\n    also b) prevents some really horrible bugs with bracket matching.\n    See https://github.com/sqlfluff/sqlfluff/issues/433\n\n    This fetches the _whole_ raw of a potentially compound segment\n    to match against, trimming off any whitespace. This is the\n    most efficient way to get at the first element of a potentially\n    longer segment.\n    \"\"\"\n    s = seg.raw_upper.split(maxsplit=1)\n    return s[0] if s else \"\"\n", "type": "function"}, {"name": "get_trailing_whitespace_from_string", "is_method": false, "class_name": null, "parameters": ["in_str"], "calls": ["range", "len"], "code_location": {"file": "string.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/helpers", "start_line": 89, "end_line": 119}, "code_snippet": "def get_trailing_whitespace_from_string(in_str: str) -> str:\n    r\"\"\"Returns the trailing whitespace from a string.\n\n    Designed to work with source strings of placeholders.\n\n    >>> get_trailing_whitespace_from_string(\"\")\n    ''\n    >>> get_trailing_whitespace_from_string(\"foo\")\n    ''\n    >>> get_trailing_whitespace_from_string(\"   \")\n    '   '\n    >>> get_trailing_whitespace_from_string(\"  foo \")\n    ' '\n    >>> get_trailing_whitespace_from_string(\"foo\\n\")\n    '\\n'\n    >>> get_trailing_whitespace_from_string(\"bar  \\t  \\n  \\r \")\n    '  \\t  \\n  \\r '\n    \"\"\"\n    whitespace_chars = \" \\t\\r\\n\"\n    if not in_str or in_str[-1] not in whitespace_chars:\n        return \"\"  # No whitespace\n    for i in range(1, len(in_str)):\n        if in_str[-(i + 1)] not in whitespace_chars:\n            # NOTE: The partial whitespace case is included as\n            # future-proofing. In testing it appears it is never\n            # required, and so only covered in the doctests above.\n            # doctest coverage isn't included in the overall coverage\n            # check and so the line below is excluded.\n            return in_str[-i:]  # pragma: no cover\n    else:\n        return in_str  # All whitespace\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3422679901123047}
{"question": "Where is the grammar definition for parsing WHEN NOT MATCHED BY SOURCE clauses in the BigQuery MERGE statement segment class?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "MergeNotMatchedBySourceClauseSegment", "docstring": "The `WHEN MATCHED BY SOURCE` clause within a `MERGE` statement.\n\nIt inherits from `ansi.MergeMatchedClauseSegment` because NotMatchedBySource clause\nis conceptually more close to a Matched clause than to NotMatched clause, i.e.\nit gets combined with an UPDATE or DELETE, not with an INSERT.", "methods": [], "attributes": ["type"], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2654, "end_line": 2677}, "type": "class"}, {"name": "MergeNotMatchedClauseSegment", "docstring": "The `WHEN NOT MATCHED` clause within a `MERGE` statement.", "methods": [], "attributes": [], "code_location": {"file": "dialect_ansi.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3126, "end_line": 3139}, "type": "class"}, {"name": "MergeNotMatchedClauseSegment", "docstring": "The `WHEN NOT MATCHED` clause within a `MERGE` statement.", "methods": [], "attributes": ["type", "match_grammar", "type", "type", "match_grammar"], "code_location": {"file": "dialect_exasol.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1659, "end_line": 1669}, "type": "class"}, {"name": "MergeNotMatchedClauseSegment", "docstring": "The `WHEN NOT MATCHED` clause within a `MERGE` statement.", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 5384, "end_line": 5416}, "type": "class"}, {"name": "MergeNotMatchedByTargetClauseSegment", "docstring": "The `WHEN NOT MATCHED [BY TARGET]` clause within a `MERGE` statement.\n\nOverriding ANSI to allow optionally `NOT MATCHED [BY TARGET]` statements", "methods": [], "attributes": ["type"], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2634, "end_line": 2651}, "type": "class"}, {"name": "MergeMatchSegment", "docstring": "Contains BigQuery specific merge operations.\n\nOverriding ANSI to allow `NOT MATCHED BY SOURCE` statements", "methods": [], "attributes": [], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2619, "end_line": 2631}, "type": "class"}, {"name": "MergeMatchedClauseSegment", "docstring": "The `WHEN MATCHED` clause within a `MERGE` statement.", "methods": [], "attributes": [], "code_location": {"file": "dialect_ansi.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3108, "end_line": 3123}, "type": "class"}, {"name": "MergeMatchedClauseSegment", "docstring": "The `WHEN MATCHED` clause within a `MERGE` statement.", "methods": [], "attributes": ["type", "match_grammar", "type", "type", "match_grammar"], "code_location": {"file": "dialect_exasol.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1644, "end_line": 1656}, "type": "class"}, {"name": "MergeMatchedClauseSegment", "docstring": "The `WHEN MATCHED` clause within a `MERGE` statement.", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 5361, "end_line": 5381}, "type": "class"}, {"name": "MergeStatementSegment", "docstring": "A `MERGE` statement.\n\nhttps://docs.aws.amazon.com/pt_br/redshift/latest/dg/r_MERGE.html", "methods": [], "attributes": ["match_grammar", "type", "match_grammar", "type", "match_grammar"], "code_location": {"file": "dialect_redshift.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2979, "end_line": 2990}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.3472330570220947}
{"question": " What is the semantic distinction in the BigQuery dialect's MERGE INSERT clause segment grammar between wildcard insertion and explicit column-value insertion?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "MergeInsertClauseSegment", "docstring": "`INSERT` clause within the `MERGE` statement.\n\nOverriding ANSI to allow `INSERT ROW` statements", "methods": [], "attributes": [], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2680, "end_line": 2695}, "type": "class"}, {"name": "MergeInsertClauseSegment", "docstring": "`INSERT` clause within the `MERGE` statement.", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 5419, "end_line": 5444}, "type": "class"}, {"name": "MergeInsertClauseSegment", "docstring": "`INSERT` clause within the `MERGE` statement.", "methods": [], "attributes": [], "code_location": {"file": "dialect_ansi.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3154, "end_line": 3164}, "type": "class"}, {"name": "MergeInsertClauseSegment", "docstring": "`INSERT` clause within the `MERGE` statement.", "methods": [], "attributes": ["type", "type", "match_grammar", "type", "type", "match_grammar", "match_grammar"], "code_location": {"file": "dialect_sparksql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3134, "end_line": 3149}, "type": "class"}, {"name": "MergeInsertClauseSegment", "docstring": "`INSERT` clause within the `MERGE` statement.", "methods": [], "attributes": [], "code_location": {"file": "dialect_exasol.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1693, "end_line": 1704}, "type": "class"}, {"name": "MergeInsertClauseSegment", "docstring": "`INSERT` clause within the `MERGE` statement.", "methods": [], "attributes": [], "code_location": {"file": "dialect_snowflake.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 8218, "end_line": 8228}, "type": "class"}, {"name": "MergeMatchSegment", "docstring": "Contains BigQuery specific merge operations.\n\nOverriding ANSI to allow `NOT MATCHED BY SOURCE` statements", "methods": [], "attributes": [], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2619, "end_line": 2631}, "type": "class"}, {"name": "MergeMatchSegment", "docstring": "Contains dialect specific merge operations.\n\nHookpoint for dialect specific behavior\ne.g. UpdateClause / DeleteClause, multiple MergeMatchedClauses", "methods": [], "attributes": [], "code_location": {"file": "dialect_ansi.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3093, "end_line": 3105}, "type": "class"}, {"name": "WildcardExpressionSegment", "docstring": "An extension of the star expression for Bigquery.", "methods": [], "attributes": [], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1247, "end_line": 1257}, "type": "class"}, {"name": "InsertStatementSegment", "docstring": "An `INSERT` statement.\n\nFull Apache Impala `INSERT` reference here:\nhttps://impala.apache.org/docs/build/html/topics/impala_insert.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_impala.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 221, "end_line": 262}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.33568787574768066}
{"question": "Where does conditional branching on the optional parameter for additional type segments in the function that generates CONVERT conversion fixes affect the structural composition of the segment replacement list through nested wrapping?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "ConvertFunctionNameSegment", "docstring": "CONVERT function name segment.\n\nFunction taking a data type identifier and an expression.\nAn alternative to CAST.", "methods": [], "attributes": ["type", "match_grammar", "type", "match_grammar"], "code_location": {"file": "dialect_redshift.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2702, "end_line": 2710}, "type": "class"}, {"name": "ConvertFunctionNameSegment", "docstring": "CONVERT function name segment.\n\nNeed to be able to specify this as type function_name\nso that linting rules identify it properly", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3075, "end_line": 3083}, "type": "class"}, {"name": "ConvertFunctionContentsSegment", "docstring": "Convert Function contents.", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3288, "end_line": 3301}, "type": "class"}, {"name": "ConvertFunctionContentsSegment", "docstring": "Convert Function contents.", "methods": [], "attributes": ["type", "match_grammar", "type", "match_grammar"], "code_location": {"file": "dialect_redshift.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2767, "end_line": 2778}, "type": "class"}, {"name": "_create_semicolon_and_delete_whitespace", "is_method": true, "class_name": "Rule_CV06", "parameters": ["self", "target_segment", "parent_segment", "anchor_segment", "whitespace_deletions", "create_segments"], "calls": ["self._choose_anchor_segment", "set", "fixes.extend", "whitespace_deletions.select", "lintfix_fn", "LintFix.delete", "LintFix.delete"], "code_location": {"file": "CV06.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "start_line": 300, "end_line": 330}, "code_snippet": "    def _create_semicolon_and_delete_whitespace(\n        self,\n        target_segment: BaseSegment,\n        parent_segment: BaseSegment,\n        anchor_segment: BaseSegment,\n        whitespace_deletions: Segments,\n        create_segments: list[BaseSegment],\n    ) -> list[LintFix]:\n        anchor_segment = self._choose_anchor_segment(\n            parent_segment, \"create_after\", anchor_segment, filter_meta=True\n        )\n        lintfix_fn = LintFix.create_after\n        whitespace_deletion_set = set(whitespace_deletions)\n        if anchor_segment in whitespace_deletion_set:\n            # Can't delete() and create_after() the same segment. Use replace()\n            # instead.\n            lintfix_fn = LintFix.replace\n            whitespace_deletions = whitespace_deletions.select(\n                lambda seg: seg is not anchor_segment\n            )\n        fixes = [\n            lintfix_fn(\n                anchor_segment,\n                create_segments,\n            ),\n            LintFix.delete(\n                target_segment,\n            ),\n        ]\n        fixes.extend(LintFix.delete(d) for d in whitespace_deletions)\n        return fixes\n", "type": "function"}, {"name": "elements_to_segments", "is_method": true, "class_name": "Lexer", "parameters": ["self", "elements", "templated_file"], "calls": ["lexer_logger.info", "self.config.get", "list", "segment_buffer.append", "tuple", "_iter_segments", "EndOfFile", "pos_marker.end_point_marker", "PositionMarker.from_point"], "code_location": {"file": "lexer.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 803, "end_line": 825}, "code_snippet": "    def elements_to_segments(\n        self, elements: list[TemplateElement], templated_file: TemplatedFile\n    ) -> tuple[RawSegment, ...]:\n        \"\"\"Convert a tuple of lexed elements into a tuple of segments.\"\"\"\n        lexer_logger.info(\"Elements to Segments.\")\n        add_indents = self.config.get(\"template_blocks_indent\", \"indentation\")\n        # Delegate to _iter_segments\n        segment_buffer: list[RawSegment] = list(\n            _iter_segments(elements, templated_file, add_indents)\n        )\n\n        # Add an end of file marker\n        segment_buffer.append(\n            EndOfFile(\n                pos_marker=(\n                    segment_buffer[-1].pos_marker.end_point_marker()\n                    if segment_buffer\n                    else PositionMarker.from_point(0, 0, templated_file)\n                )\n            )\n        )\n        # Convert to tuple before return\n        return tuple(segment_buffer)\n", "type": "function"}, {"name": "_iter_templated_patches", "is_method": false, "class_name": null, "parameters": ["segment", "templated_file"], "calls": ["linter_logger.debug", "segment.pos_marker.is_literal", "_iter_source_fix_patches", "type", "_iter_source_fix_patches", "FixPatch", "is_type", "slice", "slice", "seg.pos_marker.is_point", "_iter_templated_patches", "FixPatch", "linter_logger.debug", "FixPatch", "slice", "slice", "max", "max"], "code_location": {"file": "patch.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 54, "end_line": 213}, "code_snippet": "def _iter_templated_patches(\n    segment: BaseSegment, templated_file: TemplatedFile\n) -> Iterator[FixPatch]:\n    \"\"\"Iterate through the segments generating fix patches.\n\n    The patches are generated in TEMPLATED space. This is important\n    so that we defer dealing with any loops until later. At this stage\n    everything *should* happen in templated order.\n\n    Occasionally we have an insertion around a placeholder, so we also\n    return a hint to deal with that.\n    \"\"\"\n    # Does it match? If so we can ignore it.\n    assert segment.pos_marker\n    templated_raw = templated_file.templated_str[segment.pos_marker.templated_slice]\n    matches = segment.raw == templated_raw\n    if matches:\n        # First yield any source fixes\n        yield from _iter_source_fix_patches(segment, templated_file)\n        # Then return.\n        return\n\n    # If we're here, the segment doesn't match the original.\n    linter_logger.debug(\n        \"# Changed Segment Found: %s at %s: Original: [%r] Fixed: [%r]\",\n        type(segment).__name__,\n        segment.pos_marker.templated_slice,\n        templated_raw,\n        segment.raw,\n    )\n\n    # If it's all literal, then we don't need to recurse.\n    if segment.pos_marker.is_literal():\n        # First yield any source fixes\n        yield from _iter_source_fix_patches(segment, templated_file)\n        # Then yield the position in the source file and the patch\n        yield FixPatch(\n            source_slice=segment.pos_marker.source_slice,\n            templated_slice=segment.pos_marker.templated_slice,\n            patch_category=\"literal\",\n            fixed_raw=segment.raw,\n            templated_str=templated_file.templated_str[\n                segment.pos_marker.templated_slice\n            ],\n            source_str=templated_file.source_str[segment.pos_marker.source_slice],\n        )\n    # Can we go deeper?\n    elif not segment.segments:\n        # It's not literal, but it's also a raw segment. If we're going\n        # to yield a change, we would have done it from the parent, so\n        # we just abort from here.\n        return  # pragma: no cover TODO?\n    else:\n        # This segment isn't a literal, but has changed, we need to go deeper.\n\n        # If there's an end of file segment or indent, ignore them just for the\n        # purposes of patch iteration.\n        # NOTE: This doesn't mutate the underlying `self.segments`.\n        segments = segment.segments\n        while segments and segments[-1].is_type(\"end_of_file\", \"indent\"):\n            segments = segments[:-1]\n\n        # Iterate through the child segments\n        source_idx = segment.pos_marker.source_slice.start\n        templated_idx = segment.pos_marker.templated_slice.start\n        insert_buff = \"\"\n        first_segment_pos: Optional[PositionMarker] = None\n        for seg in segments:\n            # First check for insertions.\n            # At this stage, everything should have a position.\n            assert seg.pos_marker\n            # We know it's an insertion if it has length but not in the templated\n            # file.\n            if seg.raw and seg.pos_marker.is_point():\n                # Add it to the insertion buffer if it has length:\n                if seg.raw:\n                    insert_buff += seg.raw\n                    # We want to capture the first position where we have a point.\n                    first_segment_pos = first_segment_pos or seg.pos_marker\n                    linter_logger.debug(\n                        \"Appending insertion buffer. %r @idx: %s\",\n                        insert_buff,\n                        templated_idx,\n                    )\n                continue\n\n            # If we get here, then we know it's an original. Check for deletions at\n            # the point before this segment (vs the TEMPLATED).\n            # Deletions in this sense could also mean source consumption.\n            start_diff = seg.pos_marker.templated_slice.start - templated_idx\n\n            # Check to see whether there's a discontinuity before the current\n            # segment\n            if start_diff > 0 or insert_buff:\n                # If we have an insert buffer, then it's an edit, otherwise a\n                # deletion.\n\n                # For the start of the next segment, we need the position of the\n                # first raw, not the pos marker of the whole thing. That accounts\n                # better for loops.\n                first_segment_pos = first_segment_pos or seg.pos_marker\n                yield FixPatch(\n                    # Whether the source slice is zero depends on the start_diff.\n                    # A non-zero start diff implies a deletion, or more likely\n                    # a consumed element of the source. We can use the tracking\n                    # markers from the last segment to recreate where this element\n                    # should be inserted in both source and template.\n                    # The slices must never go backwards so the end of the slice must\n                    # be greater than or equal to the start.\n                    source_slice=slice(\n                        source_idx,\n                        max(first_segment_pos.source_slice.start, source_idx),\n                    ),\n                    templated_slice=slice(\n                        templated_idx,\n                        max(first_segment_pos.templated_slice.start, templated_idx),\n                    ),\n                    patch_category=\"mid_point\",\n                    fixed_raw=insert_buff,\n                    templated_str=\"\",\n                    source_str=\"\",\n                )\n\n                # Reset the first position so we can move the pointer forward.\n                first_segment_pos = None\n                insert_buff = \"\"\n\n            # Now we deal with any changes *within* the segment itself.\n            yield from _iter_templated_patches(seg, templated_file=templated_file)\n\n            # Once we've dealt with any patches from the segment, update\n            # our position markers.\n            source_idx = seg.pos_marker.source_slice.stop\n            templated_idx = seg.pos_marker.templated_slice.stop\n\n        # After the loop, we check whether there's a trailing deletion\n        # or insert. Also valid if we still have an insertion buffer here.\n        end_diff = segment.pos_marker.templated_slice.stop - templated_idx\n        if end_diff or insert_buff:\n            source_slice = slice(\n                source_idx,\n                segment.pos_marker.source_slice.stop,\n            )\n            templated_slice = slice(\n                templated_idx,\n                segment.pos_marker.templated_slice.stop,\n            )\n            # We determine the source_slice directly rather than\n            # inferring it so that we can be very specific that\n            # we ensure that fixes adjacent to source-only slices\n            # (e.g. {% endif %}) are placed appropriately relative\n            # to source-only slices.\n            yield FixPatch(\n                source_slice=source_slice,\n                templated_slice=templated_slice,\n                patch_category=\"end_point\",\n                fixed_raw=insert_buff,\n                templated_str=templated_file.templated_str[templated_slice],\n                source_str=templated_file.source_str[source_slice],\n            )\n", "type": "function"}, {"name": "CastFunctionContentsSegment", "docstring": "Cast Function contents.", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3274, "end_line": 3285}, "type": "class"}, {"name": "_rebuild_spacing", "is_method": true, "class_name": "Rule_ST04", "parameters": ["self", "indent_str", "nested_clauses"], "calls": ["any", "sp.is_comment", "nested_clauses.first", "seg.is_type", "seg.is_type", "sp.not_", "NewlineSegment", "WhitespaceSegment", "sp.is_whitespace", "WhitespaceSegment"], "code_location": {"file": "ST04.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 216, "end_line": 242}, "code_snippet": "    def _rebuild_spacing(\n        self, indent_str: str, nested_clauses: Segments\n    ) -> list[BaseSegment]:\n        buff = []\n        # If the first segment is a comment, add a newline\n        prior_newline = nested_clauses.first(sp.not_(sp.is_whitespace())).any(\n            sp.is_comment()\n        )\n        prior_whitespace = \"\"\n        for seg in nested_clauses:\n            if seg.is_type(\"when_clause\", \"else_clause\") or (\n                prior_newline and seg.is_comment\n            ):\n                buff += [NewlineSegment(), WhitespaceSegment(indent_str), seg]\n                prior_newline = False\n                prior_whitespace = \"\"\n            elif seg.is_type(\"newline\"):\n                prior_newline = True\n                prior_whitespace = \"\"\n            elif not prior_newline and seg.is_comment:\n                buff += [WhitespaceSegment(prior_whitespace), seg]\n                prior_newline = False\n                prior_whitespace = \"\"\n            elif seg.is_whitespace:\n                # Don't reset newline\n                prior_whitespace = seg.raw\n        return buff\n", "type": "function"}, {"name": "DatatypeSegment", "docstring": "A data type segment.\n\nIn particular here, this enabled the support for\nthe STRUCT datatypes.", "methods": [], "attributes": [], "code_location": {"file": "dialect_bigquery.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1349, "end_line": 1365}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.34223365783691406}
{"question": " What is the architectural design of the NamedTuple storing parsing results with an optional parse tree field enabling graceful degradation when parsing fails at different pipeline stages?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "ParsedVariant", "docstring": "An object to store the result of parsing a single TemplatedFile.\n\nArgs:\n    templated_file (:obj:`TemplatedFile`): Containing the details\n        of the templated file. If templating fails, this will be `None`.\n    tree (:obj:`BaseSegment`): The segment structure representing the\n        parsed file. If parsing fails due to an unrecoverable\n        violation then we will be None.\n    lexing_violations (:obj:`list` of :obj:`SQLLexError`): Any violations\n        raised during the lexing phase.\n    parsing_violations (:obj:`list` of :obj:`SQLParseError`): Any violations\n        raised during the lexing phase.", "methods": ["violations"], "attributes": [], "code_location": {"file": "common.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 42, "end_line": 64}, "type": "class"}, {"name": "MatchResult", "docstring": "This should be the default response from any `match` method.\n\nAll references and indices are in reference to a single root tuple\nof segments. This result contains enough information to actually\ncreate the nested tree structure, but shouldn't actually contain\nany new segments itself. That means keeping information about:\n1. Ranges of segments which should be included segments to be\n   created.\n2. References to the segment classes which we would create.\n3. Information about any _new_ segments to add in the process,\n   such as MetaSegment classes.\n\nGiven the segments aren't yet \"nested\", the structure of this\nresult *will* need to be nested, ideally self nested.\n\nIn the case of finding unparsable locations, we should return the\n\"best\" result, referencing the furthest that we got. That allows\nus to identify those parsing issues and create UnparsableSegment\nclasses later.", "methods": ["__post_init__", "__len__", "__bool__", "stringify", "empty_at", "is_better_than", "append", "wrap", "apply"], "attributes": [], "code_location": {"file": "match_result.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 32, "end_line": 282}, "type": "class"}, {"name": "_parse_tokens", "is_method": true, "class_name": "Linter", "parameters": ["tokens", "config", "fname", "parse_statistics"], "calls": ["Parser", "linter_logger.info", "linter_logger.info", "parsed.iter_unparsables", "parser.parse", "format", "violations.append", "linter_logger.info", "linter_logger.info", "tuple", "linter_logger.info", "violations.append", "parsed.stringify", "SQLParseError", "unparsable.stringify", "format", "len"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 231, "end_line": 281}, "code_snippet": "    def _parse_tokens(\n        tokens: Sequence[BaseSegment],\n        config: FluffConfig,\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> tuple[Optional[BaseSegment], list[SQLParseError]]:\n        parser = Parser(config=config)\n        violations = []\n        # Parse the file and log any problems\n        try:\n            parsed: Optional[BaseSegment] = parser.parse(\n                # Regardless of how the sequence was passed in, we should\n                # coerce it to a tuple here, before we head deeper into\n                # the parsing process.\n                tuple(tokens),\n                fname=fname,\n                parse_statistics=parse_statistics,\n            )\n        except SQLParseError as err:\n            linter_logger.info(\"PARSING FAILED! : %s\", err)\n            violations.append(err)\n            return None, violations\n\n        if parsed is None:  # pragma: no cover\n            return None, violations\n\n        linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n        linter_logger.info(\"\\n\" + parsed.stringify())\n        # We may succeed parsing, but still have unparsable segments. Extract them\n        # here.\n        for unparsable in parsed.iter_unparsables():\n            # No exception has been raised explicitly, but we still create one here\n            # so that we can use the common interface\n            assert unparsable.pos_marker\n            violations.append(\n                SQLParseError(\n                    \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n                    \"{1!r}\".format(\n                        unparsable.pos_marker.working_loc,\n                        (\n                            unparsable.raw\n                            if len(unparsable.raw) < 40\n                            else unparsable.raw[:40] + \"...\"\n                        ),\n                    ),\n                    segment=unparsable,\n                )\n            )\n            linter_logger.info(\"Found unparsable segment...\")\n            linter_logger.info(unparsable.stringify())\n        return parsed, violations\n", "type": "function"}, {"name": "tree", "is_method": true, "class_name": "LintingResult", "parameters": ["self"], "calls": ["len", "ValueError"], "code_location": {"file": "linting_result.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 206, "end_line": 213}, "code_snippet": "    def tree(self) -> Optional[\"BaseSegment\"]:  # pragma: no cover\n        \"\"\"A convenience method for when there is only one file and we want the tree.\"\"\"\n        if len(self.paths) > 1:\n            raise ValueError(\n                \".tree() cannot be called when a LintingResult contains more than one \"\n                \"path.\"\n            )\n        return self.paths[0].tree\n", "type": "function"}, {"name": "ParseExample", "docstring": "A tuple representing an example SQL file to parse.", "methods": [], "attributes": [], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test", "start_line": 34, "end_line": 38}, "type": "class"}, {"name": "parse_example_file", "is_method": false, "class_name": null, "parameters": ["dialect", "sqlfile"], "calls": ["FluffConfig", "load_file", "lex", "parse", "dict", "Lexer", "Parser"], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test", "start_line": 116, "end_line": 124}, "code_snippet": "def parse_example_file(dialect: str, sqlfile: str):\n    \"\"\"Parse example SQL file, return parse tree.\"\"\"\n    config = FluffConfig(overrides=dict(dialect=dialect))\n    # Load the SQL\n    raw = load_file(dialect, sqlfile)\n    # Lex and parse the file\n    tokens, _ = Lexer(config=config).lex(raw)\n    tree = Parser(config=config).parse(tokens, fname=dialect + \"/\" + sqlfile)\n    return tree\n", "type": "function"}, {"name": "ParseContext", "docstring": "Object to handle the context at hand during parsing.\n\nHolds two tiers of references.\n1. Persistent config, like references to the dialect or\n   the current verbosity and logger.\n2. Stack config, like the parse and match depth.\n\nThe manipulation of the stack config is done using a context\nmanager and layered config objects inside the context.\n\nNOTE: We use context managers here to avoid _copying_\nthe context, just to mutate it safely. This is significantly\nmore performant than the copy operation, but does require some\ncare to use properly.\n\nWhen fetching elements from the context, we first look\nat the top level stack config object and the persistent\nconfig values (stored as attributes of the ParseContext\nitself).", "methods": ["__init__", "from_config", "_set_terminators", "_reset_terminators", "deeper_match", "progress_bar", "update_progress", "stack", "check_parse_cache", "put_parse_cache"], "attributes": [], "code_location": {"file": "context.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 32, "end_line": 312}, "type": "class"}, {"name": "SQLParseError", "docstring": "An error which occurred during parsing.\n\nArgs:\n    segment (:obj:`BaseSegment`, optional): The segment which is relevant\n        for the failure in parsing. This is likely to be a subclass of\n        `BaseSegment` rather than the parent class itself. This is mostly\n        used for logging and for referencing position.", "methods": ["__init__", "__reduce__", "to_dict"], "attributes": ["_code", "_identifier"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 185, "end_line": 246}, "type": "class"}, {"name": "generate_one_parse_fixture", "is_method": false, "class_name": null, "parameters": ["example"], "calls": ["_create_file_path", "tree.type_set", "compute_parse_tree_hash", "_create_file_path", "parse_example_file", "open", "tree.as_record", "dict", "print", "yaml.dump", "SQLParseError", "SQLParseError", "f.write", "SQLParseError", "list", "records.items"], "code_location": {"file": "generate_parse_fixture_yml.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test", "start_line": 95, "end_line": 147}, "code_snippet": "def generate_one_parse_fixture(\n    example: ParseExample,\n) -> tuple[ParseExample, Optional[SQLParseError]]:\n    \"\"\"Parse example SQL file, write parse tree to YAML file.\"\"\"\n    dialect, sqlfile = example\n    sql_path = _create_file_path(example, \".sql\")\n\n    try:\n        tree = parse_example_file(dialect, sqlfile)\n    except Exception as err:\n        # Catch parsing errors, and wrap the file path only it.\n        return example, SQLParseError(f\"Fatal parsing error: {sql_path}: {err}\")\n\n    # Check we don't have any base types or unparsable sections\n    types = tree.type_set()\n    if \"base\" in types:\n        return example, SQLParseError(f\"Unnamed base section when parsing: {sql_path}\")\n    if \"unparsable\" in types:\n        return example, SQLParseError(f\"Could not parse: {sql_path}\")\n\n    _hash = compute_parse_tree_hash(tree)\n    # Remove the .sql file extension\n    path = _create_file_path(example)\n    with open(path, \"w\", newline=\"\\n\", encoding=\"utf8\") as f:\n        r: Optional[dict[str, Optional[str]]] = None\n\n        if not tree:\n            f.write(\"\")\n            return example, None\n\n        records = tree.as_record(code_only=True, show_raw=True)\n        assert records, \"TypeGuard\"\n        r = dict([(\"_hash\", _hash), *list(records.items())])\n        print(\n            \"# YML test files are auto-generated from SQL files and should not be \"\n            \"edited by\",\n            '# hand. To help enforce this, the \"hash\" field in the file must match '\n            \"a hash\",\n            \"# computed by SQLFluff when running the tests. Please run\",\n            \"# `python test/generate_parse_fixture_yml.py`  to generate them after \"\n            \"adding or\",\n            \"# altering SQL files.\",\n            file=f,\n            sep=\"\\n\",\n        )\n        yaml.dump(\n            data=r,\n            stream=f,\n            default_flow_style=False,\n            sort_keys=False,\n            allow_unicode=True,\n        )\n        return example, None\n", "type": "function"}, {"name": "ParsedString", "docstring": "An object to store the result of parsing a string.\n\nArgs:\n    parsed_variants (:obj:`list` of :obj:`ParsedVariant`): The parsed\n        variants of this file. Empty if parsing or templating failed.\n    templating_violations (:obj:`list` of :obj:`SQLTemplaterError`):\n        Any violations raised during the templating phase. Any violations\n        raised during lexing or parsing can be found in the\n        `parsed_variants`, or accessed using the `.violations()` method\n        which combines all the violations.\n    time_dict (:obj:`dict`): Contains timings for how long each step\n        took in the process.\n    config (:obj:`FluffConfig`): The active config for this file,\n        including any parsed in-file directives.\n    fname (str): The name of the file. Used mostly for user feedback.\n    source_str (str): The raw content of the source file.", "methods": ["violations", "root_variant", "tree"], "attributes": [], "code_location": {"file": "common.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 67, "end_line": 129}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.341533899307251}
{"question": " How does the test helper function that validates dialect-specific SQL statements ensure the parsed tree matches the expected segment type hierarchy and statement count through recursive validation?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "validate_dialect_specific_statements", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.fixture"], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 131, "end_line": 137}, "code_snippet": "def validate_dialect_specific_statements():\n    \"\"\"This validates one or multiple statements against specified segment class.\n\n    It even validates the number of parsed statements with the number of expected\n    statements.\n    \"\"\"\n    return _validate_dialect_specific_statements\n", "type": "function"}, {"name": "_validate_dialect_specific_statements", "is_method": false, "class_name": null, "parameters": ["dialect", "segment_cls", "raw", "stmt_count"], "calls": ["Linter", "lnt.parse_string", "parsed.tree.type_set", "len", "len", "isinstance", "parsed.tree.recursive_crawl"], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 95, "end_line": 115}, "code_snippet": "def _validate_dialect_specific_statements(dialect, segment_cls, raw, stmt_count):\n    \"\"\"This validates one or multiple statements against specified segment class.\n\n    It even validates the number of parsed statements with the number of expected\n    statements.\n    \"\"\"\n    lnt = Linter(dialect=dialect)\n    parsed = lnt.parse_string(raw)\n    assert len(parsed.violations) == 0\n\n    # Find any unparsable statements\n    typs = parsed.tree.type_set()\n    assert \"unparsable\" not in typs\n\n    # Find the expected type in the parsed segment\n    child_segments = [seg for seg in parsed.tree.recursive_crawl(segment_cls.type)]\n    assert len(child_segments) == stmt_count\n\n    # Check if all child segments are the correct type\n    for c in child_segments:\n        assert isinstance(c, segment_cls)\n", "type": "function"}, {"name": "test__dialect__ansi_specific_segment_parses", "is_method": false, "class_name": null, "parameters": ["segmentref", "raw", "caplog", "dialect_specific_segment_parses"], "calls": ["pytest.mark.parametrize", "dialect_specific_segment_parses"], "code_location": {"file": "ansi_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 128, "end_line": 138}, "code_snippet": "def test__dialect__ansi_specific_segment_parses(\n    segmentref, raw, caplog, dialect_specific_segment_parses\n):\n    \"\"\"Test that specific segments parse as expected.\n\n    NB: We're testing the PARSE function not the MATCH function\n    although this will be a recursive parse and so the match\n    function of SUBSECTIONS will be tested if present. The match\n    function of the parent will not be tested.\n    \"\"\"\n    dialect_specific_segment_parses(\"ansi\", segmentref, raw, caplog)\n", "type": "function"}, {"name": "test_dialect_postgres_specific_segment_parses", "is_method": false, "class_name": null, "parameters": ["segment_reference", "raw", "caplog", "dialect_specific_segment_parses"], "calls": ["pytest.mark.parametrize", "dialect_specific_segment_parses"], "code_location": {"file": "postgres_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 40, "end_line": 53}, "code_snippet": "def test_dialect_postgres_specific_segment_parses(\n    segment_reference: str,\n    raw: str,\n    caplog: LogCaptureFixture,\n    dialect_specific_segment_parses: Callable,\n) -> None:\n    \"\"\"Test that specific segments parse as expected.\n\n    NB: We're testing the PARSE function not the MATCH function\n    although this will be a recursive parse and so the match\n    function of SUBSECTIONS will be tested if present. The match\n    function of the parent will not be tested.\n    \"\"\"\n    dialect_specific_segment_parses(\"postgres\", segment_reference, raw, caplog)\n", "type": "function"}, {"name": "dialect_specific_segment_parses", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.fixture"], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 119, "end_line": 121}, "code_snippet": "def dialect_specific_segment_parses():\n    \"\"\"Fixture to check specific segments of a dialect.\"\"\"\n    return _dialect_specific_segment_parses\n", "type": "function"}, {"name": "_dialect_specific_segment_parses", "is_method": false, "class_name": null, "parameters": ["dialect", "segmentref", "raw", "caplog"], "calls": ["FluffConfig", "lex", "validate_segment", "is_type", "ParseContext.from_config", "isinstance", "result.apply", "print", "print", "print", "print", "parsed.type_set", "caplog.at_level", "Seg.match", "len", "type", "type", "dict"], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 42, "end_line": 75}, "code_snippet": "def _dialect_specific_segment_parses(dialect, segmentref, raw, caplog):\n    \"\"\"Test that specific segments parse as expected.\n\n    NB: We're testing the PARSE function not the MATCH function\n    although this will be a recursive parse and so the match\n    function of SUBSECTIONS will be tested if present. The match\n    function of the parent will not be tested.\n    \"\"\"\n    config = FluffConfig(overrides=dict(dialect=dialect))\n    segments = lex(raw, config=config)\n    Seg = validate_segment(segmentref, config=config)\n\n    # Most segments won't handle the end of file marker. We should strip it.\n    if segments[-1].is_type(\"end_of_file\"):\n        segments = segments[:-1]\n\n    ctx = ParseContext.from_config(config)\n    with caplog.at_level(logging.DEBUG):\n        result = Seg.match(segments, 0, parse_context=ctx)\n    assert isinstance(result, MatchResult)\n    parsed = result.apply(segments)\n    assert len(parsed) == 1\n    print(parsed)\n    parsed = parsed[0]\n\n    # Check we get a good response\n    print(parsed)\n    print(type(parsed))\n    print(type(parsed.raw))\n    # Check we're all there.\n    assert parsed.raw == raw\n    # Check that there's nothing un parsable\n    typs = parsed.type_set()\n    assert \"unparsable\" not in typs\n", "type": "function"}, {"name": "test__dialect__base_parse_struct", "is_method": false, "class_name": null, "parameters": ["dialect", "sqlfile", "code_only", "yamlfile", "yaml_loader"], "calls": ["pytest.mark.parametrize", "parse_example_file", "compute_parse_tree_hash", "yaml_loader", "parsed.to_tuple", "make_dialect_path"], "code_location": {"file": "dialects_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 126, "end_line": 157}, "code_snippet": "def test__dialect__base_parse_struct(\n    dialect,\n    sqlfile,\n    code_only,\n    yamlfile,\n    yaml_loader,\n):\n    \"\"\"For given test examples, check parsed structure against yaml.\"\"\"\n    parsed: Optional[BaseSegment] = parse_example_file(dialect, sqlfile)\n    actual_hash = compute_parse_tree_hash(parsed)\n    # Load the YAML\n    expected_hash, res = yaml_loader(make_dialect_path(dialect, yamlfile))\n    if not parsed:\n        assert parsed == res\n        return\n\n    # Verify the current parse tree matches the historic parse tree.\n    parsed_tree = parsed.to_tuple(code_only=code_only, show_raw=True)\n    # The parsed tree consists of a tuple of \"File:\", followed by the\n    # statements. So only compare when there is at least one statement.\n    if parsed_tree[1] or res[1]:\n        assert parsed_tree == res\n    # Verify the current hash matches the historic hash. The main purpose of\n    # this check is to force contributors to use the generator script to\n    # create these files. New contributors have sometimes been unaware of\n    # this tool and have attempted to craft the YAML files manually. This\n    # can lead to slight differences, confusion, and errors.\n    assert expected_hash == actual_hash, (\n        \"Parse tree hash does not match. Please run \"\n        \"'python test/generate_parse_fixture_yml.py' to create YAML files \"\n        \"in test/fixtures/dialects.\"\n    )\n", "type": "function"}, {"name": "test_dialect_exasol_specific_segment_parses", "is_method": false, "class_name": null, "parameters": ["segmentref", "raw", "caplog", "dialect_specific_segment_parses"], "calls": ["pytest.mark.parametrize", "dialect_specific_segment_parses"], "code_location": {"file": "exasol_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 17, "end_line": 21}, "code_snippet": "def test_dialect_exasol_specific_segment_parses(\n    segmentref, raw, caplog, dialect_specific_segment_parses\n):\n    \"\"\"Test exasol specific segments.\"\"\"\n    dialect_specific_segment_parses(TEST_DIALECT, segmentref, raw, caplog)\n", "type": "function"}, {"name": "test__dialect__ansi_specific_segment_not_match", "is_method": false, "class_name": null, "parameters": ["segmentref", "raw", "caplog", "dialect_specific_segment_not_match"], "calls": ["pytest.mark.parametrize", "dialect_specific_segment_not_match"], "code_location": {"file": "ansi_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 148, "end_line": 156}, "code_snippet": "def test__dialect__ansi_specific_segment_not_match(\n    segmentref, raw, caplog, dialect_specific_segment_not_match\n):\n    \"\"\"Test that specific segments do not match.\n\n    NB: We're testing the MATCH function not the PARSE function.\n    This is the opposite to the above.\n    \"\"\"\n    dialect_specific_segment_not_match(\"ansi\", segmentref, raw, caplog)\n", "type": "function"}, {"name": "_dialect_specific_segment_not_match", "is_method": false, "class_name": null, "parameters": ["dialect", "segmentref", "raw", "caplog"], "calls": ["FluffConfig", "lex", "validate_segment", "ParseContext.from_config", "caplog.at_level", "Seg.match", "dict"], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 78, "end_line": 92}, "code_snippet": "def _dialect_specific_segment_not_match(dialect, segmentref, raw, caplog):\n    \"\"\"Test that specific segments do not match.\n\n    NB: We're testing the MATCH function not the PARSE function.\n    This is the opposite to the above.\n    \"\"\"\n    config = FluffConfig(overrides=dict(dialect=dialect))\n    segments = lex(raw, config=config)\n    Seg = validate_segment(segmentref, config=config)\n\n    ctx = ParseContext.from_config(config)\n    with caplog.at_level(logging.DEBUG):\n        match = Seg.match(segments, 0, parse_context=ctx)\n\n    assert not match\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.33757710456848145}
{"question": " What would be the impact of modifying the pattern matching definition in the list partition definition segment to support nested bracketed delimited literals on parsing dependencies of the range partition definition segment and other partition classes in the Apache Doris dialect?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "PartitionSegment", "docstring": "A partition segment supporting Doris specific syntax.\n\nSupports:\n1. Auto partitioning (AUTO PARTITION BY RANGE)\n2. Manual range partitioning (PARTITION BY RANGE)\n3. List partitioning (PARTITION BY LIST)", "methods": [], "attributes": [], "code_location": {"file": "dialect_doris.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 212, "end_line": 258}, "type": "class"}, {"name": "ListPartitionDefinitionSegment", "docstring": "List partition definition with VALUES IN syntax.", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_doris.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 318, "end_line": 336}, "type": "class"}, {"name": "PartitionSegment", "docstring": "A partition segment supporting StarRocks specific syntax.\n\nSupports three types of partitioning:\n1. Range partitioning (PARTITION BY RANGE)\n2. Expression partitioning using time functions (date_trunc/time_slice)\n3. Expression partitioning using column expressions", "methods": [], "attributes": ["type", "match_grammar", "type", "match_grammar"], "code_location": {"file": "dialect_starrocks.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 158, "end_line": 233}, "type": "class"}, {"name": "RangePartitionDefinitionSegment", "docstring": "Range partition definition with VALUES LESS THAN or VALUES range.", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_doris.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 261, "end_line": 300}, "type": "class"}, {"name": "RangePartitionIntervalSegment", "docstring": "Range partition definition with FROM TO INTERVAL syntax.", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_doris.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 303, "end_line": 315}, "type": "class"}, {"name": "SetClauseListSegment", "docstring": "set clause list.\n\nOverriding ANSI to remove Delimited", "methods": [], "attributes": [], "code_location": {"file": "dialect_tsql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 4687, "end_line": 4703}, "type": "class"}, {"name": "ListComprehensionExpressionSegment", "docstring": "A list comprehension expression in duckdb.\n\nhttps://duckdb.org/docs/sql/functions/list#list-comprehension", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_duckdb.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 541, "end_line": 556}, "type": "class"}, {"name": "SelectPartitionClauseSegment", "docstring": "This is the body of a partition clause.", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_mysql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2160, "end_line": 2168}, "type": "class"}, {"name": "PatternSegment", "docstring": "A `PATTERN` expression.\n\nhttps://docs.snowflake.com/en/sql-reference/constructs/match_recognize.html", "methods": [], "attributes": ["type", "match_grammar"], "code_location": {"file": "dialect_snowflake.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1541, "end_line": 1561}, "type": "class"}, {"name": "ColumnDefinitionSegment", "docstring": "A column definition, e.g. for CREATE TABLE or ALTER TABLE.\n\nDoris-specific version that supports aggregation functions like\nMAX, MIN, REPLACE, SUM.", "methods": [], "attributes": [], "code_location": {"file": "dialect_doris.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 63, "end_line": 83}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.4590580463409424}
{"question": " Why does the SQL dialect segment class for unordered SELECT statements use grammar composition with copy and insert methods rather than defining match_grammar from scratch?", "answer": "", "relative_code_list": null, "ground_truth": null, "score": null, "retrieved_content": [{"name": "UnorderedSelectStatementSegment", "docstring": "Enhance unordered `SELECT` statement for valid SparkSQL clauses.\n\nThis is designed for use in the context of set operations,\nfor other use cases, we should use the main\nSelectStatementSegment.", "methods": [], "attributes": [], "code_location": {"file": "dialect_sparksql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2131, "end_line": 2148}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "Overrides ANSI Statement, to allow for SELECT INTO statements.", "methods": [], "attributes": [], "code_location": {"file": "dialect_postgres.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1781, "end_line": 1796}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "An unordered `SELECT` statement.\n\nhttps://dev.mysql.com/doc/refman/5.7/en/select.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_teradata.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 863, "end_line": 872}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "A `SELECT` statement without any ORDER clauses or later.\n\nReplaces (without overriding) ANSI to remove Eager Matcher", "methods": [], "attributes": [], "code_location": {"file": "dialect_sqlite.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 963, "end_line": 979}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "Enhance unordered SELECT statement to include CLUSTER, DISTRIBUTE, SORT BY.", "methods": [], "attributes": [], "code_location": {"file": "dialect_hive.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 979, "end_line": 988}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "Overrides Postgres Statement, adding DISTRIBUTED BY as a terminator.", "methods": [], "attributes": [], "code_location": {"file": "dialect_greenplum.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 331, "end_line": 338}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "A `SELECT` statement without any ORDER clauses or later.\n\nThis is designed for use in the context of set operations,\nfor other use cases, we should use the main\nSelectStatementSegment.", "methods": [], "attributes": [], "code_location": {"file": "dialect_ansi.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2807, "end_line": 2833}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "A `SELECT` statement without any ORDER clauses or later.\n\nThis is designed for use in the context of set operations,\nfor other use cases, we should use the main\nSelectStatementSegment.", "methods": [], "attributes": [], "code_location": {"file": "dialect_oracle.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1492, "end_line": 1521}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "A `SELECT` statement without any ORDER clauses or later.\n\nThis is designed for use in the context of set operations,\nfor other use cases, we should use the main\nSelectStatementSegment.", "methods": [], "attributes": [], "code_location": {"file": "dialect_mysql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 2039, "end_line": 2071}, "type": "class"}, {"name": "UnorderedSelectStatementSegment", "docstring": "A `SELECT` statement without any ORDER clauses or later.\n\nThis is designed for use in the context of set operations,\nfor other use cases, we should use the main\nSelectStatementSegment.", "methods": [], "attributes": [], "code_location": {"file": "dialect_duckdb.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 583, "end_line": 618}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.46701526641845703}
