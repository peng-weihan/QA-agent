{"question": "What is the relationship between BaseSegment and RawSegment in SQLFluff's parser?", "answer": null, "relative_code_list": null, "ground_truth": "BaseSegment and RawSegment have a hierarchical inheritance relationship in SQLFluff's parser architecture. BaseSegment is the abstract base class that serves as the foundation for all segments, while RawSegment is a concrete implementation that represents the leaf nodes of the parse tree. Key aspects of their relationship include: 1) Inheritance hierarchy - RawSegment inherits from BaseSegment (class RawSegment(BaseSegment)), making it a specialized type of segment; 2) Structural differences - BaseSegment is designed to contain other subsegments and represents composite elements in the parse tree, while RawSegment has no children (self.segments = ()) and represents atomic tokens; 3) Lexer output - RawSegment is the primary output of the lexer, representing individual tokens like keywords, identifiers, literals, and whitespace; 4) Type system - BaseSegment has a class-level 'type' attribute and _class_types, while RawSegment has instance-level instance_types that can be set during initialization; 5) Content representation - BaseSegment's raw content is computed by concatenating its child segments' raw content, while RawSegment stores its raw content directly in the _raw attribute; 6) Position tracking - both use PositionMarker for location information, but RawSegment requires it during initialization; 7) Specialization - RawSegment includes additional properties like is_code, is_comment, is_whitespace, and methods like normalize() for token-specific behavior; 8) Tree structure - BaseSegments form the internal nodes of the parse tree, while RawSegments form the leaves, creating a complete hierarchical representation of SQL code.", "score": null, "retrieved_content": [{"name": "RawSegment", "docstring": "This is a segment without any subsegments.", "methods": ["__init__", "__repr__", "__setattr__", "is_code", "is_comment", "is_whitespace", "raw", "raw_upper", "raw_segments", "class_types", "source_fixes", "invalidate_caches", "get_type", "is_type", "get_raw_segments", "raw_trimmed", "normalize", "raw_normalized", "stringify", "_suffix", "edit", "_get_raw_segment_kwargs", "from_result_segments"], "attributes": ["type", "_is_code", "_is_comment", "_is_whitespace", "_default_raw"], "code_location": {"file": "raw.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 16, "end_line": 300}, "type": "class"}, {"name": "test__parser__base_segments_raw", "is_method": false, "class_name": null, "parameters": ["raw_seg"], "calls": ["str", "repr", "raw_seg.stringify", "raw_seg.to_tuple", "raw_seg.to_tuple"], "code_location": {"file": "segments_base_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser/segments", "start_line": 196, "end_line": 211}, "code_snippet": "def test__parser__base_segments_raw(raw_seg):\n    \"\"\"Test raw segments behave as expected.\"\"\"\n    # Check Segment Return\n    assert raw_seg.segments == ()\n    assert raw_seg.raw == \"foobar\"\n    # Check Formatting and Stringification\n    assert str(raw_seg) == repr(raw_seg) == \"<CodeSegment: ([L:  1, P:  1]) 'foobar'>\"\n    assert (\n        raw_seg.stringify(ident=1, tabsize=2)\n        == \"[L:  1, P:  1]      |  raw:                                                \"\n        \"        'foobar'\\n\"\n    )\n    # Check tuple\n    assert raw_seg.to_tuple() == (\"raw\", ())\n    # Check tuple\n    assert raw_seg.to_tuple(show_raw=True) == (\"raw\", \"foobar\")\n", "type": "function"}, {"name": "BaseSegment", "docstring": "The base segment element.\n\nThis defines the base element which drives both Lexing, Parsing and Linting.\nA large chunk of the logic which defines those three operations are centered\nhere. Much of what is defined in the BaseSegment is also used by its many\nsubclasses rather than directly here.\n\nFor clarity, the `BaseSegment` is mostly centered around a segment which contains\nother subsegments. For segments which don't have *children*, refer to the\n`RawSegment` class (which still inherits from this one).\n\nSegments are used both as instances to hold chunks of text, but also as classes\nthemselves where they function a lot like grammars, and return instances of\nthemselves when they match. The many classmethods in this class are usually to serve\ntheir purpose as a matcher.", "methods": ["__init__", "__setattr__", "__eq__", "_hash", "__hash__", "__repr__", "__getstate__", "__setstate__", "_comments", "_non_comments", "is_code", "_code_indices", "is_comment", "is_whitespace", "raw", "class_types", "descendant_type_set", "direct_descendant_type_set", "raw_upper", "raw_segments", "raw_segments_with_ancestors", "source_fixes", "first_non_whitespace_segment_raw_upper", "is_templated", "_suffix", "_position_segments", "simple", "cache_key", "is_optional", "class_is_type", "structural_simplify", "match", "_recalculate_caches", "_preface", "set_as_parent", "set_parent", "get_parent", "get_type", "count_segments", "is_type", "invalidate_caches", "get_start_point_marker", "get_end_point_marker", "get_start_loc", "get_end_loc", "stringify", "to_tuple", "copy", "as_record", "get_raw_segments", "raw_normalized", "iter_segments", "iter_unparsables", "type_set", "is_raw", "get_child", "get_children", "select_children", "recursive_crawl_all", "recursive_crawl", "path_to", "_is_code_or_meta", "validate_non_code_ends", "validate_segment_with_reparse", "_log_apply_fixes_check_issue", "edit", "from_result_segments"], "attributes": ["comment_separate", "is_meta", "can_start_end_non_code", "allow_empty"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 141, "end_line": 1239}, "type": "class"}, {"name": "test__parser__base_segments_raw_compare", "is_method": false, "class_name": null, "parameters": [], "calls": ["TemplatedFile.from_string", "RawSegment", "RawSegment", "PositionMarker", "PositionMarker", "slice", "slice", "slice", "slice"], "code_location": {"file": "segments_base_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser/segments", "start_line": 245, "end_line": 250}, "code_snippet": "def test__parser__base_segments_raw_compare():\n    \"\"\"Test comparison of raw segments.\"\"\"\n    template = TemplatedFile.from_string(\"foobar\")\n    rs1 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n    rs2 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n    assert rs1 == rs2\n", "type": "function"}, {"name": "BinaryOperatorSegment", "docstring": "A binary operator segment.\n\nDefined here for type inheritance. Inherits from RawSegment.", "methods": [], "attributes": ["type"], "code_location": {"file": "common.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 93, "end_line": 99}, "type": "class"}, {"name": "from_result_segments", "is_method": true, "class_name": "RawSegment", "parameters": ["cls", "result_segments", "segment_kwargs"], "calls": ["cast", "raw_seg._get_raw_segment_kwargs", "new_segment_kwargs.update", "cls", "len"], "code_location": {"file": "raw.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 286, "end_line": 300}, "code_snippet": "    def from_result_segments(\n        cls,\n        result_segments: tuple[BaseSegment, ...],\n        segment_kwargs: dict[str, Any],\n    ) -> \"RawSegment\":\n        \"\"\"Create a RawSegment from result segments.\"\"\"\n        assert len(result_segments) == 1\n        raw_seg = cast(\"RawSegment\", result_segments[0])\n        new_segment_kwargs = raw_seg._get_raw_segment_kwargs()\n        new_segment_kwargs.update(segment_kwargs)\n        return cls(\n            raw=raw_seg.raw,\n            pos_marker=raw_seg.pos_marker,\n            **new_segment_kwargs,\n        )\n", "type": "function"}, {"name": "test__parser__raw_segment_raw_normalized", "is_method": false, "class_name": null, "parameters": [], "calls": ["TemplatedFile.from_string", "RawSegment", "RawSegment", "RawSegment", "BaseSegment", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "rs1.raw_normalized", "rs1.raw_normalized", "rs2.raw_normalized", "rs2.raw_normalized", "rs3.raw_normalized", "rs3.raw_normalized", "bs1.raw_normalized", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice"], "code_location": {"file": "segments_base_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser/segments", "start_line": 332, "end_line": 371}, "code_snippet": "def test__parser__raw_segment_raw_normalized():\n    \"\"\"Test comparison of raw segments.\"\"\"\n    template = TemplatedFile.from_string('\"a\"\"\".\"e\"')\n    rs1 = RawSegment(\n        '\"a\"\"\"',\n        PositionMarker(slice(0, 5), slice(0, 5), template),\n        quoted_value=(r'\"((?:[^\"]|\"\")*)\"', 1),\n        escape_replacements=[('\"\"', '\"')],\n        casefold=str.upper,\n    )\n    rs2 = RawSegment(\n        \".\",\n        PositionMarker(slice(6, 7), slice(6, 7), template),\n    )\n    rs3 = RawSegment(\n        '\"e\"',\n        PositionMarker(slice(8, 10), slice(8, 10), template),\n        quoted_value=(r'\"((?:[^\"]|\"\")*)\"', 1),\n        escape_replacements=[('\"\"', '\"')],\n        casefold=str.upper,\n    )\n    bs1 = BaseSegment(\n        (\n            rs1,\n            rs2,\n            rs3,\n        ),\n        PositionMarker(slice(0, 10), slice(0, 10), template),\n    )\n    assert rs1.raw == '\"a\"\"\"'\n    assert rs1.raw_normalized(False) == 'a\"'\n    assert rs1.raw_normalized() == 'A\"'\n    assert rs2.raw == \".\"\n    assert rs2.raw_normalized(False) == \".\"\n    assert rs2.raw_normalized() == \".\"\n    assert rs3.raw == '\"e\"'\n    assert rs3.raw_normalized(False) == \"e\"\n    assert rs3.raw_normalized() == \"E\"\n    assert bs1.raw == '\"a\"\"\".\"e\"'\n    assert bs1.raw_normalized() == 'A\".E'\n", "type": "function"}, {"name": "CodeSegment", "docstring": "An alias for RawSegment.\n\nThis has a more explicit name for segment creation.", "methods": [], "attributes": [], "code_location": {"file": "common.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 11, "end_line": 17}, "type": "class"}, {"name": "ComparisonOperatorSegment", "docstring": "A comparison operator segment.\n\nDefined here for type inheritance. Inherits from RawSegment.", "methods": [], "attributes": ["type"], "code_location": {"file": "common.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 111, "end_line": 117}, "type": "class"}, {"name": "test__parser__raw_get_raw_segments", "is_method": false, "class_name": null, "parameters": ["raw_segments"], "calls": ["s.get_raw_segments"], "code_location": {"file": "segments_raw_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser/segments", "start_line": 6, "end_line": 9}, "code_snippet": "def test__parser__raw_get_raw_segments(raw_segments):\n    \"\"\"Test niche case of calling get_raw_segments on a raw segment.\"\"\"\n    for s in raw_segments:\n        assert s.get_raw_segments() == [s]\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0233445167541504}
{"question": "What is SQLFluff's rule categorization system?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's rule categorization system organizes rules into groups and categories for easier management and selection. The system includes: 1) Core rules - a special 'core' group containing rules that are stable, apply to most dialects, can detect syntax issues, and aren't too opinionated toward one style. Core rules make it easier to roll out SQLFluff to teams by providing a 'common sense' subset initially; 2) Rule groups - rules can belong to multiple groups like 'all', 'layout', 'capitalisation', 'aliasing', 'references', 'ambiguous', 'structure', 'convention', etc. Each rule must belong to the 'all' group; 3) Rule references - rules can be selected by code (e.g., 'LT01'), name (e.g., 'layout.spacing'), alias (often deprecated codes like 'L003'), or group (e.g., 'layout' or 'capitalisation'); 4) Rule metadata - each rule has a code, name, description, groups tuple, and aliases tuple; 5) RuleSet class - manages rule registration, validation, and filtering with methods like register() and get_rulelist(); 6) Configuration integration - rules can be enabled/disabled via config files using rules and exclude_rules parameters, and can be downgraded to warnings using the warnings parameter.", "score": null, "retrieved_content": [{"name": "SQLFluffRule", "docstring": "SQLFluff rule directive for sphinx.\n\nRule directives can be used as shown below.\n\n.. code-block:: rst\n\n    .. sqlfluff:rule:: AM01\n                       ambiguous.distinct\n\n        Write the documentation for the rule here.\n\nTo cross reference (i.e. refer to) objects defined like this\nboth the code and name reference is available:\n\n.. code-block:: rst\n\n    :sqlfluff:ref:`CP02`\n    :sqlfluff:ref:`capitalisation.identifiers`", "methods": ["handle_signature", "add_target_and_index", "_object_hierarchy_parts", "_toc_entry_name"], "attributes": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 10, "end_line": 79}, "type": "class"}, {"name": "RuleMetaclass", "docstring": "The metaclass for rules.\n\nThis metaclass provides provides auto-enrichment of the\nrule docstring so that examples, groups, aliases and\nnames are added.\n\nThe reason we enrich the docstring is so that it can be\npicked up by autodoc and all be displayed in the sqlfluff\ndocs.", "methods": ["_populate_code_and_description", "_populate_docstring", "__new__"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 151, "end_line": 361}, "type": "class"}, {"name": "Rule_LT14", "docstring": "Keyword clauses should follow a standard for being before/after newlines.\n\n**Anti-pattern**\n\nIn this example, the keyword are not at the beginning of or alone on the line.\n\n.. code-block:: sql\n\n    SELECT 'a' AS col FROM tab WHERE x = 4 ORDER BY y LIMIT 5\n\n**Best practice**\n\n.. code-block:: sql\n\n    SELECT 'a' AS col\n    FROM tab\n    WHERE x = 4\n    ORDER BY y\n    LIMIT 5\n\n.. code-block:: sql\n\n    SELECT 'a' AS col\n    FROM\n        tab\n    WHERE\n        x = 4\n    ORDER BY\n        y\n    LIMIT\n        5", "methods": ["_eval"], "attributes": ["name", "groups", "crawl_behaviour", "is_fix_compatible"], "code_location": {"file": "LT14.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 10, "end_line": 56}, "type": "class"}, {"name": "test__api__info_rules", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.list_rules", "isinstance", "RuleTuple"], "code_location": {"file": "info_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 29, "end_line": 52}, "code_snippet": "def test__api__info_rules():\n    \"\"\"Basic linting of dialects.\"\"\"\n    rules = sqlfluff.list_rules()\n    assert isinstance(rules, list)\n    assert (\n        RuleTuple(\n            code=\"LT01\",\n            name=\"layout.spacing\",\n            description=\"Inappropriate Spacing.\",\n            groups=(\"all\", \"core\", \"layout\"),\n            aliases=(\n                \"L001\",\n                \"L005\",\n                \"L006\",\n                \"L008\",\n                \"L023\",\n                \"L024\",\n                \"L039\",\n                \"L048\",\n                \"L071\",\n            ),\n        )\n        in rules\n    )\n", "type": "function"}, {"name": "SQLLintError", "docstring": "An error which occurred during linting.\n\nIn particular we reference the rule here to do extended logging based on\nthe rule in question which caused the fail.\n\nArgs:\n    segment (:obj:`BaseSegment`, optional): The segment which is relevant\n        for the failure in parsing. This is likely to be a subclass of\n        `BaseSegment` rather than the parent class itself. This is mostly\n        used for logging and for referencing position.", "methods": ["__init__", "__reduce__", "to_dict", "fixable", "rule_code", "rule_name", "source_signature", "__repr__"], "attributes": ["_identifier"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 249, "end_line": 383}, "type": "class"}, {"name": "Rule_LT11", "docstring": "Set operators should be surrounded by newlines.\n\n**Anti-pattern**\n\nIn this example, `UNION ALL` is not on a line itself.\n\n.. code-block:: sql\n\n    SELECT 'a' AS col UNION ALL\n    SELECT 'b' AS col\n\n**Best practice**\n\n.. code-block:: sql\n\n    SELECT 'a' AS col\n    UNION ALL\n    SELECT 'b' AS col", "methods": ["_eval"], "attributes": ["name", "aliases", "groups", "is_fix_compatible", "crawl_behaviour"], "code_location": {"file": "LT11.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 8, "end_line": 52}, "type": "class"}, {"name": "Rule_RF06", "docstring": "Unnecessary quoted identifier.\n\nThis rule will fail if the quotes used to quote an identifier are (un)necessary\ndepending on the ``force_quote_identifier`` configuration. This rule applies to\nboth column *references* and their *aliases*. The *default* (safe) behaviour is\ndesigned not to unexpectedly corrupt SQL. That means the circumstances in which\nquotes can be safely removed depends on the current dialect would resolve the\nunquoted variant of the identifier (see below for examples).\n\nAdditionally this rule may be configured to a more aggressive setting by setting\n:code:`case_sensitive` to :code:`False`, in which case quotes will be removed\nregardless of the casing of the contained identifier. Any identifiers which contain\nspecial characters, spaces or keywords will still be left quoted. This setting is\nmore appropriate for projects or teams where there is more control over the inputs\nand outputs of queries, and where it's more viable to institute rules such\nas enforcing that all identifiers are the default casing (and therefore meaning\nthat using quotes to change the case of identifiers is unnecessary).\n\n.. list-table::\n   :widths: 26 26 48\n   :header-rows: 1\n\n   * - Dialect group\n     - ✅ Example where quotes are safe to remove.\n     - ⚠️ Examples where quotes are not safe to remove.\n   * - Natively :code:`UPPERCASE` dialects e.g. Snowflake, BigQuery,\n       TSQL & Oracle.\n     - Identifiers which, without quotes, would resolve to the default\n       casing of :code:`FOO` i.e. :code:`\"FOO\"`.\n     - Identifiers where the quotes are necessary to preserve case\n       (e.g. :code:`\"Foo\"` or :code:`\"foo\"`), or where the identifier\n       contains something invalid without the quotes such as keywords\n       or special characters e.g. :code:`\"SELECT\"`, :code:`\"With Space\"`\n       or :code:`\"Special&Characters\"`.\n   * - Natively :code:`lowercase` dialects e.g. Athena,\n       Hive & Postgres\n     - Identifiers which, without quotes, would resolve to the default\n       casing of :code:`foo` i.e. :code:`\"foo\"`.\n     - Identifiers where the quotes are necessary to preserve case\n       (e.g. :code:`\"Foo\"` or :code:`\"foo\"`), or where the identifier\n       contains something invalid without the quotes such as keywords\n       or special characters e.g. :code:`\"SELECT\"`, :code:`\"With Space\"`\n       or :code:`\"Special&Characters\"`.\n   * - Case insensitive dialects e.g. :ref:`duckdb_dialect_ref` or\n       :ref:`sparksql_dialect_ref`\n     - Any identifiers which are valid without quotes: e.g. :code:`\"FOO\"`,\n       :code:`\"foo\"`, :code:`\"Foo\"`, :code:`\"fOo\"`, :code:`FOO` and\n       :code:`foo` would all resolve to the same object.\n     - Identifiers which contain something invalid without the quotes\n       such as keywords or special characters e.g. :code:`\"SELECT\"`,\n       :code:`\"With Space\"` or :code:`\"Special&Characters\"`.\n\nThis rule is closely associated with (and constrained by the same above\nfactors) as :sqlfluff:ref:`aliasing.self_alias.column` (:sqlfluff:ref:`AL09`).\n\nWhen ``prefer_quoted_identifiers = False`` (default behaviour), the quotes are\nunnecessary, except for reserved keywords and special characters in identifiers.\n\n**Anti-pattern**\n\nIn this example, valid unquoted identifiers,\nthat are not also reserved keywords, are needlessly quoted.\n\n.. code-block:: sql\n\n    SELECT \"foo\" as \"bar\";  -- For lowercase dialects like Postgres\n    SELECT \"FOO\" as \"BAR\";  -- For uppercase dialects like Snowflake\n\n**Best practice**\n\nUse unquoted identifiers where possible.\n\n.. code-block:: sql\n\n    SELECT foo as bar;  -- For lowercase dialects like Postgres\n    SELECT FOO as BAR;  -- For uppercase dialects like Snowflake\n\n    -- Note that where the case of the quoted identifier requires\n    -- the quotes to remain, or where the identifier cannot be\n    -- unquoted because it would be invalid to do so, the quotes\n    -- may remain. For example:\n    SELECT\n        \"Case_Sensitive_Identifier\" as is_allowed,\n        \"Identifier with spaces or speci@l characters\" as this_too,\n        \"SELECT\" as also_reserved_words\n    FROM \"My Table With Spaces\"\n\nWhen ``prefer_quoted_identifiers = True``, the quotes are always necessary, no\nmatter if the identifier is valid, a reserved keyword, or contains special\ncharacters.\n\n.. note::\n   Note due to different quotes being used by different dialects supported by\n   `SQLFluff`, and those quotes meaning different things in different contexts,\n   this mode is not ``sqlfluff fix`` compatible.\n\n**Anti-pattern**\n\nIn this example, a valid unquoted identifier, that is also not a reserved keyword,\nis required to be quoted.\n\n.. code-block:: sql\n\n    SELECT 123 as foo\n\n**Best practice**\nUse quoted identifiers.\n\n.. code-block:: sql\n\n    SELECT 123 as \"foo\" -- For ANSI, ...\n    -- or\n    SELECT 123 as `foo` -- For BigQuery, MySql, ...", "methods": ["_eval", "ignore_words_list"], "attributes": ["name", "aliases", "groups", "config_keywords", "crawl_behaviour", "is_fix_compatible"], "code_location": {"file": "RF06.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 17, "end_line": 294}, "type": "class"}, {"name": "SQLFluffDomain", "docstring": "SQLFluff domain.", "methods": ["get_full_qualified_name", "get_objects", "resolve_xref", "add_rule"], "attributes": ["name", "label", "object_types", "roles", "directives", "initial_data"], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 82, "end_line": 142}, "type": "class"}, {"name": "Rule_LT03", "docstring": "Operators should follow a standard for being before/after newlines.\n\nThe configuration for whether operators should be ``trailing`` or\n``leading`` is part of :ref:`layoutconfig`. The default configuration is:\n\n.. code-block:: cfg\n\n    [sqlfluff:layout:type:binary_operator]\n    line_position = leading\n\n    [sqlfluff:layout:type:comparison_operator]\n    line_position = leading\n\n**Anti-pattern**\n\nIn this example, if ``line_position = leading`` (or unspecified, as is the\ndefault), then the operator ``+`` should not be at the end of the second line.\n\n.. code-block:: sql\n\n    SELECT\n        a +\n        b\n    FROM foo\n\n\n**Best practice**\n\nIf ``line_position = leading`` (or unspecified, as this is the default),\nplace the operator after the newline.\n\n.. code-block:: sql\n\n    SELECT\n        a\n        + b\n    FROM foo\n\nIf ``line_position = trailing``, place the operator before the newline.\n\n.. code-block:: sql\n\n    SELECT\n        a +\n        b\n    FROM foo", "methods": ["_seek_newline", "_check_trail_lead_shortcut", "_eval"], "attributes": ["name", "aliases", "groups", "crawl_behaviour", "is_fix_compatible"], "code_location": {"file": "LT03.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 11, "end_line": 157}, "type": "class"}, {"name": "cli", "is_method": false, "class_name": null, "parameters": [], "calls": ["click.group", "click.version_option"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 480, "end_line": 481}, "code_snippet": "def cli() -> None:\n    \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.035832166671753}
{"question": "What is SQLFluff's approach to handling complex SQL constructs?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff handles complex SQL constructs through a multi-stage approach: 1) Templating preprocessing - for templated SQL (Jinja, Python format strings, placeholders), SQLFluff first converts raw/pre-templated code into valid SQL using dummy values, then backports rule violations to the original templated sections; 2) Grammar-based parsing - uses dialect-specific grammars to parse complex constructs by breaking them down into simpler components using grammar classes like Sequence, OneOf, Delimited, Bracketed, AnyNumberOf, AnySetOf, Ref, and Conditional; 3) Tree-like structure - creates a hierarchical parse tree where complex statements are decomposed into nested segments, with FileSegment as root containing StatementSegments and their sub-components; 4) Recursive matching - segments recursively match based on their respective grammars until reaching raw segments with no children; 5) Template-aware handling - for complex templated cases where template tags cut across the parse tree, SQLFluff treats template tags as being outside the SQL structure (similar to C preprocessor directives) and pulls all tags to the least indented level; 6) Implicit indentation support - handles complex WHERE clauses and CASE expressions that can be left un-closed before line end; 7) Error recovery - if no match is found for a segment, contents are wrapped in an UnparsableSegment for graceful handling.", "score": null, "retrieved_content": [{"name": "_recursively_check_is_complex", "is_method": false, "class_name": null, "parameters": ["select_clause_or_exp_children"], "calls": ["sp.not_", "select_clause_or_exp_children.select", "len", "filtered.first", "any", "_recursively_check_is_complex", "sp.is_type", "first_el.children", "first_el.recursive_crawl", "first_el.all", "sp.is_type"], "code_location": {"file": "AL03.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/aliasing", "start_line": 115, "end_line": 144}, "code_snippet": "def _recursively_check_is_complex(select_clause_or_exp_children: Segments) -> bool:\n    forgiveable_types = [\n        \"whitespace\",\n        \"newline\",\n        \"column_reference\",\n        \"wildcard_expression\",\n        \"bracketed\",\n    ]\n    selector = sp.not_(sp.is_type(*forgiveable_types))\n    filtered = select_clause_or_exp_children.select(selector)\n    remaining_count = len(filtered)\n\n    # Once we have removed the above if nothing remains,\n    # then this statement/expression was simple\n    if remaining_count == 0:\n        return False\n\n    first_el = filtered.first()\n\n    # If the element has a select statement inside, this is likely a subquery\n    if first_el.recursive_crawl(\"select_statement\").any():\n        return True\n\n    # Anything except a single expression seg remains\n    # Then it was complex\n    if remaining_count > 1 or not first_el.all(sp.is_type(\"expression\")):\n        return True\n\n    # If we have just an expression check if it was simple\n    return _recursively_check_is_complex(first_el.children())\n", "type": "function"}, {"name": "SQLFluffRule", "docstring": "SQLFluff rule directive for sphinx.\n\nRule directives can be used as shown below.\n\n.. code-block:: rst\n\n    .. sqlfluff:rule:: AM01\n                       ambiguous.distinct\n\n        Write the documentation for the rule here.\n\nTo cross reference (i.e. refer to) objects defined like this\nboth the code and name reference is available:\n\n.. code-block:: rst\n\n    :sqlfluff:ref:`CP02`\n    :sqlfluff:ref:`capitalisation.identifiers`", "methods": ["handle_signature", "add_target_and_index", "_object_hierarchy_parts", "_toc_entry_name"], "attributes": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 10, "end_line": 79}, "type": "class"}, {"name": "cli", "is_method": false, "class_name": null, "parameters": [], "calls": ["click.group", "click.version_option"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 480, "end_line": 481}, "code_snippet": "def cli() -> None:\n    \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n", "type": "function"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}, {"name": "SQLFluffDomain", "docstring": "SQLFluff domain.", "methods": ["get_full_qualified_name", "get_objects", "resolve_xref", "add_rule"], "attributes": ["name", "label", "object_types", "roles", "directives", "initial_data"], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 82, "end_line": 142}, "type": "class"}, {"name": "_eval", "is_method": true, "class_name": "Rule_ST05", "parameters": ["self", "context"], "calls": ["FunctionalContext", "segment.all", "parent_stack.any", "parent_stack.last", "Query.from_segment", "_CTEBuilder", "query.ctes.values", "segment.all", "_get_case_preference", "SegmentCloneMap", "self._lint_query", "is_type", "is_type", "is_type", "ctes.insert_cte", "is_type", "segment.children", "any", "results_list.append", "_create_table_ref", "this_seg_clone._position_segments", "ctes.replace_with_clone", "ctes.ensure_space_after_from", "ctes.compose_select", "len", "is_type", "LintFix.replace", "segment.children", "is_keyword", "subquery_parent.recursive_crawl_all"], "code_location": {"file": "ST05.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 110, "end_line": 220}, "code_snippet": "    def _eval(self, context: RuleContext) -> EvalResultType:\n        \"\"\"Join/From clauses should not contain subqueries. Use CTEs instead.\"\"\"\n        self.forbid_subquery_in: str\n        functional_context = FunctionalContext(context)\n        segment = functional_context.segment\n        parent_stack = functional_context.parent_stack\n        is_select = segment.all(is_type(*_SELECT_TYPES))\n        is_select_child = parent_stack.any(is_type(*_SELECT_TYPES))\n        insert_parent = parent_stack.last(is_type(\"insert_statement\"))\n        if not is_select or is_select_child:\n            # Nothing to do.\n            return None\n\n        query: Query = Query.from_segment(context.segment, context.dialect)\n\n        # generate an instance which will track and shape our output CTE\n        ctes = _CTEBuilder()\n        # Init the output/final select &\n        # populate existing CTEs\n        for cte in query.ctes.values():\n            ctes.insert_cte(cte.cte_definition_segment)\n\n        is_with = segment.all(is_type(\"with_compound_statement\"))\n        # TODO: consider if we can fix recursive CTEs\n        is_recursive = is_with and len(segment.children(is_keyword(\"recursive\"))) > 0\n        case_preference = _get_case_preference(segment)\n        output_select = segment\n        if is_with:\n            output_select = segment.children(\n                is_type(\n                    \"set_expression\",\n                    \"select_statement\",\n                    \"insert_statement\",\n                )\n            )\n        elif insert_parent and context.dialect.name in self._with_before_insert:\n            # Here we select the parent `insert_statement` because it should be where\n            # we place the new CTE.\n            output_select = insert_parent\n            segment = insert_parent\n\n        # Issue 3617: In T-SQL (and possibly other dialects) the automated fix\n        # leaves parentheses in a location that causes a syntax error. This is an\n        # unusual corner case. For simplicity, we still generate the lint warning\n        # but don't try to generate a fix. Someone could look at this later (a\n        # correct fix would involve removing the parentheses.)\n        bracketed_ctas = [seg.type for seg in parent_stack[-2:]] == [\n            \"create_table_statement\",\n            \"bracketed\",\n        ]\n\n        # If there are offending elements calculate fixes\n        clone_map = SegmentCloneMap(segment[0])\n        results = self._lint_query(\n            dialect=context.dialect,\n            query=query,\n            ctes=ctes,\n            case_preference=case_preference,\n            clone_map=clone_map,\n        )\n\n        results_list: list[tuple[LintResult, BaseSegment, str, BaseSegment, bool]] = []\n        for result in results:\n            (\n                lint_result,\n                from_expression,\n                alias_name,\n                subquery_parent,\n                is_fixable,\n            ) = result\n            assert any(\n                from_expression is seg for seg in subquery_parent.recursive_crawl_all()\n            )\n            results_list.append(result)\n            if not is_fixable:\n                continue\n            this_seg_clone = clone_map[from_expression]\n            new_table_ref = _create_table_ref(alias_name, context.dialect)\n            # Add positions to the new table reference, other rules may need a position\n            # but the clone is not a typical \"fix\".\n            assert this_seg_clone.pos_marker\n            this_seg_clone.segments = this_seg_clone._position_segments(\n                (new_table_ref,), this_seg_clone.pos_marker\n            )\n            ctes.replace_with_clone(subquery_parent, clone_map)\n\n        for (\n            lint_result,\n            from_expression,\n            alias_name,\n            subquery_parent,\n            is_fixable,\n        ) in results_list:\n            if bracketed_ctas or is_recursive or not is_fixable:\n                continue\n            # Compute fix.\n            output_select_clone = clone_map[output_select[0]]\n            fixes = ctes.ensure_space_after_from(\n                output_select[0], output_select_clone, subquery_parent\n            )\n            new_select = ctes.compose_select(\n                output_select_clone, case_preference=case_preference\n            )\n            lint_result.fixes = [\n                LintFix.replace(\n                    segment[0],\n                    edit_segments=[new_select],\n                )\n            ]\n            lint_result.fixes += fixes\n        return [lint_result[0] for lint_result in results_list]\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_ST04", "parameters": ["self", "context"], "calls": ["segment.select", "segment.children", "get", "get", "get", "case1_children.select", "case1_else_clause.children", "case1_else_expressions.children", "expression_children.select", "case2.children", "get", "get", "case1_else_clause.get", "case1_children.select", "case1_to_delete.select", "select", "case1_to_delete.apply", "context.config.get", "context.config.get", "self._get_indentation", "self._get_indentation", "case2.children", "list", "self._rebuild_spacing", "self._rebuild_spacing", "fixes.append", "fixes.append", "self._nested_end_trailing_comment", "LintResult", "FunctionalContext", "sp.is_type", "sp.is_type", "sp.is_type", "sp.is_type", "LintResult", "LintResult", "case1_to_delete.find", "sp.is_type", "LintFix.create_after", "LintFix.delete", "case1_children.first", "case1_children.first", "case1_children.last", "case2_children.first", "case2_children.first", "len", "len", "get", "case1_to_delete.get", "case1_else_clause.children", "sp.is_type", "case1_else_expressions.get", "sp.is_keyword", "sp.is_type", "sp.is_type", "sp.is_keyword", "sp.is_type", "select", "select", "case1_to_delete.last", "sp.is_comment", "segment.children", "case2.children", "sp.is_code", "sp.is_code"], "code_location": {"file": "ST04.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 50, "end_line": 159}, "code_snippet": "    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Nested CASE statement in ELSE clause could be flattened.\"\"\"\n        segment = FunctionalContext(context).segment\n        assert segment.select(sp.is_type(\"case_expression\"))\n        case1_children = segment.children()\n        case1_first_case = case1_children.first(sp.is_keyword(\"CASE\")).get()\n        assert case1_first_case\n        case1_first_when = case1_children.first(\n            sp.is_type(\"when_clause\", \"else_clause\")\n        ).get()\n        case1_last_when = case1_children.last(sp.is_type(\"when_clause\")).get()\n        case1_else_clause = case1_children.select(sp.is_type(\"else_clause\"))\n        case1_else_expressions = case1_else_clause.children(sp.is_type(\"expression\"))\n        expression_children = case1_else_expressions.children()\n        case2 = expression_children.select(sp.is_type(\"case_expression\"))\n        case2_children = case2.children()\n        case2_first_case = case2_children.first(sp.is_keyword(\"CASE\")).get()\n        case2_first_when = case2_children.first(\n            sp.is_type(\"when_clause\", \"else_clause\")\n        ).get()\n        # The len() checks below are for safety, to ensure the CASE inside\n        # the ELSE is not part of a larger expression. In that case, it's\n        # not safe to simplify in this way -- we'd be deleting other code.\n        if (\n            not case1_last_when\n            or len(case1_else_expressions) > 1\n            or len(expression_children) > 1\n            or not case2\n        ):\n            return LintResult()\n\n        # Determine if we can combine the else case statement, the first and\n        # second case expressions should be the same. If they aren't, that\n        # case currently isn't handled.\n        if [\n            x.raw_upper\n            for x in segment.children(sp.is_code())\n            .select(start_seg=case1_first_case, stop_seg=case1_first_when)\n            .raw_segments\n        ] != [\n            x.raw_upper\n            for x in case2.children(sp.is_code())\n            .select(start_seg=case2_first_case, stop_seg=case2_first_when)\n            .raw_segments\n        ]:\n            return LintResult()\n\n        # We can assert that this exists because of the previous check.\n        assert case1_last_when\n        # We can also assert that we'll also have an else clause because\n        # otherwise the case2 check above would fail.\n        case1_else_clause_seg = case1_else_clause.get()\n        assert case1_else_clause_seg\n\n        # Delete stuff between the last \"WHEN\" clause and the \"ELSE\" clause.\n        case1_to_delete = case1_children.select(\n            start_seg=case1_last_when, stop_seg=case1_else_clause_seg\n        )\n        # Restore any comments that were deleted\n        after_last_comment_index = (\n            case1_to_delete.find(case1_to_delete.last(sp.is_comment()).get()) + 1\n        )\n        case1_comments_to_restore = case1_to_delete.select(\n            stop_seg=case1_to_delete.get(after_last_comment_index)\n        )\n        after_else_comment = case1_else_clause.children().select(\n            select_if=sp.is_type(\"newline\", \"comment\", \"whitespace\"),\n            stop_seg=case1_else_expressions.get(),\n        )\n\n        # Delete the nested \"CASE\" expression.\n        fixes = case1_to_delete.apply(LintFix.delete)\n\n        tab_space_size: int = context.config.get(\"tab_space_size\", [\"indentation\"])\n        indent_unit: str = context.config.get(\"indent_unit\", [\"indentation\"])\n\n        # Determine the indentation to use when we move the nested \"WHEN\"\n        # and \"ELSE\" clauses, based on the indentation of case1_last_when.\n        # If no whitespace segments found, use default indent.\n        when_indent_str = self._get_indentation(\n            case1_children, case1_last_when, tab_space_size, indent_unit\n        )\n        # Again determine indentation, but matching the \"CASE\"/\"END\" level.\n        end_indent_str = self._get_indentation(\n            case1_children, case1_first_case, tab_space_size, indent_unit\n        )\n\n        # Move the nested \"when\" and \"else\" clauses after the last outer\n        # \"when\".\n        nested_clauses = case2.children(\n            sp.is_type(\"when_clause\", \"else_clause\", \"newline\", \"comment\", \"whitespace\")\n        )\n\n        # Rebuild the nested case statement.\n        # Any comments after the last outer \"WHEN\" that were deleted\n        segments = list(case1_comments_to_restore)\n        # Any comments between the \"ELSE\" and nested \"CASE\"\n        segments += self._rebuild_spacing(when_indent_str, after_else_comment)\n        # The nested \"WHEN\", \"ELSE\" or \"comments\", with logical spacing\n        segments += self._rebuild_spacing(when_indent_str, nested_clauses)\n        fixes.append(LintFix.create_after(case1_last_when, segments, source=segments))\n\n        # Delete the outer \"else\" clause.\n        fixes.append(LintFix.delete(case1_else_clause_seg))\n        # Add spacing for any comments that may exist after the nested `END`\n        # but only on that same line.\n        fixes += self._nested_end_trailing_comment(\n            case1_children, case1_else_clause_seg, end_indent_str\n        )\n        return LintResult(case2[0], fixes=fixes)\n", "type": "function"}, {"name": "_eval_gen", "is_method": true, "class_name": "Rule_CV12", "parameters": ["self", "context"], "calls": ["select_statement.is_type", "select_statement.get_child", "self._is_where_clause_simplifable", "set", "select_statement.recursive_crawl", "collections.deque", "enumerate", "where_clause.get_child", "self._get_subexpression_chunks", "self._get_from_expression_element_alias", "next", "encountered_references.add", "any", "join_clause.get_child", "is_type", "where_clause_fix_segments.popleft", "is_type", "where_clause_fix_segments.pop", "where_clause.get_child", "is_type", "is_type", "select_statement.recursive_crawl", "join_clause.recursive_crawl", "self._get_from_expression_element_alias", "set", "enumerate", "where_clause_fix_segments.extend", "where_clause_fix_segments.append", "LintResult", "LintResult", "LintResult", "collections.deque", "enumerate", "ExpressionSegment", "JoinOnConditionSegment", "JoinClauseSegment", "BinaryOperatorSegment", "all", "this_join_clause_subexpressions.add", "consumed_subexpressions.add", "LintResult", "is_type", "join_clause_fix_segments.popleft", "is_type", "join_clause_fix_segments.pop", "tuple", "LintResult", "seg.recursive_crawl", "len", "join_clause_fix_segments.extend", "join_clause_fix_segments.append", "KeywordSegment", "WhitespaceSegment", "WhitespaceSegment", "LintFix.replace", "LintFix.delete", "LintFix.delete", "col_ref.raw_upper.startswith", "BinaryOperatorSegment", "tuple", "LintFix.replace"], "code_location": {"file": "CV12.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "start_line": 67, "end_line": 250}, "code_snippet": "    def _eval_gen(self, context: RuleContext) -> Iterator[LintResult]:\n        # We are only interested in SELECT statement.\n        select_statement = context.segment\n        assert select_statement.is_type(\"select_statement\")\n\n        maybe_where_clause = select_statement.get_child(\"where_clause\")\n        if not maybe_where_clause:\n            return\n\n        where_clause = maybe_where_clause\n        where_clause_simplifable = self._is_where_clause_simplifable(where_clause)\n\n        if where_clause_simplifable:\n            expr = where_clause.get_child(\"expression\")\n            assert expr is not None\n            subexpressions = self._get_subexpression_chunks(expr)\n        else:\n            subexpressions = []\n        consumed_subexpressions = set()\n\n        # get references in from clause\n        select_table_references = [\n            *select_statement.recursive_crawl(\n                \"from_expression_element\",\n                no_recursive_seg_type=[\"join_clause\", \"select_statement\"],\n            )\n        ]\n\n        # track all seen references (from clause + all previous joins)\n        encountered_references = {\n            self._get_from_expression_element_alias(table_ref)\n            for table_ref in select_table_references\n        }\n\n        for join_clause in select_statement.recursive_crawl(\n            \"join_clause\", no_recursive_seg_type=[\"select_statement\"]\n        ):\n            # mark table reference as seen\n            join_table_reference = next(\n                join_clause.recursive_crawl(\n                    \"from_expression_element\",\n                    no_recursive_seg_type=[\"select_statement\"],\n                )\n            )\n            encountered_references.add(\n                self._get_from_expression_element_alias(join_table_reference)\n            )\n            join_clause_keywords = [\n                seg for seg in join_clause.segments if seg.type == \"keyword\"\n            ]\n\n            if any(\n                kw.raw_upper in (\"CROSS\", \"POSITIONAL\", \"USING\", \"APPLY\")\n                for kw in join_clause_keywords\n            ):\n                # If explicit CROSS JOIN is used, disregard lack of condition\n                # If explicit POSITIONAL JOIN is used, disregard lack of condition\n                # If explicit JOIN USING is used, disregard lack of condition\n                # If explicit CROSS/OUTER APPLY is used, disregard lack of condition\n                continue\n\n            this_join_condition = join_clause.get_child(\"join_on_condition\")\n            if this_join_condition:\n                # Join condition is present, no error reported.\n                continue\n\n            if not where_clause_simplifable:\n                yield LintResult(anchor=join_clause)\n            else:\n                this_join_clause_subexpressions = set()\n                for subexpr_idx, subexpr_segments in enumerate(subexpressions):\n                    if subexpr_idx in consumed_subexpressions:\n                        continue\n                    qualified_column_references = [\n                        col_ref\n                        for seg in subexpr_segments\n                        for col_ref in seg.recursive_crawl(\n                            \"column_reference\",\n                            no_recursive_seg_type=\"select_statement\",\n                        )\n                        if \"dot\" in col_ref.descendant_type_set\n                    ]\n                    if len(qualified_column_references) > 1 and all(\n                        col_ref.raw_upper.startswith(\n                            tuple(\n                                f\"{table_ref}.\" for table_ref in encountered_references\n                            )\n                        )\n                        for col_ref in qualified_column_references\n                    ):\n                        this_join_clause_subexpressions.add(subexpr_idx)\n                        consumed_subexpressions.add(subexpr_idx)\n\n                if not this_join_clause_subexpressions:\n                    yield LintResult(join_clause)\n                else:\n                    join_clause_fix_segments: Deque[BaseSegment] = collections.deque()\n                    for subexpr_idx, subexpr_segments in enumerate(subexpressions):\n                        if subexpr_idx in this_join_clause_subexpressions:\n                            join_clause_fix_segments.extend(subexpr_segments)\n                            join_clause_fix_segments.append(\n                                BinaryOperatorSegment(\"AND\")\n                            )\n\n                    while join_clause_fix_segments and join_clause_fix_segments[\n                        0\n                    ].is_type(\"whitespace\", \"binary_operator\"):\n                        join_clause_fix_segments.popleft()\n                    while join_clause_fix_segments and join_clause_fix_segments[\n                        -1\n                    ].is_type(\"whitespace\", \"binary_operator\"):\n                        join_clause_fix_segments.pop()\n\n                    join_on_expression = ExpressionSegment(\n                        tuple(join_clause_fix_segments),\n                    )\n                    join_on = JoinOnConditionSegment(\n                        (\n                            KeywordSegment(\"ON\"),\n                            WhitespaceSegment(),\n                            join_on_expression,\n                        )\n                    )\n                    join_clause_segment = JoinClauseSegment(\n                        (\n                            *join_clause.segments,\n                            WhitespaceSegment(),\n                            join_on,\n                        )\n                    )\n\n                    yield LintResult(\n                        anchor=join_clause,\n                        fixes=[\n                            LintFix.replace(\n                                join_clause,\n                                edit_segments=[join_clause_segment],\n                            )\n                        ],\n                    )\n\n        if not where_clause_simplifable:\n            return\n\n        if not consumed_subexpressions:\n            return\n\n        # Rewrite WHERE to keep conditions not moved to ON clauses\n        where_clause_fix_segments: Deque[BaseSegment] = collections.deque()\n        for subexpr_idx, subexpr_segments in enumerate(subexpressions):\n            if subexpr_idx not in consumed_subexpressions:\n                where_clause_fix_segments.extend(subexpr_segments)\n                where_clause_fix_segments.append(BinaryOperatorSegment(\"AND\"))\n\n        while where_clause_fix_segments and where_clause_fix_segments[0].is_type(\n            \"whitespace\", \"binary_operator\"\n        ):\n            where_clause_fix_segments.popleft()\n        while where_clause_fix_segments and where_clause_fix_segments[-1].is_type(\n            \"whitespace\", \"binary_operator\"\n        ):\n            where_clause_fix_segments.pop()\n\n        if where_clause_fix_segments:\n            where_clause_expr = where_clause.get_child(\"expression\")\n            assert where_clause_expr is not None\n            yield LintResult(\n                anchor=where_clause_expr,\n                fixes=[\n                    LintFix.replace(\n                        where_clause_expr, edit_segments=[*where_clause_fix_segments]\n                    )\n                ],\n            )\n        else:\n            assert select_statement.segments[-1].is_type(\"where_clause\")\n            assert select_statement.segments[-2].is_type(\"whitespace\", \"newline\")\n            yield LintResult(\n                anchor=where_clause,\n                fixes=[\n                    LintFix.delete(select_statement.segments[-2]),\n                    LintFix.delete(select_statement.segments[-1]),\n                ],\n            )\n", "type": "function"}, {"name": "_parse_tokens", "is_method": true, "class_name": "Linter", "parameters": ["tokens", "config", "fname", "parse_statistics"], "calls": ["Parser", "linter_logger.info", "linter_logger.info", "parsed.iter_unparsables", "parser.parse", "format", "violations.append", "linter_logger.info", "linter_logger.info", "tuple", "linter_logger.info", "violations.append", "parsed.stringify", "SQLParseError", "unparsable.stringify", "format", "len"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 231, "end_line": 281}, "code_snippet": "    def _parse_tokens(\n        tokens: Sequence[BaseSegment],\n        config: FluffConfig,\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> tuple[Optional[BaseSegment], list[SQLParseError]]:\n        parser = Parser(config=config)\n        violations = []\n        # Parse the file and log any problems\n        try:\n            parsed: Optional[BaseSegment] = parser.parse(\n                # Regardless of how the sequence was passed in, we should\n                # coerce it to a tuple here, before we head deeper into\n                # the parsing process.\n                tuple(tokens),\n                fname=fname,\n                parse_statistics=parse_statistics,\n            )\n        except SQLParseError as err:\n            linter_logger.info(\"PARSING FAILED! : %s\", err)\n            violations.append(err)\n            return None, violations\n\n        if parsed is None:  # pragma: no cover\n            return None, violations\n\n        linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n        linter_logger.info(\"\\n\" + parsed.stringify())\n        # We may succeed parsing, but still have unparsable segments. Extract them\n        # here.\n        for unparsable in parsed.iter_unparsables():\n            # No exception has been raised explicitly, but we still create one here\n            # so that we can use the common interface\n            assert unparsable.pos_marker\n            violations.append(\n                SQLParseError(\n                    \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n                    \"{1!r}\".format(\n                        unparsable.pos_marker.working_loc,\n                        (\n                            unparsable.raw\n                            if len(unparsable.raw) < 40\n                            else unparsable.raw[:40] + \"...\"\n                        ),\n                    ),\n                    segment=unparsable,\n                )\n            )\n            linter_logger.info(\"Found unparsable segment...\")\n            linter_logger.info(unparsable.stringify())\n        return parsed, violations\n", "type": "function"}, {"name": "TableExpressionSegment", "docstring": "The main table expression e.g. within a FROM clause.\n\nEnhance to allow for additional clauses allowed in Spark and Delta Lake.", "methods": [], "attributes": [], "code_location": {"file": "dialect_sparksql.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 3025, "end_line": 3055}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.0378429889678955}
{"question": "What are the core components of SQLFluff's parser?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's parser consists of several core components: 1) Lexer - separates SQL into segments of whitespace and code, producing typed segments (subclasses of RawSegment); 2) Parser - the most complex component that applies dialect-specific grammars to lexed segments, creating a tree-like structure with FileSegment as the root containing StatementSegments; 3) Grammar system - defines the shape of SQL statements using classes like Sequence, OneOf, Delimited, Bracketed, AnyNumberOf, AnySetOf, Ref, and Conditional; 4) Segment system - includes BaseSegment (abstract base class), RawSegment (raw tokens), and various specialized segments like KeywordSegment, IdentifierSegment, LiteralSegment, SymbolSegment, etc.; 5) Position markers - track location information for segments; 6) Parse context - manages parsing state and configuration. The parser uses a single-pass approach where segments recursively match based on their respective grammars until reaching raw segments with no children.", "score": null, "retrieved_content": [{"name": "_parse_tokens", "is_method": true, "class_name": "Linter", "parameters": ["tokens", "config", "fname", "parse_statistics"], "calls": ["Parser", "linter_logger.info", "linter_logger.info", "parsed.iter_unparsables", "parser.parse", "format", "violations.append", "linter_logger.info", "linter_logger.info", "tuple", "linter_logger.info", "violations.append", "parsed.stringify", "SQLParseError", "unparsable.stringify", "format", "len"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 231, "end_line": 281}, "code_snippet": "    def _parse_tokens(\n        tokens: Sequence[BaseSegment],\n        config: FluffConfig,\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> tuple[Optional[BaseSegment], list[SQLParseError]]:\n        parser = Parser(config=config)\n        violations = []\n        # Parse the file and log any problems\n        try:\n            parsed: Optional[BaseSegment] = parser.parse(\n                # Regardless of how the sequence was passed in, we should\n                # coerce it to a tuple here, before we head deeper into\n                # the parsing process.\n                tuple(tokens),\n                fname=fname,\n                parse_statistics=parse_statistics,\n            )\n        except SQLParseError as err:\n            linter_logger.info(\"PARSING FAILED! : %s\", err)\n            violations.append(err)\n            return None, violations\n\n        if parsed is None:  # pragma: no cover\n            return None, violations\n\n        linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n        linter_logger.info(\"\\n\" + parsed.stringify())\n        # We may succeed parsing, but still have unparsable segments. Extract them\n        # here.\n        for unparsable in parsed.iter_unparsables():\n            # No exception has been raised explicitly, but we still create one here\n            # so that we can use the common interface\n            assert unparsable.pos_marker\n            violations.append(\n                SQLParseError(\n                    \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n                    \"{1!r}\".format(\n                        unparsable.pos_marker.working_loc,\n                        (\n                            unparsable.raw\n                            if len(unparsable.raw) < 40\n                            else unparsable.raw[:40] + \"...\"\n                        ),\n                    ),\n                    segment=unparsable,\n                )\n            )\n            linter_logger.info(\"Found unparsable segment...\")\n            linter_logger.info(unparsable.stringify())\n        return parsed, violations\n", "type": "function"}, {"name": "parse", "is_method": false, "class_name": null, "parameters": ["sql", "dialect", "config", "config_path"], "calls": ["Linter", "linter.parse_string", "parsed.root_variant", "root_variant.tree.as_record", "get_simple_config", "APIParsingError"], "code_location": {"file": "simple.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/api", "start_line": 168, "end_line": 212}, "code_snippet": "def parse(\n    sql: str,\n    dialect: Optional[str] = None,\n    config: Optional[FluffConfig] = None,\n    config_path: Optional[str] = None,\n) -> dict[str, Any]:\n    \"\"\"Parse a SQL string.\n\n    Args:\n        sql (:obj:`str`): The SQL to be parsed.\n        dialect (:obj:`Optional[str]`, optional): A reference to the dialect of the SQL\n            to be parsed. Defaults to `ansi`.\n        config (:obj:`Optional[FluffConfig]`, optional): A configuration object\n            to use for the operation. Defaults to None.\n        config_path (:obj:`Optional[str]`, optional): A path to a .sqlfluff config,\n            which is only used if a `config` is not already provided.\n            Defaults to None.\n\n    Returns:\n        :obj:`Dict[str, Any]` JSON containing the parsed structure.\n\n    Note:\n        In the case of multiple potential variants from the raw source file\n        only the first variant is returned by the simple API. For access to\n        the other variants, use the underlying main API directly.\n    \"\"\"\n    cfg = config or get_simple_config(\n        dialect=dialect,\n        config_path=config_path,\n    )\n    linter = Linter(config=cfg)\n\n    parsed = linter.parse_string(sql)\n    # If we encounter any parsing errors, raise them in a combined issue.\n    violations = parsed.violations\n    if violations:\n        raise APIParsingError(violations)\n    # Return a JSON representation of the parse tree.\n    # NOTE: For the simple API - only a single variant is returned.\n    root_variant = parsed.root_variant()\n    assert root_variant, \"Files parsed without violations must have a valid variant\"\n    assert root_variant.tree, \"Files parsed without violations must have a valid tree\"\n    record = root_variant.tree.as_record(show_raw=True)\n    assert record\n    return record\n", "type": "function"}, {"name": "SQLParseError", "docstring": "An error which occurred during parsing.\n\nArgs:\n    segment (:obj:`BaseSegment`, optional): The segment which is relevant\n        for the failure in parsing. This is likely to be a subclass of\n        `BaseSegment` rather than the parent class itself. This is mostly\n        used for logging and for referencing position.", "methods": ["__init__", "__reduce__", "to_dict"], "attributes": ["_code", "_identifier"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 185, "end_line": 246}, "type": "class"}, {"name": "SQLFluffDomain", "docstring": "SQLFluff domain.", "methods": ["get_full_qualified_name", "get_objects", "resolve_xref", "add_rule"], "attributes": ["name", "label", "object_types", "roles", "directives", "initial_data"], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 82, "end_line": 142}, "type": "class"}, {"name": "cli", "is_method": false, "class_name": null, "parameters": [], "calls": ["click.group", "click.version_option"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 480, "end_line": 481}, "code_snippet": "def cli() -> None:\n    \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n", "type": "function"}, {"name": "test__api__linter_lint", "is_method": false, "class_name": null, "parameters": [], "calls": ["lex", "parse", "lint", "Lexer", "Parser", "Linter"], "code_location": {"file": "classes_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 24, "end_line": 29}, "code_snippet": "def test__api__linter_lint():\n    \"\"\"Basic checking of parsing functionality.\"\"\"\n    tokens, _ = Lexer(dialect=\"ansi\").lex(test_query)\n    parsed = Parser(dialect=\"ansi\").parse(tokens)\n    violations = Linter(dialect=\"ansi\").lint(parsed)\n    assert [v.rule.code for v in violations] == [\"CP01\", \"LT12\"]\n", "type": "function"}, {"name": "ParsedVariant", "docstring": "An object to store the result of parsing a single TemplatedFile.\n\nArgs:\n    templated_file (:obj:`TemplatedFile`): Containing the details\n        of the templated file. If templating fails, this will be `None`.\n    tree (:obj:`BaseSegment`): The segment structure representing the\n        parsed file. If parsing fails due to an unrecoverable\n        violation then we will be None.\n    lexing_violations (:obj:`list` of :obj:`SQLLexError`): Any violations\n        raised during the lexing phase.\n    parsing_violations (:obj:`list` of :obj:`SQLParseError`): Any violations\n        raised during the lexing phase.", "methods": ["violations"], "attributes": [], "code_location": {"file": "common.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 42, "end_line": 64}, "type": "class"}, {"name": "SelectStatementSegment", "docstring": "Overrides ANSI as the parse grammar copy needs to be reapplied.\n\nAs per https://www.postgresql.org/docs/current/sql-select.html", "methods": [], "attributes": [], "code_location": {"file": "dialect_postgres.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/dialects", "start_line": 1799, "end_line": 1825}, "type": "class"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}, {"name": "Parser", "docstring": "Instantiates parsed queries from a sequence of lexed raw segments.", "methods": ["__init__", "parse"], "attributes": [], "code_location": {"file": "parser.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 14, "end_line": 79}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.0538415908813477}
{"question": "What is the purpose of the BaseRule class in SQLFluff?", "answer": null, "relative_code_list": null, "ground_truth": "The BaseRule class serves as the foundational abstract base class for all SQLFluff linting rules. Its key purposes include: 1) Rule definition framework - provides the base structure and interface that all rules must implement, including the essential _eval() method that evaluates rules against parse tree segments; 2) Configuration management - handles rule-specific configuration through config_keywords list, validates configuration options during initialization, and provides access to configuration values as class attributes; 3) Metadata management - manages rule metadata including code (unique identifier), description (human-readable explanation), name, groups (categorization), and aliases (backward compatibility); 4) Crawling behavior - defines crawl_behaviour attribute that controls how rules traverse the parse tree (e.g., RootOnlyCrawler, SegmentSeekerCrawler); 5) Linting phases - supports different linting phases (main, post) for rules that need to run at specific times; 6) Template awareness - provides flags like targets_templated and template_safe_fixes to control how rules interact with templated code; 7) Error handling - includes _works_on_unparsable flag to control whether rules should process unparsable segments; 8) Logging integration - provides custom logging with rule code context; 9) Fix compatibility - supports automatic code fixing through is_fix_compatible flag and LintFix objects; 10) Metaclass integration - uses RuleMetaclass to automatically populate documentation, code, and description from class names and docstrings.", "score": null, "retrieved_content": [{"name": "BaseRule", "docstring": "The base class for a rule.\n\nArgs:\n    code (:obj:`str`): The identifier for this rule, used in inclusion\n        or exclusion.\n    description (:obj:`str`): A human readable description of what this\n        rule does. It will be displayed when any violations are found.", "methods": ["__init__", "get_config_ref", "_eval", "crawl", "_log_critical_errors", "_process_lint_result", "filter_meta", "get_parent_of", "discard_unsafe_fixes", "_adjust_anchors_for_fixes", "_choose_anchor_segment"], "attributes": ["_check_docstring", "_works_on_unparsable", "_adjust_anchors", "targets_templated", "template_safe_fixes", "lint_phase", "is_fix_compatible", "split_comma_separated_string"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 364, "end_line": 834}, "type": "class"}, {"name": "RuleMetaclass", "docstring": "The metaclass for rules.\n\nThis metaclass provides provides auto-enrichment of the\nrule docstring so that examples, groups, aliases and\nnames are added.\n\nThe reason we enrich the docstring is so that it can be\npicked up by autodoc and all be displayed in the sqlfluff\ndocs.", "methods": ["_populate_code_and_description", "_populate_docstring", "__new__"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 151, "end_line": 361}, "type": "class"}, {"name": "SQLFluffRule", "docstring": "SQLFluff rule directive for sphinx.\n\nRule directives can be used as shown below.\n\n.. code-block:: rst\n\n    .. sqlfluff:rule:: AM01\n                       ambiguous.distinct\n\n        Write the documentation for the rule here.\n\nTo cross reference (i.e. refer to) objects defined like this\nboth the code and name reference is available:\n\n.. code-block:: rst\n\n    :sqlfluff:ref:`CP02`\n    :sqlfluff:ref:`capitalisation.identifiers`", "methods": ["handle_signature", "add_target_and_index", "_object_hierarchy_parts", "_toc_entry_name"], "attributes": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 10, "end_line": 79}, "type": "class"}, {"name": "SQLLintError", "docstring": "An error which occurred during linting.\n\nIn particular we reference the rule here to do extended logging based on\nthe rule in question which caused the fail.\n\nArgs:\n    segment (:obj:`BaseSegment`, optional): The segment which is relevant\n        for the failure in parsing. This is likely to be a subclass of\n        `BaseSegment` rather than the parent class itself. This is mostly\n        used for logging and for referencing position.", "methods": ["__init__", "__reduce__", "to_dict", "fixable", "rule_code", "rule_name", "source_signature", "__repr__"], "attributes": ["_identifier"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 249, "end_line": 383}, "type": "class"}, {"name": "BaseSegment", "docstring": "The base segment element.\n\nThis defines the base element which drives both Lexing, Parsing and Linting.\nA large chunk of the logic which defines those three operations are centered\nhere. Much of what is defined in the BaseSegment is also used by its many\nsubclasses rather than directly here.\n\nFor clarity, the `BaseSegment` is mostly centered around a segment which contains\nother subsegments. For segments which don't have *children*, refer to the\n`RawSegment` class (which still inherits from this one).\n\nSegments are used both as instances to hold chunks of text, but also as classes\nthemselves where they function a lot like grammars, and return instances of\nthemselves when they match. The many classmethods in this class are usually to serve\ntheir purpose as a matcher.", "methods": ["__init__", "__setattr__", "__eq__", "_hash", "__hash__", "__repr__", "__getstate__", "__setstate__", "_comments", "_non_comments", "is_code", "_code_indices", "is_comment", "is_whitespace", "raw", "class_types", "descendant_type_set", "direct_descendant_type_set", "raw_upper", "raw_segments", "raw_segments_with_ancestors", "source_fixes", "first_non_whitespace_segment_raw_upper", "is_templated", "_suffix", "_position_segments", "simple", "cache_key", "is_optional", "class_is_type", "structural_simplify", "match", "_recalculate_caches", "_preface", "set_as_parent", "set_parent", "get_parent", "get_type", "count_segments", "is_type", "invalidate_caches", "get_start_point_marker", "get_end_point_marker", "get_start_loc", "get_end_loc", "stringify", "to_tuple", "copy", "as_record", "get_raw_segments", "raw_normalized", "iter_segments", "iter_unparsables", "type_set", "is_raw", "get_child", "get_children", "select_children", "recursive_crawl_all", "recursive_crawl", "path_to", "_is_code_or_meta", "validate_non_code_ends", "validate_segment_with_reparse", "_log_apply_fixes_check_issue", "edit", "from_result_segments"], "attributes": ["comment_separate", "is_meta", "can_start_end_non_code", "allow_empty"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 141, "end_line": 1239}, "type": "class"}, {"name": "SQLBaseError", "docstring": "Base Error Class for all violations.", "methods": ["__init__", "__eq__", "__reduce__", "fixable", "rule_code", "rule_name", "desc", "to_dict", "check_tuple", "source_signature", "ignore_if_in", "warning_if_in"], "attributes": ["_identifier", "_warning"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 36, "end_line": 150}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "BaseRule", "parameters": ["self", "code", "description"], "calls": ["kwargs.items", "RuleLoggingAdapter", "kwargs.keys", "ValueError", "format"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 421, "end_line": 441}, "code_snippet": "    def __init__(self, code: str, description: str, **kwargs: Any) -> None:\n        self.description = description\n        self.code = code\n        # kwargs represents the config passed to the rule. Add all kwargs as class\n        # attributes so they can be accessed in rules which inherit from this class\n        for key, value in kwargs.items():\n            self.__dict__[key] = value\n\n        # We also define a custom logger here, which also includes the code\n        # of the rule in the logging.\n        self.logger = RuleLoggingAdapter(rules_logger, {\"code\": code})\n        # Validate that declared configuration options exist\n        for keyword in self.config_keywords:\n            if keyword not in kwargs.keys():\n                raise ValueError(\n                    (\n                        \"Unrecognized config '{}' for Rule {}. If this \"\n                        \"is a new option, please add it to \"\n                        \"`default_config.cfg` or plugin specific config.\"\n                    ).format(keyword, code)\n                )\n", "type": "function"}, {"name": "RulePack", "docstring": "A bundle of rules to be applied.\n\nThis contains a set of rules, post filtering but also contains the mapping\nrequired to interpret any noqa messages found in files.\n\nThe reason for this object is that rules are filtered and instantiated\ninto this pack in the main process when running in multi-processing mode so\nthat user defined rules can be used without reference issues.\n\nAttributes:\n    rules (:obj:`list` of :obj:`BaseRule`): A filtered list of instantiated\n        rules to be applied to a given file.\n    reference_map (:obj:`dict`): A mapping of rule references to the codes\n        they refer to, e.g. `{\"my_ref\": {\"LT01\", \"LT02\"}}`. The references\n        (i.e. the keys) may be codes, groups, aliases or names. The values\n        of the mapping are sets of rule codes *only*. This object acts as\n        a lookup to be able to translate selectors (which may contain\n        diverse references) into a consolidated list of rule codes. This\n        mapping contains the full set of rules, rather than just the filtered\n        set present in the `rules` attribute.", "methods": ["codes"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 850, "end_line": 878}, "type": "class"}, {"name": "RuleSet", "docstring": "Class to define a ruleset.\n\nA rule set is instantiated on module load, but the references\nto each of its classes are instantiated at runtime. This means\nthat configuration values can be passed to those rules live\nand be responsive to any changes in configuration from the\npath that the file is in.\n\nRules should be fetched using the :meth:`get_rulelist` command which\nalso handles any filtering (i.e. allowlisting and denylisting).\n\nNew rules should be added to the instance of this class using the\n:meth:`register` decorator. That decorator registers the class, but also\nperforms basic type and name-convention checks.\n\nThe code for the rule will be parsed from the name, the description\nfrom the docstring. The eval function is assumed that it will be\noverridden by the subclass, and the parent class raises an error on\nthis function if not overridden.", "methods": ["__init__", "_validate_config_options", "register", "_expand_rule_refs", "rule_reference_map", "get_rulepack", "copy"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 881, "end_line": 1213}, "type": "class"}, {"name": "RuleTuple", "docstring": "Rule Tuple object for describing rules.", "methods": [], "attributes": [], "code_location": {"file": "common.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 16, "end_line": 23}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.073904037475586}
{"question": "What is the purpose of the RuleSet class in SQLFluff's rule management?", "answer": null, "relative_code_list": null, "ground_truth": "The RuleSet class serves as the central registry and management system for SQLFluff's linting rules. Its key purposes include: 1) Rule registration - provides a decorator-based registration system (@ruleset.register) that adds rule classes to the ruleset and performs validation checks; 2) Rule storage - maintains an internal _register dictionary that maps rule codes to RuleManifest objects containing rule metadata (code, name, description, groups, aliases, rule_class); 3) Configuration validation - validates rule configuration options against predefined validation rules through _validate_config_options() method; 4) Rule filtering - handles rule allowlisting and denylisting through the get_rulelist() method that filters rules based on configuration settings; 5) Runtime instantiation - rules are registered as classes at module load time but instantiated at runtime, allowing configuration values to be passed dynamically and respond to path-specific configuration changes; 6) Naming convention enforcement - enforces the Rule_XXXX naming convention where XXXX follows the LNNN pattern (letter + three digits); 7) Code collision prevention - prevents duplicate rule codes from being registered; 8) Group validation - ensures all rules belong to the 'all' group as a requirement; 9) Plugin integration - supports plugin-based rule registration through optional plugin parameter; 10) Configuration integration - works with FluffConfig to provide rule-specific configuration sections and validation.", "score": null, "retrieved_content": [{"name": "get_rule_from_set", "is_method": false, "class_name": null, "parameters": ["code", "config"], "calls": ["ValueError", "get_rulepack", "get_ruleset", "get_ruleset"], "code_location": {"file": "rules.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/testing", "start_line": 82, "end_line": 87}, "code_snippet": "def get_rule_from_set(code: str, config: FluffConfig) -> BaseRule:\n    \"\"\"Fetch a rule from the rule set.\"\"\"\n    for r in get_ruleset().get_rulepack(config=config).rules:\n        if r.code == code:  # pragma: no cover\n            return r\n    raise ValueError(f\"{code!r} not in {get_ruleset()!r}\")\n", "type": "function"}, {"name": "RuleSet", "docstring": "Class to define a ruleset.\n\nA rule set is instantiated on module load, but the references\nto each of its classes are instantiated at runtime. This means\nthat configuration values can be passed to those rules live\nand be responsive to any changes in configuration from the\npath that the file is in.\n\nRules should be fetched using the :meth:`get_rulelist` command which\nalso handles any filtering (i.e. allowlisting and denylisting).\n\nNew rules should be added to the instance of this class using the\n:meth:`register` decorator. That decorator registers the class, but also\nperforms basic type and name-convention checks.\n\nThe code for the rule will be parsed from the name, the description\nfrom the docstring. The eval function is assumed that it will be\noverridden by the subclass, and the parent class raises an error on\nthis function if not overridden.", "methods": ["__init__", "_validate_config_options", "register", "_expand_rule_refs", "rule_reference_map", "get_rulepack", "copy"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 881, "end_line": 1213}, "type": "class"}, {"name": "RuleMetaclass", "docstring": "The metaclass for rules.\n\nThis metaclass provides provides auto-enrichment of the\nrule docstring so that examples, groups, aliases and\nnames are added.\n\nThe reason we enrich the docstring is so that it can be\npicked up by autodoc and all be displayed in the sqlfluff\ndocs.", "methods": ["_populate_code_and_description", "_populate_docstring", "__new__"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 151, "end_line": 361}, "type": "class"}, {"name": "get_rulepack", "is_method": true, "class_name": "Linter", "parameters": ["self", "config"], "calls": ["get_ruleset", "rs.get_rulepack", "rs.register"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 100, "end_line": 107}, "code_snippet": "    def get_rulepack(self, config: Optional[FluffConfig] = None) -> RulePack:\n        \"\"\"Get hold of a set of rules.\"\"\"\n        rs = get_ruleset()\n        # Register any user rules\n        for rule in self.user_rules:\n            rs.register(rule)\n        cfg = config or self.config\n        return rs.get_rulepack(config=cfg)\n", "type": "function"}, {"name": "SQLFluffRule", "docstring": "SQLFluff rule directive for sphinx.\n\nRule directives can be used as shown below.\n\n.. code-block:: rst\n\n    .. sqlfluff:rule:: AM01\n                       ambiguous.distinct\n\n        Write the documentation for the rule here.\n\nTo cross reference (i.e. refer to) objects defined like this\nboth the code and name reference is available:\n\n.. code-block:: rst\n\n    :sqlfluff:ref:`CP02`\n    :sqlfluff:ref:`capitalisation.identifiers`", "methods": ["handle_signature", "add_target_and_index", "_object_hierarchy_parts", "_toc_entry_name"], "attributes": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 10, "end_line": 79}, "type": "class"}, {"name": "rule_tuples", "is_method": true, "class_name": "Linter", "parameters": ["self"], "calls": ["self.get_rulepack", "RuleTuple"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 109, "end_line": 115}, "code_snippet": "    def rule_tuples(self) -> list[RuleTuple]:\n        \"\"\"A simple pass through to access the rule tuples of the rule set.\"\"\"\n        rs = self.get_rulepack()\n        return [\n            RuleTuple(rule.code, rule.name, rule.description, rule.groups, rule.aliases)\n            for rule in rs.rules\n        ]\n", "type": "function"}, {"name": "RulePack", "docstring": "A bundle of rules to be applied.\n\nThis contains a set of rules, post filtering but also contains the mapping\nrequired to interpret any noqa messages found in files.\n\nThe reason for this object is that rules are filtered and instantiated\ninto this pack in the main process when running in multi-processing mode so\nthat user defined rules can be used without reference issues.\n\nAttributes:\n    rules (:obj:`list` of :obj:`BaseRule`): A filtered list of instantiated\n        rules to be applied to a given file.\n    reference_map (:obj:`dict`): A mapping of rule references to the codes\n        they refer to, e.g. `{\"my_ref\": {\"LT01\", \"LT02\"}}`. The references\n        (i.e. the keys) may be codes, groups, aliases or names. The values\n        of the mapping are sets of rule codes *only*. This object acts as\n        a lookup to be able to translate selectors (which may contain\n        diverse references) into a consolidated list of rule codes. This\n        mapping contains the full set of rules, rather than just the filtered\n        set present in the `rules` attribute.", "methods": ["codes"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 850, "end_line": 878}, "type": "class"}, {"name": "_load_standard_rules", "is_method": false, "class_name": null, "parameters": [], "calls": ["RuleSet", "hook.get_rules", "get_config_info", "std_rule_set.register", "get_plugin_manager"], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 17, "end_line": 30}, "code_snippet": "def _load_standard_rules() -> RuleSet:\n    \"\"\"Initialise the standard ruleset.\n\n    We do this on each call so that dynamic rules changes\n    are possible.\n    \"\"\"\n    std_rule_set = RuleSet(name=\"standard\", config_info=get_config_info())\n\n    # Iterate through the rules list and register each rule with the standard set.\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            std_rule_set.register(rule)\n\n    return std_rule_set\n", "type": "function"}, {"name": "test_rules_name_validation", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.raises"], "code_location": {"file": "rules_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 326, "end_line": 336}, "code_snippet": "def test_rules_name_validation():\n    \"\"\"Ensure that rule names are validated.\"\"\"\n    with pytest.raises(SQLFluffUserError) as exc_info:\n\n        class RuleWithoutBadName_ZZ99(BaseRule):\n            \"\"\"A new rule without configuration.\"\"\"\n\n            name = \"MY-KEBAB-CASE-NAME\"\n\n    assert \"Tried to define rule with unexpected name\" in exc_info.value.args[0]\n    assert \"MY-KEBAB-CASE-NAME\" in exc_info.value.args[0]\n", "type": "function"}, {"name": "RuleTuple", "docstring": "Rule Tuple object for describing rules.", "methods": [], "attributes": [], "code_location": {"file": "common.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 16, "end_line": 23}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.0706374645233154}
{"question": "What dependencies exist between the Linter class and the Parser and Rule classes?", "answer": null, "relative_code_list": null, "ground_truth": "The Linter class has several key dependencies on both Parser and Rule classes, forming the core of SQLFluff's linting pipeline. Key dependencies include: 1) Parser dependency - Linter imports and uses Parser class for creating parse trees from lexed segments, calling parser.parse() to convert raw segments into structured parse trees; 2) Rule system dependency - Linter depends on BaseRule class and RulePack for rule execution, using get_rulepack() to obtain filtered rule sets and executing rules against parse trees; 3) Import dependencies - Linter imports from sqlfluff.core.parser (Parser, Lexer) and sqlfluff.core.rules (BaseRule, RulePack, get_ruleset) to access these core components; 4) Configuration integration - Linter uses FluffConfig to configure both Parser and Rule behavior, passing configuration to both components; 5) Error handling - Linter processes errors from both Parser (SQLParseError) and Rules (SQLLintError), aggregating them into unified violation reports; 6) Pipeline coordination - Linter orchestrates the flow from parsing (via Parser) to rule evaluation (via BaseRule instances), managing the complete linting workflow; 7) Fix system integration - Linter coordinates with both Parser and Rules for automatic code fixing, applying LintFix objects generated by rules; 8) Template handling - Linter works with both Parser and Rules to handle templated SQL, ensuring proper source mapping between raw and rendered code; 9) Segment processing - Linter depends on BaseSegment from parser for tree traversal and rule evaluation; 10) Architecture layering - According to pyproject.toml dependency layers, linter references many things including rules, while rules should be independent from linter but can reference parser components.", "score": null, "retrieved_content": [{"name": "Linter", "docstring": "The interface class to interact with the linter.", "methods": ["__init__", "get_rulepack", "rule_tuples", "load_raw_file_and_config", "_normalise_newlines", "_lex_templated_file", "_parse_tokens", "remove_templated_errors", "_report_conflicting_fixes_same_anchor", "_warn_unfixable", "parse_rendered", "lint_fix_parsed", "lint_parsed", "allowed_rule_ref_map", "lint_rendered", "render_string", "render_file", "parse_string", "fix", "lint", "lint_string", "lint_string_wrapped", "lint_path", "lint_paths", "parse_path"], "attributes": ["allow_process_parallelism"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 57, "end_line": 1161}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "BaseRunner", "parameters": ["self", "linter", "config"], "calls": [], "code_location": {"file": "runner.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 37, "end_line": 43}, "code_snippet": "    def __init__(\n        self,\n        linter: Linter,\n        config: FluffConfig,\n    ) -> None:\n        self.linter = linter\n        self.config = config\n", "type": "function"}, {"name": "SQLLintError", "docstring": "An error which occurred during linting.\n\nIn particular we reference the rule here to do extended logging based on\nthe rule in question which caused the fail.\n\nArgs:\n    segment (:obj:`BaseSegment`, optional): The segment which is relevant\n        for the failure in parsing. This is likely to be a subclass of\n        `BaseSegment` rather than the parent class itself. This is mostly\n        used for logging and for referencing position.", "methods": ["__init__", "__reduce__", "to_dict", "fixable", "rule_code", "rule_name", "source_signature", "__repr__"], "attributes": ["_identifier"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 249, "end_line": 383}, "type": "class"}, {"name": "RuleFailure", "docstring": "Exception class for reporting lint failure inside deeply nested code.", "methods": ["__init__"], "attributes": [], "code_location": {"file": "AM04.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/ambiguous", "start_line": 13, "end_line": 17}, "type": "class"}, {"name": "test__linter__parse_fail", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "lntr.parse_string", "any", "len", "root_variant.tree.type_set", "isinstance"], "code_location": {"file": "linter_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 419, "end_line": 439}, "code_snippet": "def test__linter__parse_fail():\n    \"\"\"Test linter behaves as expected with an unparsable string.\n\n    Much of this test is about making sure that ParsedString is\n    instantiated appropriately.\n    \"\"\"\n    lntr = Linter(dialect=\"ansi\")\n    # Try and parse something which obviously isn't SQL\n    parsed = lntr.parse_string(\"THIS IS NOT SQL\")\n    # There should still be a parsed variant\n    assert parsed.parsed_variants\n    assert len(parsed.parsed_variants) == 1\n    root_variant = parsed.parsed_variants[0]\n    # That root variant should still have a templated file and a parsed tree...\n    assert root_variant.templated_file\n    assert root_variant.tree\n    # ...but that tree should contain an unparsable segment.\n    assert \"unparsable\" in root_variant.tree.type_set()\n    # There *should* be violations because there should be a parsing fail.\n    assert parsed.violations\n    assert any(isinstance(v, SQLParseError) for v in parsed.violations)\n", "type": "function"}, {"name": "test__rules__result_unparsable", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "any", "fluff_log_catcher", "linter.lint_string", "v.rule_code"], "code_location": {"file": "rules_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 214, "end_line": 229}, "code_snippet": "def test__rules__result_unparsable():\n    \"\"\"Test that the linter won't allow rules which make the file unparsable.\"\"\"\n    # Set up a linter with the user rule\n    linter = Linter(user_rules=[Rule_T003], dialect=\"ansi\", rules=[\"T003\"])\n    # Lint a simple parsable file and check we do get issues\n    # It's parsable, so we should get issues.\n    raw_sql = \"SELECT 1 FROM a\"\n    with fluff_log_catcher(logging.WARNING, \"sqlfluff\") as caplog:\n        res = linter.lint_string(raw_sql, fix=True)\n    # Check we got the warning.\n    assert \"would result in an unparsable file\" in caplog.text\n    # Check we get the violation.\n    assert any(v.rule_code() == \"T003\" for v in res.violations)\n    # The resulting file should be _the same_ because it would have resulted\n    # in an unparsable file if applied.\n    assert res.tree.raw == raw_sql\n", "type": "function"}, {"name": "LintResult", "docstring": "A class to hold the results of a rule evaluation.\n\nArgs:\n    anchor (:obj:`BaseSegment`, optional): A segment which represents\n        the *position* of the problem. NB: Each fix will also hold\n        its own reference to position, so this position is mostly for\n        alerting the user to where the *problem* is.\n    fixes (:obj:`list` of :obj:`LintFix`, optional): An array of any\n        fixes which would correct this issue. If not present then it's\n        assumed that this issue will have to manually fixed.\n    memory (:obj:`dict`, optional): An object which stores any working\n        memory for the rule. The `memory` returned in any `LintResult`\n        will be passed as an input to the next segment to be crawled.\n    description (:obj:`str`, optional): A description of the problem\n        identified as part of this result. This will override the\n        description of the rule as what gets reported to the user\n        with the problem if provided.\n    source (:obj:`str`, optional): A string identifier for what\n        generated the result. Within larger libraries like reflow this\n        can be useful for tracking where a result came from.", "methods": ["__init__", "__repr__", "to_linting_error"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 76, "end_line": 145}, "type": "class"}, {"name": "LintedFile", "docstring": "A class to store the idea of a linted file.", "methods": ["check_tuples", "deduplicate_in_source_space", "get_violations", "num_violations", "is_clean", "fix_string", "_slice_source_file_using_patches", "_build_up_fixed_source_string", "persist_tree", "_safe_create_replace_file"], "attributes": [], "code_location": {"file": "linted_file.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 66, "end_line": 446}, "type": "class"}, {"name": "test__rules__filter_unparsable", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "linter.lint_string", "any", "linter.lint_string", "any", "v.rule_code", "v.rule_code"], "code_location": {"file": "rules_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 200, "end_line": 211}, "code_snippet": "def test__rules__filter_unparsable():\n    \"\"\"Test that rules that handle their own crawling respect unparsable.\"\"\"\n    # Set up a linter with the user rule\n    linter = Linter(user_rules=[Rule_T002], dialect=\"ansi\", rules=[\"T002\"])\n    # Lint a simple parsable file and check we do get issues\n    # It's parsable, so we should get issues.\n    res = linter.lint_string(\"SELECT 1\")\n    assert any(v.rule_code() == \"T002\" for v in res.violations)\n    # Lint an unparsable file. Check we don't get any violations.\n    # It's not parsable so we shouldn't get issues.\n    res = linter.lint_string(\"asd asdf sdfg\")\n    assert not any(v.rule_code() == \"T002\" for v in res.violations)\n", "type": "function"}, {"name": "LintingResult", "docstring": "A class to represent the result of a linting operation.\n\nNotably this might be a collection of paths, all with multiple\npotential files within them.", "methods": ["__init__", "add", "stop_timer", "check_tuples", "check_tuples_by_path", "num_violations", "get_violations", "stats", "timing_summary", "persist_timing_records", "as_records", "persist_changes", "tree", "count_tmp_prs_errors", "discard_fixes_for_lint_errors_in_files_with_tmp_or_prs_errors"], "attributes": [], "code_location": {"file": "linting_result.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 34, "end_line": 224}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.068221092224121}
{"question": "Why does SQLFluff provide a fix generation system for automatic code correction?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff provides a fix generation system for automatic code correction to enhance developer productivity and ensure consistent SQL formatting. Key reasons include: 1) Developer productivity - Automatically fixing common formatting issues saves developers time and reduces manual formatting work; 2) Consistency enforcement - Automatic fixes ensure consistent SQL formatting across teams and projects without requiring manual intervention; 3) Error reduction - Automated fixes reduce the chance of human error in manual formatting; 4) Immediate feedback - Developers can see and apply fixes immediately during development rather than waiting for code reviews; 5) CI/CD integration - Automatic fixes can be integrated into continuous integration pipelines to maintain code quality; 6) Learning tool - The fix system helps developers learn proper SQL formatting by showing them what changes are needed; 7) Rule compliance - Fixes ensure that SQL code complies with configured linting rules automatically; 8) Batch processing - Multiple files can be automatically fixed in a single operation; 9) Selective fixing - Developers can choose which rules to apply fixes for, allowing for gradual adoption; 10) Quality assurance - Automatic fixes help maintain high code quality standards across large codebases.", "score": null, "retrieved_content": [{"name": "test__fix__generate_source_patches", "is_method": false, "class_name": null, "parameters": ["tree", "templated_file", "expected_result", "caplog"], "calls": ["pytest.mark.parametrize", "caplog.at_level", "generate_source_patches", "RawSegment", "RawSegment", "BaseSegment", "BaseSegment", "BaseSegment", "PositionMarker", "PositionMarker", "FixPatch", "FixPatch", "FixPatch", "FixPatch", "FixPatch", "FixPatch", "slice", "slice", "slice", "slice", "slice", "slice", "RawSegment", "RawSegment", "RawSegment", "slice", "slice", "TemplateSegment", "RawSegment", "RawSegment", "RawSegment", "slice", "slice", "TemplateSegment", "RawSegment", "RawSegment", "RawSegment", "slice", "slice", "slice", "slice", "slice", "slice", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "SourceFix", "SourceFix", "slice", "slice", "slice", "slice"], "code_location": {"file": "fix_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 195, "end_line": 202}, "code_snippet": "def test__fix__generate_source_patches(tree, templated_file, expected_result, caplog):\n    \"\"\"Test generate_source_patches.\n\n    This is part of fix_string().\n    \"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.linter\"):\n        result = generate_source_patches(tree, templated_file)\n    assert result == expected_result\n", "type": "function"}, {"name": "fix", "is_method": false, "class_name": null, "parameters": ["sql", "dialect", "rules", "exclude_rules", "config", "config_path", "fix_even_unparsable"], "calls": ["Linter", "linter.lint_string_wrapped", "get_simple_config", "cfg.get", "result.count_tmp_prs_errors", "fix_string"], "code_location": {"file": "simple.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/api", "start_line": 115, "end_line": 165}, "code_snippet": "def fix(\n    sql: str,\n    dialect: Optional[str] = None,\n    rules: Optional[list[str]] = None,\n    exclude_rules: Optional[list[str]] = None,\n    config: Optional[FluffConfig] = None,\n    config_path: Optional[str] = None,\n    fix_even_unparsable: Optional[bool] = None,\n) -> str:\n    \"\"\"Fix a SQL string.\n\n    Args:\n        sql (:obj:`str`): The SQL to be fixed.\n        dialect (:obj:`Optional[str]`, optional): A reference to the dialect of the SQL\n            to be fixed. Defaults to `ansi`.\n        rules (:obj:`Optional[list[str]`, optional): A subset of rule\n            references to fix for. Defaults to None.\n        exclude_rules (:obj:`Optional[list[str]`, optional): A subset of rule\n            references to avoid fixing for. Defaults to None.\n        config (:obj:`Optional[FluffConfig]`, optional): A configuration object\n            to use for the operation. Defaults to None.\n        config_path (:obj:`Optional[str]`, optional): A path to a .sqlfluff config,\n            which is only used if a `config` is not already provided.\n            Defaults to None.\n        fix_even_unparsable (:obj:`bool`, optional): Optional override for the\n            corresponding SQLFluff configuration value.\n\n    Returns:\n        :obj:`str` for the fixed SQL if possible.\n    \"\"\"\n    cfg = config or get_simple_config(\n        dialect=dialect,\n        rules=rules,\n        exclude_rules=exclude_rules,\n        config_path=config_path,\n    )\n    linter = Linter(config=cfg)\n\n    result = linter.lint_string_wrapped(sql, fix=True)\n    if fix_even_unparsable is None:\n        fix_even_unparsable = cfg.get(\"fix_even_unparsable\")\n    should_fix = True\n    if not fix_even_unparsable:\n        # If fix_even_unparsable wasn't set, check for templating or parse\n        # errors and suppress fixing if there were any.\n        _, num_filtered_errors = result.count_tmp_prs_errors()\n        if num_filtered_errors > 0:\n            should_fix = False\n    if should_fix:\n        sql = result.paths[0].files[0].fix_string()[0]\n    return sql\n", "type": "function"}, {"name": "test__cli__fix_multiple_errors_no_show_errors", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 2292, "end_line": 2305}, "code_snippet": "def test__cli__fix_multiple_errors_no_show_errors():\n    \"\"\"Test the fix output.\"\"\"\n    invoke_assert_code(\n        ret_code=1,\n        args=[\n            fix,\n            [\n                \"--check\",  # Run in check mode to get the confirmation.\n                \"--disable-progress-bar\",\n                \"test/fixtures/linter/multiple_sql_errors.sql\",\n            ],\n        ],\n        assert_stdout_contains=multiple_expected_output,\n    )\n", "type": "function"}, {"name": "fix", "is_method": true, "class_name": "Linter", "parameters": ["self", "tree", "config", "fname", "templated_file"], "calls": ["self.get_rulepack", "self.lint_fix_parsed"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 931, "end_line": 950}, "code_snippet": "    def fix(\n        self,\n        tree: BaseSegment,\n        config: Optional[FluffConfig] = None,\n        fname: Optional[str] = None,\n        templated_file: Optional[\"TemplatedFile\"] = None,\n    ) -> tuple[BaseSegment, list[SQLBaseError]]:\n        \"\"\"Return the fixed tree and violations from lintfix when we're fixing.\"\"\"\n        config = config or self.config\n        rule_pack = self.get_rulepack(config=config)\n        fixed_tree, violations, _, _ = self.lint_fix_parsed(\n            tree,\n            config,\n            rule_pack,\n            fix=True,\n            fname=fname,\n            templated_file=templated_file,\n            formatter=self.formatter,\n        )\n        return fixed_tree, violations\n", "type": "function"}, {"name": "apply_fixes", "is_method": false, "class_name": null, "parameters": ["segment", "dialect", "rule_code", "fixes", "fix_even_unparsable"], "calls": ["segment.invalidate_caches", "segment.is_raw", "fixes.pop", "list", "apply_fixes", "range", "len", "range", "segment.__class__", "seg_buffer.append", "seg_fixes.reverse", "fixes_applied.append", "linter_logger.debug", "segment._position_segments", "len", "segment._is_code_or_meta", "len", "segment._is_code_or_meta", "hasattr", "hasattr", "len", "seg_buffer.append", "seg_buffer.append", "seg_buffer.append", "tuple", "segment._position_segments", "err.add_note", "new_seg.validate_segment_with_reparse", "len", "tuple", "getattr", "len"], "code_location": {"file": "fix.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 107, "end_line": 335}, "code_snippet": "def apply_fixes(\n    segment: BaseSegment,\n    dialect: \"Dialect\",\n    rule_code: str,\n    fixes: dict[int, AnchorEditInfo],\n    fix_even_unparsable: bool = False,\n) -> tuple[\"BaseSegment\", list[\"BaseSegment\"], list[\"BaseSegment\"], bool]:\n    \"\"\"Apply a dictionary of fixes to this segment.\n\n    Used in to apply fixes found in linting. If a segment remains unchanged\n    then the original is returned, but if any changes are made to it, or any\n    of it's child segments, then it returns a copy rather than mutating the\n    original.\n\n    Most fixes are usually applied when this method is called on their parent\n    segment, this is because that's where we can insert or move segments relative\n    to the anchor specified in the fix. This has the implication that if the\n    method is called on a `RawSegment`, then no changes will be applied, because\n    a `RawSegment` never has child segments.\n\n    After fixing, it calls `validate_segment_with_reparse` on the segment to\n    check that the segment still parses after any changes are made. The result\n    of this is returned as a boolean in the last element of the return tuple.\n    As the function recurses, if an inner element doesn't parse after fixing,\n    then the outer segment will also be checked, and if found to parse successfully\n    then the method returns `True` as valid. This is because sometimes the fixes\n    change the structure enough that a wider reparse is necessary.\n\n    Because of this validity checking, any unparsable sections are assumed\n    unfixable (because we won't know if we're corrupting the SQL). The method\n    will therefore return early without applying any fixes if the segment it's\n    called on is unparsable (because we already know that validation check will\n    fail already).\n\n    If `fix_even_unparsable` is True, then we will still apply fixes to unparsable\n    sections, but will do so *without validation*. That means that the final\n    element of the return value will always return `True`, so that we don't interrupt\n    the validity checking of any outer (parsable) sections.\n    \"\"\"\n    if not fixes or segment.is_raw():\n        return segment, [], [], True\n\n    seg_buffer = []\n    before = []\n    after = []\n    fixes_applied: list[LintFix] = []\n    requires_validate = False\n\n    for seg in segment.segments:\n        # Look for uuid match.\n        # This handles potential positioning ambiguity.\n        anchor_info: Optional[AnchorEditInfo] = fixes.pop(seg.uuid, None)\n\n        if anchor_info is None:\n            # No fix matches here, just add the segment and move on.\n            seg_buffer.append(seg)\n            continue\n\n        # Otherwise there is a fix match.\n        seg_fixes = anchor_info.fixes\n        if (\n            len(seg_fixes) == 2 and seg_fixes[0].edit_type == \"create_after\"\n        ):  # pragma: no cover\n            # Must be create_before & create_after. Swap so the\n            # \"before\" comes first.\n            seg_fixes.reverse()\n\n        for f in anchor_info.fixes:\n            assert f.anchor.uuid == seg.uuid\n            fixes_applied.append(f)\n            linter_logger.debug(\n                \"Matched fix for %s against segment: %s -> %s\",\n                rule_code,\n                f,\n                seg,\n            )\n\n            # Deletes are easy.\n            if f.edit_type == \"delete\":\n                # We're just getting rid of this segment.\n                requires_validate = True\n                # NOTE: We don't add the segment in this case.\n                continue\n\n            # Otherwise it must be a replace or a create.\n            assert f.edit_type in (\n                \"replace\",\n                \"create_before\",\n                \"create_after\",\n            ), f\"Unexpected edit_type: {f.edit_type!r} in {f!r}\"\n\n            if f.edit_type == \"create_after\" and len(anchor_info.fixes) == 1:\n                # in the case of a creation after that is not part\n                # of a create_before/create_after pair, also add\n                # this segment before the edit.\n                seg_buffer.append(seg)\n\n            # We're doing a replacement (it could be a single\n            # segment or an iterable)\n            assert f.edit, f\"Edit {f.edit_type!r} requires `edit`.\"\n            consumed_pos = False\n            for s in f.edit:\n                seg_buffer.append(s)\n                # If one of them has the same raw representation\n                # then the first that matches gets to take the\n                # original position marker.\n                if f.edit_type == \"replace\" and s.raw == seg.raw and not consumed_pos:\n                    seg_buffer[-1].pos_marker = seg.pos_marker\n                    consumed_pos = True\n\n            # If we're just editing a segment AND keeping the type the\n            # same then no need to validate. Otherwise we should\n            # trigger a validation (e.g. for creations or\n            # multi-replace).\n            if not (\n                f.edit_type == \"replace\"\n                and len(f.edit) == 1\n                and f.edit[0].class_types == seg.class_types\n            ):\n                requires_validate = True\n\n            if f.edit_type == \"create_before\":\n                # in the case of a creation before, also add this\n                # segment on the end\n                seg_buffer.append(seg)\n\n    # Invalidate any caches\n    segment.invalidate_caches()\n\n    # If any fixes applied, do an intermediate reposition. When applying\n    # fixes to children and then trying to reposition them, that recursion\n    # may rely on the parent having already populated positions for any\n    # of the fixes applied there first. This ensures those segments have\n    # working positions to work with.\n    if fixes_applied:\n        assert segment.pos_marker\n        seg_buffer = list(\n            segment._position_segments(tuple(seg_buffer), parent_pos=segment.pos_marker)\n        )\n\n    # Then recurse (i.e. deal with the children) (Requeueing)\n    seg_queue = seg_buffer\n    seg_buffer = []\n    for seg in seg_queue:\n        s, pre, post, validated = apply_fixes(seg, dialect, rule_code, fixes)\n        # 'before' and 'after' will usually be empty. Only used when\n        # lower-level fixes left 'seg' with non-code (usually\n        # whitespace) segments as the first or last children. This is\n        # generally not allowed (see the can_start_end_non_code field),\n        # and these segments need to be \"bubbled up\" the tree.\n        seg_buffer += pre + [s] + post\n        # If we fail to validate a child segment, make sure to validate this\n        # segment.\n        if not validated:\n            requires_validate = True\n\n    # Most correct whitespace positioning will have already been handled\n    # _however_, the exception is `replace` edits which match start or\n    # end with whitespace. We also need to handle any leading or trailing\n    # whitespace ejected from the any fixes applied to child segments.\n    # Here we handle those by checking the start and end of the resulting\n    # segment sequence for whitespace.\n    # If we're left with any non-code at the end, trim them off and pass them\n    # up to the parent segment for handling.\n    if not segment.can_start_end_non_code:\n        _idx = 0\n        for _idx in range(0, len(seg_buffer)):\n            if segment._is_code_or_meta(seg_buffer[_idx]):\n                break\n        before = seg_buffer[:_idx]\n        seg_buffer = seg_buffer[_idx:]\n\n        _idx = len(seg_buffer)\n        for _idx in range(len(seg_buffer), 0, -1):\n            if segment._is_code_or_meta(seg_buffer[_idx - 1]):\n                break\n        after = seg_buffer[_idx:]\n        seg_buffer = seg_buffer[:_idx]\n\n    # Reform into a new segment\n    assert segment.pos_marker\n    try:\n        new_seg = segment.__class__(\n            # Realign the segments within\n            segments=segment._position_segments(\n                tuple(seg_buffer), parent_pos=segment.pos_marker\n            ),\n            pos_marker=segment.pos_marker,\n            # Pass through any additional kwargs\n            **{k: getattr(segment, k) for k in segment.additional_kwargs},\n        )\n    except AssertionError as err:  # pragma: no cover\n        # An AssertionError on creating a new segment is likely a whitespace\n        # check fail. If possible add information about the fixes we tried to\n        # apply, before re-raising.\n        # NOTE: only available in python 3.11+.\n        if hasattr(err, \"add_note\"):\n            err.add_note(f\" After applying fixes: {fixes_applied}.\")\n        raise err\n\n    # Handle any necessary validation.\n    if requires_validate:\n        # Was it already unparsable?\n        if \"unparsable\" in segment.descendant_type_set | segment.class_types:\n            if fix_even_unparsable:\n                # If we're fixing even unparsable sections, there's no point trying\n                # to validate, it will always fail. We may still want to validate\n                # other sections of the file though, so we should just declare *this*\n                # part of the file to be all good.\n                validated = True\n            else:\n                # It was already unparsable, but we're being asked to validate.\n                # Don't any apply fixes from within this region and just return the\n                # original segment.\n                return segment, [], [], True\n        # Otherwise only validate if there's a match_grammar. Otherwise we may get\n        # strange results (for example with the BracketedSegment).\n        elif hasattr(new_seg, \"match_grammar\"):\n            validated = new_seg.validate_segment_with_reparse(dialect)\n    else:\n        validated = not requires_validate\n    # Return the new segment and any non-code that needs to bubble up\n    # the tree.\n    # NOTE: We pass on whether this segment has been validated. It's\n    # very possible that our parsing here may fail depending on the\n    # type of segment that has been replaced, but if not we rely on\n    # a parent segment still being valid. If we get all the way up\n    # to the root and it's still not valid - that's a problem.\n    return new_seg, before, after, validated\n", "type": "function"}, {"name": "test__dialect__base_broad_fix", "is_method": false, "class_name": null, "parameters": ["dialect", "file", "raise_critical_errors_after_fix", "caplog"], "calls": ["pytest.mark.parametrize", "load_file", "dict", "lex_and_parse", "print", "FluffConfig", "Linter", "linter.get_rulepack", "linter.lint_parsed", "parsed.tree.stringify"], "code_location": {"file": "dialects_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 85, "end_line": 120}, "code_snippet": "def test__dialect__base_broad_fix(\n    dialect, file, raise_critical_errors_after_fix, caplog\n):\n    \"\"\"Run a full fix with all rules, in search of critical errors.\n\n    NOTE: This suite does all of the same things as the above test\n    suite (the `parse_suite`), but also runs fix. In CI, we run\n    the above tests _with_ coverage tracking, but these we run\n    _without_.\n\n    The purpose of this test is as a more stretching run through\n    a wide range of test sql examples, and the full range of rules\n    to find any potential critical errors raised by any interactions\n    between different dialects and rules.\n\n    We also do not use DEBUG logging here because it gets _very_\n    noisy.\n    \"\"\"\n    raw = load_file(dialect, file)\n    config_overrides = dict(dialect=dialect)\n\n    parsed: Optional[ParsedString] = lex_and_parse(config_overrides, raw)\n    if not parsed:  # Empty file case\n        return\n    print(parsed.tree.stringify())\n\n    config = FluffConfig(overrides=config_overrides)\n    linter = Linter(config=config)\n    rule_pack = linter.get_rulepack()\n    # Due to \"raise_critical_errors_after_fix\" fixture \"fix\",\n    # will now throw.\n    linter.lint_parsed(\n        parsed,\n        rule_pack,\n        fix=True,\n    )\n", "type": "function"}, {"name": "test__linted_file__build_up_fixed_source_string", "is_method": false, "class_name": null, "parameters": ["source_slices", "source_patches", "raw_source_string", "expected_result", "caplog"], "calls": ["pytest.mark.parametrize", "caplog.at_level", "LintedFile._build_up_fixed_source_string", "slice", "slice", "slice", "slice", "FixPatch", "slice", "slice", "slice", "FixPatch", "slice", "slice", "slice", "FixPatch", "slice", "slice", "slice", "FixPatch", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice"], "code_location": {"file": "linted_file_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 51, "end_line": 62}, "code_snippet": "def test__linted_file__build_up_fixed_source_string(\n    source_slices, source_patches, raw_source_string, expected_result, caplog\n):\n    \"\"\"Test _build_up_fixed_source_string.\n\n    This is part of fix_string().\n    \"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.linter\"):\n        result = LintedFile._build_up_fixed_source_string(\n            source_slices, source_patches, raw_source_string\n        )\n    assert result == expected_result\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_CV04", "parameters": ["self", "context"], "calls": ["context.segment.get_child", "children", "is_type", "sp.and_", "len", "is_type", "LiteralSegment", "LintResult", "children", "sp.not_", "sp.not_", "is_type", "LintResult", "sp.is_type", "sp.is_meta", "sp.is_type", "len", "SymbolSegment", "LiteralSegment", "segment.children", "LintFix.replace", "sp.is_type", "LintFix.replace", "FunctionalContext", "edit", "raw.replace"], "code_location": {"file": "CV04.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "start_line": 64, "end_line": 149}, "code_snippet": "    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Find rule violations and provide fixes.\"\"\"\n        # Config type hints\n        self.prefer_count_0: bool\n        self.prefer_count_1: bool\n        new_segment: RawSegment\n\n        # We already know we're in a function because of the crawl_behaviour.\n        # This means it's very unlikely that there isn't a function_name here.\n        function_name = context.segment.get_child(\"function_name\")\n        if not function_name:  # pragma: no cover\n            return None\n\n        if function_name.raw_upper == \"COUNT\":\n            # Get bracketed content\n            f_content = (\n                FunctionalContext(context)\n                .segment.children(sp.is_type(\"function_contents\"))\n                .children(sp.is_type(\"bracketed\"))\n                .children(\n                    sp.and_(\n                        sp.not_(sp.is_meta()),\n                        sp.not_(\n                            sp.is_type(\n                                \"start_bracket\", \"end_bracket\", \"whitespace\", \"newline\"\n                            )\n                        ),\n                    )\n                )\n            )\n            if len(f_content) != 1:  # pragma: no cover\n                return None\n\n            preferred = \"*\"\n            if self.prefer_count_1:\n                preferred = \"1\"\n            elif self.prefer_count_0:\n                preferred = \"0\"\n\n            if f_content[0].is_type(\"star\") and (\n                self.prefer_count_1 or self.prefer_count_0\n            ):\n                new_segment = LiteralSegment(raw=preferred, type=\"numeric_literal\")\n                return LintResult(\n                    anchor=context.segment,\n                    fixes=[\n                        LintFix.replace(\n                            f_content[0],\n                            [new_segment],\n                        ),\n                    ],\n                )\n\n            if f_content[0].is_type(\"expression\"):\n                expression_content = [\n                    seg for seg in f_content[0].segments if not seg.is_meta\n                ]\n\n                if (\n                    len(expression_content) == 1\n                    and expression_content[0].is_type(\"literal\")\n                    and expression_content[0].raw in [\"0\", \"1\"]\n                    and expression_content[0].raw != preferred\n                ):\n                    if preferred == \"*\":\n                        new_segment = SymbolSegment(raw=preferred, type=\"star\")\n                    else:\n                        new_segment = LiteralSegment(\n                            raw=preferred, type=\"numeric_literal\"\n                        )\n                    return LintResult(\n                        anchor=context.segment,\n                        fixes=[\n                            LintFix.replace(\n                                expression_content[0],\n                                [\n                                    expression_content[0].edit(\n                                        expression_content[0].raw.replace(\n                                            expression_content[0].raw, preferred\n                                        )\n                                    ),\n                                ],\n                            ),\n                        ],\n                    )\n        return None\n", "type": "function"}, {"name": "cli", "is_method": false, "class_name": null, "parameters": [], "calls": ["click.group", "click.version_option"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 480, "end_line": 481}, "code_snippet": "def cli() -> None:\n    \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n", "type": "function"}, {"name": "test__api__fix_string", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.fix", "isinstance"], "code_location": {"file": "simple_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 417, "end_line": 431}, "code_snippet": "def test__api__fix_string():\n    \"\"\"Basic checking of lint functionality.\"\"\"\n    result = sqlfluff.fix(my_bad_query)\n    # Check return types.\n    assert isinstance(result, str)\n    # Check actual result\n    assert (\n        result\n        == \"\"\"SELECT\n    *,\n    1,\n    blah AS foo\nFROM mytable\n\"\"\"\n    )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0696625709533691}
{"question": "What is the relationship between SQLFluff's FormatterInterface and output generation?", "answer": null, "relative_code_list": null, "ground_truth": "FormatterInterface serves as the abstract interface for SQLFluff's output generation system, providing a callback mechanism for different stages of the linting process. Key relationships include: 1) Abstract interface - FormatterInterface defines abstract methods that must be implemented by concrete formatter classes to handle various output events; 2) Callback system - Provides dispatch methods for different linting stages including dispatch_lint_header(), dispatch_file_violations(), dispatch_parse_header(), and dispatch_template_header(); 3) CLI integration - OutputStreamFormatter implements FormatterInterface to provide human-readable output with colorization and formatting for command-line usage; 4) Linter integration - Linter class accepts an optional FormatterInterface parameter and calls its methods at appropriate points during linting to generate real-time output; 5) Output formatting - Handles formatting of violations, file status, configuration information, and timing statistics for display; 6) Color support - Provides colorize() method for ANSI color codes to enhance readability of output; 7) Multi-format support - Enables different output formats (human, JSON, YAML, GitHub annotations) through different formatter implementations; 8) Progress tracking - Supports progress bars and real-time feedback during long-running linting operations; 9) Error reporting - Formats and displays various types of errors (lexing, parsing, templating, linting) in a consistent manner; 10) Extensibility - Allows custom formatters to be implemented for different output requirements (logging, GUI integration, CI/CD systems).", "score": null, "retrieved_content": [{"name": "FormatterInterface", "docstring": "Generic formatter interface.", "methods": ["dispatch_persist_filename", "dispatch_lint_header", "dispatch_file_violations", "dispatch_dialect_warning", "dispatch_template_header", "dispatch_parse_header", "dispatch_processing_header", "dispatch_path", "colorize"], "attributes": [], "code_location": {"file": "formatter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 23, "end_line": 80}, "type": "class"}, {"name": "print_out_violations_and_timing", "is_method": true, "class_name": "OutputStreamFormatter", "parameters": ["self", "output_stream", "bench", "code_only", "total_time", "verbose", "parsed_strings"], "calls": ["TimingSummary", "timing.add", "len", "parsed_string.root_variant", "len", "output_stream.write", "output_stream.write", "timing.summary", "output_stream.write", "output_stream.write", "output_stream.write", "output_stream.write", "output_stream.write", "output_stream.write", "self.cli_table", "output_stream.write", "output_stream.write", "self.colorize", "output_stream.write", "output_stream.write", "enumerate", "self.format_violation", "self.format_dialect_warning", "self.cli_table", "self.cli_table", "root_variant.tree.stringify", "self.colorize", "output_stream.write", "parsed_string.config.get", "parsed_string.time_dict.items", "items", "self.colorize", "output_stream.write", "output_stream.write", "variant.tree.stringify", "self.colorize"], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 638, "end_line": 710}, "code_snippet": "    def print_out_violations_and_timing(\n        self,\n        output_stream: OutputStream,\n        bench: bool,\n        code_only: bool,\n        total_time: float,\n        verbose: int,\n        parsed_strings: list[ParsedString],\n    ) -> int:\n        \"\"\"Used by human formatting during the `sqlfluff parse` command.\"\"\"\n        violations_count = 0\n        timing = TimingSummary()\n\n        for parsed_string in parsed_strings:\n            timing.add(parsed_string.time_dict)\n\n            num_variants = len(parsed_string.parsed_variants)\n            root_variant = parsed_string.root_variant()\n            if not root_variant:\n                # TODO: Make this prettier\n                output_stream.write(\n                    self.colorize(\"...Failed to Parse...\", Color.red)\n                )  # pragma: no cover\n            elif num_variants == 1:\n                # Backward compatible single parse\n                assert root_variant.tree\n                output_stream.write(root_variant.tree.stringify(code_only=code_only))\n            else:\n                # Multi variant parse setup.\n                output_stream.write(\n                    self.colorize(\n                        f\"SQLFluff parsed {num_variants} variants of this file\",\n                        Color.blue,\n                    )\n                )\n                for idx, variant in enumerate(parsed_string.parsed_variants):\n                    output_stream.write(\n                        self.colorize(\n                            f\"Variant {idx + 1}:\",\n                            Color.blue,\n                        )\n                    )\n                    if variant.tree:\n                        output_stream.write(variant.tree.stringify(code_only=code_only))\n                    else:  # pragma: no cover\n                        output_stream.write(\n                            self.colorize(\"...Failed to Parse...\", Color.red)\n                        )\n\n            violations = parsed_string.violations\n            violations_count += len(violations)\n            if violations:\n                output_stream.write(\"==== parsing violations ====\")  # pragma: no cover\n            for v in violations:\n                output_stream.write(self.format_violation(v))  # pragma: no cover\n            if violations:\n                output_stream.write(\n                    self.format_dialect_warning(parsed_string.config.get(\"dialect\"))\n                )\n\n            if verbose >= 2:\n                output_stream.write(\"==== timings ====\")\n                output_stream.write(self.cli_table(parsed_string.time_dict.items()))\n\n        if verbose >= 2 or bench:\n            output_stream.write(\"==== overall timings ====\")\n            output_stream.write(self.cli_table([(\"Clock time\", total_time)]))\n            timing_summary = timing.summary()\n            for step in timing_summary:\n                output_stream.write(f\"=== {step} ===\")\n                output_stream.write(self.cli_table(timing_summary[step].items()))\n\n        return violations_count\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "PathAndUserErrorHandler", "parameters": ["self", "formatter"], "calls": [], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 128, "end_line": 129}, "code_snippet": "    def __init__(self, formatter: OutputStreamFormatter) -> None:\n        self.formatter = formatter\n", "type": "function"}, {"name": "OutputStreamFormatter", "docstring": "Formatter which writes to an OutputStream.\n\nOn instantiation, this formatter accepts a function to\ndispatch messages. Each public method accepts an object\nor data in a common format, with this class handling the\nformatting and output.\n\nThis class is designed to be subclassed if we eventually\nwant to provide other methods of surfacing output.\n\n\nArgs:\n    output_stream: Output is sent here\n    verbosity: Specifies how verbose output should be\n    filter_empty: If True, empty messages will not be dispatched\n    output_line_length: Maximum line length", "methods": ["__init__", "should_produce_plain_output", "_dispatch", "_format_config", "dispatch_config", "dispatch_persist_filename", "_format_path", "dispatch_path", "dispatch_template_header", "dispatch_parse_header", "dispatch_lint_header", "dispatch_compilation_header", "dispatch_processing_header", "dispatch_dialect_warning", "_format_file_violations", "dispatch_file_violations", "colorize", "colorize_helper", "cli_table_row", "cli_table", "format_filename", "format_violation", "format_linting_stats", "format_config_vals", "_format_rule_description", "format_rules", "format_dialects", "format_dialect_warning", "print_out_residual_error_counts", "print_out_violations_and_timing", "completion_message"], "attributes": [], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 69, "end_line": 714}, "type": "class"}, {"name": "process", "is_method": true, "class_name": "DbtTemplater", "parameters": ["self"], "calls": ["handle_dbt_errors", "self._get_project_dir", "self._get_profiles_dir", "self._get_dbt_skip_compilation_error", "self._unsafe_process", "os.path.abspath"], "code_location": {"file": "templater.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt", "start_line": 567, "end_line": 596}, "code_snippet": "    def process(\n        self,\n        *,\n        fname: str,\n        in_str: Optional[str] = None,\n        config: Optional[\"FluffConfig\"] = None,\n        formatter: Optional[\"OutputStreamFormatter\"] = None,\n    ) -> tuple[TemplatedFile, list[SQLTemplaterError]]:\n        \"\"\"Compile a dbt model and return the compiled SQL.\n\n        Args:\n            fname: Path to dbt model(s)\n            in_str: fname contents using configured encoding\n            config: A specific config to use for this\n                templating operation. Only necessary for some templaters.\n            formatter: Optional object for output.\n        \"\"\"\n        # Stash the formatter if provided to use in cached methods.\n        self.formatter = formatter\n        self.sqlfluff_config = config\n        self.project_dir = self._get_project_dir()\n        self.profiles_dir = self._get_profiles_dir()\n        self.dbt_skip_compilation_error = self._get_dbt_skip_compilation_error()\n        fname_absolute_path = os.path.abspath(fname) if fname != \"stdin\" else fname\n\n        # NOTE: dbt exceptions are caught and handled safely for pickling by the outer\n        # `handle_dbt_errors` decorator.\n        return self._unsafe_process(\n            fname_absolute_path, in_str, config, self.project_dir\n        )\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "FileOutput", "parameters": ["self", "config", "output_path"], "calls": ["__init__", "open", "super"], "code_location": {"file": "outputstream.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 49, "end_line": 51}, "code_snippet": "    def __init__(self, config: FluffConfig, output_path: str) -> None:\n        super().__init__(config)\n        self.file = open(output_path, \"w\")\n", "type": "function"}, {"name": "format_dialects", "is_method": true, "class_name": "OutputStreamFormatter", "parameters": ["self", "dialect_readout", "verbose"], "calls": ["StringIO", "text_buffer.write", "text_buffer.write", "text_buffer.getvalue", "self.cli_table", "dialect_readout"], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 574, "end_line": 594}, "code_snippet": "    def format_dialects(self, dialect_readout, verbose=0) -> str:\n        \"\"\"Format the dialects yielded by `dialect_readout`.\"\"\"\n        text_buffer = StringIO()\n        text_buffer.write(\"==== sqlfluff - dialects ====\\n\")\n        readouts = [\n            (\n                dialect.label,\n                f\"{dialect.name} dialect [inherits from '{dialect.inherits_from}']\",\n            )\n            for dialect in dialect_readout()\n        ]\n        text_buffer.write(\n            self.cli_table(\n                readouts,\n                col_width=60,\n                cols=1,\n                label_color=Color.blue,\n                val_align=\"right\",\n            )\n        )\n        return text_buffer.getvalue()\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "Linter", "parameters": ["self", "config", "formatter", "dialect", "rules", "user_rules", "exclude_rules"], "calls": ["cast", "cast", "ValueError", "FluffConfig.from_kwargs", "self.config.get", "self.config.get"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 63, "end_line": 98}, "code_snippet": "    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n        dialect: Optional[str] = None,\n        rules: Optional[list[str]] = None,\n        user_rules: Optional[list[type[BaseRule]]] = None,\n        exclude_rules: Optional[list[str]] = None,\n    ) -> None:\n        if config and (dialect or rules or exclude_rules):\n            raise ValueError(  # pragma: no cover\n                \"Linter does not support setting both `config` and any of \"\n                \"`dialect`, `rules` or `exclude_rules`. The latter are \"\n                \"provided as convenience methods to avoid needing to \"\n                \"set the `config` object. If using `config`, please \"\n                \"provide all the other values within that object.\"\n            )\n        # Use the provided config or create one from the kwargs.\n        self.config = config or FluffConfig.from_kwargs(\n            dialect=dialect,\n            rules=rules,\n            exclude_rules=exclude_rules,\n            # Don't require a dialect to be provided yet. Defer this until we\n            # are actually linting something, since the directory we are linting\n            # from may provide additional configuration, including a dialect.\n            require_dialect=False,\n        )\n        # Get the dialect and templater\n        self.dialect: \"Dialect\" = cast(\"Dialect\", self.config.get(\"dialect_obj\"))\n        self.templater: \"RawTemplater\" = cast(\n            \"RawTemplater\", self.config.get(\"templater_obj\")\n        )\n        # Store the formatter for output\n        self.formatter = formatter\n        # Store references to user rule classes\n        self.user_rules = user_rules or []\n", "type": "function"}, {"name": "parse_reports", "is_method": true, "class_name": "SQLFluffDriver", "parameters": ["self", "reports"], "calls": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 36, "end_line": 38}, "code_snippet": "    def parse_reports(self, reports) -> None:  # pragma: no cover\n        \"\"\"Parse report output. Not used by SQLFluff.\"\"\"\n        pass\n", "type": "function"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.0902636051177979}
{"question": "What is the structure of SQLFluff's configuration system?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's configuration system is built around the FluffConfig class and supports multiple configuration sources with a hierarchical nesting structure. The system includes: 1) Configuration files - supports multiple file formats including setup.cfg, tox.ini, pep8.ini, .sqlfluff, and pyproject.toml, with later files overriding earlier ones; 2) Hierarchical nesting - configuration is loaded from multiple locations in order: default config, user's app config directory (~/.config/sqlfluff), home directory (~), directories between home and working directory, current working directory, subdirectories between working directory and file directory, and the file's containing directory; 3) FluffConfig class - the main configuration object that combines defaults, user configs, and overrides using nested_combine(), validates configuration, and manages long-lived objects like dialects and templaters; 4) Configuration sections - uses colon-delimited sections (e.g., [sqlfluff:rules:capitalisation.keywords]) in cfg files and dot-delimited sections (e.g., [tool.sqlfluff.rules.capitalisation.keywords]) in pyproject.toml; 5) Override system - supports command-line overrides that take precedence over file-based configuration; 6) Plugin integration - uses a plugin manager to load default configurations from various plugins; 7) Special handling - includes logic for comma-separated values, dialect initialization, and templater object creation.", "score": null, "retrieved_content": [{"name": "__init__", "is_method": true, "class_name": "FluffConfig", "parameters": ["self", "configs", "extra_config_path", "ignore_local_config", "overrides", "plugin_manager", "require_dialect"], "calls": ["nested_combine", "nested_combine", "self._handle_comma_separated_values", "self._initialise_dialect", "self.get_templater", "validate_config_dict", "isinstance", "get_plugin_manager", "validate_config_dict", "get", "isinstance", "self._plugin_manager.hook.load_default_config"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 89, "end_line": 137}, "code_snippet": "    def __init__(\n        self,\n        configs: Optional[ConfigMappingType] = None,\n        extra_config_path: Optional[str] = None,\n        ignore_local_config: bool = False,\n        overrides: Optional[ConfigMappingType] = None,\n        plugin_manager: Optional[pluggy.PluginManager] = None,\n        # Ideally a dialect should be set when config is read but sometimes\n        # it might only be set in nested .sqlfluff config files, so allow it\n        # to be not required.\n        require_dialect: bool = True,\n    ) -> None:\n        self._extra_config_path = (\n            extra_config_path  # We only store this for child configs\n        )\n        self._ignore_local_config = (\n            ignore_local_config  # We only store this for child configs\n        )\n        # If overrides are provided, validate them early.\n        if overrides:\n            overrides = {\"core\": overrides}\n            validate_config_dict(overrides, \"<provided overrides>\")\n        # Stash overrides so we can pass them to child configs\n        core_overrides = overrides[\"core\"] if overrides else None\n        assert isinstance(core_overrides, dict) or core_overrides is None\n        self._overrides = core_overrides\n\n        # Fetch a fresh plugin manager if we weren't provided with one\n        self._plugin_manager = plugin_manager or get_plugin_manager()\n\n        defaults = nested_combine(*self._plugin_manager.hook.load_default_config())\n        # If any existing configs are provided. Validate them:\n        if configs:\n            validate_config_dict(configs, \"<provided configs>\")\n        self._configs = nested_combine(\n            defaults, configs or {\"core\": {}}, overrides or {}\n        )\n        # Some configs require special treatment\n        self._configs[\"core\"][\"color\"] = (\n            False if self._configs[\"core\"].get(\"nocolor\", False) else None\n        )\n        # Handle inputs which are potentially comma separated strings\n        self._handle_comma_separated_values()\n        # Dialect and Template selection.\n        _dialect = self._configs[\"core\"][\"dialect\"]\n        assert _dialect is None or isinstance(_dialect, str)\n        self._initialise_dialect(_dialect, require_dialect)\n\n        self._configs[\"core\"][\"templater_obj\"] = self.get_templater()\n", "type": "function"}, {"name": "FluffConfig", "docstring": "The persistent object for internal methods to access configuration.\n\nThis class is designed to be instantiated once for each file and then be\nreused by each part of the process. For multiple files in the same path, a\nparent object will be created for the each path and then variants of it\nare created *for each file*. The object itself contains the references\nto any long lived objects which might be used by multiple parts of the\ncodebase such as the dialect and the templater (both of which can be\nresource intensive to load & instantiate), which allows (for example),\nmultiple files to reuse the same instance of the relevant dialect.\n\nIt is also designed to pickle well for use in parallel operations.\n\nArgs:\n    configs (ConfigMappingType, optional): A nested dict of config\n        values from which to construct the config.\n    extra_config_path (str, optional): An optional additional path\n        to load config files from. These are loaded last if found\n        and take precedence over any pre-existing config values.\n        Note that when provided directly to the class, this path\n        is not loaded for the class in question (it's assumed that\n        has already been done, and the results are incorporated in\n        the `configs` argument), but it *is* passed onward to child\n        config instances, which will use it.\n    ignore_local_config (bool, optional, defaults to False): If set to\n        True, this skips loading configuration from the user home\n        directory (``~``) or ``appdir`` path.\n    overrides (ConfigMappingType, optional): A additional set of\n        configs to merge into the ``core`` section of the config\n        object at the end. These values take precedence over all\n        other provided values and are inherited by child configs.\n        For example, override values provided in the CLI use this\n        method to apply to all files in a linting operation. Note\n        that this mapping dict *only* applies to the ``core``\n        section and so cannot be used for all values.\n    plugin_manager (PluginManager, optional): Optional pre-loaded\n        config manager. Generally users should not need to provide\n        this, as the class will fetch it's own if not provided.\n        This argument is used when creating new class instances to\n        avoid reloading the manager.\n\n.. note::\n   Methods for accessing internal properties on the config are not particularly\n   standardised as the project currently assumes that few other tools are using\n   this interface directly. If you or your project would like more formally\n   supported methods for access to the config object, raise an issue on GitHub\n   with the kind of things you'd like to achieve.", "methods": ["__init__", "_handle_comma_separated_values", "_initialise_dialect", "verify_dialect_specified", "__getstate__", "__setstate__", "copy", "from_root", "from_string", "from_strings", "from_path", "from_kwargs", "get_templater_class", "get_templater", "make_child_from_path", "diff_to", "get", "get_section", "set_value", "iter_vals", "process_inline_config", "process_raw_file_for_config"], "attributes": ["private_vals"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 37, "end_line": 732}, "type": "class"}, {"name": "ConfigInfo", "docstring": "Type definition for a single config info value.\n\nThis TypedDict defines the structure for configuration information used across\nSQLFluff rules. Each config value must have a definition, and may optionally\ninclude validation criteria.\n\nArgs:\n    definition: Required string containing a detailed description of the config\n        option and its purpose. This should be clear enough for users to\n        understand when and how to use the config.\n    validation: Optional list or range of valid values for the config option.\n        Can contain boolean, string, or integer values. If not provided,\n        the config option accepts any value of its expected type.", "methods": [], "attributes": [], "code_location": {"file": "config_info.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 21, "end_line": 40}, "type": "class"}, {"name": "test__linter__path_from_paths__exts", "is_method": false, "class_name": null, "parameters": [], "calls": ["normalise_paths", "paths_from_path"], "code_location": {"file": "discovery_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 38, "end_line": 46}, "code_snippet": "def test__linter__path_from_paths__exts():\n    \"\"\"Test configuration of file discovery.\"\"\"\n    paths = normalise_paths(\n        paths_from_path(\"test/fixtures/linter\", target_file_exts=[\".txt\", \".txt.j2\"])\n    )\n    assert \"test.fixtures.linter.passing.sql\" not in paths\n    assert \"test.fixtures.linter.passing_cap_extension.SQL\" not in paths\n    assert \"test.fixtures.linter.discovery_file.txt\" in paths\n    assert \"test.fixtures.linter.discovery_file.txt.j2\" in paths\n", "type": "function"}, {"name": "get_config_info", "is_method": false, "class_name": null, "parameters": [], "calls": ["get_plugin_manager", "plugin_manager.hook.get_configs_info", "config_info_dict.items"], "code_location": {"file": "config_info.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 88, "end_line": 99}, "code_snippet": "def get_config_info() -> dict[str, ConfigInfo]:\n    \"\"\"Get the config from core sqlfluff and sqlfluff plugins and merges them.\n\n    NOTE: This should be the entry point into getting config info rather than\n    importing the default set above, as many values are defined only in rule\n    packages.\n    \"\"\"\n    plugin_manager = get_plugin_manager()\n    configs_info = plugin_manager.hook.get_configs_info()\n    return {\n        k: v for config_info_dict in configs_info for k, v in config_info_dict.items()\n    }\n", "type": "function"}, {"name": "test__config__validate_configs_indirect", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.raises", "FluffConfig"], "code_location": {"file": "fluffconfig_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/config", "start_line": 236, "end_line": 246}, "code_snippet": "def test__config__validate_configs_indirect():\n    \"\"\"Test _validate_configs method of FluffConfig indirectly.\"\"\"\n    # Instantiate config object.\n    with pytest.raises(SQLFluffUserError):\n        FluffConfig(\n            configs={\n                \"core\": {\"dialect\": \"ansi\"},\n                # This is a known removed value.\n                \"rules\": {\"L003\": {\"lint_templated_tokens\": True}},\n            }\n        )\n", "type": "function"}, {"name": "load_ini_string", "is_method": false, "class_name": null, "parameters": ["cfg_content"], "calls": ["configparser.ConfigParser", "config.read_string", "config.sections", "records_to_nested_dict", "config.items", "k.startswith", "coerce_value", "config_buffer.append", "tuple", "split", "len"], "code_location": {"file": "ini.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 35, "end_line": 86}, "code_snippet": "def load_ini_string(cfg_content: str) -> ConfigMappingType:\n    \"\"\"Read an ini-style config string.\n\n    This would include loading a `.sqlfluff` file.\n\n    Notes:\n    - We rename the root `sqlfluff` section, to `core` so that it's in\n      line with other config files.\n    - The `configparser` reads everything as strings, but this method will\n      attempt to find better types for values based on their content.\n    - Path resolution isn't done here, that all happens later.\n    - Unlike most cfg file readers, SQLFluff is case-sensitive in how\n      it reads config files. This is to ensure we support the case\n      sensitivity of jinja.\n    \"\"\"\n    # If the string is empty, no need to parse it.\n    if not cfg_content:\n        return {}\n\n    # Disable interpolation so we can load macros\n    config = configparser.ConfigParser(delimiters=\"=\", interpolation=None)\n    # NB: We want to be case sensitive in how we read from files,\n    # because jinja is also case sensitive. To do this we override\n    # the optionxform attribute.\n    config.optionxform = lambda option: option  # type: ignore\n\n    # Read the content.\n    config.read_string(cfg_content)\n\n    # Build up a buffer of config values.\n    config_buffer: list[NestedDictRecord[ConfigValueType]] = []\n    for k in config.sections():\n        if k == \"sqlfluff\":\n            key: tuple[str, ...] = (\"core\",)\n        elif k.startswith(\"sqlfluff:\"):\n            # Return a tuple of nested values\n            key = tuple(k[len(\"sqlfluff:\") :].split(\":\"))\n        else:  # pragma: no cover\n            # if it doesn't start with sqlfluff, then ignore this\n            # section. It's not relevant to sqlfluff.\n            continue\n\n        for name, val in config.items(section=k):\n            # Try to coerce it to a more specific type,\n            # otherwise just make it a string.\n            v = coerce_value(val)\n\n            # Add the name to the end of the key\n            config_buffer.append((key + (name,), v))\n\n    # Compress that buffer into a dictionary.\n    return records_to_nested_dict(config_buffer)\n", "type": "function"}, {"name": "dbt_fluff_config", "is_method": false, "class_name": null, "parameters": ["dbt_project_folder"], "calls": ["pytest.fixture"], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/test", "start_line": 34, "end_line": 47}, "code_snippet": "def dbt_fluff_config(dbt_project_folder):\n    \"\"\"Returns SQLFluff dbt configuration dictionary.\"\"\"\n    return {\n        \"core\": {\n            \"templater\": \"dbt\",\n            \"dialect\": \"postgres\",\n        },\n        \"templater\": {\n            \"dbt\": {\n                \"profiles_dir\": f\"{dbt_project_folder}/profiles_yml\",\n                \"project_dir\": f\"{dbt_project_folder}/dbt_project\",\n            },\n        },\n    }\n", "type": "function"}, {"name": "load_toml_file_config", "is_method": false, "class_name": null, "parameters": ["filepath"], "calls": ["_validate_structure", "isinstance", "records_to_nested_dict", "open", "tomllib.loads", "get", "file.read", "_condense_rule_record", "toml_dict.get", "iter_records_from_nested_dict"], "code_location": {"file": "toml.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 51, "end_line": 83}, "code_snippet": "def load_toml_file_config(filepath: str) -> ConfigMappingType:\n    \"\"\"Read the SQLFluff config section of a pyproject.toml file.\n\n    We don't need to change any key names here, because the root\n    section of the toml file format is `tool.sqlfluff.core`.\n\n    NOTE: Toml files are always encoded in UTF-8. That is a necessary\n    part of the toml spec: https://toml.io/en/v1.0.0\n    \"\"\"\n    with open(filepath, mode=\"r\", encoding=\"utf-8\") as file:\n        toml_dict = tomllib.loads(file.read())\n    config_dict = _validate_structure(toml_dict.get(\"tool\", {}).get(\"sqlfluff\", {}))\n\n    # NOTE: For the \"rules\" section of the sqlfluff config,\n    # rule names are often qualified with a dot \".\". In the\n    # toml scenario this can get interpreted as a nested\n    # section, and we resolve that edge case here.\n    if \"rules\" not in config_dict:\n        # No rules section, so no need to resolve.\n        return config_dict\n\n    rules_section = config_dict[\"rules\"]\n    assert isinstance(rules_section, dict), (\n        \"Expected to find section in `rules` section of config, \"\n        f\"but instead found {rules_section}\"\n    )\n    # Condense the rules section.\n    config_dict[\"rules\"] = records_to_nested_dict(\n        _condense_rule_record(record)\n        for record in iter_records_from_nested_dict(rules_section)\n    )\n\n    return config_dict\n", "type": "function"}, {"name": "test__config__nested_config_tests", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "lntr.lint_path", "lnt.check_tuples_by_path", "k.endswith", "FluffConfig", "k.endswith", "dict"], "code_location": {"file": "fluffconfig_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/config", "start_line": 53, "end_line": 86}, "code_snippet": "def test__config__nested_config_tests():\n    \"\"\"Test linting with overridden config in nested paths.\n\n    This looks like a linter test but it's actually a config\n    test.\n    \"\"\"\n    lntr = Linter(\n        # Exclude CP02 in overrides (similar to cli --exclude-rules)\n        config=FluffConfig(overrides=dict(exclude_rules=\"CP02\", dialect=\"ansi\"))\n    )\n    lnt = lntr.lint_path(\"test/fixtures/config/inheritance_b\")\n    violations = lnt.check_tuples_by_path()\n    for k in violations:\n        if k.endswith(\"nested\\\\example.sql\"):\n            # CP01 is enabled in the .sqlfluff file and not excluded.\n            assert (\"CP01\", 1, 4) in violations[k]\n            # LT02 is enabled in the .sqlfluff file and not excluded.\n            assert (\"LT02\", 1, 1) in violations[k]\n            # CP02 is enabled in the .sqlfluff file but excluded by the\n            # override above.\n            assert \"CP02\" not in [c[0] for c in violations[k]]\n        elif k.endswith(\"inheritance_b\\\\example.sql\"):\n            # CP01 is enabled because while disabled in the tox.ini file,\n            # the exclude-rules option is overridden by the override above\n            # which effectively sets the exclude to CP02 and in effect\n            # re-enables CP01.\n            # This may seem counter-intuitive but is in line with current\n            # documentation on how to use `rules` and `exclude-rules`.\n            # https://docs.sqlfluff.com/en/latest/perma/rule_disabling.html\n            assert (\"CP01\", 1, 4) in violations[k]\n            # CP02 is disabled because of the override above.\n            assert \"CP02\" not in [c[0] for c in violations[k]]\n            # LT02 is disabled because it is not in the `rules` of tox.ini\n            assert \"LT02\" not in [c[0] for c in violations[k]]\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1138136386871338}
{"question": "Why does SQLFluff separate BaseSegment from RawSegment in its parser architecture?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff separates BaseSegment from RawSegment in its parser architecture to create a clear hierarchy and separation of concerns between composite and atomic elements. Key reasons include: 1) Structural clarity - BaseSegment represents composite elements that can contain other segments, while RawSegment represents atomic tokens with no children; 2) Tree representation - This separation enables the creation of a proper parse tree where internal nodes (BaseSegment) contain leaf nodes (RawSegment); 3) Lexer output - RawSegment serves as the primary output of the lexer, representing individual tokens like keywords, identifiers, and literals; 4) Parser input - BaseSegment provides the foundation for building complex SQL structures by combining multiple RawSegments; 5) Type safety - The separation allows for different type systems and validation rules for composite vs atomic elements; 6) Memory efficiency - RawSegments can be optimized for atomic token storage while BaseSegments handle tree traversal and relationship management; 7) Extensibility - New segment types can inherit from either BaseSegment or RawSegment depending on their nature; 8) Processing optimization - Different processing strategies can be applied to composite vs atomic elements; 9) Error handling - Different error recovery strategies can be implemented for composite vs atomic parsing failures; 10) Rule application - Linting rules can target specific segment types more precisely based on this separation.", "score": null, "retrieved_content": [{"name": "BaseSegment", "docstring": "The base segment element.\n\nThis defines the base element which drives both Lexing, Parsing and Linting.\nA large chunk of the logic which defines those three operations are centered\nhere. Much of what is defined in the BaseSegment is also used by its many\nsubclasses rather than directly here.\n\nFor clarity, the `BaseSegment` is mostly centered around a segment which contains\nother subsegments. For segments which don't have *children*, refer to the\n`RawSegment` class (which still inherits from this one).\n\nSegments are used both as instances to hold chunks of text, but also as classes\nthemselves where they function a lot like grammars, and return instances of\nthemselves when they match. The many classmethods in this class are usually to serve\ntheir purpose as a matcher.", "methods": ["__init__", "__setattr__", "__eq__", "_hash", "__hash__", "__repr__", "__getstate__", "__setstate__", "_comments", "_non_comments", "is_code", "_code_indices", "is_comment", "is_whitespace", "raw", "class_types", "descendant_type_set", "direct_descendant_type_set", "raw_upper", "raw_segments", "raw_segments_with_ancestors", "source_fixes", "first_non_whitespace_segment_raw_upper", "is_templated", "_suffix", "_position_segments", "simple", "cache_key", "is_optional", "class_is_type", "structural_simplify", "match", "_recalculate_caches", "_preface", "set_as_parent", "set_parent", "get_parent", "get_type", "count_segments", "is_type", "invalidate_caches", "get_start_point_marker", "get_end_point_marker", "get_start_loc", "get_end_loc", "stringify", "to_tuple", "copy", "as_record", "get_raw_segments", "raw_normalized", "iter_segments", "iter_unparsables", "type_set", "is_raw", "get_child", "get_children", "select_children", "recursive_crawl_all", "recursive_crawl", "path_to", "_is_code_or_meta", "validate_non_code_ends", "validate_segment_with_reparse", "_log_apply_fixes_check_issue", "edit", "from_result_segments"], "attributes": ["comment_separate", "is_meta", "can_start_end_non_code", "allow_empty"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 141, "end_line": 1239}, "type": "class"}, {"name": "RawSegment", "docstring": "This is a segment without any subsegments.", "methods": ["__init__", "__repr__", "__setattr__", "is_code", "is_comment", "is_whitespace", "raw", "raw_upper", "raw_segments", "class_types", "source_fixes", "invalidate_caches", "get_type", "is_type", "get_raw_segments", "raw_trimmed", "normalize", "raw_normalized", "stringify", "_suffix", "edit", "_get_raw_segment_kwargs", "from_result_segments"], "attributes": ["type", "_is_code", "_is_comment", "_is_whitespace", "_default_raw"], "code_location": {"file": "raw.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 16, "end_line": 300}, "type": "class"}, {"name": "test__parser__base_segments_raw", "is_method": false, "class_name": null, "parameters": ["raw_seg"], "calls": ["str", "repr", "raw_seg.stringify", "raw_seg.to_tuple", "raw_seg.to_tuple"], "code_location": {"file": "segments_base_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser/segments", "start_line": 196, "end_line": 211}, "code_snippet": "def test__parser__base_segments_raw(raw_seg):\n    \"\"\"Test raw segments behave as expected.\"\"\"\n    # Check Segment Return\n    assert raw_seg.segments == ()\n    assert raw_seg.raw == \"foobar\"\n    # Check Formatting and Stringification\n    assert str(raw_seg) == repr(raw_seg) == \"<CodeSegment: ([L:  1, P:  1]) 'foobar'>\"\n    assert (\n        raw_seg.stringify(ident=1, tabsize=2)\n        == \"[L:  1, P:  1]      |  raw:                                                \"\n        \"        'foobar'\\n\"\n    )\n    # Check tuple\n    assert raw_seg.to_tuple() == (\"raw\", ())\n    # Check tuple\n    assert raw_seg.to_tuple(show_raw=True) == (\"raw\", \"foobar\")\n", "type": "function"}, {"name": "test__parser__base_segments_file", "is_method": false, "class_name": null, "parameters": ["raw_segments"], "calls": ["BaseFileSegment"], "code_location": {"file": "segments_file_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser/segments", "start_line": 6, "end_line": 12}, "code_snippet": "def test__parser__base_segments_file(raw_segments):\n    \"\"\"Test BaseFileSegment to behave as expected.\"\"\"\n    base_seg = BaseFileSegment(raw_segments, fname=\"/some/dir/file.sql\")\n    assert base_seg.type == \"file\"\n    assert base_seg.file_path == \"/some/dir/file.sql\"\n    assert base_seg.can_start_end_non_code\n    assert base_seg.allow_empty\n", "type": "function"}, {"name": "test__parser__raw_get_raw_segments", "is_method": false, "class_name": null, "parameters": ["raw_segments"], "calls": ["s.get_raw_segments"], "code_location": {"file": "segments_raw_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser/segments", "start_line": 6, "end_line": 9}, "code_snippet": "def test__parser__raw_get_raw_segments(raw_segments):\n    \"\"\"Test niche case of calling get_raw_segments on a raw segment.\"\"\"\n    for s in raw_segments:\n        assert s.get_raw_segments() == [s]\n", "type": "function"}, {"name": "test__parser__base_segments_raw_compare", "is_method": false, "class_name": null, "parameters": [], "calls": ["TemplatedFile.from_string", "RawSegment", "RawSegment", "PositionMarker", "PositionMarker", "slice", "slice", "slice", "slice"], "code_location": {"file": "segments_base_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser/segments", "start_line": 245, "end_line": 250}, "code_snippet": "def test__parser__base_segments_raw_compare():\n    \"\"\"Test comparison of raw segments.\"\"\"\n    template = TemplatedFile.from_string(\"foobar\")\n    rs1 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n    rs2 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n    assert rs1 == rs2\n", "type": "function"}, {"name": "CodeSegment", "docstring": "An alias for RawSegment.\n\nThis has a more explicit name for segment creation.", "methods": [], "attributes": [], "code_location": {"file": "common.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 11, "end_line": 17}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "BaseParser", "parameters": ["self", "raw_class", "type", "optional", "trim_chars", "casefold"], "calls": ["uuid4"], "code_location": {"file": "parsers.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 28, "end_line": 46}, "code_snippet": "    def __init__(\n        self,\n        raw_class: type[RawSegment],\n        type: Optional[str] = None,\n        optional: bool = False,\n        # The following kwargs are passed on to the segment:\n        trim_chars: Optional[tuple[str, ...]] = None,\n        casefold: Optional[Callable[[str], str]] = None,\n    ) -> None:\n        self.raw_class = raw_class\n        # Store instance_types rather than just type to allow\n        # for multiple possible types to be supported in derivative\n        # classes.\n        self._instance_types: tuple[str, ...] = (type or raw_class.type,)\n        self.optional = optional\n        self._trim_chars = trim_chars\n        self.casefold = casefold\n        # Generate a cache key\n        self._cache_key = uuid4().hex\n", "type": "function"}, {"name": "test__parser__base_segments_copy_isolation", "is_method": false, "class_name": null, "parameters": ["DummySegment", "raw_segments"], "calls": ["a_seg.copy", "DummySegment", "b_seg.copy", "LintFix"], "code_location": {"file": "segments_base_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser/segments", "start_line": 280, "end_line": 309}, "code_snippet": "def test__parser__base_segments_copy_isolation(DummySegment, raw_segments):\n    \"\"\"Test copy isolation in BaseSegment.\n\n    First on one of the raws and then on the dummy segment.\n    \"\"\"\n    # On a raw\n    a_seg = raw_segments[0]\n    a_copy = a_seg.copy()\n    assert a_seg is not a_copy\n    assert a_seg == a_copy\n    assert a_seg.pos_marker is a_copy.pos_marker\n    a_copy.pos_marker = None\n    assert a_copy.pos_marker is None\n    assert a_seg.pos_marker is not None\n\n    # On a base\n    b_seg = DummySegment(segments=raw_segments)\n    b_copy = b_seg.copy()\n    assert b_seg is not b_copy\n    assert b_seg == b_copy\n    assert b_seg.pos_marker is b_copy.pos_marker\n    b_copy.pos_marker = None\n    assert b_copy.pos_marker is None\n    assert b_seg.pos_marker is not None\n\n    # On addition to a lint Fix\n    fix = LintFix(\"replace\", a_seg, [b_seg])\n    for s in fix.edit:\n        assert not s.pos_marker\n    assert b_seg.pos_marker\n", "type": "function"}, {"name": "test__parser__raw_segment_raw_normalized", "is_method": false, "class_name": null, "parameters": [], "calls": ["TemplatedFile.from_string", "RawSegment", "RawSegment", "RawSegment", "BaseSegment", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "rs1.raw_normalized", "rs1.raw_normalized", "rs2.raw_normalized", "rs2.raw_normalized", "rs3.raw_normalized", "rs3.raw_normalized", "bs1.raw_normalized", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice"], "code_location": {"file": "segments_base_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser/segments", "start_line": 332, "end_line": 371}, "code_snippet": "def test__parser__raw_segment_raw_normalized():\n    \"\"\"Test comparison of raw segments.\"\"\"\n    template = TemplatedFile.from_string('\"a\"\"\".\"e\"')\n    rs1 = RawSegment(\n        '\"a\"\"\"',\n        PositionMarker(slice(0, 5), slice(0, 5), template),\n        quoted_value=(r'\"((?:[^\"]|\"\")*)\"', 1),\n        escape_replacements=[('\"\"', '\"')],\n        casefold=str.upper,\n    )\n    rs2 = RawSegment(\n        \".\",\n        PositionMarker(slice(6, 7), slice(6, 7), template),\n    )\n    rs3 = RawSegment(\n        '\"e\"',\n        PositionMarker(slice(8, 10), slice(8, 10), template),\n        quoted_value=(r'\"((?:[^\"]|\"\")*)\"', 1),\n        escape_replacements=[('\"\"', '\"')],\n        casefold=str.upper,\n    )\n    bs1 = BaseSegment(\n        (\n            rs1,\n            rs2,\n            rs3,\n        ),\n        PositionMarker(slice(0, 10), slice(0, 10), template),\n    )\n    assert rs1.raw == '\"a\"\"\"'\n    assert rs1.raw_normalized(False) == 'a\"'\n    assert rs1.raw_normalized() == 'A\"'\n    assert rs2.raw == \".\"\n    assert rs2.raw_normalized(False) == \".\"\n    assert rs2.raw_normalized() == \".\"\n    assert rs3.raw == '\"e\"'\n    assert rs3.raw_normalized(False) == \"e\"\n    assert rs3.raw_normalized() == \"E\"\n    assert bs1.raw == '\"a\"\"\".\"e\"'\n    assert bs1.raw_normalized() == 'A\".E'\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0852298736572266}
{"question": "What is the role of the Lexer class in SQLFluff's parsing pipeline?", "answer": null, "relative_code_list": null, "ground_truth": "The Lexer class serves as the second stage in SQLFluff's parsing pipeline, responsible for breaking down SQL input into atomic tokens. Its key roles include: 1) Tokenization - takes SQL input (either raw strings or TemplatedFile objects) and separates it into individual segments of whitespace and code, producing a flat sequence of typed segments (all subclasses of RawSegment); 2) Template integration - handles templated SQL by mapping lexed elements to template slices, allowing rule violations to be backported to original templated sections; 3) Dialect-specific lexing - uses dialect-specific lexer matchers (StringLexer and RegexLexer) to recognize tokens according to the specified SQL dialect; 4) Error handling - identifies unlexable content and packages it as UnlexableSegment, generating SQLLexError violations for problematic input; 5) Block tracking - manages templating blocks using BlockTracker to match opening and closing tags for proper template processing; 6) Position tracking - maintains position markers for each token to enable accurate error reporting and source mapping; 7) Matcher coordination - coordinates multiple lexer matchers in priority order, using a last-resort lexer (RegexLexer) for unmatched content; 8) Segment generation - converts lexed elements into RawSegment objects that form the input for the parser stage; 9) Template slice mapping - maps lexed elements to their corresponding template slices for proper source location tracking; 10) Pipeline integration - provides the foundation for the parser stage by creating the atomic building blocks that will be assembled into the parse tree.", "score": null, "retrieved_content": [{"name": "Lexer", "docstring": "The Lexer class actually does the lexing step.", "methods": ["__init__", "lex", "elements_to_segments", "violations_from_segments", "lex_match", "map_template_slices"], "attributes": [], "code_location": {"file": "lexer.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 726, "end_line": 889}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "Lexer", "parameters": ["self", "config", "last_resort_lexer", "dialect"], "calls": ["get_lexer_matchers", "ValueError", "FluffConfig.from_kwargs", "RegexLexer", "self.config.get"], "code_location": {"file": "lexer.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 729, "end_line": 748}, "code_snippet": "    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        last_resort_lexer: Optional[StringLexer] = None,\n        dialect: Optional[str] = None,\n    ):\n        if config and dialect:\n            raise ValueError(  # pragma: no cover\n                \"Lexer does not support setting both `config` and `dialect`.\"\n            )\n        # Use the provided config or create one from the dialect.\n        self.config = config or FluffConfig.from_kwargs(dialect=dialect)\n        # Store the matchers\n        self.lexer_matchers = self.config.get(\"dialect_obj\").get_lexer_matchers()\n\n        self.last_resort_lexer = last_resort_lexer or RegexLexer(\n            \"<unlexable>\",\n            r\"[^\\t\\n\\ ]*\",\n            UnlexableSegment,\n        )\n", "type": "function"}, {"name": "SQLLexError", "docstring": "An error which occurred during lexing.\n\nArgs:\n    pos (:obj:`PosMarker`, optional): The position which the error\n        occurred at.", "methods": [], "attributes": ["_code", "_identifier"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 172, "end_line": 182}, "type": "class"}, {"name": "test__parser__lexer_obj", "is_method": false, "class_name": null, "parameters": ["raw", "res", "caplog"], "calls": ["pytest.mark.parametrize", "Lexer", "caplog.at_level", "lex.lex", "FluffConfig"], "code_location": {"file": "lexer_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser", "start_line": 66, "end_line": 71}, "code_snippet": "def test__parser__lexer_obj(raw, res, caplog):\n    \"\"\"Test the lexer splits as expected in a selection of cases.\"\"\"\n    lex = Lexer(config=FluffConfig(overrides={\"dialect\": \"ansi\"}))\n    with caplog.at_level(logging.DEBUG):\n        lexing_segments, _ = lex.lex(raw)\n        assert [seg.raw for seg in lexing_segments] == res\n", "type": "function"}, {"name": "test__api__lexer", "is_method": false, "class_name": null, "parameters": [], "calls": ["lex", "isinstance", "Lexer"], "code_location": {"file": "classes_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 8, "end_line": 14}, "code_snippet": "def test__api__lexer():\n    \"\"\"Basic checking of lexing functionality.\"\"\"\n    tokens, violations = Lexer(dialect=\"ansi\").lex(test_query)\n    assert violations == []\n    assert isinstance(tokens, tuple)\n    # The last element is the file end marker.\n    assert [elem.raw for elem in tokens] == [\"SELECt\", \" \", \"1\", \"\"]\n", "type": "function"}, {"name": "_LexerSlicingTemplateFileCase", "docstring": "", "methods": [], "attributes": [], "code_location": {"file": "lexer_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser", "start_line": 316, "end_line": 327}, "type": "class"}, {"name": "SQLFluffDomain", "docstring": "SQLFluff domain.", "methods": ["get_full_qualified_name", "get_objects", "resolve_xref", "add_rule"], "attributes": ["name", "label", "object_types", "roles", "directives", "initial_data"], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 82, "end_line": 142}, "type": "class"}, {"name": "_LexerSlicingCase", "docstring": "", "methods": [], "attributes": [], "code_location": {"file": "lexer_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser", "start_line": 175, "end_line": 185}, "type": "class"}, {"name": "test__parser__lexer_fail", "is_method": false, "class_name": null, "parameters": [], "calls": ["Lexer", "lex.lex", "isinstance", "len", "FluffConfig"], "code_location": {"file": "lexer_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser", "start_line": 126, "end_line": 135}, "code_snippet": "def test__parser__lexer_fail():\n    \"\"\"Test the how the lexer fails and reports errors.\"\"\"\n    lex = Lexer(config=FluffConfig(overrides={\"dialect\": \"ansi\"}))\n\n    _, vs = lex.lex(\"Select \\u0394\")\n\n    assert len(vs) == 1\n    err = vs[0]\n    assert isinstance(err, SQLLexError)\n    assert err.line_pos == 8\n", "type": "function"}, {"name": "test__parser__lexer_regex", "is_method": false, "class_name": null, "parameters": ["raw", "reg", "res", "caplog"], "calls": ["pytest.mark.parametrize", "RegexLexer", "caplog.at_level", "assert_matches"], "code_location": {"file": "lexer_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser", "start_line": 106, "end_line": 110}, "code_snippet": "def test__parser__lexer_regex(raw, reg, res, caplog):\n    \"\"\"Test the RegexLexer.\"\"\"\n    matcher = RegexLexer(\"test\", reg, CodeSegment)\n    with caplog.at_level(logging.DEBUG):\n        assert_matches(raw, matcher, res)\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1171314716339111}
{"question": "Why does SQLFluff implement a noqa comment system for rule suppression?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff implements a noqa comment system for rule suppression to provide fine-grained control over linting behavior and accommodate legitimate exceptions to rules. Key reasons include: 1) Selective suppression - Allows developers to suppress specific rules for specific lines or sections where the rule doesn't apply; 2) False positive handling - Enables suppression of rules that generate false positives in legitimate edge cases; 3) Legacy code support - Allows gradual adoption of SQLFluff by suppressing rules for existing code that doesn't meet new standards; 4) Context-specific exceptions - Enables suppression when business logic or external constraints require deviations from standard formatting; 5) Gradual migration - Supports incremental adoption of SQLFluff by allowing selective rule suppression during migration; 6) Documentation - Noqa comments serve as documentation explaining why certain rules are suppressed; 7) Team flexibility - Allows teams to maintain their own standards while using SQLFluff's core functionality; 8) Integration support - Enables SQLFluff to work with existing codebases that may have legitimate deviations from standard formatting; 9) Rule testing - Allows developers to test specific rules by suppressing others; 10) Compliance requirements - Enables compliance with organizational or regulatory requirements that may conflict with certain linting rules.", "score": null, "retrieved_content": [{"name": "test_linter_noqa", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "lntr.lint_string", "result.get_violations", "FluffConfig"], "code_location": {"file": "noqa_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 376, "end_line": 417}, "code_snippet": "def test_linter_noqa():\n    \"\"\"Test \"noqa\" feature at the higher \"Linter\" level.\"\"\"\n    lntr = Linter(\n        config=FluffConfig(\n            overrides={\n                \"dialect\": \"bigquery\",  # Use bigquery to allow hash comments.\n                \"rules\": \"AL02, LT04\",\n            }\n        )\n    )\n    sql = \"\"\"\n    SELECT\n        col_a a,\n        col_b b, --noqa: disable=AL02\n        col_c c,\n        col_d d, --noqa: enable=AL02\n        col_e e,\n        col_f f,\n        col_g g,  --noqa\n        col_h h,\n        col_i i, --noqa:AL02\n        col_j j,\n        col_k k, --noqa:AL03\n        col_l l,\n        col_m m,\n        col_n n, --noqa: disable=all\n        col_o o,\n        col_p p, --noqa: enable=all\n        col_q q, --Inline comment --noqa: AL02\n        col_r r, /* Block comment */ --noqa: AL02\n        col_s s # hash comment --noqa: AL02\n        -- We trigger both AL02 (implicit aliasing)\n        -- and LT04 (leading commas) here to\n        -- test glob ignoring of multiple rules.\n        , col_t t --noqa: L01*\n        , col_u u -- Some comment --noqa: L01*\n        , col_v v -- We can ignore both AL02 and LT04 -- noqa: L01[29]\n    FROM foo\n        \"\"\"\n    result = lntr.lint_string(sql)\n    violations = result.get_violations()\n    assert {3, 6, 7, 8, 10, 12, 13, 14, 15, 18} == {v.line_no for v in violations}\n", "type": "function"}, {"name": "test_linter_disable_noqa_except", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "Linter", "lntr_disable_noqa_except_al02.lint_string", "result_disable_noqa_except_al02.get_violations", "lntr_disable_noqa_except_core.lint_string", "result_disable_noqa_except_core.get_violations", "len", "len", "FluffConfig", "FluffConfig"], "code_location": {"file": "noqa_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 556, "end_line": 601}, "code_snippet": "def test_linter_disable_noqa_except():\n    \"\"\"Test \"noqa\" comments can be disabled via the config.\"\"\"\n    lntr_disable_noqa_except_al02 = Linter(\n        config=FluffConfig(\n            overrides={\n                \"disable_noqa_except\": \"AL02\",\n                \"rules\": \"AL02, CP01\",\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    lntr_disable_noqa_except_core = Linter(\n        config=FluffConfig(\n            overrides={\n                \"disable_noqa_except\": \"core\",\n                \"rules\": \"AL02, CP01\",\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    # This query raises AL02 and CP01 but it is being suppressed by the inline noqa\n    # comments. We can partially ignore these comment by setting\n    # disable_noqa_except = rule_list in the config or by using the\n    # --disable-noqa-except option in the CLI.\n    sql = \"\"\"\n    SELECT\n    col_a a, --noqa: AL02\n    col_b b --noqa: aliasing\n    from foo; --noqa: CP01\n    \"\"\"\n\n    # Verify that noqa comment is ignored with\n    # disable_noqa_except = AL02 (base rule name).\n    result_disable_noqa_except_al02 = lntr_disable_noqa_except_al02.lint_string(sql)\n    violations_disable_noqa_except_al02 = (\n        result_disable_noqa_except_al02.get_violations()\n    )\n    assert len(violations_disable_noqa_except_al02) == 1\n    assert violations_disable_noqa_except_al02[0].rule.code == \"CP01\"\n\n    # Verify that noqa works as expected with disable_noqa_except = core (rule alias).\n    result_disable_noqa_except_core = lntr_disable_noqa_except_core.lint_string(sql)\n    violations_disable_noqa_except_core = (\n        result_disable_noqa_except_core.get_violations()\n    )\n    assert len(violations_disable_noqa_except_core) == 0\n", "type": "function"}, {"name": "test_linter_noqa_disable", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "Linter", "lntr_noqa_enabled.lint_string", "result_noqa_enabled.get_violations", "lntr_noqa_disabled.lint_string", "result_noqa_disabled.get_violations", "len", "len", "FluffConfig", "FluffConfig"], "code_location": {"file": "noqa_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 517, "end_line": 553}, "code_snippet": "def test_linter_noqa_disable():\n    \"\"\"Test \"noqa\" comments can be disabled via the config.\"\"\"\n    lntr_noqa_enabled = Linter(\n        config=FluffConfig(\n            overrides={\n                \"rules\": \"AL02\",\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    lntr_noqa_disabled = Linter(\n        config=FluffConfig(\n            overrides={\n                \"disable_noqa\": True,\n                \"rules\": \"AL02\",\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    # This query raises AL02, but it is being suppressed by the inline noqa comment.\n    # We can ignore this comment by setting disable_noqa = True in the config\n    # or by using the --disable-noqa flag in the CLI.\n    sql = \"\"\"\n    SELECT col_a a --noqa: AL02\n    FROM foo\n    \"\"\"\n\n    # Verify that noqa works as expected with disable_noqa = False (default).\n    result_noqa_enabled = lntr_noqa_enabled.lint_string(sql)\n    violations_noqa_enabled = result_noqa_enabled.get_violations()\n    assert len(violations_noqa_enabled) == 0\n\n    # Verify that noqa comment is ignored with disable_noqa = True.\n    result_noqa_disabled = lntr_noqa_disabled.lint_string(sql)\n    violations_noqa_disabled = result_noqa_disabled.get_violations()\n    assert len(violations_noqa_disabled) == 1\n    assert violations_noqa_disabled[0].rule.code == \"AL02\"\n", "type": "function"}, {"name": "test_linter_noqa_tmp", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "lntr.lint_string", "print", "result.get_violations", "result.tree.stringify", "FluffConfig"], "code_location": {"file": "noqa_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 497, "end_line": 514}, "code_snippet": "def test_linter_noqa_tmp():\n    \"\"\"Test \"noqa\" feature to ignore TMP at the higher \"Linter\" level.\"\"\"\n    lntr = Linter(\n        config=FluffConfig(\n            overrides={\n                \"exclude_rules\": \"LT13\",\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    sql = \"\"\"\nSELECT {{ col_a }} AS a -- noqa: TMP,PRS\nFROM foo;\n\"\"\"\n    result = lntr.lint_string(sql)\n    print(result.tree.stringify())\n    violations = result.get_violations()\n    assert not violations\n", "type": "function"}, {"name": "test_cli_no_disable_noqa_flag", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 2068, "end_line": 2076}, "code_snippet": "def test_cli_no_disable_noqa_flag():\n    \"\"\"Test that unset --disable-noqa flag respects inline noqa comments.\"\"\"\n    invoke_assert_code(\n        ret_code=0,\n        args=[\n            lint,\n            [\"test/fixtures/cli/disable_noqa_test.sql\"],\n        ],\n    )\n", "type": "function"}, {"name": "test_cli_disable_noqa_flag", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 2079, "end_line": 2092}, "code_snippet": "def test_cli_disable_noqa_flag():\n    \"\"\"Test that --disable-noqa flag ignores inline noqa comments.\"\"\"\n    invoke_assert_code(\n        ret_code=1,\n        args=[\n            lint,\n            [\n                \"test/fixtures/cli/disable_noqa_test.sql\",\n                \"--disable-noqa\",\n            ],\n        ],\n        # Linting error is raised even though it is inline ignored.\n        assert_stdout_contains=r\"L:   6 | P:  11 | CP01 |\",\n    )\n", "type": "function"}, {"name": "test_cli_disable_noqa_except_non_rules_flag", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 2113, "end_line": 2127}, "code_snippet": "def test_cli_disable_noqa_except_non_rules_flag():\n    \"\"\"Test that --disable-noqa-except flag ignores all inline noqa comments.\"\"\"\n    invoke_assert_code(\n        ret_code=1,\n        args=[\n            lint,\n            [\n                \"test/fixtures/cli/disable_noqa_test.sql\",\n                \"--disable-noqa-except\",\n                \"None\",\n            ],\n        ],\n        # Linting error is raised even though it is inline ignored.\n        assert_stdout_contains=r\"L:   6 | P:  11 | CP01 |\",\n    )\n", "type": "function"}, {"name": "test_linter_noqa_prs", "is_method": false, "class_name": null, "parameters": ["sql", "disable_noqa", "caplog"], "calls": ["pytest.mark.parametrize", "pytest.mark.parametrize", "Linter", "result.get_violations", "caplog.at_level", "lntr.lint_string", "FluffConfig"], "code_location": {"file": "noqa_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 471, "end_line": 494}, "code_snippet": "def test_linter_noqa_prs(sql, disable_noqa, caplog):\n    \"\"\"Test \"noqa\" feature to ignore PRS or TMP at the higher \"Linter\" level.\n\n    Because templating and parsing failures prevent a fully formed parse tree\n    to be formed the rely on slightly different routines to ensure ignores are\n    still applied.\n    \"\"\"\n    lntr = Linter(\n        config=FluffConfig(\n            overrides={\n                \"disable_noqa\": disable_noqa,\n                \"dialect\": \"ansi\",\n            }\n        )\n    )\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.linter\"):\n        result = lntr.lint_string(sql)\n    violations = result.get_violations()\n    # In both the templating fail and parsing fail cases, the failures should be\n    # ignored because of the inline ignore, _unless_ `disable_noqa`` is set.\n    if disable_noqa:\n        assert violations\n    else:\n        assert not violations\n", "type": "function"}, {"name": "test_cli_disable_noqa_except_flag", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 2095, "end_line": 2110}, "code_snippet": "def test_cli_disable_noqa_except_flag():\n    \"\"\"Test that --disable-noqa-except flag ignores inline noqa comments.\"\"\"\n    result = invoke_assert_code(\n        ret_code=1,\n        args=[\n            lint,\n            [\n                \"test/fixtures/cli/disable_noqa_test.sql\",\n                \"--disable-noqa-except\",\n                \"CP01\",\n            ],\n        ],\n        # Linting error is raised even though it is inline ignored.\n        assert_stdout_contains=r\"L:   8 | P:   5 | CP03 |\",\n    )\n    assert r\"L:   6 | P:  11 | CP01 |\" not in result.stdout\n", "type": "function"}, {"name": "test__linter__raises_malformed_noqa", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "lntr.lint_string_wrapped", "pytest.raises", "result.check_tuples"], "code_location": {"file": "noqa_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 23, "end_line": 29}, "code_snippet": "def test__linter__raises_malformed_noqa():\n    \"\"\"A badly formatted noqa gets raised as a parsing error.\"\"\"\n    lntr = Linter(dialect=\"ansi\")\n    result = lntr.lint_string_wrapped(\"select 1 --noqa missing semicolon\")\n\n    with pytest.raises(SQLParseError):\n        result.check_tuples()\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0869333744049072}
{"question": "Why does SQLFluff use a RuleSet-based approach for rule management?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff uses a RuleSet-based approach for rule management to provide centralized control, organization, and flexibility in handling linting rules. Key reasons include: 1) Centralized registration - RuleSet provides a single point for registering and managing all rules through the @ruleset.register decorator; 2) Configuration management - RuleSet validates rule configuration options and ensures they match predefined validation rules; 3) Rule filtering - Enables dynamic filtering of rules based on configuration settings (allowlisting/denylisting) through get_rulelist(); 4) Runtime instantiation - Rules are registered as classes at module load time but instantiated at runtime, allowing configuration values to be passed dynamically; 5) Metadata management - RuleSet maintains comprehensive metadata for each rule including code, name, description, groups, and aliases; 6) Naming convention enforcement - Enforces the Rule_XXXX naming convention and validates rule codes to prevent conflicts; 7) Group validation - Ensures all rules belong to required groups (like 'all') for proper categorization; 8) Plugin integration - Supports plugin-based rule registration, allowing custom rules to be added without modifying the core codebase; 9) Code collision prevention - Prevents duplicate rule codes from being registered, maintaining rule uniqueness; 10) Extensibility - Provides a framework for adding new rules and rule types while maintaining consistency and validation.", "score": null, "retrieved_content": [{"name": "SQLFluffRule", "docstring": "SQLFluff rule directive for sphinx.\n\nRule directives can be used as shown below.\n\n.. code-block:: rst\n\n    .. sqlfluff:rule:: AM01\n                       ambiguous.distinct\n\n        Write the documentation for the rule here.\n\nTo cross reference (i.e. refer to) objects defined like this\nboth the code and name reference is available:\n\n.. code-block:: rst\n\n    :sqlfluff:ref:`CP02`\n    :sqlfluff:ref:`capitalisation.identifiers`", "methods": ["handle_signature", "add_target_and_index", "_object_hierarchy_parts", "_toc_entry_name"], "attributes": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 10, "end_line": 79}, "type": "class"}, {"name": "get_rule_from_set", "is_method": false, "class_name": null, "parameters": ["code", "config"], "calls": ["ValueError", "get_rulepack", "get_ruleset", "get_ruleset"], "code_location": {"file": "rules.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/testing", "start_line": 82, "end_line": 87}, "code_snippet": "def get_rule_from_set(code: str, config: FluffConfig) -> BaseRule:\n    \"\"\"Fetch a rule from the rule set.\"\"\"\n    for r in get_ruleset().get_rulepack(config=config).rules:\n        if r.code == code:  # pragma: no cover\n            return r\n    raise ValueError(f\"{code!r} not in {get_ruleset()!r}\")\n", "type": "function"}, {"name": "get_rulepack", "is_method": true, "class_name": "Linter", "parameters": ["self", "config"], "calls": ["get_ruleset", "rs.get_rulepack", "rs.register"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 100, "end_line": 107}, "code_snippet": "    def get_rulepack(self, config: Optional[FluffConfig] = None) -> RulePack:\n        \"\"\"Get hold of a set of rules.\"\"\"\n        rs = get_ruleset()\n        # Register any user rules\n        for rule in self.user_rules:\n            rs.register(rule)\n        cfg = config or self.config\n        return rs.get_rulepack(config=cfg)\n", "type": "function"}, {"name": "_load_standard_rules", "is_method": false, "class_name": null, "parameters": [], "calls": ["RuleSet", "hook.get_rules", "get_config_info", "std_rule_set.register", "get_plugin_manager"], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 17, "end_line": 30}, "code_snippet": "def _load_standard_rules() -> RuleSet:\n    \"\"\"Initialise the standard ruleset.\n\n    We do this on each call so that dynamic rules changes\n    are possible.\n    \"\"\"\n    std_rule_set = RuleSet(name=\"standard\", config_info=get_config_info())\n\n    # Iterate through the rules list and register each rule with the standard set.\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            std_rule_set.register(rule)\n\n    return std_rule_set\n", "type": "function"}, {"name": "test_rules_name_validation", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.raises"], "code_location": {"file": "rules_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 326, "end_line": 336}, "code_snippet": "def test_rules_name_validation():\n    \"\"\"Ensure that rule names are validated.\"\"\"\n    with pytest.raises(SQLFluffUserError) as exc_info:\n\n        class RuleWithoutBadName_ZZ99(BaseRule):\n            \"\"\"A new rule without configuration.\"\"\"\n\n            name = \"MY-KEBAB-CASE-NAME\"\n\n    assert \"Tried to define rule with unexpected name\" in exc_info.value.args[0]\n    assert \"MY-KEBAB-CASE-NAME\" in exc_info.value.args[0]\n", "type": "function"}, {"name": "Rule_LT11", "docstring": "Set operators should be surrounded by newlines.\n\n**Anti-pattern**\n\nIn this example, `UNION ALL` is not on a line itself.\n\n.. code-block:: sql\n\n    SELECT 'a' AS col UNION ALL\n    SELECT 'b' AS col\n\n**Best practice**\n\n.. code-block:: sql\n\n    SELECT 'a' AS col\n    UNION ALL\n    SELECT 'b' AS col", "methods": ["_eval"], "attributes": ["name", "aliases", "groups", "is_fix_compatible", "crawl_behaviour"], "code_location": {"file": "LT11.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 8, "end_line": 52}, "type": "class"}, {"name": "test__rules__rule_selection", "is_method": false, "class_name": null, "parameters": ["rules", "exclude_rules", "resulting_codes"], "calls": ["pytest.mark.parametrize", "FluffConfig", "Linter", "set", "RootOnlyCrawler", "linter.rule_tuples"], "code_location": {"file": "rules_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 154, "end_line": 197}, "code_snippet": "def test__rules__rule_selection(rules, exclude_rules, resulting_codes):\n    \"\"\"Test that rule selection works by various means.\"\"\"\n\n    class Rule_T010(BaseRule):\n        \"\"\"Fake Basic Rule.\"\"\"\n\n        groups = (\"all\", \"test\")\n        name = \"fake_basic\"\n        aliases = (\"fb1\", \"foo\")  # NB: Foo is a group on another rule.\n        crawl_behaviour = RootOnlyCrawler()\n\n        def _eval(self, **kwargs):\n            pass\n\n    class Rule_T011(Rule_T010):\n        \"\"\"Fake Basic Rule.\n\n        NOTE: We inherit crawl behaviour and _eval from above.\n        \"\"\"\n\n        groups = (\"all\", \"test\", \"foo\")\n        name = \"fake_other\"\n        aliases = (\"fb2\",)\n\n    class Rule_T012(Rule_T010):\n        \"\"\"Fake Basic Rule.\n\n        NOTE: We inherit crawl behaviour and _eval from above.\n        \"\"\"\n\n        # NB: \"fake_other\" is the name of another rule.\n        groups = (\"all\", \"foo\", \"fake_other\")\n        # No aliases, Name collides with the alias of another rule.\n        name = \"fake_again\"\n        aliases = ()\n\n    cfg = FluffConfig(\n        overrides={\"rules\": rules, \"exclude_rules\": exclude_rules, \"dialect\": \"ansi\"}\n    )\n    linter = Linter(config=cfg, user_rules=[Rule_T010, Rule_T011, Rule_T012])\n    # Get the set of selected codes:\n    selected_codes = set(tpl[0] for tpl in linter.rule_tuples())\n    # Check selected rules\n    assert selected_codes == resulting_codes\n", "type": "function"}, {"name": "get_rulepack", "is_method": true, "class_name": "RuleSet", "parameters": ["self", "config"], "calls": ["self._validate_config_options", "config.get_section", "set", "self.rule_reference_map", "any", "any", "sorted", "self._expand_rule_refs", "self._expand_rule_refs", "RulePack", "self._register.keys", "manifest.rule_class.get_config_ref", "rules_logger.warning", "config.get", "list", "config.get", "rules_logger.warning", "rules_logger.warning", "self._register.keys", "rule_class.get_config_ref", "config.get_section", "description.format", "instantiated_rules.append", "self._register.values", "rules_config.items", "isinstance", "format", "format", "rules_config.items", "kwargs.update", "self._validate_config_options", "kwargs.update", "rule_class", "len", "rule_class.get_config_ref", "rules_logger.warning", "fnmatch.filter", "fnmatch.filter", "isinstance", "list", "rules_logger.warning", "reference_map.keys", "reference_map.keys"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 1078, "end_line": 1207}, "code_snippet": "    def get_rulepack(self, config: \"FluffConfig\") -> RulePack:\n        \"\"\"Use the config to return the appropriate rules.\n\n        We use the config both for allowlisting and denylisting, but also\n        for configuring the rules given the given config.\n        \"\"\"\n        # Validate all generic rule configs\n        self._validate_config_options(config)\n\n        # Fetch config section:\n        rules_config = config.get_section(\"rules\")\n\n        # Generate the master reference map. The priority order is:\n        # codes > names > groups > aliases\n        # (i.e. if there's a collision between a name and an\n        # alias - we assume the alias is wrong.)\n        valid_codes: set[str] = set(self._register.keys())\n        reference_map = self.rule_reference_map()\n        valid_config_lookups = {\n            manifest.rule_class.get_config_ref() for manifest in self._register.values()\n        }\n\n        # Validate config doesn't try to specify values for unknown rules.\n        # NOTE: We _warn_ here rather than error.\n        for unexpected_ref in [\n            # Filtering to dicts gives us the sections.\n            k\n            for k, v in rules_config.items()\n            if isinstance(v, dict)\n            # Only keeping ones we don't expect\n            if k not in valid_config_lookups\n        ]:\n            rules_logger.warning(\n                \"Rule configuration contain a section for unexpected \"\n                f\"rule {unexpected_ref!r}. These values will be ignored.\"\n            )\n            # For convenience (and migration), if we do find a potential match\n            # for the reference - add that as a warning.\n            # NOTE: We don't actually accept config in these cases, even though\n            # we could potentially match - because how to resolve _multiple_\n            # matching config sections is ambiguous.\n            if unexpected_ref in reference_map:\n                referenced_codes = reference_map[unexpected_ref]\n                if len(referenced_codes) == 1:\n                    referenced_code = list(referenced_codes)[0]\n                    referenced_name = self._register[referenced_code].name\n                    config_ref = self._register[\n                        referenced_code\n                    ].rule_class.get_config_ref()\n                    rules_logger.warning(\n                        \"The reference was however found as a match for rule \"\n                        f\"{referenced_code} with name {referenced_name!r}. \"\n                        \"SQLFluff assumes configuration for this rule will \"\n                        f\"be specified in 'sqlfluff:rules:{config_ref}'.\"\n                    )\n                elif referenced_codes:\n                    rules_logger.warning(\n                        \"The reference was found as a match for multiple rules: \"\n                        f\"{referenced_codes}. Config should be specified by the \"\n                        \"name of the relevant rule e.g. \"\n                        \"'sqlfluff:rules:capitalisation.keywords'.\"\n                    )\n\n        # The lists here are lists of references, which might be codes,\n        # names, aliases or groups.\n        # We default the allowlist to all the rules if not set (i.e. not specifying\n        # any rules, just means \"all the rules\").\n        allowlist = config.get(\"rule_allowlist\") or list(valid_codes)\n        denylist = config.get(\"rule_denylist\") or []\n\n        allowlisted_unknown_rule_codes = [\n            r\n            for r in allowlist\n            # Add valid groups to the register when searching for invalid rules _only_\n            if not fnmatch.filter(reference_map.keys(), r)\n        ]\n        if any(allowlisted_unknown_rule_codes):\n            rules_logger.warning(\n                \"Tried to allowlist unknown rule references: {!r}\".format(\n                    allowlisted_unknown_rule_codes\n                )\n            )\n\n        denylisted_unknown_rule_codes = [\n            r for r in denylist if not fnmatch.filter(reference_map.keys(), r)\n        ]\n        if any(denylisted_unknown_rule_codes):  # pragma: no cover\n            rules_logger.warning(\n                \"Tried to denylist unknown rules references: {!r}\".format(\n                    denylisted_unknown_rule_codes\n                )\n            )\n\n        keylist = sorted(self._register.keys())\n\n        # First we expand the allowlist and denylist globs\n        expanded_allowlist = self._expand_rule_refs(allowlist, reference_map)\n        expanded_denylist = self._expand_rule_refs(denylist, reference_map)\n\n        # Then we filter the rules\n        keylist = [\n            r for r in keylist if r in expanded_allowlist and r not in expanded_denylist\n        ]\n\n        # Construct the kwargs for each rule and instantiate in turn.\n        instantiated_rules = []\n        # Keep only config which isn't a section (for specific rule) (i.e. isn't a dict)\n        # We'll handle those directly in the specific rule config section below.\n        generic_rule_config = {\n            k: v for k, v in rules_config.items() if not isinstance(v, dict)\n        }\n        for code in keylist:\n            kwargs = {}\n            rule_class = self._register[code].rule_class\n            # Fetch the lookup code for the rule.\n            rule_config_ref = rule_class.get_config_ref()\n            specific_rule_config = config.get_section((\"rules\", rule_config_ref))\n            if generic_rule_config:\n                kwargs.update(generic_rule_config)\n            if specific_rule_config:\n                # Validate specific rule config before adding\n                self._validate_config_options(config, rule_config_ref)\n                kwargs.update(specific_rule_config)\n            kwargs[\"code\"] = code\n            # Allow variable substitution in making the description\n            kwargs[\"description\"] = self._register[code].description.format(**kwargs)\n            # Instantiate when ready\n            instantiated_rules.append(rule_class(**kwargs))\n\n        return RulePack(instantiated_rules, reference_map)\n", "type": "function"}, {"name": "test__config__rules_set_to_none", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "lntr.lint_path", "lnt.check_tuples_by_path", "FluffConfig.from_path"], "code_location": {"file": "fluffconfig_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/config", "start_line": 145, "end_line": 158}, "code_snippet": "def test__config__rules_set_to_none():\n    \"\"\"Test linting when rules are set to 'None'.\n\n    Ensure that all rules are still run.\n    \"\"\"\n    lntr = Linter(\n        config=FluffConfig.from_path(\"test/fixtures/config/rules_set_to_none\")\n    )\n    lnt = lntr.lint_path(\"test/fixtures/config/rules_set_to_none/test.sql\")\n    violations = lnt.check_tuples_by_path()\n    for k in violations:\n        assert (\"LT13\", 1, 1) in violations[k]\n        assert (\"AM04\", 12, 1) in violations[k]\n        assert (\"CP01\", 12, 10) in violations[k]\n", "type": "function"}, {"name": "test_rule_set_return_informative_error_when_rule_not_registered", "is_method": false, "class_name": null, "parameters": [], "calls": ["FluffConfig", "e.match", "pytest.raises", "get_rule_from_set"], "code_location": {"file": "rules_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 402, "end_line": 408}, "code_snippet": "def test_rule_set_return_informative_error_when_rule_not_registered():\n    \"\"\"Assert that a rule that throws an exception returns it as a validation.\"\"\"\n    cfg = FluffConfig(overrides={\"dialect\": \"ansi\"})\n    with pytest.raises(ValueError) as e:\n        get_rule_from_set(\"L000\", config=cfg)\n\n    e.match(\"'L000' not in\")\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.100903034210205}
{"question": "Why does SQLFluff include a template processing system for dynamic SQL files?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff includes a template processing system for dynamic SQL files to support modern SQL development practices where SQL code is often generated or templated. Key reasons include: 1) Modern SQL workflows - Many organizations use templating engines like Jinja, dbt, or Python format strings to generate dynamic SQL; 2) Code reusability - Templates allow SQL code to be parameterized and reused across different contexts; 3) Environment-specific SQL - Templates enable SQL to be adapted for different environments (dev, staging, production) with different parameters; 4) Dynamic queries - Many applications generate SQL queries dynamically based on user input or application state; 5) Framework integration - Tools like dbt, Apache Airflow, and other data engineering frameworks heavily use templated SQL; 6) Linting accuracy - To properly lint templated SQL, SQLFluff must first render the templates to see the actual SQL that will be executed; 7) Source mapping - The template system maintains mapping between the original templated code and the rendered SQL for accurate error reporting; 8) Conditional logic - Templates often include conditional statements that generate different SQL based on parameters; 9) Loop constructs - Templates support loops that generate repetitive SQL structures; 10) Variable substitution - Templates allow for dynamic variable substitution while maintaining SQL syntax validation.", "score": null, "retrieved_content": [{"name": "render_string", "is_method": true, "class_name": "Linter", "parameters": ["self", "in_str", "fname", "config", "encoding"], "calls": ["linter_logger.info", "time.monotonic", "self._normalise_newlines", "config.verify_dialect_specified", "config.get", "linter_logger.info", "RenderedFile", "linter_logger.warning", "self.templater.process_with_variants", "linter_logger.info", "len", "config.get", "templater_violations.append", "linter_logger.warning", "time.monotonic", "templated_variants.append", "len", "str", "config.get"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 822, "end_line": 893}, "code_snippet": "    def render_string(\n        self, in_str: str, fname: str, config: FluffConfig, encoding: str\n    ) -> RenderedFile:\n        \"\"\"Template the file.\"\"\"\n        linter_logger.info(\"Rendering String [%s] (%s)\", self.templater.name, fname)\n\n        # Start the templating timer\n        t0 = time.monotonic()\n\n        # Newlines are normalised to unix-style line endings (\\n).\n        # The motivation is that Jinja normalises newlines during templating and\n        # we want consistent mapping between the raw and templated slices.\n        in_str = self._normalise_newlines(in_str)\n\n        # Since Linter.__init__() does not require a dialect to be specified,\n        # check for one now. (We're processing a string, not a file, so we're\n        # not going to pick up a .sqlfluff or other config file to provide a\n        # missing dialect at this point.)\n        config.verify_dialect_specified()\n        if not config.get(\"templater_obj\") == self.templater:\n            linter_logger.warning(\n                f\"Attempt to set templater to {config.get('templater_obj').name} \"\n                f\"failed. Using {self.templater.name} templater. Templater cannot \"\n                \"be set in a .sqlfluff file in a subdirectory of the current \"\n                \"working directory. It can be set in a .sqlfluff in the current \"\n                \"working directory. See Nesting section of the docs for more \"\n                \"details.\"\n            )\n\n        variant_limit = config.get(\"render_variant_limit\")\n        templated_variants: list[TemplatedFile] = []\n        templater_violations: list[SQLTemplaterError] = []\n\n        try:\n            for variant, templater_errs in self.templater.process_with_variants(\n                in_str=in_str, fname=fname, config=config, formatter=self.formatter\n            ):\n                if variant:\n                    templated_variants.append(variant)\n                # NOTE: We could very easily end up with duplicate errors between\n                # different variants and this code doesn't currently do any\n                # deduplication between them. That will be resolved in further\n                # testing.\n                # TODO: Resolve potential duplicate templater violations between\n                # variants before we enable jinja variant linting by default.\n                templater_violations += templater_errs\n                if len(templated_variants) >= variant_limit:\n                    # Stop if we hit the limit.\n                    break\n        except SQLTemplaterError as templater_err:\n            # Fatal templating error. Capture it and don't generate a variant.\n            templater_violations.append(templater_err)\n        except SQLFluffSkipFile as skip_file_err:  # pragma: no cover\n            linter_logger.warning(str(skip_file_err))\n\n        if not templated_variants:\n            linter_logger.info(\"TEMPLATING FAILED: %s\", templater_violations)\n\n        linter_logger.info(\"Rendered %s variants\", len(templated_variants))\n\n        # Record time\n        time_dict = {\"templating\": time.monotonic() - t0}\n\n        return RenderedFile(\n            templated_variants,\n            templater_violations,\n            config,\n            time_dict,\n            fname,\n            encoding,\n            in_str,\n        )\n", "type": "function"}, {"name": "TemplatedFile", "docstring": "A templated SQL file.\n\nThis is the response of a templaters .process() method\nand contains both references to the original file and also\nthe capability to split up that file when lexing.", "methods": ["__init__", "from_string", "__repr__", "__str__", "get_line_pos_of_char_pos", "_find_slice_indices_of_templated_pos", "raw_slices_spanning_source_slice", "templated_slice_to_source_slice", "is_source_slice_literal", "source_only_slices", "source_position_dict_from_slice"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 132, "end_line": 503}, "type": "class"}, {"name": "test__templater_jinja_large_file_check", "is_method": false, "class_name": null, "parameters": [], "calls": ["process", "process", "pytest.raises", "process", "str", "JinjaTemplater", "FluffConfig", "JinjaTemplater", "FluffConfig", "JinjaTemplater", "FluffConfig"], "code_location": {"file": "jinja_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/templaters", "start_line": 1685, "end_line": 1716}, "code_snippet": "def test__templater_jinja_large_file_check():\n    \"\"\"Test large file skipping.\n\n    The check is separately called on each .process() method\n    so it makes sense to test a few templaters.\n    \"\"\"\n    # First check we can process the file normally without specific config.\n    # i.e. check the defaults work and the default is high.\n    JinjaTemplater().process(\n        in_str=\"SELECT 1\",\n        fname=\"<string>\",\n        config=FluffConfig(overrides={\"dialect\": \"ansi\"}),\n    )\n    # Second check setting the value low disables the check\n    JinjaTemplater().process(\n        in_str=\"SELECT 1\",\n        fname=\"<string>\",\n        config=FluffConfig(\n            overrides={\"dialect\": \"ansi\", \"large_file_skip_char_limit\": 0}\n        ),\n    )\n    # Finally check we raise a skip exception when config is set low.\n    with pytest.raises(SQLFluffSkipFile) as excinfo:\n        JinjaTemplater().process(\n            in_str=\"SELECT 1\",\n            fname=\"<string>\",\n            config=FluffConfig(\n                overrides={\"dialect\": \"ansi\", \"large_file_skip_char_limit\": 2},\n            ),\n        )\n\n    assert \"Length of file\" in str(excinfo.value)\n", "type": "function"}, {"name": "process", "is_method": true, "class_name": "PythonTemplater", "parameters": ["self"], "calls": ["self.get_context", "self.slice_file", "TemplatedFile", "re.sub", "templater_logger.debug", "raw_str_with_dot_notation_hack.format", "SQLTemplaterError", "SQLTemplaterError", "SQLTemplaterError", "format", "format"], "code_location": {"file": "python.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 211, "end_line": 302}, "code_snippet": "    def process(\n        self,\n        *,\n        in_str: str,\n        fname: str,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n    ) -> tuple[TemplatedFile, list[SQLTemplaterError]]:\n        \"\"\"Process a string and return a TemplatedFile.\n\n        Note that the arguments are enforced as keywords\n        because Templaters can have differences in their\n        `process` method signature.\n        A Templater that only supports reading from a file\n        would need the following signature:\n            process(*, fname, in_str=None, config=None)\n        (arguments are swapped)\n\n        Args:\n            in_str (:obj:`str`): The input string.\n            fname (:obj:`str`, optional): The filename of this string. This is\n                mostly for loading config files at runtime.\n            config (:obj:`FluffConfig`): A specific config to use for this\n                templating operation. Only necessary for some templaters.\n            formatter (:obj:`CallbackFormatter`): Optional object for output.\n\n        \"\"\"\n        live_context = self.get_context(fname, config)\n\n        def render_func(raw_str: str) -> str:\n            \"\"\"Render the string using the captured live_context.\n\n            In order to support mocking of template variables\n            containing \".\" characters, this function converts any\n            template variable containing \".\" into a dictionary lookup.\n                Example:  {foo.bar} => {sqlfluff[foo.bar]}\n            \"\"\"\n            try:\n                # Hack to allow template variables with dot notation (e.g. foo.bar)\n                raw_str_with_dot_notation_hack = re.sub(\n                    r\"{([^:}]*\\.[^:}]*)(:\\S*)?}\", r\"{sqlfluff[\\1]\\2}\", raw_str\n                )\n                templater_logger.debug(\n                    \"    Raw String with Dot Notation Hack: %r\",\n                    raw_str_with_dot_notation_hack,\n                )\n                rendered_str = raw_str_with_dot_notation_hack.format(**live_context)\n            except KeyError as err:\n                missing_key = err.args[0]\n                if missing_key == \"sqlfluff\":\n                    # Give more useful error message related to dot notation hack\n                    # when user has not created the required, magic context key\n                    raise SQLTemplaterError(\n                        \"Failure in Python templating: magic key 'sqlfluff' \"\n                        \"missing from context.  This key is required \"\n                        \"for template variables containing '.'. \"\n                        \"https://docs.sqlfluff.com/en/stable/\"\n                        \"perma/python_templating.html\"\n                    )\n                elif \".\" in missing_key:\n                    # Give more useful error message related to dot notation hack\n                    # for missing keys\n                    raise SQLTemplaterError(\n                        \"Failure in Python templating: {} key missing from 'sqlfluff' \"\n                        \"dict in context. Template variables containing '.' are \"\n                        \"required to use the 'sqlfluff' magic fixed context key. \"\n                        \"https://docs.sqlfluff.com/en/stable/\"\n                        \"perma/python_templating.html\".format(err)\n                    )\n                else:\n                    raise SQLTemplaterError(\n                        \"Failure in Python templating: {}. Have you configured your \"\n                        \"variables? https://docs.sqlfluff.com/en/stable/\"\n                        \"perma/variables.html\".format(err)\n                    )\n            return rendered_str\n\n        raw_sliced, sliced_file, new_str = self.slice_file(\n            in_str,\n            render_func=render_func,\n            config=config,\n        )\n        return (\n            TemplatedFile(\n                source_str=in_str,\n                templated_str=new_str,\n                fname=fname,\n                sliced_file=sliced_file,\n                raw_sliced=raw_sliced,\n            ),\n            [],\n        )\n", "type": "function"}, {"name": "_run_templater_and_verify_result", "is_method": false, "class_name": null, "parameters": ["dbt_templater", "project_dir", "fname", "dbt_fluff_config", "dbt_project_folder"], "calls": ["FluffConfig", "dbt_templater.process", "_get_fixture_path", "Lexer", "lexer.lex", "str", "fixture_path.read_text", "Path", "path.read_text", "str"], "code_location": {"file": "templater_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/test", "start_line": 131, "end_line": 152}, "code_snippet": "def _run_templater_and_verify_result(\n    dbt_templater,\n    project_dir,\n    fname,\n    dbt_fluff_config,\n    dbt_project_folder,\n):\n    path = Path(project_dir) / \"models/my_new_project\" / fname\n    config = FluffConfig(configs=dbt_fluff_config)\n    templated_file, _ = dbt_templater.process(\n        in_str=path.read_text(),\n        fname=str(path),\n        config=config,\n    )\n    template_output_folder_path = dbt_project_folder / \"templated_output/\"\n    fixture_path = _get_fixture_path(template_output_folder_path, fname)\n    assert str(templated_file) == fixture_path.read_text()\n    # Check we can lex the output too.\n    # https://github.com/sqlfluff/sqlfluff/issues/4013\n    lexer = Lexer(config=config)\n    _, lexing_violations = lexer.lex(templated_file)\n    assert not lexing_violations\n", "type": "function"}, {"name": "test__context_in_config_is_loaded", "is_method": false, "class_name": null, "parameters": ["project_dir", "dbt_templater", "model_path", "var_value", "dbt_fluff_config"], "calls": ["pytest.mark.parametrize", "deepcopy", "FluffConfig", "dbt_templater.process", "Path", "str", "path.read_text", "str"], "code_location": {"file": "templater_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/test", "start_line": 646, "end_line": 667}, "code_snippet": "def test__context_in_config_is_loaded(\n    project_dir,\n    dbt_templater,\n    model_path,\n    var_value,\n    dbt_fluff_config,\n):\n    \"\"\"Test that variables inside .sqlfluff are passed to dbt.\"\"\"\n    context = {\"passed_through_cli\": var_value} if var_value else {}\n\n    config_dict = deepcopy(dbt_fluff_config)\n    config_dict[\"templater\"][\"dbt\"][\"context\"] = context\n    config = FluffConfig(config_dict)\n\n    path = Path(project_dir) / model_path\n\n    processed, violations = dbt_templater.process(\n        in_str=path.read_text(), fname=str(path), config=config\n    )\n\n    assert violations == []\n    assert str(var_value) in processed.templated_str\n", "type": "function"}, {"name": "test__templater_python_large_file_check", "is_method": false, "class_name": null, "parameters": [], "calls": ["process", "pytest.raises", "process", "str", "PythonTemplater", "PythonTemplater", "FluffConfig"], "code_location": {"file": "python_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/templaters", "start_line": 480, "end_line": 498}, "code_snippet": "def test__templater_python_large_file_check():\n    \"\"\"Test large file skipping.\n\n    The check is separately called on each .process() method\n    so it makes sense to test a few templaters.\n    \"\"\"\n    # First check we can process the file normally without config.\n    PythonTemplater().process(in_str=\"SELECT 1\", fname=\"<string>\")\n    # Then check we raise a skip exception when config is set low.\n    with pytest.raises(SQLFluffSkipFile) as excinfo:\n        PythonTemplater().process(\n            in_str=\"SELECT 1\",\n            fname=\"<string>\",\n            config=FluffConfig(\n                overrides={\"dialect\": \"ansi\", \"large_file_skip_char_limit\": 2},\n            ),\n        )\n\n    assert \"Length of file\" in str(excinfo.value)\n", "type": "function"}, {"name": "_unsafe_process", "is_method": true, "class_name": "DbtTemplater", "parameters": ["self", "fname", "in_str", "config", "dbt_dir"], "calls": ["os.getcwd", "os.path.relpath", "self._find_node", "templater_logger.debug", "dict", "save_ephemeral_nodes.items", "kwargs.get", "old_from_string", "set_task_contextvars", "type", "self.connection", "hasattr", "getattr", "templater_logger.debug", "templater_logger.debug", "templater_logger.debug", "templater_logger.debug", "setattr", "self.slice_file", "getattr", "TemplatedFile", "globals.get", "self.dbt_compiler.compile_node", "getattr", "SQLTemplaterError", "endswith", "set_task_contextvars", "self.dbt_manifest.nodes.items", "SQLTemplaterError", "SQLFluffSkipFile", "len", "len", "model.get", "str", "SQLTemplaterError", "source_dbt_sql.rstrip", "source_dbt_sql.rstrip", "env.add_extension", "env.from_string", "template.render", "cv_project_root.set", "getattr", "str", "len", "template.render"], "code_location": {"file": "templater.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt", "start_line": 648, "end_line": 884}, "code_snippet": "    def _unsafe_process(self, fname, in_str=None, config=None, dbt_dir=os.getcwd()):\n        original_file_path = os.path.relpath(fname, start=dbt_dir)\n\n        # Below, we monkeypatch Environment.from_string() to intercept when dbt\n        # compiles (i.e. runs Jinja) to expand the \"node\" corresponding to fname.\n        # We do this to capture the Jinja context at the time of compilation, i.e.:\n        # - Jinja Environment object\n        # - Jinja \"globals\" dictionary\n        #\n        # This info is captured by the \"make_template()\" function, which in\n        # turn is used by our parent class' (JinjaTemplater) slice_file()\n        # function.\n        old_from_string = Environment.from_string\n        # Start with render_func undefined. We need to know whether it has been\n        # overwritten.\n        render_func: Optional[Callable[[str], str]] = None\n\n        if self.dbt_version_tuple >= (1, 3):\n            compiled_sql_attribute = \"compiled_code\"\n            raw_sql_attribute = \"raw_code\"\n        else:  # pragma: no cover\n            compiled_sql_attribute = \"compiled_sql\"\n            raw_sql_attribute = \"raw_sql\"\n\n        def from_string(*args, **kwargs):\n            \"\"\"Replaces (via monkeypatch) the jinja2.Environment function.\"\"\"\n            nonlocal render_func\n            # Is it processing the node corresponding to fname?\n            globals = kwargs.get(\"globals\")\n            if globals:\n                model = globals.get(\"model\")\n                if model:\n                    if model.get(\"original_file_path\") == original_file_path:\n                        # Yes. Capture the important arguments and create\n                        # a render_func() closure with overwrites the variable\n                        # from within _unsafe_process when from_string is run.\n                        env = args[0]\n                        globals = args[2] if len(args) >= 3 else kwargs[\"globals\"]\n\n                        # Overwrite the outer render_func\n                        def render_func(in_str):\n                            env.add_extension(SnapshotExtension)\n                            template = env.from_string(in_str, globals=globals)\n                            if self.dbt_version_tuple >= (1, 8):\n                                # dbt 1.8 requires a context for rendering the template.\n                                return template.render(globals)\n                            return template.render()\n\n            return old_from_string(*args, **kwargs)\n\n        # NOTE: Inject the project root for dbt contextvars compatibility.\n        # Prefer dbt_common (latest), then fallback to dbt.events or dbt.task.\n        try:\n            from dbt_common.events.contextvars import set_task_contextvars\n\n            set_task_contextvars(project_root=self.project_dir)\n        except ImportError:\n            try:\n                from dbt.events.contextvars import set_task_contextvars\n\n                set_task_contextvars(project_root=self.project_dir)\n            except ImportError:\n                # NOTE: We need to inject the project root here in reaction to the\n                # breaking change upstream with dbt. Coverage works in 1.5.2, but\n                # appears to no longer be covered in 1.5.3.\n                # This change was backported and so exists in some versions\n                # but not others. When not present, no additional action is needed.\n                # https://github.com/dbt-labs/dbt-core/pull/7949\n                # On the 1.5.x branch this was between 1.5.1 and 1.5.2\n                try:\n                    from dbt.task.contextvars import cv_project_root\n\n                    cv_project_root.set(self.project_dir)  # pragma: no cover\n                except ImportError:\n                    cv_project_root = None\n                    pass\n\n        # NOTE: _find_node will raise a compilation exception if the project\n        # fails to compile, and we catch that in the outer `.process()` method.\n        node = self._find_node(fname, config, dbt_dir)\n\n        templater_logger.debug(\n            \"_find_node for path %r returned object of type %s.\", fname, type(node)\n        )\n\n        save_ephemeral_nodes = dict(\n            (k, v)\n            for k, v in self.dbt_manifest.nodes.items()\n            if v.config.materialized == \"ephemeral\"\n            and not getattr(v, \"compiled\", False)\n        )\n\n        if self.dbt_version_tuple >= (1, 8):\n            from dbt_common.exceptions import UndefinedMacroError\n        else:\n            from dbt.exceptions import UndefinedMacroError\n\n        with self.connection():\n            # Apply the monkeypatch.\n            Environment.from_string = from_string\n            try:\n                node = self.dbt_compiler.compile_node(\n                    node=node,\n                    manifest=self.dbt_manifest,\n                    write=False,\n                )\n            except UndefinedMacroError as err:\n                # The explanation on the undefined macro error is already fairly\n                # explanatory, so just pass it straight through.\n                raise SQLTemplaterError(str(err))\n            except Exception as err:\n                # This happens if there's a fatal error at compile time. That\n                # can sometimes happen for SQLFluff related reasons (it used\n                # to happen if we tried to compile ephemeral models in the\n                # wrong order), but more often because a macro tries to query\n                # a table at compile time which doesn't exist.\n                if self.dbt_skip_compilation_error is False:\n                    raise SQLTemplaterError(str(err))\n                raise SQLFluffSkipFile(\n                    f\"Skipped file {fname} because dbt raised a fatal \"\n                    f\"exception during compilation: {err!s}\"\n                )\n                # NOTE: We don't do a `raise ... from err` here because the\n                # full trace is not useful for most users. In debugging\n                # issues here it may be valuable to add the `from err` part\n                # after the above `raise` statement.\n            finally:\n                # Undo the monkeypatch.\n                Environment.from_string = old_from_string\n\n            if hasattr(node, \"injected_sql\"):\n                # If injected SQL is present, it contains a better picture\n                # of what will actually hit the database (e.g. with tests).\n                # However it's not always present.\n                compiled_sql = node.injected_sql  # pragma: no cover\n            else:\n                compiled_sql = getattr(node, compiled_sql_attribute)\n\n            raw_sql = getattr(node, raw_sql_attribute)\n\n            if not compiled_sql:  # pragma: no cover\n                raise SQLTemplaterError(\n                    \"dbt templater compilation failed silently, check your \"\n                    \"configuration by running `dbt compile` directly.\"\n                )\n            source_dbt_sql = in_str\n            if not source_dbt_sql.rstrip().endswith(\"-%}\"):\n                n_trailing_newlines = len(source_dbt_sql) - len(\n                    source_dbt_sql.rstrip(\"\\n\")\n                )\n            else:\n                # Source file ends with right whitespace stripping, so there's\n                # no need to preserve/restore trailing newlines, as they would\n                # have been removed regardless of dbt's\n                # keep_trailing_newlines=False behavior.\n                n_trailing_newlines = 0\n\n            templater_logger.debug(\n                \"    Trailing newline count in source dbt model: %r\",\n                n_trailing_newlines,\n            )\n            templater_logger.debug(\"    Raw SQL before compile: %r\", source_dbt_sql)\n            templater_logger.debug(\"    Node raw SQL: %r\", raw_sql)\n            templater_logger.debug(\"    Node compiled SQL: %r\", compiled_sql)\n\n            # When using dbt-templater, trailing newlines are ALWAYS REMOVED during\n            # compiling. Unless fixed (like below), this will cause:\n            #    1. Assertion errors in TemplatedFile, when it sanity checks the\n            #       contents of the sliced_file array.\n            #    2. LT12 linting errors when running \"sqlfluff lint foo_bar.sql\"\n            #       since the linter will use the compiled code with the newlines\n            #       removed.\n            #    3. \"No newline at end of file\" warnings in Git/GitHub since\n            #       sqlfluff uses the compiled SQL to write fixes back to the\n            #       source SQL in the dbt model.\n            #\n            # The solution is (note that both the raw and compiled files have\n            # had trailing newline(s) removed by the dbt-templater.\n            #    1. Check for trailing newlines before compiling by looking at the\n            #       raw SQL in the source dbt file. Remember the count of trailing\n            #       newlines.\n            #    2. Set node.raw_sql/node.raw_code to the original source file contents.\n            #    3. Append the count from #1 above to compiled_sql. (In\n            #       production, slice_file() does not usually use this string,\n            #       but some test scenarios do.\n            setattr(node, raw_sql_attribute, source_dbt_sql)\n\n            # So for files that have no templated elements in them, render_func\n            # will still be null at this point. If so, we replace it with a lambda\n            # which just directly returns the input , but _also_ reset the trailing\n            # newlines counter because they also won't have been stripped.\n            if render_func is None:\n                # NOTE: In this case, we shouldn't re-add newlines, because they\n                # were never taken away.\n                n_trailing_newlines = 0\n\n                # Overwrite the render_func placeholder.\n                def render_func(in_str):\n                    \"\"\"A render function which just returns the input.\"\"\"\n                    return in_str\n\n            # At this point assert that we _have_ a render_func\n            assert render_func is not None\n\n            # TRICKY: dbt configures Jinja2 with keep_trailing_newline=False.\n            # As documented (https://jinja.palletsprojects.com/en/3.0.x/api/),\n            # this flag's behavior is: \"Preserve the trailing newline when\n            # rendering templates. The default is False, which causes a single\n            # newline, if present, to be stripped from the end of the template.\"\n            #\n            # Below, we use \"append_to_templated\" to effectively \"undo\" this.\n            raw_sliced, sliced_file, templated_sql = self.slice_file(\n                source_dbt_sql,\n                render_func=render_func,\n                config=config,\n                append_to_templated=\"\\n\" if n_trailing_newlines else \"\",\n            )\n        # :HACK: If calling compile_node() compiled any ephemeral nodes,\n        # restore them to their earlier state. This prevents a runtime error\n        # in the dbt \"_inject_ctes_into_sql()\" function that occurs with\n        # 2nd-level ephemeral model dependencies (e.g. A -> B -> C, where\n        # both B and C are ephemeral). Perhaps there is a better way to do\n        # this, but this seems good enough for now.\n        for k, v in save_ephemeral_nodes.items():\n            if getattr(self.dbt_manifest.nodes[k], \"compiled\", False):\n                self.dbt_manifest.nodes[k] = v\n        return (\n            TemplatedFile(\n                source_str=source_dbt_sql,\n                templated_str=templated_sql,\n                fname=fname,\n                sliced_file=sliced_file,\n                raw_sliced=raw_sliced,\n            ),\n            # No violations returned in this way.\n            [],\n        )\n", "type": "function"}, {"name": "test__templater_dbt_templating_result", "is_method": false, "class_name": null, "parameters": ["project_dir", "dbt_templater", "fname", "dbt_fluff_config", "dbt_project_folder"], "calls": ["pytest.mark.parametrize", "_run_templater_and_verify_result"], "code_location": {"file": "templater_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/test", "start_line": 92, "end_line": 106}, "code_snippet": "def test__templater_dbt_templating_result(\n    project_dir,\n    dbt_templater,\n    fname,\n    dbt_fluff_config,\n    dbt_project_folder,\n):\n    \"\"\"Test that input sql file gets templated into output sql file.\"\"\"\n    _run_templater_and_verify_result(\n        dbt_templater,\n        project_dir,\n        fname,\n        dbt_fluff_config,\n        dbt_project_folder,\n    )\n", "type": "function"}, {"name": "SQLFluffSkipFile", "docstring": "An error returned from a templater to skip a file.", "methods": [], "attributes": [], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 166, "end_line": 169}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.1265487670898438}
{"question": "Why does SQLFluff use a plugin-based architecture for custom rules instead of requiring users to modify the core codebase?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff uses a plugin-based architecture for custom rules to provide several key benefits: 1) Separation of concerns - Keeps the core codebase clean and focused on standard functionality while allowing custom rules to be developed independently; 2) Maintainability - Prevents the core codebase from becoming bloated with organization-specific or project-specific rules that aren't universally applicable; 3) Version control - Allows custom rules to be versioned and managed separately from the main SQLFluff releases; 4) Distribution - Enables custom rules to be packaged and distributed independently, making them easier to share across teams or organizations; 5) Stability - Reduces the risk of breaking changes in the core codebase when custom rules are added or modified; 6) Flexibility - Allows different projects or teams to use different sets of custom rules without affecting others; 7) Testing - Enables custom rules to be tested independently of the core SQLFluff functionality; 8) Upgradability - Makes it easier to upgrade SQLFluff without losing custom rule functionality; 9) Community contribution - Encourages community contributions without requiring changes to the main codebase; 10) Enterprise adoption - Makes SQLFluff more suitable for enterprise environments where custom rules are often required for compliance or organizational standards.", "score": null, "retrieved_content": [{"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 26, "end_line": 56}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.structure.ST01 import Rule_ST01\n    from sqlfluff.rules.structure.ST02 import Rule_ST02\n    from sqlfluff.rules.structure.ST03 import Rule_ST03\n    from sqlfluff.rules.structure.ST04 import Rule_ST04\n    from sqlfluff.rules.structure.ST05 import Rule_ST05\n    from sqlfluff.rules.structure.ST06 import Rule_ST06\n    from sqlfluff.rules.structure.ST07 import Rule_ST07\n    from sqlfluff.rules.structure.ST08 import Rule_ST08\n    from sqlfluff.rules.structure.ST09 import Rule_ST09\n    from sqlfluff.rules.structure.ST10 import Rule_ST10\n    from sqlfluff.rules.structure.ST11 import Rule_ST11\n\n    return [\n        Rule_ST01,\n        Rule_ST02,\n        Rule_ST03,\n        Rule_ST04,\n        Rule_ST05,\n        Rule_ST06,\n        Rule_ST07,\n        Rule_ST08,\n        Rule_ST09,\n        Rule_ST10,\n        Rule_ST11,\n    ]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/capitalisation", "start_line": 41, "end_line": 53}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.capitalisation.CP01 import Rule_CP01\n    from sqlfluff.rules.capitalisation.CP02 import Rule_CP02\n    from sqlfluff.rules.capitalisation.CP03 import Rule_CP03\n    from sqlfluff.rules.capitalisation.CP04 import Rule_CP04\n    from sqlfluff.rules.capitalisation.CP05 import Rule_CP05\n\n    return [Rule_CP01, Rule_CP02, Rule_CP03, Rule_CP04, Rule_CP05]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-plugin-example/src/sqlfluff_plugin_example", "start_line": 23, "end_line": 38}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: It is much better that we only import the rule on demand.\n    The root module of the plugin (i.e. this file which contains\n    all of the hook implementations) should have fully loaded before\n    we try and import the rules. This is partly for performance\n    reasons - but more because the definition of a BaseRule requires\n    that all of the get_configs_info() methods have both been\n    defined _and have run_ before so all the validation information\n    is available for the validation steps in the meta class.\n    \"\"\"\n    # i.e. we DO recommend importing here:\n    from sqlfluff_plugin_example.rules import Rule_Example_L001  # noqa: F811\n\n    return [Rule_Example_L001]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 56, "end_line": 69}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.references.RF01 import Rule_RF01\n    from sqlfluff.rules.references.RF02 import Rule_RF02\n    from sqlfluff.rules.references.RF03 import Rule_RF03\n    from sqlfluff.rules.references.RF04 import Rule_RF04\n    from sqlfluff.rules.references.RF05 import Rule_RF05\n    from sqlfluff.rules.references.RF06 import Rule_RF06\n\n    return [Rule_RF01, Rule_RF02, Rule_RF03, Rule_RF04, Rule_RF05, Rule_RF06]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/aliasing", "start_line": 50, "end_line": 76}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.aliasing.AL01 import Rule_AL01\n    from sqlfluff.rules.aliasing.AL02 import Rule_AL02\n    from sqlfluff.rules.aliasing.AL03 import Rule_AL03\n    from sqlfluff.rules.aliasing.AL04 import Rule_AL04\n    from sqlfluff.rules.aliasing.AL05 import Rule_AL05\n    from sqlfluff.rules.aliasing.AL06 import Rule_AL06\n    from sqlfluff.rules.aliasing.AL07 import Rule_AL07\n    from sqlfluff.rules.aliasing.AL08 import Rule_AL08\n    from sqlfluff.rules.aliasing.AL09 import Rule_AL09\n\n    return [\n        Rule_AL01,\n        Rule_AL02,\n        Rule_AL03,\n        Rule_AL04,\n        Rule_AL05,\n        Rule_AL06,\n        Rule_AL07,\n        Rule_AL08,\n        Rule_AL09,\n    ]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 47, "end_line": 85}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.layout.LT01 import Rule_LT01\n    from sqlfluff.rules.layout.LT02 import Rule_LT02\n    from sqlfluff.rules.layout.LT03 import Rule_LT03\n    from sqlfluff.rules.layout.LT04 import Rule_LT04\n    from sqlfluff.rules.layout.LT05 import Rule_LT05\n    from sqlfluff.rules.layout.LT06 import Rule_LT06\n    from sqlfluff.rules.layout.LT07 import Rule_LT07\n    from sqlfluff.rules.layout.LT08 import Rule_LT08\n    from sqlfluff.rules.layout.LT09 import Rule_LT09\n    from sqlfluff.rules.layout.LT10 import Rule_LT10\n    from sqlfluff.rules.layout.LT11 import Rule_LT11\n    from sqlfluff.rules.layout.LT12 import Rule_LT12\n    from sqlfluff.rules.layout.LT13 import Rule_LT13\n    from sqlfluff.rules.layout.LT14 import Rule_LT14\n    from sqlfluff.rules.layout.LT15 import Rule_LT15\n\n    return [\n        Rule_LT01,\n        Rule_LT02,\n        Rule_LT03,\n        Rule_LT04,\n        Rule_LT05,\n        Rule_LT06,\n        Rule_LT07,\n        Rule_LT08,\n        Rule_LT09,\n        Rule_LT10,\n        Rule_LT11,\n        Rule_LT12,\n        Rule_LT13,\n        Rule_LT14,\n        Rule_LT15,\n    ]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/ambiguous", "start_line": 29, "end_line": 53}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.ambiguous.AM01 import Rule_AM01\n    from sqlfluff.rules.ambiguous.AM02 import Rule_AM02\n    from sqlfluff.rules.ambiguous.AM03 import Rule_AM03\n    from sqlfluff.rules.ambiguous.AM04 import Rule_AM04\n    from sqlfluff.rules.ambiguous.AM05 import Rule_AM05\n    from sqlfluff.rules.ambiguous.AM06 import Rule_AM06\n    from sqlfluff.rules.ambiguous.AM07 import Rule_AM07\n    from sqlfluff.rules.ambiguous.AM08 import Rule_AM08\n\n    return [\n        Rule_AM01,\n        Rule_AM02,\n        Rule_AM03,\n        Rule_AM04,\n        Rule_AM05,\n        Rule_AM06,\n        Rule_AM07,\n        Rule_AM08,\n    ]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/jinja", "start_line": 8, "end_line": 16}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.jinja.JJ01 import Rule_JJ01\n\n    return [Rule_JJ01]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "start_line": 63, "end_line": 95}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.convention.CV01 import Rule_CV01\n    from sqlfluff.rules.convention.CV02 import Rule_CV02\n    from sqlfluff.rules.convention.CV03 import Rule_CV03\n    from sqlfluff.rules.convention.CV04 import Rule_CV04\n    from sqlfluff.rules.convention.CV05 import Rule_CV05\n    from sqlfluff.rules.convention.CV06 import Rule_CV06\n    from sqlfluff.rules.convention.CV07 import Rule_CV07\n    from sqlfluff.rules.convention.CV08 import Rule_CV08\n    from sqlfluff.rules.convention.CV09 import Rule_CV09\n    from sqlfluff.rules.convention.CV10 import Rule_CV10\n    from sqlfluff.rules.convention.CV11 import Rule_CV11\n    from sqlfluff.rules.convention.CV12 import Rule_CV12\n\n    return [\n        Rule_CV01,\n        Rule_CV02,\n        Rule_CV03,\n        Rule_CV04,\n        Rule_CV05,\n        Rule_CV06,\n        Rule_CV07,\n        Rule_CV08,\n        Rule_CV09,\n        Rule_CV10,\n        Rule_CV11,\n        Rule_CV12,\n    ]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/tsql", "start_line": 14, "end_line": 22}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.tsql.TQ01 import Rule_TQ01\n\n    return [Rule_TQ01]\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1442639827728271}
{"question": "What dependencies exist between SQLFluff's LintedFile and LintedDir classes?", "answer": null, "relative_code_list": null, "ground_truth": "LintedFile and LintedDir have a container-content dependency relationship where LintedDir depends on LintedFile. Key dependencies include: 1) Container dependency - LintedDir contains a list of LintedFile objects (self.files: list[LintedFile]) and manages them as a collection; 2) Import dependency - LintedDir imports LintedFile from sqlfluff.core.linter.linted_file module; 3) Add method dependency - LintedDir.add() method takes a LintedFile parameter and processes it to extract metadata and statistics; 4) Data extraction dependency - LintedDir extracts data from LintedFile objects including violations, statistics, timings, and file status; 5) Statistics aggregation - LintedDir aggregates statistics across multiple LintedFile objects (num_files, num_clean, num_unclean, num_violations); 6) Record management - LintedDir creates LintingRecord objects from LintedFile data for serialization and reporting; 7) Memory management - LintedDir can optionally retain or discard LintedFile objects based on retain_files parameter to manage memory usage; 8) Error tracking - LintedDir tracks template and parsing errors from LintedFile objects; 9) Timing aggregation - LintedDir aggregates timing information from multiple LintedFile objects; 10) Path management - LintedDir manages files that share a common root path, while each LintedFile represents a single file. The dependency is unidirectional - LintedFile does not depend on LintedDir, but LintedDir cannot function without LintedFile objects.", "score": null, "retrieved_content": [{"name": "LintedDir", "docstring": "A class to store the idea of a collection of linted files at a single start path.\n\nA LintedDir may contain files in subdirectories, but they all share\na common root.\n\nImportantly, this class also abstracts away from the given LintedFile\nobject and allows us to either _keep_ those objects for later use, or\nextract the results from them and allow the original object to be discarded\nand save memory overhead if not required.", "methods": ["__init__", "add", "check_tuples", "check_tuples_by_path", "num_violations", "get_violations", "as_records", "stats", "persist_changes", "discard_fixes_for_lint_errors_in_files_with_tmp_or_prs_errors", "tree"], "attributes": [], "code_location": {"file": "linted_dir.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 32, "end_line": 257}, "type": "class"}, {"name": "LintingResult", "docstring": "A class to represent the result of a linting operation.\n\nNotably this might be a collection of paths, all with multiple\npotential files within them.", "methods": ["__init__", "add", "stop_timer", "check_tuples", "check_tuples_by_path", "num_violations", "get_violations", "stats", "timing_summary", "persist_timing_records", "as_records", "persist_changes", "tree", "count_tmp_prs_errors", "discard_fixes_for_lint_errors_in_files_with_tmp_or_prs_errors"], "attributes": [], "code_location": {"file": "linting_result.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 34, "end_line": 224}, "type": "class"}, {"name": "lint", "is_method": false, "class_name": null, "parameters": ["paths", "format", "write_output", "annotation_level", "nofail", "disregard_sqlfluffignores", "logger", "bench", "processes", "disable_progress_bar", "persist_timing", "extra_config_path", "ignore_local_config", "stdin_filename"], "calls": ["cli.command", "click.option", "click.option", "click.option", "click.option", "click.argument", "get_config", "make_output_stream", "get_linter_and_formatter", "config.get", "formatter.dispatch_config", "set_logging_level", "output_stream.close", "click.echo", "PathAndUserErrorHandler", "click.echo", "json.dumps", "dump_file_payload", "result.persist_timing_records", "click.echo", "click.echo", "result.timing_summary", "isinstance", "sys.exit", "sys.exit", "click.Choice", "click.Choice", "click.Path", "format_linting_result_header", "lnt.lint_string_wrapped", "lnt.lint_paths", "formatter.format_linting_stats", "result.as_records", "yaml.dump", "formatter.cli_table", "click.echo", "click.echo", "formatter.completion_message", "result.stats", "lnt.config.make_child_from_path", "sys.stdin.read", "result.as_records", "formatter.cli_table", "result.as_records", "json.dumps", "items", "result.as_records", "join", "github_result.append", "github_result_native.append", "github_result_native.append", "github_result_native.append", "violation.get", "violation.get"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 581, "end_line": 781}, "code_snippet": "def lint(\n    paths: tuple[str],\n    format: str,\n    write_output: Optional[str],\n    annotation_level: str,\n    nofail: bool,\n    disregard_sqlfluffignores: bool,\n    logger: Optional[logging.Logger] = None,\n    bench: bool = False,\n    processes: Optional[int] = None,\n    disable_progress_bar: Optional[bool] = False,\n    persist_timing: Optional[str] = None,\n    extra_config_path: Optional[str] = None,\n    ignore_local_config: bool = False,\n    stdin_filename: Optional[str] = None,\n    **kwargs,\n) -> None:\n    \"\"\"Lint SQL files via passing a list of files or using stdin.\n\n    PATH is the path to a sql file or directory to lint. This can be either a\n    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n    be interpreted like passing the current working directory as a path argument.\n\n    Linting SQL files:\n\n        sqlfluff lint path/to/file.sql\n        sqlfluff lint directory/of/sql/files\n\n    Linting a file via stdin (note the lone '-' character):\n\n        cat path/to/file.sql | sqlfluff lint -\n        echo 'select col from tbl' | sqlfluff lint -\n\n    \"\"\"\n    config = get_config(\n        extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n    )\n    non_human_output = (format != FormatType.human.value) or (write_output is not None)\n    file_output = None\n    output_stream = make_output_stream(config, format, write_output)\n    lnt, formatter = get_linter_and_formatter(config, output_stream)\n\n    verbose = config.get(\"verbose\")\n    progress_bar_configuration.disable_progress_bar = disable_progress_bar\n\n    formatter.dispatch_config(lnt)\n\n    # Set up logging.\n    set_logging_level(\n        verbosity=verbose,\n        formatter=formatter,\n        logger=logger,\n        stderr_output=non_human_output,\n    )\n\n    # Output the results as we go\n    if verbose >= 1 and not non_human_output:\n        click.echo(format_linting_result_header())\n\n    with PathAndUserErrorHandler(formatter):\n        # add stdin if specified via lone '-'\n        if (\"-\",) == paths:\n            if stdin_filename:\n                lnt.config = lnt.config.make_child_from_path(\n                    stdin_filename, require_dialect=False\n                )\n            result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\n        else:\n            result = lnt.lint_paths(\n                paths,\n                ignore_non_existent_files=False,\n                ignore_files=not disregard_sqlfluffignores,\n                processes=processes,\n                # If we're just linting in the CLI, we don't need to retain the\n                # raw file content. This allows us to reduce memory overhead.\n                retain_files=False,\n            )\n\n    # Output the final stats\n    if verbose >= 1 and not non_human_output:\n        click.echo(formatter.format_linting_stats(result, verbose=verbose))\n\n    if format == FormatType.json.value:\n        file_output = json.dumps(result.as_records())\n    elif format == FormatType.yaml.value:\n        file_output = yaml.dump(\n            result.as_records(),\n            sort_keys=False,\n            allow_unicode=True,\n        )\n    elif format == FormatType.none.value:\n        file_output = \"\"\n    elif format == FormatType.github_annotation.value:\n        if annotation_level == \"error\":\n            annotation_level = \"failure\"\n\n        github_result = []\n        for record in result.as_records():\n            filepath = record[\"filepath\"]\n            for violation in record[\"violations\"]:\n                # NOTE: The output format is designed for this GitHub action:\n                # https://github.com/yuzutech/annotations-action\n                # It is similar, but not identical, to the native GitHub format:\n                # https://docs.github.com/en/rest/reference/checks#annotations-items\n                github_result.append(\n                    {\n                        \"file\": filepath,\n                        \"start_line\": violation[\"start_line_no\"],\n                        \"start_column\": violation[\"start_line_pos\"],\n                        # NOTE: There should always be a start, there _may_ not be an\n                        # end, so in that case we default back to just reusing\n                        # the start.\n                        \"end_line\": violation.get(\n                            \"end_line_no\", violation[\"start_line_no\"]\n                        ),\n                        \"end_column\": violation.get(\n                            \"end_line_pos\", violation[\"start_line_pos\"]\n                        ),\n                        \"title\": \"SQLFluff\",\n                        \"message\": f\"{violation['code']}: {violation['description']}\",\n                        # The annotation_level is configurable, but will only apply\n                        # to any SQLFluff rules which have not been downgraded\n                        # to warnings using the `warnings` config value. Any which have\n                        # been set to warn rather than fail will always be given the\n                        # `notice` annotation level in the serialised result.\n                        \"annotation_level\": (\n                            annotation_level if not violation[\"warning\"] else \"notice\"\n                        ),\n                    }\n                )\n        file_output = json.dumps(github_result)\n    elif format == FormatType.github_annotation_native.value:\n        if annotation_level == \"failure\":\n            annotation_level = \"error\"\n\n        github_result_native = []\n        for record in result.as_records():\n            filepath = record[\"filepath\"]\n\n            # Add a group, titled with the filename\n            if record[\"violations\"]:\n                github_result_native.append(f\"::group::{filepath}\")\n\n            for violation in record[\"violations\"]:\n                # NOTE: The output format is designed for GitHub action:\n                # https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#setting-a-notice-message\n\n                # The annotation_level is configurable, but will only apply\n                # to any SQLFluff rules which have not been downgraded\n                # to warnings using the `warnings` config value. Any which have\n                # been set to warn rather than fail will always be given the\n                # `notice` annotation level in the serialised result.\n                line = \"::notice \" if violation[\"warning\"] else f\"::{annotation_level} \"\n\n                line += \"title=SQLFluff,\"\n                line += f\"file={filepath},\"\n                line += f\"line={violation['start_line_no']},\"\n                line += f\"col={violation['start_line_pos']}\"\n                if \"end_line_no\" in violation:\n                    line += f\",endLine={violation['end_line_no']}\"\n                if \"end_line_pos\" in violation:\n                    line += f\",endColumn={violation['end_line_pos']}\"\n                line += \"::\"\n                line += f\"{violation['code']}: {violation['description']}\"\n                if violation[\"name\"]:\n                    line += f\" [{violation['name']}]\"\n\n                github_result_native.append(line)\n\n            # Close the group\n            if record[\"violations\"]:\n                github_result_native.append(\"::endgroup::\")\n\n        file_output = \"\\n\".join(github_result_native)\n\n    if file_output:\n        dump_file_payload(write_output, file_output)\n\n    if persist_timing:\n        result.persist_timing_records(persist_timing)\n\n    output_stream.close()\n    if bench:\n        click.echo(\"==== overall timings ====\")\n        click.echo(formatter.cli_table([(\"Clock time\", result.total_time)]))\n        timing_summary = result.timing_summary()\n        for step in timing_summary:\n            click.echo(f\"=== {step} ===\")\n            click.echo(\n                formatter.cli_table(timing_summary[step].items(), cols=3, col_width=20)\n            )\n\n    if not nofail:\n        if not non_human_output:\n            formatter.completion_message()\n        exit_code = result.stats(EXIT_FAIL, EXIT_SUCCESS)[\"exit code\"]\n        assert isinstance(exit_code, int), \"result.stats error code must be integer.\"\n        sys.exit(exit_code)\n    else:\n        sys.exit(EXIT_SUCCESS)\n", "type": "function"}, {"name": "lint_paths", "is_method": true, "class_name": "Linter", "parameters": ["self", "paths", "fix", "ignore_non_existent_files", "ignore_files", "processes", "apply_fixes", "fixed_file_suffix", "fix_even_unparsable", "retain_files"], "calls": ["LintingResult", "split", "len", "get_runner", "tqdm", "enumerate", "result.stop_timer", "LintedDir", "result.add", "paths_from_path", "self.config.get", "self.formatter.dispatch_processing_header", "runner.run", "linted_dir.add", "any", "progress_bar_files.update", "os.getcwd", "lower", "expanded_paths.append", "linter_logger.error", "linted_file.num_violations", "len", "progress_bar_files.set_description", "linted_file.persist_tree", "self.config.get"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 1033, "end_line": 1128}, "code_snippet": "    def lint_paths(\n        self,\n        paths: tuple[str, ...],\n        fix: bool = False,\n        ignore_non_existent_files: bool = False,\n        ignore_files: bool = True,\n        processes: Optional[int] = None,\n        apply_fixes: bool = False,\n        fixed_file_suffix: str = \"\",\n        fix_even_unparsable: bool = False,\n        retain_files: bool = True,\n    ) -> LintingResult:\n        \"\"\"Lint an iterable of paths.\"\"\"\n        # If no paths specified - assume local\n        if not paths:  # pragma: no cover\n            paths = (os.getcwd(),)\n        # Set up the result to hold what we get back\n        result = LintingResult()\n\n        expanded_paths: list[str] = []\n        expanded_path_to_linted_dir = {}\n        sql_exts = self.config.get(\"sql_file_exts\", default=\".sql\").lower().split(\",\")\n\n        for path in paths:\n            linted_dir = LintedDir(path, retain_files=retain_files)\n            result.add(linted_dir)\n            for fname in paths_from_path(\n                path,\n                ignore_non_existent_files=ignore_non_existent_files,\n                ignore_files=ignore_files,\n                target_file_exts=sql_exts,\n            ):\n                expanded_paths.append(fname)\n                expanded_path_to_linted_dir[fname] = linted_dir\n\n        files_count = len(expanded_paths)\n        if processes is None:\n            processes = self.config.get(\"processes\", default=1)\n        assert processes is not None\n        # Hard set processes to 1 if only 1 file is queued.\n        # The overhead will never be worth it with one file.\n        if files_count == 1:\n            processes = 1\n\n        # to avoid circular import\n        from sqlfluff.core.linter.runner import get_runner\n\n        runner, effective_processes = get_runner(\n            self,\n            self.config,\n            processes=processes,\n            allow_process_parallelism=self.allow_process_parallelism,\n        )\n\n        if self.formatter and effective_processes != 1:\n            self.formatter.dispatch_processing_header(effective_processes)\n\n        # Show files progress bar only when there is more than one.\n        first_path = expanded_paths[0] if expanded_paths else \"\"\n        progress_bar_files = tqdm(\n            total=files_count,\n            desc=f\"file {first_path}\",\n            leave=False,\n            disable=files_count <= 1 or progress_bar_configuration.disable_progress_bar,\n        )\n\n        for i, linted_file in enumerate(runner.run(expanded_paths, fix), start=1):\n            linted_dir = expanded_path_to_linted_dir[linted_file.path]\n            linted_dir.add(linted_file)\n            # If any fatal errors, then stop iteration.\n            if any(v.fatal for v in linted_file.violations):  # pragma: no cover\n                linter_logger.error(\"Fatal linting error. Halting further linting.\")\n                break\n\n            # If we're applying fixes, then do that here.\n            if apply_fixes:\n                num_tmp_prs_errors = linted_file.num_violations(\n                    types=TMP_PRS_ERROR_TYPES,\n                    filter_ignore=False,\n                    filter_warning=False,\n                )\n                if fix_even_unparsable or num_tmp_prs_errors == 0:\n                    linted_file.persist_tree(\n                        suffix=fixed_file_suffix, formatter=self.formatter\n                    )\n\n            # Progress bar for files is rendered only when there is more than one file.\n            # Additionally, as it's updated after each loop, we need to get file name\n            # from the next loop. This is why `enumerate` starts with `1` and there\n            # is `i < len` to not exceed files list length.\n            progress_bar_files.update(n=1)\n            if i < len(expanded_paths):\n                progress_bar_files.set_description(f\"file {expanded_paths[i]}\")\n\n        result.stop_timer()\n        return result\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "LintingResult", "parameters": ["self"], "calls": ["time.monotonic"], "code_location": {"file": "linting_result.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 41, "end_line": 44}, "code_snippet": "    def __init__(self) -> None:\n        self.paths: list[LintedDir] = []\n        self._start_time: float = time.monotonic()\n        self.total_time: float = 0.0\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "SQLFluffDriver", "parameters": ["self"], "calls": ["__init__", "super", "s.encode", "sys.getfilesystemencoding"], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 25, "end_line": 34}, "code_snippet": "    def __init__(self) -> None:\n        super().__init__(\n            [sys.executable, \"-m\", \"sqlfluff.cli.commands\"],\n            [\".sql\"],\n            [\n                s.encode(sys.getfilesystemencoding())\n                for s in [\"sqlfluff\", \"lint\", \"--format=json\"]\n            ],\n            exit_codes=[0, 1],\n        )\n", "type": "function"}, {"name": "lint_parsed", "is_method": true, "class_name": "Linter", "parameters": ["cls", "parsed", "rule_pack", "fix", "formatter", "encoding"], "calls": ["time.monotonic", "LintedFile", "linted_file.get_violations", "linter_logger.info", "linter_logger.info", "cls.lint_fix_parsed", "enumerate", "parsed.config.get", "time.monotonic", "violation.ignore_if_in", "violation.warning_if_in", "LintedFile.deduplicate_in_source_space", "FileTimings", "formatter.dispatch_file_violations", "linter_logger.info", "cls.lint_fix_parsed", "parsed.config.get", "cls.allowed_rule_ref_map", "IgnoreMask.from_source", "parsed.config.get", "parsed.config.get", "formatter.dispatch_dialect_warning", "parsed.config.get", "cast", "parsed.config.get", "parsed.config.get"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 631, "end_line": 777}, "code_snippet": "    def lint_parsed(\n        cls,\n        parsed: ParsedString,\n        rule_pack: RulePack,\n        fix: bool = False,\n        formatter: Optional[FormatterInterface] = None,\n        encoding: str = \"utf8\",\n    ) -> LintedFile:\n        \"\"\"Lint a ParsedString and return a LintedFile.\"\"\"\n        violations = parsed.violations\n        time_dict = parsed.time_dict\n        tree: Optional[BaseSegment] = None\n        templated_file: Optional[TemplatedFile] = None\n        t0 = time.monotonic()\n\n        # First identify the root variant. That's the first variant\n        # that successfully parsed.\n        root_variant: Optional[ParsedVariant] = None\n        for variant in parsed.parsed_variants:\n            if variant.tree:\n                root_variant = variant\n                break\n        else:\n            linter_logger.info(\n                \"lint_parsed found no valid root variant for %s\", parsed.fname\n            )\n\n        # If there is a root variant, handle that first.\n        if root_variant:\n            linter_logger.info(\"lint_parsed - linting root variant (%s)\", parsed.fname)\n            assert root_variant.tree  # We just checked this.\n            (\n                fixed_tree,\n                initial_linting_errors,\n                ignore_mask,\n                rule_timings,\n            ) = cls.lint_fix_parsed(\n                root_variant.tree,\n                config=parsed.config,\n                rule_pack=rule_pack,\n                fix=fix,\n                fname=parsed.fname,\n                templated_file=variant.templated_file,\n                formatter=formatter,\n            )\n\n            # Set legacy variables for now\n            # TODO: Revise this\n            templated_file = variant.templated_file\n            tree = fixed_tree\n\n            # We're only going to return the *initial* errors, rather\n            # than any generated during the fixing cycle.\n            violations += initial_linting_errors\n\n            # Attempt to lint other variants if they exist.\n            # TODO: Revise whether this is sensible...\n            for idx, alternate_variant in enumerate(parsed.parsed_variants):\n                if alternate_variant is variant or not alternate_variant.tree:\n                    continue\n                linter_logger.info(\"lint_parsed - linting alt variant (%s)\", idx)\n                (\n                    _,  # Fixed Tree\n                    alt_linting_errors,\n                    _,  # Ignore Mask\n                    _,  # Timings\n                ) = cls.lint_fix_parsed(\n                    alternate_variant.tree,\n                    config=parsed.config,\n                    rule_pack=rule_pack,\n                    fix=fix,\n                    fname=parsed.fname,\n                    templated_file=alternate_variant.templated_file,\n                    formatter=formatter,\n                )\n                violations += alt_linting_errors\n\n        # If no root variant, we should still apply ignores to any parsing\n        # or templating fails.\n        else:\n            rule_timings = []\n            disable_noqa_except: Optional[str] = parsed.config.get(\n                \"disable_noqa_except\"\n            )\n            if parsed.config.get(\"disable_noqa\") and not disable_noqa_except:\n                # NOTE: This path is only accessible if there is no valid `tree`\n                # which implies that there was a fatal templating fail. Even an\n                # unparsable file will still have a valid tree.\n                ignore_mask = None\n            else:\n                # Templating and/or parsing have failed. Look for \"noqa\"\n                # comments (the normal path for identifying these comments\n                # requires access to the parse tree, and because of the failure,\n                # we don't have a parse tree).\n                allowed_rules_ref_map = cls.allowed_rule_ref_map(\n                    rule_pack.reference_map, disable_noqa_except\n                )\n                ignore_mask, ignore_violations = IgnoreMask.from_source(\n                    parsed.source_str,\n                    [\n                        lm\n                        for lm in parsed.config.get(\"dialect_obj\").lexer_matchers\n                        if lm.name == \"inline_comment\"\n                    ][0],\n                    allowed_rules_ref_map,\n                )\n                violations += ignore_violations\n\n        # Update the timing dict\n        time_dict[\"linting\"] = time.monotonic() - t0\n\n        # We process the ignore config here if appropriate\n        for violation in violations:\n            violation.ignore_if_in(parsed.config.get(\"ignore\"))\n            violation.warning_if_in(parsed.config.get(\"warnings\"))\n\n        linted_file = LintedFile(\n            parsed.fname,\n            # Deduplicate violations\n            LintedFile.deduplicate_in_source_space(violations),\n            FileTimings(time_dict, rule_timings),\n            tree,\n            ignore_mask=ignore_mask,\n            templated_file=templated_file,\n            encoding=encoding,\n        )\n\n        # This is the main command line output from linting.\n        if formatter:\n            formatter.dispatch_file_violations(\n                parsed.fname,\n                linted_file,\n                only_fixable=fix,\n                warn_unused_ignores=parsed.config.get(\"warn_unused_ignores\"),\n            )\n\n        # Safety flag for unset dialects\n        if linted_file.get_violations(\n            fixable=True if fix else None, types=SQLParseError\n        ):\n            if formatter:  # pragma: no cover TODO?\n                formatter.dispatch_dialect_warning(\n                    # The dialect property is the string, not the dialect object\n                    cast(str, parsed.config.get(\"dialect\"))\n                )\n\n        return linted_file\n", "type": "function"}, {"name": "SQLFluffSkipFile", "docstring": "An error returned from a templater to skip a file.", "methods": [], "attributes": [], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 166, "end_line": 169}, "type": "class"}, {"name": "test__linter__linting_unexpected_error_handled_gracefully", "is_method": false, "class_name": null, "parameters": ["patched_lint", "patched_logger"], "calls": ["patch", "patch", "Exception", "Linter", "lntr.lint_paths", "replace"], "code_location": {"file": "linter_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 382, "end_line": 395}, "code_snippet": "def test__linter__linting_unexpected_error_handled_gracefully(\n    patched_lint, patched_logger\n):\n    \"\"\"Test that an unexpected internal error returns the issue-surfacing file.\"\"\"\n    patched_lint.side_effect = Exception(\"Something unexpected happened\")\n    lntr = Linter()\n    lntr.lint_paths((\"test/fixtures/linter/passing.sql\",))\n    assert (\n        \"Unable to lint test/fixtures/linter/passing.sql due to an internal error.\"\n        # NB: Replace is to handle windows-style paths.\n        in patched_logger.warning.call_args[0][0].replace(\"\\\\\", \"/\")\n        and \"Exception: Something unexpected happened\"\n        in patched_logger.warning.call_args[0][0]\n    )\n", "type": "function"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.150589942932129}
{"question": "How does SQLFluff implement its plugin system for custom rules?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff implements its plugin system for custom rules through a modular architecture that allows third-party rule development and integration. The plugin system works as follows: 1) Plugin Discovery - SQLFluff uses Python's entry point system to discover plugins, with plugins registering themselves through setuptools entry points; 2) Rule Registration - Custom rules inherit from BaseRule and are automatically registered through the RuleMetaclass when imported; 3) Plugin Loading - The plugin system loads custom rules from external packages and integrates them into the main rule registry; 4) Configuration Integration - Custom rules integrate with the configuration system, allowing users to configure plugin rules through standard configuration files; 5) Rule Validation - Plugin rules are validated against the BaseRule interface and must implement required methods like _eval(); 6) Metadata Management - Plugin rules can define their own metadata (code, name, description, groups) that gets integrated into the rule system; 7) Fix Generation - Plugin rules can generate fixes using the same LintFix system as built-in rules; 8) Error Handling - Plugin rules are wrapped with error handling to prevent plugin errors from crashing the main application; 9) Documentation Integration - Plugin rules can provide documentation that gets integrated into SQLFluff's help system; 10) Testing Support - The plugin system provides utilities for testing custom rules and integrating them into SQLFluff's test suite.", "score": null, "retrieved_content": [{"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/aliasing", "start_line": 50, "end_line": 76}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.aliasing.AL01 import Rule_AL01\n    from sqlfluff.rules.aliasing.AL02 import Rule_AL02\n    from sqlfluff.rules.aliasing.AL03 import Rule_AL03\n    from sqlfluff.rules.aliasing.AL04 import Rule_AL04\n    from sqlfluff.rules.aliasing.AL05 import Rule_AL05\n    from sqlfluff.rules.aliasing.AL06 import Rule_AL06\n    from sqlfluff.rules.aliasing.AL07 import Rule_AL07\n    from sqlfluff.rules.aliasing.AL08 import Rule_AL08\n    from sqlfluff.rules.aliasing.AL09 import Rule_AL09\n\n    return [\n        Rule_AL01,\n        Rule_AL02,\n        Rule_AL03,\n        Rule_AL04,\n        Rule_AL05,\n        Rule_AL06,\n        Rule_AL07,\n        Rule_AL08,\n        Rule_AL09,\n    ]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-plugin-example/src/sqlfluff_plugin_example", "start_line": 23, "end_line": 38}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: It is much better that we only import the rule on demand.\n    The root module of the plugin (i.e. this file which contains\n    all of the hook implementations) should have fully loaded before\n    we try and import the rules. This is partly for performance\n    reasons - but more because the definition of a BaseRule requires\n    that all of the get_configs_info() methods have both been\n    defined _and have run_ before so all the validation information\n    is available for the validation steps in the meta class.\n    \"\"\"\n    # i.e. we DO recommend importing here:\n    from sqlfluff_plugin_example.rules import Rule_Example_L001  # noqa: F811\n\n    return [Rule_Example_L001]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 26, "end_line": 56}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.structure.ST01 import Rule_ST01\n    from sqlfluff.rules.structure.ST02 import Rule_ST02\n    from sqlfluff.rules.structure.ST03 import Rule_ST03\n    from sqlfluff.rules.structure.ST04 import Rule_ST04\n    from sqlfluff.rules.structure.ST05 import Rule_ST05\n    from sqlfluff.rules.structure.ST06 import Rule_ST06\n    from sqlfluff.rules.structure.ST07 import Rule_ST07\n    from sqlfluff.rules.structure.ST08 import Rule_ST08\n    from sqlfluff.rules.structure.ST09 import Rule_ST09\n    from sqlfluff.rules.structure.ST10 import Rule_ST10\n    from sqlfluff.rules.structure.ST11 import Rule_ST11\n\n    return [\n        Rule_ST01,\n        Rule_ST02,\n        Rule_ST03,\n        Rule_ST04,\n        Rule_ST05,\n        Rule_ST06,\n        Rule_ST07,\n        Rule_ST08,\n        Rule_ST09,\n        Rule_ST10,\n        Rule_ST11,\n    ]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 56, "end_line": 69}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.references.RF01 import Rule_RF01\n    from sqlfluff.rules.references.RF02 import Rule_RF02\n    from sqlfluff.rules.references.RF03 import Rule_RF03\n    from sqlfluff.rules.references.RF04 import Rule_RF04\n    from sqlfluff.rules.references.RF05 import Rule_RF05\n    from sqlfluff.rules.references.RF06 import Rule_RF06\n\n    return [Rule_RF01, Rule_RF02, Rule_RF03, Rule_RF04, Rule_RF05, Rule_RF06]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 47, "end_line": 85}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.layout.LT01 import Rule_LT01\n    from sqlfluff.rules.layout.LT02 import Rule_LT02\n    from sqlfluff.rules.layout.LT03 import Rule_LT03\n    from sqlfluff.rules.layout.LT04 import Rule_LT04\n    from sqlfluff.rules.layout.LT05 import Rule_LT05\n    from sqlfluff.rules.layout.LT06 import Rule_LT06\n    from sqlfluff.rules.layout.LT07 import Rule_LT07\n    from sqlfluff.rules.layout.LT08 import Rule_LT08\n    from sqlfluff.rules.layout.LT09 import Rule_LT09\n    from sqlfluff.rules.layout.LT10 import Rule_LT10\n    from sqlfluff.rules.layout.LT11 import Rule_LT11\n    from sqlfluff.rules.layout.LT12 import Rule_LT12\n    from sqlfluff.rules.layout.LT13 import Rule_LT13\n    from sqlfluff.rules.layout.LT14 import Rule_LT14\n    from sqlfluff.rules.layout.LT15 import Rule_LT15\n\n    return [\n        Rule_LT01,\n        Rule_LT02,\n        Rule_LT03,\n        Rule_LT04,\n        Rule_LT05,\n        Rule_LT06,\n        Rule_LT07,\n        Rule_LT08,\n        Rule_LT09,\n        Rule_LT10,\n        Rule_LT11,\n        Rule_LT12,\n        Rule_LT13,\n        Rule_LT14,\n        Rule_LT15,\n    ]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/capitalisation", "start_line": 41, "end_line": 53}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.capitalisation.CP01 import Rule_CP01\n    from sqlfluff.rules.capitalisation.CP02 import Rule_CP02\n    from sqlfluff.rules.capitalisation.CP03 import Rule_CP03\n    from sqlfluff.rules.capitalisation.CP04 import Rule_CP04\n    from sqlfluff.rules.capitalisation.CP05 import Rule_CP05\n\n    return [Rule_CP01, Rule_CP02, Rule_CP03, Rule_CP04, Rule_CP05]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/ambiguous", "start_line": 29, "end_line": 53}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.ambiguous.AM01 import Rule_AM01\n    from sqlfluff.rules.ambiguous.AM02 import Rule_AM02\n    from sqlfluff.rules.ambiguous.AM03 import Rule_AM03\n    from sqlfluff.rules.ambiguous.AM04 import Rule_AM04\n    from sqlfluff.rules.ambiguous.AM05 import Rule_AM05\n    from sqlfluff.rules.ambiguous.AM06 import Rule_AM06\n    from sqlfluff.rules.ambiguous.AM07 import Rule_AM07\n    from sqlfluff.rules.ambiguous.AM08 import Rule_AM08\n\n    return [\n        Rule_AM01,\n        Rule_AM02,\n        Rule_AM03,\n        Rule_AM04,\n        Rule_AM05,\n        Rule_AM06,\n        Rule_AM07,\n        Rule_AM08,\n    ]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/jinja", "start_line": 8, "end_line": 16}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.jinja.JJ01 import Rule_JJ01\n\n    return [Rule_JJ01]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/tsql", "start_line": 14, "end_line": 22}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.tsql.TQ01 import Rule_TQ01\n\n    return [Rule_TQ01]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "start_line": 63, "end_line": 95}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.convention.CV01 import Rule_CV01\n    from sqlfluff.rules.convention.CV02 import Rule_CV02\n    from sqlfluff.rules.convention.CV03 import Rule_CV03\n    from sqlfluff.rules.convention.CV04 import Rule_CV04\n    from sqlfluff.rules.convention.CV05 import Rule_CV05\n    from sqlfluff.rules.convention.CV06 import Rule_CV06\n    from sqlfluff.rules.convention.CV07 import Rule_CV07\n    from sqlfluff.rules.convention.CV08 import Rule_CV08\n    from sqlfluff.rules.convention.CV09 import Rule_CV09\n    from sqlfluff.rules.convention.CV10 import Rule_CV10\n    from sqlfluff.rules.convention.CV11 import Rule_CV11\n    from sqlfluff.rules.convention.CV12 import Rule_CV12\n\n    return [\n        Rule_CV01,\n        Rule_CV02,\n        Rule_CV03,\n        Rule_CV04,\n        Rule_CV05,\n        Rule_CV06,\n        Rule_CV07,\n        Rule_CV08,\n        Rule_CV09,\n        Rule_CV10,\n        Rule_CV11,\n        Rule_CV12,\n    ]\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.184385061264038}
{"question": "What dependencies exist between SQLFluff's BaseSegment and RawSegment classes?", "answer": null, "relative_code_list": null, "ground_truth": "BaseSegment and RawSegment have a hierarchical dependency relationship where RawSegment depends on BaseSegment through inheritance. Key dependencies include: 1) Inheritance dependency - RawSegment inherits from BaseSegment (class RawSegment(BaseSegment)), making BaseSegment a required dependency for RawSegment; 2) Type system dependency - RawSegment depends on BaseSegment's type system, including the _class_types attribute and class_types property; 3) Position marker dependency - RawSegment uses BaseSegment's pos_marker attribute for location tracking; 4) Segment structure dependency - RawSegment depends on BaseSegment's segments attribute (though RawSegment sets it to empty tuple); 5) Method inheritance - RawSegment inherits and can override BaseSegment methods like raw, raw_upper, and other utility methods; 6) Metaclass dependency - RawSegment uses BaseSegment's SegmentMetaclass for class creation and type validation; 7) Tree traversal dependency - RawSegment participates in BaseSegment's tree traversal methods and parent-child relationships; 8) Serialization dependency - RawSegment uses BaseSegment's serialization methods for JSON and string representation; 9) UUID system dependency - RawSegment uses BaseSegment's UUID system for unique identification; 10) Cache key dependency - RawSegment depends on BaseSegment's cache key system for performance optimization. The dependency is unidirectional - BaseSegment does not depend on RawSegment, but RawSegment cannot exist without BaseSegment.", "score": null, "retrieved_content": [{"name": "RawSegment", "docstring": "This is a segment without any subsegments.", "methods": ["__init__", "__repr__", "__setattr__", "is_code", "is_comment", "is_whitespace", "raw", "raw_upper", "raw_segments", "class_types", "source_fixes", "invalidate_caches", "get_type", "is_type", "get_raw_segments", "raw_trimmed", "normalize", "raw_normalized", "stringify", "_suffix", "edit", "_get_raw_segment_kwargs", "from_result_segments"], "attributes": ["type", "_is_code", "_is_comment", "_is_whitespace", "_default_raw"], "code_location": {"file": "raw.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 16, "end_line": 300}, "type": "class"}, {"name": "BaseSegment", "docstring": "The base segment element.\n\nThis defines the base element which drives both Lexing, Parsing and Linting.\nA large chunk of the logic which defines those three operations are centered\nhere. Much of what is defined in the BaseSegment is also used by its many\nsubclasses rather than directly here.\n\nFor clarity, the `BaseSegment` is mostly centered around a segment which contains\nother subsegments. For segments which don't have *children*, refer to the\n`RawSegment` class (which still inherits from this one).\n\nSegments are used both as instances to hold chunks of text, but also as classes\nthemselves where they function a lot like grammars, and return instances of\nthemselves when they match. The many classmethods in this class are usually to serve\ntheir purpose as a matcher.", "methods": ["__init__", "__setattr__", "__eq__", "_hash", "__hash__", "__repr__", "__getstate__", "__setstate__", "_comments", "_non_comments", "is_code", "_code_indices", "is_comment", "is_whitespace", "raw", "class_types", "descendant_type_set", "direct_descendant_type_set", "raw_upper", "raw_segments", "raw_segments_with_ancestors", "source_fixes", "first_non_whitespace_segment_raw_upper", "is_templated", "_suffix", "_position_segments", "simple", "cache_key", "is_optional", "class_is_type", "structural_simplify", "match", "_recalculate_caches", "_preface", "set_as_parent", "set_parent", "get_parent", "get_type", "count_segments", "is_type", "invalidate_caches", "get_start_point_marker", "get_end_point_marker", "get_start_loc", "get_end_loc", "stringify", "to_tuple", "copy", "as_record", "get_raw_segments", "raw_normalized", "iter_segments", "iter_unparsables", "type_set", "is_raw", "get_child", "get_children", "select_children", "recursive_crawl_all", "recursive_crawl", "path_to", "_is_code_or_meta", "validate_non_code_ends", "validate_segment_with_reparse", "_log_apply_fixes_check_issue", "edit", "from_result_segments"], "attributes": ["comment_separate", "is_meta", "can_start_end_non_code", "allow_empty"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 141, "end_line": 1239}, "type": "class"}, {"name": "BinaryOperatorSegment", "docstring": "A binary operator segment.\n\nDefined here for type inheritance. Inherits from RawSegment.", "methods": [], "attributes": ["type"], "code_location": {"file": "common.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 93, "end_line": 99}, "type": "class"}, {"name": "ComparisonOperatorSegment", "docstring": "A comparison operator segment.\n\nDefined here for type inheritance. Inherits from RawSegment.", "methods": [], "attributes": ["type"], "code_location": {"file": "common.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 111, "end_line": 117}, "type": "class"}, {"name": "test__parser__base_segments_raw", "is_method": false, "class_name": null, "parameters": ["raw_seg"], "calls": ["str", "repr", "raw_seg.stringify", "raw_seg.to_tuple", "raw_seg.to_tuple"], "code_location": {"file": "segments_base_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser/segments", "start_line": 196, "end_line": 211}, "code_snippet": "def test__parser__base_segments_raw(raw_seg):\n    \"\"\"Test raw segments behave as expected.\"\"\"\n    # Check Segment Return\n    assert raw_seg.segments == ()\n    assert raw_seg.raw == \"foobar\"\n    # Check Formatting and Stringification\n    assert str(raw_seg) == repr(raw_seg) == \"<CodeSegment: ([L:  1, P:  1]) 'foobar'>\"\n    assert (\n        raw_seg.stringify(ident=1, tabsize=2)\n        == \"[L:  1, P:  1]      |  raw:                                                \"\n        \"        'foobar'\\n\"\n    )\n    # Check tuple\n    assert raw_seg.to_tuple() == (\"raw\", ())\n    # Check tuple\n    assert raw_seg.to_tuple(show_raw=True) == (\"raw\", \"foobar\")\n", "type": "function"}, {"name": "CodeSegment", "docstring": "An alias for RawSegment.\n\nThis has a more explicit name for segment creation.", "methods": [], "attributes": [], "code_location": {"file": "common.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 11, "end_line": 17}, "type": "class"}, {"name": "Segments", "docstring": "Encapsulates a sequence of one or more BaseSegments.\n\nThe segments may or may not be contiguous in a parse tree.\nProvides useful operations on a sequence of segments to simplify rule creation.", "methods": ["__new__", "__init__", "__add__", "__radd__", "find", "all", "any", "reversed", "raw_slices", "raw_segments", "recursive_crawl_all", "recursive_crawl", "children", "first", "last", "__iter__", "__getitem__", "__getitem__", "__getitem__", "get", "apply", "select", "iterate_segments"], "attributes": [], "code_location": {"file": "segments.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/utils/functional", "start_line": 20, "end_line": 224}, "type": "class"}, {"name": "test__parser__base_segments_raw_compare", "is_method": false, "class_name": null, "parameters": [], "calls": ["TemplatedFile.from_string", "RawSegment", "RawSegment", "PositionMarker", "PositionMarker", "slice", "slice", "slice", "slice"], "code_location": {"file": "segments_base_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser/segments", "start_line": 245, "end_line": 250}, "code_snippet": "def test__parser__base_segments_raw_compare():\n    \"\"\"Test comparison of raw segments.\"\"\"\n    template = TemplatedFile.from_string(\"foobar\")\n    rs1 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n    rs2 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n    assert rs1 == rs2\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "BaseParser", "parameters": ["self", "raw_class", "type", "optional", "trim_chars", "casefold"], "calls": ["uuid4"], "code_location": {"file": "parsers.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 28, "end_line": 46}, "code_snippet": "    def __init__(\n        self,\n        raw_class: type[RawSegment],\n        type: Optional[str] = None,\n        optional: bool = False,\n        # The following kwargs are passed on to the segment:\n        trim_chars: Optional[tuple[str, ...]] = None,\n        casefold: Optional[Callable[[str], str]] = None,\n    ) -> None:\n        self.raw_class = raw_class\n        # Store instance_types rather than just type to allow\n        # for multiple possible types to be supported in derivative\n        # classes.\n        self._instance_types: tuple[str, ...] = (type or raw_class.type,)\n        self.optional = optional\n        self._trim_chars = trim_chars\n        self.casefold = casefold\n        # Generate a cache key\n        self._cache_key = uuid4().hex\n", "type": "function"}, {"name": "test__parser__raw_segment_raw_normalized", "is_method": false, "class_name": null, "parameters": [], "calls": ["TemplatedFile.from_string", "RawSegment", "RawSegment", "RawSegment", "BaseSegment", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "rs1.raw_normalized", "rs1.raw_normalized", "rs2.raw_normalized", "rs2.raw_normalized", "rs3.raw_normalized", "rs3.raw_normalized", "bs1.raw_normalized", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice"], "code_location": {"file": "segments_base_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser/segments", "start_line": 332, "end_line": 371}, "code_snippet": "def test__parser__raw_segment_raw_normalized():\n    \"\"\"Test comparison of raw segments.\"\"\"\n    template = TemplatedFile.from_string('\"a\"\"\".\"e\"')\n    rs1 = RawSegment(\n        '\"a\"\"\"',\n        PositionMarker(slice(0, 5), slice(0, 5), template),\n        quoted_value=(r'\"((?:[^\"]|\"\")*)\"', 1),\n        escape_replacements=[('\"\"', '\"')],\n        casefold=str.upper,\n    )\n    rs2 = RawSegment(\n        \".\",\n        PositionMarker(slice(6, 7), slice(6, 7), template),\n    )\n    rs3 = RawSegment(\n        '\"e\"',\n        PositionMarker(slice(8, 10), slice(8, 10), template),\n        quoted_value=(r'\"((?:[^\"]|\"\")*)\"', 1),\n        escape_replacements=[('\"\"', '\"')],\n        casefold=str.upper,\n    )\n    bs1 = BaseSegment(\n        (\n            rs1,\n            rs2,\n            rs3,\n        ),\n        PositionMarker(slice(0, 10), slice(0, 10), template),\n    )\n    assert rs1.raw == '\"a\"\"\"'\n    assert rs1.raw_normalized(False) == 'a\"'\n    assert rs1.raw_normalized() == 'A\"'\n    assert rs2.raw == \".\"\n    assert rs2.raw_normalized(False) == \".\"\n    assert rs2.raw_normalized() == \".\"\n    assert rs3.raw == '\"e\"'\n    assert rs3.raw_normalized(False) == \"e\"\n    assert rs3.raw_normalized() == \"E\"\n    assert bs1.raw == '\"a\"\"\".\"e\"'\n    assert bs1.raw_normalized() == 'A\".E'\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.170921802520752}
{"question": "Why does SQLFluff's parallel processing reduce execution time for multiple files?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's parallel processing reduces execution time for multiple files by utilizing multiple CPU cores to process files concurrently. Key benefits include: 1) CPU utilization - Parallel processing takes advantage of multi-core systems to process multiple files simultaneously; 2) I/O optimization - Multiple files can be read and processed concurrently, reducing I/O wait times; 3) Scalability - Performance scales with the number of available CPU cores; 4) Resource efficiency - Better utilization of system resources by distributing work across multiple processes; 5) Reduced wall-clock time - Overall execution time is significantly reduced compared to sequential processing; 6) Batch processing - Large numbers of files can be processed efficiently in parallel batches; 7) Process isolation - Each file is processed in its own process, preventing memory leaks and improving stability; 8) Error isolation - Errors in one file don't affect processing of other files; 9) Configuration sharing - Common configuration and dialect objects can be shared across processes; 10) Progress tracking - Parallel processing supports progress bars and real-time feedback for long-running operations.", "score": null, "retrieved_content": [{"name": "test__linter__linting_parallel_thread", "is_method": false, "class_name": null, "parameters": ["force_error", "monkeypatch"], "calls": ["pytest.mark.parametrize", "FluffConfig", "make_output_stream", "Linter", "lntr.lint_paths", "all", "monkeypatch.setattr", "monkeypatch.setattr", "ErrorPool", "OutputStreamFormatter", "isinstance", "result.get_violations", "runner.DelayedException", "ValueError"], "code_location": {"file": "linter_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 285, "end_line": 329}, "code_snippet": "def test__linter__linting_parallel_thread(force_error, monkeypatch):\n    \"\"\"Run linter in parallel mode using threads.\n\n    Similar to test__linter__linting_result_get_violations but uses a thread\n    pool of 1 worker to test parallel mode without subprocesses. This lets the\n    tests capture code coverage information for the backend parts of parallel\n    execution without having to jump through hoops.\n    \"\"\"\n    if not force_error:\n        monkeypatch.setattr(Linter, \"allow_process_parallelism\", False)\n\n    else:\n\n        def _create_pool(*args, **kwargs):\n            class ErrorPool:\n                def __enter__(self):\n                    return self\n\n                def __exit__(self, exc_type, exc_val, exc_tb):\n                    pass\n\n                def imap_unordered(self, *args, **kwargs):\n                    yield runner.DelayedException(ValueError())\n\n            return ErrorPool()\n\n        monkeypatch.setattr(runner.MultiProcessRunner, \"_create_pool\", _create_pool)\n\n    config = FluffConfig(overrides={\"dialect\": \"ansi\"})\n    output_stream = make_output_stream(config, None, os.devnull)\n    lntr = Linter(\n        formatter=OutputStreamFormatter(output_stream, False, verbosity=0),\n        dialect=\"ansi\",\n    )\n    result = lntr.lint_paths(\n        # NOTE: Lint more than one file to make sure we enabled the multithreaded\n        # code path.\n        (\n            \"test/fixtures/linter/comma_errors.sql\",\n            \"test/fixtures/linter/whitespace_errors.sql\",\n        ),\n        processes=2,\n    )\n\n    all([isinstance(v, SQLLintError) for v in result.get_violations()])\n", "type": "function"}, {"name": "lint", "is_method": false, "class_name": null, "parameters": ["paths", "format", "write_output", "annotation_level", "nofail", "disregard_sqlfluffignores", "logger", "bench", "processes", "disable_progress_bar", "persist_timing", "extra_config_path", "ignore_local_config", "stdin_filename"], "calls": ["cli.command", "click.option", "click.option", "click.option", "click.option", "click.argument", "get_config", "make_output_stream", "get_linter_and_formatter", "config.get", "formatter.dispatch_config", "set_logging_level", "output_stream.close", "click.echo", "PathAndUserErrorHandler", "click.echo", "json.dumps", "dump_file_payload", "result.persist_timing_records", "click.echo", "click.echo", "result.timing_summary", "isinstance", "sys.exit", "sys.exit", "click.Choice", "click.Choice", "click.Path", "format_linting_result_header", "lnt.lint_string_wrapped", "lnt.lint_paths", "formatter.format_linting_stats", "result.as_records", "yaml.dump", "formatter.cli_table", "click.echo", "click.echo", "formatter.completion_message", "result.stats", "lnt.config.make_child_from_path", "sys.stdin.read", "result.as_records", "formatter.cli_table", "result.as_records", "json.dumps", "items", "result.as_records", "join", "github_result.append", "github_result_native.append", "github_result_native.append", "github_result_native.append", "violation.get", "violation.get"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 581, "end_line": 781}, "code_snippet": "def lint(\n    paths: tuple[str],\n    format: str,\n    write_output: Optional[str],\n    annotation_level: str,\n    nofail: bool,\n    disregard_sqlfluffignores: bool,\n    logger: Optional[logging.Logger] = None,\n    bench: bool = False,\n    processes: Optional[int] = None,\n    disable_progress_bar: Optional[bool] = False,\n    persist_timing: Optional[str] = None,\n    extra_config_path: Optional[str] = None,\n    ignore_local_config: bool = False,\n    stdin_filename: Optional[str] = None,\n    **kwargs,\n) -> None:\n    \"\"\"Lint SQL files via passing a list of files or using stdin.\n\n    PATH is the path to a sql file or directory to lint. This can be either a\n    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n    be interpreted like passing the current working directory as a path argument.\n\n    Linting SQL files:\n\n        sqlfluff lint path/to/file.sql\n        sqlfluff lint directory/of/sql/files\n\n    Linting a file via stdin (note the lone '-' character):\n\n        cat path/to/file.sql | sqlfluff lint -\n        echo 'select col from tbl' | sqlfluff lint -\n\n    \"\"\"\n    config = get_config(\n        extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n    )\n    non_human_output = (format != FormatType.human.value) or (write_output is not None)\n    file_output = None\n    output_stream = make_output_stream(config, format, write_output)\n    lnt, formatter = get_linter_and_formatter(config, output_stream)\n\n    verbose = config.get(\"verbose\")\n    progress_bar_configuration.disable_progress_bar = disable_progress_bar\n\n    formatter.dispatch_config(lnt)\n\n    # Set up logging.\n    set_logging_level(\n        verbosity=verbose,\n        formatter=formatter,\n        logger=logger,\n        stderr_output=non_human_output,\n    )\n\n    # Output the results as we go\n    if verbose >= 1 and not non_human_output:\n        click.echo(format_linting_result_header())\n\n    with PathAndUserErrorHandler(formatter):\n        # add stdin if specified via lone '-'\n        if (\"-\",) == paths:\n            if stdin_filename:\n                lnt.config = lnt.config.make_child_from_path(\n                    stdin_filename, require_dialect=False\n                )\n            result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\n        else:\n            result = lnt.lint_paths(\n                paths,\n                ignore_non_existent_files=False,\n                ignore_files=not disregard_sqlfluffignores,\n                processes=processes,\n                # If we're just linting in the CLI, we don't need to retain the\n                # raw file content. This allows us to reduce memory overhead.\n                retain_files=False,\n            )\n\n    # Output the final stats\n    if verbose >= 1 and not non_human_output:\n        click.echo(formatter.format_linting_stats(result, verbose=verbose))\n\n    if format == FormatType.json.value:\n        file_output = json.dumps(result.as_records())\n    elif format == FormatType.yaml.value:\n        file_output = yaml.dump(\n            result.as_records(),\n            sort_keys=False,\n            allow_unicode=True,\n        )\n    elif format == FormatType.none.value:\n        file_output = \"\"\n    elif format == FormatType.github_annotation.value:\n        if annotation_level == \"error\":\n            annotation_level = \"failure\"\n\n        github_result = []\n        for record in result.as_records():\n            filepath = record[\"filepath\"]\n            for violation in record[\"violations\"]:\n                # NOTE: The output format is designed for this GitHub action:\n                # https://github.com/yuzutech/annotations-action\n                # It is similar, but not identical, to the native GitHub format:\n                # https://docs.github.com/en/rest/reference/checks#annotations-items\n                github_result.append(\n                    {\n                        \"file\": filepath,\n                        \"start_line\": violation[\"start_line_no\"],\n                        \"start_column\": violation[\"start_line_pos\"],\n                        # NOTE: There should always be a start, there _may_ not be an\n                        # end, so in that case we default back to just reusing\n                        # the start.\n                        \"end_line\": violation.get(\n                            \"end_line_no\", violation[\"start_line_no\"]\n                        ),\n                        \"end_column\": violation.get(\n                            \"end_line_pos\", violation[\"start_line_pos\"]\n                        ),\n                        \"title\": \"SQLFluff\",\n                        \"message\": f\"{violation['code']}: {violation['description']}\",\n                        # The annotation_level is configurable, but will only apply\n                        # to any SQLFluff rules which have not been downgraded\n                        # to warnings using the `warnings` config value. Any which have\n                        # been set to warn rather than fail will always be given the\n                        # `notice` annotation level in the serialised result.\n                        \"annotation_level\": (\n                            annotation_level if not violation[\"warning\"] else \"notice\"\n                        ),\n                    }\n                )\n        file_output = json.dumps(github_result)\n    elif format == FormatType.github_annotation_native.value:\n        if annotation_level == \"failure\":\n            annotation_level = \"error\"\n\n        github_result_native = []\n        for record in result.as_records():\n            filepath = record[\"filepath\"]\n\n            # Add a group, titled with the filename\n            if record[\"violations\"]:\n                github_result_native.append(f\"::group::{filepath}\")\n\n            for violation in record[\"violations\"]:\n                # NOTE: The output format is designed for GitHub action:\n                # https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#setting-a-notice-message\n\n                # The annotation_level is configurable, but will only apply\n                # to any SQLFluff rules which have not been downgraded\n                # to warnings using the `warnings` config value. Any which have\n                # been set to warn rather than fail will always be given the\n                # `notice` annotation level in the serialised result.\n                line = \"::notice \" if violation[\"warning\"] else f\"::{annotation_level} \"\n\n                line += \"title=SQLFluff,\"\n                line += f\"file={filepath},\"\n                line += f\"line={violation['start_line_no']},\"\n                line += f\"col={violation['start_line_pos']}\"\n                if \"end_line_no\" in violation:\n                    line += f\",endLine={violation['end_line_no']}\"\n                if \"end_line_pos\" in violation:\n                    line += f\",endColumn={violation['end_line_pos']}\"\n                line += \"::\"\n                line += f\"{violation['code']}: {violation['description']}\"\n                if violation[\"name\"]:\n                    line += f\" [{violation['name']}]\"\n\n                github_result_native.append(line)\n\n            # Close the group\n            if record[\"violations\"]:\n                github_result_native.append(\"::endgroup::\")\n\n        file_output = \"\\n\".join(github_result_native)\n\n    if file_output:\n        dump_file_payload(write_output, file_output)\n\n    if persist_timing:\n        result.persist_timing_records(persist_timing)\n\n    output_stream.close()\n    if bench:\n        click.echo(\"==== overall timings ====\")\n        click.echo(formatter.cli_table([(\"Clock time\", result.total_time)]))\n        timing_summary = result.timing_summary()\n        for step in timing_summary:\n            click.echo(f\"=== {step} ===\")\n            click.echo(\n                formatter.cli_table(timing_summary[step].items(), cols=3, col_width=20)\n            )\n\n    if not nofail:\n        if not non_human_output:\n            formatter.completion_message()\n        exit_code = result.stats(EXIT_FAIL, EXIT_SUCCESS)[\"exit code\"]\n        assert isinstance(exit_code, int), \"result.stats error code must be integer.\"\n        sys.exit(exit_code)\n    else:\n        sys.exit(EXIT_SUCCESS)\n", "type": "function"}, {"name": "lint_paths", "is_method": true, "class_name": "Linter", "parameters": ["self", "paths", "fix", "ignore_non_existent_files", "ignore_files", "processes", "apply_fixes", "fixed_file_suffix", "fix_even_unparsable", "retain_files"], "calls": ["LintingResult", "split", "len", "get_runner", "tqdm", "enumerate", "result.stop_timer", "LintedDir", "result.add", "paths_from_path", "self.config.get", "self.formatter.dispatch_processing_header", "runner.run", "linted_dir.add", "any", "progress_bar_files.update", "os.getcwd", "lower", "expanded_paths.append", "linter_logger.error", "linted_file.num_violations", "len", "progress_bar_files.set_description", "linted_file.persist_tree", "self.config.get"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 1033, "end_line": 1128}, "code_snippet": "    def lint_paths(\n        self,\n        paths: tuple[str, ...],\n        fix: bool = False,\n        ignore_non_existent_files: bool = False,\n        ignore_files: bool = True,\n        processes: Optional[int] = None,\n        apply_fixes: bool = False,\n        fixed_file_suffix: str = \"\",\n        fix_even_unparsable: bool = False,\n        retain_files: bool = True,\n    ) -> LintingResult:\n        \"\"\"Lint an iterable of paths.\"\"\"\n        # If no paths specified - assume local\n        if not paths:  # pragma: no cover\n            paths = (os.getcwd(),)\n        # Set up the result to hold what we get back\n        result = LintingResult()\n\n        expanded_paths: list[str] = []\n        expanded_path_to_linted_dir = {}\n        sql_exts = self.config.get(\"sql_file_exts\", default=\".sql\").lower().split(\",\")\n\n        for path in paths:\n            linted_dir = LintedDir(path, retain_files=retain_files)\n            result.add(linted_dir)\n            for fname in paths_from_path(\n                path,\n                ignore_non_existent_files=ignore_non_existent_files,\n                ignore_files=ignore_files,\n                target_file_exts=sql_exts,\n            ):\n                expanded_paths.append(fname)\n                expanded_path_to_linted_dir[fname] = linted_dir\n\n        files_count = len(expanded_paths)\n        if processes is None:\n            processes = self.config.get(\"processes\", default=1)\n        assert processes is not None\n        # Hard set processes to 1 if only 1 file is queued.\n        # The overhead will never be worth it with one file.\n        if files_count == 1:\n            processes = 1\n\n        # to avoid circular import\n        from sqlfluff.core.linter.runner import get_runner\n\n        runner, effective_processes = get_runner(\n            self,\n            self.config,\n            processes=processes,\n            allow_process_parallelism=self.allow_process_parallelism,\n        )\n\n        if self.formatter and effective_processes != 1:\n            self.formatter.dispatch_processing_header(effective_processes)\n\n        # Show files progress bar only when there is more than one.\n        first_path = expanded_paths[0] if expanded_paths else \"\"\n        progress_bar_files = tqdm(\n            total=files_count,\n            desc=f\"file {first_path}\",\n            leave=False,\n            disable=files_count <= 1 or progress_bar_configuration.disable_progress_bar,\n        )\n\n        for i, linted_file in enumerate(runner.run(expanded_paths, fix), start=1):\n            linted_dir = expanded_path_to_linted_dir[linted_file.path]\n            linted_dir.add(linted_file)\n            # If any fatal errors, then stop iteration.\n            if any(v.fatal for v in linted_file.violations):  # pragma: no cover\n                linter_logger.error(\"Fatal linting error. Halting further linting.\")\n                break\n\n            # If we're applying fixes, then do that here.\n            if apply_fixes:\n                num_tmp_prs_errors = linted_file.num_violations(\n                    types=TMP_PRS_ERROR_TYPES,\n                    filter_ignore=False,\n                    filter_warning=False,\n                )\n                if fix_even_unparsable or num_tmp_prs_errors == 0:\n                    linted_file.persist_tree(\n                        suffix=fixed_file_suffix, formatter=self.formatter\n                    )\n\n            # Progress bar for files is rendered only when there is more than one file.\n            # Additionally, as it's updated after each loop, we need to get file name\n            # from the next loop. This is why `enumerate` starts with `1` and there\n            # is `i < len` to not exceed files list length.\n            progress_bar_files.update(n=1)\n            if i < len(expanded_paths):\n                progress_bar_files.set_description(f\"file {expanded_paths[i]}\")\n\n        result.stop_timer()\n        return result\n", "type": "function"}, {"name": "fix", "is_method": false, "class_name": null, "parameters": ["force", "paths", "disregard_sqlfluffignores", "check", "bench", "quiet", "fixed_suffix", "logger", "processes", "disable_progress_bar", "persist_timing", "extra_config_path", "ignore_local_config", "show_lint_violations", "stdin_filename"], "calls": ["cli.command", "click.option", "click.option", "click.option", "click.option", "click.option", "click.option", "click.argument", "get_config", "config.get", "make_output_stream", "get_linter_and_formatter", "config.get", "formatter.dispatch_config", "set_logging_level", "click.echo", "PathAndUserErrorHandler", "click.Path", "click.echo", "sys.exit", "formatter.colorize", "_stdin_fix", "_paths_fix", "lnt.config.make_child_from_path"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 1059, "end_line": 1149}, "code_snippet": "def fix(\n    force: bool,\n    paths: tuple[str],\n    disregard_sqlfluffignores: bool,\n    check: bool = False,\n    bench: bool = False,\n    quiet: bool = False,\n    fixed_suffix: str = \"\",\n    logger: Optional[logging.Logger] = None,\n    processes: Optional[int] = None,\n    disable_progress_bar: Optional[bool] = False,\n    persist_timing: Optional[str] = None,\n    extra_config_path: Optional[str] = None,\n    ignore_local_config: bool = False,\n    show_lint_violations: bool = False,\n    stdin_filename: Optional[str] = None,\n    **kwargs,\n) -> None:\n    \"\"\"Fix SQL files.\n\n    PATH is the path to a sql file or directory to lint. This can be either a\n    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n    be interpreted like passing the current working directory as a path argument.\n    \"\"\"\n    # some quick checks\n    fixing_stdin = (\"-\",) == paths\n    if quiet:\n        if kwargs[\"verbose\"]:\n            click.echo(\n                \"ERROR: The --quiet flag can only be used if --verbose is not set.\",\n            )\n            sys.exit(EXIT_ERROR)\n        kwargs[\"verbose\"] = -1\n\n    config = get_config(\n        extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n    )\n    fix_even_unparsable = config.get(\"fix_even_unparsable\")\n    output_stream = make_output_stream(\n        config, None, os.devnull if fixing_stdin else None\n    )\n    lnt, formatter = get_linter_and_formatter(\n        config, output_stream, show_lint_violations\n    )\n\n    verbose = config.get(\"verbose\")\n    progress_bar_configuration.disable_progress_bar = disable_progress_bar\n\n    formatter.dispatch_config(lnt)\n\n    # Set up logging.\n    set_logging_level(\n        verbosity=verbose,\n        formatter=formatter,\n        logger=logger,\n        stderr_output=fixing_stdin,\n    )\n\n    if force:\n        click.echo(\n            formatter.colorize(\n                \"The -f/--force option is deprecated as it is now the \"\n                \"default behaviour.\",\n                Color.red,\n            ),\n            err=True,\n        )\n\n    with PathAndUserErrorHandler(formatter):\n        # handle stdin case. should output formatted sql to stdout and nothing else.\n        if fixing_stdin:\n            if stdin_filename:\n                lnt.config = lnt.config.make_child_from_path(\n                    stdin_filename, require_dialect=False\n                )\n            _stdin_fix(lnt, formatter, fix_even_unparsable)\n        else:\n            _paths_fix(\n                lnt,\n                formatter,\n                paths,\n                processes,\n                fix_even_unparsable,\n                fixed_suffix,\n                bench,\n                show_lint_violations,\n                check=check,\n                persist_timing=persist_timing,\n                ignore_files=not disregard_sqlfluffignores,\n            )\n", "type": "function"}, {"name": "test_lint_path_parallel_wrapper_exception", "is_method": false, "class_name": null, "parameters": ["patched_lint"], "calls": ["patch", "ValueError", "run", "isinstance", "runner.MultiThreadRunner", "pytest.raises", "result.reraise", "Linter", "FluffConfig"], "code_location": {"file": "linter_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 333, "end_line": 347}, "code_snippet": "def test_lint_path_parallel_wrapper_exception(patched_lint):\n    \"\"\"Tests the error catching behavior of _lint_path_parallel_wrapper().\n\n    Test on MultiThread runner because otherwise we have pickling issues.\n    \"\"\"\n    patched_lint.side_effect = ValueError(\"Something unexpected happened\")\n    for result in runner.MultiThreadRunner(\n        Linter(), FluffConfig(overrides={\"dialect\": \"ansi\"}), processes=1\n    ).run(\n        [\"test/fixtures/linter/passing.sql\"],\n        fix=False,\n    ):\n        assert isinstance(result, runner.DelayedException)\n        with pytest.raises(ValueError):\n            result.reraise()\n", "type": "function"}, {"name": "test_lint_jj01_pickled_config", "is_method": false, "class_name": null, "parameters": [], "calls": ["FluffConfig", "Linter", "next", "linter.get_rulepack", "pickle.dumps", "pickle.loads", "rule.crawl", "linter.parse_path", "len", "check_tuple", "unpickled_cfg.get"], "code_location": {"file": "std_JJ01_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/rules", "start_line": 13, "end_line": 42}, "code_snippet": "def test_lint_jj01_pickled_config():\n    \"\"\"Tests the error catching behavior of _lint_path_parallel_wrapper().\n\n    Test on MultiThread runner because otherwise we have pickling issues.\n    \"\"\"\n    fname = \"test/fixtures/linter/jinja_spacing.sql\"\n    fresh_cfg = FluffConfig(overrides={\"dialect\": \"ansi\", \"rules\": \"JJ01\"})\n    # Parse the file with the fresh config.\n    linter = Linter(config=fresh_cfg)\n    parsed = next(linter.parse_path(fname))\n    rule_pack = linter.get_rulepack(config=fresh_cfg)\n    rule = rule_pack.rules[0]\n    # Check we got the right rule.\n    assert rule.code == \"JJ01\"\n    # Pickle the config and rehydrate to simulate threaded operation\n    pickled = pickle.dumps(fresh_cfg)\n    unpickled_cfg = pickle.loads(pickled)\n    # Crawl with the pickled config. Check we don't get an error.\n    linting_errors, _, fixes, _ = rule.crawl(\n        parsed.tree,\n        dialect=unpickled_cfg.get(\"dialect_obj\"),\n        fix=True,\n        templated_file=parsed.parsed_variants[0].templated_file,\n        ignore_mask=None,\n        fname=fname,\n        config=unpickled_cfg,  # <- NOTE: This is the important part.\n    )\n    # Check we successfully got the right results.\n    assert len(linting_errors) == 1\n    assert linting_errors[0].check_tuple() == (\"JJ01\", 3, 15)\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "SQLFluffDriver", "parameters": ["self"], "calls": ["__init__", "super", "s.encode", "sys.getfilesystemencoding"], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 25, "end_line": 34}, "code_snippet": "    def __init__(self) -> None:\n        super().__init__(\n            [sys.executable, \"-m\", \"sqlfluff.cli.commands\"],\n            [\".sql\"],\n            [\n                s.encode(sys.getfilesystemencoding())\n                for s in [\"sqlfluff\", \"lint\", \"--format=json\"]\n            ],\n            exit_codes=[0, 1],\n        )\n", "type": "function"}, {"name": "test__cli__command_lint_serialize_multiple_files", "is_method": false, "class_name": null, "parameters": ["serialize", "write_file", "tmp_path"], "calls": ["pytest.mark.parametrize", "pytest.mark.parametrize", "invoke_assert_code", "len", "print", "print", "print", "print", "os.path.join", "result_payload.split", "open", "payload_file.read", "ext.get", "json.loads", "len", "yaml.safe_load", "len", "json.loads", "len", "result_payload.split", "len"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 1705, "end_line": 1770}, "code_snippet": "def test__cli__command_lint_serialize_multiple_files(serialize, write_file, tmp_path):\n    \"\"\"Test the output formats for multiple files.\n\n    This tests runs both stdout checking and file checking.\n    \"\"\"\n    fpath1 = \"test/fixtures/linter/indentation_errors.sql\"\n    fpath2 = \"test/fixtures/linter/multiple_sql_errors.sql\"\n\n    cmd_args = (\n        fpath1,\n        fpath2,\n        \"--format\",\n        serialize,\n        \"--disable-progress-bar\",\n    )\n\n    if write_file:\n        ext = {\n            \"human\": \".txt\",\n            \"yaml\": \".yaml\",\n        }\n        target_file = os.path.join(tmp_path, write_file + ext.get(serialize, \".json\"))\n        cmd_args += (\"--write-output\", target_file)\n\n    # note the file is in here twice. two files = two payloads.\n    result = invoke_assert_code(\n        args=[lint, cmd_args],\n        ret_code=1,\n    )\n\n    # NOTE: The \"none\" serializer doesn't write a file even if specified.\n    if write_file and serialize != \"none\":\n        with open(target_file, \"r\") as payload_file:\n            result_payload = payload_file.read()\n    else:\n        result_payload = result.stdout\n\n    # Print for debugging.\n    payload_length = len(result_payload.split(\"\\n\"))\n    print(\"=== BEGIN RESULT OUTPUT\")\n    print(result_payload)\n    print(\"=== END RESULT OUTPUT\")\n    print(\"Result length:\", payload_length)\n\n    if serialize == \"human\":\n        assert payload_length == 25 if write_file else 34\n    elif serialize == \"none\":\n        assert payload_length == 1  # There will be a single newline.\n    elif serialize == \"json\":\n        result = json.loads(result_payload)\n        assert len(result) == 2\n    elif serialize == \"yaml\":\n        result = yaml.safe_load(result_payload)\n        assert len(result) == 2\n    elif serialize == \"github-annotation\":\n        result = json.loads(result_payload)\n        filepaths = {r[\"file\"] for r in result}\n        assert len(filepaths) == 2\n    elif serialize == \"github-annotation-native\":\n        result = result_payload.split(\"\\n\")\n        # SQLFluff produces trailing newline\n        if result[-1] == \"\":\n            del result[-1]\n        assert len(result) == 16\n    else:\n        raise Exception\n", "type": "function"}, {"name": "test__cli__multiple_files__fix_multiple_errors_show_errors", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code", "unfix_err_log.index", "unfix_err_log.index", "os.path.normpath", "os.path.normpath", "result.stdout.index"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 2431, "end_line": 2460}, "code_snippet": "def test__cli__multiple_files__fix_multiple_errors_show_errors():\n    \"\"\"Basic check of lint ensures with multiple files, filenames are listed.\"\"\"\n    sql_path = \"test/fixtures/linter/multiple_sql_errors.sql\"\n    indent_path = \"test/fixtures/linter/indentation_errors.sql\"\n    result = invoke_assert_code(\n        ret_code=1,\n        args=[\n            fix,\n            [\n                \"--disable-progress-bar\",\n                \"--check\",  # Run in check mode to get the confirmation.\n                \"--show-lint-violations\",\n                sql_path,\n                indent_path,\n            ],\n        ],\n    )\n\n    unfixable_error_msg = \"==== lint for unfixable violations ====\"\n    assert unfixable_error_msg in result.stdout\n\n    indent_pass_msg = f\"== [{os.path.normpath(indent_path)}] PASS\"\n    multi_fail_msg = f\"== [{os.path.normpath(sql_path)}] FAIL\"\n\n    unfix_err_log = result.stdout[result.stdout.index(unfixable_error_msg) :]\n    assert indent_pass_msg in unfix_err_log\n    assert multi_fail_msg in unfix_err_log\n\n    # Assert that they are sorted in alphabetical order\n    assert unfix_err_log.index(indent_pass_msg) < unfix_err_log.index(multi_fail_msg)\n", "type": "function"}, {"name": "test__linter__linting_result_stats", "is_method": false, "class_name": null, "parameters": ["path", "stats"], "calls": ["pytest.mark.parametrize", "Linter", "lntr.lint_paths", "result.stats"], "code_location": {"file": "linter_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 258, "end_line": 266}, "code_snippet": "def test__linter__linting_result_stats(path, stats):\n    \"\"\"Test that a LintingResult can get the right stats with multiple files.\n\n    https://github.com/sqlfluff/sqlfluff/issues/5673\n    \"\"\"\n    lntr = Linter()\n    result = lntr.lint_paths((f\"test/fixtures/linter/exit_codes/{path}\",))\n    # NOTE: We're using fake return codes for testing purposes.\n    assert result.stats(111, 222) == stats\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1427404880523682}
{"question": "Where does SQLFluff's parsing flow from source files through lexing to AST generation?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's parsing flow follows a multi-stage pipeline from source files to AST generation. The flow includes: 1) Source file input - SQL files are read from disk or provided as strings to the Linter class; 2) Template processing - If templated SQL is detected, the Templater (Jinja, Python, etc.) renders the template to valid SQL; 3) Lexing stage - The Lexer class breaks down SQL into individual tokens (RawSegment objects) using dialect-specific lexer matchers; 4) Token processing - Lexed tokens are mapped to template slices and converted to RawSegment objects with position markers; 5) Parser initialization - The Parser class is instantiated with the appropriate dialect and configuration; 6) Grammar application - Dialect-specific grammars are applied to lexed segments to identify SQL structures; 7) Tree construction - Segments are recursively matched and combined into a hierarchical parse tree structure; 8) AST generation - The final parse tree (AST) is created with FileSegment as root containing StatementSegments and their sub-components; 9) Validation - The parser validates that all segments were properly processed and no content was lost; 10) Error handling - Any parsing failures are captured as SQLParseError objects for reporting.", "score": null, "retrieved_content": [{"name": "_parse_tokens", "is_method": true, "class_name": "Linter", "parameters": ["tokens", "config", "fname", "parse_statistics"], "calls": ["Parser", "linter_logger.info", "linter_logger.info", "parsed.iter_unparsables", "parser.parse", "format", "violations.append", "linter_logger.info", "linter_logger.info", "tuple", "linter_logger.info", "violations.append", "parsed.stringify", "SQLParseError", "unparsable.stringify", "format", "len"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 231, "end_line": 281}, "code_snippet": "    def _parse_tokens(\n        tokens: Sequence[BaseSegment],\n        config: FluffConfig,\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> tuple[Optional[BaseSegment], list[SQLParseError]]:\n        parser = Parser(config=config)\n        violations = []\n        # Parse the file and log any problems\n        try:\n            parsed: Optional[BaseSegment] = parser.parse(\n                # Regardless of how the sequence was passed in, we should\n                # coerce it to a tuple here, before we head deeper into\n                # the parsing process.\n                tuple(tokens),\n                fname=fname,\n                parse_statistics=parse_statistics,\n            )\n        except SQLParseError as err:\n            linter_logger.info(\"PARSING FAILED! : %s\", err)\n            violations.append(err)\n            return None, violations\n\n        if parsed is None:  # pragma: no cover\n            return None, violations\n\n        linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n        linter_logger.info(\"\\n\" + parsed.stringify())\n        # We may succeed parsing, but still have unparsable segments. Extract them\n        # here.\n        for unparsable in parsed.iter_unparsables():\n            # No exception has been raised explicitly, but we still create one here\n            # so that we can use the common interface\n            assert unparsable.pos_marker\n            violations.append(\n                SQLParseError(\n                    \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n                    \"{1!r}\".format(\n                        unparsable.pos_marker.working_loc,\n                        (\n                            unparsable.raw\n                            if len(unparsable.raw) < 40\n                            else unparsable.raw[:40] + \"...\"\n                        ),\n                    ),\n                    segment=unparsable,\n                )\n            )\n            linter_logger.info(\"Found unparsable segment...\")\n            linter_logger.info(unparsable.stringify())\n        return parsed, violations\n", "type": "function"}, {"name": "parse", "is_method": false, "class_name": null, "parameters": ["path", "code_only", "include_meta", "format", "write_output", "bench", "nofail", "logger", "extra_config_path", "ignore_local_config", "parse_statistics", "stdin_filename"], "calls": ["cli.command", "click.argument", "click.option", "click.option", "click.option", "click.option", "click.option", "click.option", "get_config", "make_output_stream", "get_linter_and_formatter", "c.get", "formatter.dispatch_config", "set_logging_level", "time.monotonic", "PathAndUserErrorHandler", "time.monotonic", "formatter.print_out_violations_and_timing", "dump_file_payload", "sys.exit", "sys.exit", "click.Path", "click.Choice", "list", "parsed_string.root_variant", "len", "parsed_strings_dict.append", "yaml.add_representer", "yaml.dump", "file_config.make_child_from_path", "lnt.parse_string", "lnt.parse_path", "root_variant.tree.as_record", "json.dumps", "sys.stdin.read"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 1323, "end_line": 1442}, "code_snippet": "def parse(\n    path: str,\n    code_only: bool,\n    include_meta: bool,\n    format: str,\n    write_output: Optional[str],\n    bench: bool,\n    nofail: bool,\n    logger: Optional[logging.Logger] = None,\n    extra_config_path: Optional[str] = None,\n    ignore_local_config: bool = False,\n    parse_statistics: bool = False,\n    stdin_filename: Optional[str] = None,\n    **kwargs,\n) -> None:\n    \"\"\"Parse SQL files and just spit out the result.\n\n    PATH is the path to a sql file or directory to lint. This can be either a\n    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n    be interpreted like passing the current working directory as a path argument.\n    \"\"\"\n    c = get_config(\n        extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n    )\n    # We don't want anything else to be logged if we want json or yaml output\n    # unless we're writing to a file.\n    non_human_output = (format != FormatType.human.value) or (write_output is not None)\n    output_stream = make_output_stream(c, format, write_output)\n    lnt, formatter = get_linter_and_formatter(c, output_stream)\n    verbose = c.get(\"verbose\")\n\n    progress_bar_configuration.disable_progress_bar = True\n\n    formatter.dispatch_config(lnt)\n\n    # Set up logging.\n    set_logging_level(\n        verbosity=verbose,\n        formatter=formatter,\n        logger=logger,\n        stderr_output=non_human_output,\n    )\n\n    t0 = time.monotonic()\n\n    # handle stdin if specified via lone '-'\n    with PathAndUserErrorHandler(formatter):\n        if \"-\" == path:\n            file_config = lnt.config\n            if stdin_filename:\n                file_config = file_config.make_child_from_path(\n                    stdin_filename, require_dialect=False\n                )\n            parsed_strings = [\n                lnt.parse_string(\n                    sys.stdin.read(),\n                    \"stdin\",\n                    config=file_config,\n                    parse_statistics=parse_statistics,\n                ),\n            ]\n        else:\n            # A single path must be specified for this command\n            parsed_strings = list(\n                lnt.parse_path(\n                    path=path,\n                    parse_statistics=parse_statistics,\n                )\n            )\n\n    total_time = time.monotonic() - t0\n    violations_count = 0\n\n    # iterative print for human readout\n    if format == FormatType.human.value:\n        violations_count = formatter.print_out_violations_and_timing(\n            output_stream, bench, code_only, total_time, verbose, parsed_strings\n        )\n    else:\n        parsed_strings_dict = []\n        for parsed_string in parsed_strings:\n            # TODO: Multiple variants aren't yet supported here in the non-human\n            # output of the parse command.\n            root_variant = parsed_string.root_variant()\n            # Updating violation count ensures the correct return code below.\n            violations_count += len(parsed_string.violations)\n            if root_variant:\n                assert root_variant.tree\n                segments = root_variant.tree.as_record(\n                    code_only=code_only, show_raw=True, include_meta=include_meta\n                )\n            else:\n                # Parsing failed - return null for segments.\n                segments = None\n            parsed_strings_dict.append(\n                {\"filepath\": parsed_string.fname, \"segments\": segments}\n            )\n\n        if format == FormatType.yaml.value:\n            # For yaml dumping always dump double quoted strings if they contain\n            # tabs or newlines.\n            yaml.add_representer(str, quoted_presenter)\n            file_output = yaml.dump(\n                parsed_strings_dict,\n                sort_keys=False,\n                allow_unicode=True,\n            )\n        elif format == FormatType.json.value:\n            file_output = json.dumps(parsed_strings_dict)\n        elif format == FormatType.none.value:\n            file_output = \"\"\n\n        # Dump the output to stdout or to file as appropriate.\n        dump_file_payload(write_output, file_output)\n\n    if violations_count > 0 and not nofail:\n        sys.exit(EXIT_FAIL)  # pragma: no cover\n    else:\n        sys.exit(EXIT_SUCCESS)\n", "type": "function"}, {"name": "parse_example_file", "is_method": false, "class_name": null, "parameters": ["dialect", "sqlfile"], "calls": ["FluffConfig", "load_file", "lex", "parse", "dict", "Lexer", "Parser"], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test", "start_line": 116, "end_line": 124}, "code_snippet": "def parse_example_file(dialect: str, sqlfile: str):\n    \"\"\"Parse example SQL file, return parse tree.\"\"\"\n    config = FluffConfig(overrides=dict(dialect=dialect))\n    # Load the SQL\n    raw = load_file(dialect, sqlfile)\n    # Lex and parse the file\n    tokens, _ = Lexer(config=config).lex(raw)\n    tree = Parser(config=config).parse(tokens, fname=dialect + \"/\" + sqlfile)\n    return tree\n", "type": "function"}, {"name": "parse_path", "is_method": true, "class_name": "Linter", "parameters": ["self", "path", "parse_statistics"], "calls": ["split", "paths_from_path", "lower", "self.formatter.dispatch_path", "self.load_raw_file_and_config", "self.parse_string", "linter_logger.warning", "self.config.get", "str"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 1130, "end_line": 1161}, "code_snippet": "    def parse_path(\n        self,\n        path: str,\n        parse_statistics: bool = False,\n    ) -> Iterator[ParsedString]:\n        \"\"\"Parse a path of sql files.\n\n        NB: This a generator which will yield the result of each file\n        within the path iteratively.\n        \"\"\"\n        sql_exts = self.config.get(\"sql_file_exts\", default=\".sql\").lower().split(\",\")\n        for fname in paths_from_path(\n            path,\n            target_file_exts=sql_exts,\n        ):\n            if self.formatter:\n                self.formatter.dispatch_path(path)\n            # Load the file with the config and yield the result.\n            try:\n                raw_file, config, encoding = self.load_raw_file_and_config(\n                    fname, self.config\n                )\n            except SQLFluffSkipFile as s:\n                linter_logger.warning(str(s))\n                continue\n            yield self.parse_string(\n                raw_file,\n                fname=fname,\n                config=config,\n                encoding=encoding,\n                parse_statistics=parse_statistics,\n            )\n", "type": "function"}, {"name": "_lex_templated_file", "is_method": true, "class_name": "Linter", "parameters": ["templated_file", "config"], "calls": ["linter_logger.info", "Lexer", "config.get", "isinstance", "bool", "lexer.lex", "linter_logger.info", "sum", "new_segments.append", "linter_logger.info", "violations.append", "strip", "linter_logger.debug", "cast", "getattr", "templating_blocks_indent.lower"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 173, "end_line": 228}, "code_snippet": "    def _lex_templated_file(\n        templated_file: \"TemplatedFile\", config: FluffConfig\n    ) -> tuple[Optional[Sequence[BaseSegment]], list[SQLLexError]]:\n        \"\"\"Lex a templated file.\"\"\"\n        violations = []\n        linter_logger.info(\"LEXING RAW (%s)\", templated_file.fname)\n        # Get the lexer\n        lexer = Lexer(config=config)\n        # Lex the file and log any problems\n        try:\n            segments, lex_vs = lexer.lex(templated_file)\n            # NOTE: There will always be segments, even if it's\n            # just an end of file marker.\n            assert segments, \"The token sequence should never be empty.\"\n            # We might just get the violations as a list\n            violations += lex_vs\n            linter_logger.info(\"Lexed segments: %s\", [seg.raw for seg in segments])\n        except SQLLexError as err:  # pragma: no cover\n            linter_logger.info(\"LEXING FAILED! (%s): %s\", templated_file.fname, err)\n            violations.append(err)\n            return None, violations\n\n        # Check that we've got sensible indentation from the lexer.\n        # We might need to suppress if it's a complicated file.\n        templating_blocks_indent = config.get(\"template_blocks_indent\", \"indentation\")\n        if isinstance(templating_blocks_indent, str):\n            force_block_indent = templating_blocks_indent.lower().strip() == \"force\"\n        else:\n            force_block_indent = False\n        templating_blocks_indent = bool(templating_blocks_indent)\n        # If we're forcing it through we don't check.\n        if templating_blocks_indent and not force_block_indent:\n            indent_balance = sum(getattr(elem, \"indent_val\", 0) for elem in segments)\n            if indent_balance != 0:  # pragma: no cover\n                linter_logger.debug(\n                    \"Indent balance test failed for %r. Template indents will not be \"\n                    \"linted for this file.\",\n                    templated_file.fname,\n                )\n                # Don't enable the templating blocks.\n                templating_blocks_indent = False\n\n        # The file will have been lexed without config, so check all indents\n        # are enabled.\n        new_segments = []\n        for segment in segments:\n            if segment.is_meta:\n                meta_segment = cast(\"MetaSegment\", segment)\n                if meta_segment.indent_val != 0:\n                    # Don't allow it if we're not linting templating block indents.\n                    if not templating_blocks_indent:\n                        continue  # pragma: no cover\n            new_segments.append(segment)\n\n        # Return new buffer\n        return new_segments, violations\n", "type": "function"}, {"name": "parse", "is_method": false, "class_name": null, "parameters": ["sql", "dialect", "config", "config_path"], "calls": ["Linter", "linter.parse_string", "parsed.root_variant", "root_variant.tree.as_record", "get_simple_config", "APIParsingError"], "code_location": {"file": "simple.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/api", "start_line": 168, "end_line": 212}, "code_snippet": "def parse(\n    sql: str,\n    dialect: Optional[str] = None,\n    config: Optional[FluffConfig] = None,\n    config_path: Optional[str] = None,\n) -> dict[str, Any]:\n    \"\"\"Parse a SQL string.\n\n    Args:\n        sql (:obj:`str`): The SQL to be parsed.\n        dialect (:obj:`Optional[str]`, optional): A reference to the dialect of the SQL\n            to be parsed. Defaults to `ansi`.\n        config (:obj:`Optional[FluffConfig]`, optional): A configuration object\n            to use for the operation. Defaults to None.\n        config_path (:obj:`Optional[str]`, optional): A path to a .sqlfluff config,\n            which is only used if a `config` is not already provided.\n            Defaults to None.\n\n    Returns:\n        :obj:`Dict[str, Any]` JSON containing the parsed structure.\n\n    Note:\n        In the case of multiple potential variants from the raw source file\n        only the first variant is returned by the simple API. For access to\n        the other variants, use the underlying main API directly.\n    \"\"\"\n    cfg = config or get_simple_config(\n        dialect=dialect,\n        config_path=config_path,\n    )\n    linter = Linter(config=cfg)\n\n    parsed = linter.parse_string(sql)\n    # If we encounter any parsing errors, raise them in a combined issue.\n    violations = parsed.violations\n    if violations:\n        raise APIParsingError(violations)\n    # Return a JSON representation of the parse tree.\n    # NOTE: For the simple API - only a single variant is returned.\n    root_variant = parsed.root_variant()\n    assert root_variant, \"Files parsed without violations must have a valid variant\"\n    assert root_variant.tree, \"Files parsed without violations must have a valid tree\"\n    record = root_variant.tree.as_record(show_raw=True)\n    assert record\n    return record\n", "type": "function"}, {"name": "test__linter__skip_large_bytes", "is_method": false, "class_name": null, "parameters": ["filesize", "raises_skip"], "calls": ["pytest.mark.parametrize", "FluffConfig", "Linter", "lntr.lint_paths", "list", "result.get_violations", "lntr.parse_path", "pytest.raises", "Linter.load_raw_file_and_config", "str", "str", "result.get_violations"], "code_location": {"file": "linter_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 44, "end_line": 78}, "code_snippet": "def test__linter__skip_large_bytes(filesize, raises_skip):\n    \"\"\"Test extracting paths from a file path.\"\"\"\n    config = FluffConfig(\n        overrides={\"large_file_skip_byte_limit\": filesize, \"dialect\": \"ansi\"}\n    )\n    # First check the function directly\n    if raises_skip:\n        with pytest.raises(SQLFluffSkipFile) as excinfo:\n            Linter.load_raw_file_and_config(\n                \"test/fixtures/linter/indentation_errors.sql\", config\n            )\n        assert \"Skipping\" in str(excinfo.value)\n        assert f\"over the limit of {filesize}\" in str(excinfo.value)\n    # If NOT raises, then we'll catch the raise an error and the test will fail.\n\n    # Then check that it either is or isn't linted appropriately via lint_paths.\n    lntr = Linter(config)\n    result = lntr.lint_paths(\n        (\"test/fixtures/linter/indentation_errors.sql\",),\n    )\n    if raises_skip:\n        assert not result.get_violations()\n    else:\n        assert result.get_violations()\n\n    # Same again via parse_path, which is the other entry point.\n    result = list(\n        lntr.parse_path(\n            \"test/fixtures/linter/indentation_errors.sql\",\n        )\n    )\n    if raises_skip:\n        assert not result\n    else:\n        assert result\n", "type": "function"}, {"name": "_run_sqlfluff", "is_method": true, "class_name": "SQLFluffViolationReporter", "parameters": ["self", "src_paths"], "calls": ["copy.deepcopy", "self.options.split", "tempfile.NamedTemporaryFile", "f.close", "command.append", "src_path.endswith", "os.path.exists", "command.append", "join", "logger.warning", "execute", "os.remove", "src_path.encode", "read_text", "sys.getfilesystemencoding", "isinstance", "c.decode", "pathlib.Path", "sys.getfilesystemencoding"], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 81, "end_line": 116}, "code_snippet": "    def _run_sqlfluff(self, src_paths) -> list[str]:\n        # Prepare the SQLFluff command to run.\n        command = copy.deepcopy(self.driver.command)\n        if self.options:\n            for arg in self.options.split():\n                command.append(arg)\n        for src_path in src_paths:\n            if src_path.endswith(\".sql\") and os.path.exists(src_path):\n                command.append(src_path.encode(sys.getfilesystemencoding()))\n\n        with tempfile.NamedTemporaryFile(\n            prefix=\"sqlfluff-\", suffix=\".json\", delete=False\n        ) as f:\n            f.close()\n            try:\n                # Write output to a temporary file. This avoids issues where\n                # extraneous SQLFluff or dbt output results in the JSON output\n                # being invalid.\n                command += [\"--write-output\", f.name]\n\n                # Run SQLFluff.\n                printable_command = \" \".join(\n                    [\n                        (\n                            c.decode(sys.getfilesystemencoding())\n                            if isinstance(c, bytes)\n                            else c\n                        )\n                        for c in command\n                    ]\n                )\n                logger.warning(f\"{printable_command}\")\n                execute(command, self.driver.exit_codes)\n                return [pathlib.Path(f.name).read_text()]\n            finally:\n                os.remove(f.name)\n", "type": "function"}, {"name": "test__templater_full", "is_method": false, "class_name": null, "parameters": ["subpath", "code_only", "include_meta", "yaml_loader", "caplog"], "calls": ["pytest.mark.parametrize", "caplog.set_level", "caplog.set_level", "assert_structure"], "code_location": {"file": "jinja_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/templaters", "start_line": 754, "end_line": 765}, "code_snippet": "def test__templater_full(subpath, code_only, include_meta, yaml_loader, caplog):\n    \"\"\"Check structure can be parsed from jinja templated files.\"\"\"\n    # Log the templater and lexer throughout this test\n    caplog.set_level(logging.DEBUG, logger=\"sqlfluff.templater\")\n    caplog.set_level(logging.DEBUG, logger=\"sqlfluff.lexer\")\n\n    assert_structure(\n        yaml_loader,\n        \"test/fixtures/templater/\" + subpath,\n        code_only=code_only,\n        include_meta=include_meta,\n    )\n", "type": "function"}, {"name": "generate_one_parse_fixture", "is_method": false, "class_name": null, "parameters": ["example"], "calls": ["_create_file_path", "tree.type_set", "compute_parse_tree_hash", "_create_file_path", "parse_example_file", "open", "tree.as_record", "dict", "print", "yaml.dump", "SQLParseError", "SQLParseError", "f.write", "SQLParseError", "list", "records.items"], "code_location": {"file": "generate_parse_fixture_yml.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test", "start_line": 95, "end_line": 147}, "code_snippet": "def generate_one_parse_fixture(\n    example: ParseExample,\n) -> tuple[ParseExample, Optional[SQLParseError]]:\n    \"\"\"Parse example SQL file, write parse tree to YAML file.\"\"\"\n    dialect, sqlfile = example\n    sql_path = _create_file_path(example, \".sql\")\n\n    try:\n        tree = parse_example_file(dialect, sqlfile)\n    except Exception as err:\n        # Catch parsing errors, and wrap the file path only it.\n        return example, SQLParseError(f\"Fatal parsing error: {sql_path}: {err}\")\n\n    # Check we don't have any base types or unparsable sections\n    types = tree.type_set()\n    if \"base\" in types:\n        return example, SQLParseError(f\"Unnamed base section when parsing: {sql_path}\")\n    if \"unparsable\" in types:\n        return example, SQLParseError(f\"Could not parse: {sql_path}\")\n\n    _hash = compute_parse_tree_hash(tree)\n    # Remove the .sql file extension\n    path = _create_file_path(example)\n    with open(path, \"w\", newline=\"\\n\", encoding=\"utf8\") as f:\n        r: Optional[dict[str, Optional[str]]] = None\n\n        if not tree:\n            f.write(\"\")\n            return example, None\n\n        records = tree.as_record(code_only=True, show_raw=True)\n        assert records, \"TypeGuard\"\n        r = dict([(\"_hash\", _hash), *list(records.items())])\n        print(\n            \"# YML test files are auto-generated from SQL files and should not be \"\n            \"edited by\",\n            '# hand. To help enforce this, the \"hash\" field in the file must match '\n            \"a hash\",\n            \"# computed by SQLFluff when running the tests. Please run\",\n            \"# `python test/generate_parse_fixture_yml.py`  to generate them after \"\n            \"adding or\",\n            \"# altering SQL files.\",\n            file=f,\n            sep=\"\\n\",\n        )\n        yaml.dump(\n            data=r,\n            stream=f,\n            default_flow_style=False,\n            sort_keys=False,\n            allow_unicode=True,\n        )\n        return example, None\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1519155502319336}
{"question": "Why does SQLFluff's selective rule application reduce processing overhead?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's selective rule application reduces processing overhead by only running rules that are relevant to the current context and configuration. Key benefits include: 1) Rule filtering - Only enabled rules are executed, skipping disabled or excluded rules entirely; 2) Context-aware execution - Rules are only applied to segments they're designed to handle; 3) Configuration-based selection - Rules are filtered based on user configuration (rules, exclude_rules, warnings); 4) Dialect-specific filtering - Rules that don't apply to the current dialect are skipped; 5) Performance optimization - Avoiding unnecessary rule evaluations reduces CPU usage and execution time; 6) Memory efficiency - Selective rule application reduces memory usage by not loading unused rule logic; 7) Scalability - Performance improves as the number of available rules increases; 8) Customization - Users can focus on specific rule categories without running all rules; 9) Incremental processing - Only relevant rules are re-evaluated when configuration changes; 10) Resource management - Better resource utilization by avoiding redundant rule processing.", "score": null, "retrieved_content": [{"name": "_eval_gen", "is_method": true, "class_name": "Rule_CV12", "parameters": ["self", "context"], "calls": ["select_statement.is_type", "select_statement.get_child", "self._is_where_clause_simplifable", "set", "select_statement.recursive_crawl", "collections.deque", "enumerate", "where_clause.get_child", "self._get_subexpression_chunks", "self._get_from_expression_element_alias", "next", "encountered_references.add", "any", "join_clause.get_child", "is_type", "where_clause_fix_segments.popleft", "is_type", "where_clause_fix_segments.pop", "where_clause.get_child", "is_type", "is_type", "select_statement.recursive_crawl", "join_clause.recursive_crawl", "self._get_from_expression_element_alias", "set", "enumerate", "where_clause_fix_segments.extend", "where_clause_fix_segments.append", "LintResult", "LintResult", "LintResult", "collections.deque", "enumerate", "ExpressionSegment", "JoinOnConditionSegment", "JoinClauseSegment", "BinaryOperatorSegment", "all", "this_join_clause_subexpressions.add", "consumed_subexpressions.add", "LintResult", "is_type", "join_clause_fix_segments.popleft", "is_type", "join_clause_fix_segments.pop", "tuple", "LintResult", "seg.recursive_crawl", "len", "join_clause_fix_segments.extend", "join_clause_fix_segments.append", "KeywordSegment", "WhitespaceSegment", "WhitespaceSegment", "LintFix.replace", "LintFix.delete", "LintFix.delete", "col_ref.raw_upper.startswith", "BinaryOperatorSegment", "tuple", "LintFix.replace"], "code_location": {"file": "CV12.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "start_line": 67, "end_line": 250}, "code_snippet": "    def _eval_gen(self, context: RuleContext) -> Iterator[LintResult]:\n        # We are only interested in SELECT statement.\n        select_statement = context.segment\n        assert select_statement.is_type(\"select_statement\")\n\n        maybe_where_clause = select_statement.get_child(\"where_clause\")\n        if not maybe_where_clause:\n            return\n\n        where_clause = maybe_where_clause\n        where_clause_simplifable = self._is_where_clause_simplifable(where_clause)\n\n        if where_clause_simplifable:\n            expr = where_clause.get_child(\"expression\")\n            assert expr is not None\n            subexpressions = self._get_subexpression_chunks(expr)\n        else:\n            subexpressions = []\n        consumed_subexpressions = set()\n\n        # get references in from clause\n        select_table_references = [\n            *select_statement.recursive_crawl(\n                \"from_expression_element\",\n                no_recursive_seg_type=[\"join_clause\", \"select_statement\"],\n            )\n        ]\n\n        # track all seen references (from clause + all previous joins)\n        encountered_references = {\n            self._get_from_expression_element_alias(table_ref)\n            for table_ref in select_table_references\n        }\n\n        for join_clause in select_statement.recursive_crawl(\n            \"join_clause\", no_recursive_seg_type=[\"select_statement\"]\n        ):\n            # mark table reference as seen\n            join_table_reference = next(\n                join_clause.recursive_crawl(\n                    \"from_expression_element\",\n                    no_recursive_seg_type=[\"select_statement\"],\n                )\n            )\n            encountered_references.add(\n                self._get_from_expression_element_alias(join_table_reference)\n            )\n            join_clause_keywords = [\n                seg for seg in join_clause.segments if seg.type == \"keyword\"\n            ]\n\n            if any(\n                kw.raw_upper in (\"CROSS\", \"POSITIONAL\", \"USING\", \"APPLY\")\n                for kw in join_clause_keywords\n            ):\n                # If explicit CROSS JOIN is used, disregard lack of condition\n                # If explicit POSITIONAL JOIN is used, disregard lack of condition\n                # If explicit JOIN USING is used, disregard lack of condition\n                # If explicit CROSS/OUTER APPLY is used, disregard lack of condition\n                continue\n\n            this_join_condition = join_clause.get_child(\"join_on_condition\")\n            if this_join_condition:\n                # Join condition is present, no error reported.\n                continue\n\n            if not where_clause_simplifable:\n                yield LintResult(anchor=join_clause)\n            else:\n                this_join_clause_subexpressions = set()\n                for subexpr_idx, subexpr_segments in enumerate(subexpressions):\n                    if subexpr_idx in consumed_subexpressions:\n                        continue\n                    qualified_column_references = [\n                        col_ref\n                        for seg in subexpr_segments\n                        for col_ref in seg.recursive_crawl(\n                            \"column_reference\",\n                            no_recursive_seg_type=\"select_statement\",\n                        )\n                        if \"dot\" in col_ref.descendant_type_set\n                    ]\n                    if len(qualified_column_references) > 1 and all(\n                        col_ref.raw_upper.startswith(\n                            tuple(\n                                f\"{table_ref}.\" for table_ref in encountered_references\n                            )\n                        )\n                        for col_ref in qualified_column_references\n                    ):\n                        this_join_clause_subexpressions.add(subexpr_idx)\n                        consumed_subexpressions.add(subexpr_idx)\n\n                if not this_join_clause_subexpressions:\n                    yield LintResult(join_clause)\n                else:\n                    join_clause_fix_segments: Deque[BaseSegment] = collections.deque()\n                    for subexpr_idx, subexpr_segments in enumerate(subexpressions):\n                        if subexpr_idx in this_join_clause_subexpressions:\n                            join_clause_fix_segments.extend(subexpr_segments)\n                            join_clause_fix_segments.append(\n                                BinaryOperatorSegment(\"AND\")\n                            )\n\n                    while join_clause_fix_segments and join_clause_fix_segments[\n                        0\n                    ].is_type(\"whitespace\", \"binary_operator\"):\n                        join_clause_fix_segments.popleft()\n                    while join_clause_fix_segments and join_clause_fix_segments[\n                        -1\n                    ].is_type(\"whitespace\", \"binary_operator\"):\n                        join_clause_fix_segments.pop()\n\n                    join_on_expression = ExpressionSegment(\n                        tuple(join_clause_fix_segments),\n                    )\n                    join_on = JoinOnConditionSegment(\n                        (\n                            KeywordSegment(\"ON\"),\n                            WhitespaceSegment(),\n                            join_on_expression,\n                        )\n                    )\n                    join_clause_segment = JoinClauseSegment(\n                        (\n                            *join_clause.segments,\n                            WhitespaceSegment(),\n                            join_on,\n                        )\n                    )\n\n                    yield LintResult(\n                        anchor=join_clause,\n                        fixes=[\n                            LintFix.replace(\n                                join_clause,\n                                edit_segments=[join_clause_segment],\n                            )\n                        ],\n                    )\n\n        if not where_clause_simplifable:\n            return\n\n        if not consumed_subexpressions:\n            return\n\n        # Rewrite WHERE to keep conditions not moved to ON clauses\n        where_clause_fix_segments: Deque[BaseSegment] = collections.deque()\n        for subexpr_idx, subexpr_segments in enumerate(subexpressions):\n            if subexpr_idx not in consumed_subexpressions:\n                where_clause_fix_segments.extend(subexpr_segments)\n                where_clause_fix_segments.append(BinaryOperatorSegment(\"AND\"))\n\n        while where_clause_fix_segments and where_clause_fix_segments[0].is_type(\n            \"whitespace\", \"binary_operator\"\n        ):\n            where_clause_fix_segments.popleft()\n        while where_clause_fix_segments and where_clause_fix_segments[-1].is_type(\n            \"whitespace\", \"binary_operator\"\n        ):\n            where_clause_fix_segments.pop()\n\n        if where_clause_fix_segments:\n            where_clause_expr = where_clause.get_child(\"expression\")\n            assert where_clause_expr is not None\n            yield LintResult(\n                anchor=where_clause_expr,\n                fixes=[\n                    LintFix.replace(\n                        where_clause_expr, edit_segments=[*where_clause_fix_segments]\n                    )\n                ],\n            )\n        else:\n            assert select_statement.segments[-1].is_type(\"where_clause\")\n            assert select_statement.segments[-2].is_type(\"whitespace\", \"newline\")\n            yield LintResult(\n                anchor=where_clause,\n                fixes=[\n                    LintFix.delete(select_statement.segments[-2]),\n                    LintFix.delete(select_statement.segments[-1]),\n                ],\n            )\n", "type": "function"}, {"name": "test__config__rules_group_with_exclude", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "lntr.lint_path", "lnt.check_tuples_by_path", "FluffConfig.from_path"], "code_location": {"file": "fluffconfig_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/config", "start_line": 161, "end_line": 170}, "code_snippet": "def test__config__rules_group_with_exclude():\n    \"\"\"Test linting when a rules group is selected and rules are excluded.\"\"\"\n    lntr = Linter(\n        config=FluffConfig.from_path(\"test/fixtures/config/rules_group_with_exclude\")\n    )\n    lnt = lntr.lint_path(\"test/fixtures/config/rules_group_with_exclude/test.sql\")\n    violations = lnt.check_tuples_by_path()\n    for k in violations:\n        assert (\"CP01\", 15, 1) in violations[k]\n        assert \"LT04\" not in [c[0] for c in violations[k]]\n", "type": "function"}, {"name": "Rule_ST06", "docstring": "Select wildcards then simple targets before calculations and aggregates.\n\n**Anti-pattern**\n\n.. code-block:: sql\n\n    select\n        a,\n        *,\n        row_number() over (partition by id order by date) as y,\n        b\n    from x\n\n\n**Best practice**\n\nOrder ``select`` targets in ascending complexity\n\n.. code-block:: sql\n\n    select\n        *,\n        a,\n        b,\n        row_number() over (partition by id order by date) as y\n    from x", "methods": ["_validate", "_eval", "_implicit_column_references"], "attributes": ["name", "aliases", "groups", "crawl_behaviour", "is_fix_compatible"], "code_location": {"file": "ST06.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 17, "end_line": 248}, "type": "class"}, {"name": "SQLFluffRule", "docstring": "SQLFluff rule directive for sphinx.\n\nRule directives can be used as shown below.\n\n.. code-block:: rst\n\n    .. sqlfluff:rule:: AM01\n                       ambiguous.distinct\n\n        Write the documentation for the rule here.\n\nTo cross reference (i.e. refer to) objects defined like this\nboth the code and name reference is available:\n\n.. code-block:: rst\n\n    :sqlfluff:ref:`CP02`\n    :sqlfluff:ref:`capitalisation.identifiers`", "methods": ["handle_signature", "add_target_and_index", "_object_hierarchy_parts", "_toc_entry_name"], "attributes": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 10, "end_line": 79}, "type": "class"}, {"name": "_eval", "is_method": true, "class_name": "Rule_ST06", "parameters": ["self", "context"], "calls": ["context.segment.is_type", "reversed", "reversed", "context.segment.get_children", "seg.is_type", "seg.is_type", "enumerate", "LintResult", "seg.get_child", "seg.get_parent", "with_compound_statement.recursive_crawl", "append", "len", "any", "LintResult", "LintFix.replace", "self._implicit_column_references", "zip", "with_compound_statement.path_to", "any", "isinstance", "segment.get_child", "self._validate", "any", "isinstance", "path_step.segment.is_type", "segment.get_child", "_function.get_child", "isinstance", "path_step.segment.is_type", "self._validate", "segment.get_child", "_expression.get_child", "self._validate", "len", "len"], "code_location": {"file": "ST06.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 64, "end_line": 232}, "code_snippet": "    def _eval(self, context: RuleContext) -> EvalResultType:\n        self.violation_exists = False\n        # Bands of select targets in order to be enforced\n        select_element_order_preference: tuple[\n            tuple[Union[str, tuple[str, ...]], ...], ...\n        ] = (\n            (\"wildcard_expression\",),\n            (\n                \"object_reference\",\n                \"literal\",\n                \"cast_expression\",\n                (\"function\", \"cast\"),\n                (\"expression\", \"cast_expression\"),\n            ),\n        )\n\n        # Track which bands have been seen, with additional empty list for the\n        # non-matching elements. If we find a matching target element, we append the\n        # element to the corresponding index.\n        self.seen_band_elements: list[list[BaseSegment]] = [\n            [] for _ in select_element_order_preference\n        ] + [\n            []\n        ]  # type: ignore\n\n        assert context.segment.is_type(\"select_clause\")\n\n        # insert, merge, create table, union are order-sensitive\n        for seg in reversed(context.parent_stack):\n            if seg.is_type(\n                \"insert_statement\",\n                \"set_expression\",\n                \"create_table_statement\",\n                \"merge_statement\",\n            ):\n                return None\n\n        # CTE is order-sensitive only if CTE is referenced as SELECT * in set expression\n        for seg in reversed(context.parent_stack):\n            if seg.is_type(\"common_table_expression\"):\n                cte_identifier = seg.get_child(\"identifier\")\n                assert cte_identifier is not None\n                maybe_with_compound_statement = seg.get_parent()\n                if maybe_with_compound_statement is None:\n                    break  # pragma: no cover\n                with_compound_statement, _ = maybe_with_compound_statement\n                for ref in with_compound_statement.recursive_crawl(\"table_reference\"):\n                    if ref.raw_upper == cte_identifier.raw_upper:\n                        path = with_compound_statement.path_to(ref)\n                        if any(\n                            path_step.segment.is_type(\"set_expression\")\n                            for path_step in path\n                        ):\n                            select_statements = [\n                                path_step.segment\n                                for path_step in path\n                                if path_step.segment.is_type(\n                                    \"select_statement\",\n                                    \"unordered_select_statement_segment\",\n                                )\n                            ]\n                            if any(\n                                \"wildcard_expression\"\n                                in select_statement.descendant_type_set\n                                for select_statement in select_statements\n                            ):\n                                return None\n\n        select_clause_segment = context.segment\n        select_target_elements = context.segment.get_children(\"select_clause_element\")\n        if not select_target_elements:\n            return None\n\n        # Iterate through all the select targets to find any order violations\n        for segment in select_target_elements:\n            # The band index of the current segment in\n            # select_element_order_preference\n            self.current_element_band = None\n\n            # Compare the segment to the bands in select_element_order_preference\n            for i, band in enumerate(select_element_order_preference):\n                for e in band:\n                    # Identify simple select target\n                    if isinstance(e, str) and segment.get_child(e):\n                        self._validate(i, segment)\n\n                    # Identify function\n                    elif isinstance(e, tuple) and e[0] == \"function\":\n                        try:\n                            _function = segment.get_child(\"function\")\n                            assert _function\n                            _function_name = _function.get_child(\"function_name\")\n                            assert _function_name\n                            if _function_name.raw == e[1]:\n                                self._validate(i, segment)\n                        except (AttributeError, AssertionError):\n                            # If the segment doesn't match\n                            pass\n\n                    # Identify simple expression\n                    elif isinstance(e, tuple) and e[0] == \"expression\":\n                        try:\n                            _expression = segment.get_child(\"expression\")\n                            assert _expression\n\n                            if (\n                                _expression.get_child(e[1])\n                                and _expression.segments[0].type\n                                in (\n                                    \"column_reference\",\n                                    \"object_reference\",\n                                    \"literal\",\n                                    \"cast_expression\",\n                                )\n                                # len == 2 to ensure the expression is 'simple'\n                                and (\n                                    len(_expression.segments) == 2\n                                    # cast_expression is one length\n                                    or len(_expression.segments) == 1\n                                )\n                            ):\n                                self._validate(i, segment)\n                        except (AttributeError, AssertionError):\n                            # If the segment doesn't match\n                            pass\n\n            # If the target doesn't exist in select_element_order_preference then it\n            # is 'complex' and must go last\n            if self.current_element_band is None:\n                self.seen_band_elements[-1].append(segment)\n\n        if self.violation_exists:\n            if len(context.parent_stack) and any(\n                self._implicit_column_references(context.parent_stack[-1])\n            ):\n                # If there are implicit column references (i.e. column\n                # numbers), warn but don't fix, because it's much more\n                # complicated to autofix.\n                return LintResult(anchor=select_clause_segment)\n            # Create a list of all the edit fixes\n            # We have to do this at the end of iterating through all the\n            # select_target_elements to get the order correct. This means we can't\n            # add a lint fix to each individual LintResult as we go\n            ordered_select_target_elements = [\n                segment for band in self.seen_band_elements for segment in band\n            ]\n            # TODO: The \"if\" in the loop below compares corresponding items\n            # to avoid creating \"do-nothing\" edits. A potentially better\n            # approach would leverage difflib.SequenceMatcher.get_opcodes(),\n            # which generates a list of edit actions (similar to the\n            # command-line \"diff\" tool in Linux). This is more complex to\n            # implement, but minimizing the number of LintFixes makes the\n            # final application of patches (in \"sqlfluff fix\") more robust.\n            fixes = [\n                LintFix.replace(\n                    initial_select_target_element,\n                    [replace_select_target_element],\n                )\n                for initial_select_target_element, replace_select_target_element in zip(  # noqa: E501\n                    select_target_elements, ordered_select_target_elements\n                )\n                if initial_select_target_element is not replace_select_target_element\n            ]\n            # Anchoring on the select statement segment ensures that\n            # select statements which include macro targets are ignored\n            # when ignore_templated_areas is set\n            return LintResult(anchor=select_clause_segment, fixes=fixes)\n\n        return None\n", "type": "function"}, {"name": "test__rules__rule_selection", "is_method": false, "class_name": null, "parameters": ["rules", "exclude_rules", "resulting_codes"], "calls": ["pytest.mark.parametrize", "FluffConfig", "Linter", "set", "RootOnlyCrawler", "linter.rule_tuples"], "code_location": {"file": "rules_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 154, "end_line": 197}, "code_snippet": "def test__rules__rule_selection(rules, exclude_rules, resulting_codes):\n    \"\"\"Test that rule selection works by various means.\"\"\"\n\n    class Rule_T010(BaseRule):\n        \"\"\"Fake Basic Rule.\"\"\"\n\n        groups = (\"all\", \"test\")\n        name = \"fake_basic\"\n        aliases = (\"fb1\", \"foo\")  # NB: Foo is a group on another rule.\n        crawl_behaviour = RootOnlyCrawler()\n\n        def _eval(self, **kwargs):\n            pass\n\n    class Rule_T011(Rule_T010):\n        \"\"\"Fake Basic Rule.\n\n        NOTE: We inherit crawl behaviour and _eval from above.\n        \"\"\"\n\n        groups = (\"all\", \"test\", \"foo\")\n        name = \"fake_other\"\n        aliases = (\"fb2\",)\n\n    class Rule_T012(Rule_T010):\n        \"\"\"Fake Basic Rule.\n\n        NOTE: We inherit crawl behaviour and _eval from above.\n        \"\"\"\n\n        # NB: \"fake_other\" is the name of another rule.\n        groups = (\"all\", \"foo\", \"fake_other\")\n        # No aliases, Name collides with the alias of another rule.\n        name = \"fake_again\"\n        aliases = ()\n\n    cfg = FluffConfig(\n        overrides={\"rules\": rules, \"exclude_rules\": exclude_rules, \"dialect\": \"ansi\"}\n    )\n    linter = Linter(config=cfg, user_rules=[Rule_T010, Rule_T011, Rule_T012])\n    # Get the set of selected codes:\n    selected_codes = set(tpl[0] for tpl in linter.rule_tuples())\n    # Check selected rules\n    assert selected_codes == resulting_codes\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_LT09", "parameters": ["self", "context"], "calls": ["context.segment.is_type", "self._get_indexes", "children", "bool", "FunctionalContext", "sp.is_type", "self._eval_single_select_target_element", "len", "select_clause.children", "len", "self._eval_multiple_select_target_elements", "sp.is_type"], "code_location": {"file": "LT09.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 86, "end_line": 106}, "code_snippet": "    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        self.wildcard_policy: str\n        assert context.segment.is_type(\"select_clause\")\n        select_targets_info = self._get_indexes(context)\n        select_clause = FunctionalContext(context).segment\n        wildcards = select_clause.children(\n            sp.is_type(\"select_clause_element\")\n        ).children(sp.is_type(\"wildcard_expression\"))\n        has_wildcard = bool(wildcards)\n        if len(select_targets_info.select_targets) == 1 and (\n            not has_wildcard or self.wildcard_policy == \"single\"\n        ):\n            return self._eval_single_select_target_element(\n                select_targets_info,\n                context,\n            )\n        elif len(select_targets_info.select_targets):\n            return self._eval_multiple_select_target_elements(\n                select_targets_info, context.segment\n            )\n        return None\n", "type": "function"}, {"name": "test__config__rules_set_to_none", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "lntr.lint_path", "lnt.check_tuples_by_path", "FluffConfig.from_path"], "code_location": {"file": "fluffconfig_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/config", "start_line": 145, "end_line": 158}, "code_snippet": "def test__config__rules_set_to_none():\n    \"\"\"Test linting when rules are set to 'None'.\n\n    Ensure that all rules are still run.\n    \"\"\"\n    lntr = Linter(\n        config=FluffConfig.from_path(\"test/fixtures/config/rules_set_to_none\")\n    )\n    lnt = lntr.lint_path(\"test/fixtures/config/rules_set_to_none/test.sql\")\n    violations = lnt.check_tuples_by_path()\n    for k in violations:\n        assert (\"LT13\", 1, 1) in violations[k]\n        assert (\"AM04\", 12, 1) in violations[k]\n        assert (\"CP01\", 12, 10) in violations[k]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/ambiguous", "start_line": 29, "end_line": 53}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.ambiguous.AM01 import Rule_AM01\n    from sqlfluff.rules.ambiguous.AM02 import Rule_AM02\n    from sqlfluff.rules.ambiguous.AM03 import Rule_AM03\n    from sqlfluff.rules.ambiguous.AM04 import Rule_AM04\n    from sqlfluff.rules.ambiguous.AM05 import Rule_AM05\n    from sqlfluff.rules.ambiguous.AM06 import Rule_AM06\n    from sqlfluff.rules.ambiguous.AM07 import Rule_AM07\n    from sqlfluff.rules.ambiguous.AM08 import Rule_AM08\n\n    return [\n        Rule_AM01,\n        Rule_AM02,\n        Rule_AM03,\n        Rule_AM04,\n        Rule_AM05,\n        Rule_AM06,\n        Rule_AM07,\n        Rule_AM08,\n    ]\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_AL05", "parameters": ["self", "context"], "calls": ["context.segment.is_type", "get_select_statement_info", "cast", "self._analyze_table_aliases", "Counter", "AL05Query.from_segment", "set", "set", "aliases.intersection", "alias.from_expression_element.get_child", "aliases.add", "self.logger.debug", "self._cs_str_id", "self._is_alias_required", "self._followed_by_qualify", "_table_expression.get_child", "violations.append", "aliases.add", "seg.is_type", "ref_counter.get", "upper", "self.logger.debug", "self._cs_str_id", "self._report_unused_alias", "self._cs_str_id", "references.add", "self._cs_str_id", "seg.raw_upper.strip", "self._cs_str_id", "alias.ref_str.strip"], "code_location": {"file": "AL05.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/aliasing", "start_line": 76, "end_line": 184}, "code_snippet": "    def _eval(self, context: RuleContext) -> EvalResultType:\n        violations: list[LintResult] = []\n        assert context.segment.is_type(\"select_statement\")\n        # Exit early if the SELECT does not define any aliases.\n        select_info = get_select_statement_info(context.segment, context.dialect)\n        if not select_info or not select_info.table_aliases:\n            return None\n\n        # Analyze the SELECT.\n        alias: AliasInfo\n        query = cast(\n            AL05Query, AL05Query.from_segment(context.segment, dialect=context.dialect)\n        )\n        self._analyze_table_aliases(query)\n\n        if context.dialect.name in (\"redshift\", \"bigquery\"):\n            # Redshift supports un-nesting using aliases.\n            # Detect that situation and ignore.\n            # https://docs.aws.amazon.com/redshift/latest/dg/query-super.html#unnest\n\n            # Do any references refer to aliases in the same list?\n            references = set()\n            aliases = set()\n\n            for alias in query.aliases:\n                aliases.add(alias.ref_str)\n                if alias.segment:\n                    aliases.add(self._cs_str_id(alias.segment))\n                if not alias.object_reference:\n                    continue  # pragma: no cover\n                for seg in alias.object_reference.segments:\n                    if seg.is_type(\"identifier\"):\n                        references.add(self._cs_str_id(seg))\n\n            # If there's any overlap between aliases and reference\n            if aliases.intersection(references):\n                self.logger.debug(\n                    \"Overlapping references found. Assuming %s semi-structured.\",\n                    context.dialect.name,\n                )\n                return None\n\n        # Get the number of times an object (table/view) is referenced. While some\n        # dialects can handle the same table name reference with different schemas,\n        # we don't want to allow a conflict with AL04's uniqueness rule so we grab\n        # the base table name instead of the fully qualified one to determine naming\n        # collisions.\n        ref_counter = Counter(\n            self._cs_str_id(a.object_reference.segments[-1])\n            for a in query.aliases\n            if a.object_reference and a.object_reference.segments\n        )\n        for alias in query.aliases:\n            # Skip alias if it's required (some dialects require aliases for\n            # VALUES clauses).\n            if alias.from_expression_element and self._is_alias_required(\n                alias.from_expression_element, context.dialect.name\n            ):\n                continue\n            # Skip alias if the table is referenced more than once, some dialects\n            # require the referenced table names to be unique even if not returned\n            # by the statement.\n            if (\n                alias.object_reference\n                and alias.object_reference.segments\n                and ref_counter.get(\n                    self._cs_str_id(alias.object_reference.segments[-1]),\n                    0,\n                )\n                > 1\n            ):\n                continue\n            # Redshift requires an alias when a `QUALIFY` statement immediately follows\n            # the `FROM` clause.\n            # https://docs.aws.amazon.com/redshift/latest/dg/r_QUALIFY_clause.html\n            if (\n                context.dialect.name == \"redshift\"\n                and alias.alias_expression\n                and self._followed_by_qualify(context, alias)\n            ):\n                continue\n            # If the alias is for a _function_ rather than just a table, it's possible\n            # that it's an array function, like `unnest` or `jsonb_array_elements_text`\n            # So if that alias appears as what looks like a _column reference_ then\n            # also skip it.\n            # https://github.com/sqlfluff/sqlfluff/issues/4623\n            _table_expression = alias.from_expression_element.get_child(\n                \"table_expression\"\n            )\n            if _table_expression and _table_expression.get_child(\"function\"):\n                # Case insensitive match for conservatism\n                if alias.ref_str.strip(\"\\\"'`[]\").upper() in [\n                    seg.raw_upper.strip(\"\\\"'`[]\")\n                    for seg in select_info.reference_buffer\n                ]:\n                    self.logger.debug(\n                        f\"Alias for function {alias.ref_str} found as apparent \"\n                        \"column reference in select. Skipping\"\n                    )\n                    continue\n\n            if (\n                alias.aliased\n                and alias.segment\n                and self._cs_str_id(alias.segment) not in query.tbl_refs\n            ):\n                # Unused alias. Report and fix.\n                violations.append(self._report_unused_alias(alias))\n        return violations or None\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1579225063323975}
{"question": "Where does the data flow when SQLFluff processes templated SQL from template parsing through rule application to fix generation?", "answer": null, "relative_code_list": null, "ground_truth": "The data flow when SQLFluff processes templated SQL follows a structured sequence from template parsing through rule application to fix generation: 1) Template parsing phase begins where raw SQL with template syntax is processed through the templating engine (Jinja, Python format strings, or dbt templates) to resolve dynamic content and placeholders, creating a TemplatedFile object that maintains both original template and rendered SQL, 2) Template compilation occurs where template variables and expressions are resolved according to the templating engine's rules, with the BlockTracker maintaining information about template blocks and position mapping between template source and rendered output, 3) Lexing phase begins where the templated SQL is tokenized by the dialect-specific lexer, which processes the rendered SQL and creates a sequence of tokens while maintaining awareness of the original template structure, 4) Parsing phase occurs where tokens are processed by the dialect-specific parser to create a parse tree of BaseSegment objects, with the parser handling dialect-specific syntax constructs and maintaining structural information, 5) Rule application phase begins where the Linter applies configured rules to the parse tree, with each rule's _eval() method receiving segments and analyzing them for violations, generating LintResult objects when issues are found, 6) Violation collection happens where all rule violations are collected and organized, with each violation containing information about the location, type, and severity of the issue, 7) Fix generation phase occurs where rules that support automatic fixes create LintFix objects describing how to correct the violations, including the type of fix (create, edit, delete), target segment, and new content, 8) Fix application happens where the LintedFile.apply_fixes() method applies all generated fixes to the original SQL, creating FixPatch objects that represent the actual text changes needed, 9) Position mapping occurs throughout the process where the system maintains accurate position information between template source, rendered SQL, and final output for error reporting and fix application, 10) Output generation happens where the corrected SQL is generated with proper formatting and structure maintained, while preserving the original template structure for template-aware fixes, 11) Error handling occurs throughout the process where template processing errors, parsing errors, and fix application errors are caught and reported with appropriate context, 12) The entire data flow is coordinated through SQLFluff's core processing pipeline, ensuring that template processing, parsing, linting, and fixing work together seamlessly while maintaining accuracy and performance.", "score": null, "retrieved_content": [{"name": "_run_templater_and_verify_result", "is_method": false, "class_name": null, "parameters": ["dbt_templater", "project_dir", "fname", "dbt_fluff_config", "dbt_project_folder"], "calls": ["FluffConfig", "dbt_templater.process", "_get_fixture_path", "Lexer", "lexer.lex", "str", "fixture_path.read_text", "Path", "path.read_text", "str"], "code_location": {"file": "templater_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/test", "start_line": 131, "end_line": 152}, "code_snippet": "def _run_templater_and_verify_result(\n    dbt_templater,\n    project_dir,\n    fname,\n    dbt_fluff_config,\n    dbt_project_folder,\n):\n    path = Path(project_dir) / \"models/my_new_project\" / fname\n    config = FluffConfig(configs=dbt_fluff_config)\n    templated_file, _ = dbt_templater.process(\n        in_str=path.read_text(),\n        fname=str(path),\n        config=config,\n    )\n    template_output_folder_path = dbt_project_folder / \"templated_output/\"\n    fixture_path = _get_fixture_path(template_output_folder_path, fname)\n    assert str(templated_file) == fixture_path.read_text()\n    # Check we can lex the output too.\n    # https://github.com/sqlfluff/sqlfluff/issues/4013\n    lexer = Lexer(config=config)\n    _, lexing_violations = lexer.lex(templated_file)\n    assert not lexing_violations\n", "type": "function"}, {"name": "_iter_templated_patches", "is_method": false, "class_name": null, "parameters": ["segment", "templated_file"], "calls": ["linter_logger.debug", "segment.pos_marker.is_literal", "_iter_source_fix_patches", "type", "_iter_source_fix_patches", "FixPatch", "is_type", "slice", "slice", "seg.pos_marker.is_point", "_iter_templated_patches", "FixPatch", "linter_logger.debug", "FixPatch", "slice", "slice", "max", "max"], "code_location": {"file": "patch.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 54, "end_line": 213}, "code_snippet": "def _iter_templated_patches(\n    segment: BaseSegment, templated_file: TemplatedFile\n) -> Iterator[FixPatch]:\n    \"\"\"Iterate through the segments generating fix patches.\n\n    The patches are generated in TEMPLATED space. This is important\n    so that we defer dealing with any loops until later. At this stage\n    everything *should* happen in templated order.\n\n    Occasionally we have an insertion around a placeholder, so we also\n    return a hint to deal with that.\n    \"\"\"\n    # Does it match? If so we can ignore it.\n    assert segment.pos_marker\n    templated_raw = templated_file.templated_str[segment.pos_marker.templated_slice]\n    matches = segment.raw == templated_raw\n    if matches:\n        # First yield any source fixes\n        yield from _iter_source_fix_patches(segment, templated_file)\n        # Then return.\n        return\n\n    # If we're here, the segment doesn't match the original.\n    linter_logger.debug(\n        \"# Changed Segment Found: %s at %s: Original: [%r] Fixed: [%r]\",\n        type(segment).__name__,\n        segment.pos_marker.templated_slice,\n        templated_raw,\n        segment.raw,\n    )\n\n    # If it's all literal, then we don't need to recurse.\n    if segment.pos_marker.is_literal():\n        # First yield any source fixes\n        yield from _iter_source_fix_patches(segment, templated_file)\n        # Then yield the position in the source file and the patch\n        yield FixPatch(\n            source_slice=segment.pos_marker.source_slice,\n            templated_slice=segment.pos_marker.templated_slice,\n            patch_category=\"literal\",\n            fixed_raw=segment.raw,\n            templated_str=templated_file.templated_str[\n                segment.pos_marker.templated_slice\n            ],\n            source_str=templated_file.source_str[segment.pos_marker.source_slice],\n        )\n    # Can we go deeper?\n    elif not segment.segments:\n        # It's not literal, but it's also a raw segment. If we're going\n        # to yield a change, we would have done it from the parent, so\n        # we just abort from here.\n        return  # pragma: no cover TODO?\n    else:\n        # This segment isn't a literal, but has changed, we need to go deeper.\n\n        # If there's an end of file segment or indent, ignore them just for the\n        # purposes of patch iteration.\n        # NOTE: This doesn't mutate the underlying `self.segments`.\n        segments = segment.segments\n        while segments and segments[-1].is_type(\"end_of_file\", \"indent\"):\n            segments = segments[:-1]\n\n        # Iterate through the child segments\n        source_idx = segment.pos_marker.source_slice.start\n        templated_idx = segment.pos_marker.templated_slice.start\n        insert_buff = \"\"\n        first_segment_pos: Optional[PositionMarker] = None\n        for seg in segments:\n            # First check for insertions.\n            # At this stage, everything should have a position.\n            assert seg.pos_marker\n            # We know it's an insertion if it has length but not in the templated\n            # file.\n            if seg.raw and seg.pos_marker.is_point():\n                # Add it to the insertion buffer if it has length:\n                if seg.raw:\n                    insert_buff += seg.raw\n                    # We want to capture the first position where we have a point.\n                    first_segment_pos = first_segment_pos or seg.pos_marker\n                    linter_logger.debug(\n                        \"Appending insertion buffer. %r @idx: %s\",\n                        insert_buff,\n                        templated_idx,\n                    )\n                continue\n\n            # If we get here, then we know it's an original. Check for deletions at\n            # the point before this segment (vs the TEMPLATED).\n            # Deletions in this sense could also mean source consumption.\n            start_diff = seg.pos_marker.templated_slice.start - templated_idx\n\n            # Check to see whether there's a discontinuity before the current\n            # segment\n            if start_diff > 0 or insert_buff:\n                # If we have an insert buffer, then it's an edit, otherwise a\n                # deletion.\n\n                # For the start of the next segment, we need the position of the\n                # first raw, not the pos marker of the whole thing. That accounts\n                # better for loops.\n                first_segment_pos = first_segment_pos or seg.pos_marker\n                yield FixPatch(\n                    # Whether the source slice is zero depends on the start_diff.\n                    # A non-zero start diff implies a deletion, or more likely\n                    # a consumed element of the source. We can use the tracking\n                    # markers from the last segment to recreate where this element\n                    # should be inserted in both source and template.\n                    # The slices must never go backwards so the end of the slice must\n                    # be greater than or equal to the start.\n                    source_slice=slice(\n                        source_idx,\n                        max(first_segment_pos.source_slice.start, source_idx),\n                    ),\n                    templated_slice=slice(\n                        templated_idx,\n                        max(first_segment_pos.templated_slice.start, templated_idx),\n                    ),\n                    patch_category=\"mid_point\",\n                    fixed_raw=insert_buff,\n                    templated_str=\"\",\n                    source_str=\"\",\n                )\n\n                # Reset the first position so we can move the pointer forward.\n                first_segment_pos = None\n                insert_buff = \"\"\n\n            # Now we deal with any changes *within* the segment itself.\n            yield from _iter_templated_patches(seg, templated_file=templated_file)\n\n            # Once we've dealt with any patches from the segment, update\n            # our position markers.\n            source_idx = seg.pos_marker.source_slice.stop\n            templated_idx = seg.pos_marker.templated_slice.stop\n\n        # After the loop, we check whether there's a trailing deletion\n        # or insert. Also valid if we still have an insertion buffer here.\n        end_diff = segment.pos_marker.templated_slice.stop - templated_idx\n        if end_diff or insert_buff:\n            source_slice = slice(\n                source_idx,\n                segment.pos_marker.source_slice.stop,\n            )\n            templated_slice = slice(\n                templated_idx,\n                segment.pos_marker.templated_slice.stop,\n            )\n            # We determine the source_slice directly rather than\n            # inferring it so that we can be very specific that\n            # we ensure that fixes adjacent to source-only slices\n            # (e.g. {% endif %}) are placed appropriately relative\n            # to source-only slices.\n            yield FixPatch(\n                source_slice=source_slice,\n                templated_slice=templated_slice,\n                patch_category=\"end_point\",\n                fixed_raw=insert_buff,\n                templated_str=templated_file.templated_str[templated_slice],\n                source_str=templated_file.source_str[source_slice],\n            )\n", "type": "function"}, {"name": "test__dbt_templated_models_fix_does_not_corrupt_file", "is_method": false, "class_name": null, "parameters": ["project_dir", "path", "caplog", "dbt_fluff_config"], "calls": ["pytest.mark.parametrize", "os.path.join", "_clean_path", "Linter", "os.path.dirname", "caplog.at_level", "lntr.lint_path", "lnt.persist_changes", "_clean_path", "FluffConfig", "os.path.join", "open", "f.read", "open", "f.read", "os.path.join", "os.path.join", "path.replace"], "code_location": {"file": "templater_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/test", "start_line": 399, "end_line": 419}, "code_snippet": "def test__dbt_templated_models_fix_does_not_corrupt_file(\n    project_dir,\n    path,\n    caplog,\n    dbt_fluff_config,\n):\n    \"\"\"Test issues where previously \"sqlfluff fix\" corrupted the file.\"\"\"\n    test_glob = os.path.join(project_dir, os.path.dirname(path), \"*FIXED.sql\")\n    _clean_path(test_glob)\n    lntr = Linter(config=FluffConfig(configs=dbt_fluff_config))\n    with caplog.at_level(logging.INFO, logger=\"sqlfluff.linter\"):\n        lnt = lntr.lint_path(os.path.join(project_dir, path), fix=True)\n    try:\n        lnt.persist_changes(fixed_file_suffix=\"FIXED\")\n        with open(os.path.join(project_dir, path + \".after\")) as f:\n            comp_buff = f.read()\n        with open(os.path.join(project_dir, path.replace(\".sql\", \"FIXED.sql\"))) as f:\n            fixed_buff = f.read()\n        assert fixed_buff == comp_buff\n    finally:\n        _clean_path(test_glob)\n", "type": "function"}, {"name": "_process_lint_result", "is_method": true, "class_name": "BaseRule", "parameters": ["self", "res", "templated_file", "ignore_mask", "new_lerrs", "new_fixes", "root"], "calls": ["res.to_linting_error", "new_lerrs.append", "new_fixes.extend", "self.discard_unsafe_fixes", "root.path_to", "ignore_mask.ignore_masked_violations", "self.crawl_behaviour.passes_filter", "linter_logger.info", "all", "linter_logger.info", "self.crawl_behaviour.passes_filter"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 608, "end_line": 655}, "code_snippet": "    def _process_lint_result(\n        self,\n        res: LintResult,\n        templated_file: Optional[TemplatedFile],\n        ignore_mask: Optional[\"IgnoreMask\"],\n        new_lerrs: list[SQLLintError],\n        new_fixes: list[LintFix],\n        root: BaseSegment,\n    ) -> None:\n        # Unless the rule declares that it's already template safe. Do safety\n        # checks.\n        if not self.template_safe_fixes:\n            self.discard_unsafe_fixes(res, templated_file)\n        lerr = res.to_linting_error(rule=self)\n        if not lerr:\n            return None\n        if ignore_mask:\n            if not ignore_mask.ignore_masked_violations([lerr]):\n                return None\n\n        # Check whether this should be filtered out for being unparsable.\n        # To do that we check the parents of the anchors (of the violation\n        # and fixes) against the filter in the crawler.\n        # NOTE: We use `.passes_filter` here to do the test for unparsable\n        # to avoid duplicating code because that test is already implemented\n        # there.\n        anchors = [lerr.segment] + [fix.anchor for fix in lerr.fixes]\n        for anchor in anchors:\n            if not self.crawl_behaviour.passes_filter(anchor):  # pragma: no cover\n                # NOTE: This clause is untested, because it's a hard to produce\n                # edge case. The latter clause is much more likely.\n                linter_logger.info(\n                    \"Fix skipped due to anchor not passing filter: %s\", anchor\n                )\n                return None\n\n            parent_stack = root.path_to(anchor)\n            if not all(\n                self.crawl_behaviour.passes_filter(ps.segment) for ps in parent_stack\n            ):\n                linter_logger.info(\n                    \"Fix skipped due to parent of anchor not passing filter: %s\",\n                    [ps.segment for ps in parent_stack],\n                )\n                return None\n\n        new_lerrs.append(lerr)\n        new_fixes.extend(res.fixes)\n", "type": "function"}, {"name": "lint_fix_parsed", "is_method": true, "class_name": "Linter", "parameters": ["cls", "tree", "config", "rule_pack", "fix", "fname", "templated_file", "formatter"], "calls": ["config.get", "config.get", "linter_logger.info", "linter_logger.info", "config.get", "formatter.dispatch_lint_header", "cls.allowed_rule_ref_map", "IgnoreMask.from_tree", "phases.append", "range", "cls.remove_templated_errors", "format", "sorted", "config.get", "len", "linter_logger.info", "is_first_linter_pass", "tqdm", "tree.stringify", "rule_pack.codes", "progress_bar_crawler.set_description", "time.monotonic", "crawler.crawl", "is_first_linter_pass", "rule_timings.append", "linter_logger.info", "linter_logger.warning", "linter_logger.info", "compute_anchor_edit_info", "any", "isinstance", "is_first_linter_pass", "config.get", "anchor_info.items", "cls._report_conflicting_fixes_same_anchor", "linter_logger.debug", "apply_fixes", "time.monotonic", "anchor_info.values", "config.get", "tuple", "linter_logger.debug", "config.get", "tuple", "linter_logger.warning", "previous_versions.add", "cls._warn_unfixable"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 380, "end_line": 628}, "code_snippet": "    def lint_fix_parsed(\n        cls,\n        tree: BaseSegment,\n        config: FluffConfig,\n        rule_pack: RulePack,\n        fix: bool = False,\n        fname: Optional[str] = None,\n        templated_file: Optional[\"TemplatedFile\"] = None,\n        formatter: Optional[FormatterInterface] = None,\n    ) -> tuple[BaseSegment, list[SQLBaseError], Optional[IgnoreMask], RuleTimingsType]:\n        \"\"\"Lint and optionally fix a tree object.\"\"\"\n        # Keep track of the linting errors on the very first linter pass. The\n        # list of issues output by \"lint\" and \"fix\" only includes issues present\n        # in the initial SQL code, EXCLUDING any issues that may be created by\n        # the fixes themselves.\n        initial_linting_errors = []\n        # A placeholder for the fixes we had on the previous loop\n        last_fixes: Optional[list[LintFix]] = None\n        # Keep a set of previous versions to catch infinite loops.\n        previous_versions: set[tuple[str, tuple[\"SourceFix\", ...]]] = {(tree.raw, ())}\n        # Keep a buffer for recording rule timings.\n        rule_timings: RuleTimingsType = []\n\n        # If we are fixing then we want to loop up to the runaway_limit, otherwise just\n        # once for linting.\n        loop_limit = config.get(\"runaway_limit\") if fix else 1\n\n        # Dispatch the output for the lint header\n        if formatter:\n            formatter.dispatch_lint_header(\n                fname or \"<filename>\", sorted(rule_pack.codes())\n            )\n\n        # Look for comment segments which might indicate lines to ignore.\n        disable_noqa_except: Optional[str] = config.get(\"disable_noqa_except\")\n        if not config.get(\"disable_noqa\") or disable_noqa_except:\n            allowed_rules_ref_map = cls.allowed_rule_ref_map(\n                rule_pack.reference_map, disable_noqa_except\n            )\n            ignore_mask, ivs = IgnoreMask.from_tree(tree, allowed_rules_ref_map)\n            initial_linting_errors += ivs\n        else:\n            ignore_mask = None\n\n        save_tree = tree\n        # There are two phases of rule running.\n        # 1. The main loop is for most rules. These rules are assumed to\n        # interact and cause a cascade of fixes requiring multiple passes.\n        # These are run the `runaway_limit` number of times (default 10).\n        # 2. The post loop is for post-processing rules, not expected to trigger\n        # any downstream rules, e.g. capitalization fixes. They are run on the\n        # first loop and then twice at the end (once to fix, and once again to\n        # check result of fixes), but not in the intervening loops.\n        phases = [\"main\"]\n        if fix:\n            phases.append(\"post\")\n        for phase in phases:\n            if len(phases) > 1:\n                rules_this_phase = [\n                    rule for rule in rule_pack.rules if rule.lint_phase == phase\n                ]\n            else:\n                rules_this_phase = rule_pack.rules\n            for loop in range(loop_limit if phase == \"main\" else 2):\n\n                def is_first_linter_pass() -> bool:\n                    return phase == phases[0] and loop == 0\n\n                # Additional newlines are to assist in scanning linting loops\n                # during debugging.\n                linter_logger.info(\n                    f\"\\n\\nEntering linter phase {phase}, loop {loop + 1}/{loop_limit}\\n\"\n                )\n                changed = False\n\n                if is_first_linter_pass():\n                    # In order to compute initial_linting_errors correctly, need\n                    # to run all rules on the first loop of the main phase.\n                    rules_this_phase = rule_pack.rules\n                progress_bar_crawler = tqdm(\n                    rules_this_phase,\n                    desc=\"lint by rules\",\n                    leave=False,\n                    disable=progress_bar_configuration.disable_progress_bar,\n                )\n\n                for crawler in progress_bar_crawler:\n                    # Performance: After first loop pass, skip rules that don't\n                    # do fixes. Any results returned won't be seen by the user\n                    # anyway (linting errors ADDED by rules changing SQL, are\n                    # not reported back to the user - only initial linting errors),\n                    # so there's absolutely no reason to run them.\n                    if (\n                        fix\n                        and not is_first_linter_pass()\n                        and not crawler.is_fix_compatible\n                    ):\n                        continue\n\n                    progress_bar_crawler.set_description(f\"rule {crawler.code}\")\n                    t0 = time.monotonic()\n\n                    # fixes should be a dict {} with keys edit, delete, create\n                    # delete is just a list of segments to delete\n                    # edit and create are list of tuples. The first element is\n                    # the \"anchor\", the segment to look for either to edit or to\n                    # insert BEFORE. The second is the element to insert or create.\n                    linting_errors, _, fixes, _ = crawler.crawl(\n                        tree,\n                        dialect=config.get(\"dialect_obj\"),\n                        fix=fix,\n                        templated_file=templated_file,\n                        ignore_mask=ignore_mask,\n                        fname=fname,\n                        config=config,\n                    )\n                    if is_first_linter_pass():\n                        initial_linting_errors += linting_errors\n\n                    if fix and fixes:\n                        linter_logger.info(f\"Applying Fixes [{crawler.code}]: {fixes}\")\n                        # Do some sanity checks on the fixes before applying.\n                        anchor_info = compute_anchor_edit_info(fixes)\n                        if any(\n                            not info.is_valid for info in anchor_info.values()\n                        ):  # pragma: no cover\n                            message = (\n                                f\"Rule {crawler.code} returned conflicting \"\n                                \"fixes with the same anchor. This is only \"\n                                \"supported for create_before+create_after, so \"\n                                \"the fixes will not be applied. \"\n                            )\n                            for uuid, info in anchor_info.items():\n                                if not info.is_valid:\n                                    message += f\"\\n{uuid}:\"\n                                    for _fix in info.fixes:\n                                        message += f\"\\n    {_fix}\"\n                            cls._report_conflicting_fixes_same_anchor(message)\n                            for lint_result in linting_errors:\n                                lint_result.fixes = []\n                        elif fixes == last_fixes:\n                            # If we generate the same fixes two times in a row,\n                            # that means we're in a loop, and we want to stop.\n                            # (Fixes should address issues, hence different\n                            # and/or fewer fixes next time.)\n                            # This is most likely because fixes could not be safely\n                            # applied last time, so we should stop gracefully.\n                            linter_logger.debug(\n                                f\"Fixes generated for {crawler.code} are the same as \"\n                                \"the previous pass. Assuming that we cannot apply them \"\n                                \"safely. Passing gracefully.\"\n                            )\n                        else:\n                            # This is the happy path. We have fixes, now we want to\n                            # apply them.\n                            last_fixes = fixes\n                            new_tree, _, _, _valid = apply_fixes(\n                                tree,\n                                config.get(\"dialect_obj\"),\n                                crawler.code,\n                                anchor_info,\n                                fix_even_unparsable=config.get(\"fix_even_unparsable\"),\n                            )\n\n                            # Check for infinite loops. We use a combination of the\n                            # fixed templated file and the list of source fixes to\n                            # apply.\n                            loop_check_tuple = (\n                                new_tree.raw,\n                                tuple(new_tree.source_fixes),\n                            )\n                            # Was anything actually applied? If not, then the fixes we\n                            # had cannot be safely applied and we should stop trying.\n                            if loop_check_tuple == (tree.raw, tuple(tree.source_fixes)):\n                                linter_logger.debug(\n                                    f\"Fixes for {crawler.code} could not be safely be \"\n                                    \"applied. Likely due to initially unparsable file.\"\n                                )\n                            elif not _valid:\n                                # The fixes result in an invalid file. Don't apply\n                                # the fix and skip onward. Show a warning.\n                                linter_logger.warning(\n                                    f\"Fixes for {crawler.code} not applied, as it \"\n                                    \"would result in an unparsable file. Please \"\n                                    \"report this as a bug with a minimal query \"\n                                    \"which demonstrates this warning.\"\n                                )\n                            elif loop_check_tuple not in previous_versions:\n                                # We've not seen this version of the file so\n                                # far. Continue.\n                                tree = new_tree\n                                previous_versions.add(loop_check_tuple)\n                                changed = True\n                                continue\n                            else:\n                                # Applying these fixes took us back to a state\n                                # which we've seen before. We're in a loop, so\n                                # we want to stop.\n                                cls._warn_unfixable(crawler.code)\n\n                    # Record rule timing\n                    rule_timings.append(\n                        (crawler.code, crawler.name, time.monotonic() - t0)\n                    )\n\n                if fix and not changed:\n                    # We did not change the file. Either the file is clean (no\n                    # fixes), or any fixes which are present will take us back\n                    # to a previous state.\n                    linter_logger.info(\n                        f\"Fix loop complete for {phase} phase. Stability \"\n                        f\"achieved after {loop}/{loop_limit} loops.\"\n                    )\n                    break\n            else:\n                if fix:\n                    # The linter loop hit the limit before reaching a stable point\n                    # (i.e. free of lint errors). If this happens, it's usually\n                    # because one or more rules produced fixes which did not address\n                    # the original issue **or** created new issues.\n                    linter_logger.warning(\n                        f\"Loop limit on fixes reached [{loop_limit}].\"\n                    )\n\n                    # Discard any fixes for the linting errors, since they caused a\n                    # loop. IMPORTANT: By doing this, we are telling SQLFluff that\n                    # these linting errors are \"unfixable\". This is important,\n                    # because when \"sqlfluff fix\" encounters unfixable lint errors,\n                    # it exits with a \"failure\" exit code, which is exactly what we\n                    # want in this situation. (Reason: Although this is more of an\n                    # internal SQLFluff issue, users deserve to know about it,\n                    # because it means their file(s) weren't fixed.\n                    for violation in initial_linting_errors:\n                        if isinstance(violation, SQLLintError):\n                            violation.fixes = []\n\n                    # Return the original parse tree, before any fixes were applied.\n                    # Reason: When the linter hits the loop limit, the file is often\n                    # messy, e.g. some of the fixes were applied repeatedly, possibly\n                    # other weird things. We don't want the user to see this junk!\n                    return save_tree, initial_linting_errors, ignore_mask, rule_timings\n\n        if config.get(\"ignore_templated_areas\", default=True):\n            initial_linting_errors = cls.remove_templated_errors(initial_linting_errors)\n\n        linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Fixed Tree:\"))\n        linter_logger.info(\"\\n\" + tree.stringify())\n\n        return tree, initial_linting_errors, ignore_mask, rule_timings\n", "type": "function"}, {"name": "test__fix__generate_source_patches", "is_method": false, "class_name": null, "parameters": ["tree", "templated_file", "expected_result", "caplog"], "calls": ["pytest.mark.parametrize", "caplog.at_level", "generate_source_patches", "RawSegment", "RawSegment", "BaseSegment", "BaseSegment", "BaseSegment", "PositionMarker", "PositionMarker", "FixPatch", "FixPatch", "FixPatch", "FixPatch", "FixPatch", "FixPatch", "slice", "slice", "slice", "slice", "slice", "slice", "RawSegment", "RawSegment", "RawSegment", "slice", "slice", "TemplateSegment", "RawSegment", "RawSegment", "RawSegment", "slice", "slice", "TemplateSegment", "RawSegment", "RawSegment", "RawSegment", "slice", "slice", "slice", "slice", "slice", "slice", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "SourceFix", "SourceFix", "slice", "slice", "slice", "slice"], "code_location": {"file": "fix_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 195, "end_line": 202}, "code_snippet": "def test__fix__generate_source_patches(tree, templated_file, expected_result, caplog):\n    \"\"\"Test generate_source_patches.\n\n    This is part of fix_string().\n    \"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.linter\"):\n        result = generate_source_patches(tree, templated_file)\n    assert result == expected_result\n", "type": "function"}, {"name": "test__templater_full", "is_method": false, "class_name": null, "parameters": ["subpath", "code_only", "include_meta", "yaml_loader", "caplog"], "calls": ["pytest.mark.parametrize", "caplog.set_level", "caplog.set_level", "assert_structure"], "code_location": {"file": "jinja_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/templaters", "start_line": 754, "end_line": 765}, "code_snippet": "def test__templater_full(subpath, code_only, include_meta, yaml_loader, caplog):\n    \"\"\"Check structure can be parsed from jinja templated files.\"\"\"\n    # Log the templater and lexer throughout this test\n    caplog.set_level(logging.DEBUG, logger=\"sqlfluff.templater\")\n    caplog.set_level(logging.DEBUG, logger=\"sqlfluff.lexer\")\n\n    assert_structure(\n        yaml_loader,\n        \"test/fixtures/templater/\" + subpath,\n        code_only=code_only,\n        include_meta=include_meta,\n    )\n", "type": "function"}, {"name": "render_string", "is_method": true, "class_name": "Linter", "parameters": ["self", "in_str", "fname", "config", "encoding"], "calls": ["linter_logger.info", "time.monotonic", "self._normalise_newlines", "config.verify_dialect_specified", "config.get", "linter_logger.info", "RenderedFile", "linter_logger.warning", "self.templater.process_with_variants", "linter_logger.info", "len", "config.get", "templater_violations.append", "linter_logger.warning", "time.monotonic", "templated_variants.append", "len", "str", "config.get"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 822, "end_line": 893}, "code_snippet": "    def render_string(\n        self, in_str: str, fname: str, config: FluffConfig, encoding: str\n    ) -> RenderedFile:\n        \"\"\"Template the file.\"\"\"\n        linter_logger.info(\"Rendering String [%s] (%s)\", self.templater.name, fname)\n\n        # Start the templating timer\n        t0 = time.monotonic()\n\n        # Newlines are normalised to unix-style line endings (\\n).\n        # The motivation is that Jinja normalises newlines during templating and\n        # we want consistent mapping between the raw and templated slices.\n        in_str = self._normalise_newlines(in_str)\n\n        # Since Linter.__init__() does not require a dialect to be specified,\n        # check for one now. (We're processing a string, not a file, so we're\n        # not going to pick up a .sqlfluff or other config file to provide a\n        # missing dialect at this point.)\n        config.verify_dialect_specified()\n        if not config.get(\"templater_obj\") == self.templater:\n            linter_logger.warning(\n                f\"Attempt to set templater to {config.get('templater_obj').name} \"\n                f\"failed. Using {self.templater.name} templater. Templater cannot \"\n                \"be set in a .sqlfluff file in a subdirectory of the current \"\n                \"working directory. It can be set in a .sqlfluff in the current \"\n                \"working directory. See Nesting section of the docs for more \"\n                \"details.\"\n            )\n\n        variant_limit = config.get(\"render_variant_limit\")\n        templated_variants: list[TemplatedFile] = []\n        templater_violations: list[SQLTemplaterError] = []\n\n        try:\n            for variant, templater_errs in self.templater.process_with_variants(\n                in_str=in_str, fname=fname, config=config, formatter=self.formatter\n            ):\n                if variant:\n                    templated_variants.append(variant)\n                # NOTE: We could very easily end up with duplicate errors between\n                # different variants and this code doesn't currently do any\n                # deduplication between them. That will be resolved in further\n                # testing.\n                # TODO: Resolve potential duplicate templater violations between\n                # variants before we enable jinja variant linting by default.\n                templater_violations += templater_errs\n                if len(templated_variants) >= variant_limit:\n                    # Stop if we hit the limit.\n                    break\n        except SQLTemplaterError as templater_err:\n            # Fatal templating error. Capture it and don't generate a variant.\n            templater_violations.append(templater_err)\n        except SQLFluffSkipFile as skip_file_err:  # pragma: no cover\n            linter_logger.warning(str(skip_file_err))\n\n        if not templated_variants:\n            linter_logger.info(\"TEMPLATING FAILED: %s\", templater_violations)\n\n        linter_logger.info(\"Rendered %s variants\", len(templated_variants))\n\n        # Record time\n        time_dict = {\"templating\": time.monotonic() - t0}\n\n        return RenderedFile(\n            templated_variants,\n            templater_violations,\n            config,\n            time_dict,\n            fname,\n            encoding,\n            in_str,\n        )\n", "type": "function"}, {"name": "slice_file", "is_method": true, "class_name": "PythonTemplater", "parameters": ["self", "raw_str", "render_func", "config", "append_to_templated"], "calls": ["templater_logger.info", "templater_logger.debug", "render_func", "templater_logger.debug", "list", "templater_logger.debug", "enumerate", "templater_logger.debug", "range", "self._slice_template", "templater_logger.debug", "templater_logger.debug", "self._substring_occurrences", "self._substring_occurrences", "templater_logger.debug", "list", "templater_logger.debug", "enumerate", "list", "templater_logger.debug", "enumerate", "self._check_for_wrapped", "self._split_invariants", "templater_logger.debug", "self._split_uniques_coalesce_rest", "templater_logger.debug", "config.get"], "code_location": {"file": "python.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 304, "end_line": 380}, "code_snippet": "    def slice_file(\n        self,\n        raw_str: str,\n        render_func: Callable[[str], str],\n        config: Optional[FluffConfig] = None,\n        append_to_templated: str = \"\",\n    ) -> tuple[list[RawFileSlice], list[TemplatedFileSlice], str]:\n        \"\"\"Slice the file to determine regions where we can fix.\"\"\"\n        templater_logger.info(\"Slicing File Template\")\n        templater_logger.debug(\"    Raw String: %r\", raw_str)\n        # Render the templated string.\n        # NOTE: This seems excessive in this simple example, but for other templating\n        # engines we need more control over the rendering so may need to call this\n        # method more than once.\n        templated_str = render_func(raw_str)\n        templater_logger.debug(\"    Templated String: %r\", templated_str)\n        # Slice the raw file\n        raw_sliced = list(self._slice_template(raw_str))\n        templater_logger.debug(\"    Raw Sliced:\")\n        for idx, raw_slice in enumerate(raw_sliced):\n            templater_logger.debug(\"        %s: %r\", idx, raw_slice)\n        # Find the literals\n        literals = [\n            raw_slice.raw\n            for raw_slice in raw_sliced\n            if raw_slice.slice_type == \"literal\"\n        ]\n        templater_logger.debug(\"    Literals: %s\", literals)\n        for loop_idx in range(2):\n            templater_logger.debug(\"    # Slice Loop %s\", loop_idx)\n            # Calculate occurrences\n            raw_occurrences = self._substring_occurrences(raw_str, literals)\n            templated_occurrences = self._substring_occurrences(templated_str, literals)\n            templater_logger.debug(\n                \"    Occurrences: Raw: %s, Templated: %s\",\n                raw_occurrences,\n                templated_occurrences,\n            )\n            # Split on invariants\n            split_sliced = list(\n                self._split_invariants(\n                    raw_sliced,\n                    literals,\n                    raw_occurrences,\n                    templated_occurrences,\n                    templated_str,\n                )\n            )\n            templater_logger.debug(\"    Split Sliced:\")\n            for idx, split_slice in enumerate(split_sliced):\n                templater_logger.debug(\"        %s: %r\", idx, split_slice)\n            # Deal with uniques and coalesce the rest\n            sliced_file = list(\n                self._split_uniques_coalesce_rest(\n                    split_sliced, raw_occurrences, templated_occurrences, templated_str\n                )\n            )\n            templater_logger.debug(\"    Fully Sliced:\")\n            for idx, templ_slice in enumerate(sliced_file):\n                templater_logger.debug(\"        %s: %r\", idx, templ_slice)\n            unwrap_wrapped = (\n                True\n                if config is None\n                else config.get(\n                    \"unwrap_wrapped_queries\", section=\"templater\", default=True\n                )\n            )\n            sliced_file, new_templated_str = self._check_for_wrapped(\n                sliced_file, templated_str, unwrap_wrapped=unwrap_wrapped\n            )\n            if new_templated_str == templated_str:\n                # If we didn't change it then we're done.\n                break\n            else:\n                # If it's not equal, loop around\n                templated_str = new_templated_str\n        return raw_sliced, sliced_file, new_templated_str\n", "type": "function"}, {"name": "generate_source_patches", "is_method": false, "class_name": null, "parameters": ["tree", "templated_file"], "calls": ["linter_logger.debug", "enumerate", "sorted", "_iter_templated_patches", "linter_logger.debug", "_log_hints", "templated_file.raw_slices_spanning_source_slice", "patch.dedupe_tuple", "linter_logger.info", "linter_logger.info", "filtered_source_patches.append", "dedupe_buffer.append", "patch.dedupe_tuple", "set", "patch.dedupe_tuple", "linter_logger.info", "filtered_source_patches.append", "dedupe_buffer.append", "patch.dedupe_tuple", "linter_logger.info", "filtered_source_patches.append", "dedupe_buffer.append", "linter_logger.warning", "patch.dedupe_tuple"], "code_location": {"file": "patch.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 234, "end_line": 313}, "code_snippet": "def generate_source_patches(\n    tree: BaseSegment, templated_file: TemplatedFile\n) -> list[FixPatch]:\n    \"\"\"Use the fixed tree to generate source patches.\n\n    Importantly here we deduplicate and sort the patches from their position\n    in the templated file into their intended order in the source file.\n\n    Any source fixes are generated in `_iter_templated_patches` and included\n    alongside any standard fixes. That means we treat them the same here.\n    \"\"\"\n    # Iterate patches, filtering and translating as we go:\n    linter_logger.debug(\"### Beginning Patch Iteration.\")\n    filtered_source_patches = []\n    dedupe_buffer = []\n    # We use enumerate so that we get an index for each patch. This is entirely\n    # so when debugging logs we can find a given patch again!\n    for idx, patch in enumerate(\n        _iter_templated_patches(tree, templated_file=templated_file)\n    ):\n        linter_logger.debug(\"  %s Yielded patch: %s\", idx, patch)\n        _log_hints(patch, templated_file)\n\n        # Check for duplicates\n        if patch.dedupe_tuple() in dedupe_buffer:\n            linter_logger.info(\n                \"      - Skipping. Source space Duplicate: %s\",\n                patch.dedupe_tuple(),\n            )\n            continue\n\n        # We now evaluate patches in the source-space for whether they overlap\n        # or disrupt any templated sections unless designed to do so.\n        # NOTE: We rely here on the patches being generated in order.\n\n        # Get the affected raw slices.\n        local_raw_slices = templated_file.raw_slices_spanning_source_slice(\n            patch.source_slice\n        )\n        local_type_list = [slc.slice_type for slc in local_raw_slices]\n\n        # Deal with the easy cases of 1) New code at end 2) only literals\n        if not local_type_list or set(local_type_list) == {\"literal\"}:\n            linter_logger.info(\n                \"      * Keeping patch on new or literal-only section.\",\n            )\n            filtered_source_patches.append(patch)\n            dedupe_buffer.append(patch.dedupe_tuple())\n        # Handle the easy case of an explicit source fix\n        elif patch.patch_category == \"source\":\n            linter_logger.info(\n                \"      * Keeping explicit source fix patch.\",\n            )\n            filtered_source_patches.append(patch)\n            dedupe_buffer.append(patch.dedupe_tuple())\n        # Is it a zero length patch.\n        elif (\n            patch.source_slice.start == patch.source_slice.stop\n            and patch.source_slice.start == local_raw_slices[0].source_idx\n        ):\n            linter_logger.info(\n                \"      * Keeping insertion patch on slice boundary.\",\n            )\n            filtered_source_patches.append(patch)\n            dedupe_buffer.append(patch.dedupe_tuple())\n        else:  # pragma: no cover\n            # We've got a situation where the ends of our patch need to be\n            # more carefully mapped. This used to happen with greedy template\n            # element matching, but should now never happen. In the event that\n            # it does, we'll warn but carry on.\n            linter_logger.warning(\n                \"Skipping edit patch on uncertain templated section [%s], \"\n                \"Please report this warning on GitHub along with the query \"\n                \"that produced it.\",\n                (patch.patch_category, patch.source_slice),\n            )\n            continue\n\n    # Sort the patches before building up the file.\n    return sorted(filtered_source_patches, key=lambda x: x.source_slice.start)\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1718134880065918}
{"question": "Why does SQLFluff implement dialect-specific parsing rather than using a unified SQL parser?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff implements dialect-specific parsing rather than a unified SQL parser to address the significant variations and incompatibilities between different SQL dialects. Key reasons include: 1) Syntax differences - Different SQL dialects have varying syntax for common operations (e.g., string concatenation, date functions, window functions); 2) Keyword variations - Reserved keywords differ between dialects (e.g., TOP vs LIMIT, ISNULL vs IFNULL); 3) Data type differences - Each dialect supports different data types and type casting syntax; 4) Function variations - Built-in functions have different names and parameter patterns across dialects; 5) Extensions and proprietary features - Each database vendor adds proprietary SQL extensions that aren't part of standard SQL; 6) Parsing accuracy - Dialect-specific parsers can provide more accurate parsing and better error detection for each specific dialect; 7) Rule applicability - Different linting rules may only be relevant or applicable to certain dialects; 8) User expectations - Users expect SQLFluff to understand their specific dialect's syntax and conventions; 9) Error reporting - Dialect-specific parsing enables more precise error messages and suggestions; 10) Future extensibility - The dialect system allows SQLFluff to support new dialects without affecting existing ones.", "score": null, "retrieved_content": [{"name": "parse", "is_method": false, "class_name": null, "parameters": ["sql", "dialect", "config", "config_path"], "calls": ["Linter", "linter.parse_string", "parsed.root_variant", "root_variant.tree.as_record", "get_simple_config", "APIParsingError"], "code_location": {"file": "simple.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/api", "start_line": 168, "end_line": 212}, "code_snippet": "def parse(\n    sql: str,\n    dialect: Optional[str] = None,\n    config: Optional[FluffConfig] = None,\n    config_path: Optional[str] = None,\n) -> dict[str, Any]:\n    \"\"\"Parse a SQL string.\n\n    Args:\n        sql (:obj:`str`): The SQL to be parsed.\n        dialect (:obj:`Optional[str]`, optional): A reference to the dialect of the SQL\n            to be parsed. Defaults to `ansi`.\n        config (:obj:`Optional[FluffConfig]`, optional): A configuration object\n            to use for the operation. Defaults to None.\n        config_path (:obj:`Optional[str]`, optional): A path to a .sqlfluff config,\n            which is only used if a `config` is not already provided.\n            Defaults to None.\n\n    Returns:\n        :obj:`Dict[str, Any]` JSON containing the parsed structure.\n\n    Note:\n        In the case of multiple potential variants from the raw source file\n        only the first variant is returned by the simple API. For access to\n        the other variants, use the underlying main API directly.\n    \"\"\"\n    cfg = config or get_simple_config(\n        dialect=dialect,\n        config_path=config_path,\n    )\n    linter = Linter(config=cfg)\n\n    parsed = linter.parse_string(sql)\n    # If we encounter any parsing errors, raise them in a combined issue.\n    violations = parsed.violations\n    if violations:\n        raise APIParsingError(violations)\n    # Return a JSON representation of the parse tree.\n    # NOTE: For the simple API - only a single variant is returned.\n    root_variant = parsed.root_variant()\n    assert root_variant, \"Files parsed without violations must have a valid variant\"\n    assert root_variant.tree, \"Files parsed without violations must have a valid tree\"\n    record = root_variant.tree.as_record(show_raw=True)\n    assert record\n    return record\n", "type": "function"}, {"name": "TestFlinkSQLDialect", "docstring": "Test FlinkSQL dialect parsing.", "methods": ["test_flink_dialect_basic", "test_flink_create_table_basic", "test_flink_row_data_type", "test_flink_timestamp_with_precision", "test_flink_watermark_definition", "test_flink_computed_column", "test_flink_metadata_column", "test_flink_show_statements", "test_flink_use_statements", "test_flink_describe_statement", "test_flink_explain_statement", "test_flink_create_catalog", "test_flink_create_database", "test_flink_alternative_with_syntax"], "attributes": [], "code_location": {"file": "flink_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 6, "end_line": 231}, "type": "class"}, {"name": "_initialise_dialect", "is_method": true, "class_name": "FluffConfig", "parameters": ["self", "dialect", "require_dialect"], "calls": ["dialect_selector", "self.verify_dialect_specified"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 153, "end_line": 162}, "code_snippet": "    def _initialise_dialect(\n        self, dialect: Optional[str], require_dialect: bool = True\n    ) -> None:\n        # NB: We import here to avoid a circular references.\n        from sqlfluff.core.dialects import dialect_selector\n\n        if dialect is not None:\n            self._configs[\"core\"][\"dialect_obj\"] = dialect_selector(dialect)\n        elif require_dialect:\n            self.verify_dialect_specified()\n", "type": "function"}, {"name": "cli", "is_method": false, "class_name": null, "parameters": [], "calls": ["click.group", "click.version_option"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 480, "end_line": 481}, "code_snippet": "def cli() -> None:\n    \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n", "type": "function"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}, {"name": "_parse_tokens", "is_method": true, "class_name": "Linter", "parameters": ["tokens", "config", "fname", "parse_statistics"], "calls": ["Parser", "linter_logger.info", "linter_logger.info", "parsed.iter_unparsables", "parser.parse", "format", "violations.append", "linter_logger.info", "linter_logger.info", "tuple", "linter_logger.info", "violations.append", "parsed.stringify", "SQLParseError", "unparsable.stringify", "format", "len"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 231, "end_line": 281}, "code_snippet": "    def _parse_tokens(\n        tokens: Sequence[BaseSegment],\n        config: FluffConfig,\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> tuple[Optional[BaseSegment], list[SQLParseError]]:\n        parser = Parser(config=config)\n        violations = []\n        # Parse the file and log any problems\n        try:\n            parsed: Optional[BaseSegment] = parser.parse(\n                # Regardless of how the sequence was passed in, we should\n                # coerce it to a tuple here, before we head deeper into\n                # the parsing process.\n                tuple(tokens),\n                fname=fname,\n                parse_statistics=parse_statistics,\n            )\n        except SQLParseError as err:\n            linter_logger.info(\"PARSING FAILED! : %s\", err)\n            violations.append(err)\n            return None, violations\n\n        if parsed is None:  # pragma: no cover\n            return None, violations\n\n        linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n        linter_logger.info(\"\\n\" + parsed.stringify())\n        # We may succeed parsing, but still have unparsable segments. Extract them\n        # here.\n        for unparsable in parsed.iter_unparsables():\n            # No exception has been raised explicitly, but we still create one here\n            # so that we can use the common interface\n            assert unparsable.pos_marker\n            violations.append(\n                SQLParseError(\n                    \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n                    \"{1!r}\".format(\n                        unparsable.pos_marker.working_loc,\n                        (\n                            unparsable.raw\n                            if len(unparsable.raw) < 40\n                            else unparsable.raw[:40] + \"...\"\n                        ),\n                    ),\n                    segment=unparsable,\n                )\n            )\n            linter_logger.info(\"Found unparsable segment...\")\n            linter_logger.info(unparsable.stringify())\n        return parsed, violations\n", "type": "function"}, {"name": "test__api__info_rules", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.list_rules", "isinstance", "RuleTuple"], "code_location": {"file": "info_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 29, "end_line": 52}, "code_snippet": "def test__api__info_rules():\n    \"\"\"Basic linting of dialects.\"\"\"\n    rules = sqlfluff.list_rules()\n    assert isinstance(rules, list)\n    assert (\n        RuleTuple(\n            code=\"LT01\",\n            name=\"layout.spacing\",\n            description=\"Inappropriate Spacing.\",\n            groups=(\"all\", \"core\", \"layout\"),\n            aliases=(\n                \"L001\",\n                \"L005\",\n                \"L006\",\n                \"L008\",\n                \"L023\",\n                \"L024\",\n                \"L039\",\n                \"L048\",\n                \"L071\",\n            ),\n        )\n        in rules\n    )\n", "type": "function"}, {"name": "SQLFluffDomain", "docstring": "SQLFluff domain.", "methods": ["get_full_qualified_name", "get_objects", "resolve_xref", "add_rule"], "attributes": ["name", "label", "object_types", "roles", "directives", "initial_data"], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 82, "end_line": 142}, "type": "class"}, {"name": "_dialect_supports_dot_access", "is_method": true, "class_name": "Rule_RF01", "parameters": ["self", "dialect"], "calls": [], "code_location": {"file": "RF01.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 306, "end_line": 327}, "code_snippet": "    def _dialect_supports_dot_access(self, dialect: Dialect) -> bool:\n        # Athena:\n        # https://docs.aws.amazon.com/athena/latest/ug/filtering-with-dot.html\n        # BigQuery:\n        # https://cloud.google.com/bigquery/docs/reference/standard-sql/operators#field_access_operator\n        # Databricks:\n        # https://docs.databricks.com/en/sql/language-manual/functions/dotsign.html\n        # DuckDB:\n        # https://duckdb.org/docs/sql/data_types/struct#retrieving-from-structs\n        # Redshift:\n        # https://docs.aws.amazon.com/redshift/latest/dg/query-super.html\n        # TODO: all doc links to all referenced dialects\n        return dialect.name in (\n            \"athena\",\n            \"bigquery\",\n            \"databricks\",\n            \"duckdb\",\n            \"hive\",\n            \"redshift\",\n            \"soql\",\n            \"sparksql\",\n        )\n", "type": "function"}, {"name": "test__dialect__base_broad_fix", "is_method": false, "class_name": null, "parameters": ["dialect", "file", "raise_critical_errors_after_fix", "caplog"], "calls": ["pytest.mark.parametrize", "load_file", "dict", "lex_and_parse", "print", "FluffConfig", "Linter", "linter.get_rulepack", "linter.lint_parsed", "parsed.tree.stringify"], "code_location": {"file": "dialects_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 85, "end_line": 120}, "code_snippet": "def test__dialect__base_broad_fix(\n    dialect, file, raise_critical_errors_after_fix, caplog\n):\n    \"\"\"Run a full fix with all rules, in search of critical errors.\n\n    NOTE: This suite does all of the same things as the above test\n    suite (the `parse_suite`), but also runs fix. In CI, we run\n    the above tests _with_ coverage tracking, but these we run\n    _without_.\n\n    The purpose of this test is as a more stretching run through\n    a wide range of test sql examples, and the full range of rules\n    to find any potential critical errors raised by any interactions\n    between different dialects and rules.\n\n    We also do not use DEBUG logging here because it gets _very_\n    noisy.\n    \"\"\"\n    raw = load_file(dialect, file)\n    config_overrides = dict(dialect=dialect)\n\n    parsed: Optional[ParsedString] = lex_and_parse(config_overrides, raw)\n    if not parsed:  # Empty file case\n        return\n    print(parsed.tree.stringify())\n\n    config = FluffConfig(overrides=config_overrides)\n    linter = Linter(config=config)\n    rule_pack = linter.get_rulepack()\n    # Due to \"raise_critical_errors_after_fix\" fixture \"fix\",\n    # will now throw.\n    linter.lint_parsed(\n        parsed,\n        rule_pack,\n        fix=True,\n    )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.2037806510925293}
{"question": "Where does SQLFluff's configuration loading flow from file discovery through inheritance resolution?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's configuration loading flow follows a hierarchical path from file discovery through inheritance resolution. The flow includes: 1) Default configuration - The process starts with built-in default configuration loaded from src/sqlfluff/core/default_config.cfg; 2) User home directory - Configuration is loaded from user's home directory (~) for global user settings; 3) App config directory - System-specific app config directory (~/.config/sqlfluff on Unix, AppData on Windows) is checked; 4) Working directory hierarchy - Configuration files are searched in directories from working directory up to home directory; 5) Project-specific files - Configuration files (.sqlfluff, setup.cfg, tox.ini, pyproject.toml) are loaded from current project directory; 6) Subdirectory inheritance - Configuration is inherited from parent directories down to the file being processed; 7) File-specific configuration - In-file configuration directives (-- noqa: comments) are processed; 8) Command-line overrides - Command-line arguments override file-based configuration; 9) Configuration merging - All configuration sources are merged using nested_combine() with later sources overriding earlier ones; 10) Validation and resolution - Final configuration is validated and resolved into the FluffConfig object used throughout the system.", "score": null, "retrieved_content": [{"name": "test__linter__path_from_paths__exts", "is_method": false, "class_name": null, "parameters": [], "calls": ["normalise_paths", "paths_from_path"], "code_location": {"file": "discovery_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 38, "end_line": 46}, "code_snippet": "def test__linter__path_from_paths__exts():\n    \"\"\"Test configuration of file discovery.\"\"\"\n    paths = normalise_paths(\n        paths_from_path(\"test/fixtures/linter\", target_file_exts=[\".txt\", \".txt.j2\"])\n    )\n    assert \"test.fixtures.linter.passing.sql\" not in paths\n    assert \"test.fixtures.linter.passing_cap_extension.SQL\" not in paths\n    assert \"test.fixtures.linter.discovery_file.txt\" in paths\n    assert \"test.fixtures.linter.discovery_file.txt.j2\" in paths\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "FluffConfig", "parameters": ["self", "configs", "extra_config_path", "ignore_local_config", "overrides", "plugin_manager", "require_dialect"], "calls": ["nested_combine", "nested_combine", "self._handle_comma_separated_values", "self._initialise_dialect", "self.get_templater", "validate_config_dict", "isinstance", "get_plugin_manager", "validate_config_dict", "get", "isinstance", "self._plugin_manager.hook.load_default_config"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 89, "end_line": 137}, "code_snippet": "    def __init__(\n        self,\n        configs: Optional[ConfigMappingType] = None,\n        extra_config_path: Optional[str] = None,\n        ignore_local_config: bool = False,\n        overrides: Optional[ConfigMappingType] = None,\n        plugin_manager: Optional[pluggy.PluginManager] = None,\n        # Ideally a dialect should be set when config is read but sometimes\n        # it might only be set in nested .sqlfluff config files, so allow it\n        # to be not required.\n        require_dialect: bool = True,\n    ) -> None:\n        self._extra_config_path = (\n            extra_config_path  # We only store this for child configs\n        )\n        self._ignore_local_config = (\n            ignore_local_config  # We only store this for child configs\n        )\n        # If overrides are provided, validate them early.\n        if overrides:\n            overrides = {\"core\": overrides}\n            validate_config_dict(overrides, \"<provided overrides>\")\n        # Stash overrides so we can pass them to child configs\n        core_overrides = overrides[\"core\"] if overrides else None\n        assert isinstance(core_overrides, dict) or core_overrides is None\n        self._overrides = core_overrides\n\n        # Fetch a fresh plugin manager if we weren't provided with one\n        self._plugin_manager = plugin_manager or get_plugin_manager()\n\n        defaults = nested_combine(*self._plugin_manager.hook.load_default_config())\n        # If any existing configs are provided. Validate them:\n        if configs:\n            validate_config_dict(configs, \"<provided configs>\")\n        self._configs = nested_combine(\n            defaults, configs or {\"core\": {}}, overrides or {}\n        )\n        # Some configs require special treatment\n        self._configs[\"core\"][\"color\"] = (\n            False if self._configs[\"core\"].get(\"nocolor\", False) else None\n        )\n        # Handle inputs which are potentially comma separated strings\n        self._handle_comma_separated_values()\n        # Dialect and Template selection.\n        _dialect = self._configs[\"core\"][\"dialect\"]\n        assert _dialect is None or isinstance(_dialect, str)\n        self._initialise_dialect(_dialect, require_dialect)\n\n        self._configs[\"core\"][\"templater_obj\"] = self.get_templater()\n", "type": "function"}, {"name": "load_config_up_to_path", "is_method": false, "class_name": null, "parameters": ["path", "extra_config_path", "ignore_local_config"], "calls": ["nested_combine", "_load_user_appdir_config", "load_config_at_path", "list", "iter_intermediate_paths", "os.path.expanduser", "iter_intermediate_paths", "load_config_at_path", "absolute", "Path.cwd", "load_config_at_path", "load_config_file_as_dict", "absolute", "Path", "str", "list", "str", "str", "SQLFluffUserError", "os.path.expanduser", "p.resolve", "Path", "p.resolve", "resolve", "Path", "Path"], "code_location": {"file": "loader.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 244, "end_line": 322}, "code_snippet": "def load_config_up_to_path(\n    path: str,\n    extra_config_path: Optional[str] = None,\n    ignore_local_config: bool = False,\n) -> ConfigMappingType:\n    \"\"\"Loads a selection of config files from both the path and its parent paths.\n\n    Args:\n        path (str): The directory which is the target of the search. Config\n            files in subdirectories will not be loaded by this method, but\n            valid config files between this path and the current working\n            path will.\n        extra_config_path (str, optional): An additional path to load config\n            from. This path is not used in iterating through intermediate\n            paths, and is loaded last (taking the highest precedence in\n            combining the loaded configs).\n        ignore_local_config (bool, optional, defaults to False): If set to\n            True, this skips loading configuration from the user home\n            directory (``~``) or ``appdir`` path.\n\n    Returns:\n        :obj:`ConfigMappingType`: A nested dictionary of config values.\n\n    We layer each of the configs on top of each other, starting with any home\n    or user configs (e.g. in ``appdir`` or home (``~``)), then any local\n    project configuration and then any explicitly specified config paths.\n    \"\"\"\n    # 1) AppDir & Home config\n    if not ignore_local_config:\n        user_appdir_config = _load_user_appdir_config()\n        user_config = load_config_at_path(os.path.expanduser(\"~\"))\n    else:\n        user_config, user_appdir_config = {}, {}\n\n    # 3) Local project config\n    parent_config_stack = []\n    config_stack = []\n    if not ignore_local_config:\n        # Finding all paths between here and the home\n        # directory. We could start at the root of the filesystem,\n        # but depending on the user's setup, this might result in\n        # permissions errors.\n        parent_config_paths = list(\n            iter_intermediate_paths(\n                Path(path).absolute(), Path(os.path.expanduser(\"~\"))\n            )\n        )\n        # Stripping off the home directory and the current working\n        # directory, since they are both covered by other code\n        # here\n        parent_config_paths = parent_config_paths[1:-1]\n        parent_config_stack = [\n            load_config_at_path(str(p.resolve())) for p in list(parent_config_paths)\n        ]\n        # Resolve paths to ensure caching is accurate.\n        config_paths = iter_intermediate_paths(Path(path).absolute(), Path.cwd())\n        config_stack = [load_config_at_path(str(p.resolve())) for p in config_paths]\n\n    # 4) Extra config paths.\n    # When calling `load_config_file_as_dict` we resolve the path first so that caching\n    # is more efficient.\n    extra_config = {}\n    if extra_config_path:\n        try:\n            extra_config = load_config_file_as_dict(\n                str(Path(extra_config_path).resolve())\n            )\n        except FileNotFoundError:\n            raise SQLFluffUserError(\n                f\"Extra config path '{extra_config_path}' does not exist.\"\n            )\n\n    return nested_combine(\n        user_appdir_config,\n        user_config,\n        *parent_config_stack,\n        *config_stack,\n        extra_config,\n    )\n", "type": "function"}, {"name": "load_config_file", "is_method": false, "class_name": null, "parameters": ["file_dir", "file_name", "configs"], "calls": ["os.path.join", "load_config_file_as_dict", "nested_combine"], "code_location": {"file": "loader.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 92, "end_line": 118}, "code_snippet": "def load_config_file(\n    file_dir: str, file_name: str, configs: Optional[ConfigMappingType] = None\n) -> ConfigMappingType:\n    \"\"\"Load a config file from the filesystem.\n\n    Args:\n        file_dir (str): The path to the location of file to be loaded.\n            This should be a reference to the directory *only* and not\n            include the filename itself. Any paths in the loaded file\n            are resolved relative to this location.\n        file_name (str): The filename of the file to be loaded. If the\n            filename is ``pyproject.toml`` then the file is loaded in\n            ``toml`` format, but otherwise is assumed to be in ``ini``\n            format (as per ``.sqlfluff``).\n        configs (ConfigMappingType, optional): A base set of configs to\n            merge the loaded configs onto. If not provided, the result\n            will contain only the values loaded from the string.\n\n    Returns:\n        :obj:`ConfigMappingType`: A nested dictionary of config values.\n    \"\"\"\n    file_path = os.path.join(file_dir, file_name)\n    raw_config = load_config_file_as_dict(file_path)\n    # We always run `nested_combine()` because it has the side effect\n    # of making a copy of the objects provided. This prevents us\n    # from editing items which also sit within the cache.\n    return nested_combine(configs or {}, raw_config)\n", "type": "function"}, {"name": "test__config__load_nested", "is_method": false, "class_name": null, "parameters": [], "calls": ["load_config_up_to_path", "os.path.join", "os.path.join"], "code_location": {"file": "loader_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/config", "start_line": 71, "end_line": 100}, "code_snippet": "def test__config__load_nested():\n    \"\"\"Test nested overwrite and order of precedence of config files.\"\"\"\n    cfg = load_config_up_to_path(\n        os.path.join(\n            \"test\", \"fixtures\", \"config\", \"inheritance_a\", \"nested\", \"blah.sql\"\n        ),\n        extra_config_path=os.path.join(\n            \"test\",\n            \"fixtures\",\n            \"config\",\n            \"inheritance_a\",\n            \"extra\",\n            \"this_can_have_any_name.cfg\",\n        ),\n    )\n    assert cfg == {\n        \"core\": {\n            # Outer .sqlfluff defines dialect & testing_val and not overridden.\n            \"dialect\": \"mysql\",\n            \"testing_val\": \"foobar\",\n            # tesing_int is defined in many. Inner pyproject.toml takes precedence.\n            \"testing_int\": 1,\n            # testing_bar is defined only in setup.cfg\n            \"testing_bar\": 7.698,\n        },\n        # bar is defined in a few, but the extra_config takes precedence.\n        \"bar\": {\"foo\": \"foobarextra\"},\n        # fnarr is defined in a few. Inner tox.ini takes precedence.\n        \"fnarr\": {\"fnarr\": {\"foo\": \"foobar\"}},\n    }\n", "type": "function"}, {"name": "FluffConfig", "docstring": "The persistent object for internal methods to access configuration.\n\nThis class is designed to be instantiated once for each file and then be\nreused by each part of the process. For multiple files in the same path, a\nparent object will be created for the each path and then variants of it\nare created *for each file*. The object itself contains the references\nto any long lived objects which might be used by multiple parts of the\ncodebase such as the dialect and the templater (both of which can be\nresource intensive to load & instantiate), which allows (for example),\nmultiple files to reuse the same instance of the relevant dialect.\n\nIt is also designed to pickle well for use in parallel operations.\n\nArgs:\n    configs (ConfigMappingType, optional): A nested dict of config\n        values from which to construct the config.\n    extra_config_path (str, optional): An optional additional path\n        to load config files from. These are loaded last if found\n        and take precedence over any pre-existing config values.\n        Note that when provided directly to the class, this path\n        is not loaded for the class in question (it's assumed that\n        has already been done, and the results are incorporated in\n        the `configs` argument), but it *is* passed onward to child\n        config instances, which will use it.\n    ignore_local_config (bool, optional, defaults to False): If set to\n        True, this skips loading configuration from the user home\n        directory (``~``) or ``appdir`` path.\n    overrides (ConfigMappingType, optional): A additional set of\n        configs to merge into the ``core`` section of the config\n        object at the end. These values take precedence over all\n        other provided values and are inherited by child configs.\n        For example, override values provided in the CLI use this\n        method to apply to all files in a linting operation. Note\n        that this mapping dict *only* applies to the ``core``\n        section and so cannot be used for all values.\n    plugin_manager (PluginManager, optional): Optional pre-loaded\n        config manager. Generally users should not need to provide\n        this, as the class will fetch it's own if not provided.\n        This argument is used when creating new class instances to\n        avoid reloading the manager.\n\n.. note::\n   Methods for accessing internal properties on the config are not particularly\n   standardised as the project currently assumes that few other tools are using\n   this interface directly. If you or your project would like more formally\n   supported methods for access to the config object, raise an issue on GitHub\n   with the kind of things you'd like to achieve.", "methods": ["__init__", "_handle_comma_separated_values", "_initialise_dialect", "verify_dialect_specified", "__getstate__", "__setstate__", "copy", "from_root", "from_string", "from_strings", "from_path", "from_kwargs", "get_templater_class", "get_templater", "make_child_from_path", "diff_to", "get", "get_section", "set_value", "iter_vals", "process_inline_config", "process_raw_file_for_config"], "attributes": ["private_vals"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 37, "end_line": 732}, "type": "class"}, {"name": "load_ini_string", "is_method": false, "class_name": null, "parameters": ["cfg_content"], "calls": ["configparser.ConfigParser", "config.read_string", "config.sections", "records_to_nested_dict", "config.items", "k.startswith", "coerce_value", "config_buffer.append", "tuple", "split", "len"], "code_location": {"file": "ini.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 35, "end_line": 86}, "code_snippet": "def load_ini_string(cfg_content: str) -> ConfigMappingType:\n    \"\"\"Read an ini-style config string.\n\n    This would include loading a `.sqlfluff` file.\n\n    Notes:\n    - We rename the root `sqlfluff` section, to `core` so that it's in\n      line with other config files.\n    - The `configparser` reads everything as strings, but this method will\n      attempt to find better types for values based on their content.\n    - Path resolution isn't done here, that all happens later.\n    - Unlike most cfg file readers, SQLFluff is case-sensitive in how\n      it reads config files. This is to ensure we support the case\n      sensitivity of jinja.\n    \"\"\"\n    # If the string is empty, no need to parse it.\n    if not cfg_content:\n        return {}\n\n    # Disable interpolation so we can load macros\n    config = configparser.ConfigParser(delimiters=\"=\", interpolation=None)\n    # NB: We want to be case sensitive in how we read from files,\n    # because jinja is also case sensitive. To do this we override\n    # the optionxform attribute.\n    config.optionxform = lambda option: option  # type: ignore\n\n    # Read the content.\n    config.read_string(cfg_content)\n\n    # Build up a buffer of config values.\n    config_buffer: list[NestedDictRecord[ConfigValueType]] = []\n    for k in config.sections():\n        if k == \"sqlfluff\":\n            key: tuple[str, ...] = (\"core\",)\n        elif k.startswith(\"sqlfluff:\"):\n            # Return a tuple of nested values\n            key = tuple(k[len(\"sqlfluff:\") :].split(\":\"))\n        else:  # pragma: no cover\n            # if it doesn't start with sqlfluff, then ignore this\n            # section. It's not relevant to sqlfluff.\n            continue\n\n        for name, val in config.items(section=k):\n            # Try to coerce it to a more specific type,\n            # otherwise just make it a string.\n            v = coerce_value(val)\n\n            # Add the name to the end of the key\n            config_buffer.append((key + (name,), v))\n\n    # Compress that buffer into a dictionary.\n    return records_to_nested_dict(config_buffer)\n", "type": "function"}, {"name": "load_config_string", "is_method": false, "class_name": null, "parameters": ["config_string", "configs", "working_path"], "calls": ["load_config_string_as_dict", "nested_combine", "os.getcwd"], "code_location": {"file": "loader.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 157, "end_line": 185}, "code_snippet": "def load_config_string(\n    config_string: str,\n    configs: Optional[ConfigMappingType] = None,\n    working_path: Optional[str] = None,\n) -> ConfigMappingType:\n    \"\"\"Load a config from a string in ini format.\n\n    Args:\n        config_string (str): The raw config file as a string. The content\n            is assumed to be in the the ``.ini`` format of a ``.sqlfluff``\n            file (i.e. not in ``.toml`` format).\n        configs (ConfigMappingType, optional): A base set of configs to\n            merge the loaded configs onto. If not provided, the result\n            will contain only the values loaded from the string.\n        working_path (str, optional): The working path to use for the\n            resolution of any paths specified in the config. If not provided\n            then ``os.getcwd()`` is used as a default.\n\n    Returns:\n        :obj:`ConfigMappingType`: A nested dictionary of config values.\n    \"\"\"\n    filepath = working_path or os.getcwd()\n    raw_config = load_config_string_as_dict(\n        config_string, filepath, logging_reference=\"<config string>\"\n    )\n    # We always run `nested_combine()` because it has the side effect\n    # of making a copy of the objects provided. This prevents us\n    # from editing items which also sit within the cache.\n    return nested_combine(configs or {}, raw_config)\n", "type": "function"}, {"name": "from_root", "is_method": true, "class_name": "FluffConfig", "parameters": ["cls", "extra_config_path", "ignore_local_config", "overrides", "require_dialect"], "calls": ["load_config_up_to_path", "cls"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 238, "end_line": 277}, "code_snippet": "    def from_root(\n        cls,\n        extra_config_path: Optional[str] = None,\n        ignore_local_config: bool = False,\n        overrides: Optional[ConfigMappingType] = None,\n        require_dialect: bool = True,\n    ) -> FluffConfig:\n        \"\"\"Loads a config object based on the root directory.\n\n        Args:\n            extra_config_path (str, optional): An optional additional path\n                to load config files from. These are loaded last if found\n                and take precedence over any pre-existing config values.\n            ignore_local_config (bool, optional, defaults to False): If set to\n                True, this skips loading configuration from the user home\n                directory (``~``) or ``appdir`` path.\n            overrides (ConfigMappingType, optional): A additional set of\n                configs to merge into the config object at the end. These\n                values take precedence over all other provided values and\n                are inherited by child configs. For example, override values\n                provided in the CLI use this method to apply to all files\n                in a linting operation.\n            require_dialect (bool, optional, default is True): When True\n                an error will be raise if the dialect config value is unset.\n\n        Returns:\n            :obj:`FluffConfig`: The loaded config object.\n        \"\"\"\n        configs = load_config_up_to_path(\n            path=\".\",\n            extra_config_path=extra_config_path,\n            ignore_local_config=ignore_local_config,\n        )\n        return cls(\n            configs=configs,\n            extra_config_path=extra_config_path,\n            ignore_local_config=ignore_local_config,\n            overrides=overrides,\n            require_dialect=require_dialect,\n        )\n", "type": "function"}, {"name": "load_config_at_path", "is_method": false, "class_name": null, "parameters": ["path"], "calls": ["os.path.isdir", "os.listdir", "os.path.dirname", "os.path.expanduser", "load_config_file"], "code_location": {"file": "loader.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 189, "end_line": 232}, "code_snippet": "def load_config_at_path(path: str) -> ConfigMappingType:\n    \"\"\"Load config files at a given path.\n\n    Args:\n        path (str): The directory to search for config files.\n\n    Returns:\n        :obj:`ConfigMappingType`: A nested dictionary of config values.\n\n    This function will search for all valid config files at the given\n    path, load any found and combine them into a config mapping. If\n    multiple valid files are found, they are resolved in priority order,\n    where ``pyproject.toml`` is given the highest precedence, followed\n    by ``.sqlfluff``, ``pep8.ini``, ``tox.ini`` and finally ``setup.cfg``.\n\n    By accepting only a path string, we enable efficient caching of\n    results, such that configuration can be reused between files without\n    reloading the information from disk.\n    \"\"\"\n    # The potential filenames we would look for at this path.\n    # NB: later in this list overwrites earlier\n    filename_options = [\n        \"setup.cfg\",\n        \"tox.ini\",\n        \"pep8.ini\",\n        \".sqlfluff\",\n        \"pyproject.toml\",\n    ]\n\n    configs: ConfigMappingType = {}\n\n    if os.path.isdir(path):\n        p = path\n    else:\n        p = os.path.dirname(path)\n\n    d = os.listdir(os.path.expanduser(p))\n    # iterate this way round to make sure things overwrite is the right direction.\n    # NOTE: The `configs` variable is passed back in at each stage.\n    for fname in filename_options:\n        if fname in d:\n            configs = load_config_file(p, fname, configs=configs)\n\n    return configs\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1779205799102783}
{"question": "Why does SQLFluff's incremental parsing improve performance for large SQL files?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's incremental parsing improves performance for large SQL files by avoiding redundant parsing work and optimizing memory usage. Key benefits include: 1) Caching mechanisms - Parsed segments and grammar matches are cached to avoid re-parsing identical structures; 2) Selective re-parsing - Only modified sections of SQL files are re-parsed when changes are detected; 3) Memory efficiency - Incremental parsing reduces memory usage by reusing previously parsed structures; 4) Grammar optimization - Frequently used grammar patterns are optimized and cached for faster matching; 5) Segment reuse - Previously parsed segments can be reused when they haven't changed; 6) Parse tree optimization - The parse tree structure is optimized to minimize traversal overhead; 7) Context preservation - Parsing context is preserved between incremental updates to avoid redundant work; 8) Batch processing - Multiple small changes can be batched together for more efficient processing; 9) Dependency tracking - Only dependent sections are re-parsed when upstream changes occur; 10) Performance scaling - Incremental parsing scales better with file size compared to full re-parsing approaches.", "score": null, "retrieved_content": [{"name": "_parse_tokens", "is_method": true, "class_name": "Linter", "parameters": ["tokens", "config", "fname", "parse_statistics"], "calls": ["Parser", "linter_logger.info", "linter_logger.info", "parsed.iter_unparsables", "parser.parse", "format", "violations.append", "linter_logger.info", "linter_logger.info", "tuple", "linter_logger.info", "violations.append", "parsed.stringify", "SQLParseError", "unparsable.stringify", "format", "len"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 231, "end_line": 281}, "code_snippet": "    def _parse_tokens(\n        tokens: Sequence[BaseSegment],\n        config: FluffConfig,\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> tuple[Optional[BaseSegment], list[SQLParseError]]:\n        parser = Parser(config=config)\n        violations = []\n        # Parse the file and log any problems\n        try:\n            parsed: Optional[BaseSegment] = parser.parse(\n                # Regardless of how the sequence was passed in, we should\n                # coerce it to a tuple here, before we head deeper into\n                # the parsing process.\n                tuple(tokens),\n                fname=fname,\n                parse_statistics=parse_statistics,\n            )\n        except SQLParseError as err:\n            linter_logger.info(\"PARSING FAILED! : %s\", err)\n            violations.append(err)\n            return None, violations\n\n        if parsed is None:  # pragma: no cover\n            return None, violations\n\n        linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n        linter_logger.info(\"\\n\" + parsed.stringify())\n        # We may succeed parsing, but still have unparsable segments. Extract them\n        # here.\n        for unparsable in parsed.iter_unparsables():\n            # No exception has been raised explicitly, but we still create one here\n            # so that we can use the common interface\n            assert unparsable.pos_marker\n            violations.append(\n                SQLParseError(\n                    \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n                    \"{1!r}\".format(\n                        unparsable.pos_marker.working_loc,\n                        (\n                            unparsable.raw\n                            if len(unparsable.raw) < 40\n                            else unparsable.raw[:40] + \"...\"\n                        ),\n                    ),\n                    segment=unparsable,\n                )\n            )\n            linter_logger.info(\"Found unparsable segment...\")\n            linter_logger.info(unparsable.stringify())\n        return parsed, violations\n", "type": "function"}, {"name": "parse_path", "is_method": true, "class_name": "Linter", "parameters": ["self", "path", "parse_statistics"], "calls": ["split", "paths_from_path", "lower", "self.formatter.dispatch_path", "self.load_raw_file_and_config", "self.parse_string", "linter_logger.warning", "self.config.get", "str"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 1130, "end_line": 1161}, "code_snippet": "    def parse_path(\n        self,\n        path: str,\n        parse_statistics: bool = False,\n    ) -> Iterator[ParsedString]:\n        \"\"\"Parse a path of sql files.\n\n        NB: This a generator which will yield the result of each file\n        within the path iteratively.\n        \"\"\"\n        sql_exts = self.config.get(\"sql_file_exts\", default=\".sql\").lower().split(\",\")\n        for fname in paths_from_path(\n            path,\n            target_file_exts=sql_exts,\n        ):\n            if self.formatter:\n                self.formatter.dispatch_path(path)\n            # Load the file with the config and yield the result.\n            try:\n                raw_file, config, encoding = self.load_raw_file_and_config(\n                    fname, self.config\n                )\n            except SQLFluffSkipFile as s:\n                linter_logger.warning(str(s))\n                continue\n            yield self.parse_string(\n                raw_file,\n                fname=fname,\n                config=config,\n                encoding=encoding,\n                parse_statistics=parse_statistics,\n            )\n", "type": "function"}, {"name": "parse", "is_method": false, "class_name": null, "parameters": ["path", "code_only", "include_meta", "format", "write_output", "bench", "nofail", "logger", "extra_config_path", "ignore_local_config", "parse_statistics", "stdin_filename"], "calls": ["cli.command", "click.argument", "click.option", "click.option", "click.option", "click.option", "click.option", "click.option", "get_config", "make_output_stream", "get_linter_and_formatter", "c.get", "formatter.dispatch_config", "set_logging_level", "time.monotonic", "PathAndUserErrorHandler", "time.monotonic", "formatter.print_out_violations_and_timing", "dump_file_payload", "sys.exit", "sys.exit", "click.Path", "click.Choice", "list", "parsed_string.root_variant", "len", "parsed_strings_dict.append", "yaml.add_representer", "yaml.dump", "file_config.make_child_from_path", "lnt.parse_string", "lnt.parse_path", "root_variant.tree.as_record", "json.dumps", "sys.stdin.read"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 1323, "end_line": 1442}, "code_snippet": "def parse(\n    path: str,\n    code_only: bool,\n    include_meta: bool,\n    format: str,\n    write_output: Optional[str],\n    bench: bool,\n    nofail: bool,\n    logger: Optional[logging.Logger] = None,\n    extra_config_path: Optional[str] = None,\n    ignore_local_config: bool = False,\n    parse_statistics: bool = False,\n    stdin_filename: Optional[str] = None,\n    **kwargs,\n) -> None:\n    \"\"\"Parse SQL files and just spit out the result.\n\n    PATH is the path to a sql file or directory to lint. This can be either a\n    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n    be interpreted like passing the current working directory as a path argument.\n    \"\"\"\n    c = get_config(\n        extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n    )\n    # We don't want anything else to be logged if we want json or yaml output\n    # unless we're writing to a file.\n    non_human_output = (format != FormatType.human.value) or (write_output is not None)\n    output_stream = make_output_stream(c, format, write_output)\n    lnt, formatter = get_linter_and_formatter(c, output_stream)\n    verbose = c.get(\"verbose\")\n\n    progress_bar_configuration.disable_progress_bar = True\n\n    formatter.dispatch_config(lnt)\n\n    # Set up logging.\n    set_logging_level(\n        verbosity=verbose,\n        formatter=formatter,\n        logger=logger,\n        stderr_output=non_human_output,\n    )\n\n    t0 = time.monotonic()\n\n    # handle stdin if specified via lone '-'\n    with PathAndUserErrorHandler(formatter):\n        if \"-\" == path:\n            file_config = lnt.config\n            if stdin_filename:\n                file_config = file_config.make_child_from_path(\n                    stdin_filename, require_dialect=False\n                )\n            parsed_strings = [\n                lnt.parse_string(\n                    sys.stdin.read(),\n                    \"stdin\",\n                    config=file_config,\n                    parse_statistics=parse_statistics,\n                ),\n            ]\n        else:\n            # A single path must be specified for this command\n            parsed_strings = list(\n                lnt.parse_path(\n                    path=path,\n                    parse_statistics=parse_statistics,\n                )\n            )\n\n    total_time = time.monotonic() - t0\n    violations_count = 0\n\n    # iterative print for human readout\n    if format == FormatType.human.value:\n        violations_count = formatter.print_out_violations_and_timing(\n            output_stream, bench, code_only, total_time, verbose, parsed_strings\n        )\n    else:\n        parsed_strings_dict = []\n        for parsed_string in parsed_strings:\n            # TODO: Multiple variants aren't yet supported here in the non-human\n            # output of the parse command.\n            root_variant = parsed_string.root_variant()\n            # Updating violation count ensures the correct return code below.\n            violations_count += len(parsed_string.violations)\n            if root_variant:\n                assert root_variant.tree\n                segments = root_variant.tree.as_record(\n                    code_only=code_only, show_raw=True, include_meta=include_meta\n                )\n            else:\n                # Parsing failed - return null for segments.\n                segments = None\n            parsed_strings_dict.append(\n                {\"filepath\": parsed_string.fname, \"segments\": segments}\n            )\n\n        if format == FormatType.yaml.value:\n            # For yaml dumping always dump double quoted strings if they contain\n            # tabs or newlines.\n            yaml.add_representer(str, quoted_presenter)\n            file_output = yaml.dump(\n                parsed_strings_dict,\n                sort_keys=False,\n                allow_unicode=True,\n            )\n        elif format == FormatType.json.value:\n            file_output = json.dumps(parsed_strings_dict)\n        elif format == FormatType.none.value:\n            file_output = \"\"\n\n        # Dump the output to stdout or to file as appropriate.\n        dump_file_payload(write_output, file_output)\n\n    if violations_count > 0 and not nofail:\n        sys.exit(EXIT_FAIL)  # pragma: no cover\n    else:\n        sys.exit(EXIT_SUCCESS)\n", "type": "function"}, {"name": "test__dialect__base_broad_fix", "is_method": false, "class_name": null, "parameters": ["dialect", "file", "raise_critical_errors_after_fix", "caplog"], "calls": ["pytest.mark.parametrize", "load_file", "dict", "lex_and_parse", "print", "FluffConfig", "Linter", "linter.get_rulepack", "linter.lint_parsed", "parsed.tree.stringify"], "code_location": {"file": "dialects_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 85, "end_line": 120}, "code_snippet": "def test__dialect__base_broad_fix(\n    dialect, file, raise_critical_errors_after_fix, caplog\n):\n    \"\"\"Run a full fix with all rules, in search of critical errors.\n\n    NOTE: This suite does all of the same things as the above test\n    suite (the `parse_suite`), but also runs fix. In CI, we run\n    the above tests _with_ coverage tracking, but these we run\n    _without_.\n\n    The purpose of this test is as a more stretching run through\n    a wide range of test sql examples, and the full range of rules\n    to find any potential critical errors raised by any interactions\n    between different dialects and rules.\n\n    We also do not use DEBUG logging here because it gets _very_\n    noisy.\n    \"\"\"\n    raw = load_file(dialect, file)\n    config_overrides = dict(dialect=dialect)\n\n    parsed: Optional[ParsedString] = lex_and_parse(config_overrides, raw)\n    if not parsed:  # Empty file case\n        return\n    print(parsed.tree.stringify())\n\n    config = FluffConfig(overrides=config_overrides)\n    linter = Linter(config=config)\n    rule_pack = linter.get_rulepack()\n    # Due to \"raise_critical_errors_after_fix\" fixture \"fix\",\n    # will now throw.\n    linter.lint_parsed(\n        parsed,\n        rule_pack,\n        fix=True,\n    )\n", "type": "function"}, {"name": "ParsedVariant", "docstring": "An object to store the result of parsing a single TemplatedFile.\n\nArgs:\n    templated_file (:obj:`TemplatedFile`): Containing the details\n        of the templated file. If templating fails, this will be `None`.\n    tree (:obj:`BaseSegment`): The segment structure representing the\n        parsed file. If parsing fails due to an unrecoverable\n        violation then we will be None.\n    lexing_violations (:obj:`list` of :obj:`SQLLexError`): Any violations\n        raised during the lexing phase.\n    parsing_violations (:obj:`list` of :obj:`SQLParseError`): Any violations\n        raised during the lexing phase.", "methods": ["violations"], "attributes": [], "code_location": {"file": "common.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 42, "end_line": 64}, "type": "class"}, {"name": "test__linter__skip_large_bytes", "is_method": false, "class_name": null, "parameters": ["filesize", "raises_skip"], "calls": ["pytest.mark.parametrize", "FluffConfig", "Linter", "lntr.lint_paths", "list", "result.get_violations", "lntr.parse_path", "pytest.raises", "Linter.load_raw_file_and_config", "str", "str", "result.get_violations"], "code_location": {"file": "linter_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 44, "end_line": 78}, "code_snippet": "def test__linter__skip_large_bytes(filesize, raises_skip):\n    \"\"\"Test extracting paths from a file path.\"\"\"\n    config = FluffConfig(\n        overrides={\"large_file_skip_byte_limit\": filesize, \"dialect\": \"ansi\"}\n    )\n    # First check the function directly\n    if raises_skip:\n        with pytest.raises(SQLFluffSkipFile) as excinfo:\n            Linter.load_raw_file_and_config(\n                \"test/fixtures/linter/indentation_errors.sql\", config\n            )\n        assert \"Skipping\" in str(excinfo.value)\n        assert f\"over the limit of {filesize}\" in str(excinfo.value)\n    # If NOT raises, then we'll catch the raise an error and the test will fail.\n\n    # Then check that it either is or isn't linted appropriately via lint_paths.\n    lntr = Linter(config)\n    result = lntr.lint_paths(\n        (\"test/fixtures/linter/indentation_errors.sql\",),\n    )\n    if raises_skip:\n        assert not result.get_violations()\n    else:\n        assert result.get_violations()\n\n    # Same again via parse_path, which is the other entry point.\n    result = list(\n        lntr.parse_path(\n            \"test/fixtures/linter/indentation_errors.sql\",\n        )\n    )\n    if raises_skip:\n        assert not result\n    else:\n        assert result\n", "type": "function"}, {"name": "cli", "is_method": false, "class_name": null, "parameters": [], "calls": ["click.group", "click.version_option"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 480, "end_line": 481}, "code_snippet": "def cli() -> None:\n    \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n", "type": "function"}, {"name": "print_out_violations_and_timing", "is_method": true, "class_name": "OutputStreamFormatter", "parameters": ["self", "output_stream", "bench", "code_only", "total_time", "verbose", "parsed_strings"], "calls": ["TimingSummary", "timing.add", "len", "parsed_string.root_variant", "len", "output_stream.write", "output_stream.write", "timing.summary", "output_stream.write", "output_stream.write", "output_stream.write", "output_stream.write", "output_stream.write", "output_stream.write", "self.cli_table", "output_stream.write", "output_stream.write", "self.colorize", "output_stream.write", "output_stream.write", "enumerate", "self.format_violation", "self.format_dialect_warning", "self.cli_table", "self.cli_table", "root_variant.tree.stringify", "self.colorize", "output_stream.write", "parsed_string.config.get", "parsed_string.time_dict.items", "items", "self.colorize", "output_stream.write", "output_stream.write", "variant.tree.stringify", "self.colorize"], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 638, "end_line": 710}, "code_snippet": "    def print_out_violations_and_timing(\n        self,\n        output_stream: OutputStream,\n        bench: bool,\n        code_only: bool,\n        total_time: float,\n        verbose: int,\n        parsed_strings: list[ParsedString],\n    ) -> int:\n        \"\"\"Used by human formatting during the `sqlfluff parse` command.\"\"\"\n        violations_count = 0\n        timing = TimingSummary()\n\n        for parsed_string in parsed_strings:\n            timing.add(parsed_string.time_dict)\n\n            num_variants = len(parsed_string.parsed_variants)\n            root_variant = parsed_string.root_variant()\n            if not root_variant:\n                # TODO: Make this prettier\n                output_stream.write(\n                    self.colorize(\"...Failed to Parse...\", Color.red)\n                )  # pragma: no cover\n            elif num_variants == 1:\n                # Backward compatible single parse\n                assert root_variant.tree\n                output_stream.write(root_variant.tree.stringify(code_only=code_only))\n            else:\n                # Multi variant parse setup.\n                output_stream.write(\n                    self.colorize(\n                        f\"SQLFluff parsed {num_variants} variants of this file\",\n                        Color.blue,\n                    )\n                )\n                for idx, variant in enumerate(parsed_string.parsed_variants):\n                    output_stream.write(\n                        self.colorize(\n                            f\"Variant {idx + 1}:\",\n                            Color.blue,\n                        )\n                    )\n                    if variant.tree:\n                        output_stream.write(variant.tree.stringify(code_only=code_only))\n                    else:  # pragma: no cover\n                        output_stream.write(\n                            self.colorize(\"...Failed to Parse...\", Color.red)\n                        )\n\n            violations = parsed_string.violations\n            violations_count += len(violations)\n            if violations:\n                output_stream.write(\"==== parsing violations ====\")  # pragma: no cover\n            for v in violations:\n                output_stream.write(self.format_violation(v))  # pragma: no cover\n            if violations:\n                output_stream.write(\n                    self.format_dialect_warning(parsed_string.config.get(\"dialect\"))\n                )\n\n            if verbose >= 2:\n                output_stream.write(\"==== timings ====\")\n                output_stream.write(self.cli_table(parsed_string.time_dict.items()))\n\n        if verbose >= 2 or bench:\n            output_stream.write(\"==== overall timings ====\")\n            output_stream.write(self.cli_table([(\"Clock time\", total_time)]))\n            timing_summary = timing.summary()\n            for step in timing_summary:\n                output_stream.write(f\"=== {step} ===\")\n                output_stream.write(self.cli_table(timing_summary[step].items()))\n\n        return violations_count\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "SQLFluffDriver", "parameters": ["self"], "calls": ["__init__", "super", "s.encode", "sys.getfilesystemencoding"], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 25, "end_line": 34}, "code_snippet": "    def __init__(self) -> None:\n        super().__init__(\n            [sys.executable, \"-m\", \"sqlfluff.cli.commands\"],\n            [\".sql\"],\n            [\n                s.encode(sys.getfilesystemencoding())\n                for s in [\"sqlfluff\", \"lint\", \"--format=json\"]\n            ],\n            exit_codes=[0, 1],\n        )\n", "type": "function"}, {"name": "ParseExample", "docstring": "A tuple representing an example SQL file to parse.", "methods": [], "attributes": [], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test", "start_line": 34, "end_line": 38}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.2161834239959717}
{"question": "Where in the SQLFluff codebase is the core SQL parser implemented?", "answer": null, "relative_code_list": null, "ground_truth": "The core SQL parser in SQLFluff is implemented across several key modules in the codebase. The main components are located in: 1) src/sqlfluff/core/parser/parser.py - Contains the main Parser class that orchestrates the parsing process; 2) src/sqlfluff/core/parser/lexer.py - Implements the Lexer class for tokenizing SQL input; 3) src/sqlfluff/core/parser/segments/ - Contains segment classes including BaseSegment, RawSegment, and specialized segment types; 4) src/sqlfluff/core/parser/grammar.py - Implements grammar classes (Sequence, OneOf, Delimited, etc.) for defining SQL structure; 5) src/sqlfluff/core/parser/parsers.py - Contains parser classes for individual segment types; 6) src/sqlfluff/dialects/ - Contains dialect-specific parser implementations (dialect_ansi.py, dialect_postgres.py, etc.); 7) src/sqlfluff/core/parser/context.py - Manages parsing context and state; 8) src/sqlfluff/core/parser/match_result.py - Handles parsing match results and tree construction; 9) src/sqlfluff/core/parser/markers.py - Implements position tracking for segments; 10) src/sqlfluff/core/parser/helpers.py - Contains utility functions for parsing operations.", "score": null, "retrieved_content": [{"name": "SQLParseError", "docstring": "An error which occurred during parsing.\n\nArgs:\n    segment (:obj:`BaseSegment`, optional): The segment which is relevant\n        for the failure in parsing. This is likely to be a subclass of\n        `BaseSegment` rather than the parent class itself. This is mostly\n        used for logging and for referencing position.", "methods": ["__init__", "__reduce__", "to_dict"], "attributes": ["_code", "_identifier"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 185, "end_line": 246}, "type": "class"}, {"name": "parse", "is_method": false, "class_name": null, "parameters": ["sql", "dialect", "config", "config_path"], "calls": ["Linter", "linter.parse_string", "parsed.root_variant", "root_variant.tree.as_record", "get_simple_config", "APIParsingError"], "code_location": {"file": "simple.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/api", "start_line": 168, "end_line": 212}, "code_snippet": "def parse(\n    sql: str,\n    dialect: Optional[str] = None,\n    config: Optional[FluffConfig] = None,\n    config_path: Optional[str] = None,\n) -> dict[str, Any]:\n    \"\"\"Parse a SQL string.\n\n    Args:\n        sql (:obj:`str`): The SQL to be parsed.\n        dialect (:obj:`Optional[str]`, optional): A reference to the dialect of the SQL\n            to be parsed. Defaults to `ansi`.\n        config (:obj:`Optional[FluffConfig]`, optional): A configuration object\n            to use for the operation. Defaults to None.\n        config_path (:obj:`Optional[str]`, optional): A path to a .sqlfluff config,\n            which is only used if a `config` is not already provided.\n            Defaults to None.\n\n    Returns:\n        :obj:`Dict[str, Any]` JSON containing the parsed structure.\n\n    Note:\n        In the case of multiple potential variants from the raw source file\n        only the first variant is returned by the simple API. For access to\n        the other variants, use the underlying main API directly.\n    \"\"\"\n    cfg = config or get_simple_config(\n        dialect=dialect,\n        config_path=config_path,\n    )\n    linter = Linter(config=cfg)\n\n    parsed = linter.parse_string(sql)\n    # If we encounter any parsing errors, raise them in a combined issue.\n    violations = parsed.violations\n    if violations:\n        raise APIParsingError(violations)\n    # Return a JSON representation of the parse tree.\n    # NOTE: For the simple API - only a single variant is returned.\n    root_variant = parsed.root_variant()\n    assert root_variant, \"Files parsed without violations must have a valid variant\"\n    assert root_variant.tree, \"Files parsed without violations must have a valid tree\"\n    record = root_variant.tree.as_record(show_raw=True)\n    assert record\n    return record\n", "type": "function"}, {"name": "_parse_tokens", "is_method": true, "class_name": "Linter", "parameters": ["tokens", "config", "fname", "parse_statistics"], "calls": ["Parser", "linter_logger.info", "linter_logger.info", "parsed.iter_unparsables", "parser.parse", "format", "violations.append", "linter_logger.info", "linter_logger.info", "tuple", "linter_logger.info", "violations.append", "parsed.stringify", "SQLParseError", "unparsable.stringify", "format", "len"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 231, "end_line": 281}, "code_snippet": "    def _parse_tokens(\n        tokens: Sequence[BaseSegment],\n        config: FluffConfig,\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> tuple[Optional[BaseSegment], list[SQLParseError]]:\n        parser = Parser(config=config)\n        violations = []\n        # Parse the file and log any problems\n        try:\n            parsed: Optional[BaseSegment] = parser.parse(\n                # Regardless of how the sequence was passed in, we should\n                # coerce it to a tuple here, before we head deeper into\n                # the parsing process.\n                tuple(tokens),\n                fname=fname,\n                parse_statistics=parse_statistics,\n            )\n        except SQLParseError as err:\n            linter_logger.info(\"PARSING FAILED! : %s\", err)\n            violations.append(err)\n            return None, violations\n\n        if parsed is None:  # pragma: no cover\n            return None, violations\n\n        linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n        linter_logger.info(\"\\n\" + parsed.stringify())\n        # We may succeed parsing, but still have unparsable segments. Extract them\n        # here.\n        for unparsable in parsed.iter_unparsables():\n            # No exception has been raised explicitly, but we still create one here\n            # so that we can use the common interface\n            assert unparsable.pos_marker\n            violations.append(\n                SQLParseError(\n                    \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n                    \"{1!r}\".format(\n                        unparsable.pos_marker.working_loc,\n                        (\n                            unparsable.raw\n                            if len(unparsable.raw) < 40\n                            else unparsable.raw[:40] + \"...\"\n                        ),\n                    ),\n                    segment=unparsable,\n                )\n            )\n            linter_logger.info(\"Found unparsable segment...\")\n            linter_logger.info(unparsable.stringify())\n        return parsed, violations\n", "type": "function"}, {"name": "SQLFluffDomain", "docstring": "SQLFluff domain.", "methods": ["get_full_qualified_name", "get_objects", "resolve_xref", "add_rule"], "attributes": ["name", "label", "object_types", "roles", "directives", "initial_data"], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 82, "end_line": 142}, "type": "class"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}, {"name": "cli", "is_method": false, "class_name": null, "parameters": [], "calls": ["click.group", "click.version_option"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 480, "end_line": 481}, "code_snippet": "def cli() -> None:\n    \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n", "type": "function"}, {"name": "test__parser__parse_error", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "lnt.parse_string", "isinstance", "len", "violation.desc", "parsed.tree.stringify"], "code_location": {"file": "parse_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser", "start_line": 32, "end_line": 51}, "code_snippet": "def test__parser__parse_error():\n    \"\"\"Test that SQLParseError is raised for unparsable section.\"\"\"\n    in_str = \"SELECT ;\"\n    lnt = Linter(dialect=\"ansi\")\n    parsed = lnt.parse_string(in_str)\n\n    assert len(parsed.violations) == 1\n    violation = parsed.violations[0]\n    assert isinstance(violation, SQLParseError)\n    assert violation.desc() == \"Line 1, Position 1: Found unparsable section: 'SELECT'\"\n\n    # Check that the expected labels work for logging.\n    # TODO: This is more specific that in previous iterations, but we could\n    # definitely make this easier to read.\n    assert (\n        'Expected: \"<Delimited: '\n        \"[<Ref: 'SelectClauseElementSegment'>]> \"\n        \"after <WordSegment: ([L:  1, P:  1]) 'SELECT'>. \"\n        \"Found nothing.\"\n    ) in parsed.tree.stringify()\n", "type": "function"}, {"name": "TestFlinkSQLDialect", "docstring": "Test FlinkSQL dialect parsing.", "methods": ["test_flink_dialect_basic", "test_flink_create_table_basic", "test_flink_row_data_type", "test_flink_timestamp_with_precision", "test_flink_watermark_definition", "test_flink_computed_column", "test_flink_metadata_column", "test_flink_show_statements", "test_flink_use_statements", "test_flink_describe_statement", "test_flink_explain_statement", "test_flink_create_catalog", "test_flink_create_database", "test_flink_alternative_with_syntax"], "attributes": [], "code_location": {"file": "flink_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 6, "end_line": 231}, "type": "class"}, {"name": "ParseExample", "docstring": "A tuple representing an example SQL file to parse.", "methods": [], "attributes": [], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test", "start_line": 34, "end_line": 38}, "type": "class"}, {"name": "parse", "is_method": false, "class_name": null, "parameters": ["path", "code_only", "include_meta", "format", "write_output", "bench", "nofail", "logger", "extra_config_path", "ignore_local_config", "parse_statistics", "stdin_filename"], "calls": ["cli.command", "click.argument", "click.option", "click.option", "click.option", "click.option", "click.option", "click.option", "get_config", "make_output_stream", "get_linter_and_formatter", "c.get", "formatter.dispatch_config", "set_logging_level", "time.monotonic", "PathAndUserErrorHandler", "time.monotonic", "formatter.print_out_violations_and_timing", "dump_file_payload", "sys.exit", "sys.exit", "click.Path", "click.Choice", "list", "parsed_string.root_variant", "len", "parsed_strings_dict.append", "yaml.add_representer", "yaml.dump", "file_config.make_child_from_path", "lnt.parse_string", "lnt.parse_path", "root_variant.tree.as_record", "json.dumps", "sys.stdin.read"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 1323, "end_line": 1442}, "code_snippet": "def parse(\n    path: str,\n    code_only: bool,\n    include_meta: bool,\n    format: str,\n    write_output: Optional[str],\n    bench: bool,\n    nofail: bool,\n    logger: Optional[logging.Logger] = None,\n    extra_config_path: Optional[str] = None,\n    ignore_local_config: bool = False,\n    parse_statistics: bool = False,\n    stdin_filename: Optional[str] = None,\n    **kwargs,\n) -> None:\n    \"\"\"Parse SQL files and just spit out the result.\n\n    PATH is the path to a sql file or directory to lint. This can be either a\n    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n    be interpreted like passing the current working directory as a path argument.\n    \"\"\"\n    c = get_config(\n        extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n    )\n    # We don't want anything else to be logged if we want json or yaml output\n    # unless we're writing to a file.\n    non_human_output = (format != FormatType.human.value) or (write_output is not None)\n    output_stream = make_output_stream(c, format, write_output)\n    lnt, formatter = get_linter_and_formatter(c, output_stream)\n    verbose = c.get(\"verbose\")\n\n    progress_bar_configuration.disable_progress_bar = True\n\n    formatter.dispatch_config(lnt)\n\n    # Set up logging.\n    set_logging_level(\n        verbosity=verbose,\n        formatter=formatter,\n        logger=logger,\n        stderr_output=non_human_output,\n    )\n\n    t0 = time.monotonic()\n\n    # handle stdin if specified via lone '-'\n    with PathAndUserErrorHandler(formatter):\n        if \"-\" == path:\n            file_config = lnt.config\n            if stdin_filename:\n                file_config = file_config.make_child_from_path(\n                    stdin_filename, require_dialect=False\n                )\n            parsed_strings = [\n                lnt.parse_string(\n                    sys.stdin.read(),\n                    \"stdin\",\n                    config=file_config,\n                    parse_statistics=parse_statistics,\n                ),\n            ]\n        else:\n            # A single path must be specified for this command\n            parsed_strings = list(\n                lnt.parse_path(\n                    path=path,\n                    parse_statistics=parse_statistics,\n                )\n            )\n\n    total_time = time.monotonic() - t0\n    violations_count = 0\n\n    # iterative print for human readout\n    if format == FormatType.human.value:\n        violations_count = formatter.print_out_violations_and_timing(\n            output_stream, bench, code_only, total_time, verbose, parsed_strings\n        )\n    else:\n        parsed_strings_dict = []\n        for parsed_string in parsed_strings:\n            # TODO: Multiple variants aren't yet supported here in the non-human\n            # output of the parse command.\n            root_variant = parsed_string.root_variant()\n            # Updating violation count ensures the correct return code below.\n            violations_count += len(parsed_string.violations)\n            if root_variant:\n                assert root_variant.tree\n                segments = root_variant.tree.as_record(\n                    code_only=code_only, show_raw=True, include_meta=include_meta\n                )\n            else:\n                # Parsing failed - return null for segments.\n                segments = None\n            parsed_strings_dict.append(\n                {\"filepath\": parsed_string.fname, \"segments\": segments}\n            )\n\n        if format == FormatType.yaml.value:\n            # For yaml dumping always dump double quoted strings if they contain\n            # tabs or newlines.\n            yaml.add_representer(str, quoted_presenter)\n            file_output = yaml.dump(\n                parsed_strings_dict,\n                sort_keys=False,\n                allow_unicode=True,\n            )\n        elif format == FormatType.json.value:\n            file_output = json.dumps(parsed_strings_dict)\n        elif format == FormatType.none.value:\n            file_output = \"\"\n\n        # Dump the output to stdout or to file as appropriate.\n        dump_file_payload(write_output, file_output)\n\n    if violations_count > 0 and not nofail:\n        sys.exit(EXIT_FAIL)  # pragma: no cover\n    else:\n        sys.exit(EXIT_SUCCESS)\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.2037615776062012}
{"question": "Where does SQLFluff store its rule implementations?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff stores its rule implementations across several organized locations in the codebase. The main rule storage locations include: 1) src/sqlfluff/rules/ - The main directory containing all built-in rule implementations organized by category; 2) src/sqlfluff/rules/capitalisation/ - Rules for SQL capitalization and formatting standards; 3) src/sqlfluff/rules/layout/ - Rules for SQL layout, spacing, and indentation; 4) src/sqlfluff/rules/aliasing/ - Rules for table and column alias usage; 5) src/sqlfluff/rules/references/ - Rules for proper table and column reference handling; 6) src/sqlfluff/rules/ambiguous/ - Rules for detecting ambiguous SQL constructs; 7) src/sqlfluff/rules/structure/ - Rules for SQL structure and organization; 8) src/sqlfluff/rules/convention/ - Rules for SQL naming conventions and best practices; 9) src/sqlfluff/rules/jinja/ - Rules specific to Jinja templating; 10) src/sqlfluff/rules/tsql/ - Rules specific to T-SQL dialect; 11) src/sqlfluff/core/rules/base.py - Contains the BaseRule class that all rules inherit from; 12) plugins/ - Custom rules can be implemented as plugins in separate packages.", "score": null, "retrieved_content": [{"name": "SQLFluffRule", "docstring": "SQLFluff rule directive for sphinx.\n\nRule directives can be used as shown below.\n\n.. code-block:: rst\n\n    .. sqlfluff:rule:: AM01\n                       ambiguous.distinct\n\n        Write the documentation for the rule here.\n\nTo cross reference (i.e. refer to) objects defined like this\nboth the code and name reference is available:\n\n.. code-block:: rst\n\n    :sqlfluff:ref:`CP02`\n    :sqlfluff:ref:`capitalisation.identifiers`", "methods": ["handle_signature", "add_target_and_index", "_object_hierarchy_parts", "_toc_entry_name"], "attributes": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 10, "end_line": 79}, "type": "class"}, {"name": "RulePack", "docstring": "A bundle of rules to be applied.\n\nThis contains a set of rules, post filtering but also contains the mapping\nrequired to interpret any noqa messages found in files.\n\nThe reason for this object is that rules are filtered and instantiated\ninto this pack in the main process when running in multi-processing mode so\nthat user defined rules can be used without reference issues.\n\nAttributes:\n    rules (:obj:`list` of :obj:`BaseRule`): A filtered list of instantiated\n        rules to be applied to a given file.\n    reference_map (:obj:`dict`): A mapping of rule references to the codes\n        they refer to, e.g. `{\"my_ref\": {\"LT01\", \"LT02\"}}`. The references\n        (i.e. the keys) may be codes, groups, aliases or names. The values\n        of the mapping are sets of rule codes *only*. This object acts as\n        a lookup to be able to translate selectors (which may contain\n        diverse references) into a consolidated list of rule codes. This\n        mapping contains the full set of rules, rather than just the filtered\n        set present in the `rules` attribute.", "methods": ["codes"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 850, "end_line": 878}, "type": "class"}, {"name": "RuleMetaclass", "docstring": "The metaclass for rules.\n\nThis metaclass provides provides auto-enrichment of the\nrule docstring so that examples, groups, aliases and\nnames are added.\n\nThe reason we enrich the docstring is so that it can be\npicked up by autodoc and all be displayed in the sqlfluff\ndocs.", "methods": ["_populate_code_and_description", "_populate_docstring", "__new__"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 151, "end_line": 361}, "type": "class"}, {"name": "get_rules_from_path", "is_method": false, "class_name": null, "parameters": ["rules_path", "base_module"], "calls": ["os.path.abspath", "sorted", "os.path.join", "glob", "import_module", "rules.append", "os.path.dirname", "os.path.splitext", "getattr", "os.path.basename", "AttributeError"], "code_location": {"file": "loader.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 12, "end_line": 43}, "code_snippet": "def get_rules_from_path(\n    # All rule files are expected in the format of L*.py\n    rules_path: str = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), \"../../rules\", \"L*.py\")\n    ),\n    base_module: str = \"sqlfluff.rules\",\n) -> list[type[\"BaseRule\"]]:\n    \"\"\"Reads all of the Rule classes from a path into a list.\"\"\"\n    # Create a rules dictionary for importing in\n    # sqlfluff/src/sqlfluff/core/rules/__init__.py\n    rules = []\n\n    for module in sorted(glob(rules_path)):\n        # Manipulate the module path to extract the filename without the .py\n        rule_id = os.path.splitext(os.path.basename(module))[0]\n        # All rule classes are expected in the format of Rule_L*\n        rule_class_name = f\"Rule_{rule_id}\"\n        # NOTE: We import the module outside of the try clause to\n        # properly catch any import errors.\n        rule_module = import_module(f\"{base_module}.{rule_id}\")\n        try:\n            rule_class = getattr(rule_module, rule_class_name)\n        except AttributeError as e:\n            raise AttributeError(\n                \"Rule classes must be named in the format of Rule_*. \"\n                f\"[{rule_class_name}]\"\n            ) from e\n        # Add the rules to the rules dictionary for\n        # sqlfluff/src/sqlfluff/core/rules/__init__.py\n        rules.append(rule_class)\n\n    return rules\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 56, "end_line": 69}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.references.RF01 import Rule_RF01\n    from sqlfluff.rules.references.RF02 import Rule_RF02\n    from sqlfluff.rules.references.RF03 import Rule_RF03\n    from sqlfluff.rules.references.RF04 import Rule_RF04\n    from sqlfluff.rules.references.RF05 import Rule_RF05\n    from sqlfluff.rules.references.RF06 import Rule_RF06\n\n    return [Rule_RF01, Rule_RF02, Rule_RF03, Rule_RF04, Rule_RF05, Rule_RF06]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 26, "end_line": 56}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.structure.ST01 import Rule_ST01\n    from sqlfluff.rules.structure.ST02 import Rule_ST02\n    from sqlfluff.rules.structure.ST03 import Rule_ST03\n    from sqlfluff.rules.structure.ST04 import Rule_ST04\n    from sqlfluff.rules.structure.ST05 import Rule_ST05\n    from sqlfluff.rules.structure.ST06 import Rule_ST06\n    from sqlfluff.rules.structure.ST07 import Rule_ST07\n    from sqlfluff.rules.structure.ST08 import Rule_ST08\n    from sqlfluff.rules.structure.ST09 import Rule_ST09\n    from sqlfluff.rules.structure.ST10 import Rule_ST10\n    from sqlfluff.rules.structure.ST11 import Rule_ST11\n\n    return [\n        Rule_ST01,\n        Rule_ST02,\n        Rule_ST03,\n        Rule_ST04,\n        Rule_ST05,\n        Rule_ST06,\n        Rule_ST07,\n        Rule_ST08,\n        Rule_ST09,\n        Rule_ST10,\n        Rule_ST11,\n    ]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/jinja", "start_line": 8, "end_line": 16}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.jinja.JJ01 import Rule_JJ01\n\n    return [Rule_JJ01]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/capitalisation", "start_line": 41, "end_line": 53}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.capitalisation.CP01 import Rule_CP01\n    from sqlfluff.rules.capitalisation.CP02 import Rule_CP02\n    from sqlfluff.rules.capitalisation.CP03 import Rule_CP03\n    from sqlfluff.rules.capitalisation.CP04 import Rule_CP04\n    from sqlfluff.rules.capitalisation.CP05 import Rule_CP05\n\n    return [Rule_CP01, Rule_CP02, Rule_CP03, Rule_CP04, Rule_CP05]\n", "type": "function"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}, {"name": "SQLFluffDomain", "docstring": "SQLFluff domain.", "methods": ["get_full_qualified_name", "get_objects", "resolve_xref", "add_rule"], "attributes": ["name", "label", "object_types", "roles", "directives", "initial_data"], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 82, "end_line": 142}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.2095484733581543}
{"question": "Where does SQLFluff's rule evaluation flow from rule discovery through fix generation?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's rule evaluation flow follows a systematic process from rule discovery to fix generation. The flow includes: 1) Rule discovery - Rules are discovered through the RuleSet class which maintains a registry of all available rules; 2) Rule filtering - Rules are filtered based on configuration settings (rules, exclude_rules, dialect compatibility); 3) Rule instantiation - Filtered rules are instantiated with configuration parameters and context; 4) Parse tree traversal - The linter traverses the parse tree using rule-specific crawlers (RootOnlyCrawler, SegmentSeekerCrawler); 5) Rule evaluation - Each rule's _eval() method is called with RuleContext containing segment, parent stack, and configuration; 6) Violation detection - Rules return LintResult objects when violations are found, including anchor segments and descriptions; 7) Fix generation - Rules that support fixing generate LintFix objects specifying the type of fix (create, edit, delete); 8) Fix validation - Generated fixes are validated for safety and compatibility with templated code; 9) Fix application - Valid fixes are applied to the parse tree to generate corrected SQL; 10) Result aggregation - All violations and fixes are collected and returned as part of the LintedFile object.", "score": null, "retrieved_content": [{"name": "_eval", "is_method": true, "class_name": "Rule_ST09", "parameters": ["self", "context"], "calls": ["context.segment.is_type", "segment.children", "children.recursive_crawl", "recursive_crawl", "next", "table_aliases.append", "recursive_crawl", "len", "get_eventual_alias", "from_expression_alias_info.segment.raw_normalized", "alias.upper", "children", "conditions.append", "subconditions.append", "comparison_operator.get_children", "first_column_reference.get_child", "second_column_reference.get_child", "upper", "upper", "LintResult", "join_clauses.children", "alias_info.segment.raw_normalized", "join_on_conditions.children", "self._split_list_by_segment_type", "self._is_qualified_column_operator_qualified_column_sequence", "FunctionalContext", "cast", "get_eventual_aliases", "Segments", "expression_group.append", "first_table_seg.raw_normalized", "second_table_seg.raw_normalized", "first_column_reference.pos_marker.is_literal", "table_aliases.index", "table_aliases.index", "table_aliases.index", "table_aliases.index", "children.recursive_crawl", "cast", "LintFix.replace", "LintFix.replace", "LintFix.replace", "SymbolSegment"], "code_location": {"file": "ST09.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 76, "end_line": 283}, "code_snippet": "    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Find rule violations and provide fixes.\n\n        0. Grab all table aliases into a table_aliases list.\n        1. Grab all conditions from the different join_on_condition segments.\n        2. Break conditions down into subconditions using the \"and\" and \"or\"\n        binary operators.\n        3. Keep subconditions that are made up of a qualified column_reference,\n        a comparison_operator and another qualified column_reference segments.\n        4. Check whether the table associated with the first column_reference segment\n        has a greater index in table_aliases than the second column_reference segment.\n        If so, populate the fixes list (lower index instead of greater index\n        if preferred_first_table_in_join_clause == \"later\").\n        5.a. If fixes is empty the rule passes.\n        5.b. If fixes isn't empty we return a LintResult object with fixable violations.\n        \"\"\"\n        self.preferred_first_table_in_join_clause: str\n\n        assert context.segment.is_type(\"from_expression\")\n\n        # STEP 0.\n        table_aliases: list[str] = []\n\n        children = FunctionalContext(context).segment.children()\n\n        # we use recursive_crawl to deal with brackets\n        join_clauses = children.recursive_crawl(\"join_clause\")\n\n        join_on_conditions = join_clauses.children().recursive_crawl(\n            \"join_on_condition\"\n        )\n\n        # we only care about join_on_condition segments\n        if len(join_on_conditions) == 0:\n            return None\n\n        # the first alias comes from the from clause\n        from_expression_alias_info = next(\n            cast(\n                FromExpressionElementSegment,\n                children.recursive_crawl(\"from_expression_element\")[0],\n            ).get_eventual_alias()\n        )\n        from_expression_alias: str = (\n            from_expression_alias_info.segment.raw_normalized(False)\n            if from_expression_alias_info.segment\n            else from_expression_alias_info.ref_str\n        )\n\n        table_aliases.append(from_expression_alias)\n\n        # the rest of the aliases come from the different join clauses\n        join_clause_alias_infos: list[AliasInfo] = [\n            cast(JoinClauseSegment, join_clause).get_eventual_aliases()[0][1]\n            for join_clause in [clause for clause in join_clauses]\n        ]\n\n        join_clause_aliases = [\n            (\n                alias_info.segment.raw_normalized(False)\n                if alias_info.segment\n                else alias_info.ref_str\n            )\n            for alias_info in join_clause_alias_infos\n        ]\n\n        table_aliases += join_clause_aliases\n\n        table_aliases = [alias.upper() for alias in table_aliases]\n\n        # STEP 1.\n        conditions: list[list[BaseSegment]] = []\n\n        join_on_condition__expressions = join_on_conditions.children().recursive_crawl(\n            \"expression\"\n        )\n\n        for expression in join_on_condition__expressions:\n            expression_group = []\n            for element in Segments(expression).children():\n                if element.type not in (\"whitespace\", \"newline\"):\n                    expression_group.append(element)\n            conditions.append(expression_group)\n\n        # STEP 2.\n        subconditions: list[list[list[BaseSegment]]] = []\n\n        for expression_group in conditions:\n            subconditions.append(\n                self._split_list_by_segment_type(\n                    segment_list=expression_group,\n                    delimiter_type=\"binary_operator\",\n                    delimiters=[\"and\", \"or\"],\n                )\n            )\n\n        subconditions_flattened: list[list[BaseSegment]] = [\n            item for sublist in subconditions for item in sublist\n        ]\n\n        # STEP 3.\n        column_operator_column_subconditions: list[list[BaseSegment]] = [\n            subcondition\n            for subcondition in subconditions_flattened\n            if self._is_qualified_column_operator_qualified_column_sequence(\n                subcondition\n            )\n        ]\n\n        # STEP 4.\n        fixes: list[LintFix] = []\n        anchor_segment = context.segment  # Default anchor\n\n        for subcondition in column_operator_column_subconditions:\n            comparison_operator = subcondition[1]\n            first_column_reference = subcondition[0]\n            second_column_reference = subcondition[2]\n            raw_comparison_operators = comparison_operator.get_children(\n                \"raw_comparison_operator\"\n            )\n\n            first_table_seg = first_column_reference.get_child(\n                \"naked_identifier\", \"quoted_identifier\"\n            )\n            second_table_seg = second_column_reference.get_child(\n                \"naked_identifier\", \"quoted_identifier\"\n            )\n            assert first_table_seg and second_table_seg\n            first_table = first_table_seg.raw_normalized(False).upper()\n            second_table = second_table_seg.raw_normalized(False).upper()\n\n            # if we swap the two column references around the comparison operator\n            # we might have to replace the comparison operator with a different one\n            raw_comparison_operator_opposites = {\"<\": \">\", \">\": \"<\"}\n\n            # there seem to be edge cases where either the first table or the second\n            # table is not in table_aliases, in which case we cannot provide any fix\n            if first_table not in table_aliases or second_table not in table_aliases:\n                continue\n\n            if (\n                table_aliases.index(first_table) > table_aliases.index(second_table)\n                and self.preferred_first_table_in_join_clause == \"earlier\"\n            ) or (\n                table_aliases.index(first_table) < table_aliases.index(second_table)\n                and self.preferred_first_table_in_join_clause == \"later\"\n            ):\n                # Use the first column reference as anchor if it has a literal\n                # position marker. This ensures the violation is anchored to\n                # a literal segment which won't be filtered out in templated\n                # code.\n                if (\n                    not fixes\n                    and first_column_reference.pos_marker\n                    and first_column_reference.pos_marker.is_literal()\n                ):\n                    anchor_segment = first_column_reference\n\n                fixes = (\n                    fixes\n                    + [\n                        LintFix.replace(\n                            first_column_reference,\n                            [second_column_reference],\n                        )\n                    ]\n                    + [\n                        LintFix.replace(\n                            second_column_reference,\n                            [first_column_reference],\n                        )\n                    ]\n                    + (\n                        [\n                            LintFix.replace(\n                                raw_comparison_operators[0],\n                                [\n                                    SymbolSegment(\n                                        raw=raw_comparison_operator_opposites[\n                                            raw_comparison_operators[0].raw\n                                        ],\n                                        type=\"raw_comparison_operator\",\n                                    )\n                                ],\n                            )\n                        ]\n                        if raw_comparison_operators\n                        and raw_comparison_operators[0].raw\n                        in raw_comparison_operator_opposites\n                        and [r.raw for r in raw_comparison_operators] != [\"<\", \">\"]\n                        else []\n                    )\n                )\n\n        # STEP 5.a.\n        if not fixes:\n            return None\n\n        # STEP 5.b.\n        else:\n            return LintResult(\n                anchor=anchor_segment,\n                fixes=fixes,\n                description=(\n                    \"Joins should list the table referenced \"\n                    f\"{self.preferred_first_table_in_join_clause} first.\"\n                ),\n            )\n", "type": "function"}, {"name": "test__dialect__base_broad_fix", "is_method": false, "class_name": null, "parameters": ["dialect", "file", "raise_critical_errors_after_fix", "caplog"], "calls": ["pytest.mark.parametrize", "load_file", "dict", "lex_and_parse", "print", "FluffConfig", "Linter", "linter.get_rulepack", "linter.lint_parsed", "parsed.tree.stringify"], "code_location": {"file": "dialects_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 85, "end_line": 120}, "code_snippet": "def test__dialect__base_broad_fix(\n    dialect, file, raise_critical_errors_after_fix, caplog\n):\n    \"\"\"Run a full fix with all rules, in search of critical errors.\n\n    NOTE: This suite does all of the same things as the above test\n    suite (the `parse_suite`), but also runs fix. In CI, we run\n    the above tests _with_ coverage tracking, but these we run\n    _without_.\n\n    The purpose of this test is as a more stretching run through\n    a wide range of test sql examples, and the full range of rules\n    to find any potential critical errors raised by any interactions\n    between different dialects and rules.\n\n    We also do not use DEBUG logging here because it gets _very_\n    noisy.\n    \"\"\"\n    raw = load_file(dialect, file)\n    config_overrides = dict(dialect=dialect)\n\n    parsed: Optional[ParsedString] = lex_and_parse(config_overrides, raw)\n    if not parsed:  # Empty file case\n        return\n    print(parsed.tree.stringify())\n\n    config = FluffConfig(overrides=config_overrides)\n    linter = Linter(config=config)\n    rule_pack = linter.get_rulepack()\n    # Due to \"raise_critical_errors_after_fix\" fixture \"fix\",\n    # will now throw.\n    linter.lint_parsed(\n        parsed,\n        rule_pack,\n        fix=True,\n    )\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_CV04", "parameters": ["self", "context"], "calls": ["context.segment.get_child", "children", "is_type", "sp.and_", "len", "is_type", "LiteralSegment", "LintResult", "children", "sp.not_", "sp.not_", "is_type", "LintResult", "sp.is_type", "sp.is_meta", "sp.is_type", "len", "SymbolSegment", "LiteralSegment", "segment.children", "LintFix.replace", "sp.is_type", "LintFix.replace", "FunctionalContext", "edit", "raw.replace"], "code_location": {"file": "CV04.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "start_line": 64, "end_line": 149}, "code_snippet": "    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Find rule violations and provide fixes.\"\"\"\n        # Config type hints\n        self.prefer_count_0: bool\n        self.prefer_count_1: bool\n        new_segment: RawSegment\n\n        # We already know we're in a function because of the crawl_behaviour.\n        # This means it's very unlikely that there isn't a function_name here.\n        function_name = context.segment.get_child(\"function_name\")\n        if not function_name:  # pragma: no cover\n            return None\n\n        if function_name.raw_upper == \"COUNT\":\n            # Get bracketed content\n            f_content = (\n                FunctionalContext(context)\n                .segment.children(sp.is_type(\"function_contents\"))\n                .children(sp.is_type(\"bracketed\"))\n                .children(\n                    sp.and_(\n                        sp.not_(sp.is_meta()),\n                        sp.not_(\n                            sp.is_type(\n                                \"start_bracket\", \"end_bracket\", \"whitespace\", \"newline\"\n                            )\n                        ),\n                    )\n                )\n            )\n            if len(f_content) != 1:  # pragma: no cover\n                return None\n\n            preferred = \"*\"\n            if self.prefer_count_1:\n                preferred = \"1\"\n            elif self.prefer_count_0:\n                preferred = \"0\"\n\n            if f_content[0].is_type(\"star\") and (\n                self.prefer_count_1 or self.prefer_count_0\n            ):\n                new_segment = LiteralSegment(raw=preferred, type=\"numeric_literal\")\n                return LintResult(\n                    anchor=context.segment,\n                    fixes=[\n                        LintFix.replace(\n                            f_content[0],\n                            [new_segment],\n                        ),\n                    ],\n                )\n\n            if f_content[0].is_type(\"expression\"):\n                expression_content = [\n                    seg for seg in f_content[0].segments if not seg.is_meta\n                ]\n\n                if (\n                    len(expression_content) == 1\n                    and expression_content[0].is_type(\"literal\")\n                    and expression_content[0].raw in [\"0\", \"1\"]\n                    and expression_content[0].raw != preferred\n                ):\n                    if preferred == \"*\":\n                        new_segment = SymbolSegment(raw=preferred, type=\"star\")\n                    else:\n                        new_segment = LiteralSegment(\n                            raw=preferred, type=\"numeric_literal\"\n                        )\n                    return LintResult(\n                        anchor=context.segment,\n                        fixes=[\n                            LintFix.replace(\n                                expression_content[0],\n                                [\n                                    expression_content[0].edit(\n                                        expression_content[0].raw.replace(\n                                            expression_content[0].raw, preferred\n                                        )\n                                    ),\n                                ],\n                            ),\n                        ],\n                    )\n        return None\n", "type": "function"}, {"name": "_process_lint_result", "is_method": true, "class_name": "BaseRule", "parameters": ["self", "res", "templated_file", "ignore_mask", "new_lerrs", "new_fixes", "root"], "calls": ["res.to_linting_error", "new_lerrs.append", "new_fixes.extend", "self.discard_unsafe_fixes", "root.path_to", "ignore_mask.ignore_masked_violations", "self.crawl_behaviour.passes_filter", "linter_logger.info", "all", "linter_logger.info", "self.crawl_behaviour.passes_filter"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 608, "end_line": 655}, "code_snippet": "    def _process_lint_result(\n        self,\n        res: LintResult,\n        templated_file: Optional[TemplatedFile],\n        ignore_mask: Optional[\"IgnoreMask\"],\n        new_lerrs: list[SQLLintError],\n        new_fixes: list[LintFix],\n        root: BaseSegment,\n    ) -> None:\n        # Unless the rule declares that it's already template safe. Do safety\n        # checks.\n        if not self.template_safe_fixes:\n            self.discard_unsafe_fixes(res, templated_file)\n        lerr = res.to_linting_error(rule=self)\n        if not lerr:\n            return None\n        if ignore_mask:\n            if not ignore_mask.ignore_masked_violations([lerr]):\n                return None\n\n        # Check whether this should be filtered out for being unparsable.\n        # To do that we check the parents of the anchors (of the violation\n        # and fixes) against the filter in the crawler.\n        # NOTE: We use `.passes_filter` here to do the test for unparsable\n        # to avoid duplicating code because that test is already implemented\n        # there.\n        anchors = [lerr.segment] + [fix.anchor for fix in lerr.fixes]\n        for anchor in anchors:\n            if not self.crawl_behaviour.passes_filter(anchor):  # pragma: no cover\n                # NOTE: This clause is untested, because it's a hard to produce\n                # edge case. The latter clause is much more likely.\n                linter_logger.info(\n                    \"Fix skipped due to anchor not passing filter: %s\", anchor\n                )\n                return None\n\n            parent_stack = root.path_to(anchor)\n            if not all(\n                self.crawl_behaviour.passes_filter(ps.segment) for ps in parent_stack\n            ):\n                linter_logger.info(\n                    \"Fix skipped due to parent of anchor not passing filter: %s\",\n                    [ps.segment for ps in parent_stack],\n                )\n                return None\n\n        new_lerrs.append(lerr)\n        new_fixes.extend(res.fixes)\n", "type": "function"}, {"name": "_eval_gen", "is_method": true, "class_name": "Rule_CV12", "parameters": ["self", "context"], "calls": ["select_statement.is_type", "select_statement.get_child", "self._is_where_clause_simplifable", "set", "select_statement.recursive_crawl", "collections.deque", "enumerate", "where_clause.get_child", "self._get_subexpression_chunks", "self._get_from_expression_element_alias", "next", "encountered_references.add", "any", "join_clause.get_child", "is_type", "where_clause_fix_segments.popleft", "is_type", "where_clause_fix_segments.pop", "where_clause.get_child", "is_type", "is_type", "select_statement.recursive_crawl", "join_clause.recursive_crawl", "self._get_from_expression_element_alias", "set", "enumerate", "where_clause_fix_segments.extend", "where_clause_fix_segments.append", "LintResult", "LintResult", "LintResult", "collections.deque", "enumerate", "ExpressionSegment", "JoinOnConditionSegment", "JoinClauseSegment", "BinaryOperatorSegment", "all", "this_join_clause_subexpressions.add", "consumed_subexpressions.add", "LintResult", "is_type", "join_clause_fix_segments.popleft", "is_type", "join_clause_fix_segments.pop", "tuple", "LintResult", "seg.recursive_crawl", "len", "join_clause_fix_segments.extend", "join_clause_fix_segments.append", "KeywordSegment", "WhitespaceSegment", "WhitespaceSegment", "LintFix.replace", "LintFix.delete", "LintFix.delete", "col_ref.raw_upper.startswith", "BinaryOperatorSegment", "tuple", "LintFix.replace"], "code_location": {"file": "CV12.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "start_line": 67, "end_line": 250}, "code_snippet": "    def _eval_gen(self, context: RuleContext) -> Iterator[LintResult]:\n        # We are only interested in SELECT statement.\n        select_statement = context.segment\n        assert select_statement.is_type(\"select_statement\")\n\n        maybe_where_clause = select_statement.get_child(\"where_clause\")\n        if not maybe_where_clause:\n            return\n\n        where_clause = maybe_where_clause\n        where_clause_simplifable = self._is_where_clause_simplifable(where_clause)\n\n        if where_clause_simplifable:\n            expr = where_clause.get_child(\"expression\")\n            assert expr is not None\n            subexpressions = self._get_subexpression_chunks(expr)\n        else:\n            subexpressions = []\n        consumed_subexpressions = set()\n\n        # get references in from clause\n        select_table_references = [\n            *select_statement.recursive_crawl(\n                \"from_expression_element\",\n                no_recursive_seg_type=[\"join_clause\", \"select_statement\"],\n            )\n        ]\n\n        # track all seen references (from clause + all previous joins)\n        encountered_references = {\n            self._get_from_expression_element_alias(table_ref)\n            for table_ref in select_table_references\n        }\n\n        for join_clause in select_statement.recursive_crawl(\n            \"join_clause\", no_recursive_seg_type=[\"select_statement\"]\n        ):\n            # mark table reference as seen\n            join_table_reference = next(\n                join_clause.recursive_crawl(\n                    \"from_expression_element\",\n                    no_recursive_seg_type=[\"select_statement\"],\n                )\n            )\n            encountered_references.add(\n                self._get_from_expression_element_alias(join_table_reference)\n            )\n            join_clause_keywords = [\n                seg for seg in join_clause.segments if seg.type == \"keyword\"\n            ]\n\n            if any(\n                kw.raw_upper in (\"CROSS\", \"POSITIONAL\", \"USING\", \"APPLY\")\n                for kw in join_clause_keywords\n            ):\n                # If explicit CROSS JOIN is used, disregard lack of condition\n                # If explicit POSITIONAL JOIN is used, disregard lack of condition\n                # If explicit JOIN USING is used, disregard lack of condition\n                # If explicit CROSS/OUTER APPLY is used, disregard lack of condition\n                continue\n\n            this_join_condition = join_clause.get_child(\"join_on_condition\")\n            if this_join_condition:\n                # Join condition is present, no error reported.\n                continue\n\n            if not where_clause_simplifable:\n                yield LintResult(anchor=join_clause)\n            else:\n                this_join_clause_subexpressions = set()\n                for subexpr_idx, subexpr_segments in enumerate(subexpressions):\n                    if subexpr_idx in consumed_subexpressions:\n                        continue\n                    qualified_column_references = [\n                        col_ref\n                        for seg in subexpr_segments\n                        for col_ref in seg.recursive_crawl(\n                            \"column_reference\",\n                            no_recursive_seg_type=\"select_statement\",\n                        )\n                        if \"dot\" in col_ref.descendant_type_set\n                    ]\n                    if len(qualified_column_references) > 1 and all(\n                        col_ref.raw_upper.startswith(\n                            tuple(\n                                f\"{table_ref}.\" for table_ref in encountered_references\n                            )\n                        )\n                        for col_ref in qualified_column_references\n                    ):\n                        this_join_clause_subexpressions.add(subexpr_idx)\n                        consumed_subexpressions.add(subexpr_idx)\n\n                if not this_join_clause_subexpressions:\n                    yield LintResult(join_clause)\n                else:\n                    join_clause_fix_segments: Deque[BaseSegment] = collections.deque()\n                    for subexpr_idx, subexpr_segments in enumerate(subexpressions):\n                        if subexpr_idx in this_join_clause_subexpressions:\n                            join_clause_fix_segments.extend(subexpr_segments)\n                            join_clause_fix_segments.append(\n                                BinaryOperatorSegment(\"AND\")\n                            )\n\n                    while join_clause_fix_segments and join_clause_fix_segments[\n                        0\n                    ].is_type(\"whitespace\", \"binary_operator\"):\n                        join_clause_fix_segments.popleft()\n                    while join_clause_fix_segments and join_clause_fix_segments[\n                        -1\n                    ].is_type(\"whitespace\", \"binary_operator\"):\n                        join_clause_fix_segments.pop()\n\n                    join_on_expression = ExpressionSegment(\n                        tuple(join_clause_fix_segments),\n                    )\n                    join_on = JoinOnConditionSegment(\n                        (\n                            KeywordSegment(\"ON\"),\n                            WhitespaceSegment(),\n                            join_on_expression,\n                        )\n                    )\n                    join_clause_segment = JoinClauseSegment(\n                        (\n                            *join_clause.segments,\n                            WhitespaceSegment(),\n                            join_on,\n                        )\n                    )\n\n                    yield LintResult(\n                        anchor=join_clause,\n                        fixes=[\n                            LintFix.replace(\n                                join_clause,\n                                edit_segments=[join_clause_segment],\n                            )\n                        ],\n                    )\n\n        if not where_clause_simplifable:\n            return\n\n        if not consumed_subexpressions:\n            return\n\n        # Rewrite WHERE to keep conditions not moved to ON clauses\n        where_clause_fix_segments: Deque[BaseSegment] = collections.deque()\n        for subexpr_idx, subexpr_segments in enumerate(subexpressions):\n            if subexpr_idx not in consumed_subexpressions:\n                where_clause_fix_segments.extend(subexpr_segments)\n                where_clause_fix_segments.append(BinaryOperatorSegment(\"AND\"))\n\n        while where_clause_fix_segments and where_clause_fix_segments[0].is_type(\n            \"whitespace\", \"binary_operator\"\n        ):\n            where_clause_fix_segments.popleft()\n        while where_clause_fix_segments and where_clause_fix_segments[-1].is_type(\n            \"whitespace\", \"binary_operator\"\n        ):\n            where_clause_fix_segments.pop()\n\n        if where_clause_fix_segments:\n            where_clause_expr = where_clause.get_child(\"expression\")\n            assert where_clause_expr is not None\n            yield LintResult(\n                anchor=where_clause_expr,\n                fixes=[\n                    LintFix.replace(\n                        where_clause_expr, edit_segments=[*where_clause_fix_segments]\n                    )\n                ],\n            )\n        else:\n            assert select_statement.segments[-1].is_type(\"where_clause\")\n            assert select_statement.segments[-2].is_type(\"whitespace\", \"newline\")\n            yield LintResult(\n                anchor=where_clause,\n                fixes=[\n                    LintFix.delete(select_statement.segments[-2]),\n                    LintFix.delete(select_statement.segments[-1]),\n                ],\n            )\n", "type": "function"}, {"name": "lint_fix_parsed", "is_method": true, "class_name": "Linter", "parameters": ["cls", "tree", "config", "rule_pack", "fix", "fname", "templated_file", "formatter"], "calls": ["config.get", "config.get", "linter_logger.info", "linter_logger.info", "config.get", "formatter.dispatch_lint_header", "cls.allowed_rule_ref_map", "IgnoreMask.from_tree", "phases.append", "range", "cls.remove_templated_errors", "format", "sorted", "config.get", "len", "linter_logger.info", "is_first_linter_pass", "tqdm", "tree.stringify", "rule_pack.codes", "progress_bar_crawler.set_description", "time.monotonic", "crawler.crawl", "is_first_linter_pass", "rule_timings.append", "linter_logger.info", "linter_logger.warning", "linter_logger.info", "compute_anchor_edit_info", "any", "isinstance", "is_first_linter_pass", "config.get", "anchor_info.items", "cls._report_conflicting_fixes_same_anchor", "linter_logger.debug", "apply_fixes", "time.monotonic", "anchor_info.values", "config.get", "tuple", "linter_logger.debug", "config.get", "tuple", "linter_logger.warning", "previous_versions.add", "cls._warn_unfixable"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 380, "end_line": 628}, "code_snippet": "    def lint_fix_parsed(\n        cls,\n        tree: BaseSegment,\n        config: FluffConfig,\n        rule_pack: RulePack,\n        fix: bool = False,\n        fname: Optional[str] = None,\n        templated_file: Optional[\"TemplatedFile\"] = None,\n        formatter: Optional[FormatterInterface] = None,\n    ) -> tuple[BaseSegment, list[SQLBaseError], Optional[IgnoreMask], RuleTimingsType]:\n        \"\"\"Lint and optionally fix a tree object.\"\"\"\n        # Keep track of the linting errors on the very first linter pass. The\n        # list of issues output by \"lint\" and \"fix\" only includes issues present\n        # in the initial SQL code, EXCLUDING any issues that may be created by\n        # the fixes themselves.\n        initial_linting_errors = []\n        # A placeholder for the fixes we had on the previous loop\n        last_fixes: Optional[list[LintFix]] = None\n        # Keep a set of previous versions to catch infinite loops.\n        previous_versions: set[tuple[str, tuple[\"SourceFix\", ...]]] = {(tree.raw, ())}\n        # Keep a buffer for recording rule timings.\n        rule_timings: RuleTimingsType = []\n\n        # If we are fixing then we want to loop up to the runaway_limit, otherwise just\n        # once for linting.\n        loop_limit = config.get(\"runaway_limit\") if fix else 1\n\n        # Dispatch the output for the lint header\n        if formatter:\n            formatter.dispatch_lint_header(\n                fname or \"<filename>\", sorted(rule_pack.codes())\n            )\n\n        # Look for comment segments which might indicate lines to ignore.\n        disable_noqa_except: Optional[str] = config.get(\"disable_noqa_except\")\n        if not config.get(\"disable_noqa\") or disable_noqa_except:\n            allowed_rules_ref_map = cls.allowed_rule_ref_map(\n                rule_pack.reference_map, disable_noqa_except\n            )\n            ignore_mask, ivs = IgnoreMask.from_tree(tree, allowed_rules_ref_map)\n            initial_linting_errors += ivs\n        else:\n            ignore_mask = None\n\n        save_tree = tree\n        # There are two phases of rule running.\n        # 1. The main loop is for most rules. These rules are assumed to\n        # interact and cause a cascade of fixes requiring multiple passes.\n        # These are run the `runaway_limit` number of times (default 10).\n        # 2. The post loop is for post-processing rules, not expected to trigger\n        # any downstream rules, e.g. capitalization fixes. They are run on the\n        # first loop and then twice at the end (once to fix, and once again to\n        # check result of fixes), but not in the intervening loops.\n        phases = [\"main\"]\n        if fix:\n            phases.append(\"post\")\n        for phase in phases:\n            if len(phases) > 1:\n                rules_this_phase = [\n                    rule for rule in rule_pack.rules if rule.lint_phase == phase\n                ]\n            else:\n                rules_this_phase = rule_pack.rules\n            for loop in range(loop_limit if phase == \"main\" else 2):\n\n                def is_first_linter_pass() -> bool:\n                    return phase == phases[0] and loop == 0\n\n                # Additional newlines are to assist in scanning linting loops\n                # during debugging.\n                linter_logger.info(\n                    f\"\\n\\nEntering linter phase {phase}, loop {loop + 1}/{loop_limit}\\n\"\n                )\n                changed = False\n\n                if is_first_linter_pass():\n                    # In order to compute initial_linting_errors correctly, need\n                    # to run all rules on the first loop of the main phase.\n                    rules_this_phase = rule_pack.rules\n                progress_bar_crawler = tqdm(\n                    rules_this_phase,\n                    desc=\"lint by rules\",\n                    leave=False,\n                    disable=progress_bar_configuration.disable_progress_bar,\n                )\n\n                for crawler in progress_bar_crawler:\n                    # Performance: After first loop pass, skip rules that don't\n                    # do fixes. Any results returned won't be seen by the user\n                    # anyway (linting errors ADDED by rules changing SQL, are\n                    # not reported back to the user - only initial linting errors),\n                    # so there's absolutely no reason to run them.\n                    if (\n                        fix\n                        and not is_first_linter_pass()\n                        and not crawler.is_fix_compatible\n                    ):\n                        continue\n\n                    progress_bar_crawler.set_description(f\"rule {crawler.code}\")\n                    t0 = time.monotonic()\n\n                    # fixes should be a dict {} with keys edit, delete, create\n                    # delete is just a list of segments to delete\n                    # edit and create are list of tuples. The first element is\n                    # the \"anchor\", the segment to look for either to edit or to\n                    # insert BEFORE. The second is the element to insert or create.\n                    linting_errors, _, fixes, _ = crawler.crawl(\n                        tree,\n                        dialect=config.get(\"dialect_obj\"),\n                        fix=fix,\n                        templated_file=templated_file,\n                        ignore_mask=ignore_mask,\n                        fname=fname,\n                        config=config,\n                    )\n                    if is_first_linter_pass():\n                        initial_linting_errors += linting_errors\n\n                    if fix and fixes:\n                        linter_logger.info(f\"Applying Fixes [{crawler.code}]: {fixes}\")\n                        # Do some sanity checks on the fixes before applying.\n                        anchor_info = compute_anchor_edit_info(fixes)\n                        if any(\n                            not info.is_valid for info in anchor_info.values()\n                        ):  # pragma: no cover\n                            message = (\n                                f\"Rule {crawler.code} returned conflicting \"\n                                \"fixes with the same anchor. This is only \"\n                                \"supported for create_before+create_after, so \"\n                                \"the fixes will not be applied. \"\n                            )\n                            for uuid, info in anchor_info.items():\n                                if not info.is_valid:\n                                    message += f\"\\n{uuid}:\"\n                                    for _fix in info.fixes:\n                                        message += f\"\\n    {_fix}\"\n                            cls._report_conflicting_fixes_same_anchor(message)\n                            for lint_result in linting_errors:\n                                lint_result.fixes = []\n                        elif fixes == last_fixes:\n                            # If we generate the same fixes two times in a row,\n                            # that means we're in a loop, and we want to stop.\n                            # (Fixes should address issues, hence different\n                            # and/or fewer fixes next time.)\n                            # This is most likely because fixes could not be safely\n                            # applied last time, so we should stop gracefully.\n                            linter_logger.debug(\n                                f\"Fixes generated for {crawler.code} are the same as \"\n                                \"the previous pass. Assuming that we cannot apply them \"\n                                \"safely. Passing gracefully.\"\n                            )\n                        else:\n                            # This is the happy path. We have fixes, now we want to\n                            # apply them.\n                            last_fixes = fixes\n                            new_tree, _, _, _valid = apply_fixes(\n                                tree,\n                                config.get(\"dialect_obj\"),\n                                crawler.code,\n                                anchor_info,\n                                fix_even_unparsable=config.get(\"fix_even_unparsable\"),\n                            )\n\n                            # Check for infinite loops. We use a combination of the\n                            # fixed templated file and the list of source fixes to\n                            # apply.\n                            loop_check_tuple = (\n                                new_tree.raw,\n                                tuple(new_tree.source_fixes),\n                            )\n                            # Was anything actually applied? If not, then the fixes we\n                            # had cannot be safely applied and we should stop trying.\n                            if loop_check_tuple == (tree.raw, tuple(tree.source_fixes)):\n                                linter_logger.debug(\n                                    f\"Fixes for {crawler.code} could not be safely be \"\n                                    \"applied. Likely due to initially unparsable file.\"\n                                )\n                            elif not _valid:\n                                # The fixes result in an invalid file. Don't apply\n                                # the fix and skip onward. Show a warning.\n                                linter_logger.warning(\n                                    f\"Fixes for {crawler.code} not applied, as it \"\n                                    \"would result in an unparsable file. Please \"\n                                    \"report this as a bug with a minimal query \"\n                                    \"which demonstrates this warning.\"\n                                )\n                            elif loop_check_tuple not in previous_versions:\n                                # We've not seen this version of the file so\n                                # far. Continue.\n                                tree = new_tree\n                                previous_versions.add(loop_check_tuple)\n                                changed = True\n                                continue\n                            else:\n                                # Applying these fixes took us back to a state\n                                # which we've seen before. We're in a loop, so\n                                # we want to stop.\n                                cls._warn_unfixable(crawler.code)\n\n                    # Record rule timing\n                    rule_timings.append(\n                        (crawler.code, crawler.name, time.monotonic() - t0)\n                    )\n\n                if fix and not changed:\n                    # We did not change the file. Either the file is clean (no\n                    # fixes), or any fixes which are present will take us back\n                    # to a previous state.\n                    linter_logger.info(\n                        f\"Fix loop complete for {phase} phase. Stability \"\n                        f\"achieved after {loop}/{loop_limit} loops.\"\n                    )\n                    break\n            else:\n                if fix:\n                    # The linter loop hit the limit before reaching a stable point\n                    # (i.e. free of lint errors). If this happens, it's usually\n                    # because one or more rules produced fixes which did not address\n                    # the original issue **or** created new issues.\n                    linter_logger.warning(\n                        f\"Loop limit on fixes reached [{loop_limit}].\"\n                    )\n\n                    # Discard any fixes for the linting errors, since they caused a\n                    # loop. IMPORTANT: By doing this, we are telling SQLFluff that\n                    # these linting errors are \"unfixable\". This is important,\n                    # because when \"sqlfluff fix\" encounters unfixable lint errors,\n                    # it exits with a \"failure\" exit code, which is exactly what we\n                    # want in this situation. (Reason: Although this is more of an\n                    # internal SQLFluff issue, users deserve to know about it,\n                    # because it means their file(s) weren't fixed.\n                    for violation in initial_linting_errors:\n                        if isinstance(violation, SQLLintError):\n                            violation.fixes = []\n\n                    # Return the original parse tree, before any fixes were applied.\n                    # Reason: When the linter hits the loop limit, the file is often\n                    # messy, e.g. some of the fixes were applied repeatedly, possibly\n                    # other weird things. We don't want the user to see this junk!\n                    return save_tree, initial_linting_errors, ignore_mask, rule_timings\n\n        if config.get(\"ignore_templated_areas\", default=True):\n            initial_linting_errors = cls.remove_templated_errors(initial_linting_errors)\n\n        linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Fixed Tree:\"))\n        linter_logger.info(\"\\n\" + tree.stringify())\n\n        return tree, initial_linting_errors, ignore_mask, rule_timings\n", "type": "function"}, {"name": "crawl", "is_method": true, "class_name": "BaseRule", "parameters": ["self", "tree", "dialect", "fix", "templated_file", "ignore_mask", "fname", "config"], "calls": ["RuleContext", "self.crawl_behaviour.crawl", "self._eval", "isinstance", "self.logger.info", "tuple", "pathlib.Path", "self.logger.critical", "context.segment.pos_marker.source_position", "self._log_critical_errors", "vs.append", "self._adjust_anchors_for_fixes", "self._process_lint_result", "rules_logger.error", "self.logger.info", "SQLLintError", "isinstance", "all", "TypeError", "self._adjust_anchors_for_fixes", "self._process_lint_result", "format", "isinstance", "str"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 480, "end_line": 600}, "code_snippet": "    def crawl(\n        self,\n        tree: BaseSegment,\n        dialect: \"Dialect\",\n        fix: bool,\n        templated_file: Optional[\"TemplatedFile\"],\n        ignore_mask: Optional[\"IgnoreMask\"],\n        fname: Optional[str],\n        config: \"FluffConfig\",\n    ) -> tuple[\n        list[SQLLintError],\n        tuple[RawSegment, ...],\n        list[LintFix],\n        Optional[dict[str, Any]],\n    ]:\n        \"\"\"Run the rule on a given tree.\n\n        Returns:\n            A tuple of (vs, raw_stack, fixes, memory)\n\n        \"\"\"\n        root_context = RuleContext(\n            dialect=dialect,\n            fix=fix,\n            templated_file=templated_file,\n            path=pathlib.Path(fname) if fname else None,\n            segment=tree,\n            config=config,\n        )\n        vs: list[SQLLintError] = []\n        fixes: list[LintFix] = []\n\n        # Propagates memory from one rule _eval() to the next.\n        memory = root_context.memory\n        context = root_context\n        for context in self.crawl_behaviour.crawl(root_context):\n            try:\n                context.memory = memory\n                res = self._eval(context=context)\n            except (bdb.BdbQuit, KeyboardInterrupt):  # pragma: no cover\n                raise\n            # Any exception at this point would halt the linter and\n            # cause the user to get no results\n            except Exception as e:\n                # If a filename is present, include it in the critical exception.\n                self.logger.critical(\n                    (\n                        f\"Applying rule {self.code} to {fname!r} \"\n                        f\"threw an Exception: {e}\"\n                        if fname\n                        else f\"Applying rule {self.code} threw an Exception: {e}\"\n                    ),\n                    exc_info=True,\n                )\n                assert context.segment.pos_marker\n                exception_line, _ = context.segment.pos_marker.source_position()\n                self._log_critical_errors(e)\n                vs.append(\n                    SQLLintError(\n                        rule=self,\n                        segment=context.segment,\n                        fixes=[],\n                        description=(\n                            f\"Unexpected exception: {str(e)};\\n\"\n                            \"Could you open an issue at \"\n                            \"https://github.com/sqlfluff/sqlfluff/issues ?\\n\"\n                            \"You can ignore this exception for now, by adding \"\n                            f\"'-- noqa: {self.code}' at the end\\n\"\n                            f\"of line {exception_line}\\n\"\n                        ),\n                    )\n                )\n                return vs, context.raw_stack, fixes, context.memory\n\n            new_lerrs: list[SQLLintError] = []\n            new_fixes: list[LintFix] = []\n\n            if res is None or res == []:\n                # Assume this means no problems (also means no memory)\n                pass\n            elif isinstance(res, LintResult):\n                # Extract any memory\n                memory = res.memory\n                self._adjust_anchors_for_fixes(context, res)\n                self._process_lint_result(\n                    res, templated_file, ignore_mask, new_lerrs, new_fixes, tree\n                )\n            elif isinstance(res, list) and all(\n                isinstance(elem, LintResult) for elem in res\n            ):\n                # Extract any memory from the *last* one, assuming\n                # it was the last to be added\n                memory = res[-1].memory\n                for elem in res:\n                    self._adjust_anchors_for_fixes(context, elem)\n                    self._process_lint_result(\n                        elem, templated_file, ignore_mask, new_lerrs, new_fixes, tree\n                    )\n            else:  # pragma: no cover\n                raise TypeError(\n                    \"Got unexpected result [{!r}] back from linting rule: {!r}\".format(\n                        res, self.code\n                    )\n                )\n\n            for lerr in new_lerrs:\n                self.logger.info(\"!! Violation Found: %r\", lerr.description)\n            if new_fixes:\n                if not self.is_fix_compatible:  # pragma: no cover\n                    rules_logger.error(\n                        f\"Rule {self.code} returned a fix but is not documented as \"\n                        \"`is_fix_compatible`, you may encounter unusual fixing \"\n                        \"behaviour. Report this a bug to the developer of this rule.\"\n                    )\n                for lfix in new_fixes:\n                    self.logger.info(\"!! Fix Proposed: %r\", lfix)\n\n            # Consume the new results\n            vs += new_lerrs\n            fixes += new_fixes\n        return vs, context.raw_stack if context else tuple(), fixes, context.memory\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_ST01", "parameters": ["self", "context"], "calls": ["context.segment.is_type", "segment.children", "children.first", "sp.is_type", "else_clause.children", "select", "LintResult", "FunctionalContext", "children.reversed", "sp.or_", "sp.is_type", "sp.is_meta", "LintFix.delete", "LintFix.delete"], "code_location": {"file": "ST01.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 45, "end_line": 78}, "code_snippet": "    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Find rule violations and provide fixes.\n\n        0. Look for a case expression\n        1. Look for \"ELSE\"\n        2. Mark \"ELSE\" for deletion (populate \"fixes\")\n        3. Backtrack and mark all newlines/whitespaces for deletion\n        4. Look for a raw \"NULL\" segment\n        5.a. The raw \"NULL\" segment is found, we mark it for deletion and return\n        5.b. We reach the end of case when without matching \"NULL\": the rule passes\n        \"\"\"\n        assert context.segment.is_type(\"case_expression\")\n        children = FunctionalContext(context).segment.children()\n        else_clause = children.first(sp.is_type(\"else_clause\"))\n\n        # Does the \"ELSE\" have a \"NULL\"? NOTE: Here, it's safe to look for\n        # \"NULL\", as an expression would *contain* NULL but not be == NULL.\n        if else_clause and else_clause.children(\n            lambda child: child.raw_upper == \"NULL\"\n        ):\n            # Found ELSE with NULL. Delete the whole else clause as well as\n            # indents/whitespaces/meta preceding the ELSE. :TRICKY: Note\n            # the use of reversed() to make select() effectively search in\n            # reverse.\n            before_else = children.reversed().select(\n                start_seg=else_clause[0],\n                loop_while=sp.or_(sp.is_type(\"whitespace\", \"newline\"), sp.is_meta()),\n            )\n            return LintResult(\n                anchor=context.segment,\n                fixes=[LintFix.delete(else_clause[0])]\n                + [LintFix.delete(seg) for seg in before_else],\n            )\n        return None\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_ST06", "parameters": ["self", "context"], "calls": ["context.segment.is_type", "reversed", "reversed", "context.segment.get_children", "seg.is_type", "seg.is_type", "enumerate", "LintResult", "seg.get_child", "seg.get_parent", "with_compound_statement.recursive_crawl", "append", "len", "any", "LintResult", "LintFix.replace", "self._implicit_column_references", "zip", "with_compound_statement.path_to", "any", "isinstance", "segment.get_child", "self._validate", "any", "isinstance", "path_step.segment.is_type", "segment.get_child", "_function.get_child", "isinstance", "path_step.segment.is_type", "self._validate", "segment.get_child", "_expression.get_child", "self._validate", "len", "len"], "code_location": {"file": "ST06.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 64, "end_line": 232}, "code_snippet": "    def _eval(self, context: RuleContext) -> EvalResultType:\n        self.violation_exists = False\n        # Bands of select targets in order to be enforced\n        select_element_order_preference: tuple[\n            tuple[Union[str, tuple[str, ...]], ...], ...\n        ] = (\n            (\"wildcard_expression\",),\n            (\n                \"object_reference\",\n                \"literal\",\n                \"cast_expression\",\n                (\"function\", \"cast\"),\n                (\"expression\", \"cast_expression\"),\n            ),\n        )\n\n        # Track which bands have been seen, with additional empty list for the\n        # non-matching elements. If we find a matching target element, we append the\n        # element to the corresponding index.\n        self.seen_band_elements: list[list[BaseSegment]] = [\n            [] for _ in select_element_order_preference\n        ] + [\n            []\n        ]  # type: ignore\n\n        assert context.segment.is_type(\"select_clause\")\n\n        # insert, merge, create table, union are order-sensitive\n        for seg in reversed(context.parent_stack):\n            if seg.is_type(\n                \"insert_statement\",\n                \"set_expression\",\n                \"create_table_statement\",\n                \"merge_statement\",\n            ):\n                return None\n\n        # CTE is order-sensitive only if CTE is referenced as SELECT * in set expression\n        for seg in reversed(context.parent_stack):\n            if seg.is_type(\"common_table_expression\"):\n                cte_identifier = seg.get_child(\"identifier\")\n                assert cte_identifier is not None\n                maybe_with_compound_statement = seg.get_parent()\n                if maybe_with_compound_statement is None:\n                    break  # pragma: no cover\n                with_compound_statement, _ = maybe_with_compound_statement\n                for ref in with_compound_statement.recursive_crawl(\"table_reference\"):\n                    if ref.raw_upper == cte_identifier.raw_upper:\n                        path = with_compound_statement.path_to(ref)\n                        if any(\n                            path_step.segment.is_type(\"set_expression\")\n                            for path_step in path\n                        ):\n                            select_statements = [\n                                path_step.segment\n                                for path_step in path\n                                if path_step.segment.is_type(\n                                    \"select_statement\",\n                                    \"unordered_select_statement_segment\",\n                                )\n                            ]\n                            if any(\n                                \"wildcard_expression\"\n                                in select_statement.descendant_type_set\n                                for select_statement in select_statements\n                            ):\n                                return None\n\n        select_clause_segment = context.segment\n        select_target_elements = context.segment.get_children(\"select_clause_element\")\n        if not select_target_elements:\n            return None\n\n        # Iterate through all the select targets to find any order violations\n        for segment in select_target_elements:\n            # The band index of the current segment in\n            # select_element_order_preference\n            self.current_element_band = None\n\n            # Compare the segment to the bands in select_element_order_preference\n            for i, band in enumerate(select_element_order_preference):\n                for e in band:\n                    # Identify simple select target\n                    if isinstance(e, str) and segment.get_child(e):\n                        self._validate(i, segment)\n\n                    # Identify function\n                    elif isinstance(e, tuple) and e[0] == \"function\":\n                        try:\n                            _function = segment.get_child(\"function\")\n                            assert _function\n                            _function_name = _function.get_child(\"function_name\")\n                            assert _function_name\n                            if _function_name.raw == e[1]:\n                                self._validate(i, segment)\n                        except (AttributeError, AssertionError):\n                            # If the segment doesn't match\n                            pass\n\n                    # Identify simple expression\n                    elif isinstance(e, tuple) and e[0] == \"expression\":\n                        try:\n                            _expression = segment.get_child(\"expression\")\n                            assert _expression\n\n                            if (\n                                _expression.get_child(e[1])\n                                and _expression.segments[0].type\n                                in (\n                                    \"column_reference\",\n                                    \"object_reference\",\n                                    \"literal\",\n                                    \"cast_expression\",\n                                )\n                                # len == 2 to ensure the expression is 'simple'\n                                and (\n                                    len(_expression.segments) == 2\n                                    # cast_expression is one length\n                                    or len(_expression.segments) == 1\n                                )\n                            ):\n                                self._validate(i, segment)\n                        except (AttributeError, AssertionError):\n                            # If the segment doesn't match\n                            pass\n\n            # If the target doesn't exist in select_element_order_preference then it\n            # is 'complex' and must go last\n            if self.current_element_band is None:\n                self.seen_band_elements[-1].append(segment)\n\n        if self.violation_exists:\n            if len(context.parent_stack) and any(\n                self._implicit_column_references(context.parent_stack[-1])\n            ):\n                # If there are implicit column references (i.e. column\n                # numbers), warn but don't fix, because it's much more\n                # complicated to autofix.\n                return LintResult(anchor=select_clause_segment)\n            # Create a list of all the edit fixes\n            # We have to do this at the end of iterating through all the\n            # select_target_elements to get the order correct. This means we can't\n            # add a lint fix to each individual LintResult as we go\n            ordered_select_target_elements = [\n                segment for band in self.seen_band_elements for segment in band\n            ]\n            # TODO: The \"if\" in the loop below compares corresponding items\n            # to avoid creating \"do-nothing\" edits. A potentially better\n            # approach would leverage difflib.SequenceMatcher.get_opcodes(),\n            # which generates a list of edit actions (similar to the\n            # command-line \"diff\" tool in Linux). This is more complex to\n            # implement, but minimizing the number of LintFixes makes the\n            # final application of patches (in \"sqlfluff fix\") more robust.\n            fixes = [\n                LintFix.replace(\n                    initial_select_target_element,\n                    [replace_select_target_element],\n                )\n                for initial_select_target_element, replace_select_target_element in zip(  # noqa: E501\n                    select_target_elements, ordered_select_target_elements\n                )\n                if initial_select_target_element is not replace_select_target_element\n            ]\n            # Anchoring on the select statement segment ensures that\n            # select statements which include macro targets are ignored\n            # when ignore_templated_areas is set\n            return LintResult(anchor=select_clause_segment, fixes=fixes)\n\n        return None\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_CV10", "parameters": ["self", "context"], "calls": ["segment.raw_slices.select", "self._normalize_preferred_quoted_literal_style", "context.segment.is_type", "LintResult", "rsp.is_slice_type", "isinstance", "memory.get", "LintResult", "LintResult", "self.logger.debug", "lstrip", "pos_marker.source_str", "FunctionalContext", "LintFix.replace", "pos_marker.source_str", "LiteralSegment"], "code_location": {"file": "CV10.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "start_line": 92, "end_line": 197}, "code_snippet": "    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        # Config type hints\n        self.preferred_quoted_literal_style: str\n        self.force_enable: bool\n\n        # Only care about quoted literal segments.\n        if not context.segment.is_type(\"quoted_literal\"):\n            return None\n\n        if not (\n            self.force_enable\n            or context.dialect.name in self._dialects_with_double_quoted_strings\n        ):\n            return LintResult(memory=context.memory)\n\n        # This rule can also cover quoted literals that are partially templated.\n        # I.e. when the quotes characters are _not_ part of the template we can\n        # meaningfully apply this rule.\n        templated_raw_slices = FunctionalContext(context).segment.raw_slices.select(\n            rsp.is_slice_type(\"templated\")\n        )\n        for raw_slice in templated_raw_slices:\n            pos_marker = context.segment.pos_marker\n            # This is to make mypy happy.\n            assert isinstance(pos_marker, PositionMarker)\n\n            # Check whether the quote characters are inside the template.\n            # For the leading quote we need to account for string prefix characters.\n            leading_quote_inside_template = pos_marker.source_str()[:2].lstrip(\n                self._string_prefix_chars\n            )[0] not in ['\"', \"'\"]\n            trailing_quote_inside_template = pos_marker.source_str()[-1] not in [\n                '\"',\n                \"'\",\n            ]\n\n            # quotes are not entirely outside of a template, nothing we can do\n            if leading_quote_inside_template or trailing_quote_inside_template:\n                return LintResult(memory=context.memory)\n\n        # If quoting style is set to consistent we use the quoting style of the first\n        # quoted_literal that we encounter.\n        if self.preferred_quoted_literal_style == \"consistent\":\n            memory = context.memory\n            preferred_quoted_literal_style = memory.get(\n                \"preferred_quoted_literal_style\"\n            )\n\n            if not preferred_quoted_literal_style:\n                # Getting the quote from LAST character to be able to handle STRING\n                # prefixes\n                preferred_quoted_literal_style = (\n                    \"double_quotes\"\n                    if context.segment.raw[-1] == '\"'\n                    else \"single_quotes\"\n                )\n                memory[\"preferred_quoted_literal_style\"] = (\n                    preferred_quoted_literal_style\n                )\n                self.logger.debug(\n                    \"Preferred string quotes is set to `consistent`. Derived quoting \"\n                    \"style %s from first quoted literal.\",\n                    preferred_quoted_literal_style,\n                )\n        else:\n            preferred_quoted_literal_style = self.preferred_quoted_literal_style\n\n        fixed_string = self._normalize_preferred_quoted_literal_style(\n            context.segment.raw,\n            preferred_quote_char=self._quotes_mapping[preferred_quoted_literal_style][\n                \"preferred_quote_char\"\n            ],\n            alternate_quote_char=self._quotes_mapping[preferred_quoted_literal_style][\n                \"alternate_quote_char\"\n            ],\n        )\n\n        if fixed_string != context.segment.raw:\n            # We can't just set the primary type, but we have to ensure that the\n            # subtypes are properly set too so that the re-parse checks pass.\n            if fixed_string[0] == \"'\":\n                _instance_types = (\"quoted_literal\", \"single_quote\")\n            else:\n                _instance_types = (\"quoted_literal\", \"double_quote\")\n            return LintResult(\n                anchor=context.segment,\n                memory=context.memory,\n                fixes=[\n                    LintFix.replace(\n                        context.segment,\n                        [\n                            LiteralSegment(\n                                raw=fixed_string,\n                                instance_types=_instance_types,\n                            )\n                        ],\n                    )\n                ],\n                description=(\n                    \"Inconsistent use of preferred quote style '\"\n                    f\"{self._quotes_mapping[preferred_quoted_literal_style]['common_name']}\"  # noqa: E501\n                    f\"'. Use {fixed_string} instead of {context.segment.raw}.\"\n                ),\n            )\n\n        return None\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.2139892578125}
{"question": "Where in SQLFluff is the configuration system implemented?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's configuration system is implemented across several key modules in the codebase. The main configuration components are located in: 1) src/sqlfluff/core/config/fluffconfig.py - Contains the main FluffConfig class that manages all configuration; 2) src/sqlfluff/core/default_config.cfg - Contains the built-in default configuration values; 3) src/sqlfluff/core/config/__init__.py - Provides configuration loading and validation utilities; 4) src/sqlfluff/core/config/helpers.py - Contains helper functions for configuration processing; 5) src/sqlfluff/core/config/loader.py - Handles loading configuration from various file formats; 6) src/sqlfluff/core/config/validation.py - Implements configuration validation logic; 7) src/sqlfluff/core/rules/config_info.py - Manages rule-specific configuration information; 8) src/sqlfluff/core/rules/base.py - Contains configuration handling for individual rules; 9) src/sqlfluff/cli/commands.py - Handles command-line configuration overrides; 10) src/sqlfluff/core/config/nested_combine.py - Implements configuration merging and inheritance logic.", "score": null, "retrieved_content": [{"name": "FluffConfig", "docstring": "The persistent object for internal methods to access configuration.\n\nThis class is designed to be instantiated once for each file and then be\nreused by each part of the process. For multiple files in the same path, a\nparent object will be created for the each path and then variants of it\nare created *for each file*. The object itself contains the references\nto any long lived objects which might be used by multiple parts of the\ncodebase such as the dialect and the templater (both of which can be\nresource intensive to load & instantiate), which allows (for example),\nmultiple files to reuse the same instance of the relevant dialect.\n\nIt is also designed to pickle well for use in parallel operations.\n\nArgs:\n    configs (ConfigMappingType, optional): A nested dict of config\n        values from which to construct the config.\n    extra_config_path (str, optional): An optional additional path\n        to load config files from. These are loaded last if found\n        and take precedence over any pre-existing config values.\n        Note that when provided directly to the class, this path\n        is not loaded for the class in question (it's assumed that\n        has already been done, and the results are incorporated in\n        the `configs` argument), but it *is* passed onward to child\n        config instances, which will use it.\n    ignore_local_config (bool, optional, defaults to False): If set to\n        True, this skips loading configuration from the user home\n        directory (``~``) or ``appdir`` path.\n    overrides (ConfigMappingType, optional): A additional set of\n        configs to merge into the ``core`` section of the config\n        object at the end. These values take precedence over all\n        other provided values and are inherited by child configs.\n        For example, override values provided in the CLI use this\n        method to apply to all files in a linting operation. Note\n        that this mapping dict *only* applies to the ``core``\n        section and so cannot be used for all values.\n    plugin_manager (PluginManager, optional): Optional pre-loaded\n        config manager. Generally users should not need to provide\n        this, as the class will fetch it's own if not provided.\n        This argument is used when creating new class instances to\n        avoid reloading the manager.\n\n.. note::\n   Methods for accessing internal properties on the config are not particularly\n   standardised as the project currently assumes that few other tools are using\n   this interface directly. If you or your project would like more formally\n   supported methods for access to the config object, raise an issue on GitHub\n   with the kind of things you'd like to achieve.", "methods": ["__init__", "_handle_comma_separated_values", "_initialise_dialect", "verify_dialect_specified", "__getstate__", "__setstate__", "copy", "from_root", "from_string", "from_strings", "from_path", "from_kwargs", "get_templater_class", "get_templater", "make_child_from_path", "diff_to", "get", "get_section", "set_value", "iter_vals", "process_inline_config", "process_raw_file_for_config"], "attributes": ["private_vals"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 37, "end_line": 732}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "FluffConfig", "parameters": ["self", "configs", "extra_config_path", "ignore_local_config", "overrides", "plugin_manager", "require_dialect"], "calls": ["nested_combine", "nested_combine", "self._handle_comma_separated_values", "self._initialise_dialect", "self.get_templater", "validate_config_dict", "isinstance", "get_plugin_manager", "validate_config_dict", "get", "isinstance", "self._plugin_manager.hook.load_default_config"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 89, "end_line": 137}, "code_snippet": "    def __init__(\n        self,\n        configs: Optional[ConfigMappingType] = None,\n        extra_config_path: Optional[str] = None,\n        ignore_local_config: bool = False,\n        overrides: Optional[ConfigMappingType] = None,\n        plugin_manager: Optional[pluggy.PluginManager] = None,\n        # Ideally a dialect should be set when config is read but sometimes\n        # it might only be set in nested .sqlfluff config files, so allow it\n        # to be not required.\n        require_dialect: bool = True,\n    ) -> None:\n        self._extra_config_path = (\n            extra_config_path  # We only store this for child configs\n        )\n        self._ignore_local_config = (\n            ignore_local_config  # We only store this for child configs\n        )\n        # If overrides are provided, validate them early.\n        if overrides:\n            overrides = {\"core\": overrides}\n            validate_config_dict(overrides, \"<provided overrides>\")\n        # Stash overrides so we can pass them to child configs\n        core_overrides = overrides[\"core\"] if overrides else None\n        assert isinstance(core_overrides, dict) or core_overrides is None\n        self._overrides = core_overrides\n\n        # Fetch a fresh plugin manager if we weren't provided with one\n        self._plugin_manager = plugin_manager or get_plugin_manager()\n\n        defaults = nested_combine(*self._plugin_manager.hook.load_default_config())\n        # If any existing configs are provided. Validate them:\n        if configs:\n            validate_config_dict(configs, \"<provided configs>\")\n        self._configs = nested_combine(\n            defaults, configs or {\"core\": {}}, overrides or {}\n        )\n        # Some configs require special treatment\n        self._configs[\"core\"][\"color\"] = (\n            False if self._configs[\"core\"].get(\"nocolor\", False) else None\n        )\n        # Handle inputs which are potentially comma separated strings\n        self._handle_comma_separated_values()\n        # Dialect and Template selection.\n        _dialect = self._configs[\"core\"][\"dialect\"]\n        assert _dialect is None or isinstance(_dialect, str)\n        self._initialise_dialect(_dialect, require_dialect)\n\n        self._configs[\"core\"][\"templater_obj\"] = self.get_templater()\n", "type": "function"}, {"name": "test__linter__path_from_paths__exts", "is_method": false, "class_name": null, "parameters": [], "calls": ["normalise_paths", "paths_from_path"], "code_location": {"file": "discovery_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 38, "end_line": 46}, "code_snippet": "def test__linter__path_from_paths__exts():\n    \"\"\"Test configuration of file discovery.\"\"\"\n    paths = normalise_paths(\n        paths_from_path(\"test/fixtures/linter\", target_file_exts=[\".txt\", \".txt.j2\"])\n    )\n    assert \"test.fixtures.linter.passing.sql\" not in paths\n    assert \"test.fixtures.linter.passing_cap_extension.SQL\" not in paths\n    assert \"test.fixtures.linter.discovery_file.txt\" in paths\n    assert \"test.fixtures.linter.discovery_file.txt.j2\" in paths\n", "type": "function"}, {"name": "test__config__validate_configs_indirect", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.raises", "FluffConfig"], "code_location": {"file": "fluffconfig_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/config", "start_line": 236, "end_line": 246}, "code_snippet": "def test__config__validate_configs_indirect():\n    \"\"\"Test _validate_configs method of FluffConfig indirectly.\"\"\"\n    # Instantiate config object.\n    with pytest.raises(SQLFluffUserError):\n        FluffConfig(\n            configs={\n                \"core\": {\"dialect\": \"ansi\"},\n                # This is a known removed value.\n                \"rules\": {\"L003\": {\"lint_templated_tokens\": True}},\n            }\n        )\n", "type": "function"}, {"name": "cli", "is_method": false, "class_name": null, "parameters": [], "calls": ["click.group", "click.version_option"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 480, "end_line": 481}, "code_snippet": "def cli() -> None:\n    \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n", "type": "function"}, {"name": "get_config_info", "is_method": false, "class_name": null, "parameters": [], "calls": ["get_plugin_manager", "plugin_manager.hook.get_configs_info", "config_info_dict.items"], "code_location": {"file": "config_info.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 88, "end_line": 99}, "code_snippet": "def get_config_info() -> dict[str, ConfigInfo]:\n    \"\"\"Get the config from core sqlfluff and sqlfluff plugins and merges them.\n\n    NOTE: This should be the entry point into getting config info rather than\n    importing the default set above, as many values are defined only in rule\n    packages.\n    \"\"\"\n    plugin_manager = get_plugin_manager()\n    configs_info = plugin_manager.hook.get_configs_info()\n    return {\n        k: v for config_info_dict in configs_info for k, v in config_info_dict.items()\n    }\n", "type": "function"}, {"name": "test__context_in_config_is_loaded", "is_method": false, "class_name": null, "parameters": ["project_dir", "dbt_templater", "model_path", "var_value", "dbt_fluff_config"], "calls": ["pytest.mark.parametrize", "deepcopy", "FluffConfig", "dbt_templater.process", "Path", "str", "path.read_text", "str"], "code_location": {"file": "templater_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/test", "start_line": 646, "end_line": 667}, "code_snippet": "def test__context_in_config_is_loaded(\n    project_dir,\n    dbt_templater,\n    model_path,\n    var_value,\n    dbt_fluff_config,\n):\n    \"\"\"Test that variables inside .sqlfluff are passed to dbt.\"\"\"\n    context = {\"passed_through_cli\": var_value} if var_value else {}\n\n    config_dict = deepcopy(dbt_fluff_config)\n    config_dict[\"templater\"][\"dbt\"][\"context\"] = context\n    config = FluffConfig(config_dict)\n\n    path = Path(project_dir) / model_path\n\n    processed, violations = dbt_templater.process(\n        in_str=path.read_text(), fname=str(path), config=config\n    )\n\n    assert violations == []\n    assert str(var_value) in processed.templated_str\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "SQLFluffDriver", "parameters": ["self"], "calls": ["__init__", "super", "s.encode", "sys.getfilesystemencoding"], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 25, "end_line": 34}, "code_snippet": "    def __init__(self) -> None:\n        super().__init__(\n            [sys.executable, \"-m\", \"sqlfluff.cli.commands\"],\n            [\".sql\"],\n            [\n                s.encode(sys.getfilesystemencoding())\n                for s in [\"sqlfluff\", \"lint\", \"--format=json\"]\n            ],\n            exit_codes=[0, 1],\n        )\n", "type": "function"}, {"name": "test__config__from_string", "is_method": false, "class_name": null, "parameters": [], "calls": ["FluffConfig.from_string", "open", "f.read", "cfg.get", "cfg.get", "os.path.join"], "code_location": {"file": "fluffconfig_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/config", "start_line": 217, "end_line": 226}, "code_snippet": "def test__config__from_string():\n    \"\"\"Test from_string method of FluffConfig.\"\"\"\n    with open(\n        os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\", \".sqlfluff\")\n    ) as f:\n        config_string = f.read()\n    cfg = FluffConfig.from_string(config_string)\n    # Verify we can later retrieve the config values.\n    assert cfg.get(\"testing_val\") == \"foobar\"\n    assert cfg.get(\"dialect\") == \"mysql\"\n", "type": "function"}, {"name": "SQLFluffDomain", "docstring": "SQLFluff domain.", "methods": ["get_full_qualified_name", "get_objects", "resolve_xref", "add_rule"], "attributes": ["name", "label", "object_types", "roles", "directives", "initial_data"], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 82, "end_line": 142}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.2593529224395752}
{"question": "Where does SQLFluff implement its fix generation logic?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff implements its fix generation logic across several key modules in the codebase. The main fix generation components are located in: 1) src/sqlfluff/core/rules/fix.py - Contains the LintFix class and fix generation utilities; 2) src/sqlfluff/core/linter/fix.py - Implements fix application logic and patch generation; 3) src/sqlfluff/core/linter/patch.py - Contains FixPatch class for managing code patches; 4) src/sqlfluff/core/rules/base.py - Contains fix-related methods in BaseRule class; 5) src/sqlfluff/core/linter/linted_file.py - Implements fix application to files; 6) src/sqlfluff/core/parser/segments/base.py - Contains segment manipulation methods for fixes; 7) src/sqlfluff/core/parser/segments/raw.py - Implements raw segment fix handling; 8) src/sqlfluff/core/linter/linter.py - Orchestrates fix generation and application; 9) src/sqlfluff/core/rules/context.py - Provides context for fix generation; 10) src/sqlfluff/core/parser/helpers.py - Contains utilities for segment manipulation during fixes.", "score": null, "retrieved_content": [{"name": "test__fix__generate_source_patches", "is_method": false, "class_name": null, "parameters": ["tree", "templated_file", "expected_result", "caplog"], "calls": ["pytest.mark.parametrize", "caplog.at_level", "generate_source_patches", "RawSegment", "RawSegment", "BaseSegment", "BaseSegment", "BaseSegment", "PositionMarker", "PositionMarker", "FixPatch", "FixPatch", "FixPatch", "FixPatch", "FixPatch", "FixPatch", "slice", "slice", "slice", "slice", "slice", "slice", "RawSegment", "RawSegment", "RawSegment", "slice", "slice", "TemplateSegment", "RawSegment", "RawSegment", "RawSegment", "slice", "slice", "TemplateSegment", "RawSegment", "RawSegment", "RawSegment", "slice", "slice", "slice", "slice", "slice", "slice", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "SourceFix", "SourceFix", "slice", "slice", "slice", "slice"], "code_location": {"file": "fix_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 195, "end_line": 202}, "code_snippet": "def test__fix__generate_source_patches(tree, templated_file, expected_result, caplog):\n    \"\"\"Test generate_source_patches.\n\n    This is part of fix_string().\n    \"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.linter\"):\n        result = generate_source_patches(tree, templated_file)\n    assert result == expected_result\n", "type": "function"}, {"name": "LintFix", "docstring": "A class to hold a potential fix to a linting violation.\n\nArgs:\n    edit_type (:obj:`str`): One of `create_before`, `create_after`,\n        `replace`, `delete` to indicate the kind of fix this represents.\n    anchor (:obj:`BaseSegment`): A segment which represents\n        the *position* that this fix should be applied at. For deletions\n        it represents the segment to delete, for creations it implies the\n        position to create at (with the existing element at this position\n        to be moved *after* the edit), for a `replace` it implies the\n        segment to be replaced.\n    edit (iterable of :obj:`BaseSegment`, optional): For `replace` and\n        `create` fixes, this holds the iterable of segments to create\n        or replace at the given `anchor` point.\n    source (iterable of :obj:`BaseSegment`, optional): For `replace` and\n        `create` fixes, this holds iterable of segments that provided\n        code. IMPORTANT: The linter uses this to prevent copying material\n        from templated areas.", "methods": ["__init__", "is_just_source_edit", "__repr__", "to_dict", "__eq__", "delete", "replace", "create_before", "create_after", "get_fix_slices", "has_template_conflicts", "_raw_slices_from_templated_slices"], "attributes": [], "code_location": {"file": "fix.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 14, "end_line": 413}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "LintFix", "parameters": ["self", "edit_type", "anchor", "edit", "source"], "calls": ["ValueError", "ValueError", "all", "s.copy", "rules_logger.debug"], "code_location": {"file": "fix.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 35, "end_line": 88}, "code_snippet": "    def __init__(\n        self,\n        edit_type: str,\n        anchor: BaseSegment,\n        edit: Optional[Iterable[BaseSegment]] = None,\n        source: Optional[Iterable[BaseSegment]] = None,\n    ) -> None:\n        if edit_type not in (\n            \"create_before\",\n            \"create_after\",\n            \"replace\",\n            \"delete\",\n        ):  # pragma: no cover\n            raise ValueError(f\"Unexpected edit_type: {edit_type}\")\n        self.edit_type = edit_type\n        if not anchor:  # pragma: no cover\n            raise ValueError(\"Fixes must provide an anchor.\")\n        self.anchor = anchor\n        self.edit: Optional[list[BaseSegment]] = None\n        if edit is not None:\n            # Copy all the elements of edit to stop contamination.\n            # We're about to start stripping the position markers\n            # off some of the elements and we don't want to end up\n            # stripping the positions of the original elements of\n            # the parsed structure.\n            self.edit = [s.copy() for s in edit]\n            # Check that any edits don't have a position marker set.\n            # We should rely on realignment to make position markers.\n            # Strip position markers of anything enriched, otherwise things can get\n            # blurry\n            for seg in self.edit:\n                if seg.pos_marker:\n                    # Developer warning.\n                    rules_logger.debug(\n                        \"Developer Note: Edit segment found with preset position \"\n                        \"marker. These should be unset and calculated later.\"\n                    )\n                    seg.pos_marker = None\n            # Once stripped, we shouldn't replace any markers because\n            # later code may rely on them being accurate, which we\n            # can't guarantee with edits.\n        self.source = [seg for seg in source if seg.pos_marker] if source else []\n\n        # On creation of the fix we'll also validate the edits are non-trivial.\n        if self.edit_type in (\"create_before\", \"create_after\"):\n            assert self.edit, \"A create fix must have an edit.\"\n            # They should all have a non-zero raw.\n            assert all(\n                seg.raw for seg in self.edit\n            ), f\"Invalid edit found: {self.edit}.\"\n        elif self.edit_type == \"replace\":\n            assert (\n                self.edit != self.anchor\n            ), \"Fix created which replaces segment with itself.\"\n", "type": "function"}, {"name": "fix", "is_method": true, "class_name": "Linter", "parameters": ["self", "tree", "config", "fname", "templated_file"], "calls": ["self.get_rulepack", "self.lint_fix_parsed"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 931, "end_line": 950}, "code_snippet": "    def fix(\n        self,\n        tree: BaseSegment,\n        config: Optional[FluffConfig] = None,\n        fname: Optional[str] = None,\n        templated_file: Optional[\"TemplatedFile\"] = None,\n    ) -> tuple[BaseSegment, list[SQLBaseError]]:\n        \"\"\"Return the fixed tree and violations from lintfix when we're fixing.\"\"\"\n        config = config or self.config\n        rule_pack = self.get_rulepack(config=config)\n        fixed_tree, violations, _, _ = self.lint_fix_parsed(\n            tree,\n            config,\n            rule_pack,\n            fix=True,\n            fname=fname,\n            templated_file=templated_file,\n            formatter=self.formatter,\n        )\n        return fixed_tree, violations\n", "type": "function"}, {"name": "fix", "is_method": false, "class_name": null, "parameters": ["sql", "dialect", "rules", "exclude_rules", "config", "config_path", "fix_even_unparsable"], "calls": ["Linter", "linter.lint_string_wrapped", "get_simple_config", "cfg.get", "result.count_tmp_prs_errors", "fix_string"], "code_location": {"file": "simple.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/api", "start_line": 115, "end_line": 165}, "code_snippet": "def fix(\n    sql: str,\n    dialect: Optional[str] = None,\n    rules: Optional[list[str]] = None,\n    exclude_rules: Optional[list[str]] = None,\n    config: Optional[FluffConfig] = None,\n    config_path: Optional[str] = None,\n    fix_even_unparsable: Optional[bool] = None,\n) -> str:\n    \"\"\"Fix a SQL string.\n\n    Args:\n        sql (:obj:`str`): The SQL to be fixed.\n        dialect (:obj:`Optional[str]`, optional): A reference to the dialect of the SQL\n            to be fixed. Defaults to `ansi`.\n        rules (:obj:`Optional[list[str]`, optional): A subset of rule\n            references to fix for. Defaults to None.\n        exclude_rules (:obj:`Optional[list[str]`, optional): A subset of rule\n            references to avoid fixing for. Defaults to None.\n        config (:obj:`Optional[FluffConfig]`, optional): A configuration object\n            to use for the operation. Defaults to None.\n        config_path (:obj:`Optional[str]`, optional): A path to a .sqlfluff config,\n            which is only used if a `config` is not already provided.\n            Defaults to None.\n        fix_even_unparsable (:obj:`bool`, optional): Optional override for the\n            corresponding SQLFluff configuration value.\n\n    Returns:\n        :obj:`str` for the fixed SQL if possible.\n    \"\"\"\n    cfg = config or get_simple_config(\n        dialect=dialect,\n        rules=rules,\n        exclude_rules=exclude_rules,\n        config_path=config_path,\n    )\n    linter = Linter(config=cfg)\n\n    result = linter.lint_string_wrapped(sql, fix=True)\n    if fix_even_unparsable is None:\n        fix_even_unparsable = cfg.get(\"fix_even_unparsable\")\n    should_fix = True\n    if not fix_even_unparsable:\n        # If fix_even_unparsable wasn't set, check for templating or parse\n        # errors and suppress fixing if there were any.\n        _, num_filtered_errors = result.count_tmp_prs_errors()\n        if num_filtered_errors > 0:\n            should_fix = False\n    if should_fix:\n        sql = result.paths[0].files[0].fix_string()[0]\n    return sql\n", "type": "function"}, {"name": "apply_fixes", "is_method": false, "class_name": null, "parameters": ["segment", "dialect", "rule_code", "fixes", "fix_even_unparsable"], "calls": ["segment.invalidate_caches", "segment.is_raw", "fixes.pop", "list", "apply_fixes", "range", "len", "range", "segment.__class__", "seg_buffer.append", "seg_fixes.reverse", "fixes_applied.append", "linter_logger.debug", "segment._position_segments", "len", "segment._is_code_or_meta", "len", "segment._is_code_or_meta", "hasattr", "hasattr", "len", "seg_buffer.append", "seg_buffer.append", "seg_buffer.append", "tuple", "segment._position_segments", "err.add_note", "new_seg.validate_segment_with_reparse", "len", "tuple", "getattr", "len"], "code_location": {"file": "fix.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 107, "end_line": 335}, "code_snippet": "def apply_fixes(\n    segment: BaseSegment,\n    dialect: \"Dialect\",\n    rule_code: str,\n    fixes: dict[int, AnchorEditInfo],\n    fix_even_unparsable: bool = False,\n) -> tuple[\"BaseSegment\", list[\"BaseSegment\"], list[\"BaseSegment\"], bool]:\n    \"\"\"Apply a dictionary of fixes to this segment.\n\n    Used in to apply fixes found in linting. If a segment remains unchanged\n    then the original is returned, but if any changes are made to it, or any\n    of it's child segments, then it returns a copy rather than mutating the\n    original.\n\n    Most fixes are usually applied when this method is called on their parent\n    segment, this is because that's where we can insert or move segments relative\n    to the anchor specified in the fix. This has the implication that if the\n    method is called on a `RawSegment`, then no changes will be applied, because\n    a `RawSegment` never has child segments.\n\n    After fixing, it calls `validate_segment_with_reparse` on the segment to\n    check that the segment still parses after any changes are made. The result\n    of this is returned as a boolean in the last element of the return tuple.\n    As the function recurses, if an inner element doesn't parse after fixing,\n    then the outer segment will also be checked, and if found to parse successfully\n    then the method returns `True` as valid. This is because sometimes the fixes\n    change the structure enough that a wider reparse is necessary.\n\n    Because of this validity checking, any unparsable sections are assumed\n    unfixable (because we won't know if we're corrupting the SQL). The method\n    will therefore return early without applying any fixes if the segment it's\n    called on is unparsable (because we already know that validation check will\n    fail already).\n\n    If `fix_even_unparsable` is True, then we will still apply fixes to unparsable\n    sections, but will do so *without validation*. That means that the final\n    element of the return value will always return `True`, so that we don't interrupt\n    the validity checking of any outer (parsable) sections.\n    \"\"\"\n    if not fixes or segment.is_raw():\n        return segment, [], [], True\n\n    seg_buffer = []\n    before = []\n    after = []\n    fixes_applied: list[LintFix] = []\n    requires_validate = False\n\n    for seg in segment.segments:\n        # Look for uuid match.\n        # This handles potential positioning ambiguity.\n        anchor_info: Optional[AnchorEditInfo] = fixes.pop(seg.uuid, None)\n\n        if anchor_info is None:\n            # No fix matches here, just add the segment and move on.\n            seg_buffer.append(seg)\n            continue\n\n        # Otherwise there is a fix match.\n        seg_fixes = anchor_info.fixes\n        if (\n            len(seg_fixes) == 2 and seg_fixes[0].edit_type == \"create_after\"\n        ):  # pragma: no cover\n            # Must be create_before & create_after. Swap so the\n            # \"before\" comes first.\n            seg_fixes.reverse()\n\n        for f in anchor_info.fixes:\n            assert f.anchor.uuid == seg.uuid\n            fixes_applied.append(f)\n            linter_logger.debug(\n                \"Matched fix for %s against segment: %s -> %s\",\n                rule_code,\n                f,\n                seg,\n            )\n\n            # Deletes are easy.\n            if f.edit_type == \"delete\":\n                # We're just getting rid of this segment.\n                requires_validate = True\n                # NOTE: We don't add the segment in this case.\n                continue\n\n            # Otherwise it must be a replace or a create.\n            assert f.edit_type in (\n                \"replace\",\n                \"create_before\",\n                \"create_after\",\n            ), f\"Unexpected edit_type: {f.edit_type!r} in {f!r}\"\n\n            if f.edit_type == \"create_after\" and len(anchor_info.fixes) == 1:\n                # in the case of a creation after that is not part\n                # of a create_before/create_after pair, also add\n                # this segment before the edit.\n                seg_buffer.append(seg)\n\n            # We're doing a replacement (it could be a single\n            # segment or an iterable)\n            assert f.edit, f\"Edit {f.edit_type!r} requires `edit`.\"\n            consumed_pos = False\n            for s in f.edit:\n                seg_buffer.append(s)\n                # If one of them has the same raw representation\n                # then the first that matches gets to take the\n                # original position marker.\n                if f.edit_type == \"replace\" and s.raw == seg.raw and not consumed_pos:\n                    seg_buffer[-1].pos_marker = seg.pos_marker\n                    consumed_pos = True\n\n            # If we're just editing a segment AND keeping the type the\n            # same then no need to validate. Otherwise we should\n            # trigger a validation (e.g. for creations or\n            # multi-replace).\n            if not (\n                f.edit_type == \"replace\"\n                and len(f.edit) == 1\n                and f.edit[0].class_types == seg.class_types\n            ):\n                requires_validate = True\n\n            if f.edit_type == \"create_before\":\n                # in the case of a creation before, also add this\n                # segment on the end\n                seg_buffer.append(seg)\n\n    # Invalidate any caches\n    segment.invalidate_caches()\n\n    # If any fixes applied, do an intermediate reposition. When applying\n    # fixes to children and then trying to reposition them, that recursion\n    # may rely on the parent having already populated positions for any\n    # of the fixes applied there first. This ensures those segments have\n    # working positions to work with.\n    if fixes_applied:\n        assert segment.pos_marker\n        seg_buffer = list(\n            segment._position_segments(tuple(seg_buffer), parent_pos=segment.pos_marker)\n        )\n\n    # Then recurse (i.e. deal with the children) (Requeueing)\n    seg_queue = seg_buffer\n    seg_buffer = []\n    for seg in seg_queue:\n        s, pre, post, validated = apply_fixes(seg, dialect, rule_code, fixes)\n        # 'before' and 'after' will usually be empty. Only used when\n        # lower-level fixes left 'seg' with non-code (usually\n        # whitespace) segments as the first or last children. This is\n        # generally not allowed (see the can_start_end_non_code field),\n        # and these segments need to be \"bubbled up\" the tree.\n        seg_buffer += pre + [s] + post\n        # If we fail to validate a child segment, make sure to validate this\n        # segment.\n        if not validated:\n            requires_validate = True\n\n    # Most correct whitespace positioning will have already been handled\n    # _however_, the exception is `replace` edits which match start or\n    # end with whitespace. We also need to handle any leading or trailing\n    # whitespace ejected from the any fixes applied to child segments.\n    # Here we handle those by checking the start and end of the resulting\n    # segment sequence for whitespace.\n    # If we're left with any non-code at the end, trim them off and pass them\n    # up to the parent segment for handling.\n    if not segment.can_start_end_non_code:\n        _idx = 0\n        for _idx in range(0, len(seg_buffer)):\n            if segment._is_code_or_meta(seg_buffer[_idx]):\n                break\n        before = seg_buffer[:_idx]\n        seg_buffer = seg_buffer[_idx:]\n\n        _idx = len(seg_buffer)\n        for _idx in range(len(seg_buffer), 0, -1):\n            if segment._is_code_or_meta(seg_buffer[_idx - 1]):\n                break\n        after = seg_buffer[_idx:]\n        seg_buffer = seg_buffer[:_idx]\n\n    # Reform into a new segment\n    assert segment.pos_marker\n    try:\n        new_seg = segment.__class__(\n            # Realign the segments within\n            segments=segment._position_segments(\n                tuple(seg_buffer), parent_pos=segment.pos_marker\n            ),\n            pos_marker=segment.pos_marker,\n            # Pass through any additional kwargs\n            **{k: getattr(segment, k) for k in segment.additional_kwargs},\n        )\n    except AssertionError as err:  # pragma: no cover\n        # An AssertionError on creating a new segment is likely a whitespace\n        # check fail. If possible add information about the fixes we tried to\n        # apply, before re-raising.\n        # NOTE: only available in python 3.11+.\n        if hasattr(err, \"add_note\"):\n            err.add_note(f\" After applying fixes: {fixes_applied}.\")\n        raise err\n\n    # Handle any necessary validation.\n    if requires_validate:\n        # Was it already unparsable?\n        if \"unparsable\" in segment.descendant_type_set | segment.class_types:\n            if fix_even_unparsable:\n                # If we're fixing even unparsable sections, there's no point trying\n                # to validate, it will always fail. We may still want to validate\n                # other sections of the file though, so we should just declare *this*\n                # part of the file to be all good.\n                validated = True\n            else:\n                # It was already unparsable, but we're being asked to validate.\n                # Don't any apply fixes from within this region and just return the\n                # original segment.\n                return segment, [], [], True\n        # Otherwise only validate if there's a match_grammar. Otherwise we may get\n        # strange results (for example with the BracketedSegment).\n        elif hasattr(new_seg, \"match_grammar\"):\n            validated = new_seg.validate_segment_with_reparse(dialect)\n    else:\n        validated = not requires_validate\n    # Return the new segment and any non-code that needs to bubble up\n    # the tree.\n    # NOTE: We pass on whether this segment has been validated. It's\n    # very possible that our parsing here may fail depending on the\n    # type of segment that has been replaced, but if not we rely on\n    # a parent segment still being valid. If we get all the way up\n    # to the root and it's still not valid - that's a problem.\n    return new_seg, before, after, validated\n", "type": "function"}, {"name": "_get_fix", "is_method": true, "class_name": "Rule_CP03", "parameters": ["self", "segment", "fixed_raw"], "calls": ["_get_fix", "super"], "code_location": {"file": "CP03.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/capitalisation", "start_line": 53, "end_line": 54}, "code_snippet": "    def _get_fix(self, segment: BaseSegment, fixed_raw: str) -> LintFix:\n        return super()._get_fix(segment, fixed_raw)\n", "type": "function"}, {"name": "test__linted_file__build_up_fixed_source_string", "is_method": false, "class_name": null, "parameters": ["source_slices", "source_patches", "raw_source_string", "expected_result", "caplog"], "calls": ["pytest.mark.parametrize", "caplog.at_level", "LintedFile._build_up_fixed_source_string", "slice", "slice", "slice", "slice", "FixPatch", "slice", "slice", "slice", "FixPatch", "slice", "slice", "slice", "FixPatch", "slice", "slice", "slice", "FixPatch", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice"], "code_location": {"file": "linted_file_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 51, "end_line": 62}, "code_snippet": "def test__linted_file__build_up_fixed_source_string(\n    source_slices, source_patches, raw_source_string, expected_result, caplog\n):\n    \"\"\"Test _build_up_fixed_source_string.\n\n    This is part of fix_string().\n    \"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.linter\"):\n        result = LintedFile._build_up_fixed_source_string(\n            source_slices, source_patches, raw_source_string\n        )\n    assert result == expected_result\n", "type": "function"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}, {"name": "test__cli__fix_multiple_errors_no_show_errors", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 2292, "end_line": 2305}, "code_snippet": "def test__cli__fix_multiple_errors_no_show_errors():\n    \"\"\"Test the fix output.\"\"\"\n    invoke_assert_code(\n        ret_code=1,\n        args=[\n            fix,\n            [\n                \"--check\",  # Run in check mode to get the confirmation.\n                \"--disable-progress-bar\",\n                \"test/fixtures/linter/multiple_sql_errors.sql\",\n            ],\n        ],\n        assert_stdout_contains=multiple_expected_output,\n    )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.32819437980651855}
{"question": "Where in SQLFluff's codebase is the \"parse\" method defined?", "answer": null, "relative_code_list": null, "ground_truth": "The \"parse\" method in SQLFluff is defined in several key locations in the codebase. The main parse method implementations are located in: 1) src/sqlfluff/core/parser/parser.py - Contains the main Parser.parse() method that orchestrates the parsing process; 2) src/sqlfluff/core/linter/linter.py - Contains Linter.parse_string() and Linter.parse_path() methods for parsing SQL strings and files; 3) src/sqlfluff/core/parser/segments/base.py - Contains BaseSegment.parse() method for parsing individual segments; 4) src/sqlfluff/core/parser/segments/file.py - Contains BaseFileSegment.root_parse() method for parsing complete files; 5) src/sqlfluff/core/parser/grammar.py - Contains grammar parsing methods for different grammar types; 6) src/sqlfluff/core/parser/parsers.py - Contains parser classes with parse() methods for individual segment types; 7) src/sqlfluff/core/linter/linter.py - Contains _parse_tokens() static method for parsing lexed tokens; 8) src/sqlfluff/core/parser/context.py - Contains parsing context management for the parse process; 9) src/sqlfluff/core/parser/match_result.py - Contains methods for handling parse results; 10) src/sqlfluff/api/simple.py - Contains parse() function for the simple API interface.", "score": null, "retrieved_content": [{"name": "SQLParseError", "docstring": "An error which occurred during parsing.\n\nArgs:\n    segment (:obj:`BaseSegment`, optional): The segment which is relevant\n        for the failure in parsing. This is likely to be a subclass of\n        `BaseSegment` rather than the parent class itself. This is mostly\n        used for logging and for referencing position.", "methods": ["__init__", "__reduce__", "to_dict"], "attributes": ["_code", "_identifier"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 185, "end_line": 246}, "type": "class"}, {"name": "parse", "is_method": false, "class_name": null, "parameters": ["sql", "dialect", "config", "config_path"], "calls": ["Linter", "linter.parse_string", "parsed.root_variant", "root_variant.tree.as_record", "get_simple_config", "APIParsingError"], "code_location": {"file": "simple.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/api", "start_line": 168, "end_line": 212}, "code_snippet": "def parse(\n    sql: str,\n    dialect: Optional[str] = None,\n    config: Optional[FluffConfig] = None,\n    config_path: Optional[str] = None,\n) -> dict[str, Any]:\n    \"\"\"Parse a SQL string.\n\n    Args:\n        sql (:obj:`str`): The SQL to be parsed.\n        dialect (:obj:`Optional[str]`, optional): A reference to the dialect of the SQL\n            to be parsed. Defaults to `ansi`.\n        config (:obj:`Optional[FluffConfig]`, optional): A configuration object\n            to use for the operation. Defaults to None.\n        config_path (:obj:`Optional[str]`, optional): A path to a .sqlfluff config,\n            which is only used if a `config` is not already provided.\n            Defaults to None.\n\n    Returns:\n        :obj:`Dict[str, Any]` JSON containing the parsed structure.\n\n    Note:\n        In the case of multiple potential variants from the raw source file\n        only the first variant is returned by the simple API. For access to\n        the other variants, use the underlying main API directly.\n    \"\"\"\n    cfg = config or get_simple_config(\n        dialect=dialect,\n        config_path=config_path,\n    )\n    linter = Linter(config=cfg)\n\n    parsed = linter.parse_string(sql)\n    # If we encounter any parsing errors, raise them in a combined issue.\n    violations = parsed.violations\n    if violations:\n        raise APIParsingError(violations)\n    # Return a JSON representation of the parse tree.\n    # NOTE: For the simple API - only a single variant is returned.\n    root_variant = parsed.root_variant()\n    assert root_variant, \"Files parsed without violations must have a valid variant\"\n    assert root_variant.tree, \"Files parsed without violations must have a valid tree\"\n    record = root_variant.tree.as_record(show_raw=True)\n    assert record\n    return record\n", "type": "function"}, {"name": "parse_string", "is_method": true, "class_name": "Linter", "parameters": ["self", "in_str", "fname", "config", "encoding", "parse_statistics"], "calls": ["copy", "config.process_raw_file_for_config", "self.render_string", "self.parse_rendered", "self.formatter.dispatch_template_header", "self.formatter.dispatch_parse_header"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 902, "end_line": 929}, "code_snippet": "    def parse_string(\n        self,\n        in_str: str,\n        fname: str = \"<string>\",\n        config: Optional[FluffConfig] = None,\n        encoding: str = \"utf-8\",\n        parse_statistics: bool = False,\n    ) -> ParsedString:\n        \"\"\"Parse a string.\"\"\"\n        violations: list[SQLBaseError] = []\n\n        # Dispatch the output for the template header (including the config diff)\n        if self.formatter:\n            self.formatter.dispatch_template_header(fname, self.config, config)\n\n        # Just use the local config from here:\n        config = (config or self.config).copy()\n\n        # Scan the raw file for config commands.\n        config.process_raw_file_for_config(in_str, fname)\n        rendered = self.render_string(in_str, fname, config, encoding)\n        violations += rendered.templater_violations\n\n        # Dispatch the output for the parse header\n        if self.formatter:\n            self.formatter.dispatch_parse_header(fname)\n\n        return self.parse_rendered(rendered, parse_statistics=parse_statistics)\n", "type": "function"}, {"name": "_parse_tokens", "is_method": true, "class_name": "Linter", "parameters": ["tokens", "config", "fname", "parse_statistics"], "calls": ["Parser", "linter_logger.info", "linter_logger.info", "parsed.iter_unparsables", "parser.parse", "format", "violations.append", "linter_logger.info", "linter_logger.info", "tuple", "linter_logger.info", "violations.append", "parsed.stringify", "SQLParseError", "unparsable.stringify", "format", "len"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 231, "end_line": 281}, "code_snippet": "    def _parse_tokens(\n        tokens: Sequence[BaseSegment],\n        config: FluffConfig,\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> tuple[Optional[BaseSegment], list[SQLParseError]]:\n        parser = Parser(config=config)\n        violations = []\n        # Parse the file and log any problems\n        try:\n            parsed: Optional[BaseSegment] = parser.parse(\n                # Regardless of how the sequence was passed in, we should\n                # coerce it to a tuple here, before we head deeper into\n                # the parsing process.\n                tuple(tokens),\n                fname=fname,\n                parse_statistics=parse_statistics,\n            )\n        except SQLParseError as err:\n            linter_logger.info(\"PARSING FAILED! : %s\", err)\n            violations.append(err)\n            return None, violations\n\n        if parsed is None:  # pragma: no cover\n            return None, violations\n\n        linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n        linter_logger.info(\"\\n\" + parsed.stringify())\n        # We may succeed parsing, but still have unparsable segments. Extract them\n        # here.\n        for unparsable in parsed.iter_unparsables():\n            # No exception has been raised explicitly, but we still create one here\n            # so that we can use the common interface\n            assert unparsable.pos_marker\n            violations.append(\n                SQLParseError(\n                    \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n                    \"{1!r}\".format(\n                        unparsable.pos_marker.working_loc,\n                        (\n                            unparsable.raw\n                            if len(unparsable.raw) < 40\n                            else unparsable.raw[:40] + \"...\"\n                        ),\n                    ),\n                    segment=unparsable,\n                )\n            )\n            linter_logger.info(\"Found unparsable segment...\")\n            linter_logger.info(unparsable.stringify())\n        return parsed, violations\n", "type": "function"}, {"name": "ParseExample", "docstring": "A tuple representing an example SQL file to parse.", "methods": [], "attributes": [], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test", "start_line": 34, "end_line": 38}, "type": "class"}, {"name": "test__parser__parse_error", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "lnt.parse_string", "isinstance", "len", "violation.desc", "parsed.tree.stringify"], "code_location": {"file": "parse_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/parser", "start_line": 32, "end_line": 51}, "code_snippet": "def test__parser__parse_error():\n    \"\"\"Test that SQLParseError is raised for unparsable section.\"\"\"\n    in_str = \"SELECT ;\"\n    lnt = Linter(dialect=\"ansi\")\n    parsed = lnt.parse_string(in_str)\n\n    assert len(parsed.violations) == 1\n    violation = parsed.violations[0]\n    assert isinstance(violation, SQLParseError)\n    assert violation.desc() == \"Line 1, Position 1: Found unparsable section: 'SELECT'\"\n\n    # Check that the expected labels work for logging.\n    # TODO: This is more specific that in previous iterations, but we could\n    # definitely make this easier to read.\n    assert (\n        'Expected: \"<Delimited: '\n        \"[<Ref: 'SelectClauseElementSegment'>]> \"\n        \"after <WordSegment: ([L:  1, P:  1]) 'SELECT'>. \"\n        \"Found nothing.\"\n    ) in parsed.tree.stringify()\n", "type": "function"}, {"name": "test__api__parse_fail", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.parse", "pytest.fail", "isinstance", "len", "str"], "code_location": {"file": "simple_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 488, "end_line": 504}, "code_snippet": "def test__api__parse_fail():\n    \"\"\"Basic failure mode of parse functionality.\"\"\"\n    try:\n        sqlfluff.parse(\"Select (1 + 2 +++) FROM mytable as blah blah\")\n        pytest.fail(\"sqlfluff.parse should have raised an exception.\")\n    except Exception as err:\n        # Check it's the right kind of exception\n        assert isinstance(err, sqlfluff.api.APIParsingError)\n        # Check there are two violations in there.\n        assert len(err.violations) == 2\n        # Check it prints nicely.\n        assert (\n            str(err)\n            == \"\"\"Found 2 issues while parsing string.\nLine 1, Position 15: Found unparsable section: '+++'\nLine 1, Position 41: Found unparsable section: 'blah'\"\"\"\n        )\n", "type": "function"}, {"name": "test__api__parse_exceptions", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.parse", "pytest.raises", "sqlfluff.parse", "pytest.raises", "sqlfluff.parse"], "code_location": {"file": "simple_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 670, "end_line": 680}, "code_snippet": "def test__api__parse_exceptions():\n    \"\"\"Test parse behaviour with errors.\"\"\"\n    # Parsable content\n    result = sqlfluff.parse(\"SELECT 1\")\n    assert result\n    # Templater fail\n    with pytest.raises(APIParsingError):\n        sqlfluff.parse('SELECT {{ 1 > \"a\"}}')\n    # Templater success but parsing fail\n    with pytest.raises(APIParsingError):\n        sqlfluff.parse(\"THIS IS NOT SQL\")\n", "type": "function"}, {"name": "print_out_violations_and_timing", "is_method": true, "class_name": "OutputStreamFormatter", "parameters": ["self", "output_stream", "bench", "code_only", "total_time", "verbose", "parsed_strings"], "calls": ["TimingSummary", "timing.add", "len", "parsed_string.root_variant", "len", "output_stream.write", "output_stream.write", "timing.summary", "output_stream.write", "output_stream.write", "output_stream.write", "output_stream.write", "output_stream.write", "output_stream.write", "self.cli_table", "output_stream.write", "output_stream.write", "self.colorize", "output_stream.write", "output_stream.write", "enumerate", "self.format_violation", "self.format_dialect_warning", "self.cli_table", "self.cli_table", "root_variant.tree.stringify", "self.colorize", "output_stream.write", "parsed_string.config.get", "parsed_string.time_dict.items", "items", "self.colorize", "output_stream.write", "output_stream.write", "variant.tree.stringify", "self.colorize"], "code_location": {"file": "formatters.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 638, "end_line": 710}, "code_snippet": "    def print_out_violations_and_timing(\n        self,\n        output_stream: OutputStream,\n        bench: bool,\n        code_only: bool,\n        total_time: float,\n        verbose: int,\n        parsed_strings: list[ParsedString],\n    ) -> int:\n        \"\"\"Used by human formatting during the `sqlfluff parse` command.\"\"\"\n        violations_count = 0\n        timing = TimingSummary()\n\n        for parsed_string in parsed_strings:\n            timing.add(parsed_string.time_dict)\n\n            num_variants = len(parsed_string.parsed_variants)\n            root_variant = parsed_string.root_variant()\n            if not root_variant:\n                # TODO: Make this prettier\n                output_stream.write(\n                    self.colorize(\"...Failed to Parse...\", Color.red)\n                )  # pragma: no cover\n            elif num_variants == 1:\n                # Backward compatible single parse\n                assert root_variant.tree\n                output_stream.write(root_variant.tree.stringify(code_only=code_only))\n            else:\n                # Multi variant parse setup.\n                output_stream.write(\n                    self.colorize(\n                        f\"SQLFluff parsed {num_variants} variants of this file\",\n                        Color.blue,\n                    )\n                )\n                for idx, variant in enumerate(parsed_string.parsed_variants):\n                    output_stream.write(\n                        self.colorize(\n                            f\"Variant {idx + 1}:\",\n                            Color.blue,\n                        )\n                    )\n                    if variant.tree:\n                        output_stream.write(variant.tree.stringify(code_only=code_only))\n                    else:  # pragma: no cover\n                        output_stream.write(\n                            self.colorize(\"...Failed to Parse...\", Color.red)\n                        )\n\n            violations = parsed_string.violations\n            violations_count += len(violations)\n            if violations:\n                output_stream.write(\"==== parsing violations ====\")  # pragma: no cover\n            for v in violations:\n                output_stream.write(self.format_violation(v))  # pragma: no cover\n            if violations:\n                output_stream.write(\n                    self.format_dialect_warning(parsed_string.config.get(\"dialect\"))\n                )\n\n            if verbose >= 2:\n                output_stream.write(\"==== timings ====\")\n                output_stream.write(self.cli_table(parsed_string.time_dict.items()))\n\n        if verbose >= 2 or bench:\n            output_stream.write(\"==== overall timings ====\")\n            output_stream.write(self.cli_table([(\"Clock time\", total_time)]))\n            timing_summary = timing.summary()\n            for step in timing_summary:\n                output_stream.write(f\"=== {step} ===\")\n                output_stream.write(self.cli_table(timing_summary[step].items()))\n\n        return violations_count\n", "type": "function"}, {"name": "test__api__parse_string", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.parse", "isinstance", "open", "json.load"], "code_location": {"file": "simple_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 473, "end_line": 485}, "code_snippet": "def test__api__parse_string():\n    \"\"\"Basic checking of parse functionality.\"\"\"\n    parsed = sqlfluff.parse(my_bad_query)\n\n    # Check a JSON object is returned.\n    assert isinstance(parsed, dict)\n\n    # Load in expected result.\n    with open(\"test/fixtures/api/parse_test/parse_test.json\", \"r\") as f:\n        expected_parsed = json.load(f)\n\n    # Compare JSON from parse to expected result.\n    assert parsed == expected_parsed\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.33608269691467285}
{"question": "Where are SQLFluff's BaseRule class definitions located?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's BaseRule class definitions are located in several key locations in the codebase. The main BaseRule implementations are located in: 1) src/sqlfluff/core/rules/base.py - Contains the main BaseRule class definition and RuleMetaclass; 2) src/sqlfluff/core/rules/ruleset.py - Contains RuleSet class for managing collections of rules; 3) src/sqlfluff/core/rules/context.py - Contains rule context and evaluation utilities; 4) src/sqlfluff/core/rules/flow.py - Contains rule flow control and execution logic; 5) src/sqlfluff/core/rules/analysis.py - Contains rule analysis and validation utilities; 6) src/sqlfluff/core/rules/config_info.py - Contains rule configuration management; 7) src/sqlfluff/core/rules/__init__.py - Contains rule module initialization and imports; 8) src/sqlfluff/core/rules/fix.py - Contains fix-related rule utilities; 9) src/sqlfluff/core/rules/code_0100.py through code_9999.py - Contains specific rule implementations that inherit from BaseRule; 10) src/sqlfluff/core/rules/rule_helpers.py - Contains helper functions for rule development.", "score": null, "retrieved_content": [{"name": "SQLFluffRule", "docstring": "SQLFluff rule directive for sphinx.\n\nRule directives can be used as shown below.\n\n.. code-block:: rst\n\n    .. sqlfluff:rule:: AM01\n                       ambiguous.distinct\n\n        Write the documentation for the rule here.\n\nTo cross reference (i.e. refer to) objects defined like this\nboth the code and name reference is available:\n\n.. code-block:: rst\n\n    :sqlfluff:ref:`CP02`\n    :sqlfluff:ref:`capitalisation.identifiers`", "methods": ["handle_signature", "add_target_and_index", "_object_hierarchy_parts", "_toc_entry_name"], "attributes": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 10, "end_line": 79}, "type": "class"}, {"name": "BaseRule", "docstring": "The base class for a rule.\n\nArgs:\n    code (:obj:`str`): The identifier for this rule, used in inclusion\n        or exclusion.\n    description (:obj:`str`): A human readable description of what this\n        rule does. It will be displayed when any violations are found.", "methods": ["__init__", "get_config_ref", "_eval", "crawl", "_log_critical_errors", "_process_lint_result", "filter_meta", "get_parent_of", "discard_unsafe_fixes", "_adjust_anchors_for_fixes", "_choose_anchor_segment"], "attributes": ["_check_docstring", "_works_on_unparsable", "_adjust_anchors", "targets_templated", "template_safe_fixes", "lint_phase", "is_fix_compatible", "split_comma_separated_string"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 364, "end_line": 834}, "type": "class"}, {"name": "SQLLintError", "docstring": "An error which occurred during linting.\n\nIn particular we reference the rule here to do extended logging based on\nthe rule in question which caused the fail.\n\nArgs:\n    segment (:obj:`BaseSegment`, optional): The segment which is relevant\n        for the failure in parsing. This is likely to be a subclass of\n        `BaseSegment` rather than the parent class itself. This is mostly\n        used for logging and for referencing position.", "methods": ["__init__", "__reduce__", "to_dict", "fixable", "rule_code", "rule_name", "source_signature", "__repr__"], "attributes": ["_identifier"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 249, "end_line": 383}, "type": "class"}, {"name": "RuleMetaclass", "docstring": "The metaclass for rules.\n\nThis metaclass provides provides auto-enrichment of the\nrule docstring so that examples, groups, aliases and\nnames are added.\n\nThe reason we enrich the docstring is so that it can be\npicked up by autodoc and all be displayed in the sqlfluff\ndocs.", "methods": ["_populate_code_and_description", "_populate_docstring", "__new__"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 151, "end_line": 361}, "type": "class"}, {"name": "get_rules_from_path", "is_method": false, "class_name": null, "parameters": ["rules_path", "base_module"], "calls": ["os.path.abspath", "sorted", "os.path.join", "glob", "import_module", "rules.append", "os.path.dirname", "os.path.splitext", "getattr", "os.path.basename", "AttributeError"], "code_location": {"file": "loader.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 12, "end_line": 43}, "code_snippet": "def get_rules_from_path(\n    # All rule files are expected in the format of L*.py\n    rules_path: str = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), \"../../rules\", \"L*.py\")\n    ),\n    base_module: str = \"sqlfluff.rules\",\n) -> list[type[\"BaseRule\"]]:\n    \"\"\"Reads all of the Rule classes from a path into a list.\"\"\"\n    # Create a rules dictionary for importing in\n    # sqlfluff/src/sqlfluff/core/rules/__init__.py\n    rules = []\n\n    for module in sorted(glob(rules_path)):\n        # Manipulate the module path to extract the filename without the .py\n        rule_id = os.path.splitext(os.path.basename(module))[0]\n        # All rule classes are expected in the format of Rule_L*\n        rule_class_name = f\"Rule_{rule_id}\"\n        # NOTE: We import the module outside of the try clause to\n        # properly catch any import errors.\n        rule_module = import_module(f\"{base_module}.{rule_id}\")\n        try:\n            rule_class = getattr(rule_module, rule_class_name)\n        except AttributeError as e:\n            raise AttributeError(\n                \"Rule classes must be named in the format of Rule_*. \"\n                f\"[{rule_class_name}]\"\n            ) from e\n        # Add the rules to the rules dictionary for\n        # sqlfluff/src/sqlfluff/core/rules/__init__.py\n        rules.append(rule_class)\n\n    return rules\n", "type": "function"}, {"name": "SQLBaseError", "docstring": "Base Error Class for all violations.", "methods": ["__init__", "__eq__", "__reduce__", "fixable", "rule_code", "rule_name", "desc", "to_dict", "check_tuple", "source_signature", "ignore_if_in", "warning_if_in"], "attributes": ["_identifier", "_warning"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 36, "end_line": 150}, "type": "class"}, {"name": "SQLFluffDomain", "docstring": "SQLFluff domain.", "methods": ["get_full_qualified_name", "get_objects", "resolve_xref", "add_rule"], "attributes": ["name", "label", "object_types", "roles", "directives", "initial_data"], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 82, "end_line": 142}, "type": "class"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}, {"name": "RulePack", "docstring": "A bundle of rules to be applied.\n\nThis contains a set of rules, post filtering but also contains the mapping\nrequired to interpret any noqa messages found in files.\n\nThe reason for this object is that rules are filtered and instantiated\ninto this pack in the main process when running in multi-processing mode so\nthat user defined rules can be used without reference issues.\n\nAttributes:\n    rules (:obj:`list` of :obj:`BaseRule`): A filtered list of instantiated\n        rules to be applied to a given file.\n    reference_map (:obj:`dict`): A mapping of rule references to the codes\n        they refer to, e.g. `{\"my_ref\": {\"LT01\", \"LT02\"}}`. The references\n        (i.e. the keys) may be codes, groups, aliases or names. The values\n        of the mapping are sets of rule codes *only*. This object acts as\n        a lookup to be able to translate selectors (which may contain\n        diverse references) into a consolidated list of rule codes. This\n        mapping contains the full set of rules, rather than just the filtered\n        set present in the `rules` attribute.", "methods": ["codes"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 850, "end_line": 878}, "type": "class"}, {"name": "Rule_L000", "docstring": "Test std rule import.", "methods": [], "attributes": ["groups"], "code_location": {"file": "L000.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/fixtures/rules/custom", "start_line": 4, "end_line": 8}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.34439969062805176}
{"question": "Where is the \"lint\" method defined in SQLFluff's class hierarchy?", "answer": null, "relative_code_list": null, "ground_truth": "The \"lint\" method in SQLFluff's class hierarchy is defined in several key locations. The main lint method implementations are located in: 1) src/sqlfluff/core/linter/linter.py - Contains the main Linter.lint() method that orchestrates the entire linting process; 2) src/sqlfluff/core/rules/base.py - Contains BaseRule._eval() method which is the core linting logic for individual rules; 3) src/sqlfluff/core/linter/linted_file.py - Contains LintedFile.lint() method for linting individual files; 4) src/sqlfluff/core/linter/linted_dir.py - Contains LintedDir.lint() method for linting directories; 5) src/sqlfluff/core/rules/ruleset.py - Contains RuleSet.lint() method for applying rule sets; 6) src/sqlfluff/cli/commands.py - Contains lint command implementation; 7) src/sqlfluff/api/simple.py - Contains lint() function for the simple API; 8) src/sqlfluff/core/rules/context.py - Contains linting context management; 9) src/sqlfluff/core/rules/flow.py - Contains linting flow control logic; 10) src/sqlfluff/core/rules/analysis.py - Contains linting analysis utilities.", "score": null, "retrieved_content": [{"name": "SQLLintError", "docstring": "An error which occurred during linting.\n\nIn particular we reference the rule here to do extended logging based on\nthe rule in question which caused the fail.\n\nArgs:\n    segment (:obj:`BaseSegment`, optional): The segment which is relevant\n        for the failure in parsing. This is likely to be a subclass of\n        `BaseSegment` rather than the parent class itself. This is mostly\n        used for logging and for referencing position.", "methods": ["__init__", "__reduce__", "to_dict", "fixable", "rule_code", "rule_name", "source_signature", "__repr__"], "attributes": ["_identifier"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 249, "end_line": 383}, "type": "class"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "SQLFluffDriver", "parameters": ["self"], "calls": ["__init__", "super", "s.encode", "sys.getfilesystemencoding"], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 25, "end_line": 34}, "code_snippet": "    def __init__(self) -> None:\n        super().__init__(\n            [sys.executable, \"-m\", \"sqlfluff.cli.commands\"],\n            [\".sql\"],\n            [\n                s.encode(sys.getfilesystemencoding())\n                for s in [\"sqlfluff\", \"lint\", \"--format=json\"]\n            ],\n            exit_codes=[0, 1],\n        )\n", "type": "function"}, {"name": "lint", "is_method": false, "class_name": null, "parameters": ["sql", "dialect", "rules", "exclude_rules", "config", "config_path"], "calls": ["Linter", "linter.lint_string_wrapped", "result.as_records", "get_simple_config"], "code_location": {"file": "simple.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/api", "start_line": 74, "end_line": 112}, "code_snippet": "def lint(\n    sql: str,\n    dialect: Optional[str] = None,\n    rules: Optional[list[str]] = None,\n    exclude_rules: Optional[list[str]] = None,\n    config: Optional[FluffConfig] = None,\n    config_path: Optional[str] = None,\n) -> list[dict[str, Any]]:\n    \"\"\"Lint a SQL string.\n\n    Args:\n        sql (:obj:`str`): The SQL to be linted.\n        dialect (:obj:`Optional[str]`, optional): A reference to the dialect of the SQL\n            to be linted. Defaults to `ansi`.\n        rules (:obj:`Optional[list[str]`, optional): A list of rule\n            references to lint for. Defaults to None.\n        exclude_rules (:obj:`Optional[list[str]`, optional): A list of rule\n            references to avoid linting for. Defaults to None.\n        config (:obj:`Optional[FluffConfig]`, optional): A configuration object\n            to use for the operation. Defaults to None.\n        config_path (:obj:`Optional[str]`, optional): A path to a .sqlfluff config,\n            which is only used if a `config` is not already provided.\n            Defaults to None.\n\n    Returns:\n        :obj:`list[dict[str, Any]]` for each violation found.\n    \"\"\"\n    cfg = config or get_simple_config(\n        dialect=dialect,\n        rules=rules,\n        exclude_rules=exclude_rules,\n        config_path=config_path,\n    )\n    linter = Linter(config=cfg)\n\n    result = linter.lint_string_wrapped(sql)\n    result_records = result.as_records()\n    # Return just the violations for this file\n    return [] if not result_records else result_records[0][\"violations\"]\n", "type": "function"}, {"name": "DummyLintError", "docstring": "Fake lint error used by tests, similar to SQLLintError.", "methods": [], "attributes": [], "code_location": {"file": "noqa_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 15, "end_line": 20}, "type": "class"}, {"name": "DummyLintError", "docstring": "Fake lint error used by tests, similar to SQLLintError.", "methods": ["__init__", "__init__"], "attributes": [], "code_location": {"file": "linter_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 26, "end_line": 31}, "type": "class"}, {"name": "Linter", "docstring": "The interface class to interact with the linter.", "methods": ["__init__", "get_rulepack", "rule_tuples", "load_raw_file_and_config", "_normalise_newlines", "_lex_templated_file", "_parse_tokens", "remove_templated_errors", "_report_conflicting_fixes_same_anchor", "_warn_unfixable", "parse_rendered", "lint_fix_parsed", "lint_parsed", "allowed_rule_ref_map", "lint_rendered", "render_string", "render_file", "parse_string", "fix", "lint", "lint_string", "lint_string_wrapped", "lint_path", "lint_paths", "parse_path"], "attributes": ["allow_process_parallelism"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 57, "end_line": 1161}, "type": "class"}, {"name": "lint_fix_parsed", "is_method": true, "class_name": "Linter", "parameters": ["cls", "tree", "config", "rule_pack", "fix", "fname", "templated_file", "formatter"], "calls": ["config.get", "config.get", "linter_logger.info", "linter_logger.info", "config.get", "formatter.dispatch_lint_header", "cls.allowed_rule_ref_map", "IgnoreMask.from_tree", "phases.append", "range", "cls.remove_templated_errors", "format", "sorted", "config.get", "len", "linter_logger.info", "is_first_linter_pass", "tqdm", "tree.stringify", "rule_pack.codes", "progress_bar_crawler.set_description", "time.monotonic", "crawler.crawl", "is_first_linter_pass", "rule_timings.append", "linter_logger.info", "linter_logger.warning", "linter_logger.info", "compute_anchor_edit_info", "any", "isinstance", "is_first_linter_pass", "config.get", "anchor_info.items", "cls._report_conflicting_fixes_same_anchor", "linter_logger.debug", "apply_fixes", "time.monotonic", "anchor_info.values", "config.get", "tuple", "linter_logger.debug", "config.get", "tuple", "linter_logger.warning", "previous_versions.add", "cls._warn_unfixable"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 380, "end_line": 628}, "code_snippet": "    def lint_fix_parsed(\n        cls,\n        tree: BaseSegment,\n        config: FluffConfig,\n        rule_pack: RulePack,\n        fix: bool = False,\n        fname: Optional[str] = None,\n        templated_file: Optional[\"TemplatedFile\"] = None,\n        formatter: Optional[FormatterInterface] = None,\n    ) -> tuple[BaseSegment, list[SQLBaseError], Optional[IgnoreMask], RuleTimingsType]:\n        \"\"\"Lint and optionally fix a tree object.\"\"\"\n        # Keep track of the linting errors on the very first linter pass. The\n        # list of issues output by \"lint\" and \"fix\" only includes issues present\n        # in the initial SQL code, EXCLUDING any issues that may be created by\n        # the fixes themselves.\n        initial_linting_errors = []\n        # A placeholder for the fixes we had on the previous loop\n        last_fixes: Optional[list[LintFix]] = None\n        # Keep a set of previous versions to catch infinite loops.\n        previous_versions: set[tuple[str, tuple[\"SourceFix\", ...]]] = {(tree.raw, ())}\n        # Keep a buffer for recording rule timings.\n        rule_timings: RuleTimingsType = []\n\n        # If we are fixing then we want to loop up to the runaway_limit, otherwise just\n        # once for linting.\n        loop_limit = config.get(\"runaway_limit\") if fix else 1\n\n        # Dispatch the output for the lint header\n        if formatter:\n            formatter.dispatch_lint_header(\n                fname or \"<filename>\", sorted(rule_pack.codes())\n            )\n\n        # Look for comment segments which might indicate lines to ignore.\n        disable_noqa_except: Optional[str] = config.get(\"disable_noqa_except\")\n        if not config.get(\"disable_noqa\") or disable_noqa_except:\n            allowed_rules_ref_map = cls.allowed_rule_ref_map(\n                rule_pack.reference_map, disable_noqa_except\n            )\n            ignore_mask, ivs = IgnoreMask.from_tree(tree, allowed_rules_ref_map)\n            initial_linting_errors += ivs\n        else:\n            ignore_mask = None\n\n        save_tree = tree\n        # There are two phases of rule running.\n        # 1. The main loop is for most rules. These rules are assumed to\n        # interact and cause a cascade of fixes requiring multiple passes.\n        # These are run the `runaway_limit` number of times (default 10).\n        # 2. The post loop is for post-processing rules, not expected to trigger\n        # any downstream rules, e.g. capitalization fixes. They are run on the\n        # first loop and then twice at the end (once to fix, and once again to\n        # check result of fixes), but not in the intervening loops.\n        phases = [\"main\"]\n        if fix:\n            phases.append(\"post\")\n        for phase in phases:\n            if len(phases) > 1:\n                rules_this_phase = [\n                    rule for rule in rule_pack.rules if rule.lint_phase == phase\n                ]\n            else:\n                rules_this_phase = rule_pack.rules\n            for loop in range(loop_limit if phase == \"main\" else 2):\n\n                def is_first_linter_pass() -> bool:\n                    return phase == phases[0] and loop == 0\n\n                # Additional newlines are to assist in scanning linting loops\n                # during debugging.\n                linter_logger.info(\n                    f\"\\n\\nEntering linter phase {phase}, loop {loop + 1}/{loop_limit}\\n\"\n                )\n                changed = False\n\n                if is_first_linter_pass():\n                    # In order to compute initial_linting_errors correctly, need\n                    # to run all rules on the first loop of the main phase.\n                    rules_this_phase = rule_pack.rules\n                progress_bar_crawler = tqdm(\n                    rules_this_phase,\n                    desc=\"lint by rules\",\n                    leave=False,\n                    disable=progress_bar_configuration.disable_progress_bar,\n                )\n\n                for crawler in progress_bar_crawler:\n                    # Performance: After first loop pass, skip rules that don't\n                    # do fixes. Any results returned won't be seen by the user\n                    # anyway (linting errors ADDED by rules changing SQL, are\n                    # not reported back to the user - only initial linting errors),\n                    # so there's absolutely no reason to run them.\n                    if (\n                        fix\n                        and not is_first_linter_pass()\n                        and not crawler.is_fix_compatible\n                    ):\n                        continue\n\n                    progress_bar_crawler.set_description(f\"rule {crawler.code}\")\n                    t0 = time.monotonic()\n\n                    # fixes should be a dict {} with keys edit, delete, create\n                    # delete is just a list of segments to delete\n                    # edit and create are list of tuples. The first element is\n                    # the \"anchor\", the segment to look for either to edit or to\n                    # insert BEFORE. The second is the element to insert or create.\n                    linting_errors, _, fixes, _ = crawler.crawl(\n                        tree,\n                        dialect=config.get(\"dialect_obj\"),\n                        fix=fix,\n                        templated_file=templated_file,\n                        ignore_mask=ignore_mask,\n                        fname=fname,\n                        config=config,\n                    )\n                    if is_first_linter_pass():\n                        initial_linting_errors += linting_errors\n\n                    if fix and fixes:\n                        linter_logger.info(f\"Applying Fixes [{crawler.code}]: {fixes}\")\n                        # Do some sanity checks on the fixes before applying.\n                        anchor_info = compute_anchor_edit_info(fixes)\n                        if any(\n                            not info.is_valid for info in anchor_info.values()\n                        ):  # pragma: no cover\n                            message = (\n                                f\"Rule {crawler.code} returned conflicting \"\n                                \"fixes with the same anchor. This is only \"\n                                \"supported for create_before+create_after, so \"\n                                \"the fixes will not be applied. \"\n                            )\n                            for uuid, info in anchor_info.items():\n                                if not info.is_valid:\n                                    message += f\"\\n{uuid}:\"\n                                    for _fix in info.fixes:\n                                        message += f\"\\n    {_fix}\"\n                            cls._report_conflicting_fixes_same_anchor(message)\n                            for lint_result in linting_errors:\n                                lint_result.fixes = []\n                        elif fixes == last_fixes:\n                            # If we generate the same fixes two times in a row,\n                            # that means we're in a loop, and we want to stop.\n                            # (Fixes should address issues, hence different\n                            # and/or fewer fixes next time.)\n                            # This is most likely because fixes could not be safely\n                            # applied last time, so we should stop gracefully.\n                            linter_logger.debug(\n                                f\"Fixes generated for {crawler.code} are the same as \"\n                                \"the previous pass. Assuming that we cannot apply them \"\n                                \"safely. Passing gracefully.\"\n                            )\n                        else:\n                            # This is the happy path. We have fixes, now we want to\n                            # apply them.\n                            last_fixes = fixes\n                            new_tree, _, _, _valid = apply_fixes(\n                                tree,\n                                config.get(\"dialect_obj\"),\n                                crawler.code,\n                                anchor_info,\n                                fix_even_unparsable=config.get(\"fix_even_unparsable\"),\n                            )\n\n                            # Check for infinite loops. We use a combination of the\n                            # fixed templated file and the list of source fixes to\n                            # apply.\n                            loop_check_tuple = (\n                                new_tree.raw,\n                                tuple(new_tree.source_fixes),\n                            )\n                            # Was anything actually applied? If not, then the fixes we\n                            # had cannot be safely applied and we should stop trying.\n                            if loop_check_tuple == (tree.raw, tuple(tree.source_fixes)):\n                                linter_logger.debug(\n                                    f\"Fixes for {crawler.code} could not be safely be \"\n                                    \"applied. Likely due to initially unparsable file.\"\n                                )\n                            elif not _valid:\n                                # The fixes result in an invalid file. Don't apply\n                                # the fix and skip onward. Show a warning.\n                                linter_logger.warning(\n                                    f\"Fixes for {crawler.code} not applied, as it \"\n                                    \"would result in an unparsable file. Please \"\n                                    \"report this as a bug with a minimal query \"\n                                    \"which demonstrates this warning.\"\n                                )\n                            elif loop_check_tuple not in previous_versions:\n                                # We've not seen this version of the file so\n                                # far. Continue.\n                                tree = new_tree\n                                previous_versions.add(loop_check_tuple)\n                                changed = True\n                                continue\n                            else:\n                                # Applying these fixes took us back to a state\n                                # which we've seen before. We're in a loop, so\n                                # we want to stop.\n                                cls._warn_unfixable(crawler.code)\n\n                    # Record rule timing\n                    rule_timings.append(\n                        (crawler.code, crawler.name, time.monotonic() - t0)\n                    )\n\n                if fix and not changed:\n                    # We did not change the file. Either the file is clean (no\n                    # fixes), or any fixes which are present will take us back\n                    # to a previous state.\n                    linter_logger.info(\n                        f\"Fix loop complete for {phase} phase. Stability \"\n                        f\"achieved after {loop}/{loop_limit} loops.\"\n                    )\n                    break\n            else:\n                if fix:\n                    # The linter loop hit the limit before reaching a stable point\n                    # (i.e. free of lint errors). If this happens, it's usually\n                    # because one or more rules produced fixes which did not address\n                    # the original issue **or** created new issues.\n                    linter_logger.warning(\n                        f\"Loop limit on fixes reached [{loop_limit}].\"\n                    )\n\n                    # Discard any fixes for the linting errors, since they caused a\n                    # loop. IMPORTANT: By doing this, we are telling SQLFluff that\n                    # these linting errors are \"unfixable\". This is important,\n                    # because when \"sqlfluff fix\" encounters unfixable lint errors,\n                    # it exits with a \"failure\" exit code, which is exactly what we\n                    # want in this situation. (Reason: Although this is more of an\n                    # internal SQLFluff issue, users deserve to know about it,\n                    # because it means their file(s) weren't fixed.\n                    for violation in initial_linting_errors:\n                        if isinstance(violation, SQLLintError):\n                            violation.fixes = []\n\n                    # Return the original parse tree, before any fixes were applied.\n                    # Reason: When the linter hits the loop limit, the file is often\n                    # messy, e.g. some of the fixes were applied repeatedly, possibly\n                    # other weird things. We don't want the user to see this junk!\n                    return save_tree, initial_linting_errors, ignore_mask, rule_timings\n\n        if config.get(\"ignore_templated_areas\", default=True):\n            initial_linting_errors = cls.remove_templated_errors(initial_linting_errors)\n\n        linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Fixed Tree:\"))\n        linter_logger.info(\"\\n\" + tree.stringify())\n\n        return tree, initial_linting_errors, ignore_mask, rule_timings\n", "type": "function"}, {"name": "cli", "is_method": false, "class_name": null, "parameters": [], "calls": ["click.group", "click.version_option"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 480, "end_line": 481}, "code_snippet": "def cli() -> None:\n    \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "BaseRunner", "parameters": ["self", "linter", "config"], "calls": [], "code_location": {"file": "runner.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 37, "end_line": 43}, "code_snippet": "    def __init__(\n        self,\n        linter: Linter,\n        config: FluffConfig,\n    ) -> None:\n        self.linter = linter\n        self.config = config\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3525865077972412}
{"question": "Where are SQLFluff's dialect-specific parser implementations located?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's dialect-specific parser implementations are located in several key locations in the codebase. The main dialect-specific parser components are located in: 1) src/sqlfluff/core/dialects/ - Contains all dialect-specific implementations including ansi.py, postgres.py, mysql.py, snowflake.py, etc.; 2) src/sqlfluff/core/dialects/base.py - Contains the base Dialect class definition; 3) src/sqlfluff/core/dialects/common.py - Contains common dialect utilities and shared components; 4) src/sqlfluff/core/parser/grammar.py - Contains grammar definitions that are dialect-specific; 5) src/sqlfluff/core/parser/parsers.py - Contains parser classes that handle dialect-specific syntax; 6) src/sqlfluff/core/parser/segments/ - Contains segment definitions that vary by dialect; 7) src/sqlfluff/core/lexer.py - Contains dialect-specific lexer matchers and tokenization logic; 8) src/sqlfluff/core/dialects/__init__.py - Contains dialect registration and selection logic; 9) src/sqlfluff/core/parser/context.py - Contains dialect-aware parsing context; 10) src/sqlfluff/core/config/fluffconfig.py - Contains dialect configuration management.", "score": null, "retrieved_content": [{"name": "TestFlinkSQLDialect", "docstring": "Test FlinkSQL dialect parsing.", "methods": ["test_flink_dialect_basic", "test_flink_create_table_basic", "test_flink_row_data_type", "test_flink_timestamp_with_precision", "test_flink_watermark_definition", "test_flink_computed_column", "test_flink_metadata_column", "test_flink_show_statements", "test_flink_use_statements", "test_flink_describe_statement", "test_flink_explain_statement", "test_flink_create_catalog", "test_flink_create_database", "test_flink_alternative_with_syntax"], "attributes": [], "code_location": {"file": "flink_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 6, "end_line": 231}, "type": "class"}, {"name": "_initialise_dialect", "is_method": true, "class_name": "FluffConfig", "parameters": ["self", "dialect", "require_dialect"], "calls": ["dialect_selector", "self.verify_dialect_specified"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 153, "end_line": 162}, "code_snippet": "    def _initialise_dialect(\n        self, dialect: Optional[str], require_dialect: bool = True\n    ) -> None:\n        # NB: We import here to avoid a circular references.\n        from sqlfluff.core.dialects import dialect_selector\n\n        if dialect is not None:\n            self._configs[\"core\"][\"dialect_obj\"] = dialect_selector(dialect)\n        elif require_dialect:\n            self.verify_dialect_specified()\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "Parser", "parameters": ["self", "config", "dialect"], "calls": ["get_root_segment", "ValueError", "FluffConfig.from_kwargs", "self.config.get"], "code_location": {"file": "parser.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser", "start_line": 17, "end_line": 28}, "code_snippet": "    def __init__(\n        self, config: Optional[FluffConfig] = None, dialect: Optional[str] = None\n    ):\n        if config and dialect:\n            raise ValueError(  # pragma: no cover\n                \"Parser does not support setting both `config` and `dialect`.\"\n            )\n        # Use the provided config or create one from the dialect.\n        self.config = config or FluffConfig.from_kwargs(dialect=dialect)\n        self.RootSegment: type[BaseFileSegment] = self.config.get(\n            \"dialect_obj\"\n        ).get_root_segment()\n", "type": "function"}, {"name": "parse", "is_method": false, "class_name": null, "parameters": ["sql", "dialect", "config", "config_path"], "calls": ["Linter", "linter.parse_string", "parsed.root_variant", "root_variant.tree.as_record", "get_simple_config", "APIParsingError"], "code_location": {"file": "simple.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/api", "start_line": 168, "end_line": 212}, "code_snippet": "def parse(\n    sql: str,\n    dialect: Optional[str] = None,\n    config: Optional[FluffConfig] = None,\n    config_path: Optional[str] = None,\n) -> dict[str, Any]:\n    \"\"\"Parse a SQL string.\n\n    Args:\n        sql (:obj:`str`): The SQL to be parsed.\n        dialect (:obj:`Optional[str]`, optional): A reference to the dialect of the SQL\n            to be parsed. Defaults to `ansi`.\n        config (:obj:`Optional[FluffConfig]`, optional): A configuration object\n            to use for the operation. Defaults to None.\n        config_path (:obj:`Optional[str]`, optional): A path to a .sqlfluff config,\n            which is only used if a `config` is not already provided.\n            Defaults to None.\n\n    Returns:\n        :obj:`Dict[str, Any]` JSON containing the parsed structure.\n\n    Note:\n        In the case of multiple potential variants from the raw source file\n        only the first variant is returned by the simple API. For access to\n        the other variants, use the underlying main API directly.\n    \"\"\"\n    cfg = config or get_simple_config(\n        dialect=dialect,\n        config_path=config_path,\n    )\n    linter = Linter(config=cfg)\n\n    parsed = linter.parse_string(sql)\n    # If we encounter any parsing errors, raise them in a combined issue.\n    violations = parsed.violations\n    if violations:\n        raise APIParsingError(violations)\n    # Return a JSON representation of the parse tree.\n    # NOTE: For the simple API - only a single variant is returned.\n    root_variant = parsed.root_variant()\n    assert root_variant, \"Files parsed without violations must have a valid variant\"\n    assert root_variant.tree, \"Files parsed without violations must have a valid tree\"\n    record = root_variant.tree.as_record(show_raw=True)\n    assert record\n    return record\n", "type": "function"}, {"name": "SQLFluffDomain", "docstring": "SQLFluff domain.", "methods": ["get_full_qualified_name", "get_objects", "resolve_xref", "add_rule"], "attributes": ["name", "label", "object_types", "roles", "directives", "initial_data"], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 82, "end_line": 142}, "type": "class"}, {"name": "Dialect", "docstring": "Serves as the basis for runtime resolution of Grammar.\n\nArgs:\n    name (:obj:`str`): The name of the dialect, used for lookup.\n    lexer_matchers (iterable of :obj:`StringLexer`): A structure defining\n        the lexing config for this dialect.", "methods": ["__init__", "__repr__", "expand", "sets", "bracket_sets", "update_keywords_set_from_multiline_string", "copy_as", "add", "replace", "add_update_segments", "get_grammar", "get_segment", "ref", "set_lexer_matchers", "get_lexer_matchers", "patch_lexer_matchers", "insert_lexer_matchers", "get_root_segment"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "start_line": 18, "end_line": 397}, "type": "class"}, {"name": "test__api__invalid_dialect", "is_method": false, "class_name": null, "parameters": [], "calls": ["open", "f.read", "pytest.raises", "sqlfluff.parse", "str"], "code_location": {"file": "simple_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 653, "end_line": 667}, "code_snippet": "def test__api__invalid_dialect():\n    \"\"\"Test that SQLFluffUserError is raised for a bad dialect.\"\"\"\n    # Load test SQL file.\n    with open(\"test/fixtures/api/config_path_test/config_path_test.sql\", \"r\") as f:\n        sql = f.read()\n\n    # Pass a fake dialect to the API and test the correct error is raised.\n    with pytest.raises(SQLFluffUserError) as err:\n        sqlfluff.parse(\n            sql,\n            dialect=\"not_a_real_dialect\",\n            config_path=\"test/fixtures/api/config_path_test/extra_configs/.sqlfluff\",\n        )\n\n    assert str(err.value) == \"Error: Unknown dialect 'not_a_real_dialect'\"\n", "type": "function"}, {"name": "test__api__parse_dialect_config_path", "is_method": false, "class_name": null, "parameters": ["dialect", "config_path", "expectation"], "calls": ["pytest.mark.parametrize", "isinstance", "open", "f.read", "open", "json.load", "sqlfluff.parse", "nullcontext", "pytest.raises", "nullcontext", "pytest.raises"], "code_location": {"file": "simple_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 536, "end_line": 561}, "code_snippet": "def test__api__parse_dialect_config_path(dialect, config_path, expectation):\n    \"\"\"Test that we can load a dialect from a config file in the Simple API parse.\"\"\"\n    # Load test SQL file.\n    with open(\"test/fixtures/api/config_dialect/config_dialect.sql\", \"r\") as f:\n        sql = f.read()\n\n    # Load in expected result.\n    with open(\"test/fixtures/api/config_dialect/config_dialect_parse.json\", \"r\") as f:\n        expected_parsed = json.load(f)\n\n    was_parsed = False\n    with expectation:\n        # Pass a config path to the Simple API.\n        parsed = sqlfluff.parse(\n            sql,\n            dialect=dialect,\n            config_path=config_path,\n        )\n        was_parsed = True\n        # Compare JSON from parse to expected result.\n        assert parsed == expected_parsed\n\n    if isinstance(expectation, nullcontext):\n        assert was_parsed\n    else:\n        assert not was_parsed\n", "type": "function"}, {"name": "load_raw_dialect", "is_method": false, "class_name": null, "parameters": ["label", "base_module"], "calls": ["import_module", "getattr", "result.add_update_segments", "SQLFluffUserError", "KeyError", "getattr", "dir"], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "start_line": 67, "end_line": 77}, "code_snippet": "def load_raw_dialect(label: str, base_module: str = \"sqlfluff.dialects\") -> Dialect:\n    \"\"\"Dynamically load a dialect.\"\"\"\n    if label in _legacy_dialects:\n        raise SQLFluffUserError(_legacy_dialects[label])\n    elif label not in _dialect_lookup:\n        raise KeyError(\"Unknown dialect\")\n    module_name, name = _dialect_lookup[label]\n    module = import_module(f\"{base_module}.{module_name}\")\n    result: Dialect = getattr(module, name)\n    result.add_update_segments({k: getattr(module, k) for k in dir(module)})\n    return result\n", "type": "function"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.34369635581970215}
{"question": "How does SQLFluff implement its SQL parsing system?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff implements its SQL parsing system through a multi-stage pipeline that transforms raw SQL text into a structured parse tree. The parsing system works as follows: 1) Templating Stage - Raw SQL is first processed through templating engines (Jinja, Python format strings, dbt) to handle dynamic content and placeholders; 2) Lexing Stage - The templated SQL is tokenized by the Lexer class into RawSegment objects using dialect-specific matchers and patterns; 3) Parsing Stage - The Parser class transforms lexed tokens into a hierarchical parse tree using grammar rules defined in grammar.py, with different grammar types (Sequence, OneOf, Delimited, Bracketed, etc.); 4) Segment System - The parse tree is built using BaseSegment and RawSegment classes, where BaseSegment represents composite nodes and RawSegment represents atomic tokens; 5) Grammar Matching - The parser uses match() methods to recursively apply grammar rules and build the tree structure; 6) Context Management - ParseContext tracks parsing state, position, and dialect-specific information throughout the process; 7) Error Handling - Unparsable segments are wrapped in UnparsableSegment with error information; 8) Dialect Support - Different SQL dialects have specialized grammar rules and segment definitions; 9) Position Tracking - PositionMarker objects maintain source position information for error reporting; 10) Tree Validation - The resulting parse tree is validated for structural correctness and completeness.", "score": null, "retrieved_content": [{"name": "parse", "is_method": false, "class_name": null, "parameters": ["sql", "dialect", "config", "config_path"], "calls": ["Linter", "linter.parse_string", "parsed.root_variant", "root_variant.tree.as_record", "get_simple_config", "APIParsingError"], "code_location": {"file": "simple.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/api", "start_line": 168, "end_line": 212}, "code_snippet": "def parse(\n    sql: str,\n    dialect: Optional[str] = None,\n    config: Optional[FluffConfig] = None,\n    config_path: Optional[str] = None,\n) -> dict[str, Any]:\n    \"\"\"Parse a SQL string.\n\n    Args:\n        sql (:obj:`str`): The SQL to be parsed.\n        dialect (:obj:`Optional[str]`, optional): A reference to the dialect of the SQL\n            to be parsed. Defaults to `ansi`.\n        config (:obj:`Optional[FluffConfig]`, optional): A configuration object\n            to use for the operation. Defaults to None.\n        config_path (:obj:`Optional[str]`, optional): A path to a .sqlfluff config,\n            which is only used if a `config` is not already provided.\n            Defaults to None.\n\n    Returns:\n        :obj:`Dict[str, Any]` JSON containing the parsed structure.\n\n    Note:\n        In the case of multiple potential variants from the raw source file\n        only the first variant is returned by the simple API. For access to\n        the other variants, use the underlying main API directly.\n    \"\"\"\n    cfg = config or get_simple_config(\n        dialect=dialect,\n        config_path=config_path,\n    )\n    linter = Linter(config=cfg)\n\n    parsed = linter.parse_string(sql)\n    # If we encounter any parsing errors, raise them in a combined issue.\n    violations = parsed.violations\n    if violations:\n        raise APIParsingError(violations)\n    # Return a JSON representation of the parse tree.\n    # NOTE: For the simple API - only a single variant is returned.\n    root_variant = parsed.root_variant()\n    assert root_variant, \"Files parsed without violations must have a valid variant\"\n    assert root_variant.tree, \"Files parsed without violations must have a valid tree\"\n    record = root_variant.tree.as_record(show_raw=True)\n    assert record\n    return record\n", "type": "function"}, {"name": "_parse_tokens", "is_method": true, "class_name": "Linter", "parameters": ["tokens", "config", "fname", "parse_statistics"], "calls": ["Parser", "linter_logger.info", "linter_logger.info", "parsed.iter_unparsables", "parser.parse", "format", "violations.append", "linter_logger.info", "linter_logger.info", "tuple", "linter_logger.info", "violations.append", "parsed.stringify", "SQLParseError", "unparsable.stringify", "format", "len"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 231, "end_line": 281}, "code_snippet": "    def _parse_tokens(\n        tokens: Sequence[BaseSegment],\n        config: FluffConfig,\n        fname: Optional[str] = None,\n        parse_statistics: bool = False,\n    ) -> tuple[Optional[BaseSegment], list[SQLParseError]]:\n        parser = Parser(config=config)\n        violations = []\n        # Parse the file and log any problems\n        try:\n            parsed: Optional[BaseSegment] = parser.parse(\n                # Regardless of how the sequence was passed in, we should\n                # coerce it to a tuple here, before we head deeper into\n                # the parsing process.\n                tuple(tokens),\n                fname=fname,\n                parse_statistics=parse_statistics,\n            )\n        except SQLParseError as err:\n            linter_logger.info(\"PARSING FAILED! : %s\", err)\n            violations.append(err)\n            return None, violations\n\n        if parsed is None:  # pragma: no cover\n            return None, violations\n\n        linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n        linter_logger.info(\"\\n\" + parsed.stringify())\n        # We may succeed parsing, but still have unparsable segments. Extract them\n        # here.\n        for unparsable in parsed.iter_unparsables():\n            # No exception has been raised explicitly, but we still create one here\n            # so that we can use the common interface\n            assert unparsable.pos_marker\n            violations.append(\n                SQLParseError(\n                    \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n                    \"{1!r}\".format(\n                        unparsable.pos_marker.working_loc,\n                        (\n                            unparsable.raw\n                            if len(unparsable.raw) < 40\n                            else unparsable.raw[:40] + \"...\"\n                        ),\n                    ),\n                    segment=unparsable,\n                )\n            )\n            linter_logger.info(\"Found unparsable segment...\")\n            linter_logger.info(unparsable.stringify())\n        return parsed, violations\n", "type": "function"}, {"name": "SQLFluffDomain", "docstring": "SQLFluff domain.", "methods": ["get_full_qualified_name", "get_objects", "resolve_xref", "add_rule"], "attributes": ["name", "label", "object_types", "roles", "directives", "initial_data"], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 82, "end_line": 142}, "type": "class"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}, {"name": "cli", "is_method": false, "class_name": null, "parameters": [], "calls": ["click.group", "click.version_option"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 480, "end_line": 481}, "code_snippet": "def cli() -> None:\n    \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "SQLFluffDriver", "parameters": ["self"], "calls": ["__init__", "super", "s.encode", "sys.getfilesystemencoding"], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 25, "end_line": 34}, "code_snippet": "    def __init__(self) -> None:\n        super().__init__(\n            [sys.executable, \"-m\", \"sqlfluff.cli.commands\"],\n            [\".sql\"],\n            [\n                s.encode(sys.getfilesystemencoding())\n                for s in [\"sqlfluff\", \"lint\", \"--format=json\"]\n            ],\n            exit_codes=[0, 1],\n        )\n", "type": "function"}, {"name": "SQLParseError", "docstring": "An error which occurred during parsing.\n\nArgs:\n    segment (:obj:`BaseSegment`, optional): The segment which is relevant\n        for the failure in parsing. This is likely to be a subclass of\n        `BaseSegment` rather than the parent class itself. This is mostly\n        used for logging and for referencing position.", "methods": ["__init__", "__reduce__", "to_dict"], "attributes": ["_code", "_identifier"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 185, "end_line": 246}, "type": "class"}, {"name": "SQLFluffRule", "docstring": "SQLFluff rule directive for sphinx.\n\nRule directives can be used as shown below.\n\n.. code-block:: rst\n\n    .. sqlfluff:rule:: AM01\n                       ambiguous.distinct\n\n        Write the documentation for the rule here.\n\nTo cross reference (i.e. refer to) objects defined like this\nboth the code and name reference is available:\n\n.. code-block:: rst\n\n    :sqlfluff:ref:`CP02`\n    :sqlfluff:ref:`capitalisation.identifiers`", "methods": ["handle_signature", "add_target_and_index", "_object_hierarchy_parts", "_toc_entry_name"], "attributes": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 10, "end_line": 79}, "type": "class"}, {"name": "TestFlinkSQLDialect", "docstring": "Test FlinkSQL dialect parsing.", "methods": ["test_flink_dialect_basic", "test_flink_create_table_basic", "test_flink_row_data_type", "test_flink_timestamp_with_precision", "test_flink_watermark_definition", "test_flink_computed_column", "test_flink_metadata_column", "test_flink_show_statements", "test_flink_use_statements", "test_flink_describe_statement", "test_flink_explain_statement", "test_flink_create_catalog", "test_flink_create_database", "test_flink_alternative_with_syntax"], "attributes": [], "code_location": {"file": "flink_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 6, "end_line": 231}, "type": "class"}, {"name": "test__api__linter_lint", "is_method": false, "class_name": null, "parameters": [], "calls": ["lex", "parse", "lint", "Lexer", "Parser", "Linter"], "code_location": {"file": "classes_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 24, "end_line": 29}, "code_snippet": "def test__api__linter_lint():\n    \"\"\"Basic checking of parsing functionality.\"\"\"\n    tokens, _ = Lexer(dialect=\"ansi\").lex(test_query)\n    parsed = Parser(dialect=\"ansi\").parse(tokens)\n    violations = Linter(dialect=\"ansi\").lint(parsed)\n    assert [v.rule.code for v in violations] == [\"CP01\", \"LT12\"]\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.34177207946777344}
{"question": "How does SQLFluff implement its multi-dialect support system?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff implements its multi-dialect support system through a modular architecture that allows different SQL dialects to coexist and be selected at runtime. The multi-dialect system works as follows: 1) Dialect Base Class - All dialects inherit from a base Dialect class defined in dialects/base.py, which provides common functionality and interface; 2) Dialect Registration - Each dialect registers itself with the dialect system through entry points or direct registration in dialects/__init__.py; 3) Dialect Selection - Users can select dialects through configuration (dialect setting) or command-line arguments, with the system loading the appropriate dialect implementation; 4) Grammar Specialization - Each dialect defines its own grammar rules in grammar.py, extending or overriding base grammar patterns for dialect-specific syntax; 5) Lexer Customization - Dialects can customize the lexer through get_lexer_matchers() method, adding dialect-specific tokens and patterns; 6) Segment Definitions - Dialects can define custom segment types that handle dialect-specific syntax constructs; 7) Parser Extensions - Dialect-specific parsers can handle unique syntax patterns and constructs not supported by the base parser; 8) Configuration Integration - Dialect settings are integrated into the configuration system, allowing dialect-specific configuration options; 9) Error Handling - Dialect-specific error messages and handling for syntax that's valid in one dialect but not another; 10) Testing Support - Each dialect has its own test suite to ensure dialect-specific features work correctly.", "score": null, "retrieved_content": [{"name": "test__api__info_rules", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.list_rules", "isinstance", "RuleTuple"], "code_location": {"file": "info_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 29, "end_line": 52}, "code_snippet": "def test__api__info_rules():\n    \"\"\"Basic linting of dialects.\"\"\"\n    rules = sqlfluff.list_rules()\n    assert isinstance(rules, list)\n    assert (\n        RuleTuple(\n            code=\"LT01\",\n            name=\"layout.spacing\",\n            description=\"Inappropriate Spacing.\",\n            groups=(\"all\", \"core\", \"layout\"),\n            aliases=(\n                \"L001\",\n                \"L005\",\n                \"L006\",\n                \"L008\",\n                \"L023\",\n                \"L024\",\n                \"L039\",\n                \"L048\",\n                \"L071\",\n            ),\n        )\n        in rules\n    )\n", "type": "function"}, {"name": "_initialise_dialect", "is_method": true, "class_name": "FluffConfig", "parameters": ["self", "dialect", "require_dialect"], "calls": ["dialect_selector", "self.verify_dialect_specified"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 153, "end_line": 162}, "code_snippet": "    def _initialise_dialect(\n        self, dialect: Optional[str], require_dialect: bool = True\n    ) -> None:\n        # NB: We import here to avoid a circular references.\n        from sqlfluff.core.dialects import dialect_selector\n\n        if dialect is not None:\n            self._configs[\"core\"][\"dialect_obj\"] = dialect_selector(dialect)\n        elif require_dialect:\n            self.verify_dialect_specified()\n", "type": "function"}, {"name": "load_raw_dialect", "is_method": false, "class_name": null, "parameters": ["label", "base_module"], "calls": ["import_module", "getattr", "result.add_update_segments", "SQLFluffUserError", "KeyError", "getattr", "dir"], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "start_line": 67, "end_line": 77}, "code_snippet": "def load_raw_dialect(label: str, base_module: str = \"sqlfluff.dialects\") -> Dialect:\n    \"\"\"Dynamically load a dialect.\"\"\"\n    if label in _legacy_dialects:\n        raise SQLFluffUserError(_legacy_dialects[label])\n    elif label not in _dialect_lookup:\n        raise KeyError(\"Unknown dialect\")\n    module_name, name = _dialect_lookup[label]\n    module = import_module(f\"{base_module}.{module_name}\")\n    result: Dialect = getattr(module, name)\n    result.add_update_segments({k: getattr(module, k) for k in dir(module)})\n    return result\n", "type": "function"}, {"name": "test__api__info_dialects", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.list_dialects", "isinstance"], "code_location": {"file": "info_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 7, "end_line": 26}, "code_snippet": "def test__api__info_dialects():\n    \"\"\"Basic linting of dialects.\"\"\"\n    dialects = sqlfluff.list_dialects()\n    assert isinstance(dialects, list)\n    # Turn it into a dict so we can look for items in there.\n    dialect_dict = {dialect.label: dialect for dialect in dialects}\n    # Check the ansi dialect works\n    assert \"ansi\" in dialect_dict\n    ansi = dialect_dict[\"ansi\"]\n    assert ansi.label == \"ansi\"\n    assert ansi.name == \"ANSI\"\n    assert ansi.inherits_from == \"nothing\"\n    assert \"This is the base dialect\" in ansi.docstring\n    # Check one other works\n    assert \"postgres\" in dialect_dict\n    postgres = dialect_dict[\"postgres\"]\n    assert postgres.label == \"postgres\"\n    assert postgres.name == \"PostgreSQL\"\n    assert postgres.inherits_from == \"ansi\"\n    assert \"this is often the dialect to use\" in postgres.docstring\n", "type": "function"}, {"name": "Dialect", "docstring": "Serves as the basis for runtime resolution of Grammar.\n\nArgs:\n    name (:obj:`str`): The name of the dialect, used for lookup.\n    lexer_matchers (iterable of :obj:`StringLexer`): A structure defining\n        the lexing config for this dialect.", "methods": ["__init__", "__repr__", "expand", "sets", "bracket_sets", "update_keywords_set_from_multiline_string", "copy_as", "add", "replace", "add_update_segments", "get_grammar", "get_segment", "ref", "set_lexer_matchers", "get_lexer_matchers", "patch_lexer_matchers", "insert_lexer_matchers", "get_root_segment"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/dialects", "start_line": 18, "end_line": 397}, "type": "class"}, {"name": "test__api__invalid_dialect", "is_method": false, "class_name": null, "parameters": [], "calls": ["open", "f.read", "pytest.raises", "sqlfluff.parse", "str"], "code_location": {"file": "simple_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 653, "end_line": 667}, "code_snippet": "def test__api__invalid_dialect():\n    \"\"\"Test that SQLFluffUserError is raised for a bad dialect.\"\"\"\n    # Load test SQL file.\n    with open(\"test/fixtures/api/config_path_test/config_path_test.sql\", \"r\") as f:\n        sql = f.read()\n\n    # Pass a fake dialect to the API and test the correct error is raised.\n    with pytest.raises(SQLFluffUserError) as err:\n        sqlfluff.parse(\n            sql,\n            dialect=\"not_a_real_dialect\",\n            config_path=\"test/fixtures/api/config_path_test/extra_configs/.sqlfluff\",\n        )\n\n    assert str(err.value) == \"Error: Unknown dialect 'not_a_real_dialect'\"\n", "type": "function"}, {"name": "TestFlinkSQLDialect", "docstring": "Test FlinkSQL dialect parsing.", "methods": ["test_flink_dialect_basic", "test_flink_create_table_basic", "test_flink_row_data_type", "test_flink_timestamp_with_precision", "test_flink_watermark_definition", "test_flink_computed_column", "test_flink_metadata_column", "test_flink_show_statements", "test_flink_use_statements", "test_flink_describe_statement", "test_flink_explain_statement", "test_flink_create_catalog", "test_flink_create_database", "test_flink_alternative_with_syntax"], "attributes": [], "code_location": {"file": "flink_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 6, "end_line": 231}, "type": "class"}, {"name": "SQLFluffDomain", "docstring": "SQLFluff domain.", "methods": ["get_full_qualified_name", "get_objects", "resolve_xref", "add_rule"], "attributes": ["name", "label", "object_types", "roles", "directives", "initial_data"], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 82, "end_line": 142}, "type": "class"}, {"name": "cli", "is_method": false, "class_name": null, "parameters": [], "calls": ["click.group", "click.version_option"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 480, "end_line": 481}, "code_snippet": "def cli() -> None:\n    \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n", "type": "function"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.35013747215270996}
{"question": "How does SQLFluff handle different SQL dialects and syntax variations?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff handles different SQL dialects and syntax variations through a comprehensive system that adapts parsing, lexing, and linting to dialect-specific requirements. The dialect handling works as follows: 1) Dialect Detection - SQLFluff can automatically detect dialects based on SQL syntax patterns or users can explicitly specify the dialect through configuration; 2) Grammar Adaptation - Each dialect defines its own grammar rules that extend or override base grammar patterns to handle dialect-specific syntax constructs; 3) Lexer Customization - Dialects customize the lexer through get_lexer_matchers() method, adding dialect-specific keywords, operators, and token patterns; 4) Segment Specialization - Dialect-specific segment types handle unique syntax constructs like PostgreSQL's JSON operators or Snowflake's dollar-quoted strings; 5) Parser Extensions - Dialect parsers can handle syntax variations like different function call syntax, window function implementations, or CTE syntax differences; 6) Rule Adaptation - Linting rules can be dialect-aware, applying different standards or skipping certain checks based on dialect capabilities; 7) Configuration Integration - Dialect settings integrate with the configuration system, allowing dialect-specific rule configurations and options; 8) Error Context - Error messages and suggestions are tailored to the specific dialect being used; 9) Template Support - Templating engines can be dialect-aware, handling dialect-specific placeholder syntax; 10) Testing Framework - Each dialect has comprehensive test suites covering dialect-specific syntax variations and edge cases.", "score": null, "retrieved_content": [{"name": "TestFlinkSQLDialect", "docstring": "Test FlinkSQL dialect parsing.", "methods": ["test_flink_dialect_basic", "test_flink_create_table_basic", "test_flink_row_data_type", "test_flink_timestamp_with_precision", "test_flink_watermark_definition", "test_flink_computed_column", "test_flink_metadata_column", "test_flink_show_statements", "test_flink_use_statements", "test_flink_describe_statement", "test_flink_explain_statement", "test_flink_create_catalog", "test_flink_create_database", "test_flink_alternative_with_syntax"], "attributes": [], "code_location": {"file": "flink_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 6, "end_line": 231}, "type": "class"}, {"name": "test__api__info_rules", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.list_rules", "isinstance", "RuleTuple"], "code_location": {"file": "info_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 29, "end_line": 52}, "code_snippet": "def test__api__info_rules():\n    \"\"\"Basic linting of dialects.\"\"\"\n    rules = sqlfluff.list_rules()\n    assert isinstance(rules, list)\n    assert (\n        RuleTuple(\n            code=\"LT01\",\n            name=\"layout.spacing\",\n            description=\"Inappropriate Spacing.\",\n            groups=(\"all\", \"core\", \"layout\"),\n            aliases=(\n                \"L001\",\n                \"L005\",\n                \"L006\",\n                \"L008\",\n                \"L023\",\n                \"L024\",\n                \"L039\",\n                \"L048\",\n                \"L071\",\n            ),\n        )\n        in rules\n    )\n", "type": "function"}, {"name": "SQLFluffDomain", "docstring": "SQLFluff domain.", "methods": ["get_full_qualified_name", "get_objects", "resolve_xref", "add_rule"], "attributes": ["name", "label", "object_types", "roles", "directives", "initial_data"], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 82, "end_line": 142}, "type": "class"}, {"name": "test__api__invalid_dialect", "is_method": false, "class_name": null, "parameters": [], "calls": ["open", "f.read", "pytest.raises", "sqlfluff.parse", "str"], "code_location": {"file": "simple_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 653, "end_line": 667}, "code_snippet": "def test__api__invalid_dialect():\n    \"\"\"Test that SQLFluffUserError is raised for a bad dialect.\"\"\"\n    # Load test SQL file.\n    with open(\"test/fixtures/api/config_path_test/config_path_test.sql\", \"r\") as f:\n        sql = f.read()\n\n    # Pass a fake dialect to the API and test the correct error is raised.\n    with pytest.raises(SQLFluffUserError) as err:\n        sqlfluff.parse(\n            sql,\n            dialect=\"not_a_real_dialect\",\n            config_path=\"test/fixtures/api/config_path_test/extra_configs/.sqlfluff\",\n        )\n\n    assert str(err.value) == \"Error: Unknown dialect 'not_a_real_dialect'\"\n", "type": "function"}, {"name": "test__dialect__base_broad_fix", "is_method": false, "class_name": null, "parameters": ["dialect", "file", "raise_critical_errors_after_fix", "caplog"], "calls": ["pytest.mark.parametrize", "load_file", "dict", "lex_and_parse", "print", "FluffConfig", "Linter", "linter.get_rulepack", "linter.lint_parsed", "parsed.tree.stringify"], "code_location": {"file": "dialects_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 85, "end_line": 120}, "code_snippet": "def test__dialect__base_broad_fix(\n    dialect, file, raise_critical_errors_after_fix, caplog\n):\n    \"\"\"Run a full fix with all rules, in search of critical errors.\n\n    NOTE: This suite does all of the same things as the above test\n    suite (the `parse_suite`), but also runs fix. In CI, we run\n    the above tests _with_ coverage tracking, but these we run\n    _without_.\n\n    The purpose of this test is as a more stretching run through\n    a wide range of test sql examples, and the full range of rules\n    to find any potential critical errors raised by any interactions\n    between different dialects and rules.\n\n    We also do not use DEBUG logging here because it gets _very_\n    noisy.\n    \"\"\"\n    raw = load_file(dialect, file)\n    config_overrides = dict(dialect=dialect)\n\n    parsed: Optional[ParsedString] = lex_and_parse(config_overrides, raw)\n    if not parsed:  # Empty file case\n        return\n    print(parsed.tree.stringify())\n\n    config = FluffConfig(overrides=config_overrides)\n    linter = Linter(config=config)\n    rule_pack = linter.get_rulepack()\n    # Due to \"raise_critical_errors_after_fix\" fixture \"fix\",\n    # will now throw.\n    linter.lint_parsed(\n        parsed,\n        rule_pack,\n        fix=True,\n    )\n", "type": "function"}, {"name": "_dialect_supports_dot_access", "is_method": true, "class_name": "Rule_RF01", "parameters": ["self", "dialect"], "calls": [], "code_location": {"file": "RF01.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 306, "end_line": 327}, "code_snippet": "    def _dialect_supports_dot_access(self, dialect: Dialect) -> bool:\n        # Athena:\n        # https://docs.aws.amazon.com/athena/latest/ug/filtering-with-dot.html\n        # BigQuery:\n        # https://cloud.google.com/bigquery/docs/reference/standard-sql/operators#field_access_operator\n        # Databricks:\n        # https://docs.databricks.com/en/sql/language-manual/functions/dotsign.html\n        # DuckDB:\n        # https://duckdb.org/docs/sql/data_types/struct#retrieving-from-structs\n        # Redshift:\n        # https://docs.aws.amazon.com/redshift/latest/dg/query-super.html\n        # TODO: all doc links to all referenced dialects\n        return dialect.name in (\n            \"athena\",\n            \"bigquery\",\n            \"databricks\",\n            \"duckdb\",\n            \"hive\",\n            \"redshift\",\n            \"soql\",\n            \"sparksql\",\n        )\n", "type": "function"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}, {"name": "cli", "is_method": false, "class_name": null, "parameters": [], "calls": ["click.group", "click.version_option"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 480, "end_line": 481}, "code_snippet": "def cli() -> None:\n    \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n", "type": "function"}, {"name": "SQLFluffRule", "docstring": "SQLFluff rule directive for sphinx.\n\nRule directives can be used as shown below.\n\n.. code-block:: rst\n\n    .. sqlfluff:rule:: AM01\n                       ambiguous.distinct\n\n        Write the documentation for the rule here.\n\nTo cross reference (i.e. refer to) objects defined like this\nboth the code and name reference is available:\n\n.. code-block:: rst\n\n    :sqlfluff:ref:`CP02`\n    :sqlfluff:ref:`capitalisation.identifiers`", "methods": ["handle_signature", "add_target_and_index", "_object_hierarchy_parts", "_toc_entry_name"], "attributes": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 10, "end_line": 79}, "type": "class"}, {"name": "test_flink_dialect_basic", "is_method": true, "class_name": "TestFlinkSQLDialect", "parameters": ["self"], "calls": ["FluffConfig", "Linter", "linter.lint_string", "len", "v.rule.code.startswith"], "code_location": {"file": "flink_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 9, "end_line": 20}, "code_snippet": "    def test_flink_dialect_basic(self):\n        \"\"\"Test basic FlinkSQL dialect functionality.\"\"\"\n        config = FluffConfig(overrides={\"dialect\": \"flink\"})\n        linter = Linter(config=config)\n\n        # Test simple SELECT statement\n        sql = \"SELECT * FROM my_table;\\n\"\n        result = linter.lint_string(sql)\n        assert result is not None\n        # Check for parsing errors only, ignore style warnings\n        parsing_errors = [v for v in result.violations if v.rule.code.startswith(\"PRS\")]\n        assert len(parsing_errors) == 0\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.34597039222717285}
{"question": "How does SQLFluff's rule system work?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's rule system works through a hierarchical architecture that enables modular, configurable linting rules. The rule system operates as follows: 1) BaseRule Class - All rules inherit from BaseRule class defined in base.py, which provides common functionality including configuration handling, metadata management, and the core _eval() method; 2) RuleMetaclass - Automatically registers rules and manages rule metadata (code, name, description, groups, aliases) through metaclass processing; 3) RuleSet Management - The RuleSet class manages collections of rules, handles rule filtering, validation, and provides methods for rule discovery and application; 4) Rule Evaluation - Each rule implements an _eval() method that receives a segment and context, analyzes the parse tree, and returns violations or fixes; 5) Configuration Integration - Rules integrate with the configuration system through config_info.py, allowing per-rule configuration and rule enablement/disablement; 6) Rule Categorization - Rules are organized into groups (Core, Layout, References, etc.) and can be referenced by code, name, alias, or group; 7) Fix Generation - Rules can generate LintFix objects that describe how to automatically correct violations; 8) Rule Context - The rule context provides access to parse tree, configuration, and other contextual information during evaluation; 9) Rule Flow Control - The rule system supports different linting phases and crawl behaviors for efficient rule application; 10) Plugin Support - The rule system supports custom rules through plugin mechanisms, allowing third-party rule development.", "score": null, "retrieved_content": [{"name": "SQLFluffRule", "docstring": "SQLFluff rule directive for sphinx.\n\nRule directives can be used as shown below.\n\n.. code-block:: rst\n\n    .. sqlfluff:rule:: AM01\n                       ambiguous.distinct\n\n        Write the documentation for the rule here.\n\nTo cross reference (i.e. refer to) objects defined like this\nboth the code and name reference is available:\n\n.. code-block:: rst\n\n    :sqlfluff:ref:`CP02`\n    :sqlfluff:ref:`capitalisation.identifiers`", "methods": ["handle_signature", "add_target_and_index", "_object_hierarchy_parts", "_toc_entry_name"], "attributes": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 10, "end_line": 79}, "type": "class"}, {"name": "RuleMetaclass", "docstring": "The metaclass for rules.\n\nThis metaclass provides provides auto-enrichment of the\nrule docstring so that examples, groups, aliases and\nnames are added.\n\nThe reason we enrich the docstring is so that it can be\npicked up by autodoc and all be displayed in the sqlfluff\ndocs.", "methods": ["_populate_code_and_description", "_populate_docstring", "__new__"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 151, "end_line": 361}, "type": "class"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}, {"name": "test_rules_name_validation", "is_method": false, "class_name": null, "parameters": [], "calls": ["pytest.raises"], "code_location": {"file": "rules_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 326, "end_line": 336}, "code_snippet": "def test_rules_name_validation():\n    \"\"\"Ensure that rule names are validated.\"\"\"\n    with pytest.raises(SQLFluffUserError) as exc_info:\n\n        class RuleWithoutBadName_ZZ99(BaseRule):\n            \"\"\"A new rule without configuration.\"\"\"\n\n            name = \"MY-KEBAB-CASE-NAME\"\n\n    assert \"Tried to define rule with unexpected name\" in exc_info.value.args[0]\n    assert \"MY-KEBAB-CASE-NAME\" in exc_info.value.args[0]\n", "type": "function"}, {"name": "test__api__info_rules", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.list_rules", "isinstance", "RuleTuple"], "code_location": {"file": "info_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 29, "end_line": 52}, "code_snippet": "def test__api__info_rules():\n    \"\"\"Basic linting of dialects.\"\"\"\n    rules = sqlfluff.list_rules()\n    assert isinstance(rules, list)\n    assert (\n        RuleTuple(\n            code=\"LT01\",\n            name=\"layout.spacing\",\n            description=\"Inappropriate Spacing.\",\n            groups=(\"all\", \"core\", \"layout\"),\n            aliases=(\n                \"L001\",\n                \"L005\",\n                \"L006\",\n                \"L008\",\n                \"L023\",\n                \"L024\",\n                \"L039\",\n                \"L048\",\n                \"L071\",\n            ),\n        )\n        in rules\n    )\n", "type": "function"}, {"name": "cli", "is_method": false, "class_name": null, "parameters": [], "calls": ["click.group", "click.version_option"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 480, "end_line": 481}, "code_snippet": "def cli() -> None:\n    \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n", "type": "function"}, {"name": "SQLLintError", "docstring": "An error which occurred during linting.\n\nIn particular we reference the rule here to do extended logging based on\nthe rule in question which caused the fail.\n\nArgs:\n    segment (:obj:`BaseSegment`, optional): The segment which is relevant\n        for the failure in parsing. This is likely to be a subclass of\n        `BaseSegment` rather than the parent class itself. This is mostly\n        used for logging and for referencing position.", "methods": ["__init__", "__reduce__", "to_dict", "fixable", "rule_code", "rule_name", "source_signature", "__repr__"], "attributes": ["_identifier"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 249, "end_line": 383}, "type": "class"}, {"name": "RulePack", "docstring": "A bundle of rules to be applied.\n\nThis contains a set of rules, post filtering but also contains the mapping\nrequired to interpret any noqa messages found in files.\n\nThe reason for this object is that rules are filtered and instantiated\ninto this pack in the main process when running in multi-processing mode so\nthat user defined rules can be used without reference issues.\n\nAttributes:\n    rules (:obj:`list` of :obj:`BaseRule`): A filtered list of instantiated\n        rules to be applied to a given file.\n    reference_map (:obj:`dict`): A mapping of rule references to the codes\n        they refer to, e.g. `{\"my_ref\": {\"LT01\", \"LT02\"}}`. The references\n        (i.e. the keys) may be codes, groups, aliases or names. The values\n        of the mapping are sets of rule codes *only*. This object acts as\n        a lookup to be able to translate selectors (which may contain\n        diverse references) into a consolidated list of rule codes. This\n        mapping contains the full set of rules, rather than just the filtered\n        set present in the `rules` attribute.", "methods": ["codes"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 850, "end_line": 878}, "type": "class"}, {"name": "SQLFluffDomain", "docstring": "SQLFluff domain.", "methods": ["get_full_qualified_name", "get_objects", "resolve_xref", "add_rule"], "attributes": ["name", "label", "object_types", "roles", "directives", "initial_data"], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 82, "end_line": 142}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "SQLFluffDriver", "parameters": ["self"], "calls": ["__init__", "super", "s.encode", "sys.getfilesystemencoding"], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 25, "end_line": 34}, "code_snippet": "    def __init__(self) -> None:\n        super().__init__(\n            [sys.executable, \"-m\", \"sqlfluff.cli.commands\"],\n            [\".sql\"],\n            [\n                s.encode(sys.getfilesystemencoding())\n                for s in [\"sqlfluff\", \"lint\", \"--format=json\"]\n            ],\n            exit_codes=[0, 1],\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3730788230895996}
{"question": "How does SQLFluff implement its fix system?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff implements its fix system through a comprehensive mechanism that allows rules to generate and apply automatic corrections to SQL code. The fix system works as follows: 1) LintFix Objects - Rules generate LintFix objects that describe specific changes to be made, including the type of fix (create, edit, delete), target segment, and new content; 2) Fix Generation - During rule evaluation, rules can create LintFix objects when violations are detected, specifying exactly how to correct the issue; 3) Fix Application - The LintedFile.apply_fixes() method applies all generated fixes to the original SQL, creating a corrected version; 4) Patch Generation - The fix system generates FixPatch objects that represent the actual text changes needed, including line numbers and character positions; 5) Edit Computation - The compute_anchor_edit_info() function calculates the precise text edits needed to implement each fix; 6) Segment Manipulation - Fixes can create, edit, or delete segments in the parse tree, with the system handling the structural changes; 7) Position Tracking - The fix system maintains accurate position information throughout the fix application process; 8) Fix Validation - Applied fixes are validated to ensure they don't introduce new syntax errors or violate other rules; 9) Output Generation - The corrected SQL is generated with proper formatting and structure maintained; 10) Error Handling - The fix system handles edge cases and provides fallback behavior when fixes cannot be applied cleanly.", "score": null, "retrieved_content": [{"name": "apply_fixes", "is_method": false, "class_name": null, "parameters": ["segment", "dialect", "rule_code", "fixes", "fix_even_unparsable"], "calls": ["segment.invalidate_caches", "segment.is_raw", "fixes.pop", "list", "apply_fixes", "range", "len", "range", "segment.__class__", "seg_buffer.append", "seg_fixes.reverse", "fixes_applied.append", "linter_logger.debug", "segment._position_segments", "len", "segment._is_code_or_meta", "len", "segment._is_code_or_meta", "hasattr", "hasattr", "len", "seg_buffer.append", "seg_buffer.append", "seg_buffer.append", "tuple", "segment._position_segments", "err.add_note", "new_seg.validate_segment_with_reparse", "len", "tuple", "getattr", "len"], "code_location": {"file": "fix.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 107, "end_line": 335}, "code_snippet": "def apply_fixes(\n    segment: BaseSegment,\n    dialect: \"Dialect\",\n    rule_code: str,\n    fixes: dict[int, AnchorEditInfo],\n    fix_even_unparsable: bool = False,\n) -> tuple[\"BaseSegment\", list[\"BaseSegment\"], list[\"BaseSegment\"], bool]:\n    \"\"\"Apply a dictionary of fixes to this segment.\n\n    Used in to apply fixes found in linting. If a segment remains unchanged\n    then the original is returned, but if any changes are made to it, or any\n    of it's child segments, then it returns a copy rather than mutating the\n    original.\n\n    Most fixes are usually applied when this method is called on their parent\n    segment, this is because that's where we can insert or move segments relative\n    to the anchor specified in the fix. This has the implication that if the\n    method is called on a `RawSegment`, then no changes will be applied, because\n    a `RawSegment` never has child segments.\n\n    After fixing, it calls `validate_segment_with_reparse` on the segment to\n    check that the segment still parses after any changes are made. The result\n    of this is returned as a boolean in the last element of the return tuple.\n    As the function recurses, if an inner element doesn't parse after fixing,\n    then the outer segment will also be checked, and if found to parse successfully\n    then the method returns `True` as valid. This is because sometimes the fixes\n    change the structure enough that a wider reparse is necessary.\n\n    Because of this validity checking, any unparsable sections are assumed\n    unfixable (because we won't know if we're corrupting the SQL). The method\n    will therefore return early without applying any fixes if the segment it's\n    called on is unparsable (because we already know that validation check will\n    fail already).\n\n    If `fix_even_unparsable` is True, then we will still apply fixes to unparsable\n    sections, but will do so *without validation*. That means that the final\n    element of the return value will always return `True`, so that we don't interrupt\n    the validity checking of any outer (parsable) sections.\n    \"\"\"\n    if not fixes or segment.is_raw():\n        return segment, [], [], True\n\n    seg_buffer = []\n    before = []\n    after = []\n    fixes_applied: list[LintFix] = []\n    requires_validate = False\n\n    for seg in segment.segments:\n        # Look for uuid match.\n        # This handles potential positioning ambiguity.\n        anchor_info: Optional[AnchorEditInfo] = fixes.pop(seg.uuid, None)\n\n        if anchor_info is None:\n            # No fix matches here, just add the segment and move on.\n            seg_buffer.append(seg)\n            continue\n\n        # Otherwise there is a fix match.\n        seg_fixes = anchor_info.fixes\n        if (\n            len(seg_fixes) == 2 and seg_fixes[0].edit_type == \"create_after\"\n        ):  # pragma: no cover\n            # Must be create_before & create_after. Swap so the\n            # \"before\" comes first.\n            seg_fixes.reverse()\n\n        for f in anchor_info.fixes:\n            assert f.anchor.uuid == seg.uuid\n            fixes_applied.append(f)\n            linter_logger.debug(\n                \"Matched fix for %s against segment: %s -> %s\",\n                rule_code,\n                f,\n                seg,\n            )\n\n            # Deletes are easy.\n            if f.edit_type == \"delete\":\n                # We're just getting rid of this segment.\n                requires_validate = True\n                # NOTE: We don't add the segment in this case.\n                continue\n\n            # Otherwise it must be a replace or a create.\n            assert f.edit_type in (\n                \"replace\",\n                \"create_before\",\n                \"create_after\",\n            ), f\"Unexpected edit_type: {f.edit_type!r} in {f!r}\"\n\n            if f.edit_type == \"create_after\" and len(anchor_info.fixes) == 1:\n                # in the case of a creation after that is not part\n                # of a create_before/create_after pair, also add\n                # this segment before the edit.\n                seg_buffer.append(seg)\n\n            # We're doing a replacement (it could be a single\n            # segment or an iterable)\n            assert f.edit, f\"Edit {f.edit_type!r} requires `edit`.\"\n            consumed_pos = False\n            for s in f.edit:\n                seg_buffer.append(s)\n                # If one of them has the same raw representation\n                # then the first that matches gets to take the\n                # original position marker.\n                if f.edit_type == \"replace\" and s.raw == seg.raw and not consumed_pos:\n                    seg_buffer[-1].pos_marker = seg.pos_marker\n                    consumed_pos = True\n\n            # If we're just editing a segment AND keeping the type the\n            # same then no need to validate. Otherwise we should\n            # trigger a validation (e.g. for creations or\n            # multi-replace).\n            if not (\n                f.edit_type == \"replace\"\n                and len(f.edit) == 1\n                and f.edit[0].class_types == seg.class_types\n            ):\n                requires_validate = True\n\n            if f.edit_type == \"create_before\":\n                # in the case of a creation before, also add this\n                # segment on the end\n                seg_buffer.append(seg)\n\n    # Invalidate any caches\n    segment.invalidate_caches()\n\n    # If any fixes applied, do an intermediate reposition. When applying\n    # fixes to children and then trying to reposition them, that recursion\n    # may rely on the parent having already populated positions for any\n    # of the fixes applied there first. This ensures those segments have\n    # working positions to work with.\n    if fixes_applied:\n        assert segment.pos_marker\n        seg_buffer = list(\n            segment._position_segments(tuple(seg_buffer), parent_pos=segment.pos_marker)\n        )\n\n    # Then recurse (i.e. deal with the children) (Requeueing)\n    seg_queue = seg_buffer\n    seg_buffer = []\n    for seg in seg_queue:\n        s, pre, post, validated = apply_fixes(seg, dialect, rule_code, fixes)\n        # 'before' and 'after' will usually be empty. Only used when\n        # lower-level fixes left 'seg' with non-code (usually\n        # whitespace) segments as the first or last children. This is\n        # generally not allowed (see the can_start_end_non_code field),\n        # and these segments need to be \"bubbled up\" the tree.\n        seg_buffer += pre + [s] + post\n        # If we fail to validate a child segment, make sure to validate this\n        # segment.\n        if not validated:\n            requires_validate = True\n\n    # Most correct whitespace positioning will have already been handled\n    # _however_, the exception is `replace` edits which match start or\n    # end with whitespace. We also need to handle any leading or trailing\n    # whitespace ejected from the any fixes applied to child segments.\n    # Here we handle those by checking the start and end of the resulting\n    # segment sequence for whitespace.\n    # If we're left with any non-code at the end, trim them off and pass them\n    # up to the parent segment for handling.\n    if not segment.can_start_end_non_code:\n        _idx = 0\n        for _idx in range(0, len(seg_buffer)):\n            if segment._is_code_or_meta(seg_buffer[_idx]):\n                break\n        before = seg_buffer[:_idx]\n        seg_buffer = seg_buffer[_idx:]\n\n        _idx = len(seg_buffer)\n        for _idx in range(len(seg_buffer), 0, -1):\n            if segment._is_code_or_meta(seg_buffer[_idx - 1]):\n                break\n        after = seg_buffer[_idx:]\n        seg_buffer = seg_buffer[:_idx]\n\n    # Reform into a new segment\n    assert segment.pos_marker\n    try:\n        new_seg = segment.__class__(\n            # Realign the segments within\n            segments=segment._position_segments(\n                tuple(seg_buffer), parent_pos=segment.pos_marker\n            ),\n            pos_marker=segment.pos_marker,\n            # Pass through any additional kwargs\n            **{k: getattr(segment, k) for k in segment.additional_kwargs},\n        )\n    except AssertionError as err:  # pragma: no cover\n        # An AssertionError on creating a new segment is likely a whitespace\n        # check fail. If possible add information about the fixes we tried to\n        # apply, before re-raising.\n        # NOTE: only available in python 3.11+.\n        if hasattr(err, \"add_note\"):\n            err.add_note(f\" After applying fixes: {fixes_applied}.\")\n        raise err\n\n    # Handle any necessary validation.\n    if requires_validate:\n        # Was it already unparsable?\n        if \"unparsable\" in segment.descendant_type_set | segment.class_types:\n            if fix_even_unparsable:\n                # If we're fixing even unparsable sections, there's no point trying\n                # to validate, it will always fail. We may still want to validate\n                # other sections of the file though, so we should just declare *this*\n                # part of the file to be all good.\n                validated = True\n            else:\n                # It was already unparsable, but we're being asked to validate.\n                # Don't any apply fixes from within this region and just return the\n                # original segment.\n                return segment, [], [], True\n        # Otherwise only validate if there's a match_grammar. Otherwise we may get\n        # strange results (for example with the BracketedSegment).\n        elif hasattr(new_seg, \"match_grammar\"):\n            validated = new_seg.validate_segment_with_reparse(dialect)\n    else:\n        validated = not requires_validate\n    # Return the new segment and any non-code that needs to bubble up\n    # the tree.\n    # NOTE: We pass on whether this segment has been validated. It's\n    # very possible that our parsing here may fail depending on the\n    # type of segment that has been replaced, but if not we rely on\n    # a parent segment still being valid. If we get all the way up\n    # to the root and it's still not valid - that's a problem.\n    return new_seg, before, after, validated\n", "type": "function"}, {"name": "fix", "is_method": true, "class_name": "Linter", "parameters": ["self", "tree", "config", "fname", "templated_file"], "calls": ["self.get_rulepack", "self.lint_fix_parsed"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 931, "end_line": 950}, "code_snippet": "    def fix(\n        self,\n        tree: BaseSegment,\n        config: Optional[FluffConfig] = None,\n        fname: Optional[str] = None,\n        templated_file: Optional[\"TemplatedFile\"] = None,\n    ) -> tuple[BaseSegment, list[SQLBaseError]]:\n        \"\"\"Return the fixed tree and violations from lintfix when we're fixing.\"\"\"\n        config = config or self.config\n        rule_pack = self.get_rulepack(config=config)\n        fixed_tree, violations, _, _ = self.lint_fix_parsed(\n            tree,\n            config,\n            rule_pack,\n            fix=True,\n            fname=fname,\n            templated_file=templated_file,\n            formatter=self.formatter,\n        )\n        return fixed_tree, violations\n", "type": "function"}, {"name": "fix", "is_method": false, "class_name": null, "parameters": ["sql", "dialect", "rules", "exclude_rules", "config", "config_path", "fix_even_unparsable"], "calls": ["Linter", "linter.lint_string_wrapped", "get_simple_config", "cfg.get", "result.count_tmp_prs_errors", "fix_string"], "code_location": {"file": "simple.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/api", "start_line": 115, "end_line": 165}, "code_snippet": "def fix(\n    sql: str,\n    dialect: Optional[str] = None,\n    rules: Optional[list[str]] = None,\n    exclude_rules: Optional[list[str]] = None,\n    config: Optional[FluffConfig] = None,\n    config_path: Optional[str] = None,\n    fix_even_unparsable: Optional[bool] = None,\n) -> str:\n    \"\"\"Fix a SQL string.\n\n    Args:\n        sql (:obj:`str`): The SQL to be fixed.\n        dialect (:obj:`Optional[str]`, optional): A reference to the dialect of the SQL\n            to be fixed. Defaults to `ansi`.\n        rules (:obj:`Optional[list[str]`, optional): A subset of rule\n            references to fix for. Defaults to None.\n        exclude_rules (:obj:`Optional[list[str]`, optional): A subset of rule\n            references to avoid fixing for. Defaults to None.\n        config (:obj:`Optional[FluffConfig]`, optional): A configuration object\n            to use for the operation. Defaults to None.\n        config_path (:obj:`Optional[str]`, optional): A path to a .sqlfluff config,\n            which is only used if a `config` is not already provided.\n            Defaults to None.\n        fix_even_unparsable (:obj:`bool`, optional): Optional override for the\n            corresponding SQLFluff configuration value.\n\n    Returns:\n        :obj:`str` for the fixed SQL if possible.\n    \"\"\"\n    cfg = config or get_simple_config(\n        dialect=dialect,\n        rules=rules,\n        exclude_rules=exclude_rules,\n        config_path=config_path,\n    )\n    linter = Linter(config=cfg)\n\n    result = linter.lint_string_wrapped(sql, fix=True)\n    if fix_even_unparsable is None:\n        fix_even_unparsable = cfg.get(\"fix_even_unparsable\")\n    should_fix = True\n    if not fix_even_unparsable:\n        # If fix_even_unparsable wasn't set, check for templating or parse\n        # errors and suppress fixing if there were any.\n        _, num_filtered_errors = result.count_tmp_prs_errors()\n        if num_filtered_errors > 0:\n            should_fix = False\n    if should_fix:\n        sql = result.paths[0].files[0].fix_string()[0]\n    return sql\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "LintFix", "parameters": ["self", "edit_type", "anchor", "edit", "source"], "calls": ["ValueError", "ValueError", "all", "s.copy", "rules_logger.debug"], "code_location": {"file": "fix.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 35, "end_line": 88}, "code_snippet": "    def __init__(\n        self,\n        edit_type: str,\n        anchor: BaseSegment,\n        edit: Optional[Iterable[BaseSegment]] = None,\n        source: Optional[Iterable[BaseSegment]] = None,\n    ) -> None:\n        if edit_type not in (\n            \"create_before\",\n            \"create_after\",\n            \"replace\",\n            \"delete\",\n        ):  # pragma: no cover\n            raise ValueError(f\"Unexpected edit_type: {edit_type}\")\n        self.edit_type = edit_type\n        if not anchor:  # pragma: no cover\n            raise ValueError(\"Fixes must provide an anchor.\")\n        self.anchor = anchor\n        self.edit: Optional[list[BaseSegment]] = None\n        if edit is not None:\n            # Copy all the elements of edit to stop contamination.\n            # We're about to start stripping the position markers\n            # off some of the elements and we don't want to end up\n            # stripping the positions of the original elements of\n            # the parsed structure.\n            self.edit = [s.copy() for s in edit]\n            # Check that any edits don't have a position marker set.\n            # We should rely on realignment to make position markers.\n            # Strip position markers of anything enriched, otherwise things can get\n            # blurry\n            for seg in self.edit:\n                if seg.pos_marker:\n                    # Developer warning.\n                    rules_logger.debug(\n                        \"Developer Note: Edit segment found with preset position \"\n                        \"marker. These should be unset and calculated later.\"\n                    )\n                    seg.pos_marker = None\n            # Once stripped, we shouldn't replace any markers because\n            # later code may rely on them being accurate, which we\n            # can't guarantee with edits.\n        self.source = [seg for seg in source if seg.pos_marker] if source else []\n\n        # On creation of the fix we'll also validate the edits are non-trivial.\n        if self.edit_type in (\"create_before\", \"create_after\"):\n            assert self.edit, \"A create fix must have an edit.\"\n            # They should all have a non-zero raw.\n            assert all(\n                seg.raw for seg in self.edit\n            ), f\"Invalid edit found: {self.edit}.\"\n        elif self.edit_type == \"replace\":\n            assert (\n                self.edit != self.anchor\n            ), \"Fix created which replaces segment with itself.\"\n", "type": "function"}, {"name": "_get_fix", "is_method": true, "class_name": "Rule_CP03", "parameters": ["self", "segment", "fixed_raw"], "calls": ["_get_fix", "super"], "code_location": {"file": "CP03.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/capitalisation", "start_line": 53, "end_line": 54}, "code_snippet": "    def _get_fix(self, segment: BaseSegment, fixed_raw: str) -> LintFix:\n        return super()._get_fix(segment, fixed_raw)\n", "type": "function"}, {"name": "lint_fix_parsed", "is_method": true, "class_name": "Linter", "parameters": ["cls", "tree", "config", "rule_pack", "fix", "fname", "templated_file", "formatter"], "calls": ["config.get", "config.get", "linter_logger.info", "linter_logger.info", "config.get", "formatter.dispatch_lint_header", "cls.allowed_rule_ref_map", "IgnoreMask.from_tree", "phases.append", "range", "cls.remove_templated_errors", "format", "sorted", "config.get", "len", "linter_logger.info", "is_first_linter_pass", "tqdm", "tree.stringify", "rule_pack.codes", "progress_bar_crawler.set_description", "time.monotonic", "crawler.crawl", "is_first_linter_pass", "rule_timings.append", "linter_logger.info", "linter_logger.warning", "linter_logger.info", "compute_anchor_edit_info", "any", "isinstance", "is_first_linter_pass", "config.get", "anchor_info.items", "cls._report_conflicting_fixes_same_anchor", "linter_logger.debug", "apply_fixes", "time.monotonic", "anchor_info.values", "config.get", "tuple", "linter_logger.debug", "config.get", "tuple", "linter_logger.warning", "previous_versions.add", "cls._warn_unfixable"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 380, "end_line": 628}, "code_snippet": "    def lint_fix_parsed(\n        cls,\n        tree: BaseSegment,\n        config: FluffConfig,\n        rule_pack: RulePack,\n        fix: bool = False,\n        fname: Optional[str] = None,\n        templated_file: Optional[\"TemplatedFile\"] = None,\n        formatter: Optional[FormatterInterface] = None,\n    ) -> tuple[BaseSegment, list[SQLBaseError], Optional[IgnoreMask], RuleTimingsType]:\n        \"\"\"Lint and optionally fix a tree object.\"\"\"\n        # Keep track of the linting errors on the very first linter pass. The\n        # list of issues output by \"lint\" and \"fix\" only includes issues present\n        # in the initial SQL code, EXCLUDING any issues that may be created by\n        # the fixes themselves.\n        initial_linting_errors = []\n        # A placeholder for the fixes we had on the previous loop\n        last_fixes: Optional[list[LintFix]] = None\n        # Keep a set of previous versions to catch infinite loops.\n        previous_versions: set[tuple[str, tuple[\"SourceFix\", ...]]] = {(tree.raw, ())}\n        # Keep a buffer for recording rule timings.\n        rule_timings: RuleTimingsType = []\n\n        # If we are fixing then we want to loop up to the runaway_limit, otherwise just\n        # once for linting.\n        loop_limit = config.get(\"runaway_limit\") if fix else 1\n\n        # Dispatch the output for the lint header\n        if formatter:\n            formatter.dispatch_lint_header(\n                fname or \"<filename>\", sorted(rule_pack.codes())\n            )\n\n        # Look for comment segments which might indicate lines to ignore.\n        disable_noqa_except: Optional[str] = config.get(\"disable_noqa_except\")\n        if not config.get(\"disable_noqa\") or disable_noqa_except:\n            allowed_rules_ref_map = cls.allowed_rule_ref_map(\n                rule_pack.reference_map, disable_noqa_except\n            )\n            ignore_mask, ivs = IgnoreMask.from_tree(tree, allowed_rules_ref_map)\n            initial_linting_errors += ivs\n        else:\n            ignore_mask = None\n\n        save_tree = tree\n        # There are two phases of rule running.\n        # 1. The main loop is for most rules. These rules are assumed to\n        # interact and cause a cascade of fixes requiring multiple passes.\n        # These are run the `runaway_limit` number of times (default 10).\n        # 2. The post loop is for post-processing rules, not expected to trigger\n        # any downstream rules, e.g. capitalization fixes. They are run on the\n        # first loop and then twice at the end (once to fix, and once again to\n        # check result of fixes), but not in the intervening loops.\n        phases = [\"main\"]\n        if fix:\n            phases.append(\"post\")\n        for phase in phases:\n            if len(phases) > 1:\n                rules_this_phase = [\n                    rule for rule in rule_pack.rules if rule.lint_phase == phase\n                ]\n            else:\n                rules_this_phase = rule_pack.rules\n            for loop in range(loop_limit if phase == \"main\" else 2):\n\n                def is_first_linter_pass() -> bool:\n                    return phase == phases[0] and loop == 0\n\n                # Additional newlines are to assist in scanning linting loops\n                # during debugging.\n                linter_logger.info(\n                    f\"\\n\\nEntering linter phase {phase}, loop {loop + 1}/{loop_limit}\\n\"\n                )\n                changed = False\n\n                if is_first_linter_pass():\n                    # In order to compute initial_linting_errors correctly, need\n                    # to run all rules on the first loop of the main phase.\n                    rules_this_phase = rule_pack.rules\n                progress_bar_crawler = tqdm(\n                    rules_this_phase,\n                    desc=\"lint by rules\",\n                    leave=False,\n                    disable=progress_bar_configuration.disable_progress_bar,\n                )\n\n                for crawler in progress_bar_crawler:\n                    # Performance: After first loop pass, skip rules that don't\n                    # do fixes. Any results returned won't be seen by the user\n                    # anyway (linting errors ADDED by rules changing SQL, are\n                    # not reported back to the user - only initial linting errors),\n                    # so there's absolutely no reason to run them.\n                    if (\n                        fix\n                        and not is_first_linter_pass()\n                        and not crawler.is_fix_compatible\n                    ):\n                        continue\n\n                    progress_bar_crawler.set_description(f\"rule {crawler.code}\")\n                    t0 = time.monotonic()\n\n                    # fixes should be a dict {} with keys edit, delete, create\n                    # delete is just a list of segments to delete\n                    # edit and create are list of tuples. The first element is\n                    # the \"anchor\", the segment to look for either to edit or to\n                    # insert BEFORE. The second is the element to insert or create.\n                    linting_errors, _, fixes, _ = crawler.crawl(\n                        tree,\n                        dialect=config.get(\"dialect_obj\"),\n                        fix=fix,\n                        templated_file=templated_file,\n                        ignore_mask=ignore_mask,\n                        fname=fname,\n                        config=config,\n                    )\n                    if is_first_linter_pass():\n                        initial_linting_errors += linting_errors\n\n                    if fix and fixes:\n                        linter_logger.info(f\"Applying Fixes [{crawler.code}]: {fixes}\")\n                        # Do some sanity checks on the fixes before applying.\n                        anchor_info = compute_anchor_edit_info(fixes)\n                        if any(\n                            not info.is_valid for info in anchor_info.values()\n                        ):  # pragma: no cover\n                            message = (\n                                f\"Rule {crawler.code} returned conflicting \"\n                                \"fixes with the same anchor. This is only \"\n                                \"supported for create_before+create_after, so \"\n                                \"the fixes will not be applied. \"\n                            )\n                            for uuid, info in anchor_info.items():\n                                if not info.is_valid:\n                                    message += f\"\\n{uuid}:\"\n                                    for _fix in info.fixes:\n                                        message += f\"\\n    {_fix}\"\n                            cls._report_conflicting_fixes_same_anchor(message)\n                            for lint_result in linting_errors:\n                                lint_result.fixes = []\n                        elif fixes == last_fixes:\n                            # If we generate the same fixes two times in a row,\n                            # that means we're in a loop, and we want to stop.\n                            # (Fixes should address issues, hence different\n                            # and/or fewer fixes next time.)\n                            # This is most likely because fixes could not be safely\n                            # applied last time, so we should stop gracefully.\n                            linter_logger.debug(\n                                f\"Fixes generated for {crawler.code} are the same as \"\n                                \"the previous pass. Assuming that we cannot apply them \"\n                                \"safely. Passing gracefully.\"\n                            )\n                        else:\n                            # This is the happy path. We have fixes, now we want to\n                            # apply them.\n                            last_fixes = fixes\n                            new_tree, _, _, _valid = apply_fixes(\n                                tree,\n                                config.get(\"dialect_obj\"),\n                                crawler.code,\n                                anchor_info,\n                                fix_even_unparsable=config.get(\"fix_even_unparsable\"),\n                            )\n\n                            # Check for infinite loops. We use a combination of the\n                            # fixed templated file and the list of source fixes to\n                            # apply.\n                            loop_check_tuple = (\n                                new_tree.raw,\n                                tuple(new_tree.source_fixes),\n                            )\n                            # Was anything actually applied? If not, then the fixes we\n                            # had cannot be safely applied and we should stop trying.\n                            if loop_check_tuple == (tree.raw, tuple(tree.source_fixes)):\n                                linter_logger.debug(\n                                    f\"Fixes for {crawler.code} could not be safely be \"\n                                    \"applied. Likely due to initially unparsable file.\"\n                                )\n                            elif not _valid:\n                                # The fixes result in an invalid file. Don't apply\n                                # the fix and skip onward. Show a warning.\n                                linter_logger.warning(\n                                    f\"Fixes for {crawler.code} not applied, as it \"\n                                    \"would result in an unparsable file. Please \"\n                                    \"report this as a bug with a minimal query \"\n                                    \"which demonstrates this warning.\"\n                                )\n                            elif loop_check_tuple not in previous_versions:\n                                # We've not seen this version of the file so\n                                # far. Continue.\n                                tree = new_tree\n                                previous_versions.add(loop_check_tuple)\n                                changed = True\n                                continue\n                            else:\n                                # Applying these fixes took us back to a state\n                                # which we've seen before. We're in a loop, so\n                                # we want to stop.\n                                cls._warn_unfixable(crawler.code)\n\n                    # Record rule timing\n                    rule_timings.append(\n                        (crawler.code, crawler.name, time.monotonic() - t0)\n                    )\n\n                if fix and not changed:\n                    # We did not change the file. Either the file is clean (no\n                    # fixes), or any fixes which are present will take us back\n                    # to a previous state.\n                    linter_logger.info(\n                        f\"Fix loop complete for {phase} phase. Stability \"\n                        f\"achieved after {loop}/{loop_limit} loops.\"\n                    )\n                    break\n            else:\n                if fix:\n                    # The linter loop hit the limit before reaching a stable point\n                    # (i.e. free of lint errors). If this happens, it's usually\n                    # because one or more rules produced fixes which did not address\n                    # the original issue **or** created new issues.\n                    linter_logger.warning(\n                        f\"Loop limit on fixes reached [{loop_limit}].\"\n                    )\n\n                    # Discard any fixes for the linting errors, since they caused a\n                    # loop. IMPORTANT: By doing this, we are telling SQLFluff that\n                    # these linting errors are \"unfixable\". This is important,\n                    # because when \"sqlfluff fix\" encounters unfixable lint errors,\n                    # it exits with a \"failure\" exit code, which is exactly what we\n                    # want in this situation. (Reason: Although this is more of an\n                    # internal SQLFluff issue, users deserve to know about it,\n                    # because it means their file(s) weren't fixed.\n                    for violation in initial_linting_errors:\n                        if isinstance(violation, SQLLintError):\n                            violation.fixes = []\n\n                    # Return the original parse tree, before any fixes were applied.\n                    # Reason: When the linter hits the loop limit, the file is often\n                    # messy, e.g. some of the fixes were applied repeatedly, possibly\n                    # other weird things. We don't want the user to see this junk!\n                    return save_tree, initial_linting_errors, ignore_mask, rule_timings\n\n        if config.get(\"ignore_templated_areas\", default=True):\n            initial_linting_errors = cls.remove_templated_errors(initial_linting_errors)\n\n        linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Fixed Tree:\"))\n        linter_logger.info(\"\\n\" + tree.stringify())\n\n        return tree, initial_linting_errors, ignore_mask, rule_timings\n", "type": "function"}, {"name": "test__linted_file__build_up_fixed_source_string", "is_method": false, "class_name": null, "parameters": ["source_slices", "source_patches", "raw_source_string", "expected_result", "caplog"], "calls": ["pytest.mark.parametrize", "caplog.at_level", "LintedFile._build_up_fixed_source_string", "slice", "slice", "slice", "slice", "FixPatch", "slice", "slice", "slice", "FixPatch", "slice", "slice", "slice", "FixPatch", "slice", "slice", "slice", "FixPatch", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice"], "code_location": {"file": "linted_file_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 51, "end_line": 62}, "code_snippet": "def test__linted_file__build_up_fixed_source_string(\n    source_slices, source_patches, raw_source_string, expected_result, caplog\n):\n    \"\"\"Test _build_up_fixed_source_string.\n\n    This is part of fix_string().\n    \"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.linter\"):\n        result = LintedFile._build_up_fixed_source_string(\n            source_slices, source_patches, raw_source_string\n        )\n    assert result == expected_result\n", "type": "function"}, {"name": "LintFix", "docstring": "A class to hold a potential fix to a linting violation.\n\nArgs:\n    edit_type (:obj:`str`): One of `create_before`, `create_after`,\n        `replace`, `delete` to indicate the kind of fix this represents.\n    anchor (:obj:`BaseSegment`): A segment which represents\n        the *position* that this fix should be applied at. For deletions\n        it represents the segment to delete, for creations it implies the\n        position to create at (with the existing element at this position\n        to be moved *after* the edit), for a `replace` it implies the\n        segment to be replaced.\n    edit (iterable of :obj:`BaseSegment`, optional): For `replace` and\n        `create` fixes, this holds the iterable of segments to create\n        or replace at the given `anchor` point.\n    source (iterable of :obj:`BaseSegment`, optional): For `replace` and\n        `create` fixes, this holds iterable of segments that provided\n        code. IMPORTANT: The linter uses this to prevent copying material\n        from templated areas.", "methods": ["__init__", "is_just_source_edit", "__repr__", "to_dict", "__eq__", "delete", "replace", "create_before", "create_after", "get_fix_slices", "has_template_conflicts", "_raw_slices_from_templated_slices"], "attributes": [], "code_location": {"file": "fix.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 14, "end_line": 413}, "type": "class"}, {"name": "do_fixes", "is_method": false, "class_name": null, "parameters": ["result", "formatter", "fixed_file_suffix"], "calls": ["result.persist_changes", "all", "click.echo", "click.echo", "click.echo", "res.values", "click.echo"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 784, "end_line": 806}, "code_snippet": "def do_fixes(\n    result: LintingResult,\n    formatter: Optional[OutputStreamFormatter] = None,\n    fixed_file_suffix: str = \"\",\n) -> bool:\n    \"\"\"Actually do the fixes.\"\"\"\n    if formatter and formatter.verbosity >= 0:\n        click.echo(\"Persisting Changes...\")\n    res = result.persist_changes(\n        formatter=formatter, fixed_file_suffix=fixed_file_suffix\n    )\n    if all(res.values()):\n        if formatter and formatter.verbosity >= 0:\n            click.echo(\"Done. Please check your files to confirm.\")\n        return True\n    # If some failed then return false\n    click.echo(\n        \"Done. Some operations failed. Please check your files to confirm.\"\n    )  # pragma: no cover\n    click.echo(\n        \"Some errors cannot be fixed or there is another error blocking it.\"\n    )  # pragma: no cover\n    return False  # pragma: no cover\n", "type": "function"}, {"name": "test__fix__generate_source_patches", "is_method": false, "class_name": null, "parameters": ["tree", "templated_file", "expected_result", "caplog"], "calls": ["pytest.mark.parametrize", "caplog.at_level", "generate_source_patches", "RawSegment", "RawSegment", "BaseSegment", "BaseSegment", "BaseSegment", "PositionMarker", "PositionMarker", "FixPatch", "FixPatch", "FixPatch", "FixPatch", "FixPatch", "FixPatch", "slice", "slice", "slice", "slice", "slice", "slice", "RawSegment", "RawSegment", "RawSegment", "slice", "slice", "TemplateSegment", "RawSegment", "RawSegment", "RawSegment", "slice", "slice", "TemplateSegment", "RawSegment", "RawSegment", "RawSegment", "slice", "slice", "slice", "slice", "slice", "slice", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "SourceFix", "SourceFix", "slice", "slice", "slice", "slice"], "code_location": {"file": "fix_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 195, "end_line": 202}, "code_snippet": "def test__fix__generate_source_patches(tree, templated_file, expected_result, caplog):\n    \"\"\"Test generate_source_patches.\n\n    This is part of fix_string().\n    \"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.linter\"):\n        result = generate_source_patches(tree, templated_file)\n    assert result == expected_result\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.34880518913269043}
{"question": "How can SQLFluff's plugin API be used to create custom linting rules?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's plugin API can be used to create custom linting rules through a well-defined extension mechanism that integrates seamlessly with the core system. The plugin API usage works as follows: 1) Rule Definition - Custom rules inherit from BaseRule class and implement the required _eval() method that contains the core linting logic; 2) Metadata Declaration - Rules declare their metadata including code, name, description, groups, and aliases through class attributes or the RuleMetaclass; 3) Plugin Registration - Custom rules are registered through Python's entry point system in setup.py or pyproject.toml, making them discoverable by SQLFluff; 4) Configuration Integration - Plugin rules can define their own configuration options that integrate with SQLFluff's configuration system; 5) Fix Generation - Custom rules can generate LintFix objects to provide automatic corrections for violations they detect; 6) Context Access - Plugin rules have access to the same rule context as built-in rules, including parse tree, configuration, and dialect information; 7) Error Handling - Plugin rules should implement proper error handling to prevent crashes and provide meaningful error messages; 8) Documentation - Custom rules can provide documentation that gets integrated into SQLFluff's help system; 9) Testing - Plugin rules should include comprehensive tests to ensure they work correctly across different SQL dialects and scenarios; 10) Distribution - Custom rules can be distributed as separate Python packages that users can install and configure independently.", "score": null, "retrieved_content": [{"name": "diff_cover_report_quality", "is_method": false, "class_name": null, "parameters": [], "calls": ["SQLFluffViolationReporter"], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 123, "end_line": 131}, "code_snippet": "def diff_cover_report_quality(**kw) -> SQLFluffViolationReporter:\n    \"\"\"Returns the SQLFluff plugin.\n\n    This function is registered as a diff_cover entry point. diff-quality calls\n    it in order to \"discover\" the SQLFluff plugin.\n\n    :return: Object that implements the BaseViolationReporter ABC\n    \"\"\"\n    return SQLFluffViolationReporter(**kw)\n", "type": "function"}, {"name": "SQLFluffRule", "docstring": "SQLFluff rule directive for sphinx.\n\nRule directives can be used as shown below.\n\n.. code-block:: rst\n\n    .. sqlfluff:rule:: AM01\n                       ambiguous.distinct\n\n        Write the documentation for the rule here.\n\nTo cross reference (i.e. refer to) objects defined like this\nboth the code and name reference is available:\n\n.. code-block:: rst\n\n    :sqlfluff:ref:`CP02`\n    :sqlfluff:ref:`capitalisation.identifiers`", "methods": ["handle_signature", "add_target_and_index", "_object_hierarchy_parts", "_toc_entry_name"], "attributes": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 10, "end_line": 79}, "type": "class"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-plugin-example/src/sqlfluff_plugin_example", "start_line": 23, "end_line": 38}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: It is much better that we only import the rule on demand.\n    The root module of the plugin (i.e. this file which contains\n    all of the hook implementations) should have fully loaded before\n    we try and import the rules. This is partly for performance\n    reasons - but more because the definition of a BaseRule requires\n    that all of the get_configs_info() methods have both been\n    defined _and have run_ before so all the validation information\n    is available for the validation steps in the meta class.\n    \"\"\"\n    # i.e. we DO recommend importing here:\n    from sqlfluff_plugin_example.rules import Rule_Example_L001  # noqa: F811\n\n    return [Rule_Example_L001]\n", "type": "function"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/aliasing", "start_line": 50, "end_line": 76}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.aliasing.AL01 import Rule_AL01\n    from sqlfluff.rules.aliasing.AL02 import Rule_AL02\n    from sqlfluff.rules.aliasing.AL03 import Rule_AL03\n    from sqlfluff.rules.aliasing.AL04 import Rule_AL04\n    from sqlfluff.rules.aliasing.AL05 import Rule_AL05\n    from sqlfluff.rules.aliasing.AL06 import Rule_AL06\n    from sqlfluff.rules.aliasing.AL07 import Rule_AL07\n    from sqlfluff.rules.aliasing.AL08 import Rule_AL08\n    from sqlfluff.rules.aliasing.AL09 import Rule_AL09\n\n    return [\n        Rule_AL01,\n        Rule_AL02,\n        Rule_AL03,\n        Rule_AL04,\n        Rule_AL05,\n        Rule_AL06,\n        Rule_AL07,\n        Rule_AL08,\n        Rule_AL09,\n    ]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/jinja", "start_line": 8, "end_line": 16}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.jinja.JJ01 import Rule_JJ01\n\n    return [Rule_JJ01]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/capitalisation", "start_line": 41, "end_line": 53}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.capitalisation.CP01 import Rule_CP01\n    from sqlfluff.rules.capitalisation.CP02 import Rule_CP02\n    from sqlfluff.rules.capitalisation.CP03 import Rule_CP03\n    from sqlfluff.rules.capitalisation.CP04 import Rule_CP04\n    from sqlfluff.rules.capitalisation.CP05 import Rule_CP05\n\n    return [Rule_CP01, Rule_CP02, Rule_CP03, Rule_CP04, Rule_CP05]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 26, "end_line": 56}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.structure.ST01 import Rule_ST01\n    from sqlfluff.rules.structure.ST02 import Rule_ST02\n    from sqlfluff.rules.structure.ST03 import Rule_ST03\n    from sqlfluff.rules.structure.ST04 import Rule_ST04\n    from sqlfluff.rules.structure.ST05 import Rule_ST05\n    from sqlfluff.rules.structure.ST06 import Rule_ST06\n    from sqlfluff.rules.structure.ST07 import Rule_ST07\n    from sqlfluff.rules.structure.ST08 import Rule_ST08\n    from sqlfluff.rules.structure.ST09 import Rule_ST09\n    from sqlfluff.rules.structure.ST10 import Rule_ST10\n    from sqlfluff.rules.structure.ST11 import Rule_ST11\n\n    return [\n        Rule_ST01,\n        Rule_ST02,\n        Rule_ST03,\n        Rule_ST04,\n        Rule_ST05,\n        Rule_ST06,\n        Rule_ST07,\n        Rule_ST08,\n        Rule_ST09,\n        Rule_ST10,\n        Rule_ST11,\n    ]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 56, "end_line": 69}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.references.RF01 import Rule_RF01\n    from sqlfluff.rules.references.RF02 import Rule_RF02\n    from sqlfluff.rules.references.RF03 import Rule_RF03\n    from sqlfluff.rules.references.RF04 import Rule_RF04\n    from sqlfluff.rules.references.RF05 import Rule_RF05\n    from sqlfluff.rules.references.RF06 import Rule_RF06\n\n    return [Rule_RF01, Rule_RF02, Rule_RF03, Rule_RF04, Rule_RF05, Rule_RF06]\n", "type": "function"}, {"name": "cli", "is_method": false, "class_name": null, "parameters": [], "calls": ["click.group", "click.version_option"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 480, "end_line": 481}, "code_snippet": "def cli() -> None:\n    \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.34369707107543945}
{"question": "How can SQLFluff's BaseRule API be extended to implement new rule types?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's BaseRule API can be extended to implement new rule types through inheritance and customization of the base rule infrastructure. The BaseRule API extension works as follows: 1) Class Inheritance - New rule types inherit from BaseRule class and can override specific methods to customize behavior while maintaining compatibility; 2) _eval() Method Implementation - The core rule logic is implemented in the _eval() method, which receives segments and context and returns violations or fixes; 3) Metadata Customization - Rules can customize their metadata including code, name, description, groups, aliases, and configuration options; 4) Crawl Behavior Configuration - Rules can specify their crawl behavior (root, segment, or statement-level) to control how they traverse the parse tree; 5) Configuration Integration - New rule types can define their own configuration parameters that integrate with SQLFluff's configuration system; 6) Fix Generation - Extended rules can generate LintFix objects to provide automatic corrections, leveraging the existing fix infrastructure; 7) Context Access - Extended rules have access to the full rule context including parse tree, configuration, dialect information, and helper methods; 8) Error Handling - Extended rules can implement custom error handling and validation logic specific to their requirements; 9) Performance Optimization - Extended rules can implement caching and optimization strategies to improve performance; 10) Testing Framework - Extended rules can leverage SQLFluff's testing utilities to ensure they work correctly across different scenarios and dialects.", "score": null, "retrieved_content": [{"name": "RuleMetaclass", "docstring": "The metaclass for rules.\n\nThis metaclass provides provides auto-enrichment of the\nrule docstring so that examples, groups, aliases and\nnames are added.\n\nThe reason we enrich the docstring is so that it can be\npicked up by autodoc and all be displayed in the sqlfluff\ndocs.", "methods": ["_populate_code_and_description", "_populate_docstring", "__new__"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 151, "end_line": 361}, "type": "class"}, {"name": "BaseRule", "docstring": "The base class for a rule.\n\nArgs:\n    code (:obj:`str`): The identifier for this rule, used in inclusion\n        or exclusion.\n    description (:obj:`str`): A human readable description of what this\n        rule does. It will be displayed when any violations are found.", "methods": ["__init__", "get_config_ref", "_eval", "crawl", "_log_critical_errors", "_process_lint_result", "filter_meta", "get_parent_of", "discard_unsafe_fixes", "_adjust_anchors_for_fixes", "_choose_anchor_segment"], "attributes": ["_check_docstring", "_works_on_unparsable", "_adjust_anchors", "targets_templated", "template_safe_fixes", "lint_phase", "is_fix_compatible", "split_comma_separated_string"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 364, "end_line": 834}, "type": "class"}, {"name": "SQLFluffRule", "docstring": "SQLFluff rule directive for sphinx.\n\nRule directives can be used as shown below.\n\n.. code-block:: rst\n\n    .. sqlfluff:rule:: AM01\n                       ambiguous.distinct\n\n        Write the documentation for the rule here.\n\nTo cross reference (i.e. refer to) objects defined like this\nboth the code and name reference is available:\n\n.. code-block:: rst\n\n    :sqlfluff:ref:`CP02`\n    :sqlfluff:ref:`capitalisation.identifiers`", "methods": ["handle_signature", "add_target_and_index", "_object_hierarchy_parts", "_toc_entry_name"], "attributes": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 10, "end_line": 79}, "type": "class"}, {"name": "SQLLintError", "docstring": "An error which occurred during linting.\n\nIn particular we reference the rule here to do extended logging based on\nthe rule in question which caused the fail.\n\nArgs:\n    segment (:obj:`BaseSegment`, optional): The segment which is relevant\n        for the failure in parsing. This is likely to be a subclass of\n        `BaseSegment` rather than the parent class itself. This is mostly\n        used for logging and for referencing position.", "methods": ["__init__", "__reduce__", "to_dict", "fixable", "rule_code", "rule_name", "source_signature", "__repr__"], "attributes": ["_identifier"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 249, "end_line": 383}, "type": "class"}, {"name": "__new__", "is_method": true, "class_name": "RuleMetaclass", "parameters": ["mcs", "name", "bases", "class_dict"], "calls": ["reversed", "reversed", "RuleMetaclass._populate_docstring", "class_dict.get", "__new__", "RuleMetaclass._populate_code_and_description", "RuleMetaclass._valid_rule_name_regex.match", "SQLFluffUserError", "super"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 314, "end_line": 361}, "code_snippet": "    def __new__(\n        mcs,\n        name: str,\n        bases: list[\"BaseRule\"],\n        class_dict: dict[str, Any],\n    ) -> \"RuleMetaclass\":\n        \"\"\"Generate a new class.\"\"\"\n        # Optionally, groups may be inherited. At this stage of initialisation\n        # they won't have been. Check parent classes if they exist.\n        # names, aliases and description are less appropriate to inherit.\n        # NOTE: This applies in particular to CP02, which inherits all groups\n        # from CP01. If we don't do this, those groups don't show in the docs.\n        for base in reversed(bases):\n            if \"groups\" in class_dict:\n                break\n            elif base.groups:\n                class_dict[\"groups\"] = base.groups\n                break\n\n        # If the rule doesn't itself define `config_keywords`, check the parent\n        # classes for them. If we don't do this then they'll still be available to\n        # the rule, but they won't appear in the docs.\n        for base in reversed(bases):\n            if \"config_keywords\" in class_dict:\n                break\n            elif base.config_keywords:\n                class_dict[\"config_keywords\"] = base.config_keywords\n                break\n\n        class_dict = RuleMetaclass._populate_docstring(name, class_dict)\n        # Don't try and infer code and description for the base classes\n        if name not in (\"BaseRule\",):\n            class_dict = RuleMetaclass._populate_code_and_description(name, class_dict)\n        # Validate rule names\n        rule_name = class_dict.get(\"name\", \"\")\n        if rule_name:\n            if not RuleMetaclass._valid_rule_name_regex.match(rule_name):\n                raise SQLFluffUserError(\n                    f\"Tried to define rule with unexpected \"\n                    f\"name format: {rule_name}. Rule names should be lowercase \"\n                    \"and snake_case with optional `.` characters to indicate \"\n                    \"a namespace or grouping. e.g. `layout.spacing`.\"\n                )\n\n        # Use the stock __new__ method now we've adjusted the docstring.\n        # There are no overload variants of type.__new__ that are compatible, so\n        # we ignore type checking in this case.\n        return super().__new__(mcs, name, bases, class_dict)  # type: ignore\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "BaseRule", "parameters": ["self", "code", "description"], "calls": ["kwargs.items", "RuleLoggingAdapter", "kwargs.keys", "ValueError", "format"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 421, "end_line": 441}, "code_snippet": "    def __init__(self, code: str, description: str, **kwargs: Any) -> None:\n        self.description = description\n        self.code = code\n        # kwargs represents the config passed to the rule. Add all kwargs as class\n        # attributes so they can be accessed in rules which inherit from this class\n        for key, value in kwargs.items():\n            self.__dict__[key] = value\n\n        # We also define a custom logger here, which also includes the code\n        # of the rule in the logging.\n        self.logger = RuleLoggingAdapter(rules_logger, {\"code\": code})\n        # Validate that declared configuration options exist\n        for keyword in self.config_keywords:\n            if keyword not in kwargs.keys():\n                raise ValueError(\n                    (\n                        \"Unrecognized config '{}' for Rule {}. If this \"\n                        \"is a new option, please add it to \"\n                        \"`default_config.cfg` or plugin specific config.\"\n                    ).format(keyword, code)\n                )\n", "type": "function"}, {"name": "BaseSegment", "docstring": "The base segment element.\n\nThis defines the base element which drives both Lexing, Parsing and Linting.\nA large chunk of the logic which defines those three operations are centered\nhere. Much of what is defined in the BaseSegment is also used by its many\nsubclasses rather than directly here.\n\nFor clarity, the `BaseSegment` is mostly centered around a segment which contains\nother subsegments. For segments which don't have *children*, refer to the\n`RawSegment` class (which still inherits from this one).\n\nSegments are used both as instances to hold chunks of text, but also as classes\nthemselves where they function a lot like grammars, and return instances of\nthemselves when they match. The many classmethods in this class are usually to serve\ntheir purpose as a matcher.", "methods": ["__init__", "__setattr__", "__eq__", "_hash", "__hash__", "__repr__", "__getstate__", "__setstate__", "_comments", "_non_comments", "is_code", "_code_indices", "is_comment", "is_whitespace", "raw", "class_types", "descendant_type_set", "direct_descendant_type_set", "raw_upper", "raw_segments", "raw_segments_with_ancestors", "source_fixes", "first_non_whitespace_segment_raw_upper", "is_templated", "_suffix", "_position_segments", "simple", "cache_key", "is_optional", "class_is_type", "structural_simplify", "match", "_recalculate_caches", "_preface", "set_as_parent", "set_parent", "get_parent", "get_type", "count_segments", "is_type", "invalidate_caches", "get_start_point_marker", "get_end_point_marker", "get_start_loc", "get_end_loc", "stringify", "to_tuple", "copy", "as_record", "get_raw_segments", "raw_normalized", "iter_segments", "iter_unparsables", "type_set", "is_raw", "get_child", "get_children", "select_children", "recursive_crawl_all", "recursive_crawl", "path_to", "_is_code_or_meta", "validate_non_code_ends", "validate_segment_with_reparse", "_log_apply_fixes_check_issue", "edit", "from_result_segments"], "attributes": ["comment_separate", "is_meta", "can_start_end_non_code", "allow_empty"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/parser/segments", "start_line": 141, "end_line": 1239}, "type": "class"}, {"name": "RuleSet", "docstring": "Class to define a ruleset.\n\nA rule set is instantiated on module load, but the references\nto each of its classes are instantiated at runtime. This means\nthat configuration values can be passed to those rules live\nand be responsive to any changes in configuration from the\npath that the file is in.\n\nRules should be fetched using the :meth:`get_rulelist` command which\nalso handles any filtering (i.e. allowlisting and denylisting).\n\nNew rules should be added to the instance of this class using the\n:meth:`register` decorator. That decorator registers the class, but also\nperforms basic type and name-convention checks.\n\nThe code for the rule will be parsed from the name, the description\nfrom the docstring. The eval function is assumed that it will be\noverridden by the subclass, and the parent class raises an error on\nthis function if not overridden.", "methods": ["__init__", "_validate_config_options", "register", "_expand_rule_refs", "rule_reference_map", "get_rulepack", "copy"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 881, "end_line": 1213}, "type": "class"}, {"name": "SQLBaseError", "docstring": "Base Error Class for all violations.", "methods": ["__init__", "__eq__", "__reduce__", "fixable", "rule_code", "rule_name", "desc", "to_dict", "check_tuple", "source_signature", "ignore_if_in", "warning_if_in"], "attributes": ["_identifier", "_warning"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 36, "end_line": 150}, "type": "class"}, {"name": "test__api__fix_string_specific", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.fix"], "code_location": {"file": "simple_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 434, "end_line": 438}, "code_snippet": "def test__api__fix_string_specific():\n    \"\"\"Basic checking of lint functionality with a specific rule.\"\"\"\n    result = sqlfluff.fix(my_bad_query, rules=[\"CP01\"])\n    # Check actual result\n    assert result == \"SELECT  *, 1, blah AS  fOO  FROM myTable\"\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3368825912475586}
{"question": "How do different SQLFluff linting rules interact with the parsed SQL structure?", "answer": null, "relative_code_list": null, "ground_truth": "Different SQLFluff linting rules interact with the parsed SQL structure through a systematic approach that traverses the parse tree and analyzes specific patterns. The rule interaction works as follows: 1) Parse Tree Traversal - Rules receive the parsed SQL structure as a tree of BaseSegment objects and traverse it using crawl behaviors (root, segment, or statement-level crawling); 2) Segment Analysis - Each rule's _eval() method receives individual segments and analyzes their properties, children, and relationships within the parse tree; 3) Context Access - Rules access contextual information through the rule context, including parent segments, sibling relationships, and broader tree structure; 4) Pattern Matching - Rules use pattern matching to identify specific SQL constructs, such as finding all SELECT statements, JOIN clauses, or function calls; 5) Structural Validation - Rules validate the structural correctness of SQL constructs, checking for proper nesting, required elements, and syntax compliance; 6) Cross-Reference Analysis - Rules can analyze relationships between different parts of the SQL, such as checking if referenced tables exist or if aliases are properly defined; 7) Configuration Integration - Rules apply configuration settings to determine what constitutes a violation, allowing for flexible rule behavior; 8) Fix Generation - When violations are found, rules can generate LintFix objects that describe how to correct the issues in the parse tree; 9) Error Reporting - Rules report violations with specific locations, messages, and severity levels based on their analysis of the parse structure; 10) Performance Optimization - Rules use efficient traversal patterns and caching to minimize the performance impact of analyzing large parse trees.", "score": null, "retrieved_content": [{"name": "SQLLintError", "docstring": "An error which occurred during linting.\n\nIn particular we reference the rule here to do extended logging based on\nthe rule in question which caused the fail.\n\nArgs:\n    segment (:obj:`BaseSegment`, optional): The segment which is relevant\n        for the failure in parsing. This is likely to be a subclass of\n        `BaseSegment` rather than the parent class itself. This is mostly\n        used for logging and for referencing position.", "methods": ["__init__", "__reduce__", "to_dict", "fixable", "rule_code", "rule_name", "source_signature", "__repr__"], "attributes": ["_identifier"], "code_location": {"file": "errors.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core", "start_line": 249, "end_line": 383}, "type": "class"}, {"name": "test__rules__result_unparsable", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "any", "fluff_log_catcher", "linter.lint_string", "v.rule_code"], "code_location": {"file": "rules_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 214, "end_line": 229}, "code_snippet": "def test__rules__result_unparsable():\n    \"\"\"Test that the linter won't allow rules which make the file unparsable.\"\"\"\n    # Set up a linter with the user rule\n    linter = Linter(user_rules=[Rule_T003], dialect=\"ansi\", rules=[\"T003\"])\n    # Lint a simple parsable file and check we do get issues\n    # It's parsable, so we should get issues.\n    raw_sql = \"SELECT 1 FROM a\"\n    with fluff_log_catcher(logging.WARNING, \"sqlfluff\") as caplog:\n        res = linter.lint_string(raw_sql, fix=True)\n    # Check we got the warning.\n    assert \"would result in an unparsable file\" in caplog.text\n    # Check we get the violation.\n    assert any(v.rule_code() == \"T003\" for v in res.violations)\n    # The resulting file should be _the same_ because it would have resulted\n    # in an unparsable file if applied.\n    assert res.tree.raw == raw_sql\n", "type": "function"}, {"name": "test__config__rules_set_to_none", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "lntr.lint_path", "lnt.check_tuples_by_path", "FluffConfig.from_path"], "code_location": {"file": "fluffconfig_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/config", "start_line": 145, "end_line": 158}, "code_snippet": "def test__config__rules_set_to_none():\n    \"\"\"Test linting when rules are set to 'None'.\n\n    Ensure that all rules are still run.\n    \"\"\"\n    lntr = Linter(\n        config=FluffConfig.from_path(\"test/fixtures/config/rules_set_to_none\")\n    )\n    lnt = lntr.lint_path(\"test/fixtures/config/rules_set_to_none/test.sql\")\n    violations = lnt.check_tuples_by_path()\n    for k in violations:\n        assert (\"LT13\", 1, 1) in violations[k]\n        assert (\"AM04\", 12, 1) in violations[k]\n        assert (\"CP01\", 12, 10) in violations[k]\n", "type": "function"}, {"name": "SQLFluffRule", "docstring": "SQLFluff rule directive for sphinx.\n\nRule directives can be used as shown below.\n\n.. code-block:: rst\n\n    .. sqlfluff:rule:: AM01\n                       ambiguous.distinct\n\n        Write the documentation for the rule here.\n\nTo cross reference (i.e. refer to) objects defined like this\nboth the code and name reference is available:\n\n.. code-block:: rst\n\n    :sqlfluff:ref:`CP02`\n    :sqlfluff:ref:`capitalisation.identifiers`", "methods": ["handle_signature", "add_target_and_index", "_object_hierarchy_parts", "_toc_entry_name"], "attributes": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 10, "end_line": 79}, "type": "class"}, {"name": "test__api__info_rules", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.list_rules", "isinstance", "RuleTuple"], "code_location": {"file": "info_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 29, "end_line": 52}, "code_snippet": "def test__api__info_rules():\n    \"\"\"Basic linting of dialects.\"\"\"\n    rules = sqlfluff.list_rules()\n    assert isinstance(rules, list)\n    assert (\n        RuleTuple(\n            code=\"LT01\",\n            name=\"layout.spacing\",\n            description=\"Inappropriate Spacing.\",\n            groups=(\"all\", \"core\", \"layout\"),\n            aliases=(\n                \"L001\",\n                \"L005\",\n                \"L006\",\n                \"L008\",\n                \"L023\",\n                \"L024\",\n                \"L039\",\n                \"L048\",\n                \"L071\",\n            ),\n        )\n        in rules\n    )\n", "type": "function"}, {"name": "_lint_references_and_aliases", "is_method": true, "class_name": "Rule_RF02", "parameters": ["self", "table_aliases", "standalone_aliases", "references", "col_aliases", "using_cols", "parent_select", "rule_context"], "calls": ["self._find_sql_variables", "get_select_statement_info", "len", "r.qualification", "self._init_ignore_words_list", "r.raw.lower", "regex.search", "violation_buff.append", "self._is_root_from_clause", "table_aliases.append", "r.raw.lower", "LintResult", "table_alias.from_expression_element.path_to"], "code_location": {"file": "RF02.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 55, "end_line": 143}, "code_snippet": "    def _lint_references_and_aliases(\n        self,\n        table_aliases: list[AliasInfo],\n        standalone_aliases: list[BaseSegment],\n        references,\n        col_aliases: list[ColumnAliasInfo],\n        using_cols: list[BaseSegment],\n        parent_select: Optional[BaseSegment],\n        rule_context: RuleContext,\n    ) -> Optional[list[LintResult]]:\n        if parent_select:\n            parent_select_info = get_select_statement_info(\n                parent_select, rule_context.dialect\n            )\n            if parent_select_info:\n                # If we are looking at a subquery, include any table references\n                for table_alias in parent_select_info.table_aliases:\n                    is_from = self._is_root_from_clause(rule_context)\n                    if (\n                        table_alias.from_expression_element.path_to(\n                            rule_context.segment\n                        )\n                        or is_from\n                        or self.subqueries_ignore_external_references\n                    ):\n                        # Skip the subquery alias itself or if the subquery is inside\n                        # of a `from` or `join`` clause that isn't a nested where clause\n                        continue\n                    table_aliases.append(table_alias)\n\n        # Do we have more than one? If so, all references should be qualified.\n        if len(table_aliases) <= 1:\n            return None\n\n        # Get the ignore_words_list configuration.\n        try:\n            ignore_words_list = self.ignore_words_list\n        except AttributeError:\n            # First-time only, read the settings from configuration. This is\n            # very slow.\n            ignore_words_list = self._init_ignore_words_list()\n\n        sql_variables = self._find_sql_variables(rule_context)\n\n        # A buffer to keep any violations.\n        violation_buff = []\n        # Check all the references that we have.\n        for r in references:\n            # Skip if in ignore list\n            if ignore_words_list and r.raw.lower() in ignore_words_list:\n                continue\n\n            # Skip if a sql variable name inside the file\n            if r.raw.lower() in sql_variables:\n                continue\n\n            # Skip if matches ignore regex\n            if self.ignore_words_regex and regex.search(self.ignore_words_regex, r.raw):\n                continue\n\n            this_ref_type = r.qualification()\n            # Discard column aliases that\n            # refer to the current column reference.\n            col_alias_names = [\n                c.alias_identifier_name\n                for c in col_aliases\n                if r not in c.column_reference_segments\n            ]\n            if (\n                this_ref_type == \"unqualified\"\n                # Allow unqualified columns that\n                # are actually aliases defined\n                # in a different select clause element.\n                and r.raw not in col_alias_names\n                # Allow columns defined in a USING expression.\n                and r.raw not in [using_col.raw for using_col in using_cols]\n                # Allow columns defined as standalone aliases\n                # (e.g. value table functions from bigquery)\n                and r.raw not in [a.raw for a in standalone_aliases]\n            ):\n                violation_buff.append(\n                    LintResult(\n                        anchor=r,\n                        description=f\"Unqualified reference {r.raw!r} found in \"\n                        \"select with more than one referenced table/view.\",\n                    )\n                )\n\n        return violation_buff or None\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_RF01", "parameters": ["self", "context"], "calls": ["self.logger.debug", "self.logger.debug", "RF01Query.from_segment", "self._analyze_table_references", "context.segment.is_type", "next", "context.segment.recursive_crawl", "self._table_ref_as_tuple", "cast"], "code_location": {"file": "RF01.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 72, "end_line": 95}, "code_snippet": "    def _eval(self, context: RuleContext) -> list[LintResult]:\n        violations: list[LintResult] = []\n        dml_target_table: Optional[list[tuple[str, ...]]] = None\n        self.logger.debug(\"Trigger on: %s\", context.segment)\n        if not context.segment.is_type(\"select_statement\"):\n            # Extract first table reference. This will be the target\n            # table in a DML statement.\n            table_reference = next(\n                context.segment.recursive_crawl(\"table_reference\"), None\n            )\n            if table_reference:\n                dml_target_table = self._table_ref_as_tuple(\n                    cast(ObjectReferenceSegment, table_reference)\n                )\n\n        self.logger.debug(\"DML Reference Table: %s\", dml_target_table)\n        # Verify table references in any SELECT statements found in or\n        # below context.segment in the parser tree.\n        query: RF01Query = RF01Query.from_segment(context.segment, context.dialect)\n        query.parent_stack = context.parent_stack\n        self._analyze_table_references(\n            query, dml_target_table, context.dialect, violations\n        )\n        return violations\n", "type": "function"}, {"name": "test__dialect__base_broad_fix", "is_method": false, "class_name": null, "parameters": ["dialect", "file", "raise_critical_errors_after_fix", "caplog"], "calls": ["pytest.mark.parametrize", "load_file", "dict", "lex_and_parse", "print", "FluffConfig", "Linter", "linter.get_rulepack", "linter.lint_parsed", "parsed.tree.stringify"], "code_location": {"file": "dialects_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 85, "end_line": 120}, "code_snippet": "def test__dialect__base_broad_fix(\n    dialect, file, raise_critical_errors_after_fix, caplog\n):\n    \"\"\"Run a full fix with all rules, in search of critical errors.\n\n    NOTE: This suite does all of the same things as the above test\n    suite (the `parse_suite`), but also runs fix. In CI, we run\n    the above tests _with_ coverage tracking, but these we run\n    _without_.\n\n    The purpose of this test is as a more stretching run through\n    a wide range of test sql examples, and the full range of rules\n    to find any potential critical errors raised by any interactions\n    between different dialects and rules.\n\n    We also do not use DEBUG logging here because it gets _very_\n    noisy.\n    \"\"\"\n    raw = load_file(dialect, file)\n    config_overrides = dict(dialect=dialect)\n\n    parsed: Optional[ParsedString] = lex_and_parse(config_overrides, raw)\n    if not parsed:  # Empty file case\n        return\n    print(parsed.tree.stringify())\n\n    config = FluffConfig(overrides=config_overrides)\n    linter = Linter(config=config)\n    rule_pack = linter.get_rulepack()\n    # Due to \"raise_critical_errors_after_fix\" fixture \"fix\",\n    # will now throw.\n    linter.lint_parsed(\n        parsed,\n        rule_pack,\n        fix=True,\n    )\n", "type": "function"}, {"name": "_eval_gen", "is_method": true, "class_name": "Rule_CV12", "parameters": ["self", "context"], "calls": ["select_statement.is_type", "select_statement.get_child", "self._is_where_clause_simplifable", "set", "select_statement.recursive_crawl", "collections.deque", "enumerate", "where_clause.get_child", "self._get_subexpression_chunks", "self._get_from_expression_element_alias", "next", "encountered_references.add", "any", "join_clause.get_child", "is_type", "where_clause_fix_segments.popleft", "is_type", "where_clause_fix_segments.pop", "where_clause.get_child", "is_type", "is_type", "select_statement.recursive_crawl", "join_clause.recursive_crawl", "self._get_from_expression_element_alias", "set", "enumerate", "where_clause_fix_segments.extend", "where_clause_fix_segments.append", "LintResult", "LintResult", "LintResult", "collections.deque", "enumerate", "ExpressionSegment", "JoinOnConditionSegment", "JoinClauseSegment", "BinaryOperatorSegment", "all", "this_join_clause_subexpressions.add", "consumed_subexpressions.add", "LintResult", "is_type", "join_clause_fix_segments.popleft", "is_type", "join_clause_fix_segments.pop", "tuple", "LintResult", "seg.recursive_crawl", "len", "join_clause_fix_segments.extend", "join_clause_fix_segments.append", "KeywordSegment", "WhitespaceSegment", "WhitespaceSegment", "LintFix.replace", "LintFix.delete", "LintFix.delete", "col_ref.raw_upper.startswith", "BinaryOperatorSegment", "tuple", "LintFix.replace"], "code_location": {"file": "CV12.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/convention", "start_line": 67, "end_line": 250}, "code_snippet": "    def _eval_gen(self, context: RuleContext) -> Iterator[LintResult]:\n        # We are only interested in SELECT statement.\n        select_statement = context.segment\n        assert select_statement.is_type(\"select_statement\")\n\n        maybe_where_clause = select_statement.get_child(\"where_clause\")\n        if not maybe_where_clause:\n            return\n\n        where_clause = maybe_where_clause\n        where_clause_simplifable = self._is_where_clause_simplifable(where_clause)\n\n        if where_clause_simplifable:\n            expr = where_clause.get_child(\"expression\")\n            assert expr is not None\n            subexpressions = self._get_subexpression_chunks(expr)\n        else:\n            subexpressions = []\n        consumed_subexpressions = set()\n\n        # get references in from clause\n        select_table_references = [\n            *select_statement.recursive_crawl(\n                \"from_expression_element\",\n                no_recursive_seg_type=[\"join_clause\", \"select_statement\"],\n            )\n        ]\n\n        # track all seen references (from clause + all previous joins)\n        encountered_references = {\n            self._get_from_expression_element_alias(table_ref)\n            for table_ref in select_table_references\n        }\n\n        for join_clause in select_statement.recursive_crawl(\n            \"join_clause\", no_recursive_seg_type=[\"select_statement\"]\n        ):\n            # mark table reference as seen\n            join_table_reference = next(\n                join_clause.recursive_crawl(\n                    \"from_expression_element\",\n                    no_recursive_seg_type=[\"select_statement\"],\n                )\n            )\n            encountered_references.add(\n                self._get_from_expression_element_alias(join_table_reference)\n            )\n            join_clause_keywords = [\n                seg for seg in join_clause.segments if seg.type == \"keyword\"\n            ]\n\n            if any(\n                kw.raw_upper in (\"CROSS\", \"POSITIONAL\", \"USING\", \"APPLY\")\n                for kw in join_clause_keywords\n            ):\n                # If explicit CROSS JOIN is used, disregard lack of condition\n                # If explicit POSITIONAL JOIN is used, disregard lack of condition\n                # If explicit JOIN USING is used, disregard lack of condition\n                # If explicit CROSS/OUTER APPLY is used, disregard lack of condition\n                continue\n\n            this_join_condition = join_clause.get_child(\"join_on_condition\")\n            if this_join_condition:\n                # Join condition is present, no error reported.\n                continue\n\n            if not where_clause_simplifable:\n                yield LintResult(anchor=join_clause)\n            else:\n                this_join_clause_subexpressions = set()\n                for subexpr_idx, subexpr_segments in enumerate(subexpressions):\n                    if subexpr_idx in consumed_subexpressions:\n                        continue\n                    qualified_column_references = [\n                        col_ref\n                        for seg in subexpr_segments\n                        for col_ref in seg.recursive_crawl(\n                            \"column_reference\",\n                            no_recursive_seg_type=\"select_statement\",\n                        )\n                        if \"dot\" in col_ref.descendant_type_set\n                    ]\n                    if len(qualified_column_references) > 1 and all(\n                        col_ref.raw_upper.startswith(\n                            tuple(\n                                f\"{table_ref}.\" for table_ref in encountered_references\n                            )\n                        )\n                        for col_ref in qualified_column_references\n                    ):\n                        this_join_clause_subexpressions.add(subexpr_idx)\n                        consumed_subexpressions.add(subexpr_idx)\n\n                if not this_join_clause_subexpressions:\n                    yield LintResult(join_clause)\n                else:\n                    join_clause_fix_segments: Deque[BaseSegment] = collections.deque()\n                    for subexpr_idx, subexpr_segments in enumerate(subexpressions):\n                        if subexpr_idx in this_join_clause_subexpressions:\n                            join_clause_fix_segments.extend(subexpr_segments)\n                            join_clause_fix_segments.append(\n                                BinaryOperatorSegment(\"AND\")\n                            )\n\n                    while join_clause_fix_segments and join_clause_fix_segments[\n                        0\n                    ].is_type(\"whitespace\", \"binary_operator\"):\n                        join_clause_fix_segments.popleft()\n                    while join_clause_fix_segments and join_clause_fix_segments[\n                        -1\n                    ].is_type(\"whitespace\", \"binary_operator\"):\n                        join_clause_fix_segments.pop()\n\n                    join_on_expression = ExpressionSegment(\n                        tuple(join_clause_fix_segments),\n                    )\n                    join_on = JoinOnConditionSegment(\n                        (\n                            KeywordSegment(\"ON\"),\n                            WhitespaceSegment(),\n                            join_on_expression,\n                        )\n                    )\n                    join_clause_segment = JoinClauseSegment(\n                        (\n                            *join_clause.segments,\n                            WhitespaceSegment(),\n                            join_on,\n                        )\n                    )\n\n                    yield LintResult(\n                        anchor=join_clause,\n                        fixes=[\n                            LintFix.replace(\n                                join_clause,\n                                edit_segments=[join_clause_segment],\n                            )\n                        ],\n                    )\n\n        if not where_clause_simplifable:\n            return\n\n        if not consumed_subexpressions:\n            return\n\n        # Rewrite WHERE to keep conditions not moved to ON clauses\n        where_clause_fix_segments: Deque[BaseSegment] = collections.deque()\n        for subexpr_idx, subexpr_segments in enumerate(subexpressions):\n            if subexpr_idx not in consumed_subexpressions:\n                where_clause_fix_segments.extend(subexpr_segments)\n                where_clause_fix_segments.append(BinaryOperatorSegment(\"AND\"))\n\n        while where_clause_fix_segments and where_clause_fix_segments[0].is_type(\n            \"whitespace\", \"binary_operator\"\n        ):\n            where_clause_fix_segments.popleft()\n        while where_clause_fix_segments and where_clause_fix_segments[-1].is_type(\n            \"whitespace\", \"binary_operator\"\n        ):\n            where_clause_fix_segments.pop()\n\n        if where_clause_fix_segments:\n            where_clause_expr = where_clause.get_child(\"expression\")\n            assert where_clause_expr is not None\n            yield LintResult(\n                anchor=where_clause_expr,\n                fixes=[\n                    LintFix.replace(\n                        where_clause_expr, edit_segments=[*where_clause_fix_segments]\n                    )\n                ],\n            )\n        else:\n            assert select_statement.segments[-1].is_type(\"where_clause\")\n            assert select_statement.segments[-2].is_type(\"whitespace\", \"newline\")\n            yield LintResult(\n                anchor=where_clause,\n                fixes=[\n                    LintFix.delete(select_statement.segments[-2]),\n                    LintFix.delete(select_statement.segments[-1]),\n                ],\n            )\n", "type": "function"}, {"name": "_eval", "is_method": true, "class_name": "Rule_CP02", "parameters": ["self", "context"], "calls": ["identifiers_policy_applicable", "_eval", "LintResult", "super"], "code_location": {"file": "CP02.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/capitalisation", "start_line": 98, "end_line": 116}, "code_snippet": "    def _eval(self, context: RuleContext) -> Optional[list[LintResult]]:\n        # Return None if identifier is case-sensitive property to enable Change\n        # Data Feed\n        # https://docs.delta.io/2.0.0/delta-change-data-feed.html#enable-change-data-feed\n        if (\n            context.dialect.name in [\"databricks\", \"sparksql\"]\n            and context.parent_stack\n            and context.parent_stack[-1].type == \"property_name_identifier\"\n            and context.segment.raw == \"enableChangeDataFeed\"\n        ):\n            return None\n\n        if identifiers_policy_applicable(\n            self.unquoted_identifiers_policy,  # type: ignore\n            context.parent_stack,\n        ):\n            return super()._eval(context=context)\n        else:\n            return [LintResult(memory=context.memory)]\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3591735363006592}
{"question": "How does SQLFluff handle template processing in SQL files?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff handles template processing in SQL files through a sophisticated templating system that supports multiple templating engines and handles dynamic SQL content. The template processing works as follows: 1) Template Detection - SQLFluff automatically detects templating engines based on file content, supporting Jinja, Python format strings, and dbt templates; 2) Template Compilation - Raw SQL with template syntax is processed through the appropriate templating engine to resolve dynamic content and placeholders; 3) TemplatedFile Objects - The templating system creates TemplatedFile objects that maintain both the original template and the rendered SQL, along with mapping information; 4) Block Tracking - The BlockTracker maintains information about template blocks, allowing the system to map between template positions and rendered SQL positions; 5) Template Elements - TemplateElement objects represent different parts of the template, including static SQL, template blocks, and dynamic content; 6) Position Mapping - The system maintains accurate position mapping between template source and rendered output for error reporting and fix application; 7) Variable Resolution - Template variables and expressions are resolved according to the templating engine's rules and any provided context; 8) Conditional Logic - Template conditionals and loops are processed to generate the appropriate SQL variants; 9) Error Handling - Template processing errors are caught and reported with context about the template syntax and rendering issues; 10) Integration with Parsing - The templated SQL is then passed to the lexer and parser, with the system maintaining awareness of the original template structure.", "score": null, "retrieved_content": [{"name": "render_string", "is_method": true, "class_name": "Linter", "parameters": ["self", "in_str", "fname", "config", "encoding"], "calls": ["linter_logger.info", "time.monotonic", "self._normalise_newlines", "config.verify_dialect_specified", "config.get", "linter_logger.info", "RenderedFile", "linter_logger.warning", "self.templater.process_with_variants", "linter_logger.info", "len", "config.get", "templater_violations.append", "linter_logger.warning", "time.monotonic", "templated_variants.append", "len", "str", "config.get"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 822, "end_line": 893}, "code_snippet": "    def render_string(\n        self, in_str: str, fname: str, config: FluffConfig, encoding: str\n    ) -> RenderedFile:\n        \"\"\"Template the file.\"\"\"\n        linter_logger.info(\"Rendering String [%s] (%s)\", self.templater.name, fname)\n\n        # Start the templating timer\n        t0 = time.monotonic()\n\n        # Newlines are normalised to unix-style line endings (\\n).\n        # The motivation is that Jinja normalises newlines during templating and\n        # we want consistent mapping between the raw and templated slices.\n        in_str = self._normalise_newlines(in_str)\n\n        # Since Linter.__init__() does not require a dialect to be specified,\n        # check for one now. (We're processing a string, not a file, so we're\n        # not going to pick up a .sqlfluff or other config file to provide a\n        # missing dialect at this point.)\n        config.verify_dialect_specified()\n        if not config.get(\"templater_obj\") == self.templater:\n            linter_logger.warning(\n                f\"Attempt to set templater to {config.get('templater_obj').name} \"\n                f\"failed. Using {self.templater.name} templater. Templater cannot \"\n                \"be set in a .sqlfluff file in a subdirectory of the current \"\n                \"working directory. It can be set in a .sqlfluff in the current \"\n                \"working directory. See Nesting section of the docs for more \"\n                \"details.\"\n            )\n\n        variant_limit = config.get(\"render_variant_limit\")\n        templated_variants: list[TemplatedFile] = []\n        templater_violations: list[SQLTemplaterError] = []\n\n        try:\n            for variant, templater_errs in self.templater.process_with_variants(\n                in_str=in_str, fname=fname, config=config, formatter=self.formatter\n            ):\n                if variant:\n                    templated_variants.append(variant)\n                # NOTE: We could very easily end up with duplicate errors between\n                # different variants and this code doesn't currently do any\n                # deduplication between them. That will be resolved in further\n                # testing.\n                # TODO: Resolve potential duplicate templater violations between\n                # variants before we enable jinja variant linting by default.\n                templater_violations += templater_errs\n                if len(templated_variants) >= variant_limit:\n                    # Stop if we hit the limit.\n                    break\n        except SQLTemplaterError as templater_err:\n            # Fatal templating error. Capture it and don't generate a variant.\n            templater_violations.append(templater_err)\n        except SQLFluffSkipFile as skip_file_err:  # pragma: no cover\n            linter_logger.warning(str(skip_file_err))\n\n        if not templated_variants:\n            linter_logger.info(\"TEMPLATING FAILED: %s\", templater_violations)\n\n        linter_logger.info(\"Rendered %s variants\", len(templated_variants))\n\n        # Record time\n        time_dict = {\"templating\": time.monotonic() - t0}\n\n        return RenderedFile(\n            templated_variants,\n            templater_violations,\n            config,\n            time_dict,\n            fname,\n            encoding,\n            in_str,\n        )\n", "type": "function"}, {"name": "TemplatedFile", "docstring": "A templated SQL file.\n\nThis is the response of a templaters .process() method\nand contains both references to the original file and also\nthe capability to split up that file when lexing.", "methods": ["__init__", "from_string", "__repr__", "__str__", "get_line_pos_of_char_pos", "_find_slice_indices_of_templated_pos", "raw_slices_spanning_source_slice", "templated_slice_to_source_slice", "is_source_slice_literal", "source_only_slices", "source_position_dict_from_slice"], "attributes": [], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 132, "end_line": 503}, "type": "class"}, {"name": "_run_templater_and_verify_result", "is_method": false, "class_name": null, "parameters": ["dbt_templater", "project_dir", "fname", "dbt_fluff_config", "dbt_project_folder"], "calls": ["FluffConfig", "dbt_templater.process", "_get_fixture_path", "Lexer", "lexer.lex", "str", "fixture_path.read_text", "Path", "path.read_text", "str"], "code_location": {"file": "templater_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/test", "start_line": 131, "end_line": 152}, "code_snippet": "def _run_templater_and_verify_result(\n    dbt_templater,\n    project_dir,\n    fname,\n    dbt_fluff_config,\n    dbt_project_folder,\n):\n    path = Path(project_dir) / \"models/my_new_project\" / fname\n    config = FluffConfig(configs=dbt_fluff_config)\n    templated_file, _ = dbt_templater.process(\n        in_str=path.read_text(),\n        fname=str(path),\n        config=config,\n    )\n    template_output_folder_path = dbt_project_folder / \"templated_output/\"\n    fixture_path = _get_fixture_path(template_output_folder_path, fname)\n    assert str(templated_file) == fixture_path.read_text()\n    # Check we can lex the output too.\n    # https://github.com/sqlfluff/sqlfluff/issues/4013\n    lexer = Lexer(config=config)\n    _, lexing_violations = lexer.lex(templated_file)\n    assert not lexing_violations\n", "type": "function"}, {"name": "test__templater_jinja_large_file_check", "is_method": false, "class_name": null, "parameters": [], "calls": ["process", "process", "pytest.raises", "process", "str", "JinjaTemplater", "FluffConfig", "JinjaTemplater", "FluffConfig", "JinjaTemplater", "FluffConfig"], "code_location": {"file": "jinja_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/templaters", "start_line": 1685, "end_line": 1716}, "code_snippet": "def test__templater_jinja_large_file_check():\n    \"\"\"Test large file skipping.\n\n    The check is separately called on each .process() method\n    so it makes sense to test a few templaters.\n    \"\"\"\n    # First check we can process the file normally without specific config.\n    # i.e. check the defaults work and the default is high.\n    JinjaTemplater().process(\n        in_str=\"SELECT 1\",\n        fname=\"<string>\",\n        config=FluffConfig(overrides={\"dialect\": \"ansi\"}),\n    )\n    # Second check setting the value low disables the check\n    JinjaTemplater().process(\n        in_str=\"SELECT 1\",\n        fname=\"<string>\",\n        config=FluffConfig(\n            overrides={\"dialect\": \"ansi\", \"large_file_skip_char_limit\": 0}\n        ),\n    )\n    # Finally check we raise a skip exception when config is set low.\n    with pytest.raises(SQLFluffSkipFile) as excinfo:\n        JinjaTemplater().process(\n            in_str=\"SELECT 1\",\n            fname=\"<string>\",\n            config=FluffConfig(\n                overrides={\"dialect\": \"ansi\", \"large_file_skip_char_limit\": 2},\n            ),\n        )\n\n    assert \"Length of file\" in str(excinfo.value)\n", "type": "function"}, {"name": "process", "is_method": true, "class_name": "PythonTemplater", "parameters": ["self"], "calls": ["self.get_context", "self.slice_file", "TemplatedFile", "re.sub", "templater_logger.debug", "raw_str_with_dot_notation_hack.format", "SQLTemplaterError", "SQLTemplaterError", "SQLTemplaterError", "format", "format"], "code_location": {"file": "python.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 211, "end_line": 302}, "code_snippet": "    def process(\n        self,\n        *,\n        in_str: str,\n        fname: str,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n    ) -> tuple[TemplatedFile, list[SQLTemplaterError]]:\n        \"\"\"Process a string and return a TemplatedFile.\n\n        Note that the arguments are enforced as keywords\n        because Templaters can have differences in their\n        `process` method signature.\n        A Templater that only supports reading from a file\n        would need the following signature:\n            process(*, fname, in_str=None, config=None)\n        (arguments are swapped)\n\n        Args:\n            in_str (:obj:`str`): The input string.\n            fname (:obj:`str`, optional): The filename of this string. This is\n                mostly for loading config files at runtime.\n            config (:obj:`FluffConfig`): A specific config to use for this\n                templating operation. Only necessary for some templaters.\n            formatter (:obj:`CallbackFormatter`): Optional object for output.\n\n        \"\"\"\n        live_context = self.get_context(fname, config)\n\n        def render_func(raw_str: str) -> str:\n            \"\"\"Render the string using the captured live_context.\n\n            In order to support mocking of template variables\n            containing \".\" characters, this function converts any\n            template variable containing \".\" into a dictionary lookup.\n                Example:  {foo.bar} => {sqlfluff[foo.bar]}\n            \"\"\"\n            try:\n                # Hack to allow template variables with dot notation (e.g. foo.bar)\n                raw_str_with_dot_notation_hack = re.sub(\n                    r\"{([^:}]*\\.[^:}]*)(:\\S*)?}\", r\"{sqlfluff[\\1]\\2}\", raw_str\n                )\n                templater_logger.debug(\n                    \"    Raw String with Dot Notation Hack: %r\",\n                    raw_str_with_dot_notation_hack,\n                )\n                rendered_str = raw_str_with_dot_notation_hack.format(**live_context)\n            except KeyError as err:\n                missing_key = err.args[0]\n                if missing_key == \"sqlfluff\":\n                    # Give more useful error message related to dot notation hack\n                    # when user has not created the required, magic context key\n                    raise SQLTemplaterError(\n                        \"Failure in Python templating: magic key 'sqlfluff' \"\n                        \"missing from context.  This key is required \"\n                        \"for template variables containing '.'. \"\n                        \"https://docs.sqlfluff.com/en/stable/\"\n                        \"perma/python_templating.html\"\n                    )\n                elif \".\" in missing_key:\n                    # Give more useful error message related to dot notation hack\n                    # for missing keys\n                    raise SQLTemplaterError(\n                        \"Failure in Python templating: {} key missing from 'sqlfluff' \"\n                        \"dict in context. Template variables containing '.' are \"\n                        \"required to use the 'sqlfluff' magic fixed context key. \"\n                        \"https://docs.sqlfluff.com/en/stable/\"\n                        \"perma/python_templating.html\".format(err)\n                    )\n                else:\n                    raise SQLTemplaterError(\n                        \"Failure in Python templating: {}. Have you configured your \"\n                        \"variables? https://docs.sqlfluff.com/en/stable/\"\n                        \"perma/variables.html\".format(err)\n                    )\n            return rendered_str\n\n        raw_sliced, sliced_file, new_str = self.slice_file(\n            in_str,\n            render_func=render_func,\n            config=config,\n        )\n        return (\n            TemplatedFile(\n                source_str=in_str,\n                templated_str=new_str,\n                fname=fname,\n                sliced_file=sliced_file,\n                raw_sliced=raw_sliced,\n            ),\n            [],\n        )\n", "type": "function"}, {"name": "_unsafe_process", "is_method": true, "class_name": "DbtTemplater", "parameters": ["self", "fname", "in_str", "config", "dbt_dir"], "calls": ["os.getcwd", "os.path.relpath", "self._find_node", "templater_logger.debug", "dict", "save_ephemeral_nodes.items", "kwargs.get", "old_from_string", "set_task_contextvars", "type", "self.connection", "hasattr", "getattr", "templater_logger.debug", "templater_logger.debug", "templater_logger.debug", "templater_logger.debug", "setattr", "self.slice_file", "getattr", "TemplatedFile", "globals.get", "self.dbt_compiler.compile_node", "getattr", "SQLTemplaterError", "endswith", "set_task_contextvars", "self.dbt_manifest.nodes.items", "SQLTemplaterError", "SQLFluffSkipFile", "len", "len", "model.get", "str", "SQLTemplaterError", "source_dbt_sql.rstrip", "source_dbt_sql.rstrip", "env.add_extension", "env.from_string", "template.render", "cv_project_root.set", "getattr", "str", "len", "template.render"], "code_location": {"file": "templater.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt", "start_line": 648, "end_line": 884}, "code_snippet": "    def _unsafe_process(self, fname, in_str=None, config=None, dbt_dir=os.getcwd()):\n        original_file_path = os.path.relpath(fname, start=dbt_dir)\n\n        # Below, we monkeypatch Environment.from_string() to intercept when dbt\n        # compiles (i.e. runs Jinja) to expand the \"node\" corresponding to fname.\n        # We do this to capture the Jinja context at the time of compilation, i.e.:\n        # - Jinja Environment object\n        # - Jinja \"globals\" dictionary\n        #\n        # This info is captured by the \"make_template()\" function, which in\n        # turn is used by our parent class' (JinjaTemplater) slice_file()\n        # function.\n        old_from_string = Environment.from_string\n        # Start with render_func undefined. We need to know whether it has been\n        # overwritten.\n        render_func: Optional[Callable[[str], str]] = None\n\n        if self.dbt_version_tuple >= (1, 3):\n            compiled_sql_attribute = \"compiled_code\"\n            raw_sql_attribute = \"raw_code\"\n        else:  # pragma: no cover\n            compiled_sql_attribute = \"compiled_sql\"\n            raw_sql_attribute = \"raw_sql\"\n\n        def from_string(*args, **kwargs):\n            \"\"\"Replaces (via monkeypatch) the jinja2.Environment function.\"\"\"\n            nonlocal render_func\n            # Is it processing the node corresponding to fname?\n            globals = kwargs.get(\"globals\")\n            if globals:\n                model = globals.get(\"model\")\n                if model:\n                    if model.get(\"original_file_path\") == original_file_path:\n                        # Yes. Capture the important arguments and create\n                        # a render_func() closure with overwrites the variable\n                        # from within _unsafe_process when from_string is run.\n                        env = args[0]\n                        globals = args[2] if len(args) >= 3 else kwargs[\"globals\"]\n\n                        # Overwrite the outer render_func\n                        def render_func(in_str):\n                            env.add_extension(SnapshotExtension)\n                            template = env.from_string(in_str, globals=globals)\n                            if self.dbt_version_tuple >= (1, 8):\n                                # dbt 1.8 requires a context for rendering the template.\n                                return template.render(globals)\n                            return template.render()\n\n            return old_from_string(*args, **kwargs)\n\n        # NOTE: Inject the project root for dbt contextvars compatibility.\n        # Prefer dbt_common (latest), then fallback to dbt.events or dbt.task.\n        try:\n            from dbt_common.events.contextvars import set_task_contextvars\n\n            set_task_contextvars(project_root=self.project_dir)\n        except ImportError:\n            try:\n                from dbt.events.contextvars import set_task_contextvars\n\n                set_task_contextvars(project_root=self.project_dir)\n            except ImportError:\n                # NOTE: We need to inject the project root here in reaction to the\n                # breaking change upstream with dbt. Coverage works in 1.5.2, but\n                # appears to no longer be covered in 1.5.3.\n                # This change was backported and so exists in some versions\n                # but not others. When not present, no additional action is needed.\n                # https://github.com/dbt-labs/dbt-core/pull/7949\n                # On the 1.5.x branch this was between 1.5.1 and 1.5.2\n                try:\n                    from dbt.task.contextvars import cv_project_root\n\n                    cv_project_root.set(self.project_dir)  # pragma: no cover\n                except ImportError:\n                    cv_project_root = None\n                    pass\n\n        # NOTE: _find_node will raise a compilation exception if the project\n        # fails to compile, and we catch that in the outer `.process()` method.\n        node = self._find_node(fname, config, dbt_dir)\n\n        templater_logger.debug(\n            \"_find_node for path %r returned object of type %s.\", fname, type(node)\n        )\n\n        save_ephemeral_nodes = dict(\n            (k, v)\n            for k, v in self.dbt_manifest.nodes.items()\n            if v.config.materialized == \"ephemeral\"\n            and not getattr(v, \"compiled\", False)\n        )\n\n        if self.dbt_version_tuple >= (1, 8):\n            from dbt_common.exceptions import UndefinedMacroError\n        else:\n            from dbt.exceptions import UndefinedMacroError\n\n        with self.connection():\n            # Apply the monkeypatch.\n            Environment.from_string = from_string\n            try:\n                node = self.dbt_compiler.compile_node(\n                    node=node,\n                    manifest=self.dbt_manifest,\n                    write=False,\n                )\n            except UndefinedMacroError as err:\n                # The explanation on the undefined macro error is already fairly\n                # explanatory, so just pass it straight through.\n                raise SQLTemplaterError(str(err))\n            except Exception as err:\n                # This happens if there's a fatal error at compile time. That\n                # can sometimes happen for SQLFluff related reasons (it used\n                # to happen if we tried to compile ephemeral models in the\n                # wrong order), but more often because a macro tries to query\n                # a table at compile time which doesn't exist.\n                if self.dbt_skip_compilation_error is False:\n                    raise SQLTemplaterError(str(err))\n                raise SQLFluffSkipFile(\n                    f\"Skipped file {fname} because dbt raised a fatal \"\n                    f\"exception during compilation: {err!s}\"\n                )\n                # NOTE: We don't do a `raise ... from err` here because the\n                # full trace is not useful for most users. In debugging\n                # issues here it may be valuable to add the `from err` part\n                # after the above `raise` statement.\n            finally:\n                # Undo the monkeypatch.\n                Environment.from_string = old_from_string\n\n            if hasattr(node, \"injected_sql\"):\n                # If injected SQL is present, it contains a better picture\n                # of what will actually hit the database (e.g. with tests).\n                # However it's not always present.\n                compiled_sql = node.injected_sql  # pragma: no cover\n            else:\n                compiled_sql = getattr(node, compiled_sql_attribute)\n\n            raw_sql = getattr(node, raw_sql_attribute)\n\n            if not compiled_sql:  # pragma: no cover\n                raise SQLTemplaterError(\n                    \"dbt templater compilation failed silently, check your \"\n                    \"configuration by running `dbt compile` directly.\"\n                )\n            source_dbt_sql = in_str\n            if not source_dbt_sql.rstrip().endswith(\"-%}\"):\n                n_trailing_newlines = len(source_dbt_sql) - len(\n                    source_dbt_sql.rstrip(\"\\n\")\n                )\n            else:\n                # Source file ends with right whitespace stripping, so there's\n                # no need to preserve/restore trailing newlines, as they would\n                # have been removed regardless of dbt's\n                # keep_trailing_newlines=False behavior.\n                n_trailing_newlines = 0\n\n            templater_logger.debug(\n                \"    Trailing newline count in source dbt model: %r\",\n                n_trailing_newlines,\n            )\n            templater_logger.debug(\"    Raw SQL before compile: %r\", source_dbt_sql)\n            templater_logger.debug(\"    Node raw SQL: %r\", raw_sql)\n            templater_logger.debug(\"    Node compiled SQL: %r\", compiled_sql)\n\n            # When using dbt-templater, trailing newlines are ALWAYS REMOVED during\n            # compiling. Unless fixed (like below), this will cause:\n            #    1. Assertion errors in TemplatedFile, when it sanity checks the\n            #       contents of the sliced_file array.\n            #    2. LT12 linting errors when running \"sqlfluff lint foo_bar.sql\"\n            #       since the linter will use the compiled code with the newlines\n            #       removed.\n            #    3. \"No newline at end of file\" warnings in Git/GitHub since\n            #       sqlfluff uses the compiled SQL to write fixes back to the\n            #       source SQL in the dbt model.\n            #\n            # The solution is (note that both the raw and compiled files have\n            # had trailing newline(s) removed by the dbt-templater.\n            #    1. Check for trailing newlines before compiling by looking at the\n            #       raw SQL in the source dbt file. Remember the count of trailing\n            #       newlines.\n            #    2. Set node.raw_sql/node.raw_code to the original source file contents.\n            #    3. Append the count from #1 above to compiled_sql. (In\n            #       production, slice_file() does not usually use this string,\n            #       but some test scenarios do.\n            setattr(node, raw_sql_attribute, source_dbt_sql)\n\n            # So for files that have no templated elements in them, render_func\n            # will still be null at this point. If so, we replace it with a lambda\n            # which just directly returns the input , but _also_ reset the trailing\n            # newlines counter because they also won't have been stripped.\n            if render_func is None:\n                # NOTE: In this case, we shouldn't re-add newlines, because they\n                # were never taken away.\n                n_trailing_newlines = 0\n\n                # Overwrite the render_func placeholder.\n                def render_func(in_str):\n                    \"\"\"A render function which just returns the input.\"\"\"\n                    return in_str\n\n            # At this point assert that we _have_ a render_func\n            assert render_func is not None\n\n            # TRICKY: dbt configures Jinja2 with keep_trailing_newline=False.\n            # As documented (https://jinja.palletsprojects.com/en/3.0.x/api/),\n            # this flag's behavior is: \"Preserve the trailing newline when\n            # rendering templates. The default is False, which causes a single\n            # newline, if present, to be stripped from the end of the template.\"\n            #\n            # Below, we use \"append_to_templated\" to effectively \"undo\" this.\n            raw_sliced, sliced_file, templated_sql = self.slice_file(\n                source_dbt_sql,\n                render_func=render_func,\n                config=config,\n                append_to_templated=\"\\n\" if n_trailing_newlines else \"\",\n            )\n        # :HACK: If calling compile_node() compiled any ephemeral nodes,\n        # restore them to their earlier state. This prevents a runtime error\n        # in the dbt \"_inject_ctes_into_sql()\" function that occurs with\n        # 2nd-level ephemeral model dependencies (e.g. A -> B -> C, where\n        # both B and C are ephemeral). Perhaps there is a better way to do\n        # this, but this seems good enough for now.\n        for k, v in save_ephemeral_nodes.items():\n            if getattr(self.dbt_manifest.nodes[k], \"compiled\", False):\n                self.dbt_manifest.nodes[k] = v\n        return (\n            TemplatedFile(\n                source_str=source_dbt_sql,\n                templated_str=templated_sql,\n                fname=fname,\n                sliced_file=sliced_file,\n                raw_sliced=raw_sliced,\n            ),\n            # No violations returned in this way.\n            [],\n        )\n", "type": "function"}, {"name": "test__context_in_config_is_loaded", "is_method": false, "class_name": null, "parameters": ["project_dir", "dbt_templater", "model_path", "var_value", "dbt_fluff_config"], "calls": ["pytest.mark.parametrize", "deepcopy", "FluffConfig", "dbt_templater.process", "Path", "str", "path.read_text", "str"], "code_location": {"file": "templater_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/test", "start_line": 646, "end_line": 667}, "code_snippet": "def test__context_in_config_is_loaded(\n    project_dir,\n    dbt_templater,\n    model_path,\n    var_value,\n    dbt_fluff_config,\n):\n    \"\"\"Test that variables inside .sqlfluff are passed to dbt.\"\"\"\n    context = {\"passed_through_cli\": var_value} if var_value else {}\n\n    config_dict = deepcopy(dbt_fluff_config)\n    config_dict[\"templater\"][\"dbt\"][\"context\"] = context\n    config = FluffConfig(config_dict)\n\n    path = Path(project_dir) / model_path\n\n    processed, violations = dbt_templater.process(\n        in_str=path.read_text(), fname=str(path), config=config\n    )\n\n    assert violations == []\n    assert str(var_value) in processed.templated_str\n", "type": "function"}, {"name": "test__templater_dbt_templating_result", "is_method": false, "class_name": null, "parameters": ["project_dir", "dbt_templater", "fname", "dbt_fluff_config", "dbt_project_folder"], "calls": ["pytest.mark.parametrize", "_run_templater_and_verify_result"], "code_location": {"file": "templater_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/test", "start_line": 92, "end_line": 106}, "code_snippet": "def test__templater_dbt_templating_result(\n    project_dir,\n    dbt_templater,\n    fname,\n    dbt_fluff_config,\n    dbt_project_folder,\n):\n    \"\"\"Test that input sql file gets templated into output sql file.\"\"\"\n    _run_templater_and_verify_result(\n        dbt_templater,\n        project_dir,\n        fname,\n        dbt_fluff_config,\n        dbt_project_folder,\n    )\n", "type": "function"}, {"name": "test__templater_python_large_file_check", "is_method": false, "class_name": null, "parameters": [], "calls": ["process", "pytest.raises", "process", "str", "PythonTemplater", "PythonTemplater", "FluffConfig"], "code_location": {"file": "python_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/templaters", "start_line": 480, "end_line": 498}, "code_snippet": "def test__templater_python_large_file_check():\n    \"\"\"Test large file skipping.\n\n    The check is separately called on each .process() method\n    so it makes sense to test a few templaters.\n    \"\"\"\n    # First check we can process the file normally without config.\n    PythonTemplater().process(in_str=\"SELECT 1\", fname=\"<string>\")\n    # Then check we raise a skip exception when config is set low.\n    with pytest.raises(SQLFluffSkipFile) as excinfo:\n        PythonTemplater().process(\n            in_str=\"SELECT 1\",\n            fname=\"<string>\",\n            config=FluffConfig(\n                overrides={\"dialect\": \"ansi\", \"large_file_skip_char_limit\": 2},\n            ),\n        )\n\n    assert \"Length of file\" in str(excinfo.value)\n", "type": "function"}, {"name": "process", "is_method": true, "class_name": "PlaceholderTemplater", "parameters": ["self"], "calls": ["self.get_context", "regex.finditer", "found_param.span", "template_slices.append", "raw_slices.append", "template_slices.append", "raw_slices.append", "len", "template_slices.append", "raw_slices.append", "TemplatedFile", "found_param.groupdict", "str", "str", "found_param.groupdict", "TemplatedFileSlice", "RawFileSlice", "TemplatedFileSlice", "RawFileSlice", "len", "TemplatedFileSlice", "RawFileSlice", "slice", "offset_slice", "slice", "offset_slice", "slice", "offset_slice", "len", "len", "len"], "code_location": {"file": "placeholder.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/templaters", "start_line": 119, "end_line": 243}, "code_snippet": "    def process(\n        self,\n        *,\n        in_str: str,\n        fname: str,\n        config: Optional[FluffConfig] = None,\n        formatter: Optional[FormatterInterface] = None,\n    ) -> tuple[TemplatedFile, list[SQLTemplaterError]]:\n        \"\"\"Process a string and return a TemplatedFile.\n\n        Note that the arguments are enforced as keywords\n        because Templaters can have differences in their\n        `process` method signature.\n        A Templater that only supports reading from a file\n        would need the following signature:\n            process(*, fname, in_str=None, config=None)\n        (arguments are swapped)\n\n        Args:\n            in_str (:obj:`str`): The input string.\n            fname (:obj:`str`, optional): The filename of this string. This is\n                mostly for loading config files at runtime.\n            config (:obj:`FluffConfig`): A specific config to use for this\n                templating operation. Only necessary for some templaters.\n            formatter (:obj:`CallbackFormatter`): Optional object for output.\n\n        \"\"\"\n        context = self.get_context(fname, config)\n        template_slices = []\n        raw_slices = []\n        last_pos_raw, last_pos_templated = 0, 0\n        out_str = \"\"\n\n        regex = context[\"__bind_param_regex\"]\n        # when the param has no name, use a 1-based index\n        param_counter = 1\n        for found_param in regex.finditer(in_str):\n            span = found_param.span()\n            if \"param_name\" not in found_param.groupdict():\n                param_name = str(param_counter)\n                param_counter += 1\n            else:\n                param_name = found_param[\"param_name\"]\n            last_literal_length = span[0] - last_pos_raw\n            if param_name in context:\n                replacement = str(context[param_name])\n            else:\n                replacement = param_name\n            if \"quotation\" in found_param.groupdict():\n                quotation = found_param[\"quotation\"]\n                replacement = quotation + replacement + quotation\n            # add the literal to the slices\n            template_slices.append(\n                TemplatedFileSlice(\n                    slice_type=\"literal\",\n                    source_slice=slice(last_pos_raw, span[0], None),\n                    templated_slice=offset_slice(\n                        last_pos_templated,\n                        last_literal_length,\n                    ),\n                )\n            )\n            raw_slices.append(\n                RawFileSlice(\n                    raw=in_str[last_pos_raw : span[0]],\n                    slice_type=\"literal\",\n                    source_idx=last_pos_raw,\n                )\n            )\n            out_str += in_str[last_pos_raw : span[0]]\n            # add the current replaced element\n            start_template_pos = last_pos_templated + last_literal_length\n            template_slices.append(\n                TemplatedFileSlice(\n                    slice_type=\"templated\",\n                    source_slice=slice(span[0], span[1]),\n                    templated_slice=offset_slice(start_template_pos, len(replacement)),\n                )\n            )\n            raw_slices.append(\n                RawFileSlice(\n                    raw=in_str[span[0] : span[1]],\n                    slice_type=\"templated\",\n                    source_idx=span[0],\n                )\n            )\n            out_str += replacement\n            # update the indexes\n            last_pos_raw = span[1]\n            last_pos_templated = start_template_pos + len(replacement)\n        # add the last literal, if any\n        if len(in_str) > last_pos_raw:\n            template_slices.append(\n                TemplatedFileSlice(\n                    slice_type=\"literal\",\n                    source_slice=slice(last_pos_raw, len(in_str)),\n                    templated_slice=offset_slice(\n                        last_pos_templated,\n                        (len(in_str) - last_pos_raw),\n                    ),\n                )\n            )\n            raw_slices.append(\n                RawFileSlice(\n                    raw=in_str[last_pos_raw:],\n                    slice_type=\"literal\",\n                    source_idx=last_pos_raw,\n                )\n            )\n            out_str += in_str[last_pos_raw:]\n        return (\n            TemplatedFile(\n                # original string\n                source_str=in_str,\n                # string after all replacements\n                templated_str=out_str,\n                # filename\n                fname=fname,\n                # list of TemplatedFileSlice\n                sliced_file=template_slices,\n                # list of RawFileSlice, same size\n                raw_sliced=raw_slices,\n            ),\n            [],  # violations, always empty\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.35123586654663086}
{"question": "How can SQLFluff's fix API be leveraged for custom code transformations?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's fix API can be leveraged for custom code transformations through its comprehensive fix generation and application system that provides precise control over SQL code modifications. The fix API for custom transformations works as follows: 1) LintFix Creation - Custom transformations can create LintFix objects that describe specific changes including create, edit, or delete operations on segments; 2) Segment Manipulation - The fix API provides methods to manipulate segments in the parse tree, allowing for complex transformations like restructuring SQL statements; 3) Position-Aware Changes - Fixes maintain accurate position information, enabling precise transformations that preserve code structure and formatting; 4) Batch Operations - Multiple fixes can be generated and applied together, allowing for complex multi-step transformations; 5) Context Preservation - The fix system maintains context information during transformations, ensuring that changes are applied correctly within the broader SQL structure; 6) Validation Integration - Custom transformations can leverage SQLFluff's validation system to ensure that generated code is syntactically correct; 7) Template Awareness - The fix API is template-aware, allowing transformations to work correctly with templated SQL while preserving template structure; 8) Error Handling - Custom transformations can implement error handling to gracefully handle edge cases and provide meaningful error messages; 9) Performance Optimization - The fix API supports efficient transformations through caching and optimized segment manipulation; 10) Integration with Rules - Custom transformations can be integrated with linting rules to provide automatic code improvements based on detected issues.", "score": null, "retrieved_content": [{"name": "fix", "is_method": false, "class_name": null, "parameters": ["sql", "dialect", "rules", "exclude_rules", "config", "config_path", "fix_even_unparsable"], "calls": ["Linter", "linter.lint_string_wrapped", "get_simple_config", "cfg.get", "result.count_tmp_prs_errors", "fix_string"], "code_location": {"file": "simple.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/api", "start_line": 115, "end_line": 165}, "code_snippet": "def fix(\n    sql: str,\n    dialect: Optional[str] = None,\n    rules: Optional[list[str]] = None,\n    exclude_rules: Optional[list[str]] = None,\n    config: Optional[FluffConfig] = None,\n    config_path: Optional[str] = None,\n    fix_even_unparsable: Optional[bool] = None,\n) -> str:\n    \"\"\"Fix a SQL string.\n\n    Args:\n        sql (:obj:`str`): The SQL to be fixed.\n        dialect (:obj:`Optional[str]`, optional): A reference to the dialect of the SQL\n            to be fixed. Defaults to `ansi`.\n        rules (:obj:`Optional[list[str]`, optional): A subset of rule\n            references to fix for. Defaults to None.\n        exclude_rules (:obj:`Optional[list[str]`, optional): A subset of rule\n            references to avoid fixing for. Defaults to None.\n        config (:obj:`Optional[FluffConfig]`, optional): A configuration object\n            to use for the operation. Defaults to None.\n        config_path (:obj:`Optional[str]`, optional): A path to a .sqlfluff config,\n            which is only used if a `config` is not already provided.\n            Defaults to None.\n        fix_even_unparsable (:obj:`bool`, optional): Optional override for the\n            corresponding SQLFluff configuration value.\n\n    Returns:\n        :obj:`str` for the fixed SQL if possible.\n    \"\"\"\n    cfg = config or get_simple_config(\n        dialect=dialect,\n        rules=rules,\n        exclude_rules=exclude_rules,\n        config_path=config_path,\n    )\n    linter = Linter(config=cfg)\n\n    result = linter.lint_string_wrapped(sql, fix=True)\n    if fix_even_unparsable is None:\n        fix_even_unparsable = cfg.get(\"fix_even_unparsable\")\n    should_fix = True\n    if not fix_even_unparsable:\n        # If fix_even_unparsable wasn't set, check for templating or parse\n        # errors and suppress fixing if there were any.\n        _, num_filtered_errors = result.count_tmp_prs_errors()\n        if num_filtered_errors > 0:\n            should_fix = False\n    if should_fix:\n        sql = result.paths[0].files[0].fix_string()[0]\n    return sql\n", "type": "function"}, {"name": "fix", "is_method": true, "class_name": "Linter", "parameters": ["self", "tree", "config", "fname", "templated_file"], "calls": ["self.get_rulepack", "self.lint_fix_parsed"], "code_location": {"file": "linter.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 931, "end_line": 950}, "code_snippet": "    def fix(\n        self,\n        tree: BaseSegment,\n        config: Optional[FluffConfig] = None,\n        fname: Optional[str] = None,\n        templated_file: Optional[\"TemplatedFile\"] = None,\n    ) -> tuple[BaseSegment, list[SQLBaseError]]:\n        \"\"\"Return the fixed tree and violations from lintfix when we're fixing.\"\"\"\n        config = config or self.config\n        rule_pack = self.get_rulepack(config=config)\n        fixed_tree, violations, _, _ = self.lint_fix_parsed(\n            tree,\n            config,\n            rule_pack,\n            fix=True,\n            fname=fname,\n            templated_file=templated_file,\n            formatter=self.formatter,\n        )\n        return fixed_tree, violations\n", "type": "function"}, {"name": "test__cli__fix_multiple_errors_no_show_errors", "is_method": false, "class_name": null, "parameters": [], "calls": ["invoke_assert_code"], "code_location": {"file": "commands_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/cli", "start_line": 2292, "end_line": 2305}, "code_snippet": "def test__cli__fix_multiple_errors_no_show_errors():\n    \"\"\"Test the fix output.\"\"\"\n    invoke_assert_code(\n        ret_code=1,\n        args=[\n            fix,\n            [\n                \"--check\",  # Run in check mode to get the confirmation.\n                \"--disable-progress-bar\",\n                \"test/fixtures/linter/multiple_sql_errors.sql\",\n            ],\n        ],\n        assert_stdout_contains=multiple_expected_output,\n    )\n", "type": "function"}, {"name": "test__fix__generate_source_patches", "is_method": false, "class_name": null, "parameters": ["tree", "templated_file", "expected_result", "caplog"], "calls": ["pytest.mark.parametrize", "caplog.at_level", "generate_source_patches", "RawSegment", "RawSegment", "BaseSegment", "BaseSegment", "BaseSegment", "PositionMarker", "PositionMarker", "FixPatch", "FixPatch", "FixPatch", "FixPatch", "FixPatch", "FixPatch", "slice", "slice", "slice", "slice", "slice", "slice", "RawSegment", "RawSegment", "RawSegment", "slice", "slice", "TemplateSegment", "RawSegment", "RawSegment", "RawSegment", "slice", "slice", "TemplateSegment", "RawSegment", "RawSegment", "RawSegment", "slice", "slice", "slice", "slice", "slice", "slice", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "PositionMarker", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "SourceFix", "SourceFix", "slice", "slice", "slice", "slice"], "code_location": {"file": "fix_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 195, "end_line": 202}, "code_snippet": "def test__fix__generate_source_patches(tree, templated_file, expected_result, caplog):\n    \"\"\"Test generate_source_patches.\n\n    This is part of fix_string().\n    \"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.linter\"):\n        result = generate_source_patches(tree, templated_file)\n    assert result == expected_result\n", "type": "function"}, {"name": "test__dbt_templated_models_fix_does_not_corrupt_file", "is_method": false, "class_name": null, "parameters": ["project_dir", "path", "caplog", "dbt_fluff_config"], "calls": ["pytest.mark.parametrize", "os.path.join", "_clean_path", "Linter", "os.path.dirname", "caplog.at_level", "lntr.lint_path", "lnt.persist_changes", "_clean_path", "FluffConfig", "os.path.join", "open", "f.read", "open", "f.read", "os.path.join", "os.path.join", "path.replace"], "code_location": {"file": "templater_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-templater-dbt/test", "start_line": 399, "end_line": 419}, "code_snippet": "def test__dbt_templated_models_fix_does_not_corrupt_file(\n    project_dir,\n    path,\n    caplog,\n    dbt_fluff_config,\n):\n    \"\"\"Test issues where previously \"sqlfluff fix\" corrupted the file.\"\"\"\n    test_glob = os.path.join(project_dir, os.path.dirname(path), \"*FIXED.sql\")\n    _clean_path(test_glob)\n    lntr = Linter(config=FluffConfig(configs=dbt_fluff_config))\n    with caplog.at_level(logging.INFO, logger=\"sqlfluff.linter\"):\n        lnt = lntr.lint_path(os.path.join(project_dir, path), fix=True)\n    try:\n        lnt.persist_changes(fixed_file_suffix=\"FIXED\")\n        with open(os.path.join(project_dir, path + \".after\")) as f:\n            comp_buff = f.read()\n        with open(os.path.join(project_dir, path.replace(\".sql\", \"FIXED.sql\"))) as f:\n            fixed_buff = f.read()\n        assert fixed_buff == comp_buff\n    finally:\n        _clean_path(test_glob)\n", "type": "function"}, {"name": "test__linted_file__build_up_fixed_source_string", "is_method": false, "class_name": null, "parameters": ["source_slices", "source_patches", "raw_source_string", "expected_result", "caplog"], "calls": ["pytest.mark.parametrize", "caplog.at_level", "LintedFile._build_up_fixed_source_string", "slice", "slice", "slice", "slice", "FixPatch", "slice", "slice", "slice", "FixPatch", "slice", "slice", "slice", "FixPatch", "slice", "slice", "slice", "FixPatch", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice"], "code_location": {"file": "linted_file_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 51, "end_line": 62}, "code_snippet": "def test__linted_file__build_up_fixed_source_string(\n    source_slices, source_patches, raw_source_string, expected_result, caplog\n):\n    \"\"\"Test _build_up_fixed_source_string.\n\n    This is part of fix_string().\n    \"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.linter\"):\n        result = LintedFile._build_up_fixed_source_string(\n            source_slices, source_patches, raw_source_string\n        )\n    assert result == expected_result\n", "type": "function"}, {"name": "test__linted_file__slice_source_file_using_patches", "is_method": false, "class_name": null, "parameters": ["source_patches", "source_only_slices", "raw_source_string", "expected_result", "caplog"], "calls": ["pytest.mark.parametrize", "caplog.at_level", "LintedFile._slice_source_file_using_patches", "slice", "FixPatch", "slice", "slice", "slice", "slice", "RawFileSlice", "slice", "FixPatch", "RawFileSlice", "slice", "slice", "FixPatch", "RawFileSlice", "slice", "slice", "slice", "slice", "FixPatch", "RawFileSlice", "slice", "slice", "slice", "FixPatch", "FixPatch", "FixPatch", "RawFileSlice", "RawFileSlice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice", "slice"], "code_location": {"file": "linted_file_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/linter", "start_line": 199, "end_line": 210}, "code_snippet": "def test__linted_file__slice_source_file_using_patches(\n    source_patches, source_only_slices, raw_source_string, expected_result, caplog\n):\n    \"\"\"Test _slice_source_file_using_patches.\n\n    This is part of fix_string().\n    \"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.linter\"):\n        result = LintedFile._slice_source_file_using_patches(\n            source_patches, source_only_slices, raw_source_string\n        )\n    assert result == expected_result\n", "type": "function"}, {"name": "_get_fix", "is_method": true, "class_name": "Rule_CP03", "parameters": ["self", "segment", "fixed_raw"], "calls": ["_get_fix", "super"], "code_location": {"file": "CP03.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/capitalisation", "start_line": 53, "end_line": 54}, "code_snippet": "    def _get_fix(self, segment: BaseSegment, fixed_raw: str) -> LintFix:\n        return super()._get_fix(segment, fixed_raw)\n", "type": "function"}, {"name": "apply_fixes", "is_method": false, "class_name": null, "parameters": ["segment", "dialect", "rule_code", "fixes", "fix_even_unparsable"], "calls": ["segment.invalidate_caches", "segment.is_raw", "fixes.pop", "list", "apply_fixes", "range", "len", "range", "segment.__class__", "seg_buffer.append", "seg_fixes.reverse", "fixes_applied.append", "linter_logger.debug", "segment._position_segments", "len", "segment._is_code_or_meta", "len", "segment._is_code_or_meta", "hasattr", "hasattr", "len", "seg_buffer.append", "seg_buffer.append", "seg_buffer.append", "tuple", "segment._position_segments", "err.add_note", "new_seg.validate_segment_with_reparse", "len", "tuple", "getattr", "len"], "code_location": {"file": "fix.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/linter", "start_line": 107, "end_line": 335}, "code_snippet": "def apply_fixes(\n    segment: BaseSegment,\n    dialect: \"Dialect\",\n    rule_code: str,\n    fixes: dict[int, AnchorEditInfo],\n    fix_even_unparsable: bool = False,\n) -> tuple[\"BaseSegment\", list[\"BaseSegment\"], list[\"BaseSegment\"], bool]:\n    \"\"\"Apply a dictionary of fixes to this segment.\n\n    Used in to apply fixes found in linting. If a segment remains unchanged\n    then the original is returned, but if any changes are made to it, or any\n    of it's child segments, then it returns a copy rather than mutating the\n    original.\n\n    Most fixes are usually applied when this method is called on their parent\n    segment, this is because that's where we can insert or move segments relative\n    to the anchor specified in the fix. This has the implication that if the\n    method is called on a `RawSegment`, then no changes will be applied, because\n    a `RawSegment` never has child segments.\n\n    After fixing, it calls `validate_segment_with_reparse` on the segment to\n    check that the segment still parses after any changes are made. The result\n    of this is returned as a boolean in the last element of the return tuple.\n    As the function recurses, if an inner element doesn't parse after fixing,\n    then the outer segment will also be checked, and if found to parse successfully\n    then the method returns `True` as valid. This is because sometimes the fixes\n    change the structure enough that a wider reparse is necessary.\n\n    Because of this validity checking, any unparsable sections are assumed\n    unfixable (because we won't know if we're corrupting the SQL). The method\n    will therefore return early without applying any fixes if the segment it's\n    called on is unparsable (because we already know that validation check will\n    fail already).\n\n    If `fix_even_unparsable` is True, then we will still apply fixes to unparsable\n    sections, but will do so *without validation*. That means that the final\n    element of the return value will always return `True`, so that we don't interrupt\n    the validity checking of any outer (parsable) sections.\n    \"\"\"\n    if not fixes or segment.is_raw():\n        return segment, [], [], True\n\n    seg_buffer = []\n    before = []\n    after = []\n    fixes_applied: list[LintFix] = []\n    requires_validate = False\n\n    for seg in segment.segments:\n        # Look for uuid match.\n        # This handles potential positioning ambiguity.\n        anchor_info: Optional[AnchorEditInfo] = fixes.pop(seg.uuid, None)\n\n        if anchor_info is None:\n            # No fix matches here, just add the segment and move on.\n            seg_buffer.append(seg)\n            continue\n\n        # Otherwise there is a fix match.\n        seg_fixes = anchor_info.fixes\n        if (\n            len(seg_fixes) == 2 and seg_fixes[0].edit_type == \"create_after\"\n        ):  # pragma: no cover\n            # Must be create_before & create_after. Swap so the\n            # \"before\" comes first.\n            seg_fixes.reverse()\n\n        for f in anchor_info.fixes:\n            assert f.anchor.uuid == seg.uuid\n            fixes_applied.append(f)\n            linter_logger.debug(\n                \"Matched fix for %s against segment: %s -> %s\",\n                rule_code,\n                f,\n                seg,\n            )\n\n            # Deletes are easy.\n            if f.edit_type == \"delete\":\n                # We're just getting rid of this segment.\n                requires_validate = True\n                # NOTE: We don't add the segment in this case.\n                continue\n\n            # Otherwise it must be a replace or a create.\n            assert f.edit_type in (\n                \"replace\",\n                \"create_before\",\n                \"create_after\",\n            ), f\"Unexpected edit_type: {f.edit_type!r} in {f!r}\"\n\n            if f.edit_type == \"create_after\" and len(anchor_info.fixes) == 1:\n                # in the case of a creation after that is not part\n                # of a create_before/create_after pair, also add\n                # this segment before the edit.\n                seg_buffer.append(seg)\n\n            # We're doing a replacement (it could be a single\n            # segment or an iterable)\n            assert f.edit, f\"Edit {f.edit_type!r} requires `edit`.\"\n            consumed_pos = False\n            for s in f.edit:\n                seg_buffer.append(s)\n                # If one of them has the same raw representation\n                # then the first that matches gets to take the\n                # original position marker.\n                if f.edit_type == \"replace\" and s.raw == seg.raw and not consumed_pos:\n                    seg_buffer[-1].pos_marker = seg.pos_marker\n                    consumed_pos = True\n\n            # If we're just editing a segment AND keeping the type the\n            # same then no need to validate. Otherwise we should\n            # trigger a validation (e.g. for creations or\n            # multi-replace).\n            if not (\n                f.edit_type == \"replace\"\n                and len(f.edit) == 1\n                and f.edit[0].class_types == seg.class_types\n            ):\n                requires_validate = True\n\n            if f.edit_type == \"create_before\":\n                # in the case of a creation before, also add this\n                # segment on the end\n                seg_buffer.append(seg)\n\n    # Invalidate any caches\n    segment.invalidate_caches()\n\n    # If any fixes applied, do an intermediate reposition. When applying\n    # fixes to children and then trying to reposition them, that recursion\n    # may rely on the parent having already populated positions for any\n    # of the fixes applied there first. This ensures those segments have\n    # working positions to work with.\n    if fixes_applied:\n        assert segment.pos_marker\n        seg_buffer = list(\n            segment._position_segments(tuple(seg_buffer), parent_pos=segment.pos_marker)\n        )\n\n    # Then recurse (i.e. deal with the children) (Requeueing)\n    seg_queue = seg_buffer\n    seg_buffer = []\n    for seg in seg_queue:\n        s, pre, post, validated = apply_fixes(seg, dialect, rule_code, fixes)\n        # 'before' and 'after' will usually be empty. Only used when\n        # lower-level fixes left 'seg' with non-code (usually\n        # whitespace) segments as the first or last children. This is\n        # generally not allowed (see the can_start_end_non_code field),\n        # and these segments need to be \"bubbled up\" the tree.\n        seg_buffer += pre + [s] + post\n        # If we fail to validate a child segment, make sure to validate this\n        # segment.\n        if not validated:\n            requires_validate = True\n\n    # Most correct whitespace positioning will have already been handled\n    # _however_, the exception is `replace` edits which match start or\n    # end with whitespace. We also need to handle any leading or trailing\n    # whitespace ejected from the any fixes applied to child segments.\n    # Here we handle those by checking the start and end of the resulting\n    # segment sequence for whitespace.\n    # If we're left with any non-code at the end, trim them off and pass them\n    # up to the parent segment for handling.\n    if not segment.can_start_end_non_code:\n        _idx = 0\n        for _idx in range(0, len(seg_buffer)):\n            if segment._is_code_or_meta(seg_buffer[_idx]):\n                break\n        before = seg_buffer[:_idx]\n        seg_buffer = seg_buffer[_idx:]\n\n        _idx = len(seg_buffer)\n        for _idx in range(len(seg_buffer), 0, -1):\n            if segment._is_code_or_meta(seg_buffer[_idx - 1]):\n                break\n        after = seg_buffer[_idx:]\n        seg_buffer = seg_buffer[:_idx]\n\n    # Reform into a new segment\n    assert segment.pos_marker\n    try:\n        new_seg = segment.__class__(\n            # Realign the segments within\n            segments=segment._position_segments(\n                tuple(seg_buffer), parent_pos=segment.pos_marker\n            ),\n            pos_marker=segment.pos_marker,\n            # Pass through any additional kwargs\n            **{k: getattr(segment, k) for k in segment.additional_kwargs},\n        )\n    except AssertionError as err:  # pragma: no cover\n        # An AssertionError on creating a new segment is likely a whitespace\n        # check fail. If possible add information about the fixes we tried to\n        # apply, before re-raising.\n        # NOTE: only available in python 3.11+.\n        if hasattr(err, \"add_note\"):\n            err.add_note(f\" After applying fixes: {fixes_applied}.\")\n        raise err\n\n    # Handle any necessary validation.\n    if requires_validate:\n        # Was it already unparsable?\n        if \"unparsable\" in segment.descendant_type_set | segment.class_types:\n            if fix_even_unparsable:\n                # If we're fixing even unparsable sections, there's no point trying\n                # to validate, it will always fail. We may still want to validate\n                # other sections of the file though, so we should just declare *this*\n                # part of the file to be all good.\n                validated = True\n            else:\n                # It was already unparsable, but we're being asked to validate.\n                # Don't any apply fixes from within this region and just return the\n                # original segment.\n                return segment, [], [], True\n        # Otherwise only validate if there's a match_grammar. Otherwise we may get\n        # strange results (for example with the BracketedSegment).\n        elif hasattr(new_seg, \"match_grammar\"):\n            validated = new_seg.validate_segment_with_reparse(dialect)\n    else:\n        validated = not requires_validate\n    # Return the new segment and any non-code that needs to bubble up\n    # the tree.\n    # NOTE: We pass on whether this segment has been validated. It's\n    # very possible that our parsing here may fail depending on the\n    # type of segment that has been replaced, but if not we rely on\n    # a parent segment still being valid. If we get all the way up\n    # to the root and it's still not valid - that's a problem.\n    return new_seg, before, after, validated\n", "type": "function"}, {"name": "test__dialect__base_broad_fix", "is_method": false, "class_name": null, "parameters": ["dialect", "file", "raise_critical_errors_after_fix", "caplog"], "calls": ["pytest.mark.parametrize", "load_file", "dict", "lex_and_parse", "print", "FluffConfig", "Linter", "linter.get_rulepack", "linter.lint_parsed", "parsed.tree.stringify"], "code_location": {"file": "dialects_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 85, "end_line": 120}, "code_snippet": "def test__dialect__base_broad_fix(\n    dialect, file, raise_critical_errors_after_fix, caplog\n):\n    \"\"\"Run a full fix with all rules, in search of critical errors.\n\n    NOTE: This suite does all of the same things as the above test\n    suite (the `parse_suite`), but also runs fix. In CI, we run\n    the above tests _with_ coverage tracking, but these we run\n    _without_.\n\n    The purpose of this test is as a more stretching run through\n    a wide range of test sql examples, and the full range of rules\n    to find any potential critical errors raised by any interactions\n    between different dialects and rules.\n\n    We also do not use DEBUG logging here because it gets _very_\n    noisy.\n    \"\"\"\n    raw = load_file(dialect, file)\n    config_overrides = dict(dialect=dialect)\n\n    parsed: Optional[ParsedString] = lex_and_parse(config_overrides, raw)\n    if not parsed:  # Empty file case\n        return\n    print(parsed.tree.stringify())\n\n    config = FluffConfig(overrides=config_overrides)\n    linter = Linter(config=config)\n    rule_pack = linter.get_rulepack()\n    # Due to \"raise_critical_errors_after_fix\" fixture \"fix\",\n    # will now throw.\n    linter.lint_parsed(\n        parsed,\n        rule_pack,\n        fix=True,\n    )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3347597122192383}
{"question": "How can SQLFluff's configuration API be used to implement dynamic rule loading?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's configuration API can be used to implement dynamic rule loading through its flexible configuration system that supports runtime rule management and customization. The configuration API for dynamic rule loading works as follows: 1) Rule Discovery - The configuration system can discover and load rules dynamically based on configuration settings, supporting both built-in and plugin rules; 2) Rule Filtering - Configuration allows filtering rules by various criteria including rule codes, names, groups, and aliases through include/exclude patterns; 3) Rule Enablement - Rules can be dynamically enabled or disabled through configuration settings, allowing for flexible rule sets based on project requirements; 4) Rule Configuration - Each rule can have its own configuration parameters that are loaded dynamically and can be customized per project or environment; 5) Plugin Integration - The configuration system integrates with the plugin system to load custom rules from external packages based on configuration settings; 6) Rule Set Management - Configuration supports defining multiple rule sets that can be loaded and applied dynamically based on context or conditions; 7) Hierarchical Configuration - Rules can be configured at different levels (global, project, file-specific) with proper inheritance and override mechanisms; 8) Validation - The configuration system validates rule configurations and provides error messages for invalid settings; 9) Documentation Integration - Dynamic rule loading integrates with SQLFluff's help system to provide documentation for loaded rules; 10) Performance Optimization - The configuration system caches rule configurations and loading results to improve performance for repeated operations.", "score": null, "retrieved_content": [{"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/plugins/sqlfluff-plugin-example/src/sqlfluff_plugin_example", "start_line": 23, "end_line": 38}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: It is much better that we only import the rule on demand.\n    The root module of the plugin (i.e. this file which contains\n    all of the hook implementations) should have fully loaded before\n    we try and import the rules. This is partly for performance\n    reasons - but more because the definition of a BaseRule requires\n    that all of the get_configs_info() methods have both been\n    defined _and have run_ before so all the validation information\n    is available for the validation steps in the meta class.\n    \"\"\"\n    # i.e. we DO recommend importing here:\n    from sqlfluff_plugin_example.rules import Rule_Example_L001  # noqa: F811\n\n    return [Rule_Example_L001]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/aliasing", "start_line": 50, "end_line": 76}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.aliasing.AL01 import Rule_AL01\n    from sqlfluff.rules.aliasing.AL02 import Rule_AL02\n    from sqlfluff.rules.aliasing.AL03 import Rule_AL03\n    from sqlfluff.rules.aliasing.AL04 import Rule_AL04\n    from sqlfluff.rules.aliasing.AL05 import Rule_AL05\n    from sqlfluff.rules.aliasing.AL06 import Rule_AL06\n    from sqlfluff.rules.aliasing.AL07 import Rule_AL07\n    from sqlfluff.rules.aliasing.AL08 import Rule_AL08\n    from sqlfluff.rules.aliasing.AL09 import Rule_AL09\n\n    return [\n        Rule_AL01,\n        Rule_AL02,\n        Rule_AL03,\n        Rule_AL04,\n        Rule_AL05,\n        Rule_AL06,\n        Rule_AL07,\n        Rule_AL08,\n        Rule_AL09,\n    ]\n", "type": "function"}, {"name": "get_rulepack", "is_method": true, "class_name": "RuleSet", "parameters": ["self", "config"], "calls": ["self._validate_config_options", "config.get_section", "set", "self.rule_reference_map", "any", "any", "sorted", "self._expand_rule_refs", "self._expand_rule_refs", "RulePack", "self._register.keys", "manifest.rule_class.get_config_ref", "rules_logger.warning", "config.get", "list", "config.get", "rules_logger.warning", "rules_logger.warning", "self._register.keys", "rule_class.get_config_ref", "config.get_section", "description.format", "instantiated_rules.append", "self._register.values", "rules_config.items", "isinstance", "format", "format", "rules_config.items", "kwargs.update", "self._validate_config_options", "kwargs.update", "rule_class", "len", "rule_class.get_config_ref", "rules_logger.warning", "fnmatch.filter", "fnmatch.filter", "isinstance", "list", "rules_logger.warning", "reference_map.keys", "reference_map.keys"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 1078, "end_line": 1207}, "code_snippet": "    def get_rulepack(self, config: \"FluffConfig\") -> RulePack:\n        \"\"\"Use the config to return the appropriate rules.\n\n        We use the config both for allowlisting and denylisting, but also\n        for configuring the rules given the given config.\n        \"\"\"\n        # Validate all generic rule configs\n        self._validate_config_options(config)\n\n        # Fetch config section:\n        rules_config = config.get_section(\"rules\")\n\n        # Generate the master reference map. The priority order is:\n        # codes > names > groups > aliases\n        # (i.e. if there's a collision between a name and an\n        # alias - we assume the alias is wrong.)\n        valid_codes: set[str] = set(self._register.keys())\n        reference_map = self.rule_reference_map()\n        valid_config_lookups = {\n            manifest.rule_class.get_config_ref() for manifest in self._register.values()\n        }\n\n        # Validate config doesn't try to specify values for unknown rules.\n        # NOTE: We _warn_ here rather than error.\n        for unexpected_ref in [\n            # Filtering to dicts gives us the sections.\n            k\n            for k, v in rules_config.items()\n            if isinstance(v, dict)\n            # Only keeping ones we don't expect\n            if k not in valid_config_lookups\n        ]:\n            rules_logger.warning(\n                \"Rule configuration contain a section for unexpected \"\n                f\"rule {unexpected_ref!r}. These values will be ignored.\"\n            )\n            # For convenience (and migration), if we do find a potential match\n            # for the reference - add that as a warning.\n            # NOTE: We don't actually accept config in these cases, even though\n            # we could potentially match - because how to resolve _multiple_\n            # matching config sections is ambiguous.\n            if unexpected_ref in reference_map:\n                referenced_codes = reference_map[unexpected_ref]\n                if len(referenced_codes) == 1:\n                    referenced_code = list(referenced_codes)[0]\n                    referenced_name = self._register[referenced_code].name\n                    config_ref = self._register[\n                        referenced_code\n                    ].rule_class.get_config_ref()\n                    rules_logger.warning(\n                        \"The reference was however found as a match for rule \"\n                        f\"{referenced_code} with name {referenced_name!r}. \"\n                        \"SQLFluff assumes configuration for this rule will \"\n                        f\"be specified in 'sqlfluff:rules:{config_ref}'.\"\n                    )\n                elif referenced_codes:\n                    rules_logger.warning(\n                        \"The reference was found as a match for multiple rules: \"\n                        f\"{referenced_codes}. Config should be specified by the \"\n                        \"name of the relevant rule e.g. \"\n                        \"'sqlfluff:rules:capitalisation.keywords'.\"\n                    )\n\n        # The lists here are lists of references, which might be codes,\n        # names, aliases or groups.\n        # We default the allowlist to all the rules if not set (i.e. not specifying\n        # any rules, just means \"all the rules\").\n        allowlist = config.get(\"rule_allowlist\") or list(valid_codes)\n        denylist = config.get(\"rule_denylist\") or []\n\n        allowlisted_unknown_rule_codes = [\n            r\n            for r in allowlist\n            # Add valid groups to the register when searching for invalid rules _only_\n            if not fnmatch.filter(reference_map.keys(), r)\n        ]\n        if any(allowlisted_unknown_rule_codes):\n            rules_logger.warning(\n                \"Tried to allowlist unknown rule references: {!r}\".format(\n                    allowlisted_unknown_rule_codes\n                )\n            )\n\n        denylisted_unknown_rule_codes = [\n            r for r in denylist if not fnmatch.filter(reference_map.keys(), r)\n        ]\n        if any(denylisted_unknown_rule_codes):  # pragma: no cover\n            rules_logger.warning(\n                \"Tried to denylist unknown rules references: {!r}\".format(\n                    denylisted_unknown_rule_codes\n                )\n            )\n\n        keylist = sorted(self._register.keys())\n\n        # First we expand the allowlist and denylist globs\n        expanded_allowlist = self._expand_rule_refs(allowlist, reference_map)\n        expanded_denylist = self._expand_rule_refs(denylist, reference_map)\n\n        # Then we filter the rules\n        keylist = [\n            r for r in keylist if r in expanded_allowlist and r not in expanded_denylist\n        ]\n\n        # Construct the kwargs for each rule and instantiate in turn.\n        instantiated_rules = []\n        # Keep only config which isn't a section (for specific rule) (i.e. isn't a dict)\n        # We'll handle those directly in the specific rule config section below.\n        generic_rule_config = {\n            k: v for k, v in rules_config.items() if not isinstance(v, dict)\n        }\n        for code in keylist:\n            kwargs = {}\n            rule_class = self._register[code].rule_class\n            # Fetch the lookup code for the rule.\n            rule_config_ref = rule_class.get_config_ref()\n            specific_rule_config = config.get_section((\"rules\", rule_config_ref))\n            if generic_rule_config:\n                kwargs.update(generic_rule_config)\n            if specific_rule_config:\n                # Validate specific rule config before adding\n                self._validate_config_options(config, rule_config_ref)\n                kwargs.update(specific_rule_config)\n            kwargs[\"code\"] = code\n            # Allow variable substitution in making the description\n            kwargs[\"description\"] = self._register[code].description.format(**kwargs)\n            # Instantiate when ready\n            instantiated_rules.append(rule_class(**kwargs))\n\n        return RulePack(instantiated_rules, reference_map)\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/layout", "start_line": 47, "end_line": 85}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.layout.LT01 import Rule_LT01\n    from sqlfluff.rules.layout.LT02 import Rule_LT02\n    from sqlfluff.rules.layout.LT03 import Rule_LT03\n    from sqlfluff.rules.layout.LT04 import Rule_LT04\n    from sqlfluff.rules.layout.LT05 import Rule_LT05\n    from sqlfluff.rules.layout.LT06 import Rule_LT06\n    from sqlfluff.rules.layout.LT07 import Rule_LT07\n    from sqlfluff.rules.layout.LT08 import Rule_LT08\n    from sqlfluff.rules.layout.LT09 import Rule_LT09\n    from sqlfluff.rules.layout.LT10 import Rule_LT10\n    from sqlfluff.rules.layout.LT11 import Rule_LT11\n    from sqlfluff.rules.layout.LT12 import Rule_LT12\n    from sqlfluff.rules.layout.LT13 import Rule_LT13\n    from sqlfluff.rules.layout.LT14 import Rule_LT14\n    from sqlfluff.rules.layout.LT15 import Rule_LT15\n\n    return [\n        Rule_LT01,\n        Rule_LT02,\n        Rule_LT03,\n        Rule_LT04,\n        Rule_LT05,\n        Rule_LT06,\n        Rule_LT07,\n        Rule_LT08,\n        Rule_LT09,\n        Rule_LT10,\n        Rule_LT11,\n        Rule_LT12,\n        Rule_LT13,\n        Rule_LT14,\n        Rule_LT15,\n    ]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/jinja", "start_line": 8, "end_line": 16}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.jinja.JJ01 import Rule_JJ01\n\n    return [Rule_JJ01]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/capitalisation", "start_line": 41, "end_line": 53}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.capitalisation.CP01 import Rule_CP01\n    from sqlfluff.rules.capitalisation.CP02 import Rule_CP02\n    from sqlfluff.rules.capitalisation.CP03 import Rule_CP03\n    from sqlfluff.rules.capitalisation.CP04 import Rule_CP04\n    from sqlfluff.rules.capitalisation.CP05 import Rule_CP05\n\n    return [Rule_CP01, Rule_CP02, Rule_CP03, Rule_CP04, Rule_CP05]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/ambiguous", "start_line": 29, "end_line": 53}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.ambiguous.AM01 import Rule_AM01\n    from sqlfluff.rules.ambiguous.AM02 import Rule_AM02\n    from sqlfluff.rules.ambiguous.AM03 import Rule_AM03\n    from sqlfluff.rules.ambiguous.AM04 import Rule_AM04\n    from sqlfluff.rules.ambiguous.AM05 import Rule_AM05\n    from sqlfluff.rules.ambiguous.AM06 import Rule_AM06\n    from sqlfluff.rules.ambiguous.AM07 import Rule_AM07\n    from sqlfluff.rules.ambiguous.AM08 import Rule_AM08\n\n    return [\n        Rule_AM01,\n        Rule_AM02,\n        Rule_AM03,\n        Rule_AM04,\n        Rule_AM05,\n        Rule_AM06,\n        Rule_AM07,\n        Rule_AM08,\n    ]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 56, "end_line": 69}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.references.RF01 import Rule_RF01\n    from sqlfluff.rules.references.RF02 import Rule_RF02\n    from sqlfluff.rules.references.RF03 import Rule_RF03\n    from sqlfluff.rules.references.RF04 import Rule_RF04\n    from sqlfluff.rules.references.RF05 import Rule_RF05\n    from sqlfluff.rules.references.RF06 import Rule_RF06\n\n    return [Rule_RF01, Rule_RF02, Rule_RF03, Rule_RF04, Rule_RF05, Rule_RF06]\n", "type": "function"}, {"name": "get_rules", "is_method": false, "class_name": null, "parameters": [], "calls": [], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/structure", "start_line": 26, "end_line": 56}, "code_snippet": "def get_rules() -> list[type[BaseRule]]:\n    \"\"\"Get plugin rules.\n\n    NOTE: Rules are imported only on fetch to manage import times\n    when rules aren't used.\n    \"\"\"\n    from sqlfluff.rules.structure.ST01 import Rule_ST01\n    from sqlfluff.rules.structure.ST02 import Rule_ST02\n    from sqlfluff.rules.structure.ST03 import Rule_ST03\n    from sqlfluff.rules.structure.ST04 import Rule_ST04\n    from sqlfluff.rules.structure.ST05 import Rule_ST05\n    from sqlfluff.rules.structure.ST06 import Rule_ST06\n    from sqlfluff.rules.structure.ST07 import Rule_ST07\n    from sqlfluff.rules.structure.ST08 import Rule_ST08\n    from sqlfluff.rules.structure.ST09 import Rule_ST09\n    from sqlfluff.rules.structure.ST10 import Rule_ST10\n    from sqlfluff.rules.structure.ST11 import Rule_ST11\n\n    return [\n        Rule_ST01,\n        Rule_ST02,\n        Rule_ST03,\n        Rule_ST04,\n        Rule_ST05,\n        Rule_ST06,\n        Rule_ST07,\n        Rule_ST08,\n        Rule_ST09,\n        Rule_ST10,\n        Rule_ST11,\n    ]\n", "type": "function"}, {"name": "test__api__config_override", "is_method": false, "class_name": null, "parameters": ["kwargs", "expected", "tmpdir"], "calls": ["pytest.mark.parametrize", "sqlfluff.lint", "intersection", "set", "dict"], "code_location": {"file": "simple_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 643, "end_line": 650}, "code_snippet": "def test__api__config_override(kwargs, expected, tmpdir):\n    \"\"\"Test that parameters to lint() override .sqlfluff correctly (or not).\"\"\"\n    config_path = \"test/fixtures/api/config_override/.sqlfluff\"\n    sql = \"SELECT TRIM(name) AS name FROM some_table\"\n    lint_results = sqlfluff.lint(sql, config_path=config_path, **kwargs)\n    assert expected == {\"RF02\", \"RF04\"}.intersection(\n        {lr[\"code\"] for lr in lint_results}\n    )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3374152183532715}
{"question": "Why does SQLFluff implement a multi-dialect SQL parser for different SQL variants?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff implements a multi-dialect SQL parser to support the diverse ecosystem of SQL variants used across different database systems and applications. Key reasons include: 1) Database diversity - Different database systems (PostgreSQL, MySQL, Snowflake, BigQuery, etc.) have their own SQL dialects with unique syntax and features; 2) Vendor-specific extensions - Each database vendor adds proprietary SQL extensions that aren't part of standard SQL; 3) Syntax variations - Common operations have different syntax across dialects (e.g., string concatenation, date functions, window functions); 4) Keyword differences - Reserved keywords vary between dialects, requiring different lexing and parsing rules; 5) Function variations - Built-in functions have different names, parameters, and behavior across dialects; 6) Data type differences - Each dialect supports different data types and type casting syntax; 7) User requirements - Users need to lint SQL written for their specific database system; 8) Migration support - Organizations often need to support multiple dialects during database migrations; 9) Tool integration - SQLFluff needs to integrate with various database tools and frameworks that use different dialects; 10) Community needs - The SQL community uses multiple dialects, and SQLFluff aims to serve the entire community.", "score": null, "retrieved_content": [{"name": "TestFlinkSQLDialect", "docstring": "Test FlinkSQL dialect parsing.", "methods": ["test_flink_dialect_basic", "test_flink_create_table_basic", "test_flink_row_data_type", "test_flink_timestamp_with_precision", "test_flink_watermark_definition", "test_flink_computed_column", "test_flink_metadata_column", "test_flink_show_statements", "test_flink_use_statements", "test_flink_describe_statement", "test_flink_explain_statement", "test_flink_create_catalog", "test_flink_create_database", "test_flink_alternative_with_syntax"], "attributes": [], "code_location": {"file": "flink_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 6, "end_line": 231}, "type": "class"}, {"name": "cli", "is_method": false, "class_name": null, "parameters": [], "calls": ["click.group", "click.version_option"], "code_location": {"file": "commands.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/cli", "start_line": 480, "end_line": 481}, "code_snippet": "def cli() -> None:\n    \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n", "type": "function"}, {"name": "parse", "is_method": false, "class_name": null, "parameters": ["sql", "dialect", "config", "config_path"], "calls": ["Linter", "linter.parse_string", "parsed.root_variant", "root_variant.tree.as_record", "get_simple_config", "APIParsingError"], "code_location": {"file": "simple.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/api", "start_line": 168, "end_line": 212}, "code_snippet": "def parse(\n    sql: str,\n    dialect: Optional[str] = None,\n    config: Optional[FluffConfig] = None,\n    config_path: Optional[str] = None,\n) -> dict[str, Any]:\n    \"\"\"Parse a SQL string.\n\n    Args:\n        sql (:obj:`str`): The SQL to be parsed.\n        dialect (:obj:`Optional[str]`, optional): A reference to the dialect of the SQL\n            to be parsed. Defaults to `ansi`.\n        config (:obj:`Optional[FluffConfig]`, optional): A configuration object\n            to use for the operation. Defaults to None.\n        config_path (:obj:`Optional[str]`, optional): A path to a .sqlfluff config,\n            which is only used if a `config` is not already provided.\n            Defaults to None.\n\n    Returns:\n        :obj:`Dict[str, Any]` JSON containing the parsed structure.\n\n    Note:\n        In the case of multiple potential variants from the raw source file\n        only the first variant is returned by the simple API. For access to\n        the other variants, use the underlying main API directly.\n    \"\"\"\n    cfg = config or get_simple_config(\n        dialect=dialect,\n        config_path=config_path,\n    )\n    linter = Linter(config=cfg)\n\n    parsed = linter.parse_string(sql)\n    # If we encounter any parsing errors, raise them in a combined issue.\n    violations = parsed.violations\n    if violations:\n        raise APIParsingError(violations)\n    # Return a JSON representation of the parse tree.\n    # NOTE: For the simple API - only a single variant is returned.\n    root_variant = parsed.root_variant()\n    assert root_variant, \"Files parsed without violations must have a valid variant\"\n    assert root_variant.tree, \"Files parsed without violations must have a valid tree\"\n    record = root_variant.tree.as_record(show_raw=True)\n    assert record\n    return record\n", "type": "function"}, {"name": "SQLFluffDriver", "docstring": "SQLFluff driver for use by SQLFluffViolationReporter.", "methods": ["__init__", "parse_reports", "installed"], "attributes": [], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 22, "end_line": 42}, "type": "class"}, {"name": "test_flink_dialect_basic", "is_method": true, "class_name": "TestFlinkSQLDialect", "parameters": ["self"], "calls": ["FluffConfig", "Linter", "linter.lint_string", "len", "v.rule.code.startswith"], "code_location": {"file": "flink_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/dialects", "start_line": 9, "end_line": 20}, "code_snippet": "    def test_flink_dialect_basic(self):\n        \"\"\"Test basic FlinkSQL dialect functionality.\"\"\"\n        config = FluffConfig(overrides={\"dialect\": \"flink\"})\n        linter = Linter(config=config)\n\n        # Test simple SELECT statement\n        sql = \"SELECT * FROM my_table;\\n\"\n        result = linter.lint_string(sql)\n        assert result is not None\n        # Check for parsing errors only, ignore style warnings\n        parsing_errors = [v for v in result.violations if v.rule.code.startswith(\"PRS\")]\n        assert len(parsing_errors) == 0\n", "type": "function"}, {"name": "SQLFluffDomain", "docstring": "SQLFluff domain.", "methods": ["get_full_qualified_name", "get_objects", "resolve_xref", "add_rule"], "attributes": ["name", "label", "object_types", "roles", "directives", "initial_data"], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 82, "end_line": 142}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "SQLFluffDriver", "parameters": ["self"], "calls": ["__init__", "super", "s.encode", "sys.getfilesystemencoding"], "code_location": {"file": "diff_quality_plugin.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff", "start_line": 25, "end_line": 34}, "code_snippet": "    def __init__(self) -> None:\n        super().__init__(\n            [sys.executable, \"-m\", \"sqlfluff.cli.commands\"],\n            [\".sql\"],\n            [\n                s.encode(sys.getfilesystemencoding())\n                for s in [\"sqlfluff\", \"lint\", \"--format=json\"]\n            ],\n            exit_codes=[0, 1],\n        )\n", "type": "function"}, {"name": "test__api__info_rules", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.list_rules", "isinstance", "RuleTuple"], "code_location": {"file": "info_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 29, "end_line": 52}, "code_snippet": "def test__api__info_rules():\n    \"\"\"Basic linting of dialects.\"\"\"\n    rules = sqlfluff.list_rules()\n    assert isinstance(rules, list)\n    assert (\n        RuleTuple(\n            code=\"LT01\",\n            name=\"layout.spacing\",\n            description=\"Inappropriate Spacing.\",\n            groups=(\"all\", \"core\", \"layout\"),\n            aliases=(\n                \"L001\",\n                \"L005\",\n                \"L006\",\n                \"L008\",\n                \"L023\",\n                \"L024\",\n                \"L039\",\n                \"L048\",\n                \"L071\",\n            ),\n        )\n        in rules\n    )\n", "type": "function"}, {"name": "_initialise_dialect", "is_method": true, "class_name": "FluffConfig", "parameters": ["self", "dialect", "require_dialect"], "calls": ["dialect_selector", "self.verify_dialect_specified"], "code_location": {"file": "fluffconfig.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/config", "start_line": 153, "end_line": 162}, "code_snippet": "    def _initialise_dialect(\n        self, dialect: Optional[str], require_dialect: bool = True\n    ) -> None:\n        # NB: We import here to avoid a circular references.\n        from sqlfluff.core.dialects import dialect_selector\n\n        if dialect is not None:\n            self._configs[\"core\"][\"dialect_obj\"] = dialect_selector(dialect)\n        elif require_dialect:\n            self.verify_dialect_specified()\n", "type": "function"}, {"name": "test__api__info_dialects", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.list_dialects", "isinstance"], "code_location": {"file": "info_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 7, "end_line": 26}, "code_snippet": "def test__api__info_dialects():\n    \"\"\"Basic linting of dialects.\"\"\"\n    dialects = sqlfluff.list_dialects()\n    assert isinstance(dialects, list)\n    # Turn it into a dict so we can look for items in there.\n    dialect_dict = {dialect.label: dialect for dialect in dialects}\n    # Check the ansi dialect works\n    assert \"ansi\" in dialect_dict\n    ansi = dialect_dict[\"ansi\"]\n    assert ansi.label == \"ansi\"\n    assert ansi.name == \"ANSI\"\n    assert ansi.inherits_from == \"nothing\"\n    assert \"This is the base dialect\" in ansi.docstring\n    # Check one other works\n    assert \"postgres\" in dialect_dict\n    postgres = dialect_dict[\"postgres\"]\n    assert postgres.label == \"postgres\"\n    assert postgres.name == \"PostgreSQL\"\n    assert postgres.inherits_from == \"ansi\"\n    assert \"this is often the dialect to use\" in postgres.docstring\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.5533335208892822}
{"question": "Why does SQLFluff's rule caching mechanism optimize repeated linting operations?", "answer": null, "relative_code_list": null, "ground_truth": "SQLFluff's rule caching mechanism optimizes repeated linting operations by storing and reusing rule evaluation results to avoid redundant computations. Key benefits include: 1) Result caching - Rule evaluation results are cached to avoid re-computing the same checks on identical code segments; 2) Grammar caching - Frequently used grammar patterns and parse tree structures are cached for faster rule matching; 3) Configuration caching - Rule configurations and settings are cached to avoid repeated validation and processing; 4) Segment caching - Parsed segments and their metadata are cached to avoid re-parsing identical structures; 5) Context caching - Rule evaluation context and state information are cached for reuse; 6) Memory efficiency - Caching reduces memory allocation and deallocation overhead; 7) CPU optimization - Avoids redundant CPU-intensive operations like regex matching and tree traversal; 8) Incremental updates - Only changed segments trigger rule re-evaluation, while cached results are reused for unchanged code; 9) Batch processing - Multiple files can benefit from shared cached results; 10) Performance scaling - Caching improves performance more significantly as the number of rules and file size increases.", "score": null, "retrieved_content": [{"name": "test__config__rules_set_to_none", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "lntr.lint_path", "lnt.check_tuples_by_path", "FluffConfig.from_path"], "code_location": {"file": "fluffconfig_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/config", "start_line": 145, "end_line": 158}, "code_snippet": "def test__config__rules_set_to_none():\n    \"\"\"Test linting when rules are set to 'None'.\n\n    Ensure that all rules are still run.\n    \"\"\"\n    lntr = Linter(\n        config=FluffConfig.from_path(\"test/fixtures/config/rules_set_to_none\")\n    )\n    lnt = lntr.lint_path(\"test/fixtures/config/rules_set_to_none/test.sql\")\n    violations = lnt.check_tuples_by_path()\n    for k in violations:\n        assert (\"LT13\", 1, 1) in violations[k]\n        assert (\"AM04\", 12, 1) in violations[k]\n        assert (\"CP01\", 12, 10) in violations[k]\n", "type": "function"}, {"name": "SQLFluffRule", "docstring": "SQLFluff rule directive for sphinx.\n\nRule directives can be used as shown below.\n\n.. code-block:: rst\n\n    .. sqlfluff:rule:: AM01\n                       ambiguous.distinct\n\n        Write the documentation for the rule here.\n\nTo cross reference (i.e. refer to) objects defined like this\nboth the code and name reference is available:\n\n.. code-block:: rst\n\n    :sqlfluff:ref:`CP02`\n    :sqlfluff:ref:`capitalisation.identifiers`", "methods": ["handle_signature", "add_target_and_index", "_object_hierarchy_parts", "_toc_entry_name"], "attributes": [], "code_location": {"file": "sqlfluff_domain.py", "path": "/data3/pwh/swebench-repos/sqlfluff/docs/source/_ext", "start_line": 10, "end_line": 79}, "type": "class"}, {"name": "_init_ignore_words_list", "is_method": true, "class_name": "Rule_RF02", "parameters": ["self"], "calls": ["str", "getattr", "self.split_comma_separated_string", "ignore_words_config.lower"], "code_location": {"file": "RF02.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 160, "end_line": 170}, "code_snippet": "    def _init_ignore_words_list(self) -> list[str]:\n        \"\"\"Called first time rule is evaluated to fetch & cache the policy.\"\"\"\n        ignore_words_config: str = str(getattr(self, \"ignore_words\"))\n        if ignore_words_config and ignore_words_config != \"None\":\n            self.ignore_words_list = self.split_comma_separated_string(\n                ignore_words_config.lower()\n            )\n        else:\n            self.ignore_words_list = []\n\n        return self.ignore_words_list\n", "type": "function"}, {"name": "_init_ignore_words_list", "is_method": true, "class_name": "Rule_RF05", "parameters": ["self"], "calls": ["str", "getattr", "self.split_comma_separated_string", "ignore_words_config.lower"], "code_location": {"file": "RF05.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/references", "start_line": 227, "end_line": 237}, "code_snippet": "    def _init_ignore_words_list(self) -> list[str]:\n        \"\"\"Called first time rule is evaluated to fetch & cache the policy.\"\"\"\n        ignore_words_config: str = str(getattr(self, \"ignore_words\"))\n        if ignore_words_config and ignore_words_config != \"None\":\n            self.ignore_words_list = self.split_comma_separated_string(\n                ignore_words_config.lower()\n            )\n        else:\n            self.ignore_words_list = []\n\n        return self.ignore_words_list\n", "type": "function"}, {"name": "crawl", "is_method": true, "class_name": "BaseRule", "parameters": ["self", "tree", "dialect", "fix", "templated_file", "ignore_mask", "fname", "config"], "calls": ["RuleContext", "self.crawl_behaviour.crawl", "self._eval", "isinstance", "self.logger.info", "tuple", "pathlib.Path", "self.logger.critical", "context.segment.pos_marker.source_position", "self._log_critical_errors", "vs.append", "self._adjust_anchors_for_fixes", "self._process_lint_result", "rules_logger.error", "self.logger.info", "SQLLintError", "isinstance", "all", "TypeError", "self._adjust_anchors_for_fixes", "self._process_lint_result", "format", "isinstance", "str"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 480, "end_line": 600}, "code_snippet": "    def crawl(\n        self,\n        tree: BaseSegment,\n        dialect: \"Dialect\",\n        fix: bool,\n        templated_file: Optional[\"TemplatedFile\"],\n        ignore_mask: Optional[\"IgnoreMask\"],\n        fname: Optional[str],\n        config: \"FluffConfig\",\n    ) -> tuple[\n        list[SQLLintError],\n        tuple[RawSegment, ...],\n        list[LintFix],\n        Optional[dict[str, Any]],\n    ]:\n        \"\"\"Run the rule on a given tree.\n\n        Returns:\n            A tuple of (vs, raw_stack, fixes, memory)\n\n        \"\"\"\n        root_context = RuleContext(\n            dialect=dialect,\n            fix=fix,\n            templated_file=templated_file,\n            path=pathlib.Path(fname) if fname else None,\n            segment=tree,\n            config=config,\n        )\n        vs: list[SQLLintError] = []\n        fixes: list[LintFix] = []\n\n        # Propagates memory from one rule _eval() to the next.\n        memory = root_context.memory\n        context = root_context\n        for context in self.crawl_behaviour.crawl(root_context):\n            try:\n                context.memory = memory\n                res = self._eval(context=context)\n            except (bdb.BdbQuit, KeyboardInterrupt):  # pragma: no cover\n                raise\n            # Any exception at this point would halt the linter and\n            # cause the user to get no results\n            except Exception as e:\n                # If a filename is present, include it in the critical exception.\n                self.logger.critical(\n                    (\n                        f\"Applying rule {self.code} to {fname!r} \"\n                        f\"threw an Exception: {e}\"\n                        if fname\n                        else f\"Applying rule {self.code} threw an Exception: {e}\"\n                    ),\n                    exc_info=True,\n                )\n                assert context.segment.pos_marker\n                exception_line, _ = context.segment.pos_marker.source_position()\n                self._log_critical_errors(e)\n                vs.append(\n                    SQLLintError(\n                        rule=self,\n                        segment=context.segment,\n                        fixes=[],\n                        description=(\n                            f\"Unexpected exception: {str(e)};\\n\"\n                            \"Could you open an issue at \"\n                            \"https://github.com/sqlfluff/sqlfluff/issues ?\\n\"\n                            \"You can ignore this exception for now, by adding \"\n                            f\"'-- noqa: {self.code}' at the end\\n\"\n                            f\"of line {exception_line}\\n\"\n                        ),\n                    )\n                )\n                return vs, context.raw_stack, fixes, context.memory\n\n            new_lerrs: list[SQLLintError] = []\n            new_fixes: list[LintFix] = []\n\n            if res is None or res == []:\n                # Assume this means no problems (also means no memory)\n                pass\n            elif isinstance(res, LintResult):\n                # Extract any memory\n                memory = res.memory\n                self._adjust_anchors_for_fixes(context, res)\n                self._process_lint_result(\n                    res, templated_file, ignore_mask, new_lerrs, new_fixes, tree\n                )\n            elif isinstance(res, list) and all(\n                isinstance(elem, LintResult) for elem in res\n            ):\n                # Extract any memory from the *last* one, assuming\n                # it was the last to be added\n                memory = res[-1].memory\n                for elem in res:\n                    self._adjust_anchors_for_fixes(context, elem)\n                    self._process_lint_result(\n                        elem, templated_file, ignore_mask, new_lerrs, new_fixes, tree\n                    )\n            else:  # pragma: no cover\n                raise TypeError(\n                    \"Got unexpected result [{!r}] back from linting rule: {!r}\".format(\n                        res, self.code\n                    )\n                )\n\n            for lerr in new_lerrs:\n                self.logger.info(\"!! Violation Found: %r\", lerr.description)\n            if new_fixes:\n                if not self.is_fix_compatible:  # pragma: no cover\n                    rules_logger.error(\n                        f\"Rule {self.code} returned a fix but is not documented as \"\n                        \"`is_fix_compatible`, you may encounter unusual fixing \"\n                        \"behaviour. Report this a bug to the developer of this rule.\"\n                    )\n                for lfix in new_fixes:\n                    self.logger.info(\"!! Fix Proposed: %r\", lfix)\n\n            # Consume the new results\n            vs += new_lerrs\n            fixes += new_fixes\n        return vs, context.raw_stack if context else tuple(), fixes, context.memory\n", "type": "function"}, {"name": "test__api__fix_string_specific", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.fix"], "code_location": {"file": "simple_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 434, "end_line": 438}, "code_snippet": "def test__api__fix_string_specific():\n    \"\"\"Basic checking of lint functionality with a specific rule.\"\"\"\n    result = sqlfluff.fix(my_bad_query, rules=[\"CP01\"])\n    # Check actual result\n    assert result == \"SELECT  *, 1, blah AS  fOO  FROM myTable\"\n", "type": "function"}, {"name": "_init_capitalisation_policy", "is_method": true, "class_name": "Rule_CP01", "parameters": ["self", "context"], "calls": ["next", "getattr", "str", "context.config.get", "self.logger.debug", "getattr", "self.split_comma_separated_string", "ignore_words_config.lower", "k.endswith", "get_config_info"], "code_location": {"file": "CP01.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/rules/capitalisation", "start_line": 277, "end_line": 302}, "code_snippet": "    def _init_capitalisation_policy(self, context: RuleContext):\n        \"\"\"Called first time rule is evaluated to fetch & cache the policy.\"\"\"\n        cap_policy_name = next(\n            k for k in self.config_keywords if k.endswith(\"capitalisation_policy\")\n        )\n        self.cap_policy = getattr(self, cap_policy_name)\n        valid_options = get_config_info()[cap_policy_name][\"validation\"] or []\n        self.cap_policy_opts = [opt for opt in valid_options if opt != \"consistent\"]\n        # Use str() as CP04 uses bools which might otherwise be read as bool\n        ignore_words_config = str(getattr(self, \"ignore_words\"))\n        if ignore_words_config and ignore_words_config != \"None\":\n            self.ignore_words_list = self.split_comma_separated_string(\n                ignore_words_config.lower()\n            )\n        else:\n            self.ignore_words_list = []\n        self.ignore_templated_areas = context.config.get(\"ignore_templated_areas\")\n        self.logger.debug(\n            f\"Selected '{cap_policy_name}': '{self.cap_policy}' from options \"\n            f\"{self.cap_policy_opts}\"\n        )\n        cap_policy = self.cap_policy\n        cap_policy_opts = self.cap_policy_opts\n        ignore_words_list = self.ignore_words_list\n        ignore_templated_areas = self.ignore_templated_areas\n        return cap_policy, cap_policy_opts, ignore_words_list, ignore_templated_areas\n", "type": "function"}, {"name": "test__rules__result_unparsable", "is_method": false, "class_name": null, "parameters": [], "calls": ["Linter", "any", "fluff_log_catcher", "linter.lint_string", "v.rule_code"], "code_location": {"file": "rules_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/core/rules", "start_line": 214, "end_line": 229}, "code_snippet": "def test__rules__result_unparsable():\n    \"\"\"Test that the linter won't allow rules which make the file unparsable.\"\"\"\n    # Set up a linter with the user rule\n    linter = Linter(user_rules=[Rule_T003], dialect=\"ansi\", rules=[\"T003\"])\n    # Lint a simple parsable file and check we do get issues\n    # It's parsable, so we should get issues.\n    raw_sql = \"SELECT 1 FROM a\"\n    with fluff_log_catcher(logging.WARNING, \"sqlfluff\") as caplog:\n        res = linter.lint_string(raw_sql, fix=True)\n    # Check we got the warning.\n    assert \"would result in an unparsable file\" in caplog.text\n    # Check we get the violation.\n    assert any(v.rule_code() == \"T003\" for v in res.violations)\n    # The resulting file should be _the same_ because it would have resulted\n    # in an unparsable file if applied.\n    assert res.tree.raw == raw_sql\n", "type": "function"}, {"name": "test__api__fix_string_specific_exclude", "is_method": false, "class_name": null, "parameters": [], "calls": ["sqlfluff.fix"], "code_location": {"file": "simple_test.py", "path": "/data3/pwh/swebench-repos/sqlfluff/test/api", "start_line": 441, "end_line": 445}, "code_snippet": "def test__api__fix_string_specific_exclude():\n    \"\"\"Basic checking of lint functionality with a specific rule exclusion.\"\"\"\n    result = sqlfluff.fix(my_bad_query, exclude_rules=[\"LT09\"])\n    # Check actual result\n    assert result == \"SELECT *, 1, blah AS foo FROM mytable\\n\"\n", "type": "function"}, {"name": "_process_lint_result", "is_method": true, "class_name": "BaseRule", "parameters": ["self", "res", "templated_file", "ignore_mask", "new_lerrs", "new_fixes", "root"], "calls": ["res.to_linting_error", "new_lerrs.append", "new_fixes.extend", "self.discard_unsafe_fixes", "root.path_to", "ignore_mask.ignore_masked_violations", "self.crawl_behaviour.passes_filter", "linter_logger.info", "all", "linter_logger.info", "self.crawl_behaviour.passes_filter"], "code_location": {"file": "base.py", "path": "/data3/pwh/swebench-repos/sqlfluff/src/sqlfluff/core/rules", "start_line": 608, "end_line": 655}, "code_snippet": "    def _process_lint_result(\n        self,\n        res: LintResult,\n        templated_file: Optional[TemplatedFile],\n        ignore_mask: Optional[\"IgnoreMask\"],\n        new_lerrs: list[SQLLintError],\n        new_fixes: list[LintFix],\n        root: BaseSegment,\n    ) -> None:\n        # Unless the rule declares that it's already template safe. Do safety\n        # checks.\n        if not self.template_safe_fixes:\n            self.discard_unsafe_fixes(res, templated_file)\n        lerr = res.to_linting_error(rule=self)\n        if not lerr:\n            return None\n        if ignore_mask:\n            if not ignore_mask.ignore_masked_violations([lerr]):\n                return None\n\n        # Check whether this should be filtered out for being unparsable.\n        # To do that we check the parents of the anchors (of the violation\n        # and fixes) against the filter in the crawler.\n        # NOTE: We use `.passes_filter` here to do the test for unparsable\n        # to avoid duplicating code because that test is already implemented\n        # there.\n        anchors = [lerr.segment] + [fix.anchor for fix in lerr.fixes]\n        for anchor in anchors:\n            if not self.crawl_behaviour.passes_filter(anchor):  # pragma: no cover\n                # NOTE: This clause is untested, because it's a hard to produce\n                # edge case. The latter clause is much more likely.\n                linter_logger.info(\n                    \"Fix skipped due to anchor not passing filter: %s\", anchor\n                )\n                return None\n\n            parent_stack = root.path_to(anchor)\n            if not all(\n                self.crawl_behaviour.passes_filter(ps.segment) for ps in parent_stack\n            ):\n                linter_logger.info(\n                    \"Fix skipped due to parent of anchor not passing filter: %s\",\n                    [ps.segment for ps in parent_stack],\n                )\n                return None\n\n        new_lerrs.append(lerr)\n        new_fixes.extend(res.fixes)\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.6897451877593994}
