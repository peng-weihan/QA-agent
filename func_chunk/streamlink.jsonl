{"question": "What are the dataflow dependencies between the composite data structure that aggregates token status and parsed token data and the supporting data classes during JSON serialization and deserialization?", "answer": "", "relative_code_list": null, "ground_truth": "The OriginTrialTokenWithStatus class has bidirectional dataflow dependencies with both OriginTrialTokenStatus and OriginTrialToken utility classes during JSON operations. In the to_json method, it calls status.to_json() and parsed_token.to_json() (when present), creating an outward dataflow where OriginTrialTokenWithStatus depends on these utility classes to serialize their state. Conversely, in the from_json method, it calls OriginTrialTokenStatus.from_json() and OriginTrialToken.from_json() (when the parsedToken key exists), creating an inward dataflow where the utility classes are responsible for reconstructing their instances from JSON data. This creates a circular dependency pattern where OriginTrialTokenWithStatus orchestrates the JSON conversion process but delegates the actual serialization/deserialization logic to the dependent utility classes, establishing a clear hierarchical dataflow relationship.", "score": null, "retrieved_content": [{"name": "to_json", "is_method": true, "class_name": "OriginTrialTokenWithStatus", "parameters": ["self"], "calls": ["self.status.to_json", "self.parsed_token.to_json"], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 485, "end_line": 491}, "code_snippet": "    def to_json(self) -> T_JSON_DICT:\n        json: T_JSON_DICT = {}\n        json[\"rawTokenText\"] = self.raw_token_text\n        json[\"status\"] = self.status.to_json()\n        if self.parsed_token is not None:\n            json[\"parsedToken\"] = self.parsed_token.to_json()\n        return json\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "OriginTrialTokenWithStatus", "parameters": ["cls", "json"], "calls": ["cls", "str", "OriginTrialTokenStatus.from_json", "OriginTrialToken.from_json"], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 494, "end_line": 499}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> OriginTrialTokenWithStatus:\n        return cls(\n            raw_token_text=str(json[\"rawTokenText\"]),\n            status=OriginTrialTokenStatus.from_json(json[\"status\"]),\n            parsed_token=OriginTrialToken.from_json(json[\"parsedToken\"]) if \"parsedToken\" in json else None,\n        )\n", "type": "function"}, {"name": "to_json", "is_method": true, "class_name": "OriginTrial", "parameters": ["self"], "calls": ["self.status.to_json", "i.to_json"], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 510, "end_line": 515}, "code_snippet": "    def to_json(self) -> T_JSON_DICT:\n        json: T_JSON_DICT = {}\n        json[\"trialName\"] = self.trial_name\n        json[\"status\"] = self.status.to_json()\n        json[\"tokensWithStatus\"] = [i.to_json() for i in self.tokens_with_status]\n        return json\n", "type": "function"}, {"name": "OriginTrialTokenWithStatus", "docstring": "", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 476, "end_line": 499}, "type": "class"}, {"name": "from_json", "is_method": true, "class_name": "OriginTrialTokenStatus", "parameters": ["cls", "json"], "calls": ["cls"], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 406, "end_line": 407}, "code_snippet": "    def from_json(cls, json: str) -> OriginTrialTokenStatus:\n        return cls(json)\n", "type": "function"}, {"name": "OriginTrialTokenStatus", "docstring": "Origin Trial(https://www.chromium.org/blink/origin-trials) support.\nStatus for an Origin Trial token.", "methods": ["to_json", "from_json"], "attributes": ["SUCCESS", "NOT_SUPPORTED", "INSECURE", "EXPIRED", "WRONG_ORIGIN", "INVALID_SIGNATURE", "MALFORMED", "WRONG_VERSION", "FEATURE_DISABLED", "TOKEN_DISABLED", "FEATURE_DISABLED_FOR_USER", "UNKNOWN_TRIAL"], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 384, "end_line": 407}, "type": "class"}, {"name": "from_json", "is_method": true, "class_name": "OriginTrialToken", "parameters": ["cls", "json"], "calls": ["cls", "str", "bool", "str", "network.TimeSinceEpoch.from_json", "bool", "OriginTrialUsageRestriction.from_json"], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 464, "end_line": 472}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> OriginTrialToken:\n        return cls(\n            origin=str(json[\"origin\"]),\n            match_sub_domains=bool(json[\"matchSubDomains\"]),\n            trial_name=str(json[\"trialName\"]),\n            expiry_time=network.TimeSinceEpoch.from_json(json[\"expiryTime\"]),\n            is_third_party=bool(json[\"isThirdParty\"]),\n            usage_restriction=OriginTrialUsageRestriction.from_json(json[\"usageRestriction\"]),\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "TrustTokenOperationDone", "parameters": ["cls", "json"], "calls": ["cls", "str", "TrustTokenOperationType.from_json", "RequestId.from_json", "str", "str", "int"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 4377, "end_line": 4385}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> TrustTokenOperationDone:\n        return cls(\n            status=str(json[\"status\"]),\n            type_=TrustTokenOperationType.from_json(json[\"type\"]),\n            request_id=RequestId.from_json(json[\"requestId\"]),\n            top_level_origin=str(json[\"topLevelOrigin\"]) if \"topLevelOrigin\" in json else None,\n            issuer_origin=str(json[\"issuerOrigin\"]) if \"issuerOrigin\" in json else None,\n            issued_token_count=int(json[\"issuedTokenCount\"]) if \"issuedTokenCount\" in json else None,\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "OriginTrial", "parameters": ["cls", "json"], "calls": ["cls", "str", "OriginTrialStatus.from_json", "OriginTrialTokenWithStatus.from_json"], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 518, "end_line": 523}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> OriginTrial:\n        return cls(\n            trial_name=str(json[\"trialName\"]),\n            status=OriginTrialStatus.from_json(json[\"status\"]),\n            tokens_with_status=[OriginTrialTokenWithStatus.from_json(i) for i in json[\"tokensWithStatus\"]],\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "CookiePartitionKey", "parameters": ["cls", "json"], "calls": ["cls", "str", "bool"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 1308, "end_line": 1312}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> CookiePartitionKey:\n        return cls(\n            top_level_site=str(json[\"topLevelSite\"]),\n            has_cross_site_ancestor=bool(json[\"hasCrossSiteAncestor\"]),\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0174224376678467}
{"question": "How does the specialized HLS stream class that coordinates with a WebSocket client integrate WebSocket communication with HLS stream processing to maintain real-time synchronization?", "answer": "", "relative_code_list": null, "ground_truth": "The NicoLiveHLSStream class extends HLSStream and initializes with a NicoLiveWsClient instance, enabling bidirectional communication between HLS segment fetching and WebSocket events for real-time stream synchronization and metadata updates.", "score": null, "retrieved_content": [{"name": "EventedWorkerHLSStream", "docstring": "", "methods": [], "attributes": ["__reader__"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 144, "end_line": 145}, "type": "class"}, {"name": "on_message_stream", "is_method": true, "class_name": "NicoLiveWsClient", "parameters": ["self", "data"], "calls": ["data.get", "self.ready.set", "self.opened.wait", "data.get", "self._SCHEMA_COOKIES.validate", "log.debug", "log.info", "self.close", "data.get", "data.get", "self.session.http.cookies.set"], "code_location": {"file": "nicolive.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 81, "end_line": 96}, "code_snippet": "    def on_message_stream(self, data):\n        if data.get(\"protocol\") != \"hls\" or not data.get(\"uri\"):\n            return\n\n        # cookies may be required by some HLS multivariant playlists\n        if cookies := data.get(\"cookies\", []):\n            for cookie in self._SCHEMA_COOKIES.validate(cookies):\n                self.session.http.cookies.set(**cookie)\n\n        self.hls_stream_url = data.get(\"uri\")\n        self.ready.set()\n        if self.opened.wait(self.STREAM_OPENED_TIMEOUT):\n            log.debug(\"Stream opened, keeping websocket connection alive\")\n        else:\n            log.info(\"Closing websocket connection\")\n            self.close()\n", "type": "function"}, {"name": "EventedHLSStreamWorker", "docstring": "", "methods": ["__init__", "reload", "wait"], "attributes": [], "code_location": {"file": "stream_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/mixins", "start_line": 96, "end_line": 110}, "type": "class"}, {"name": "EventedWriterHLSStream", "docstring": "", "methods": [], "attributes": ["__reader__"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 148, "end_line": 149}, "type": "class"}, {"name": "_get_hls_stream_url", "is_method": true, "class_name": "NicoLive", "parameters": ["self"], "calls": ["log.debug", "log.error", "self.wsclient.close", "self.wsclient.ready.wait", "self.wsclient.is_alive"], "code_location": {"file": "nicolive.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 265, "end_line": 272}, "code_snippet": "    def _get_hls_stream_url(self):\n        log.debug(f\"Waiting for permit (for at most {self.STREAM_READY_TIMEOUT} seconds)...\")\n        if not self.wsclient.ready.wait(self.STREAM_READY_TIMEOUT) or not self.wsclient.is_alive():\n            log.error(\"Waiting for permit timed out.\")\n            self.wsclient.close()\n            return\n\n        return self.wsclient.hls_stream_url\n", "type": "function"}, {"name": "EventedHLSStreamWriter", "docstring": "", "methods": ["__init__", "_queue_put", "_queue_get", "_future_result", "write"], "attributes": [], "code_location": {"file": "stream_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/mixins", "start_line": 113, "end_line": 135}, "type": "class"}, {"name": "HLSStreamWorker", "docstring": "", "methods": ["__init__", "_fetch_playlist", "reload", "_get_reload_time", "process_segments", "valid_segment", "_segment_queue_timing_threshold_reached", "duration_to_sequence", "iter_segments"], "attributes": ["SEGMENT_QUEUE_TIMING_THRESHOLD_MIN", "_RELOAD_TIME_MIN", "_RELOAD_TIME_DEFAULT"], "code_location": {"file": "hls.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/hls", "start_line": 303, "end_line": 546}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "UStreamTVStreamWorker", "parameters": ["self"], "calls": ["__init__", "self.wsclient.segments_subscribe", "super"], "code_location": {"file": "ustreamtv.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 416, "end_line": 420}, "code_snippet": "    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.wsclient = self.stream.wsclient\n        self.segment_id = self.wsclient.stream_initial_id\n        self.queue = self.wsclient.segments_subscribe()\n", "type": "function"}, {"name": "EventedWorkerHLSStreamReader", "docstring": "", "methods": [], "attributes": ["__worker__"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 136, "end_line": 137}, "type": "class"}, {"name": "ChzzkHLSStreamWorker", "docstring": "Custom HLS stream worker that adds __bgda__ query parameter to segment URLs", "methods": ["process_segments"], "attributes": [], "code_location": {"file": "chzzk.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 25, "end_line": 41}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.017545461654663}
{"question": "What architectural patterns enable a streaming service plugin that handles multiple content types to maintain separation of concerns between API data retrieval and stream processing while handling multiple content types?", "answer": "", "relative_code_list": null, "ground_truth": "The Pluto plugin employs a strategy pattern architecture where the main _get_streams method acts as a context that delegates to specialized methods (_get_streams_live, _get_streams_series, _get_streams_movies) based on content type. This separation is achieved through a centralized API data retrieval method (_get_api_data) that handles all HTTP requests and schema validation, while the content-specific methods process the returned data structure differently. The architecture uses template method pattern where the base class defines the overall stream retrieval flow, but the specific implementation details for different content types are handled in separate methods, ensuring clean separation between API communication logic and content-specific processing while maintaining a unified interface for stream generation.", "score": null, "retrieved_content": [{"name": "_get_streams_content", "is_method": true, "class_name": "WWENetwork", "parameters": ["self", "content_type", "content_id", "token"], "calls": ["self.session.http.get", "validate.Schema", "self.session.http.get", "format", "log.error", "log.error", "log.error", "validate.union_get", "playback_streams.get", "validate.Schema", "validate.optional", "validate.url", "validate.none_or_all", "validate.Schema", "items", "self._API_URLS.get", "validate.parse_json", "validate.any", "validate.filter", "validate.parse_json", "self.session.get_option", "validate.all", "validate.all", "validate.optional", "validate.any", "HLSStream.parse_variant_playlist", "HTTPStream", "validate.get", "validate.transform", "validate.union_get", "validate.transform", "validate.url", "validate.all", "validate.optional", "validate.optional", "validate.optional", "validate.any", "validate.list", "validate.get", "MuxedStream", "validate.url"], "code_location": {"file": "wwenetwork.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 81, "end_line": 165}, "code_snippet": "    def _get_streams_content(self, content_type, content_id, token):\n        success, data = self.session.http.get(\n            self._API_URLS.get(content_type).format(content_id),\n            acceptable_status=(200, 401, 404),\n            params={\"includePlaybackDetails\": \"URL\"},\n            headers={\"Authorization\": f\"Bearer {token}\", **self._API_HEADERS},\n            schema=validate.Schema(\n                validate.parse_json(),\n                validate.any(\n                    validate.all(\n                        {\"messages\": [str]},\n                        validate.get((\"messages\", 0)),\n                        validate.transform(lambda message: (False, message)),\n                    ),\n                    validate.all(\n                        {\n                            \"accessLevel\": str,\n                            validate.optional(\"id\"): int,\n                            validate.optional(\"title\"): str,\n                            validate.optional(\"playerUrlCallback\"): validate.any(None, validate.url()),\n                        },\n                        validate.union_get(\n                            \"accessLevel\",\n                            \"playerUrlCallback\",\n                            \"id\",\n                            \"title\",\n                        ),\n                        validate.transform(lambda data: (True, data)),\n                    ),\n                ),\n            ),\n        )\n\n        if not success:\n            log.error(data)\n            return\n\n        access, playback_url, self.id, self.title = data\n\n        if access != \"GRANTED\":\n            log.error(\"Paid subscription required for this video\")\n            return\n\n        if not playback_url:\n            log.error(\"Failed to get playerUrlCallback from response\")\n            return\n\n        playback_schema = validate.Schema(\n            {\n                \"url\": validate.url(),\n                validate.optional(\"subtitles\"): validate.none_or_all(\n                    [{\"language\": str, \"format\": str, \"url\": validate.url()}],\n                    validate.filter(lambda s: s[\"format\"] == \"srt\"),\n                ),\n            },\n            validate.union_get(\"url\", \"subtitles\"),\n        )\n\n        playback_streams = self.session.http.get(\n            playback_url,\n            schema=validate.Schema(\n                validate.parse_json(),\n                {\n                    validate.optional(\"hls\"): validate.any(\n                        validate.all(\n                            validate.list(playback_schema),\n                            validate.get(0),\n                        ),\n                        playback_schema,\n                    ),\n                },\n            ),\n        )\n\n        if playback_stream := playback_streams.get(\"hls\"):\n            url, subtitles = playback_stream\n\n            if streams := HLSStream.parse_variant_playlist(self.session, url).items():\n                if subtitles and self.session.get_option(\"mux-subtitles\"):\n                    substreams = {s[\"language\"]: HTTPStream(self.session, s[\"url\"]) for s in subtitles}\n\n                    for quality, stream in streams:\n                        yield quality, MuxedStream(self.session, stream, subtitles=substreams)\n                else:\n                    yield from streams\n", "type": "function"}, {"name": "_get_streams", "is_method": true, "class_name": "AdultSwim", "parameters": ["self"], "calls": ["self.match.groups", "log.debug", "log.debug", "self.session.http.get", "self.session.http.json", "log.debug", "HLSStream.parse_variant_playlist", "format", "self._get_stream_data", "PluginError", "format", "self.video_data_url.format", "format", "PluginError", "self._get_stream_data", "urlparse", "self._get_token", "urlunparse", "PluginError", "self._get_video_data", "PluginError", "PluginError", "format", "format", "format"], "code_location": {"file": "adultswim.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 164, "end_line": 214}, "code_snippet": "    def _get_streams(self):\n        url_type, show_name, episode_name = self.match.groups()\n\n        if url_type == \"streams\" and not show_name:\n            url_type = \"live-stream\"\n        elif not show_name:\n            raise PluginError(f\"Missing show_name for url_type: {url_type}\")\n\n        log.debug(\"URL type={0}\".format(url_type))\n\n        if url_type == \"live-stream\":\n            video_id = self._get_stream_data(url_type)\n        elif url_type == \"streams\":\n            video_id = self._get_stream_data(show_name)\n        elif url_type == \"videos\":\n            if show_name is None or episode_name is None:\n                raise PluginError(\n                    \"Missing show_name or episode_name for url_type: {0}\".format(\n                        url_type,\n                    ),\n                )\n            video_id = self._get_video_data(episode_name)\n        else:\n            raise PluginError(\"Unrecognised url_type: {0}\".format(url_type))\n\n        if video_id is None:\n            raise PluginError(\"Could not find video_id\")\n        log.debug(\"Video ID={0}\".format(video_id))\n\n        res = self.session.http.get(self.video_data_url.format(video_id))\n\n        url_data = self.session.http.json(res, schema=self._api_schema)\n        if \"unprotected\" in url_data:\n            url = url_data[\"unprotected\"][\"url\"]\n        elif \"bulkaes\" in url_data:\n            url_parsed = urlparse(url_data[\"bulkaes\"][\"url\"])\n            token = self._get_token(url_parsed.path)\n            url = urlunparse((\n                url_parsed.scheme,\n                url_parsed.netloc,\n                url_parsed.path,\n                url_parsed.params,\n                \"{0}={1}\".format(\"hdnts\", token),\n                url_parsed.fragment,\n            ))\n        else:\n            raise PluginError(\"Could not find a usable URL in url_data\")\n\n        log.debug(\"URL={0}\".format(url))\n\n        return HLSStream.parse_variant_playlist(self.session, url)\n", "type": "function"}, {"name": "_get_streams_api", "is_method": true, "class_name": "Delfi", "parameters": ["self", "video_id"], "calls": ["log.debug", "self.match.group", "itertools.chain", "self.session.http.get", "update_scheme", "self._api.get", "log.error", "data.values", "items", "dict", "validate.Schema", "validate.parse_json", "validate.get", "HLSStream.parse_variant_playlist", "validate.all", "validate.filter"], "code_location": {"file": "delfi.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 34, "end_line": 69}, "code_snippet": "    def _get_streams_api(self, video_id):\n        log.debug(f\"Found video ID: {video_id}\")\n\n        tld = self.match.group(\"tld\")\n        try:\n            data = self.session.http.get(\n                self._api.get(tld, \"lt\"),\n                params=dict(video_id=video_id),\n                schema=validate.Schema(\n                    validate.parse_json(),\n                    {\n                        \"success\": True,\n                        \"data\": {\n                            \"versions\": {\n                                str: validate.all(\n                                    [\n                                        {\n                                            \"type\": str,\n                                            \"src\": str,\n                                        },\n                                    ],\n                                    validate.filter(lambda item: item[\"type\"] == \"application/x-mpegurl\"),\n                                ),\n                            },\n                        },\n                    },\n                    validate.get((\"data\", \"versions\")),\n                ),\n            )\n        except PluginError:\n            log.error(\"Failed to get streams from API\")\n            return\n\n        for stream in itertools.chain(*data.values()):\n            src = update_scheme(\"https://\", stream[\"src\"], force=False)\n            yield from HLSStream.parse_variant_playlist(self.session, src).items()\n", "type": "function"}, {"name": "mediaselector", "is_method": true, "class_name": "BBCiPlayer", "parameters": ["self", "vpid"], "calls": ["defaultdict", "urls.items", "self.api_url.format", "log.debug", "self.session.http.get", "log.debug", "list", "self._hash_vpid", "add", "log.debug", "len", "log.debug", "items", "items", "connection.get", "HLSStream.parse_variant_playlist", "DASHStream.parse_manifest"], "code_location": {"file": "bbciplayer.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 135, "end_line": 155}, "code_snippet": "    def mediaselector(self, vpid):\n        urls = defaultdict(set)\n        for platform in self.platforms:\n            url = self.api_url.format(vpid=vpid, vpid_hash=self._hash_vpid(vpid), platform=platform)\n            log.debug(f\"Info API request: {url}\")\n            medias = self.session.http.get(url, schema=self.mediaselector_schema)\n            for media in medias:\n                for connection in media[\"connection\"]:\n                    urls[connection.get(\"transferFormat\")].add(connection[\"href\"])\n\n        for stream_type, urlitems in urls.items():\n            log.debug(f\"{len(urlitems)} {stream_type} streams\")\n            for url in list(urlitems):\n                try:\n                    if stream_type == \"hls\":\n                        yield from HLSStream.parse_variant_playlist(self.session, url).items()\n                    if stream_type == \"dash\":\n                        yield from DASHStream.parse_manifest(self.session, url).items()\n                    log.debug(f\"  OK:   {url}\")\n                except Exception:\n                    log.debug(f\"  FAIL: {url}\")\n", "type": "function"}, {"name": "test_plugin_api", "is_method": true, "class_name": "TestPlugins", "parameters": ["self", "plugin"], "calls": ["callable", "hasattr", "hasattr"], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 88, "end_line": 92}, "code_snippet": "    def test_plugin_api(self, plugin):\n        pluginclass = plugin.__plugin__\n        assert not hasattr(pluginclass, \"can_handle_url\"), \"Does not implement deprecated can_handle_url(url)\"\n        assert not hasattr(pluginclass, \"priority\"), \"Does not implement deprecated priority(url)\"\n        assert callable(pluginclass._get_streams), \"Implements _get_streams()\"\n", "type": "function"}, {"name": "_get_streams", "is_method": true, "class_name": "Rtve", "parameters": ["self"], "calls": ["self.session.http.get", "items", "self.session.get_option", "self.session.http.get", "next", "self.session.http.get", "validate.Schema", "self.URL_VIDEOS.format", "self.URL_M3U8.format", "next", "HLSStream.parse_variant_playlist", "self.URL_SUBTITLES.format", "validate.parse_html", "validate.xml_xpath_string", "validate.none_or_all", "validate.Schema", "validate.Schema", "HTTPStream", "validate.parse_json", "validate.get", "validate.transform", "validate.transform", "validate.length", "path.endswith", "validate.parse_json", "validate.get", "update_scheme", "validate.any", "path.endswith", "HTTPStream", "MuxedStream", "validate.all", "validate.url", "validate.transform", "urlparse", "urlparse", "validate.url"], "code_location": {"file": "rtve.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 145, "end_line": 215}, "code_snippet": "    def _get_streams(self):\n        self.id = self.session.http.get(\n            self.url,\n            schema=validate.Schema(\n                validate.parse_html(),\n                validate.xml_xpath_string(\".//*[contains(@class,'videoPlayer')][@data-setup][1]/@data-setup\"),\n                validate.none_or_all(\n                    validate.parse_json(),\n                    {\n                        \"idAsset\": validate.any(int, validate.all(str, validate.transform(int))),\n                    },\n                    validate.get(\"idAsset\"),\n                ),\n            ),\n        )\n        if not self.id:\n            return\n\n        # check obfuscated stream URLs via self.URL_VIDEOS and ZTNR.translate() first\n        # self.URL_M3U8 appears to be valid for all streams, but doesn't provide any content in some cases\n        try:\n            urls = self.session.http.get(\n                self.URL_VIDEOS.format(id=self.id),\n                schema=validate.Schema(\n                    validate.transform(ZTNR.translate),\n                    validate.transform(list),\n                    [(str, validate.url())],\n                    validate.length(1),\n                ),\n            )\n        except PluginError:\n            # catch HTTP errors and validation errors, and fall back to generic HLS URL template\n            url = self.URL_M3U8.format(id=self.id)\n        else:\n            url = next((url for _, url in urls if urlparse(url).path.endswith(\".m3u8\")), None)\n            if not url:\n                url = next((url for _, url in urls if urlparse(url).path.endswith(\".mp4\")), None)\n                if url:\n                    yield \"vod\", HTTPStream(self.session, url)\n                return\n\n        streams = HLSStream.parse_variant_playlist(self.session, url).items()\n\n        if self.session.get_option(\"mux-subtitles\"):\n            subs = self.session.http.get(\n                self.URL_SUBTITLES.format(id=self.id),\n                schema=validate.Schema(\n                    validate.parse_json(),\n                    {\n                        \"page\": {\n                            \"items\": [\n                                {\n                                    \"lang\": str,\n                                    \"src\": validate.url(),\n                                },\n                            ],\n                        },\n                    },\n                    validate.get((\"page\", \"items\")),\n                ),\n            )\n            if subs:\n                subtitles = {\n                    s[\"lang\"]: HTTPStream(self.session, update_scheme(\"https://\", s[\"src\"], force=True))\n                    for s in subs\n                }  # fmt: skip\n                for quality, stream in streams:\n                    yield quality, MuxedStream(self.session, stream, subtitles=subtitles)\n                return\n\n        yield from streams\n", "type": "function"}, {"name": "_get_stream_data", "is_method": true, "class_name": "FilmOnHLS", "parameters": ["self"], "calls": ["log.debug", "self.api.channel", "log.debug", "self.api.vod"], "code_location": {"file": "filmon.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 65, "end_line": 71}, "code_snippet": "    def _get_stream_data(self) -> Iterator[tuple[str, str, int]]:\n        if self.channel:\n            log.debug(f\"Reloading FilmOn channel playlist: {self.channel}\")\n            yield from self.api.channel(self.channel)\n        elif self.vod_id:\n            log.debug(f\"Reloading FilmOn VOD playlist: {self.vod_id}\")\n            yield from self.api.vod(self.vod_id)\n", "type": "function"}, {"name": "_get_streams", "is_method": true, "class_name": "YouTube", "parameters": ["self"], "calls": ["self._get_res", "self._get_data_from_regex", "self._schema_videodetails", "log.debug", "self._schema_streamingdata", "any", "self._get_data_from_regex", "self._data_video_id", "self._url_canonical.format", "self._get_res", "self._data_status", "self._get_data_from_api", "log.debug", "log.debug", "HTTPStream", "streams.update", "streams.update", "log.error", "self._data_status", "self._create_adaptive_streams", "HLSStream.parse_variant_playlist", "PluginError", "PluginError", "self.session.http.head"], "code_location": {"file": "youtube.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 385, "end_line": 435}, "code_snippet": "    def _get_streams(self):\n        res = self._get_res(self.url)\n\n        if self.matches[\"channel\"] and not self.match[\"live\"]:\n            initial = self._get_data_from_regex(res, self._re_ytInitialData, \"initial data\")\n            video_id = self._data_video_id(initial)\n            if video_id is None:\n                log.error(\"Could not find videoId on channel page\")\n                return\n            self.url = self._url_canonical.format(video_id=video_id)\n            res = self._get_res(self.url)\n\n        data = self._get_data_from_regex(res, self._re_ytInitialPlayerResponse, \"initial player response\")\n        if not self._data_status(data):\n            data = self._get_data_from_api(res)\n            if not self._data_status(data, True):\n                return\n\n        self.id, self.author, self.category, self.title, is_live = self._schema_videodetails(data)\n        log.debug(f\"Using video ID: {self.id}\")\n\n        if is_live:\n            log.debug(\"This video is live.\")\n\n        streams = {}\n        hls_manifest, formats, adaptive_formats = self._schema_streamingdata(data)\n\n        protected = any(url is None for url, *_ in formats + adaptive_formats)\n        if protected:\n            log.debug(\"This video may be protected.\")\n\n        for url, label in formats:\n            if url is None:\n                continue\n            if self.session.http.head(url, raise_for_status=False).status_code >= 400:\n                break\n            streams[label] = HTTPStream(self.session, url)\n\n        if not is_live:\n            streams.update(self._create_adaptive_streams(adaptive_formats))\n\n        if hls_manifest:\n            streams.update(HLSStream.parse_variant_playlist(self.session, hls_manifest, name_key=\"pixels\"))\n\n        if not streams:\n            if protected:\n                raise PluginError(\"This plugin does not support protected videos, try yt-dlp instead\")\n            if formats or adaptive_formats:\n                raise PluginError(\"This plugin does not support VOD content, try yt-dlp instead\")\n\n        return streams\n", "type": "function"}, {"name": "_get_streams", "is_method": true, "class_name": "Filmon", "parameters": ["self"], "calls": ["self.match.group", "self.match.group", "self.match.group", "self.session.http.get", "self.api.vod", "url.endswith", "channel.isdigit", "self.cache.get", "PluginError", "self.api.channel", "HLSStream.parse_variant_playlist", "url.endswith", "log.debug", "self.session.http.get", "log.debug", "self.cache.set", "self.cache.set", "log.debug", "streams.items", "HLSStream", "validate.Schema", "FilmOnHLS", "channel.isdigit", "HTTPStream", "re.compile", "validate.any", "validate.get"], "code_location": {"file": "filmon.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 207, "end_line": 255}, "code_snippet": "    def _get_streams(self):\n        channel = self.match.group(\"channel\")\n        vod_id = self.match.group(\"vod_id\")\n        is_group = self.match.group(\"is_group\")\n\n        # get cookies\n        self.session.http.get(self.url)\n\n        if vod_id:\n            for quality, url, _timeout in self.api.vod(vod_id):\n                if url.endswith(\".m3u8\"):\n                    streams = HLSStream.parse_variant_playlist(self.session, url)\n                    if streams:\n                        yield from streams.items()\n                        return\n                    yield quality, HLSStream(self.session, url)\n                elif url.endswith(\".mp4\"):\n                    yield quality, HTTPStream(self.session, url)\n        else:\n            if not channel or channel.isdigit():\n                id_ = channel\n            else:\n                id_ = self.cache.get(channel)\n                if id_ is not None:\n                    log.debug(f\"Found cached channel ID: {id_}\")\n                else:\n                    id_ = self.session.http.get(\n                        self.url,\n                        schema=validate.Schema(\n                            re.compile(r\"\"\"channel_id\\s*=\\s*(?P<q>['\"]?)(?P<value>\\d+)(?P=q)\"\"\"),\n                            validate.any(None, validate.get(\"value\")),\n                        ),\n                    )\n                    log.debug(f\"Found channel ID: {id_}\")\n                    # do not cache a group url\n                    if id_ and not is_group:\n                        self.cache.set(channel, id_, expires=self.TIME_CHANNEL)\n\n            if id_ is None:\n                raise PluginError(f\"Unable to find channel ID: {channel}\")\n\n            try:\n                for quality, url, _timeout in self.api.channel(id_):\n                    yield quality, FilmOnHLS(self.session, url, self.api, channel=id_, quality=quality)\n            except Exception:\n                if channel and not channel.isdigit():\n                    self.cache.set(channel, None, expires=0)\n                    log.debug(f\"Reset cached channel: {channel}\")\n                raise\n", "type": "function"}, {"name": "_handle_module_info_stream", "is_method": true, "class_name": "UStreamTVWsClient", "parameters": ["self", "data"], "calls": ["get", "sorted", "len", "datetime.now", "self._set_ready", "data.get", "self._set_error", "list", "list", "self._schema_stream_segments.validate", "hashes.keys", "enumerate", "data.get", "self._schema_stream_formats.validate", "filter", "filter", "log.error", "range", "self._set_error", "self._segments_append", "UStreamTVSegment", "type", "type", "timedelta"], "code_location": {"file": "ustreamtv.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 271, "end_line": 334}, "code_snippet": "    def _handle_module_info_stream(self, data: dict):\n        if data.get(\"contentAvailable\") is False:\n            return self._set_error(\"This stream is currently offline\")\n\n        mp4_segmented = data.get(\"streamFormats\", {}).get(\"mp4/segmented\")\n        if not mp4_segmented:\n            return\n\n        # parse the stream formats once\n        if self.stream_initial_id is None:\n            try:\n                formats = self._schema_stream_formats.validate(mp4_segmented)\n                formats = formats[\"streams\"]\n            except PluginError as err:\n                return self._set_error(err)\n            self.stream_formats_video = list(filter(lambda f: type(f) is StreamFormatVideo, formats))\n            self.stream_formats_audio = list(filter(lambda f: type(f) is StreamFormatAudio, formats))\n\n        # parse segment duration and hashes, and queue new segments\n        try:\n            segmentdata: dict = self._schema_stream_segments.validate(mp4_segmented)\n        except PluginError:\n            log.error(\"Failed parsing hashes\")\n            return\n\n        current_id: int = segmentdata[\"chunkId\"]\n        duration: int = segmentdata[\"chunkTime\"]\n        path: str = segmentdata[\"contentAccess\"]\n        hashes: dict[int, str] = segmentdata[\"hashes\"]\n\n        sorted_ids = sorted(hashes.keys())\n        count = len(sorted_ids)\n        if count == 0:\n            return\n\n        # initial segment ID (needed by the workers to filter queued segments)\n        if self.stream_initial_id is None:\n            self.stream_initial_id = current_id\n\n        current_time = datetime.now(timezone.utc)\n\n        # lock the stream segments deques for the worker threads\n        with self.stream_segments_lock:\n            # interpolate and extrapolate segments from the provided id->hash data\n            diff = 10 - sorted_ids[0] % 10  # if there's only one id->hash item, extrapolate until the next decimal\n            for idx, segment_id in enumerate(sorted_ids):\n                idx_next = idx + 1\n                if idx_next < count:\n                    # calculate the difference between IDs and use that to interpolate segment IDs\n                    # the last id->hash item will use the previous diff to extrapolate segment IDs\n                    diff = sorted_ids[idx_next] - segment_id\n                for num in range(segment_id, segment_id + diff):\n                    self._segments_append(\n                        UStreamTVSegment(\n                            uri=\"\",\n                            num=num,\n                            duration=duration,\n                            available_at=current_time + timedelta(seconds=(num - current_id - 1) * duration / 1000),\n                            hash=hashes[segment_id],\n                            path=path,\n                        ),\n                    )\n\n        self._set_ready()\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0456395149230957}
{"question": "What is the conceptual relationship between the directional force measurement property and the spatial orientation angle properties in the touch interaction point data structure in defining the physical orientation and force characteristics of a touch interaction?", "answer": "", "relative_code_list": null, "ground_truth": "The tangential pressure and tilt properties in the TouchPoint class work together to define the complete physical characteristics of a touch interaction. Tangential pressure represents the normalized pressure applied parallel to the touch surface with a range of [-1,1], indicating directional force application. The tilt_x and tilt_y properties define the spatial orientation of the stylus or touch implement: tilt_x measures the angle between the Y-Z plane and the plane containing both the stylus axis and Y axis (positive tiltX is to the right), while tilt_y measures the angle between the X-Z plane and the plane containing both the stylus axis and X axis (positive tiltY is towards the user). Collectively, these properties model the three-dimensional force vector and spatial orientation of the touch interaction, enabling precise representation of how a user applies pressure and angles their touch implement on the surface, which is crucial for advanced touch and stylus input processing in graphical applications.", "score": null, "retrieved_content": [{"name": "TouchPoint", "docstring": "", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "input_.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 20, "end_line": 93}, "type": "class"}, {"name": "from_json", "is_method": true, "class_name": "TouchPoint", "parameters": ["cls", "json"], "calls": ["cls", "float", "float", "float", "float", "float", "float", "float", "float", "float", "int", "float"], "code_location": {"file": "input_.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 80, "end_line": 93}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> TouchPoint:\n        return cls(\n            x=float(json[\"x\"]),\n            y=float(json[\"y\"]),\n            radius_x=float(json[\"radiusX\"]) if \"radiusX\" in json else None,\n            radius_y=float(json[\"radiusY\"]) if \"radiusY\" in json else None,\n            rotation_angle=float(json[\"rotationAngle\"]) if \"rotationAngle\" in json else None,\n            force=float(json[\"force\"]) if \"force\" in json else None,\n            tangential_pressure=float(json[\"tangentialPressure\"]) if \"tangentialPressure\" in json else None,\n            tilt_x=float(json[\"tiltX\"]) if \"tiltX\" in json else None,\n            tilt_y=float(json[\"tiltY\"]) if \"tiltY\" in json else None,\n            twist=int(json[\"twist\"]) if \"twist\" in json else None,\n            id_=float(json[\"id\"]) if \"id\" in json else None,\n        )\n", "type": "function"}, {"name": "ScrollOrientation", "docstring": "Physical scroll orientation", "methods": ["to_json", "from_json"], "attributes": ["HORIZONTAL", "VERTICAL"], "code_location": {"file": "dom.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 197, "end_line": 209}, "type": "class"}, {"name": "from_json", "is_method": true, "class_name": "ScreenOrientation", "parameters": ["cls", "json"], "calls": ["cls", "str", "int"], "code_location": {"file": "emulation.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 100, "end_line": 104}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> ScreenOrientation:\n        return cls(\n            type_=str(json[\"type\"]),\n            angle=int(json[\"angle\"]),\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "SensorReadingQuaternion", "parameters": ["cls", "json"], "calls": ["cls", "float", "float", "float", "float"], "code_location": {"file": "emulation.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 394, "end_line": 400}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> SensorReadingQuaternion:\n        return cls(\n            x=float(json[\"x\"]),\n            y=float(json[\"y\"]),\n            z=float(json[\"z\"]),\n            w=float(json[\"w\"]),\n        )\n", "type": "function"}, {"name": "SensorReadingQuaternion", "docstring": "", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "emulation.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 376, "end_line": 400}, "type": "class"}, {"name": "PressureMetadata", "docstring": "", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "emulation.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 456, "end_line": 469}, "type": "class"}, {"name": "from_json", "is_method": true, "class_name": "SensorReading", "parameters": ["cls", "json"], "calls": ["cls", "SensorReadingSingle.from_json", "SensorReadingXYZ.from_json", "SensorReadingQuaternion.from_json"], "code_location": {"file": "emulation.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 422, "end_line": 427}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> SensorReading:\n        return cls(\n            single=SensorReadingSingle.from_json(json[\"single\"]) if \"single\" in json else None,\n            xyz=SensorReadingXYZ.from_json(json[\"xyz\"]) if \"xyz\" in json else None,\n            quaternion=SensorReadingQuaternion.from_json(json[\"quaternion\"]) if \"quaternion\" in json else None,\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "SensorReadingXYZ", "parameters": ["cls", "json"], "calls": ["cls", "float", "float", "float"], "code_location": {"file": "emulation.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 367, "end_line": 372}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> SensorReadingXYZ:\n        return cls(\n            x=float(json[\"x\"]),\n            y=float(json[\"y\"]),\n            z=float(json[\"z\"]),\n        )\n", "type": "function"}, {"name": "PhysicalAxes", "docstring": "ContainerSelector physical axes", "methods": ["to_json", "from_json"], "attributes": ["HORIZONTAL", "VERTICAL", "BOTH"], "code_location": {"file": "dom.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 165, "end_line": 178}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.03755784034729}
{"question": "What is the impact of the JSON-to-object deserialization method in the WebSocket handshake initiation event class's dependency on multiple deserialization methods across different data type classes on the maintainability and evolution of the browser debugging protocol's deserialization architecture?", "answer": "", "relative_code_list": null, "ground_truth": "The WebSocketWillSendHandshakeRequest.from_json method depends on from_json implementations from RequestId, MonotonicTime, TimeSinceEpoch, and WebSocketRequest classes, creating a distributed dependency network. This architecture requires coordinated changes across multiple classes when modifying the JSON structure or deserialization logic, increasing maintenance complexity. However, it promotes separation of concerns by delegating type-specific parsing to respective classes. The dependency chain creates coupling between WebSocketWillSendHandshakeRequest and these classes, meaning any changes to their from_json signatures or behavior could break WebSocket handshake request deserialization. This design pattern supports protocol evolution but requires careful version management and testing across all dependent deserialization methods.", "score": null, "retrieved_content": [{"name": "from_json", "is_method": true, "class_name": "WebSocketWillSendHandshakeRequest", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "MonotonicTime.from_json", "TimeSinceEpoch.from_json", "WebSocketRequest.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3888, "end_line": 3894}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> WebSocketWillSendHandshakeRequest:\n        return cls(\n            request_id=RequestId.from_json(json[\"requestId\"]),\n            timestamp=MonotonicTime.from_json(json[\"timestamp\"]),\n            wall_time=TimeSinceEpoch.from_json(json[\"wallTime\"]),\n            request=WebSocketRequest.from_json(json[\"request\"]),\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "WebSocketHandshakeResponseReceived", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "MonotonicTime.from_json", "WebSocketResponse.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3864, "end_line": 3869}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> WebSocketHandshakeResponseReceived:\n        return cls(\n            request_id=RequestId.from_json(json[\"requestId\"]),\n            timestamp=MonotonicTime.from_json(json[\"timestamp\"]),\n            response=WebSocketResponse.from_json(json[\"response\"]),\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "WebSocketCreated", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "str", "Initiator.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3776, "end_line": 3781}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> WebSocketCreated:\n        return cls(\n            request_id=RequestId.from_json(json[\"requestId\"]),\n            url=str(json[\"url\"]),\n            initiator=Initiator.from_json(json[\"initiator\"]) if \"initiator\" in json else None,\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "WebSocketClosed", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "MonotonicTime.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3755, "end_line": 3759}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> WebSocketClosed:\n        return cls(\n            request_id=RequestId.from_json(json[\"requestId\"]),\n            timestamp=MonotonicTime.from_json(json[\"timestamp\"]),\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "WebSocketFrameReceived", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "MonotonicTime.from_json", "WebSocketFrame.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3820, "end_line": 3825}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> WebSocketFrameReceived:\n        return cls(\n            request_id=RequestId.from_json(json[\"requestId\"]),\n            timestamp=MonotonicTime.from_json(json[\"timestamp\"]),\n            response=WebSocketFrame.from_json(json[\"response\"]),\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "WebTransportCreated", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "str", "MonotonicTime.from_json", "Initiator.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3913, "end_line": 3919}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> WebTransportCreated:\n        return cls(\n            transport_id=RequestId.from_json(json[\"transportId\"]),\n            url=str(json[\"url\"]),\n            timestamp=MonotonicTime.from_json(json[\"timestamp\"]),\n            initiator=Initiator.from_json(json[\"initiator\"]) if \"initiator\" in json else None,\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "WebSocketResponse", "parameters": ["cls", "json"], "calls": ["cls", "int", "str", "Headers.from_json", "str", "Headers.from_json", "str"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 1156, "end_line": 1164}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> WebSocketResponse:\n        return cls(\n            status=int(json[\"status\"]),\n            status_text=str(json[\"statusText\"]),\n            headers=Headers.from_json(json[\"headers\"]),\n            headers_text=str(json[\"headersText\"]) if \"headersText\" in json else None,\n            request_headers=Headers.from_json(json[\"requestHeaders\"]) if \"requestHeaders\" in json else None,\n            request_headers_text=str(json[\"requestHeadersText\"]) if \"requestHeadersText\" in json else None,\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "WebSocketFrameSent", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "MonotonicTime.from_json", "WebSocketFrame.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3842, "end_line": 3847}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> WebSocketFrameSent:\n        return cls(\n            request_id=RequestId.from_json(json[\"requestId\"]),\n            timestamp=MonotonicTime.from_json(json[\"timestamp\"]),\n            response=WebSocketFrame.from_json(json[\"response\"]),\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "SubresourceWebBundleMetadataReceived", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "str"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 4420, "end_line": 4424}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> SubresourceWebBundleMetadataReceived:\n        return cls(\n            request_id=RequestId.from_json(json[\"requestId\"]),\n            urls=[str(i) for i in json[\"urls\"]],\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "WebTransportConnectionEstablished", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "MonotonicTime.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3934, "end_line": 3938}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> WebTransportConnectionEstablished:\n        return cls(\n            transport_id=RequestId.from_json(json[\"transportId\"]),\n            timestamp=MonotonicTime.from_json(json[\"timestamp\"]),\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0330748558044434}
{"question": "What architectural role does the event class that handles WebSocket message errors play in the browser protocol event handling module's error handling strategy?", "answer": "", "relative_code_list": null, "ground_truth": "The WebSocketFrameError class serves as a critical architectural component in the CDP devtools module's error handling infrastructure, acting as a domain-specific data transfer object that bridges the gap between raw JSON data from the Chrome DevTools Protocol and the application's internal object model. Its from_json method implements a factory pattern that performs multi-layer data transformation: it converts the raw JSON requestId into a RequestId domain object using its own from_json method, transforms the timestamp into a MonotonicTime object through similar deserialization, and casts the errorMessage to a native string. This architectural approach ensures type safety, maintains separation of concerns between data serialization and business logic, and enables consistent error object creation across different layers of the application while preserving the module's testability and modularity principles.", "score": null, "retrieved_content": [{"name": "WebSocketFrameError", "docstring": "Fired when WebSocket message error occurs.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3786, "end_line": 3803}, "type": "class"}, {"name": "WebSocketClosed", "docstring": "Fired when WebSocket is closed.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3745, "end_line": 3759}, "type": "class"}, {"name": "WebSocketFrameReceived", "docstring": "Fired when WebSocket message is received.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3808, "end_line": 3825}, "type": "class"}, {"name": "EventSourceMessageReceived", "docstring": "Fired when EventSource message is received.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3454, "end_line": 3477}, "type": "class"}, {"name": "WebbrowserError", "docstring": "", "methods": [], "attributes": [], "code_location": {"file": "exceptions.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser", "start_line": 4, "end_line": 5}, "type": "class"}, {"name": "WebSocketCreated", "docstring": "Fired upon WebSocket creation.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3764, "end_line": 3781}, "type": "class"}, {"name": "WebSocketFrame", "docstring": "WebSocket message data. This represents an entire WebSocket message, not just a fragmented frame as the name suggests.", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 1168, "end_line": 1196}, "type": "class"}, {"name": "SubresourceWebBundleMetadataError", "docstring": "**EXPERIMENTAL**\n\nFired once when parsing the .wbn file has failed.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 4429, "end_line": 4445}, "type": "class"}, {"name": "WebSocketResponse", "docstring": "WebSocket response data.", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 1120, "end_line": 1164}, "type": "class"}, {"name": "WebSocketFrameSent", "docstring": "Fired when WebSocket message is sent.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3830, "end_line": 3847}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.0580792427062988}
{"question": "What architectural role does the prefetch metadata handler class play in the adaptive streaming protocol module's layered design?", "answer": "", "relative_code_list": null, "ground_truth": "The SegmentPrefetch class serves as a specialized implementation within the HLS streaming module's presentation layer, extending the base Segment class to handle Twitch-specific prefetch metadata tags. It operates at the protocol abstraction level, translating segment URL information into the #EXT-X-TWITCH-PREFETCH format required by Twitch's HLS implementation, demonstrating a layered architecture where specialized subclasses handle platform-specific protocol extensions while maintaining interface consistency with the core HLS segment handling infrastructure.", "score": null, "retrieved_content": [{"name": "SegmentPrefetch", "docstring": "", "methods": [], "attributes": [], "code_location": {"file": "test_kick.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 42, "end_line": 44}, "type": "class"}, {"name": "SegmentPrefetch", "docstring": "", "methods": ["build", "build"], "attributes": [], "code_location": {"file": "test_twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 161, "end_line": 163}, "type": "class"}, {"name": "test_hls_no_low_latency_has_prefetch", "is_method": true, "class_name": "TestTwitchHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.await_write", "self.await_read", "all", "self.session.options.get", "self.session.options.get", "self.content", "any", "Playlist", "Playlist", "self.called", "call", "segments.values", "self.called", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "segments.values"], "code_location": {"file": "test_twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 395, "end_line": 415}, "code_snippet": "    def test_hls_no_low_latency_has_prefetch(self, mock_log):\n        segments = self.subject(\n            [\n                Playlist(0, [Segment(0), Segment(1), Segment(2), Segment(3), SegmentPrefetch(4), SegmentPrefetch(5)]),\n                Playlist(4, [Segment(4), Segment(5), Segment(6), Segment(7), SegmentPrefetch(8), SegmentPrefetch(9)], end=True),\n            ],\n            streamoptions={\"low_latency\": False},\n        )\n\n        assert self.session.options.get(\"hls-live-edge\") == 4\n        assert not self.session.options.get(\"hls-segment-stream-data\")\n\n        self.await_write(8)\n        data = self.await_read(read_all=True)\n        assert data == self.content(segments, cond=lambda s: s.num < 8), \"Ignores prefetch segments\"\n        assert all(self.called(s) for s in segments.values() if s.num <= 7), \"Ignores prefetch segments\"\n        assert not any(self.called(s) for s in segments.values() if s.num > 7), \"Ignores prefetch segments\"\n        assert mock_log.info.mock_calls == [\n            call(\"Will skip ad segments\"),\n        ]\n        assert self.thread.reader.worker._reload_time == 3.0\n", "type": "function"}, {"name": "M3U8ParserMeta", "docstring": "", "methods": ["__init__"], "attributes": [], "code_location": {"file": "m3u8.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/hls", "start_line": 103, "end_line": 113}, "type": "class"}, {"name": "test_hls_no_low_latency_has_prefetch", "is_method": true, "class_name": "TestKickHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.await_write", "self.await_read", "all", "self.session.options.get", "self.session.options.get", "self.content", "any", "Playlist", "Playlist", "self.called", "segments.values", "self.called", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "segments.values"], "code_location": {"file": "test_kick.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 93, "end_line": 111}, "code_snippet": "    def test_hls_no_low_latency_has_prefetch(self, mock_log):\n        segments = self.subject(\n            [\n                Playlist(0, [Segment(0), Segment(1), Segment(2), Segment(3), SegmentPrefetch(4), SegmentPrefetch(5)]),\n                Playlist(4, [Segment(4), Segment(5), Segment(6), Segment(7), SegmentPrefetch(8), SegmentPrefetch(9)], end=True),\n            ],\n            streamoptions={\"low_latency\": False},\n        )\n\n        assert self.session.options.get(\"hls-live-edge\") == 4\n        assert not self.session.options.get(\"hls-segment-stream-data\")\n\n        self.await_write(8)\n        data = self.await_read(read_all=True)\n        assert data == self.content(segments, cond=lambda s: s.num < 8), \"Ignores prefetch segments\"\n        assert all(self.called(s) for s in segments.values() if s.num <= 7), \"Ignores prefetch segments\"\n        assert not any(self.called(s) for s in segments.values() if s.num > 7), \"Ignores prefetch segments\"\n        assert mock_log.info.mock_calls == []\n        assert self.thread.reader.worker._reload_time == 3.0\n", "type": "function"}, {"name": "test_hls_low_latency_has_prefetch_no_preroll_with_prefetch_ads", "is_method": true, "class_name": "TestTwitchHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "Tag", "self.subject", "self.await_write", "self.await_read", "Tag", "TagDateRangeAd", "self.content", "Playlist", "Playlist", "Playlist", "Playlist", "Playlist", "Playlist", "Playlist", "Playlist", "call", "call", "call", "timedelta", "Seg", "Seg", "Pre", "Pre", "Seg", "Pre", "Pre", "Pre", "Seg", "Seg", "Seg", "Seg", "Seg", "Seg", "Seg", "Seg", "Seg", "Seg", "Seg", "Seg", "Pre", "Pre", "Seg", "Seg", "Seg", "Seg", "Pre", "Pre", "Seg", "Seg", "Seg", "Seg", "Pre", "Pre", "Seg", "Seg", "Seg", "Seg", "Pre", "Pre"], "code_location": {"file": "test_twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 461, "end_line": 501}, "code_snippet": "    def test_hls_low_latency_has_prefetch_no_preroll_with_prefetch_ads(self, mock_log):\n        # segment 1 has a shorter duration, to mess with the extrapolation of the prefetch start times\n        # segments 3-6 are ads\n        Seg, Pre = Segment, SegmentPrefetch\n        ads = [\n            Tag(\"EXT-X-DISCONTINUITY\"),\n            TagDateRangeAd(\n                start=DATETIME_BASE + timedelta(seconds=3),\n                duration=4,\n                custom={\"X-TV-TWITCH-AD-ROLL-TYPE\": \"MIDROLL\"},\n            ),\n        ]\n        tls = Tag(\"EXT-X-TWITCH-LIVE-SEQUENCE\", 7)\n        # noinspection PyTypeChecker\n        segments = self.subject(\n            [\n                # regular stream data with prefetch segments\n                Playlist(0, [Seg(0), Seg(1, duration=0.5), Pre(2), Pre(3)]),\n                # three prefetch segments, one regular (2) and two ads (3 and 4)\n                Playlist(1, [Seg(1, duration=0.5), Pre(2), *ads, Pre(3), Pre(4)]),\n                # all prefetch segments are gone once regular prefetch segments have shifted\n                Playlist(2, [Seg(2, duration=1.5), *ads, Seg(3), Seg(4), Seg(5)]),\n                # still no prefetch segments while ads are playing\n                Playlist(3, [*ads, Seg(3), Seg(4), Seg(5), Seg(6)]),\n                # new prefetch segments on the first regular segment occurrence\n                Playlist(4, [*ads, Seg(4), Seg(5), Seg(6), tls, Seg(7), Pre(8), Pre(9)]),\n                Playlist(5, [*ads, Seg(5), Seg(6), tls, Seg(7), Seg(8), Pre(9), Pre(10)]),\n                Playlist(6, [*ads, Seg(6), tls, Seg(7), Seg(8), Seg(9), Pre(10), Pre(11)]),\n                Playlist(7, [Seg(7), Seg(8), Seg(9), Seg(10), Pre(11), Pre(12)], end=True),\n            ],\n            streamoptions={\"low_latency\": True},\n        )\n\n        self.await_write(11)\n        data = self.await_read(read_all=True)\n        assert data == self.content(segments, cond=lambda s: 2 <= s.num <= 3 or 7 <= s.num)\n        assert mock_log.info.mock_calls == [\n            call(\"Will skip ad segments\"),\n            call(\"Low latency streaming (HLS live edge: 2)\"),\n            call(\"Detected advertisement break of 4 seconds\"),\n        ]\n", "type": "function"}, {"name": "test_hls_low_latency_has_prefetch", "is_method": true, "class_name": "TestTwitchHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.session.options.get", "self.await_write", "self.await_read", "all", "self.session.options.get", "self.content", "any", "Playlist", "Playlist", "self.called", "call", "call", "self.called", "segments.values", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "segments.values"], "code_location": {"file": "test_twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 372, "end_line": 392}, "code_snippet": "    def test_hls_low_latency_has_prefetch(self, mock_log):\n        segments = self.subject(\n            [\n                Playlist(0, [Segment(0), Segment(1), Segment(2), Segment(3), SegmentPrefetch(4), SegmentPrefetch(5)]),\n                Playlist(4, [Segment(4), Segment(5), Segment(6), Segment(7), SegmentPrefetch(8), SegmentPrefetch(9)], end=True),\n            ],\n            streamoptions={\"low_latency\": True},\n        )\n\n        assert self.session.options.get(\"hls-live-edge\") == 2\n        assert self.session.options.get(\"hls-segment-stream-data\")\n\n        self.await_write(6)\n        data = self.await_read(read_all=True)\n        assert data == self.content(segments, cond=lambda s: s.num >= 4), \"Skips first four segments due to reduced live-edge\"\n        assert not any(self.called(s) for s in segments.values() if s.num < 4), \"Doesn't download old segments\"\n        assert all(self.called(s) for s in segments.values() if s.num >= 4), \"Downloads all remaining segments\"\n        assert mock_log.info.mock_calls == [\n            call(\"Will skip ad segments\"),\n            call(\"Low latency streaming (HLS live edge: 2)\"),\n        ]\n", "type": "function"}, {"name": "DASHStream", "docstring": "Implementation of the \"Dynamic Adaptive Streaming over HTTP\" protocol (MPEG-DASH)", "methods": ["__init__", "__json__", "to_url", "fetch_manifest", "parse_mpd", "parse_manifest", "open"], "attributes": ["__shortname__"], "code_location": {"file": "dash.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/dash", "start_line": 199, "end_line": 422}, "type": "class"}, {"name": "test_hls_low_latency_has_prefetch", "is_method": true, "class_name": "TestKickHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.session.options.get", "self.await_write", "self.await_read", "all", "self.session.options.get", "self.content", "any", "Playlist", "Playlist", "self.called", "call", "self.called", "segments.values", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "segments.values"], "code_location": {"file": "test_kick.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 71, "end_line": 90}, "code_snippet": "    def test_hls_low_latency_has_prefetch(self, mock_log):\n        segments = self.subject(\n            [\n                Playlist(0, [Segment(0), Segment(1), Segment(2), Segment(3), SegmentPrefetch(4), SegmentPrefetch(5)]),\n                Playlist(4, [Segment(4), Segment(5), Segment(6), Segment(7), SegmentPrefetch(8), SegmentPrefetch(9)], end=True),\n            ],\n            streamoptions={\"low_latency\": True},\n        )\n\n        assert self.session.options.get(\"hls-live-edge\") == 2\n        assert self.session.options.get(\"hls-segment-stream-data\")\n\n        self.await_write(6)\n        data = self.await_read(read_all=True)\n        assert data == self.content(segments, cond=lambda s: s.num >= 4), \"Skips first four segments due to reduced live-edge\"\n        assert not any(self.called(s) for s in segments.values() if s.num < 4), \"Doesn't download old segments\"\n        assert all(self.called(s) for s in segments.values() if s.num >= 4), \"Downloads all remaining segments\"\n        assert mock_log.info.mock_calls == [\n            call(\"Low latency streaming (HLS live edge: 2)\"),\n        ]\n", "type": "function"}, {"name": "test_hls_low_latency_has_prefetch_has_preroll", "is_method": true, "class_name": "TestTwitchHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "TagDateRangeAd", "self.subject", "self.await_write", "self.await_read", "Playlist", "Playlist", "call", "call", "call", "call", "Segment", "Segment", "Segment", "Segment", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch"], "code_location": {"file": "test_twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 438, "end_line": 458}, "code_snippet": "    def test_hls_low_latency_has_prefetch_has_preroll(self, mock_log):\n        daterange = TagDateRangeAd(\n            duration=4,\n            custom={\"X-TV-TWITCH-AD-ROLL-TYPE\": \"PREROLL\"},\n        )\n        self.subject(\n            [\n                Playlist(0, [daterange, Segment(0), Segment(1), Segment(2), Segment(3)]),\n                Playlist(4, [Segment(4), Segment(5), Segment(6), Segment(7), SegmentPrefetch(8), SegmentPrefetch(9)], end=True),\n            ],\n            streamoptions={\"low_latency\": True},\n        )\n\n        self.await_write(8)\n        self.await_read(read_all=True)\n        assert mock_log.info.mock_calls == [\n            call(\"Will skip ad segments\"),\n            call(\"Low latency streaming (HLS live edge: 2)\"),\n            call(\"Waiting for pre-roll ads to finish, be patient\"),\n            call(\"Detected advertisement break of 4 seconds\"),\n        ]\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.064821720123291}
{"question": "What is the conceptual role of the handshake mechanism in the test class for HTTP output operations' test method for write error handling regarding OSError exception handling during HTTP output operations?", "answer": "", "relative_code_list": null, "ground_truth": "The handshake mechanism in TestHTTPServer's test_writeerror method serves as a synchronization protocol between the stream runner thread and the HTTP output, ensuring controlled error handling during write operations. For acceptable OSError exceptions (EPIPE, EINVAL, ECONNRESET), the handshake allows graceful termination with informational logging about connection closure. For non-acceptable errors, the handshake facilitates proper exception propagation and error logging while maintaining thread synchronization until termination, demonstrating the mechanism's role in distinguishing between expected network-related errors and unexpected system errors.", "score": null, "retrieved_content": [{"name": "test_writeerror", "is_method": true, "class_name": "TestHTTPServer", "parameters": ["self", "caplog", "runnerthread", "stream_runner", "stream", "output", "logs", "writeerror"], "calls": ["pytest.mark.parametrize", "runnerthread.start", "assert_thread_termination", "patch.object", "output.handshake.step", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "OSError", "OSError", "OSError", "OSError", "OSError"], "code_location": {"file": "test_streamrunner.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli", "start_line": 534, "end_line": 556}, "code_snippet": "    def test_writeerror(\n        self,\n        caplog: pytest.LogCaptureFixture,\n        runnerthread: Thread,\n        stream_runner: FakeStreamRunner,\n        stream: FakeStream,\n        output: FakePlayerOutput,\n        logs: bool,\n        writeerror: Exception,\n    ):\n        runnerthread.start()\n\n        with patch.object(output, \"_write\", side_effect=writeerror):\n            assert output.handshake.step(TIMEOUT_AWAIT_HANDSHAKE)\n            assert output.data == []\n\n        # wait for runner thread to terminate first before asserting log records\n        assert_thread_termination(runnerthread, \"Runner thread has terminated\")\n        expectedlogs = (\n            ([(\"streamrunner\", \"info\", \"HTTP connection closed\")] if logs else [])\n            + [(\"streamrunner\", \"info\", \"Stream ended\")]\n        )  # fmt: skip\n        assert [(record.module, record.levelname, record.message) for record in caplog.records] == expectedlogs\n", "type": "function"}, {"name": "FakeOutput", "docstring": "Common output/http-server/progress interface, for caching all write() calls and simulating write errors", "methods": ["__init__", "write", "_write"], "attributes": [], "code_location": {"file": "test_streamrunner.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli", "start_line": 62, "end_line": 74}, "type": "class"}, {"name": "write", "is_method": true, "class_name": "EventedHLSStreamWriter", "parameters": ["self"], "calls": ["self.handshake", "self.reader.close", "write", "super"], "code_location": {"file": "stream_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/mixins", "start_line": 128, "end_line": 135}, "code_snippet": "    def write(self, *args, **kwargs):\n        # only write once per step\n        with self.handshake(Exception) as cm:\n            # don't write again during teardown\n            if not self.closed:\n                super().write(*args, **kwargs)\n        if cm.error:  # pragma: no cover\n            self.reader.close()\n", "type": "function"}, {"name": "test_early_write", "is_method": false, "class_name": null, "parameters": ["tmp_path"], "calls": ["FileOutput", "filename.exists", "pytest.raises", "fo.write"], "code_location": {"file": "test_file.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli/output", "start_line": 52, "end_line": 59}, "code_snippet": "def test_early_write(tmp_path: Path):\n    filename = tmp_path / \"foo\" / \"bar\"\n    fo = FileOutput(filename=filename)\n\n    assert not fo.opened\n    assert not filename.exists()\n    with pytest.raises(OSError, match=r\"^Output is not opened$\"):\n        fo.write(b\"foo\")\n", "type": "function"}, {"name": "test_close_error", "is_method": true, "class_name": "TestNamedPipeWindows", "parameters": ["self", "monkeypatch", "method"], "calls": ["pytest.mark.parametrize", "Mock", "Mock", "monkeypatch.setattr", "NamedPipeWindows", "pipe.open", "pytest.raises", "pipe.close", "call"], "code_location": {"file": "test_named_pipe.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/utils", "start_line": 183, "end_line": 197}, "code_snippet": "    def test_close_error(self, monkeypatch: pytest.MonkeyPatch, method: str):\n        mock_method = Mock(side_effect=OSError)\n        mock_kernel32 = Mock(**{method: mock_method})\n        monkeypatch.setattr(\"streamlink.utils.named_pipe.windll.kernel32\", mock_kernel32)\n\n        pipe = NamedPipeWindows()\n        mock_pipe = pipe.pipe\n        assert mock_pipe is not None\n        pipe.open()\n        assert mock_method.call_args_list == []\n\n        with pytest.raises(OSError):  # noqa: PT011\n            pipe.close()\n        assert mock_method.call_args_list == [call(mock_pipe)]\n        assert pipe.pipe is None\n", "type": "function"}, {"name": "write", "is_method": true, "class_name": "FakeOutput", "parameters": ["self", "data"], "calls": ["self.handshake", "self._write"], "code_location": {"file": "test_streamrunner.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli", "start_line": 69, "end_line": 71}, "code_snippet": "    def write(self, data):\n        with self.handshake():\n            return self._write(data)\n", "type": "function"}, {"name": "handshake", "is_method": true, "class_name": "TestThreadedRingBuffer", "parameters": ["self"], "calls": ["pytest.fixture", "Handshake"], "code_location": {"file": "test_buffers.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 174, "end_line": 177}, "code_snippet": "    def handshake(self):\n        handshake = Handshake()\n        yield handshake\n        assert not handshake._context.error\n", "type": "function"}, {"name": "test_close_error", "is_method": true, "class_name": "TestNamedPipePosix", "parameters": ["self", "monkeypatch"], "calls": ["Mock", "Mock", "monkeypatch.setattr", "NamedPipePosix", "pipe.path.is_fifo", "pipe.open", "Mock", "pytest.raises", "pipe.close", "pipe.path.is_fifo", "call"], "code_location": {"file": "test_named_pipe.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/utils", "start_line": 108, "end_line": 121}, "code_snippet": "    def test_close_error(self, monkeypatch: pytest.MonkeyPatch):\n        mock_fd_close = Mock(side_effect=OSError)\n        mock_fd = Mock(close=mock_fd_close)\n        monkeypatch.setattr(\"pathlib.Path.open\", Mock(return_value=mock_fd))\n\n        pipe = NamedPipePosix()\n        assert pipe.path.is_fifo()\n        pipe.open()\n        assert mock_fd_close.call_args_list == []\n\n        with pytest.raises(OSError):  # noqa: PT011\n            pipe.close()\n        assert mock_fd_close.call_args_list == [call()]\n        assert not pipe.path.is_fifo()\n", "type": "function"}, {"name": "write", "is_method": true, "class_name": "Output", "parameters": ["self", "data"], "calls": ["self._write", "OSError"], "code_location": {"file": "abc.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink_cli/output", "start_line": 18, "end_line": 22}, "code_snippet": "    def write(self, data):\n        if not self.opened:\n            raise OSError(\"Output is not opened\")\n\n        return self._write(data)\n", "type": "function"}, {"name": "Handshake", "docstring": "Control execution flow between one producer thread (application logic) and one consumer thread (tests),\nto be able to assert application state at certain points during execution.", "methods": ["__init__", "__call__", "ready", "done", "go", "wait_ready", "wait_done", "step"], "attributes": [], "code_location": {"file": "handshake.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/testutils", "start_line": 19, "end_line": 84}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.0582003593444824}
{"question": "Why does the design rationale justify the specific enumeration of cookie exemption reason values in relation to third-party cookie blocking policies?", "answer": "", "relative_code_list": null, "ground_truth": "The CookieExemptionReason enumeration is designed to capture the comprehensive set of scenarios where third-party cookie blocking (3PCD) policies would normally block a cookie but specific exemptions apply. The design rationale includes: 1) Covering user-controlled settings (USER_SETTING), 2) Accounting for metadata-based exemptions (TPCD_METADATA), 3) Supporting deprecation trial mechanisms (TPCD_DEPRECATION_TRIAL, TOP_LEVEL_TPCD_DEPRECATION_TRIAL), 4) Incorporating heuristic-based decisions (TPCD_HEURISTICS), 5) Enabling enterprise policy overrides (ENTERPRISE_POLICY), 6) Handling storage access API scenarios (STORAGE_ACCESS, TOP_LEVEL_STORAGE_ACCESS), 7) Considering scheme-specific exemptions (SCHEME), and 8) Addressing sandbox environment constraints (SAME_SITE_NONE_COOKIES_IN_SANDBOX). This comprehensive design ensures the CDP protocol can accurately represent the complex landscape of modern cookie blocking exemptions while maintaining backward compatibility and extensibility.", "score": null, "retrieved_content": [{"name": "CookieExemptionReason", "docstring": "Types of reasons why a cookie should have been blocked by 3PCD but is exempted for the request.", "methods": ["to_json", "from_json"], "attributes": ["NONE", "USER_SETTING", "TPCD_METADATA", "TPCD_DEPRECATION_TRIAL", "TOP_LEVEL_TPCD_DEPRECATION_TRIAL", "TPCD_HEURISTICS", "ENTERPRISE_POLICY", "STORAGE_ACCESS", "TOP_LEVEL_STORAGE_ACCESS", "SCHEME", "SAME_SITE_NONE_COOKIES_IN_SANDBOX"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 1482, "end_line": 1503}, "type": "class"}, {"name": "ExemptedSetCookieWithReason", "docstring": "A cookie should have been blocked by 3PCD but is exempted and stored from a response with the\ncorresponding reason. A cookie could only have at most one exemption reason.", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 1541, "end_line": 1568}, "type": "class"}, {"name": "CookieBlockedReason", "docstring": "Types of reasons why a cookie may not be sent with a request.", "methods": ["to_json", "from_json"], "attributes": ["SECURE_ONLY", "NOT_ON_PATH", "DOMAIN_MISMATCH", "SAME_SITE_STRICT", "SAME_SITE_LAX", "SAME_SITE_UNSPECIFIED_TREATED_AS_LAX", "SAME_SITE_NONE_INSECURE", "USER_PREFERENCES", "THIRD_PARTY_PHASEOUT", "THIRD_PARTY_BLOCKED_IN_FIRST_PARTY_SET", "UNKNOWN_ERROR", "SCHEMEFUL_SAME_SITE_STRICT", "SCHEMEFUL_SAME_SITE_LAX", "SCHEMEFUL_SAME_SITE_UNSPECIFIED_TREATED_AS_LAX", "SAME_PARTY_FROM_CROSS_PARTY_CONTEXT", "NAME_VALUE_PAIR_EXCEEDS_MAX_SIZE", "PORT_MISMATCH", "SCHEME_MISMATCH", "ANONYMOUS_CONTEXT"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 1450, "end_line": 1479}, "type": "class"}, {"name": "AssociatedCookie", "docstring": "A cookie associated with the request which may or may not be sent with it.\nIncludes the cookies itself and reasons for blocking or exemption.", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 1572, "end_line": 1601}, "type": "class"}, {"name": "SetCookieBlockedReason", "docstring": "Types of reasons why a cookie may not be stored from a response.", "methods": ["to_json", "from_json"], "attributes": ["SECURE_ONLY", "SAME_SITE_STRICT", "SAME_SITE_LAX", "SAME_SITE_UNSPECIFIED_TREATED_AS_LAX", "SAME_SITE_NONE_INSECURE", "USER_PREFERENCES", "THIRD_PARTY_PHASEOUT", "THIRD_PARTY_BLOCKED_IN_FIRST_PARTY_SET", "SYNTAX_ERROR", "SCHEME_NOT_SUPPORTED", "OVERWRITE_SECURE", "INVALID_DOMAIN", "INVALID_PREFIX", "UNKNOWN_ERROR", "SCHEMEFUL_SAME_SITE_STRICT", "SCHEMEFUL_SAME_SITE_LAX", "SCHEMEFUL_SAME_SITE_UNSPECIFIED_TREATED_AS_LAX", "SAME_PARTY_FROM_CROSS_PARTY_CONTEXT", "SAME_PARTY_CONFLICTS_WITH_OTHER_ATTRIBUTES", "NAME_VALUE_PAIR_EXCEEDS_MAX_SIZE", "DISALLOWED_CHARACTER", "NO_COOKIE_CONTENT"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 1415, "end_line": 1447}, "type": "class"}, {"name": "BlockedSetCookieWithReason", "docstring": "A cookie which was not stored from a response with the corresponding reason.", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 1507, "end_line": 1537}, "type": "class"}, {"name": "CookiePriority", "docstring": "Represents the cookie's 'Priority' status:\nhttps://tools.ietf.org/html/draft-west-cookie-priority-00", "methods": ["to_json", "from_json"], "attributes": ["LOW", "MEDIUM", "HIGH"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 215, "end_line": 229}, "type": "class"}, {"name": "CookieSameSite", "docstring": "Represents the cookie's 'SameSite' status:\nhttps://tools.ietf.org/html/draft-west-first-party-cookies", "methods": ["to_json", "from_json"], "attributes": ["STRICT", "LAX", "NONE"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 198, "end_line": 212}, "type": "class"}, {"name": "PermissionsPolicyBlockReason", "docstring": "Reason for a permissions policy feature to be disabled.", "methods": ["to_json", "from_json"], "attributes": ["HEADER", "IFRAME_ATTRIBUTE", "IN_FENCED_FRAME_TREE", "IN_ISOLATED_APP"], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 322, "end_line": 336}, "type": "class"}, {"name": "from_json", "is_method": true, "class_name": "ExemptedSetCookieWithReason", "parameters": ["cls", "json"], "calls": ["cls", "CookieExemptionReason.from_json", "str", "Cookie.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 1563, "end_line": 1568}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> ExemptedSetCookieWithReason:\n        return cls(\n            exemption_reason=CookieExemptionReason.from_json(json[\"exemptionReason\"]),\n            cookie_line=str(json[\"cookieLine\"]),\n            cookie=Cookie.from_json(json[\"cookie\"]),\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.045210599899292}
{"question": "How does the test class that validates encrypted HLS stream decryption handle AES-128 decryption when switching between encrypted and unencrypted segments within the same HLS stream?", "answer": "", "relative_code_list": null, "ground_truth": "The TestHLSStreamEncrypted class handles AES-128 decryption context switching through the test_hls_encrypted_switch_methods method, which demonstrates the framework's ability to dynamically change encryption methods within a single HLS stream. When switching from AES-128 encrypted segments to unencrypted segments (METHOD: NONE) and back to AES-128, the framework maintains proper decryption context by tracking the active EXT-X-KEY tags in the playlist. The HTTP headers (including the custom X-FOO: BAR header set in get_session) are consistently propagated to all segment and key requests regardless of encryption state, ensuring proper authentication and header management throughout the stream processing lifecycle. The framework validates that decrypted content matches expected plaintext values and that all appropriate segments are downloaded while maintaining the correct encryption state transitions.", "score": null, "retrieved_content": [{"name": "test_hls_encrypted_switch_methods", "is_method": true, "class_name": "TestHLSStreamEncrypted", "parameters": ["self"], "calls": ["self.gen_key", "self.gen_key", "Tag", "self.subject", "self.await_write", "self.await_read", "self.await_close", "self.content", "self.content", "self.content", "Playlist", "SegmentEnc", "SegmentEnc", "Segment", "Segment", "SegmentEnc", "SegmentEnc"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 953, "end_line": 983}, "code_snippet": "    def test_hls_encrypted_switch_methods(self):\n        aesKey1, aesIv1, key_aes128_1 = self.gen_key()\n        aesKey2, aesIv2, key_aes128_2 = self.gen_key()\n        key_none = Tag(\"EXT-X-KEY\", {\"METHOD\": \"NONE\"})\n\n        segments = self.subject([\n            Playlist(\n                0,\n                [\n                    key_aes128_1,\n                    SegmentEnc(0, aesKey1, aesIv1),\n                    SegmentEnc(1, aesKey1, aesIv1),\n                    key_none,\n                    Segment(2),\n                    Segment(3),\n                    key_aes128_2,\n                    SegmentEnc(4, aesKey2, aesIv2),\n                    SegmentEnc(5, aesKey2, aesIv2),\n                ],\n                end=True,\n            ),\n        ])\n\n        self.await_write(6)\n        data = self.await_read(read_all=True)\n        self.await_close()\n\n        expected = self.content(segments, prop=\"content_plain\", cond=lambda s: 0 <= s.num <= 1)\n        expected += self.content(segments, cond=lambda s: 2 <= s.num <= 3)\n        expected += self.content(segments, prop=\"content_plain\", cond=lambda s: 4 <= s.num <= 5)\n        assert data == expected, \"Switches between encryption key methods\"\n", "type": "function"}, {"name": "test_hls_encrypted_aes128", "is_method": true, "class_name": "TestHLSStreamEncrypted", "parameters": ["self"], "calls": ["self.gen_key", "self.subject", "self.await_write", "self.await_read", "self.await_close", "self.content", "self.called", "all", "last_request._request.headers.get", "any", "last_request._request.headers.get", "Playlist", "Playlist", "self.called", "self.called", "segments.values", "segments.values", "SegmentEnc", "SegmentEnc", "range", "range", "self.get_mock", "self.get_mock"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 746, "end_line": 766}, "code_snippet": "    def test_hls_encrypted_aes128(self):\n        aesKey, aesIv, key = self.gen_key()\n        long = b\"Test cipher block chaining mode by using a long bytes string\"\n\n        # noinspection PyTypeChecker\n        segments = self.subject([\n            Playlist(0, [key] + [SegmentEnc(num, aesKey, aesIv) for num in range(4)]),\n            Playlist(4, [key] + [SegmentEnc(num, aesKey, aesIv, content=long) for num in range(4, 8)], end=True),\n        ])\n\n        self.await_write(3 + 4)\n        data = self.await_read(read_all=True)\n        self.await_close()\n\n        expected = self.content(segments, prop=\"content_plain\", cond=lambda s: s.num >= 1)\n        assert data == expected, \"Decrypts the AES-128 identity stream\"\n        assert self.called(key, once=True), \"Downloads encryption key only once\"\n        assert self.get_mock(key).last_request._request.headers.get(\"X-FOO\") == \"BAR\"\n        assert not any(self.called(s) for s in segments.values() if s.num < 1), \"Skips first segment\"\n        assert all(self.called(s) for s in segments.values() if s.num >= 1), \"Downloads all remaining segments\"\n        assert self.get_mock(segments[1]).last_request._request.headers.get(\"X-FOO\") == \"BAR\"\n", "type": "function"}, {"name": "test_hls_encrypted_aes128_with_differently_encrypted_map", "is_method": true, "class_name": "TestHLSStreamEncrypted", "parameters": ["self"], "calls": ["self.gen_key", "self.gen_key", "TagMapEnc", "TagMapEnc", "self.mock", "self.mock", "self.subject", "self.await_write", "self.await_read", "self.await_close", "self.url", "self.url", "self.content", "self.id", "self.id", "Playlist", "Playlist", "SegmentEnc", "SegmentEnc", "range", "range"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 796, "end_line": 823}, "code_snippet": "    def test_hls_encrypted_aes128_with_differently_encrypted_map(self):\n        aesKey1, aesIv1, key1 = self.gen_key()  # init key\n        aesKey2, aesIv2, key2 = self.gen_key()  # media key\n        map1 = TagMapEnc(1, namespace=self.id(), key=aesKey1, iv=aesIv1)\n        map2 = TagMapEnc(2, namespace=self.id(), key=aesKey1, iv=aesIv1)\n        self.mock(\"GET\", self.url(map1), content=map1.content)\n        self.mock(\"GET\", self.url(map2), content=map2.content)\n\n        segments = self.subject([\n            Playlist(0, [key1, map1, key2] + [SegmentEnc(num, aesKey2, aesIv2) for num in range(2)]),\n            Playlist(2, [key1, map2, key2] + [SegmentEnc(num, aesKey2, aesIv2) for num in range(2, 4)], end=True),\n        ])\n\n        self.await_write(1 + 2 + 1 + 2)  # 1 map, 2 segments, 1 map, 2 segments\n        data = self.await_read(read_all=True)\n        self.await_close()\n\n        assert data == self.content(\n            [\n                map1,\n                segments[0],\n                segments[1],\n                map2,\n                segments[2],\n                segments[3],\n            ],\n            prop=\"content_plain\",\n        )\n", "type": "function"}, {"name": "test_hls_encrypted_aes128_with_plaintext_map", "is_method": true, "class_name": "TestHLSStreamEncrypted", "parameters": ["self"], "calls": ["self.gen_key", "TagMap", "TagMap", "self.mock", "self.mock", "self.subject", "self.await_write", "self.await_read", "self.await_close", "self.url", "self.url", "self.id", "self.id", "Playlist", "Playlist", "SegmentEnc", "SegmentEnc", "range", "range"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 825, "end_line": 848}, "code_snippet": "    def test_hls_encrypted_aes128_with_plaintext_map(self):\n        aesKey, aesIv, key = self.gen_key()\n        map1 = TagMap(1, namespace=self.id())\n        map2 = TagMap(2, namespace=self.id())\n        self.mock(\"GET\", self.url(map1), content=map1.content)\n        self.mock(\"GET\", self.url(map2), content=map2.content)\n\n        segments = self.subject([\n            Playlist(0, [map1, key] + [SegmentEnc(num, aesKey, aesIv) for num in range(2)]),\n            Playlist(2, [map2, key] + [SegmentEnc(num, aesKey, aesIv) for num in range(2, 4)], end=True),\n        ])\n\n        self.await_write(1 + 2 + 1 + 2)  # 1 map, 2 segments, 1 map, 2 segments\n        data = self.await_read(read_all=True)\n        self.await_close()\n\n        assert data == (\n            map1.content\n            + segments[0].content_plain\n            + segments[1].content_plain\n            + map2.content\n            + segments[2].content_plain\n            + segments[3].content_plain\n        )\n", "type": "function"}, {"name": "test_hls_encrypted_aes128_key_uri_override", "is_method": true, "class_name": "TestHLSStreamEncrypted", "parameters": ["self"], "calls": ["self.gen_key", "bytes", "self.gen_key", "self.subject", "self.await_write", "self.await_read", "self.await_close", "self.content", "self.called", "self.called", "last_request._request.headers.get", "Playlist", "Playlist", "ord", "range", "SegmentEnc", "SegmentEnc", "range", "range", "self.get_mock"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 850, "end_line": 872}, "code_snippet": "    def test_hls_encrypted_aes128_key_uri_override(self):\n        aesKey, aesIv, key = self.gen_key(uri=\"http://real-mocked/{namespace}/encryption.key?foo=bar\")\n        aesKeyInvalid = bytes(ord(aesKey[i : i + 1]) ^ 0xFF for i in range(16))\n        _, __, key_invalid = self.gen_key(aesKeyInvalid, aesIv, uri=\"http://mocked/{namespace}/encryption.key?foo=bar\")\n\n        # noinspection PyTypeChecker\n        segments = self.subject(\n            [\n                Playlist(0, [key_invalid] + [SegmentEnc(num, aesKey, aesIv) for num in range(4)]),\n                Playlist(4, [key_invalid] + [SegmentEnc(num, aesKey, aesIv) for num in range(4, 8)], end=True),\n            ],\n            options={\"hls-segment-key-uri\": \"{scheme}://real-{netloc}{path}?{query}\"},\n        )\n\n        self.await_write(3 + 4)\n        data = self.await_read(read_all=True)\n        self.await_close()\n\n        expected = self.content(segments, prop=\"content_plain\", cond=lambda s: s.num >= 1)\n        assert data == expected, \"Decrypts stream from custom key\"\n        assert not self.called(key_invalid), \"Skips encryption key\"\n        assert self.called(key, once=True), \"Downloads custom encryption key\"\n        assert self.get_mock(key).last_request._request.headers.get(\"X-FOO\") == \"BAR\"\n", "type": "function"}, {"name": "test_hls_encrypted_aes128_with_map", "is_method": true, "class_name": "TestHLSStreamEncrypted", "parameters": ["self"], "calls": ["self.gen_key", "TagMapEnc", "TagMapEnc", "self.mock", "self.mock", "self.subject", "self.await_write", "self.await_read", "self.await_close", "self.url", "self.url", "self.content", "self.id", "self.id", "Playlist", "Playlist", "SegmentEnc", "SegmentEnc", "range", "range"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 768, "end_line": 794}, "code_snippet": "    def test_hls_encrypted_aes128_with_map(self):\n        aesKey, aesIv, key = self.gen_key()\n        map1 = TagMapEnc(1, namespace=self.id(), key=aesKey, iv=aesIv)\n        map2 = TagMapEnc(2, namespace=self.id(), key=aesKey, iv=aesIv)\n        self.mock(\"GET\", self.url(map1), content=map1.content)\n        self.mock(\"GET\", self.url(map2), content=map2.content)\n\n        segments = self.subject([\n            Playlist(0, [key, map1] + [SegmentEnc(num, aesKey, aesIv) for num in range(2)]),\n            Playlist(2, [key, map2] + [SegmentEnc(num, aesKey, aesIv) for num in range(2, 4)], end=True),\n        ])\n\n        self.await_write(1 + 2 + 1 + 2)  # 1 map, 2 segments, 1 map, 2 segments\n        data = self.await_read(read_all=True)\n        self.await_close()\n\n        assert data == self.content(\n            [\n                map1,\n                segments[0],\n                segments[1],\n                map2,\n                segments[2],\n                segments[3],\n            ],\n            prop=\"content_plain\",\n        )\n", "type": "function"}, {"name": "test_hls_encrypted_aes128_incorrect_block_length", "is_method": true, "class_name": "TestHLSStreamEncrypted", "parameters": ["self", "mock_log"], "calls": ["patch", "self.gen_key", "self.subject", "self.await_write", "self.thread.reader.writer.is_alive", "self.await_write", "self.await_read", "self.await_close", "self.content", "Playlist", "call", "SegmentEnc", "SegmentEnc"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 875, "end_line": 899}, "code_snippet": "    def test_hls_encrypted_aes128_incorrect_block_length(self, mock_log: Mock):\n        aesKey, aesIv, key = self.gen_key()\n\n        segments = self.subject([\n            Playlist(\n                0,\n                [\n                    key,\n                    SegmentEnc(0, aesKey, aesIv, append=b\"?\"),\n                    SegmentEnc(1, aesKey, aesIv),\n                ],\n                end=True,\n            ),\n        ])\n        self.await_write()\n        assert self.thread.reader.writer.is_alive()\n\n        self.await_write()\n        data = self.await_read(read_all=True)\n        self.await_close()\n\n        assert data == self.content([segments[1]], prop=\"content_plain\")\n        assert mock_log.error.mock_calls == [\n            call(\"Error while decrypting segment 0: Data must be padded to 16 byte boundary in CBC mode\"),\n        ]\n", "type": "function"}, {"name": "test_hls_encrypted_aes128_incorrect_padding_length", "is_method": true, "class_name": "TestHLSStreamEncrypted", "parameters": ["self", "mock_log"], "calls": ["patch", "self.gen_key", "self.subject", "self.await_write", "self.thread.reader.writer.is_alive", "self.await_write", "self.await_read", "self.await_close", "self.content", "len", "Playlist", "call", "SegmentEnc", "SegmentEnc"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 902, "end_line": 925}, "code_snippet": "    def test_hls_encrypted_aes128_incorrect_padding_length(self, mock_log: Mock):\n        aesKey, aesIv, key = self.gen_key()\n\n        padding = b\"\\x00\" * (AES.block_size - len(b\"[0]\"))\n        segments = self.subject([\n            Playlist(\n                0,\n                [\n                    key,\n                    SegmentEnc(0, aesKey, aesIv, padding=padding),\n                    SegmentEnc(1, aesKey, aesIv),\n                ],\n                end=True,\n            ),\n        ])\n        self.await_write()\n        assert self.thread.reader.writer.is_alive()\n\n        self.await_write()\n        data = self.await_read(read_all=True)\n        self.await_close()\n\n        assert data == self.content([segments[1]], prop=\"content_plain\")\n        assert mock_log.error.mock_calls == [call(\"Error while decrypting segment 0: Padding is incorrect.\")]\n", "type": "function"}, {"name": "test_hls_encrypted_aes128_incorrect_padding_content", "is_method": true, "class_name": "TestHLSStreamEncrypted", "parameters": ["self", "mock_log"], "calls": ["patch", "self.gen_key", "self.subject", "self.await_write", "self.thread.reader.writer.is_alive", "self.await_write", "self.await_read", "self.await_close", "bytes", "self.content", "Playlist", "call", "len", "SegmentEnc", "SegmentEnc"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 928, "end_line": 951}, "code_snippet": "    def test_hls_encrypted_aes128_incorrect_padding_content(self, mock_log: Mock):\n        aesKey, aesIv, key = self.gen_key()\n\n        padding = (b\"\\x00\" * (AES.block_size - len(b\"[0]\") - 1)) + bytes([AES.block_size])\n        segments = self.subject([\n            Playlist(\n                0,\n                [\n                    key,\n                    SegmentEnc(0, aesKey, aesIv, padding=padding),\n                    SegmentEnc(1, aesKey, aesIv),\n                ],\n                end=True,\n            ),\n        ])\n        self.await_write()\n        assert self.thread.reader.writer.is_alive()\n\n        self.await_write()\n        data = self.await_read(read_all=True)\n        self.await_close()\n\n        assert data == self.content([segments[1]], prop=\"content_plain\")\n        assert mock_log.error.mock_calls == [call(\"Error while decrypting segment 0: PKCS#7 padding is incorrect.\")]\n", "type": "function"}, {"name": "test_hls_encrypted_invalid_method", "is_method": true, "class_name": "TestHLSStreamEncrypted", "parameters": ["self", "mock_log"], "calls": ["patch", "self.gen_key", "self.subject", "self.await_write", "self.thread.close", "self.await_close", "join", "Playlist", "call", "SegmentEnc"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 696, "end_line": 710}, "code_snippet": "    def test_hls_encrypted_invalid_method(self, mock_log: Mock):\n        aesKey, aesIv, key = self.gen_key(method=\"INVALID\")\n\n        self.subject([\n            Playlist(0, [key, SegmentEnc(1, aesKey, aesIv)], end=True),\n        ])\n        self.await_write()\n\n        self.thread.close()\n        self.await_close()\n\n        assert b\"\".join(self.thread.data) == b\"\"\n        assert mock_log.error.mock_calls == [\n            call(\"Failed to create decryptor: Unable to decrypt cipher INVALID\"),\n        ]\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0649669170379639}
{"question": "What specific cryptographic padding scheme must be maintained for the padding removal function to correctly process the decrypted output from the block cipher in the data decryption method?", "answer": "", "relative_code_list": null, "ground_truth": "The unpad function requires PKCS7 padding scheme with a block size of 16 bytes to correctly process the decrypted output, as specified by the parameters '16' and '\"pkcs7\"' in the unpad call within the decrypt_data method.", "score": null, "retrieved_content": [{"name": "decrypt_data", "is_method": true, "class_name": "Mjunoon", "parameters": ["self", "cipher_data", "encrypted_data"], "calls": ["AES.new", "unpad", "bytes", "bytes", "cipher.decrypt", "binascii.unhexlify"], "code_location": {"file": "mjunoon.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 115, "end_line": 122}, "code_snippet": "    def decrypt_data(self, cipher_data, encrypted_data):\n        cipher = AES.new(\n            bytes(cipher_data[\"key\"], \"utf-8\"),\n            self.encryption_algorithm[cipher_data[\"algorithm\"]],\n            bytes(cipher_data[\"iv\"], \"utf-8\"),\n        )\n\n        return unpad(cipher.decrypt(binascii.unhexlify(encrypted_data)), 16, \"pkcs7\")\n", "type": "function"}, {"name": "decrypt_data", "is_method": true, "class_name": "USTVNow", "parameters": ["cls", "data", "key", "iv"], "calls": ["encode", "encode", "encode", "AES.new", "cipher.decrypt", "base64.b64decode", "unpad", "join", "join", "reversed", "reversed", "hexdigest", "SHA256.new"], "code_location": {"file": "ustvnow.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 66, "end_line": 77}, "code_snippet": "    def decrypt_data(cls, data, key, iv):\n        rkey = \"\".join(reversed(key)).encode(\"utf8\")\n        riv = \"\".join(reversed(iv)).encode(\"utf8\")\n\n        fkey = SHA256.new(rkey).hexdigest()[:32].encode(\"utf8\")\n\n        cipher = AES.new(fkey, AES.MODE_CBC, riv)\n        decrypted = cipher.decrypt(base64.b64decode(data))\n        if decrypted:\n            return unpad(decrypted, 16, \"pkcs7\")\n        else:\n            return decrypted\n", "type": "function"}, {"name": "unpad_pkcs5", "is_method": false, "class_name": null, "parameters": ["padded"], "calls": [], "code_location": {"file": "crypto.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/utils", "start_line": 46, "end_line": 47}, "code_snippet": "def unpad_pkcs5(padded):\n    return padded[: -padded[-1]]\n", "type": "function"}, {"name": "decrypt_openssl", "is_method": false, "class_name": null, "parameters": ["data", "passphrase", "key_length"], "calls": ["data.startswith", "evp_bytestokey", "AES.new", "d.decrypt", "unpad_pkcs5", "len"], "code_location": {"file": "crypto.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/utils", "start_line": 37, "end_line": 43}, "code_snippet": "def decrypt_openssl(data, passphrase, key_length=32):\n    if data.startswith(b\"Salted__\"):\n        salt = data[len(b\"Salted__\") : AES.block_size]\n        key, iv = evp_bytestokey(passphrase, salt, key_length, AES.block_size)\n        d = AES.new(key, AES.MODE_CBC, iv)\n        out = d.decrypt(data[AES.block_size :])\n        return unpad_pkcs5(out)\n", "type": "function"}, {"name": "test_decrypt_data", "is_method": true, "class_name": "TestPluginUSTVNow", "parameters": ["self"], "calls": ["USTVNow.decrypt_data"], "code_location": {"file": "test_ustvnow.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 30, "end_line": 42}, "code_snippet": "    def test_decrypt_data(self):\n        key = \"80035ad42d7d-bb08-7a14-f726-78403b29\"\n        iv = \"3157b5680927cc4a\"\n\n        assert (\n            USTVNow.decrypt_data(\n                b\"KcRLETVAmHlosM0OyUd5hdTQ6WhBRTe/YRAHiLJWrzf94OLkSueXTtQ9QZ1fjOLCbpX2qteEPUWVnzvvSgVDkQmRUttN\"\n                + b\"/royoxW2aL0gYQSoH1NWoDV8sIgvS5vDiQ85\",\n                key,\n                iv,\n            )\n            == b'{\"status\":false,\"error\":{\"code\":-2,\"type\":\"\",\"message\":\"Invalid credentials.\",\"details\":{}}}'\n        )\n", "type": "function"}, {"name": "encrypt_data", "is_method": true, "class_name": "USTVNow", "parameters": ["cls", "data", "key", "iv"], "calls": ["encode", "encode", "encode", "AES.new", "cipher.encrypt", "base64.b64encode", "pad", "join", "join", "reversed", "reversed", "hexdigest", "SHA256.new"], "code_location": {"file": "ustvnow.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 55, "end_line": 63}, "code_snippet": "    def encrypt_data(cls, data, key, iv):\n        rkey = \"\".join(reversed(key)).encode(\"utf8\")\n        riv = \"\".join(reversed(iv)).encode(\"utf8\")\n\n        fkey = SHA256.new(rkey).hexdigest()[:32].encode(\"utf8\")\n\n        cipher = AES.new(fkey, AES.MODE_CBC, riv)\n        encrypted = cipher.encrypt(pad(data, 16, \"pkcs7\"))\n        return base64.b64encode(encrypted)\n", "type": "function"}, {"name": "create_decryptor", "is_method": true, "class_name": "HLSStreamWriter", "parameters": ["self", "key", "num"], "calls": ["AES.new", "StreamError", "StreamError", "urlparse", "Formatter", "formatter.format", "self.num_to_iv", "self.session.http.get", "getattr", "isinstance", "len", "StreamError"], "code_location": {"file": "hls.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/hls", "start_line": 105, "end_line": 149}, "code_snippet": "    def create_decryptor(self, key: Key, num: int):\n        if key.method != \"AES-128\":\n            raise StreamError(f\"Unable to decrypt cipher {key.method}\")\n\n        if not self.key_uri_override and not key.uri:\n            raise StreamError(\"Missing URI for decryption key\")\n\n        if not self.key_uri_override:\n            key_uri = key.uri\n        else:\n            p = urlparse(key.uri)\n            formatter = Formatter({\n                \"url\": lambda: key.uri,\n                \"scheme\": lambda: p.scheme,\n                \"netloc\": lambda: p.netloc,\n                \"path\": lambda: p.path,\n                \"query\": lambda: p.query,\n            })\n            key_uri = formatter.format(self.key_uri_override)\n\n        if key_uri and self.key_uri != key_uri:\n            try:\n                res = self.session.http.get(\n                    key_uri,\n                    exception=StreamError,\n                    retries=self.retries,\n                    **self.reader.request_params,\n                )\n            except StreamError as err:\n                # FIXME: fix HTTPSession.request()\n                original_error = getattr(err, \"err\", None)\n                if isinstance(original_error, InvalidSchema):\n                    raise StreamError(f\"Unable to find connection adapter for key URI: {key_uri}\") from original_error\n                raise  # pragma: no cover\n\n            res.encoding = \"binary/octet-stream\"\n            self.key_data = res.content\n            self.key_uri = key_uri\n\n        iv = key.iv or self.num_to_iv(num)\n\n        # Pad IV if needed\n        iv = b\"\\x00\" * (16 - len(iv)) + iv\n\n        return AES.new(self.key_data, AES.MODE_CBC, iv)\n", "type": "function"}, {"name": "test_hls_encrypted_aes128_incorrect_padding_length", "is_method": true, "class_name": "TestHLSStreamEncrypted", "parameters": ["self", "mock_log"], "calls": ["patch", "self.gen_key", "self.subject", "self.await_write", "self.thread.reader.writer.is_alive", "self.await_write", "self.await_read", "self.await_close", "self.content", "len", "Playlist", "call", "SegmentEnc", "SegmentEnc"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 902, "end_line": 925}, "code_snippet": "    def test_hls_encrypted_aes128_incorrect_padding_length(self, mock_log: Mock):\n        aesKey, aesIv, key = self.gen_key()\n\n        padding = b\"\\x00\" * (AES.block_size - len(b\"[0]\"))\n        segments = self.subject([\n            Playlist(\n                0,\n                [\n                    key,\n                    SegmentEnc(0, aesKey, aesIv, padding=padding),\n                    SegmentEnc(1, aesKey, aesIv),\n                ],\n                end=True,\n            ),\n        ])\n        self.await_write()\n        assert self.thread.reader.writer.is_alive()\n\n        self.await_write()\n        data = self.await_read(read_all=True)\n        self.await_close()\n\n        assert data == self.content([segments[1]], prop=\"content_plain\")\n        assert mock_log.error.mock_calls == [call(\"Error while decrypting segment 0: Padding is incorrect.\")]\n", "type": "function"}, {"name": "test_hls_encrypted_aes128_incorrect_padding_content", "is_method": true, "class_name": "TestHLSStreamEncrypted", "parameters": ["self", "mock_log"], "calls": ["patch", "self.gen_key", "self.subject", "self.await_write", "self.thread.reader.writer.is_alive", "self.await_write", "self.await_read", "self.await_close", "bytes", "self.content", "Playlist", "call", "len", "SegmentEnc", "SegmentEnc"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 928, "end_line": 951}, "code_snippet": "    def test_hls_encrypted_aes128_incorrect_padding_content(self, mock_log: Mock):\n        aesKey, aesIv, key = self.gen_key()\n\n        padding = (b\"\\x00\" * (AES.block_size - len(b\"[0]\") - 1)) + bytes([AES.block_size])\n        segments = self.subject([\n            Playlist(\n                0,\n                [\n                    key,\n                    SegmentEnc(0, aesKey, aesIv, padding=padding),\n                    SegmentEnc(1, aesKey, aesIv),\n                ],\n                end=True,\n            ),\n        ])\n        self.await_write()\n        assert self.thread.reader.writer.is_alive()\n\n        self.await_write()\n        data = self.await_read(read_all=True)\n        self.await_close()\n\n        assert data == self.content([segments[1]], prop=\"content_plain\")\n        assert mock_log.error.mock_calls == [call(\"Error while decrypting segment 0: PKCS#7 padding is incorrect.\")]\n", "type": "function"}, {"name": "test_encrypt_data", "is_method": true, "class_name": "TestPluginUSTVNow", "parameters": ["self"], "calls": ["USTVNow.encrypt_data"], "code_location": {"file": "test_ustvnow.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 14, "end_line": 28}, "code_snippet": "    def test_encrypt_data(self):\n        key = \"80035ad42d7d-bb08-7a14-f726-78403b29\"\n        iv = \"3157b5680927cc4a\"\n\n        assert (\n            USTVNow.encrypt_data(\n                b'{\"login_id\":\"test@test.com\",\"login_key\":\"testtest1234\",\"login_mode\":\"1\",\"manufacturer\":\"123\"}',\n                key,\n                iv,\n            )\n            == (\n                b\"uawIc5n+TnmsmR+aP2iEDKG/eMKji6EKzjI4mE+zMhlyCbHm7K4hz7IDJDWwM3aE+Ro4ydSsgJf4ZInnoW6gqvXvG0qB\"\n                + b\"/J2WJeypTSt4W124zkJpvfoJJmGAvBg2t0HT\"\n            )\n        )  # fmt: skip\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0874760150909424}
{"question": "What is the integration of the scheduled navigation event with the broader browser protocol event handling architecture for coordinating frame navigation lifecycle management?", "answer": "", "relative_code_list": null, "ground_truth": "The FrameScheduledNavigation event is part of Chrome DevTools Protocol's page domain event system that coordinates frame navigation lifecycle. It integrates through a hierarchical event architecture where this specific event triggers when a frame schedules navigation, working in conjunction with other navigation events like FrameStartedNavigating, FrameNavigated, and FrameClearedScheduledNavigation. The architecture employs a state machine pattern where FrameScheduledNavigation represents the 'scheduled' state, requiring coordination with network layer for resource loading, DOM layer for frame structure management, and runtime layer for JavaScript execution context. The event's timing (delay parameter) and reason (ClientNavigationReason) determine how the navigation scheduler component prioritizes and sequences multiple frame navigations while maintaining browser context integrity and preventing race conditions in multi-frame environments.", "score": null, "retrieved_content": [{"name": "FrameScheduledNavigation", "docstring": "Fired when frame schedules a potential navigation.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3585, "end_line": 3606}, "type": "class"}, {"name": "FrameClearedScheduledNavigation", "docstring": "Fired when frame no longer has a scheduled navigation.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3416, "end_line": 3427}, "type": "class"}, {"name": "FrameRequestedNavigation", "docstring": "**EXPERIMENTAL**\n\nFired when a renderer-initiated navigation is requested.\nNavigation may still be cancelled after the event is issued.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3557, "end_line": 3580}, "type": "class"}, {"name": "FrameStartedNavigating", "docstring": "**EXPERIMENTAL**\n\nFired when a navigation starts. This event is fired for both\nrenderer-initiated and browser-initiated navigations. For renderer-initiated\nnavigations, the event is fired after ``frameRequestedNavigation``.\nNavigation may still be cancelled after the event is issued. Multiple events\ncan be fired for a single navigation, for example, when a same-document\nnavigation becomes a cross-document navigation (such as in the case of a\nframeset).", "methods": ["from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3522, "end_line": 3552}, "type": "class"}, {"name": "LifecycleEvent", "docstring": "Fired for lifecycle events (navigation, load, paint, etc) in the current\ntarget (including local frames).", "methods": ["from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3790, "end_line": 3809}, "type": "class"}, {"name": "FrameNavigated", "docstring": "Fired once navigation of the frame has completed. Frame is now associated with the new loader.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3469, "end_line": 3482}, "type": "class"}, {"name": "from_json", "is_method": true, "class_name": "FrameScheduledNavigation", "parameters": ["cls", "json"], "calls": ["cls", "FrameId.from_json", "float", "ClientNavigationReason.from_json", "str"], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3600, "end_line": 3606}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> FrameScheduledNavigation:\n        return cls(\n            frame_id=FrameId.from_json(json[\"frameId\"]),\n            delay=float(json[\"delay\"]),\n            reason=ClientNavigationReason.from_json(json[\"reason\"]),\n            url=str(json[\"url\"]),\n        )\n", "type": "function"}, {"name": "NavigationType", "docstring": "The type of a frameNavigated event.", "methods": ["to_json", "from_json"], "attributes": ["NAVIGATION", "BACK_FORWARD_CACHE_RESTORE"], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 1727, "end_line": 1739}, "type": "class"}, {"name": "NavigatedWithinDocument", "docstring": "**EXPERIMENTAL**\n\nFired when same-document navigation happens, e.g. due to history API usage or anchor navigation.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3856, "end_line": 3875}, "type": "class"}, {"name": "from_json", "is_method": true, "class_name": "FrameClearedScheduledNavigation", "parameters": ["cls", "json"], "calls": ["cls", "FrameId.from_json"], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3424, "end_line": 3427}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> FrameClearedScheduledNavigation:\n        return cls(\n            frame_id=FrameId.from_json(json[\"frameId\"]),\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1008577346801758}
{"question": "How does the browser automation control class's method that retrieves the WebSocket connection URL implement error handling mechanisms when establishing WebSocket connections to the browser debugging protocol?", "answer": "", "relative_code_list": null, "ground_truth": "The ChromiumWebbrowser's get_websocket_url method implements timeout retry strategies through multiple HTTP GET requests to the debugger address with configurable retry attempts (num parameter). It handles Timeout exceptions from the requests library by retrying the specified number of times before proceeding. Error handling includes context manager-based exception handling (raises parameter) for WebbrowserError scenarios, IPv6 address formatting for host addresses containing colons, and proper WebSocket URL construction from the Chrome DevTools Protocol response payload while ensuring proxy configuration isolation in the final connection request.", "score": null, "retrieved_content": [{"name": "get_websocket_url", "is_method": true, "class_name": "ChromiumWebbrowser", "parameters": ["self", "session"], "calls": ["session.http.get", "validate.Schema", "validate.parse_json", "validate.get", "validate.url"], "code_location": {"file": "chromium.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser", "start_line": 194, "end_line": 209}, "code_snippet": "    def get_websocket_url(self, session: Streamlink) -> str:\n        return session.http.get(\n            f\"http://{f'[{self.host}]' if ':' in self.host else self.host}:{self.port}/json/version\",\n            retries=10,\n            retry_backoff=0.25,\n            retry_max_backoff=0.25,\n            timeout=0.1,\n            proxies={\n                \"http\": \"\",\n            },\n            schema=validate.Schema(\n                validate.parse_json(),\n                {\"webSocketDebuggerUrl\": validate.url(scheme=\"ws\")},\n                validate.get(\"webSocketDebuggerUrl\"),\n            ),\n        )\n", "type": "function"}, {"name": "test_get_websocket_address", "is_method": false, "class_name": null, "parameters": ["monkeypatch", "requests_mock", "session", "host", "port", "address", "num", "raises"], "calls": ["pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.mark.parametrize", "monkeypatch.setattr", "responses.append", "requests_mock.register_uri", "ChromiumWebbrowser", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "Timeout", "range", "webbrowser.get_websocket_url", "mock.last_request.proxies.get", "nullcontext", "pytest.raises"], "code_location": {"file": "test_chromium.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/webbrowser", "start_line": 191, "end_line": 222}, "code_snippet": "def test_get_websocket_address(\n    monkeypatch: pytest.MonkeyPatch,\n    requests_mock: rm.Mocker,\n    session: Streamlink,\n    host: str,\n    port: int,\n    address: str,\n    num: int,\n    raises: nullcontext,\n):\n    monkeypatch.setattr(\"time.sleep\", lambda _: None)\n    hostaddr = f\"[{host}]\" if \":\" in host else host\n\n    payload = {\n        \"Browser\": \"Chrome/114.0.5735.133\",\n        \"Protocol-Version\": \"1.3\",\n        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n        \"V8-Version\": \"11.4.183.23\",\n        \"WebKit-Version\": \"537.36 (@fbfa2ce68d01b2201d8c667c2e73f648a61c4f4a)\",\n        \"webSocketDebuggerUrl\": f\"ws://{hostaddr}:{port}/devtools/browser/some-uuid4\",\n    }\n\n    responses: list[dict[str, Any]] = [{\"exc\": Timeout()} for _ in range(num)]\n    responses.append({\"json\": payload})\n    mock = requests_mock.register_uri(\"GET\", address, responses)\n\n    webbrowser = ChromiumWebbrowser(host=host, port=port)\n    with raises:\n        assert webbrowser.get_websocket_url(session) == f\"ws://{hostaddr}:{port}/devtools/browser/some-uuid4\"\n        assert mock.called\n        assert mock.last_request\n        assert not mock.last_request.proxies.get(\"http\")\n", "type": "function"}, {"name": "WebbrowserError", "docstring": "", "methods": [], "attributes": [], "code_location": {"file": "exceptions.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser", "start_line": 4, "end_line": 5}, "type": "class"}, {"name": "websocket_connection", "is_method": false, "class_name": null, "parameters": ["monkeypatch"], "calls": ["pytest.fixture", "FakeWebsocketConnection", "AsyncMock", "monkeypatch.setattr", "call"], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/webbrowser/cdp", "start_line": 9, "end_line": 18}, "code_snippet": "def websocket_connection(monkeypatch: pytest.MonkeyPatch):\n    fake_websocket_connection = FakeWebsocketConnection()\n    mock_connect_websocket_url = AsyncMock(return_value=fake_websocket_connection)\n    monkeypatch.setattr(\"streamlink.webbrowser.cdp.connection.connect_websocket_url\", mock_connect_websocket_url)\n\n    try:\n        yield fake_websocket_connection\n    finally:\n        assert fake_websocket_connection.closed\n        assert mock_connect_websocket_url.call_args_list == [call(ANY, \"ws://localhost:1234/fake\", max_message_size=2**24)]\n", "type": "function"}, {"name": "_get_cookies_from_webbrowser", "is_method": true, "class_name": "Kick", "parameters": ["self"], "calls": ["log.info", "self.session.http.headers.update", "self.cache.set", "CDPClient.launch", "log.info", "self.save_cookies", "client_session.continue_request", "client.session", "client_session.add_request_handler", "log.exception", "log.error", "client_session.navigate", "client_session.loaded", "client_session.retrieve_cookies"], "code_location": {"file": "kick.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 180, "end_line": 213}, "code_snippet": "    def _get_cookies_from_webbrowser(self) -> bool:\n        from streamlink.compat import BaseExceptionGroup  # noqa: PLC0415\n        from streamlink.webbrowser.cdp import CDPClient, CDPClientSession  # noqa: PLC0415, TC001\n        from streamlink.webbrowser.cdp.devtools import fetch  # noqa: PLC0415, TC001\n\n        async def on_main(client_session: CDPClientSession, request: fetch.RequestPaused):\n            # get Chromium's request headers, update HTTP session headers and also cache them\n            self.session.http.headers.update(request.request.headers)\n            self.cache.set(self._CACHE_HEADERS, request.request.headers, expires=self._CACHE_EXPIRATION)\n            await client_session.continue_request(request)\n\n        async def get_challenge_cookies(client: CDPClient):\n            client_session: CDPClientSession\n            async with client.session() as client_session:\n                client_session.add_request_handler(on_main, url_pattern=self.url, on_request=True)\n                async with client_session.navigate(self.url) as frame_id:\n                    await client_session.loaded(frame_id)\n                    await client_session.retrieve_cookies()\n\n        log.info(\"Solving JS challenge\")\n\n        try:\n            CDPClient.launch(self.session, get_challenge_cookies)\n        except BaseExceptionGroup:\n            log.exception(\"Failed solving JS challenge\")\n        except Exception as err:\n            log.error(err)\n        else:\n            log.info(\"JS challenge solved, storing cookies\")\n            self.save_cookies()\n\n            return True\n\n        return False\n", "type": "function"}, {"name": "_ws_init", "is_method": true, "class_name": "WebsocketClient", "parameters": ["self", "url", "subprotocols", "header", "cookie"], "calls": ["WebSocketApp"], "code_location": {"file": "websocket.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugin/api", "start_line": 98, "end_line": 112}, "code_snippet": "    def _ws_init(self, url, subprotocols, header, cookie):\n        self.ws = WebSocketApp(\n            url=url,\n            subprotocols=subprotocols,\n            header=header,\n            cookie=cookie,\n            on_open=self.on_open,\n            on_error=self.on_error,\n            on_close=self.on_close,\n            on_ping=self.on_ping,\n            on_pong=self.on_pong,\n            on_message=self.on_message,\n            on_cont_message=self.on_cont_message,\n            on_data=self.on_data,\n        )\n", "type": "function"}, {"name": "on_open", "is_method": true, "class_name": "WebsocketClient", "parameters": ["self", "wsapp"], "calls": ["log.debug"], "code_location": {"file": "websocket.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugin/api", "start_line": 164, "end_line": 165}, "code_snippet": "    def on_open(self, wsapp: WebSocketApp) -> None:\n        log.debug(f\"Connected: {wsapp.url}\")  # pragma: no cover\n", "type": "function"}, {"name": "find_wss_api_url", "is_method": true, "class_name": "NicoLive", "parameters": ["data"], "calls": ["validate.Schema", "schema.validate", "validate.get", "validate.union_get", "update_qsd", "validate.optional", "validate.any", "validate.url"], "code_location": {"file": "nicolive.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 307, "end_line": 331}, "code_snippet": "    def find_wss_api_url(data):\n        schema = validate.Schema(\n            {\n                \"site\": {\n                    \"relive\": {\n                        \"webSocketUrl\": validate.any(\n                            validate.url(scheme=\"wss\"),\n                            \"\",\n                        ),\n                    },\n                    validate.optional(\"frontendId\"): int,\n                },\n            },\n            validate.get(\"site\"),\n            validate.union_get((\"relive\", \"webSocketUrl\"), \"frontendId\"),\n        )\n\n        wss_api_url, frontend_id = schema.validate(data)\n        if not wss_api_url:\n            return\n\n        if frontend_id is not None:\n            wss_api_url = update_qsd(wss_api_url, {\"frontend_id\": frontend_id})\n\n        return wss_api_url\n", "type": "function"}, {"name": "CDPClient", "docstring": "The public interface around :class:`ChromiumWebbrowser <streamlink.webbrowser.chromium.ChromiumWebbrowser>`\nand :class:`CDPConnection <streamlink.webbrowser.cdp.connection.CDPConnection>`.\n\nIt launches the Chromium-based web browser, establishes the remote debugging WebSocket connection using\nthe `Chrome Devtools Protocol <https://chromedevtools.github.io/devtools-protocol/>`_,  and provides\nthe :meth:`session()` method for creating a new :class:`CDPClientSession` that is tied to an empty new browser tab.\n\n:class:`CDPClientSession` provides a high-level API for navigating websites, intercepting network requests and responses,\nas well as evaluating JavaScript expressions and retrieving async results.\n\nDon't instantiate this class yourself, use the :meth:`CDPClient.launch()` async context manager classmethod.\n\nFor low-level Chrome Devtools Protocol interfaces, please see Streamlink's automatically generated\n``streamlink.webbrowser.cdp.devtools`` package, but be aware that only a subset of the available domains is supported.", "methods": ["__init__", "launch"], "attributes": [], "code_location": {"file": "client.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp", "start_line": 77, "end_line": 212}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "CDPBase", "parameters": ["self", "websocket", "target_id", "session_id", "cmd_timeout"], "calls": ["defaultdict", "itertools.count"], "code_location": {"file": "connection.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp", "start_line": 154, "end_line": 167}, "code_snippet": "    def __init__(\n        self,\n        websocket: WebSocketConnection,\n        target_id: TargetID | None = None,\n        session_id: SessionID | None = None,\n        cmd_timeout: float = CMD_TIMEOUT,\n    ) -> None:\n        self.websocket = websocket\n        self.target_id = target_id\n        self.session_id = session_id\n        self.cmd_timeout = cmd_timeout\n        self.event_channels: TEventChannels = defaultdict(set)\n        self.cmd_buffers: dict[int, _CDPCmdBuffer] = {}\n        self.cmd_id = itertools.count()\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0751957893371582}
{"question": "What is the mechanism by which the terminal-constrained progress message formatter's formatting method dynamically adjusts variable segment widths based on terminal constraints and parameter callbacks?", "answer": "", "relative_code_list": null, "ground_truth": "The ProgressFormatter's format method first calculates the total available terminal width using term_width(), then processes format specifications by separating static text segments from variable segments. For variable segments with callable parameters, it calculates the maximum available width for each variable segment by subtracting the total static text length from the terminal width and dividing equally among variables (using floor division). It validates that this calculated width meets each variable's minimum width requirement (from format_spec) before executing the callbacks with the computed width to generate the final formatted output.", "score": null, "retrieved_content": [{"name": "format", "is_method": true, "class_name": "ProgressFormatter", "parameters": ["cls", "formats", "params"], "calls": ["term_width", "join", "static.clear", "variable.clear", "static.append", "len", "int", "callable", "static.append", "len", "variable.append", "static.append", "any", "len", "fn", "len", "int"], "code_location": {"file": "progress.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink_cli/console", "start_line": 54, "end_line": 101}, "code_snippet": "    def format(cls, formats: _TFormat, params: Mapping[str, str | Callable[[int], str]]) -> str:\n        width = term_width()\n        static: list[str] = []\n        variable: list[tuple[int, Callable[[int], str], int]] = []\n\n        for fmt in formats:\n            static.clear()\n            variable.clear()\n            length = 0\n            # Get literal texts, static segments and variable segments from the parsed format\n            # and calculate the overall length of the literal texts and static segments after substituting them.\n            for literal_text, field_name, format_spec, _conversion in fmt:\n                static.append(literal_text)\n                length += len(literal_text)\n                if field_name is None:\n                    continue\n                if field_name not in params:\n                    break\n                value_or_callable = params[field_name]\n                if not callable(value_or_callable):\n                    static.append(value_or_callable)\n                    length += len(value_or_callable)\n                else:\n                    variable.append((len(static), value_or_callable, int(format_spec or 0)))\n                    static.append(\"\")\n            else:\n                # No variable segments? Just check if the resulting string fits into the size constraints.\n                if not variable:\n                    if length > width:\n                        continue\n                    else:\n                        break\n\n                # Get the available space for each variable segment (share space equally and round down).\n                max_width = int((width - length) / len(variable))\n                # If at least one variable segment doesn't fit, continue with the next format.\n                if max_width < 1 or any(max_width < min_width for _, __, min_width in variable):\n                    continue\n                # All variable segments fit, so finally format them, but continue with the next format if there's an error.\n                # noinspection PyBroadException\n                try:\n                    for idx, fn, _ in variable:\n                        static[idx] = fn(max_width)\n                except Exception:\n                    continue\n                break\n\n        return \"\".join(static)\n", "type": "function"}, {"name": "test_format", "is_method": true, "class_name": "TestProgressFormatter", "parameters": ["self", "params", "term_width", "expected"], "calls": ["pytest.mark.parametrize", "ProgressFormatter.format"], "code_location": {"file": "test_progress.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli/console", "start_line": 46, "end_line": 47}, "code_snippet": "    def test_format(self, params, term_width, expected):\n        assert ProgressFormatter.format(ProgressFormatter.FORMATS, params) == expected\n", "type": "function"}, {"name": "test_format_nospeed", "is_method": true, "class_name": "TestProgressFormatter", "parameters": ["self", "params", "term_width", "expected"], "calls": ["pytest.mark.parametrize", "ProgressFormatter.format"], "code_location": {"file": "test_progress.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli/console", "start_line": 63, "end_line": 64}, "code_snippet": "    def test_format_nospeed(self, params, term_width, expected):\n        assert ProgressFormatter.format(ProgressFormatter.FORMATS_NOSPEED, params) == expected\n", "type": "function"}, {"name": "test_format_path", "is_method": true, "class_name": "_TestFormatPath", "parameters": ["self", "monkeypatch", "path", "max_width", "expected"], "calls": ["monkeypatch.setattr", "ProgressFormatter.format_path", "type"], "code_location": {"file": "test_progress.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli/console", "start_line": 130, "end_line": 132}, "code_snippet": "    def test_format_path(self, monkeypatch: pytest.MonkeyPatch, path: PurePath, max_width: int, expected: str):\n        monkeypatch.setattr(\"os.path.sep\", \"\\\\\" if type(path) is PureWindowsPath else \"/\")\n        assert ProgressFormatter.format_path(path, max_width) == expected\n", "type": "function"}, {"name": "test_format_error", "is_method": true, "class_name": "TestProgressFormatter", "parameters": ["self", "params"], "calls": ["dict", "Mock", "ProgressFormatter.format", "ValueError"], "code_location": {"file": "test_progress.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli/console", "start_line": 69, "end_line": 72}, "code_snippet": "    def test_format_error(self, params):\n        params = dict(**params)\n        params[\"path\"] = Mock(side_effect=ValueError(\"fail\"))\n        assert ProgressFormatter.format(ProgressFormatter.FORMATS, params) == \"[download] Written WRITTEN (ELAPSED @ SPEED)\"\n", "type": "function"}, {"name": "test_format_missing", "is_method": true, "class_name": "TestProgressFormatter", "parameters": ["self", "params"], "calls": ["ProgressFormatter.format"], "code_location": {"file": "test_progress.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli/console", "start_line": 66, "end_line": 67}, "code_snippet": "    def test_format_missing(self, params):\n        assert ProgressFormatter.format(ProgressFormatter.FORMATS, {\"written\": \"0\"}) == \"[download] 0\"\n", "type": "function"}, {"name": "format_path", "is_method": true, "class_name": "ProgressFormatter", "parameters": ["cls", "path", "max_width"], "calls": ["str", "text_width", "os.path.sep.join", "cut_text", "text_width", "text_width"], "code_location": {"file": "progress.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink_cli/console", "start_line": 132, "end_line": 147}, "code_snippet": "    def format_path(cls, path: PurePath, max_width: int) -> str:\n        # Quick check if the path fits\n        string = str(path)\n        width = text_width(string)\n        if width <= max_width:\n            return string\n\n        # Since the path doesn't fit, we always need to add an ellipsis.\n        # On Windows, we also need to add the \"drive\" part (which is an empty string on PurePosixPath)\n        max_width -= text_width(path.drive) + text_width(cls.ELLIPSIS)\n\n        # Ignore the path's first part, aka the \"anchor\" (drive + root)\n        parts = os.path.sep.join(path.parts[1:] if path.drive else path.parts)\n        truncated = cut_text(parts, max_width)\n\n        return f\"{path.drive}{cls.ELLIPSIS}{truncated}\"\n", "type": "function"}, {"name": "term_width", "is_method": false, "class_name": null, "parameters": [], "calls": ["get_terminal_size"], "code_location": {"file": "terminal.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink_cli/console", "start_line": 57, "end_line": 58}, "code_snippet": "def term_width():\n    return get_terminal_size().columns - GAP\n", "type": "function"}, {"name": "update", "is_method": true, "class_name": "Progress", "parameters": ["self"], "calls": ["time", "history.append", "dict", "formatter.format", "len", "formatter.format_filesize", "self.console.msg_status", "self.console.msg", "formatter.format_filesize", "formatter.format_time", "sum", "formatter.format_path"], "code_location": {"file": "progress.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink_cli/console", "start_line": 201, "end_line": 229}, "code_snippet": "    def update(self):\n        with self._lock:\n            now = time()\n            formatter = self.formatter\n            history = self.history\n\n            history.append((now, self.written))\n            self.written = 0\n\n            has_history = len(history) >= self.threshold\n            if not has_history or now == history[0][0]:\n                formats = formatter.FORMATS_NOSPEED\n                speed = \"\"\n            else:\n                formats = formatter.FORMATS\n                speed = formatter.format_filesize(sum(size for _, size in history) / (now - history[0][0]), \"/s\")\n\n            params = dict(\n                written=formatter.format_filesize(self.overall),\n                elapsed=formatter.format_time(now - self.started),\n                speed=speed,\n                path=lambda max_width: formatter.format_path(self.path, max_width),\n            )\n\n            status = formatter.format(formats, params)\n            if self.status:\n                self.console.msg_status(status)\n            else:\n                self.console.msg(status)\n", "type": "function"}, {"name": "test_download_speed", "is_method": true, "class_name": "TestProgress", "parameters": ["self", "mock_width", "frozen_time"], "calls": ["Mock", "Progress", "time", "progress.update", "progress.update", "frozen_time.tick", "progress.write", "progress.update", "frozen_time.tick", "progress.write", "progress.update", "frozen_time.tick", "progress.write", "progress.update", "frozen_time.tick", "progress.write", "progress.update", "frozen_time.tick", "progress.write", "progress.update", "frozen_time.tick", "progress.update", "messages.pop", "messages.pop", "messages.pop", "messages.pop", "messages.pop", "messages.pop", "messages.pop", "messages.pop", "Mock", "PurePosixPath"], "code_location": {"file": "test_progress.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli/console", "start_line": 234, "end_line": 284}, "code_snippet": "    def test_download_speed(self, mock_width: Mock, frozen_time):\n        kib = b\"\\x00\" * 1024\n        messages: list[str] = []\n        console = Mock(msg_status=Mock(side_effect=messages.append))\n        progress = Progress(\n            console=console,\n            path=PurePosixPath(\"../../the/path/where/we/write/to\"),\n            interval=1,\n            history=3,\n            threshold=2,\n        )\n\n        progress.started = time()\n        assert messages == []\n\n        progress.update()\n        assert messages.pop() == \"[download] Written 0 bytes to ../../the/path/where/we/write/to (0s)\"\n\n        progress.update()\n        assert messages.pop() == \"[download] Written 0 bytes to ../../the/path/where/we/write/to (0s)\"\n\n        frozen_time.tick()\n        progress.write(kib * 1)\n        progress.update()\n        assert messages.pop() == \"[download] Written 1.00 KiB to th/where/we/write/to (1s @ 1.00 KiB/s)\"\n\n        frozen_time.tick()\n        mock_width.return_value = 65\n        progress.write(kib * 3)\n        progress.update()\n        assert messages.pop() == \"[download] Written 4.00 KiB to ere/we/write/to (2s @ 2.00 KiB/s)\"\n\n        frozen_time.tick()\n        mock_width.return_value = 60\n        progress.write(kib * 5)\n        progress.update()\n        assert messages.pop() == \"[download] Written 9.00 KiB (3s @ 4.50 KiB/s)\"\n\n        frozen_time.tick()\n        progress.write(kib * 7)\n        progress.update()\n        assert messages.pop() == \"[download] Written 16.00 KiB (4s @ 7.50 KiB/s)\"\n\n        frozen_time.tick()\n        progress.write(kib * 5)\n        progress.update()\n        assert messages.pop() == \"[download] Written 21.00 KiB (5s @ 8.50 KiB/s)\"\n\n        frozen_time.tick()\n        progress.update()\n        assert messages.pop() == \"[download] Written 21.00 KiB (6s @ 6.00 KiB/s)\"\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0971345901489258}
{"question": "Why is a JSON deserialization factory method implemented in a browser context identifier type wrapper instead of using direct JSON deserialization through the class constructor?", "answer": "", "relative_code_list": null, "ground_truth": "The implementation of a dedicated from_json factory method in BrowserContextID follows the factory pattern design principle to provide a clear, type-safe interface for JSON deserialization that explicitly communicates the conversion intent, separates construction logic from the main class responsibilities, and allows for potential future extension of deserialization behavior without modifying the primary constructor, while maintaining consistency with the CDP devtools protocol's JSON-based communication pattern.", "score": null, "retrieved_content": [{"name": "from_json", "is_method": true, "class_name": "BrowserContextID", "parameters": ["cls", "json"], "calls": ["cls"], "code_location": {"file": "browser.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 26, "end_line": 27}, "code_snippet": "    def from_json(cls, json: str) -> BrowserContextID:\n        return cls(json)\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "BrowserCommandId", "parameters": ["cls", "json"], "calls": ["cls"], "code_location": {"file": "browser.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 232, "end_line": 233}, "code_snippet": "    def from_json(cls, json: str) -> BrowserCommandId:\n        return cls(json)\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "CrossOriginIsolatedContextType", "parameters": ["cls", "json"], "calls": ["cls"], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 182, "end_line": 183}, "code_snippet": "    def from_json(cls, json: str) -> CrossOriginIsolatedContextType:\n        return cls(json)\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "ExecutionContextId", "parameters": ["cls", "json"], "calls": ["cls"], "code_location": {"file": "runtime.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 532, "end_line": 533}, "code_snippet": "    def from_json(cls, json: int) -> ExecutionContextId:\n        return cls(json)\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "WindowID", "parameters": ["cls", "json"], "calls": ["cls"], "code_location": {"file": "browser.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 38, "end_line": 39}, "code_snippet": "    def from_json(cls, json: int) -> WindowID:\n        return cls(json)\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "SecureContextType", "parameters": ["cls", "json"], "calls": ["cls"], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 166, "end_line": 167}, "code_snippet": "    def from_json(cls, json: str) -> SecureContextType:\n        return cls(json)\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "CertificateId", "parameters": ["cls", "json"], "calls": ["cls"], "code_location": {"file": "security.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 28, "end_line": 29}, "code_snippet": "    def from_json(cls, json: int) -> CertificateId:\n        return cls(json)\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "RemoteObjectId", "parameters": ["cls", "json"], "calls": ["cls"], "code_location": {"file": "runtime.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 112, "end_line": 113}, "code_snippet": "    def from_json(cls, json: str) -> RemoteObjectId:\n        return cls(json)\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "BackForwardCacheNotRestoredReasonType", "parameters": ["cls", "json"], "calls": ["cls"], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 1910, "end_line": 1911}, "code_snippet": "    def from_json(cls, json: str) -> BackForwardCacheNotRestoredReasonType:\n        return cls(json)\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "UniqueDebuggerId", "parameters": ["cls", "json"], "calls": ["cls"], "code_location": {"file": "runtime.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 774, "end_line": 775}, "code_snippet": "    def from_json(cls, json: str) -> UniqueDebuggerId:\n        return cls(json)\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0725407600402832}
{"question": "What is the semantic meaning of the boolean parameter that controls reduced-latency streaming mode in the constructor of a platform-specific HLS stream implementation class?", "answer": "", "relative_code_list": null, "ground_truth": "The low_latency parameter in the TwitchHLSStream constructor enables or disables low-latency streaming mode, which fundamentally alters the stream's operational behavior by modifying segment fetching strategies, buffer management, and playback timing to reduce end-to-end latency. Unlike the base HLSStream implementation that follows standard HLS playback with typical buffer delays, when low_latency is True, the TwitchHLSStream employs partial segment loading, reduced buffer sizes, and optimized timing mechanisms specific to Twitch's low-latency HLS implementation, resulting in significantly reduced latency at the potential cost of increased network sensitivity and potential playback stability issues.", "score": null, "retrieved_content": [{"name": "__init__", "is_method": true, "class_name": "KickHLSStreamReader", "parameters": ["self", "stream"], "calls": ["__init__", "max", "stream.session.options.set", "stream.session.options.set", "log.info", "min", "super", "stream.session.options.get"], "code_location": {"file": "kick.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 108, "end_line": 115}, "code_snippet": "    def __init__(self, stream: KickHLSStream, **kwargs):\n        if stream.low_latency:\n            live_edge = max(1, min(LOW_LATENCY_MAX_LIVE_EDGE, stream.session.options.get(\"hls-live-edge\")))\n            stream.session.options.set(\"hls-live-edge\", live_edge)\n            stream.session.options.set(\"hls-segment-stream-data\", True)\n            log.info(f\"Low latency streaming (HLS live edge: {live_edge})\")\n\n        super().__init__(stream, **kwargs)\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "TwitchHLSStreamReader", "parameters": ["self", "stream"], "calls": ["log.info", "__init__", "max", "stream.session.options.set", "stream.session.options.set", "log.info", "min", "super", "stream.session.options.get"], "code_location": {"file": "twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 243, "end_line": 251}, "code_snippet": "    def __init__(self, stream: TwitchHLSStream, **kwargs):\n        log.info(\"Will skip ad segments\")\n        if stream.low_latency:\n            live_edge = max(1, min(LOW_LATENCY_MAX_LIVE_EDGE, stream.session.options.get(\"hls-live-edge\")))\n            stream.session.options.set(\"hls-live-edge\", live_edge)\n            stream.session.options.set(\"hls-segment-stream-data\", True)\n            log.info(f\"Low latency streaming (HLS live edge: {live_edge})\")\n\n        super().__init__(stream, **kwargs)\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "KickHLSStreamWorker", "parameters": ["self"], "calls": ["__init__", "super"], "code_location": {"file": "kick.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 82, "end_line": 85}, "code_snippet": "    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if self.stream.low_latency:\n            self.reload_time = \"segment\"\n", "type": "function"}, {"name": "ChzzkHLSStream", "docstring": "Custom HLS stream that adds __bgda__ query parameter to segment URLs", "methods": ["__init__"], "attributes": ["__reader__"], "code_location": {"file": "chzzk.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 51, "end_line": 58}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "HLSStreamReader", "parameters": ["self", "stream", "name"], "calls": ["dict", "self.request_params.pop", "self.request_params.pop", "self.request_params.pop", "self.request_params.pop", "__init__", "super"], "code_location": {"file": "hls.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/hls", "start_line": 558, "end_line": 566}, "code_snippet": "    def __init__(self, stream: HLSStream, name: str | None = None):\n        self.request_params = dict(stream.args)\n        # These params are reserved for internal use\n        self.request_params.pop(\"exception\", None)\n        self.request_params.pop(\"stream\", None)\n        self.request_params.pop(\"timeout\", None)\n        self.request_params.pop(\"url\", None)\n\n        super().__init__(stream, name=name)\n", "type": "function"}, {"name": "HLSStream", "docstring": "Implementation of the Apple HTTP Live Streaming protocol.", "methods": ["__init__", "__json__", "to_manifest_url", "open", "_fetch_variant_playlist", "parse_variant_playlist"], "attributes": ["__shortname__"], "code_location": {"file": "hls.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/hls", "start_line": 638, "end_line": 909}, "type": "class"}, {"name": "SoopHLSStream", "docstring": "", "methods": [], "attributes": ["__reader__"], "code_location": {"file": "soop.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 35, "end_line": 36}, "type": "class"}, {"name": "EventedWriterHLSStream", "docstring": "", "methods": [], "attributes": ["__reader__"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 148, "end_line": 149}, "type": "class"}, {"name": "test_hls_low_latency_no_prefetch", "is_method": true, "class_name": "TestKickHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.await_write", "self.await_read", "Playlist", "Playlist", "call", "Segment", "Segment", "Segment", "Segment", "Segment", "Segment", "Segment", "Segment"], "code_location": {"file": "test_kick.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 114, "end_line": 129}, "code_snippet": "    def test_hls_low_latency_no_prefetch(self, mock_log):\n        self.subject(\n            [\n                Playlist(0, [Segment(0), Segment(1), Segment(2), Segment(3)]),\n                Playlist(4, [Segment(4), Segment(5), Segment(6), Segment(7)], end=True),\n            ],\n            streamoptions={\"low_latency\": True},\n        )\n\n        assert self.stream.low_latency\n\n        self.await_write(6)\n        self.await_read(read_all=True)\n        assert mock_log.info.mock_calls == [\n            call(\"Low latency streaming (HLS live edge: 2)\"),\n        ]\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "TwitchHLSStream", "parameters": ["self"], "calls": ["__init__", "super"], "code_location": {"file": "twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 258, "end_line": 260}, "code_snippet": "    def __init__(self, *args, low_latency: bool = False, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.low_latency = low_latency\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1097047328948975}
{"question": "How does the stream retrieval method implement multi-stage validation schema composition for extracting and processing embedded media delivery platform metadata?", "answer": "", "relative_code_list": null, "ground_truth": "The _get_streams method implements multi-stage validation through nested validate.Schema compositions: first extracting author and iframe_url via HTML parsing with XPath expressions and regex pattern matching, then processing the iframe response using JSON schema validation with regex extraction, string transformation, and structured data validation that handles both success and error response formats while ensuring proper type checking and fallback handling.", "score": null, "retrieved_content": [{"name": "_schema_v2_streams", "is_method": true, "class_name": "Bilibili", "parameters": ["self"], "calls": ["validate.all", "validate.filter", "validate.all", "validate.filter", "validate.all", "validate.filter", "validate.url"], "code_location": {"file": "bilibili.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 78, "end_line": 109}, "code_snippet": "    def _schema_v2_streams(self):\n        return validate.all(\n            [\n                {\n                    \"protocol_name\": str,\n                    \"format\": validate.all(\n                        [\n                            {\n                                \"format_name\": str,\n                                \"codec\": validate.all(\n                                    [\n                                        {\n                                            \"codec_name\": str,\n                                            \"base_url\": str,\n                                            \"url_info\": [\n                                                {\n                                                    \"host\": validate.url(),\n                                                    \"extra\": str,\n                                                },\n                                            ],\n                                        },\n                                    ],\n                                    validate.filter(lambda item: item[\"codec_name\"] == \"avc\"),\n                                ),\n                            },\n                        ],\n                        validate.filter(lambda item: item[\"format_name\"] in (\"fmp4\", \"ts\")),\n                    ),\n                },\n            ],\n            validate.filter(lambda item: item[\"protocol_name\"] == \"http_hls\"),\n        )\n", "type": "function"}, {"name": "_schema_streamingdata", "is_method": true, "class_name": "YouTube", "parameters": ["cls", "data"], "calls": ["validate.Schema", "schema.validate", "validate.get", "validate.union_get", "validate.optional", "validate.optional", "validate.optional", "validate.all", "validate.all", "validate.union_get", "validate.union_get", "validate.optional", "validate.url", "validate.optional", "validate.optional", "validate.all", "validate.url", "validate.regex", "validate.union_get", "re.compile"], "code_location": {"file": "youtube.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 206, "end_line": 244}, "code_snippet": "    def _schema_streamingdata(cls, data):\n        schema = validate.Schema(\n            {\n                \"streamingData\": {\n                    validate.optional(\"hlsManifestUrl\"): str,\n                    validate.optional(\"formats\"): [\n                        validate.all(\n                            {\n                                \"itag\": int,\n                                \"qualityLabel\": str,\n                                validate.optional(\"url\"): validate.url(scheme=\"http\"),\n                            },\n                            validate.union_get(\"url\", \"qualityLabel\"),\n                        ),\n                    ],\n                    validate.optional(\"adaptiveFormats\"): [\n                        validate.all(\n                            {\n                                \"itag\": int,\n                                \"mimeType\": validate.all(\n                                    str,\n                                    validate.regex(\n                                        re.compile(r\"\"\"^(?P<type>\\w+)/(?P<container>\\w+); codecs=\"(?P<codecs>.+)\"$\"\"\"),\n                                    ),\n                                    validate.union_get(\"type\", \"codecs\"),\n                                ),\n                                validate.optional(\"url\"): validate.url(scheme=\"http\"),\n                                validate.optional(\"qualityLabel\"): str,\n                            },\n                            validate.union_get(\"url\", \"qualityLabel\", \"itag\", \"mimeType\"),\n                        ),\n                    ],\n                },\n            },\n            validate.get(\"streamingData\"),\n            validate.union_get(\"hlsManifestUrl\", \"formats\", \"adaptiveFormats\"),\n        )\n        hls_manifest, formats, adaptive_formats = schema.validate(data)\n        return hls_manifest, formats or [], adaptive_formats or []\n", "type": "function"}, {"name": "_schema_data_ht", "is_method": true, "class_name": "CinerGroup", "parameters": [], "calls": ["validate.Schema", "validate.xml_xpath_string", "validate.none_or_all", "validate.parse_json", "validate.get", "validate.url"], "code_location": {"file": "cinergroup.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 54, "end_line": 64}, "code_snippet": "    def _schema_data_ht():\n        return validate.Schema(\n            validate.xml_xpath_string(\".//div[@data-ht][1]/@data-ht\"),\n            validate.none_or_all(\n                validate.parse_json(),\n                {\n                    \"ht_stream_m3u8\": validate.url(),\n                },\n                validate.get(\"ht_stream_m3u8\"),\n            ),\n        )\n", "type": "function"}, {"name": "_get_stream_data", "is_method": true, "class_name": "Vidio", "parameters": ["self"], "calls": ["self.session.http.get", "validate.Schema", "validate.parse_html", "validate.xml_find", "validate.union", "validate.get", "validate.get", "validate.all", "validate.get", "validate.get", "validate.get", "validate.transform"], "code_location": {"file": "vidio.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 52, "end_line": 69}, "code_snippet": "    def _get_stream_data(self):\n        return self.session.http.get(\n            self.url,\n            schema=validate.Schema(\n                validate.parse_html(),\n                validate.xml_find(\".//*[@data-video-id]\"),\n                validate.union((\n                    validate.get(\"data-video-id\"),\n                    validate.get(\"data-video-title\"),\n                    validate.all(\n                        validate.get(\"data-video-has-token\"),\n                        validate.transform(lambda val: val and val != \"false\"),\n                    ),\n                    validate.get(\"data-vjs-clip-hls-url\"),\n                    validate.get(\"data-vjs-clip-dash-url\"),\n                )),\n            ),\n        )\n", "type": "function"}, {"name": "_get_data", "is_method": true, "class_name": "Reuters", "parameters": ["self"], "calls": ["self.session.http.get", "validate.xml_findtext", "validate.all", "log.debug", "validate.Schema", "schema.validate", "log.debug", "validate.Schema", "schema.validate", "validate.get", "log.debug", "validate.Schema", "schema.validate", "log.debug", "validate.Schema", "schema.validate", "validate.Schema", "validate.xml_xpath_string", "validate.url", "validate.xml_findtext", "validate.parse_json", "validate.get", "validate.regex", "validate.get", "validate.parse_json", "validate.get", "validate.regex", "validate.get", "validate.parse_json", "validate.get", "validate.transform", "validate.get", "validate.parse_html", "validate.url", "validate.url", "re.compile", "re.compile", "next", "iter", "obj.keys"], "code_location": {"file": "reuters.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 23, "end_line": 88}, "code_snippet": "    def _get_data(self):\n        root = self.session.http.get(\n            self.url,\n            schema=validate.Schema(\n                validate.parse_html(),\n            ),\n        )\n\n        try:\n            log.debug(\"Trying to find source via meta tag\")\n            schema = validate.Schema(\n                validate.xml_xpath_string(\".//meta[@property='og:video'][1]/@content\"),\n                validate.url(),\n            )\n            return schema.validate(root)\n        except PluginError:\n            pass\n\n        try:\n            log.debug(\"Trying to find source via next-head\")\n            schema = validate.Schema(\n                validate.xml_findtext(\".//script[@type='application/ld+json'][@class='next-head']\"),\n                validate.parse_json(),\n                {\"contentUrl\": validate.url()},\n                validate.get(\"contentUrl\"),\n            )\n            return schema.validate(root)\n        except PluginError:\n            pass\n\n        schema_fusion = validate.xml_findtext(\".//script[@type='application/javascript'][@id='fusion-metadata']\")\n        schema_video = validate.all(\n            {\"source\": {\"hls\": validate.url()}},\n            validate.get((\"source\", \"hls\")),\n        )\n        try:\n            log.debug(\"Trying to find source via fusion-metadata globalContent\")\n            schema = validate.Schema(\n                schema_fusion,\n                validate.regex(re.compile(r\"Fusion\\s*\\.\\s*globalContent\\s*=\\s*(?P<json>{.+?})\\s*;\\s*Fusion\\s*\\.\", re.DOTALL)),\n                validate.get(\"json\"),\n                validate.parse_json(),\n                {\"result\": {\"related_content\": {\"videos\": list}}},\n                validate.get((\"result\", \"related_content\", \"videos\", 0)),\n                schema_video,\n            )\n            return schema.validate(root)\n        except PluginError:\n            pass\n\n        try:\n            log.debug(\"Trying to find source via fusion-metadata contentCache\")\n            schema = validate.Schema(\n                schema_fusion,\n                validate.regex(re.compile(r\"Fusion\\s*\\.\\s*contentCache\\s*=\\s*(?P<json>{.+?})\\s*;\\s*Fusion\\s*\\.\", re.DOTALL)),\n                validate.get(\"json\"),\n                validate.parse_json(),\n                {\"videohub-by-guid-v1\": {str: {\"data\": {\"result\": {\"videos\": list}}}}},\n                validate.get(\"videohub-by-guid-v1\"),\n                validate.transform(lambda obj: obj[next(iter((obj.keys())))]),\n                validate.get((\"data\", \"result\", \"videos\", 0)),\n                schema_video,\n            )\n            return schema.validate(root)\n        except PluginError:\n            pass\n", "type": "function"}, {"name": "_schema_videourl", "is_method": true, "class_name": "CinerGroup", "parameters": [], "calls": ["validate.Schema", "validate.xml_xpath_string", "validate.none_or_all", "re.compile", "validate.none_or_all", "validate.get", "validate.url"], "code_location": {"file": "cinergroup.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 41, "end_line": 51}, "code_snippet": "    def _schema_videourl():\n        return validate.Schema(\n            validate.xml_xpath_string(\".//script[contains(text(), 'videoUrl')]/text()\"),\n            validate.none_or_all(\n                re.compile(r\"\"\"(?<!//)\\s*var\\s+videoUrl\\s*=\\s*(?P<q>['\"])(?P<url>.+?)(?P=q)\"\"\"),\n                validate.none_or_all(\n                    validate.get(\"url\"),\n                    validate.url(),\n                ),\n            ),\n        )\n", "type": "function"}, {"name": "_get_streams", "is_method": true, "class_name": "CMMedia", "parameters": ["self"], "calls": ["self.session.http.get", "re.search", "m.group", "m.group", "self.session.http.get", "json.get", "self.is_restricted", "log.error", "log.error", "validate.Schema", "validate.Schema", "items", "validate.parse_html", "validate.union", "re.compile", "validate.none_or_all", "validate.get", "validate.transform", "validate.parse_json", "validate.any", "HLSStream.parse_variant_playlist", "validate.xml_xpath_string", "validate.all", "validate.xml_xpath_string", "validate.none_or_all", "re.sub", "re.compile", "validate.none_or_all", "validate.get", "validate.url", "validate.any"], "code_location": {"file": "cmmedia.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 38, "end_line": 126}, "code_snippet": "    def _get_streams(self):\n        self.author, iframe_url = self.session.http.get(\n            self.url,\n            schema=validate.Schema(\n                validate.parse_html(),\n                validate.union((\n                    validate.xml_xpath_string(\".//h1[contains(@class, 'btn-title')]/text()\"),\n                    validate.all(\n                        validate.xml_xpath_string(\".//script[contains(text(), '/embedIframeJs/')]/text()\"),\n                        validate.none_or_all(\n                            re.compile(r\"\"\"{getKs\\((?P<q>\")(?P<url>.+?)(?P=q)\"\"\"),\n                            validate.none_or_all(validate.get(\"url\"), validate.url()),\n                        ),\n                    ),\n                )),\n            ),\n        )\n        if not iframe_url:\n            return\n\n        m = re.search(r\"/p/(\\d+)/sp/(\\d+)/\", iframe_url)\n        if not m:\n            log.error(\"Failed to find partner IDs in IFRAME URL\")\n            return\n\n        p = m.group(1)\n        sp = m.group(2)\n\n        json = self.session.http.get(\n            iframe_url,\n            schema=validate.Schema(\n                re.compile(r\"twindow\\.kalturaIframePackageData\\s*=\\s*({.*});[\\\\nt]*var isIE8\"),\n                validate.none_or_all(\n                    validate.get(1),\n                    validate.transform(lambda text: re.sub(r'\\\\\"', r'\"', text)),\n                    validate.parse_json(),\n                    validate.any(\n                        {\n                            \"entryResult\": {\n                                \"contextData\": {\n                                    \"isSiteRestricted\": bool,\n                                    \"isCountryRestricted\": bool,\n                                    \"isSessionRestricted\": bool,\n                                    \"isIpAddressRestricted\": bool,\n                                    \"isUserAgentRestricted\": bool,\n                                    \"flavorAssets\": [\n                                        {\n                                            \"id\": str,\n                                        },\n                                    ],\n                                },\n                                \"meta\": {\n                                    \"id\": str,\n                                    \"name\": str,\n                                    \"categories\": validate.any(None, str),\n                                },\n                            },\n                        },\n                        {\"error\": str},\n                    ),\n                ),\n            ),\n        )\n\n        if not json:\n            return\n\n        if \"error\" in json:\n            log.error(f\"API error: {json['error']}\")\n            return\n\n        json = json.get(\"entryResult\")\n\n        if self.is_restricted(json[\"contextData\"]):\n            return\n\n        self.id = json[\"meta\"][\"id\"]\n        self.title = json[\"meta\"][\"name\"]\n        self.category = json[\"meta\"][\"categories\"]\n\n        for asset in json[\"contextData\"][\"flavorAssets\"]:\n            yield from HLSStream.parse_variant_playlist(\n                self.session,\n                (\n                    f\"https://cdnapisec.kaltura.com/p/{p}/sp/{sp}/playManifest/entryId/{json['meta']['id']}\"\n                    + f\"/flavorIds/{asset['id']}/format/applehttp/protocol/https/a.m3u8\"\n                ),\n                name_fmt=\"{pixels}_{bitrate}\",\n            ).items()\n", "type": "function"}, {"name": "_get_streams", "is_method": true, "class_name": "MDStrm", "parameters": ["self"], "calls": ["update_scheme", "log.debug", "self.session.http.get", "validate.Schema", "schema.validate", "validate.Schema", "self.get_script_str", "self.get_script_str", "self.get_script_str", "self.get_script_str", "self.get_script_str", "validate.Schema", "schema.validate", "log.trace", "HLSStream.parse_variant_playlist", "urlparse", "self.session.http.get", "validate.xml_xpath_string", "log.error", "validate.parse_json", "validate.xml_xpath_string", "update_scheme", "log.debug", "self.session.http.get", "validate.Schema", "isinstance", "log.debug", "validate.Schema", "validate.parse_html", "validate.url", "validate.Schema", "validate.parse_html", "validate.xml_xpath_string", "validate.parse_html", "validate.xml_xpath_string", "validate.none_or_all", "re.compile", "validate.none_or_all", "validate.get"], "code_location": {"file": "mdstrm.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 60, "end_line": 160}, "code_snippet": "    def _get_streams(self):\n        p_netloc = urlparse(self.url).netloc\n        if p_netloc == \"mdstrm.com\":\n            url_iframe = self.url\n        else:\n            url_iframe = self.session.http.get(\n                url=self.url,\n                schema=validate.Schema(\n                    validate.parse_html(),\n                    validate.xml_xpath_string(\"normalize-space(.//iframe[contains(@src,'mdstrm.com')]/@src)\"),\n                ),\n            )\n            if not url_iframe:\n                return\n\n        url_iframe = update_scheme(\"https://\", url_iframe, force=False)\n        log.debug(f\"iframe={url_iframe}\")\n        root = self.session.http.get(\n            url_iframe,\n            schema=validate.Schema(validate.parse_html()),\n        )\n\n        schema = validate.Schema(validate.xml_xpath_string(\".//div[@id='message']/text()\"))\n        error_msg = schema.validate(root)\n        if error_msg:\n            log.error(f\"{error_msg}\")\n\n        schema_options = validate.Schema(\n            validate.parse_json(),\n            {\n                \"id\": str,\n                \"isOnline\": bool,\n                \"src\": {\"hls\": validate.url()},\n                \"type\": str,\n                \"without_cookies\": bool,\n                \"title\": str,\n            },\n        )\n        options = self.get_script_str(\n            root,\n            \"window.MDSTRM.OPTIONS\",\n            r\"window\\.MDSTRM\\.OPTIONS\\s*=\\s*({.*?});\",\n            custom_schema=schema_options,\n        )\n        if not options or not isinstance(options, dict):\n            return\n\n        sid = self.get_script_str(root, \"window.MDSTRMSID\")\n        pid = self.get_script_str(root, \"window.MDSTRMPID\")\n        uid = self.get_script_str(root, \"window.MDSTRMUID\")\n        av = self.get_script_str(root, \"window.VERSION\")\n        if not (sid and pid and uid and av):\n            return\n\n        params = {\n            \"sid\": sid,\n            \"uid\": uid,\n            \"pid\": pid,\n            \"av\": av,\n            \"an\": \"screen\",\n            \"at\": \"web-app\",\n            \"res\": \"1280x720\",\n            \"dnt\": \"true\",\n            \"without_cookies\": \"false\",\n        }\n\n        schema = validate.Schema(\n            validate.xml_xpath_string(\n                \"normalize-space(.//iframe[contains(@src,'mdstrm.com')][@id='programmatic']/@src)\",\n            ),\n        )\n        programmatic_url = schema.validate(root)\n        if programmatic_url:\n            programmatic_url = update_scheme(\"https://\", programmatic_url, force=False)\n            log.debug(f\"programmatic_url={programmatic_url}\")\n\n            ad = self.session.http.get(\n                programmatic_url,\n                schema=validate.Schema(\n                    validate.parse_html(),\n                    validate.xml_xpath_string(\".//script[contains(text(),'parent._dai_session')]/text()\"),\n                    validate.none_or_all(\n                        re.compile(r\"\"\"parent\\._dai_session\\s*=\\s*(?P<q>['\"])(?P<dai_session>.+?)(?P=q);\"\"\"),\n                        validate.none_or_all(validate.get(\"dai_session\")),\n                    ),\n                ),\n            )\n            if ad:\n                params[\"adInsertionSessionId\"] = ad\n            else:\n                log.debug(\"Failed to find 'parent._dai_session'\")\n\n        log.trace(f\"{params!r}\")\n        self.id = options[\"id\"]\n        self.title = options[\"title\"]\n        return HLSStream.parse_variant_playlist(\n            self.session,\n            options[\"src\"][\"hls\"],\n            headers={\"Referer\": \"https://mdstrm.com/\"},\n            params=params,\n        )\n", "type": "function"}, {"name": "_get_streams", "is_method": true, "class_name": "NOS", "parameters": ["self"], "calls": ["self.session.http.get", "self.session.http.get", "HLSStream.parse_variant_playlist", "log.error", "validate.Schema", "validate.parse_html", "validate.xml_xpath_string", "validate.none_or_all", "validate.parse_json", "validate.union_get", "validate.any", "validate.url", "validate.any", "validate.all", "validate.contains"], "code_location": {"file": "nos.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 25, "end_line": 64}, "code_snippet": "    def _get_streams(self):\n        data = self.session.http.get(\n            self.url,\n            schema=validate.Schema(\n                validate.parse_html(),\n                validate.xml_xpath_string(\".//script[@type='application/ld+json'][1]/text()\"),\n                validate.none_or_all(\n                    validate.parse_json(),\n                    {\n                        \"@type\": validate.any(\n                            \"VideoObject\",\n                            validate.all(\n                                list,\n                                validate.contains(\"VideoObject\"),\n                            ),\n                        ),\n                        \"encodingFormat\": \"application/vnd.apple.mpegurl\",\n                        \"contentUrl\": validate.url(),\n                        \"identifier\": validate.any(int, str),\n                        \"name\": str,\n                    },\n                    validate.union_get(\n                        \"contentUrl\",\n                        \"identifier\",\n                        \"name\",\n                    ),\n                ),\n            ),\n        )\n        if not data:\n            return\n\n        hls_url, self.id, self.title = data\n\n        res = self.session.http.get(hls_url, raise_for_status=False)\n        if res.status_code >= 400:\n            log.error(\"Content is inaccessible or may have expired\")\n            return\n\n        return HLSStream.parse_variant_playlist(self.session, hls_url)\n", "type": "function"}, {"name": "_schema_videodetails", "is_method": true, "class_name": "YouTube", "parameters": ["cls", "data"], "calls": ["validate.Schema", "schema.validate", "log.trace", "validate.union_get", "validate.all", "validate.optional", "validate.optional", "validate.optional", "validate.optional", "validate.optional", "validate.transform", "validate.transform", "validate.transform", "validate.transform", "validate.transform", "validate.any", "validate.all", "validate.all", "validate.get", "validate.get"], "code_location": {"file": "youtube.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 164, "end_line": 203}, "code_snippet": "    def _schema_videodetails(cls, data):\n        schema = validate.Schema(\n            {\n                \"videoDetails\": {\n                    \"videoId\": str,\n                    \"author\": str,\n                    \"title\": str,\n                    validate.optional(\"isLive\"): validate.transform(bool),\n                    validate.optional(\"isLiveContent\"): validate.transform(bool),\n                    validate.optional(\"isLiveDvrEnabled\"): validate.transform(bool),\n                    validate.optional(\"isLowLatencyLiveStream\"): validate.transform(bool),\n                    validate.optional(\"isPrivate\"): validate.transform(bool),\n                },\n                \"microformat\": validate.all(\n                    validate.any(\n                        validate.all(\n                            {\"playerMicroformatRenderer\": dict},\n                            validate.get(\"playerMicroformatRenderer\"),\n                        ),\n                        validate.all(\n                            {\"microformatDataRenderer\": dict},\n                            validate.get(\"microformatDataRenderer\"),\n                        ),\n                    ),\n                    {\n                        \"category\": str,\n                    },\n                ),\n            },\n            validate.union_get(\n                (\"videoDetails\", \"videoId\"),\n                (\"videoDetails\", \"author\"),\n                (\"microformat\", \"category\"),\n                (\"videoDetails\", \"title\"),\n                (\"videoDetails\", \"isLive\"),\n            ),\n        )\n        videoDetails = schema.validate(data)\n        log.trace(f\"videoDetails = {videoDetails!r}\")\n        return videoDetails\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.0972974300384521}
{"question": "How can a prefetching mechanism for adaptive streaming protocol stream segments be designed to integrate with the existing segment data structure hierarchy while maintaining backward compatibility?", "answer": "", "relative_code_list": null, "ground_truth": "The SegmentPrefetch class extends the base Segment class and implements a specialized build method that generates the #EXT-X-TWITCH-PREFETCH tag format. This design maintains compatibility with the existing segment hierarchy while adding Twitch-specific prefetching functionality. The implementation should ensure proper namespace handling through the url method inheritance and maintain the same interface as other segment types to allow seamless integration into HLS playlist generation pipelines.", "score": null, "retrieved_content": [{"name": "SegmentPrefetch", "docstring": "", "methods": [], "attributes": [], "code_location": {"file": "test_kick.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 42, "end_line": 44}, "type": "class"}, {"name": "SegmentPrefetch", "docstring": "", "methods": ["build", "build"], "attributes": [], "code_location": {"file": "test_twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 161, "end_line": 163}, "type": "class"}, {"name": "parse_tag_ext_x_twitch_prefetch", "is_method": true, "class_name": "TwitchM3U8Parser", "parameters": ["self", "value"], "calls": ["parse_tag", "dataclass_replace", "segments.append", "timedelta", "self._is_segment_ad", "sum", "float", "self.uri", "len"], "code_location": {"file": "twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 93, "end_line": 128}, "code_snippet": "    def parse_tag_ext_x_twitch_prefetch(self, value):\n        segments = self.m3u8.segments\n        if not segments:  # pragma: no cover\n            return\n        last = segments[-1]\n\n        # Use the average duration of all regular segments for the duration of prefetch segments.\n        # This is better than using the duration of the last segment when regular segment durations vary a lot.\n        # In low latency mode, the playlist reload time is the duration of the last segment.\n        duration = last.duration if last.prefetch else sum(segment.duration for segment in segments) / float(len(segments))\n\n        # Use the last duration for extrapolating the start time of the prefetch segment, which is needed for checking\n        # whether it is an ad segment and matches the parsed date ranges or not\n        date = last.date + timedelta(seconds=last.duration)\n\n        # Always treat prefetch segments after a discontinuity as ad segments\n        # (discontinuity tag inserted after last regular segment)\n        # Don't reset discontinuity state: the date extrapolation might be inaccurate,\n        # so all following prefetch segments should be considered an ad after a discontinuity\n        ad = self._discontinuity or self._is_segment_ad(date)\n\n        # Since we don't reset the discontinuity state in prefetch segments for the purpose of ad detection,\n        # set the prefetch segment's discontinuity attribute based on ad transitions\n        discontinuity = ad != last.ad\n\n        segment = dataclass_replace(\n            last,\n            uri=self.uri(value),\n            duration=duration,\n            title=None,\n            discontinuity=discontinuity,\n            date=date,\n            ad=ad,\n            prefetch=True,\n        )\n        segments.append(segment)\n", "type": "function"}, {"name": "parse_tag_ext_x_prefetch", "is_method": true, "class_name": "KickM3U8Parser", "parameters": ["self", "value"], "calls": ["parse_tag", "dataclass_replace", "segments.append", "sum", "float", "self.uri", "len"], "code_location": {"file": "kick.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 56, "end_line": 74}, "code_snippet": "    def parse_tag_ext_x_prefetch(self, value):\n        segments = self.m3u8.segments\n        if not segments:  # pragma: no cover\n            return\n        last = segments[-1]\n\n        # Use the average duration of all regular segments for the duration of prefetch segments.\n        # This is better than using the duration of the last segment when regular segment durations vary a lot.\n        # In low latency mode, the playlist reload time is the duration of the last segment.\n        duration = last.duration if last.prefetch else sum(segment.duration for segment in segments) / float(len(segments))\n\n        segment = dataclass_replace(\n            last,\n            uri=self.uri(value),\n            duration=duration,\n            title=None,\n            prefetch=True,\n        )\n        segments.append(segment)\n", "type": "function"}, {"name": "test_hls_no_low_latency_has_prefetch", "is_method": true, "class_name": "TestTwitchHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.await_write", "self.await_read", "all", "self.session.options.get", "self.session.options.get", "self.content", "any", "Playlist", "Playlist", "self.called", "call", "segments.values", "self.called", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "segments.values"], "code_location": {"file": "test_twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 395, "end_line": 415}, "code_snippet": "    def test_hls_no_low_latency_has_prefetch(self, mock_log):\n        segments = self.subject(\n            [\n                Playlist(0, [Segment(0), Segment(1), Segment(2), Segment(3), SegmentPrefetch(4), SegmentPrefetch(5)]),\n                Playlist(4, [Segment(4), Segment(5), Segment(6), Segment(7), SegmentPrefetch(8), SegmentPrefetch(9)], end=True),\n            ],\n            streamoptions={\"low_latency\": False},\n        )\n\n        assert self.session.options.get(\"hls-live-edge\") == 4\n        assert not self.session.options.get(\"hls-segment-stream-data\")\n\n        self.await_write(8)\n        data = self.await_read(read_all=True)\n        assert data == self.content(segments, cond=lambda s: s.num < 8), \"Ignores prefetch segments\"\n        assert all(self.called(s) for s in segments.values() if s.num <= 7), \"Ignores prefetch segments\"\n        assert not any(self.called(s) for s in segments.values() if s.num > 7), \"Ignores prefetch segments\"\n        assert mock_log.info.mock_calls == [\n            call(\"Will skip ad segments\"),\n        ]\n        assert self.thread.reader.worker._reload_time == 3.0\n", "type": "function"}, {"name": "get_segment", "is_method": true, "class_name": "TwitchM3U8Parser", "parameters": ["self", "uri"], "calls": ["self._is_segment_ad", "get_segment", "super"], "code_location": {"file": "twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 138, "end_line": 151}, "code_snippet": "    def get_segment(self, uri: str, **data) -> TwitchHLSSegment:\n        ad = self._is_segment_ad(self._date, self._extinf.title if self._extinf else None)\n        segment: TwitchHLSSegment = super().get_segment(uri, ad=ad, prefetch=False)  # type: ignore[assignment]\n\n        # Special case where Twitch incorrectly inserts discontinuity tags between segments of the live content\n        if (\n            segment.discontinuity\n            and not segment.ad\n            and self.m3u8.segments\n            and not self.m3u8.segments[-1].ad\n        ):  # fmt: skip\n            segment.discontinuity = False\n\n        return segment\n", "type": "function"}, {"name": "test_hls_low_latency_has_prefetch", "is_method": true, "class_name": "TestTwitchHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.session.options.get", "self.await_write", "self.await_read", "all", "self.session.options.get", "self.content", "any", "Playlist", "Playlist", "self.called", "call", "call", "self.called", "segments.values", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "segments.values"], "code_location": {"file": "test_twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 372, "end_line": 392}, "code_snippet": "    def test_hls_low_latency_has_prefetch(self, mock_log):\n        segments = self.subject(\n            [\n                Playlist(0, [Segment(0), Segment(1), Segment(2), Segment(3), SegmentPrefetch(4), SegmentPrefetch(5)]),\n                Playlist(4, [Segment(4), Segment(5), Segment(6), Segment(7), SegmentPrefetch(8), SegmentPrefetch(9)], end=True),\n            ],\n            streamoptions={\"low_latency\": True},\n        )\n\n        assert self.session.options.get(\"hls-live-edge\") == 2\n        assert self.session.options.get(\"hls-segment-stream-data\")\n\n        self.await_write(6)\n        data = self.await_read(read_all=True)\n        assert data == self.content(segments, cond=lambda s: s.num >= 4), \"Skips first four segments due to reduced live-edge\"\n        assert not any(self.called(s) for s in segments.values() if s.num < 4), \"Doesn't download old segments\"\n        assert all(self.called(s) for s in segments.values() if s.num >= 4), \"Downloads all remaining segments\"\n        assert mock_log.info.mock_calls == [\n            call(\"Will skip ad segments\"),\n            call(\"Low latency streaming (HLS live edge: 2)\"),\n        ]\n", "type": "function"}, {"name": "test_hls_no_low_latency_has_prefetch", "is_method": true, "class_name": "TestKickHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.await_write", "self.await_read", "all", "self.session.options.get", "self.session.options.get", "self.content", "any", "Playlist", "Playlist", "self.called", "segments.values", "self.called", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "segments.values"], "code_location": {"file": "test_kick.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 93, "end_line": 111}, "code_snippet": "    def test_hls_no_low_latency_has_prefetch(self, mock_log):\n        segments = self.subject(\n            [\n                Playlist(0, [Segment(0), Segment(1), Segment(2), Segment(3), SegmentPrefetch(4), SegmentPrefetch(5)]),\n                Playlist(4, [Segment(4), Segment(5), Segment(6), Segment(7), SegmentPrefetch(8), SegmentPrefetch(9)], end=True),\n            ],\n            streamoptions={\"low_latency\": False},\n        )\n\n        assert self.session.options.get(\"hls-live-edge\") == 4\n        assert not self.session.options.get(\"hls-segment-stream-data\")\n\n        self.await_write(8)\n        data = self.await_read(read_all=True)\n        assert data == self.content(segments, cond=lambda s: s.num < 8), \"Ignores prefetch segments\"\n        assert all(self.called(s) for s in segments.values() if s.num <= 7), \"Ignores prefetch segments\"\n        assert not any(self.called(s) for s in segments.values() if s.num > 7), \"Ignores prefetch segments\"\n        assert mock_log.info.mock_calls == []\n        assert self.thread.reader.worker._reload_time == 3.0\n", "type": "function"}, {"name": "test_hls_low_latency_has_prefetch_no_preroll_with_prefetch_ads", "is_method": true, "class_name": "TestTwitchHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "Tag", "self.subject", "self.await_write", "self.await_read", "Tag", "TagDateRangeAd", "self.content", "Playlist", "Playlist", "Playlist", "Playlist", "Playlist", "Playlist", "Playlist", "Playlist", "call", "call", "call", "timedelta", "Seg", "Seg", "Pre", "Pre", "Seg", "Pre", "Pre", "Pre", "Seg", "Seg", "Seg", "Seg", "Seg", "Seg", "Seg", "Seg", "Seg", "Seg", "Seg", "Seg", "Pre", "Pre", "Seg", "Seg", "Seg", "Seg", "Pre", "Pre", "Seg", "Seg", "Seg", "Seg", "Pre", "Pre", "Seg", "Seg", "Seg", "Seg", "Pre", "Pre"], "code_location": {"file": "test_twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 461, "end_line": 501}, "code_snippet": "    def test_hls_low_latency_has_prefetch_no_preroll_with_prefetch_ads(self, mock_log):\n        # segment 1 has a shorter duration, to mess with the extrapolation of the prefetch start times\n        # segments 3-6 are ads\n        Seg, Pre = Segment, SegmentPrefetch\n        ads = [\n            Tag(\"EXT-X-DISCONTINUITY\"),\n            TagDateRangeAd(\n                start=DATETIME_BASE + timedelta(seconds=3),\n                duration=4,\n                custom={\"X-TV-TWITCH-AD-ROLL-TYPE\": \"MIDROLL\"},\n            ),\n        ]\n        tls = Tag(\"EXT-X-TWITCH-LIVE-SEQUENCE\", 7)\n        # noinspection PyTypeChecker\n        segments = self.subject(\n            [\n                # regular stream data with prefetch segments\n                Playlist(0, [Seg(0), Seg(1, duration=0.5), Pre(2), Pre(3)]),\n                # three prefetch segments, one regular (2) and two ads (3 and 4)\n                Playlist(1, [Seg(1, duration=0.5), Pre(2), *ads, Pre(3), Pre(4)]),\n                # all prefetch segments are gone once regular prefetch segments have shifted\n                Playlist(2, [Seg(2, duration=1.5), *ads, Seg(3), Seg(4), Seg(5)]),\n                # still no prefetch segments while ads are playing\n                Playlist(3, [*ads, Seg(3), Seg(4), Seg(5), Seg(6)]),\n                # new prefetch segments on the first regular segment occurrence\n                Playlist(4, [*ads, Seg(4), Seg(5), Seg(6), tls, Seg(7), Pre(8), Pre(9)]),\n                Playlist(5, [*ads, Seg(5), Seg(6), tls, Seg(7), Seg(8), Pre(9), Pre(10)]),\n                Playlist(6, [*ads, Seg(6), tls, Seg(7), Seg(8), Seg(9), Pre(10), Pre(11)]),\n                Playlist(7, [Seg(7), Seg(8), Seg(9), Seg(10), Pre(11), Pre(12)], end=True),\n            ],\n            streamoptions={\"low_latency\": True},\n        )\n\n        self.await_write(11)\n        data = self.await_read(read_all=True)\n        assert data == self.content(segments, cond=lambda s: 2 <= s.num <= 3 or 7 <= s.num)\n        assert mock_log.info.mock_calls == [\n            call(\"Will skip ad segments\"),\n            call(\"Low latency streaming (HLS live edge: 2)\"),\n            call(\"Detected advertisement break of 4 seconds\"),\n        ]\n", "type": "function"}, {"name": "test_hls_low_latency_has_prefetch", "is_method": true, "class_name": "TestKickHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.session.options.get", "self.await_write", "self.await_read", "all", "self.session.options.get", "self.content", "any", "Playlist", "Playlist", "self.called", "call", "self.called", "segments.values", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "segments.values"], "code_location": {"file": "test_kick.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 71, "end_line": 90}, "code_snippet": "    def test_hls_low_latency_has_prefetch(self, mock_log):\n        segments = self.subject(\n            [\n                Playlist(0, [Segment(0), Segment(1), Segment(2), Segment(3), SegmentPrefetch(4), SegmentPrefetch(5)]),\n                Playlist(4, [Segment(4), Segment(5), Segment(6), Segment(7), SegmentPrefetch(8), SegmentPrefetch(9)], end=True),\n            ],\n            streamoptions={\"low_latency\": True},\n        )\n\n        assert self.session.options.get(\"hls-live-edge\") == 2\n        assert self.session.options.get(\"hls-segment-stream-data\")\n\n        self.await_write(6)\n        data = self.await_read(read_all=True)\n        assert data == self.content(segments, cond=lambda s: s.num >= 4), \"Skips first four segments due to reduced live-edge\"\n        assert not any(self.called(s) for s in segments.values() if s.num < 4), \"Doesn't download old segments\"\n        assert all(self.called(s) for s in segments.values() if s.num >= 4), \"Downloads all remaining segments\"\n        assert mock_log.info.mock_calls == [\n            call(\"Low latency streaming (HLS live edge: 2)\"),\n        ]\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.087606430053711}
{"question": "How does the test method that verifies error handling when plugin module execution fails simulate and verify plugin loading failure when the module execution function raises ImportError during lazy URL matching?", "answer": "", "relative_code_list": null, "ground_truth": "The test_fail_builtin method uses monkeypatch to replace streamlink.session.plugins.exec_module with a Mock object configured to raise ImportError when called. This simulates a scenario where the plugin module cannot be imported during lazy loading. The test then verifies that session.plugins.match_url returns None, indicating no plugin was found, checks that session.plugins.get_loaded() returns an empty dictionary confirming no plugin was loaded, and validates the expected debug and error log messages are recorded through caplog to ensure proper error handling and logging during plugin loading failure.", "score": null, "retrieved_content": [{"name": "test_fail_builtin", "is_method": true, "class_name": "TestMatchURLLoadLazy", "parameters": ["self", "monkeypatch", "caplog", "session"], "calls": ["pytest.mark.usefixtures", "str", "Mock", "monkeypatch.setattr", "session.plugins.match_url", "session.plugins.get_loaded", "ImportError"], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/session", "start_line": 623, "end_line": 633}, "code_snippet": "    def test_fail_builtin(self, monkeypatch: pytest.MonkeyPatch, caplog: pytest.LogCaptureFixture, session: Streamlink):\n        path = str(PATH_TESTPLUGINS / \"testplugin.py\")\n        mock = Mock(side_effect=ImportError(\"\", path=path))\n        monkeypatch.setattr(\"streamlink.session.plugins.exec_module\", mock)\n\n        assert session.plugins.match_url(\"http://test.se\") is None\n        assert session.plugins.get_loaded() == {}\n        assert [(record.name, record.levelname, record.message) for record in caplog.records] == [\n            (\"streamlink.session\", \"debug\", \"Loading plugin: testplugin\"),\n            (\"streamlink.session\", \"error\", f\"Failed to load plugin testplugin from {path}\\n\"),\n        ]\n", "type": "function"}, {"name": "test_importerror", "is_method": true, "class_name": "TestLoad", "parameters": ["self", "monkeypatch", "caplog", "session"], "calls": ["monkeypatch.setattr", "Mock", "session.plugins.load_path", "session.plugins.get_names", "bool"], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/session", "start_line": 177, "end_line": 201}, "code_snippet": "    def test_importerror(self, monkeypatch: pytest.MonkeyPatch, caplog: pytest.LogCaptureFixture, session: Streamlink):\n        monkeypatch.setattr(\"importlib.machinery.FileFinder.find_spec\", Mock(return_value=None))\n        assert not session.plugins.load_path(PATH_TESTPLUGINS)\n        assert \"testplugin\" not in session.plugins\n        assert session.plugins.get_names() == []\n        assert [(record.name, record.levelname, record.message, bool(record.exc_info)) for record in caplog.records] == [\n            (\n                \"streamlink.session\",\n                \"error\",\n                f\"Failed to load plugin testplugin from {PATH_TESTPLUGINS}\\n\",\n                True,\n            ),\n            (\n                \"streamlink.session\",\n                \"error\",\n                f\"Failed to load plugin testplugin_invalid from {PATH_TESTPLUGINS}\\n\",\n                True,\n            ),\n            (\n                \"streamlink.session\",\n                \"error\",\n                f\"Failed to load plugin testplugin_missing from {PATH_TESTPLUGINS}\\n\",\n                True,\n            ),\n        ]\n", "type": "function"}, {"name": "test_matchers_failure", "is_method": true, "class_name": "TestLoadPluginsData", "parameters": ["self", "caplog", "session", "pluginsdata"], "calls": ["pytest.mark.parametrize", "session.plugins.get_names", "pytest.param", "session.plugins.iter_matchers", "caplog.get_records"], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/session", "start_line": 391, "end_line": 399}, "code_snippet": "    def test_matchers_failure(self, caplog: pytest.LogCaptureFixture, session: Streamlink, pluginsdata: str):\n        assert \"fake\" in session.plugins\n        assert \"success\" not in session.plugins\n        assert \"fail\" not in session.plugins\n        assert session.plugins.get_names() == [\"fake\"]\n        assert [name for name, matchers in session.plugins.iter_matchers()] == [\"fake\"]\n        assert [(record.name, record.levelname, record.message) for record in caplog.get_records(when=\"setup\")] == [\n            (\"streamlink.session\", \"error\", \"Error while loading pluginmatcher data from JSON\"),\n        ]\n", "type": "function"}, {"name": "test_matchers_not_matching", "is_method": true, "class_name": "TestPluginMatcher", "parameters": ["self"], "calls": ["pluginmatcher", "re.compile", "pytest.raises", "MyPlugin", "Mock"], "code_location": {"file": "test_plugin.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 207, "end_line": 213}, "code_snippet": "    def test_matchers_not_matching(self):\n        @pluginmatcher(re.compile(r\"http://foo\"))\n        class MyPlugin(FakePlugin):\n            pass\n\n        with pytest.raises(PluginError, match=r\"^The input URL did not match any of this plugin's matchers$\"):\n            MyPlugin(Mock(), \"http://bar\")\n", "type": "function"}, {"name": "test_load", "is_method": true, "class_name": "TestMatchURLLoadLazy", "parameters": ["self", "caplog", "session"], "calls": ["pytest.mark.usefixtures", "session.plugins.match_url", "session.plugins.get_loaded"], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/session", "start_line": 608, "end_line": 620}, "code_snippet": "    def test_load(self, caplog: pytest.LogCaptureFixture, session: Streamlink):\n        lookup = session.plugins.match_url(\"http://test.se\")\n        assert lookup is not None\n        name, plugin = lookup\n        assert name == \"testplugin\"\n        # can't compare plugin classes here due to exec_module(), so just compare module name, matchers and arguments\n        assert plugin.__module__ == \"streamlink.plugins.testplugin\"\n        assert plugin.matchers == _TestPlugin.matchers\n        assert plugin.arguments == _TestPlugin.arguments\n        assert \"testplugin\" in session.plugins.get_loaded()\n        assert [(record.name, record.levelname, record.message) for record in caplog.records] == [\n            (\"streamlink.session\", \"debug\", \"Loading plugin: testplugin\"),\n        ]\n", "type": "function"}, {"name": "test_syntaxerror", "is_method": true, "class_name": "TestLoad", "parameters": ["self", "monkeypatch", "caplog", "session"], "calls": ["monkeypatch.setattr", "Mock", "pytest.raises", "session.plugins.load_path", "session.plugins.get_names"], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/session", "start_line": 203, "end_line": 208}, "code_snippet": "    def test_syntaxerror(self, monkeypatch: pytest.MonkeyPatch, caplog: pytest.LogCaptureFixture, session: Streamlink):\n        monkeypatch.setattr(\"importlib.machinery.SourceFileLoader.exec_module\", Mock(side_effect=SyntaxError))\n        with pytest.raises(SyntaxError):\n            session.plugins.load_path(PATH_TESTPLUGINS)\n        assert session.plugins.get_names() == []\n        assert caplog.record_tuples == []\n", "type": "function"}, {"name": "test_loaded", "is_method": true, "class_name": "TestMatchURLLoadLazy", "parameters": ["self", "caplog", "session"], "calls": ["pytest.mark.usefixtures", "session.plugins.match_url"], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/session", "start_line": 603, "end_line": 605}, "code_snippet": "    def test_loaded(self, caplog: pytest.LogCaptureFixture, session: Streamlink):\n        assert session.plugins.match_url(\"http://test.se\") == (\"testplugin\", _TestPlugin)\n        assert [(record.name, record.levelname, record.message) for record in caplog.records] == []\n", "type": "function"}, {"name": "test_load_module_importerror", "is_method": false, "class_name": null, "parameters": ["name", "path", "expected"], "calls": ["pytest.mark.parametrize", "pytest.raises", "load_module", "pytest.param", "pytest.param", "ImportError", "ImportError", "str", "str"], "code_location": {"file": "test_module.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/utils", "start_line": 39, "end_line": 44}, "code_snippet": "def test_load_module_importerror(name: str, path: Path, expected: ImportError):\n    with pytest.raises(ImportError) as cm:\n        load_module(name, path)\n    assert cm.value.msg == expected.msg\n    assert cm.value.name == expected.name\n    assert cm.value.path == expected.path\n", "type": "function"}, {"name": "test_url_setter", "is_method": true, "class_name": "TestPluginMatcher", "parameters": ["self"], "calls": ["pluginmatcher", "pluginmatcher", "pluginmatcher", "MyPlugin", "re.compile", "re.compile", "re.compile", "Mock", "plugin.match.group", "plugin.match.group", "plugin.match.group"], "code_location": {"file": "test_plugin.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 219, "end_line": 248}, "code_snippet": "    def test_url_setter(self):\n        @pluginmatcher(re.compile(r\"http://(foo)\"))\n        @pluginmatcher(re.compile(r\"http://(bar)\"))\n        @pluginmatcher(re.compile(r\"http://(baz)\"))\n        class MyPlugin(FakePlugin):\n            pass\n\n        plugin = MyPlugin(Mock(), \"http://foo\")\n        assert plugin.url == \"http://foo\"\n        assert [m is not None for m in plugin.matches] == [True, False, False]\n        assert plugin.matcher is plugin.matchers[0].pattern\n        assert plugin.match.group(1) == \"foo\"\n\n        plugin.url = \"http://bar\"\n        assert plugin.url == \"http://bar\"\n        assert [m is not None for m in plugin.matches] == [False, True, False]\n        assert plugin.matcher is plugin.matchers[1].pattern\n        assert plugin.match.group(1) == \"bar\"\n\n        plugin.url = \"http://baz\"\n        assert plugin.url == \"http://baz\"\n        assert [m is not None for m in plugin.matches] == [False, False, True]\n        assert plugin.matcher is plugin.matchers[2].pattern\n        assert plugin.match.group(1) == \"baz\"\n\n        plugin.url = \"http://qux\"\n        assert plugin.url == \"http://qux\"\n        assert [m is not None for m in plugin.matches] == [False, False, False]\n        assert plugin.matcher is None\n        assert plugin.match is None\n", "type": "function"}, {"name": "test_priority", "is_method": true, "class_name": "TestMatchURL", "parameters": ["self", "session"], "calls": ["pluginmatcher", "pluginmatcher", "pluginmatcher", "pluginmatcher", "session.plugins.update", "session.plugins.match_url", "session.plugins.match_url", "session.plugins.match_url", "session.plugins.match_url", "session.plugins.match_url", "session.plugins.match_url", "session.plugins.match_url", "session.plugins.match_url", "re.compile", "re.compile", "re.compile", "re.compile"], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/session", "start_line": 537, "end_line": 571}, "code_snippet": "    def test_priority(self, session: Streamlink):\n        @pluginmatcher(priority=HIGH_PRIORITY, pattern=re.compile(r\"^(high|normal|low|no)$\"))\n        class HighPriority(_Plugin):\n            pass\n\n        @pluginmatcher(priority=NORMAL_PRIORITY, pattern=re.compile(r\"^(normal|low|no)$\"))\n        class NormalPriority(_Plugin):\n            pass\n\n        @pluginmatcher(priority=LOW_PRIORITY, pattern=re.compile(r\"^(low|no)$\"))\n        class LowPriority(_Plugin):\n            pass\n\n        @pluginmatcher(priority=NO_PRIORITY, pattern=re.compile(r\"^no$\"))\n        class NoPriority(_Plugin):\n            pass\n\n        session.plugins.update({\n            \"high\": HighPriority,\n            \"normal\": NormalPriority,\n            \"low\": LowPriority,\n            \"no\": NoPriority,\n        })\n\n        assert session.plugins.match_url(\"no\") == (\"high\", HighPriority)\n        assert session.plugins.match_url(\"low\") == (\"high\", HighPriority)\n        assert session.plugins.match_url(\"normal\") == (\"high\", HighPriority)\n        assert session.plugins.match_url(\"high\") == (\"high\", HighPriority)\n\n        del session.plugins[\"high\"]\n\n        assert session.plugins.match_url(\"no\") == (\"normal\", NormalPriority)\n        assert session.plugins.match_url(\"low\") == (\"normal\", NormalPriority)\n        assert session.plugins.match_url(\"normal\") == (\"normal\", NormalPriority)\n        assert session.plugins.match_url(\"high\") is None\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.104395866394043}
{"question": "What specific data dependencies must the drag-and-drop data structure resolve to properly instantiate the intercepted drag operation event when the method that enables drag interception is enabled?", "answer": "", "relative_code_list": null, "ground_truth": "The DragIntercepted class depends on the DragData class to provide the complete drag event data structure, which includes drag data items, drag operations, and potentially files or custom data. The DragData.from_json() method must successfully parse and validate the JSON payload containing drag-related information such as drag items (with mimeType, data, and baseURL), drag operations bitmask, and any files involved. This dependency requires that DragData properly handles the serialization format and data validation to ensure DragIntercepted events contain accurate drag state information for Input.dispatchDragEvent to restore normal drag and drop behavior.", "score": null, "retrieved_content": [{"name": "DragIntercepted", "docstring": "**EXPERIMENTAL**\n\nEmitted only when ``Input.setInterceptDrags`` is enabled. Use this data with ``Input.dispatchDragEvent`` to\nrestore normal drag and drop behavior.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "input_.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 692, "end_line": 705}, "type": "class"}, {"name": "from_json", "is_method": true, "class_name": "DragIntercepted", "parameters": ["cls", "json"], "calls": ["cls", "DragData.from_json"], "code_location": {"file": "input_.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 702, "end_line": 705}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> DragIntercepted:\n        return cls(\n            data=DragData.from_json(json[\"data\"]),\n        )\n", "type": "function"}, {"name": "DragData", "docstring": "", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "input_.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 177, "end_line": 200}, "type": "class"}, {"name": "DragDataItem", "docstring": "", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "input_.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 141, "end_line": 173}, "type": "class"}, {"name": "InterceptionId", "docstring": "Unique intercepted request identifier.", "methods": ["to_json", "from_json", "__repr__"], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 89, "end_line": 101}, "type": "class"}, {"name": "from_json", "is_method": true, "class_name": "RequestIntercepted", "parameters": ["cls", "json"], "calls": ["cls", "InterceptionId.from_json", "Request.from_json", "page.FrameId.from_json", "ResourceType.from_json", "bool", "bool", "str", "AuthChallenge.from_json", "ErrorReason.from_json", "int", "Headers.from_json", "RequestId.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3579, "end_line": 3593}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> RequestIntercepted:\n        return cls(\n            interception_id=InterceptionId.from_json(json[\"interceptionId\"]),\n            request=Request.from_json(json[\"request\"]),\n            frame_id=page.FrameId.from_json(json[\"frameId\"]),\n            resource_type=ResourceType.from_json(json[\"resourceType\"]),\n            is_navigation_request=bool(json[\"isNavigationRequest\"]),\n            is_download=bool(json[\"isDownload\"]) if \"isDownload\" in json else None,\n            redirect_url=str(json[\"redirectUrl\"]) if \"redirectUrl\" in json else None,\n            auth_challenge=AuthChallenge.from_json(json[\"authChallenge\"]) if \"authChallenge\" in json else None,\n            response_error_reason=ErrorReason.from_json(json[\"responseErrorReason\"]) if \"responseErrorReason\" in json else None,\n            response_status_code=int(json[\"responseStatusCode\"]) if \"responseStatusCode\" in json else None,\n            response_headers=Headers.from_json(json[\"responseHeaders\"]) if \"responseHeaders\" in json else None,\n            request_id=RequestId.from_json(json[\"requestId\"]) if \"requestId\" in json else None,\n        )\n", "type": "function"}, {"name": "RequestIntercepted", "docstring": "**EXPERIMENTAL**\n\nDetails of an intercepted HTTP request, which must be either allowed, blocked, modified or\nmocked.\nDeprecated, use Fetch.requestPaused instead.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3538, "end_line": 3593}, "type": "class"}, {"name": "InterceptionStage", "docstring": "Stages of the interception to begin intercepting. Request will intercept before the request is\nsent. Response will intercept after the response is received.", "methods": ["to_json", "from_json"], "attributes": ["REQUEST", "HEADERS_RECEIVED"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 1776, "end_line": 1789}, "type": "class"}, {"name": "RequestPattern", "docstring": "Request pattern for interception.", "methods": [], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 1793, "end_line": 1823}, "type": "class"}, {"name": "FileChooserOpened", "docstring": "Emitted only when ``page.interceptFileChooser`` is enabled.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3372, "end_line": 3389}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.1244385242462158}
{"question": "How does a test utility class that combines event-based testing capabilities with streaming service-specific stream writing functionality use multiple inheritance from a test utility class that provides event-based synchronization for testing stream writing operations and a streaming service-specific stream writer class that extends the base HLS stream writer to implement the HLS streaming protocol?", "answer": "", "relative_code_list": null, "ground_truth": "The _KickHLSStreamWriter class inherits from both EventedHLSStreamWriter and KickHLSStreamWriter, combining the core HLS streaming functionality of KickHLSStreamWriter with the event-driven testing capabilities of EventedHLSStreamWriter. This multiple inheritance pattern allows the class to handle HLS segment writing operations while providing event hooks for testing purposes. The EventedHLSStreamWriter mixin from the tests framework enables monitoring of stream events during testing, while KickHLSStreamWriter provides the actual implementation for writing HLS segments specific to the Kick platform. The empty class body indicates that all functionality is inherited, requiring careful method resolution order (MRO) handling to ensure proper integration of both parent classes' methods without conflicts.", "score": null, "retrieved_content": [{"name": "EventedWriterHLSStream", "docstring": "", "methods": [], "attributes": ["__reader__"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 148, "end_line": 149}, "type": "class"}, {"name": "EventedWriterHLSStreamReader", "docstring": "", "methods": [], "attributes": ["__writer__"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 140, "end_line": 141}, "type": "class"}, {"name": "TestHLSStream", "docstring": "", "methods": ["get_session", "test_thread_names", "test_playlist_end", "test_playlist_end_on_empty_reload", "test_offset_and_duration", "test_map"], "attributes": [], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 153, "end_line": 216}, "type": "class"}, {"name": "EventedHLSStreamWriter", "docstring": "", "methods": ["__init__", "_queue_put", "_queue_get", "_future_result", "write"], "attributes": [], "code_location": {"file": "stream_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/mixins", "start_line": 113, "end_line": 135}, "type": "class"}, {"name": "TestHLSStreamWorker", "docstring": "", "methods": ["tearDown", "get_session", "test_segment_queue_timing_threshold_reached", "test_segment_queue_timing_threshold_reached_ignored", "test_segment_queue_timing_threshold_reached_min", "test_playlist_reload_offset"], "attributes": ["__stream__", "OPTIONS"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 270, "end_line": 542}, "type": "class"}, {"name": "TestMixinStreamHLS", "docstring": "", "methods": ["__init__", "setUp", "tearDown", "mock", "get_mock", "called", "url", "content", "await_close", "await_reload", "await_playlist_wait", "await_write", "await_read", "get_session", "subject", "start", "close"], "attributes": ["__stream__", "__readthread__"], "code_location": {"file": "stream_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/mixins", "start_line": 194, "end_line": 316}, "type": "class"}, {"name": "_TestSubjectHLSStream", "docstring": "", "methods": [], "attributes": ["__reader__"], "code_location": {"file": "test_hls_filtered.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 25, "end_line": 26}, "type": "class"}, {"name": "EventedWorkerHLSStream", "docstring": "", "methods": [], "attributes": ["__reader__"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 144, "end_line": 145}, "type": "class"}, {"name": "TestKickHLSStream", "docstring": "", "methods": ["get_session", "test_hls_low_latency_has_prefetch", "test_hls_no_low_latency_has_prefetch", "test_hls_low_latency_no_prefetch", "test_hls_low_latency_no_ads_reload_time"], "attributes": ["__stream__"], "code_location": {"file": "test_kick.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 60, "end_line": 142}, "type": "class"}, {"name": "EventedHLSStreamWorker", "docstring": "", "methods": ["__init__", "reload", "wait"], "attributes": [], "code_location": {"file": "stream_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/mixins", "start_line": 96, "end_line": 110}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.1099200248718262}
{"question": "How does the configuration flag that determines whether unmatched network requests should be blocked or allowed to proceed in the client session manager for browser tab interactions and network request interception affect the request interception behavior during the asynchronous event processing cycle when handling network request interception events?", "answer": "", "relative_code_list": null, "ground_truth": "The fail_unhandled_requests parameter in CDPClientSession controls whether unhandled network requests should raise exceptions or be silently ignored. When set to True, any request that isn't explicitly handled by a RequestPausedHandler will result in a CDPError being raised, terminating the session. When set to False, unhandled requests are automatically continued without intervention. This parameter directly impacts the CDP protocol's Fetch.requestPaused event handling, where the RequestPausedHandler evaluates whether to intercept, modify, or continue requests based on the fail_unhandled_requests flag and registered handler callbacks.", "score": null, "retrieved_content": [{"name": "__init__", "is_method": true, "class_name": "CDPClientSession", "parameters": ["self", "cdp_client", "cdp_session", "fail_unhandled_requests", "max_buffer_size"], "calls": ["set"], "code_location": {"file": "client.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp", "start_line": 223, "end_line": 235}, "code_snippet": "    def __init__(\n        self,\n        cdp_client: CDPClient,\n        cdp_session: CDPSession,\n        fail_unhandled_requests: bool = False,\n        max_buffer_size: int | None = None,\n    ):\n        self.cdp_client = cdp_client\n        self.cdp_session = cdp_session\n        self._fail_unhandled = fail_unhandled_requests\n        self._request_handlers: list[RequestPausedHandler] = []\n        self._requests_handled: set[str] = set()\n        self._max_buffer_size = max_buffer_size\n", "type": "function"}, {"name": "add_request_handler", "is_method": true, "class_name": "CDPClientSession", "parameters": ["self", "async_handler", "url_pattern", "on_request"], "calls": ["self._request_handlers.append", "RequestPausedHandler"], "code_location": {"file": "client.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp", "start_line": 237, "end_line": 257}, "code_snippet": "    def add_request_handler(\n        self,\n        async_handler: TRequestHandlerCallable,\n        url_pattern: str = \"*\",\n        on_request: bool = False,\n    ):\n        \"\"\"\n        :param async_handler: An async request handler which must call :meth:`continue_request()`, :meth:`fail_request()`,\n                              :meth:`fulfill_request()` or :meth:`alter_request()`, or the next matching request handler\n                              will be run. If no matching request handler was found or if no matching one called one of\n                              the just mentioned methods, then the request will be continued if the session was initialized\n                              with ``fail_unhandled_requests=False``, otherwise it will be blocked.\n        :param url_pattern:   An optional URL wildcard string which defaults to ``\"*\"``. Only matching URLs will cause\n                              ``Fetch.requestPraused`` events to be emitted over the CDP connection.\n                              The async request handler will be called on each matching URL unless another request handler\n                              has already handled the request (see description above).\n        :param on_request:    Whether to intercept the network request or the network response.\n        \"\"\"\n        self._request_handlers.append(\n            RequestPausedHandler(async_handler=async_handler, url_pattern=url_pattern, on_request=on_request),\n        )\n", "type": "function"}, {"name": "RequestIntercepted", "docstring": "**EXPERIMENTAL**\n\nDetails of an intercepted HTTP request, which must be either allowed, blocked, modified or\nmocked.\nDeprecated, use Fetch.requestPaused instead.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3538, "end_line": 3593}, "type": "class"}, {"name": "CDPClientSession", "docstring": "High-level API for navigating websites, intercepting network requests/responses,\nand for evaluating async JavaScript expressions.\n\nDon't instantiate this class yourself, use the :meth:`CDPClient.session()` async contextmanager.", "methods": ["__init__", "add_request_handler", "_headers_entries_from_mapping"], "attributes": [], "code_location": {"file": "client.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp", "start_line": 215, "end_line": 492}, "type": "class"}, {"name": "continue_intercepted_request", "is_method": false, "class_name": null, "parameters": ["interception_id", "error_reason", "raw_response", "url", "method", "post_data", "headers", "auth_challenge_response"], "calls": ["interception_id.to_json", "error_reason.to_json", "headers.to_json", "auth_challenge_response.to_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 2706, "end_line": 2754}, "code_snippet": "def continue_intercepted_request(\n    interception_id: InterceptionId,\n    error_reason: ErrorReason | None = None,\n    raw_response: str | None = None,\n    url: str | None = None,\n    method: str | None = None,\n    post_data: str | None = None,\n    headers: Headers | None = None,\n    auth_challenge_response: AuthChallengeResponse | None = None,\n) -> Generator[T_JSON_DICT, T_JSON_DICT, None]:\n    \"\"\"\n    Response to Network.requestIntercepted which either modifies the request to continue with any\n    modifications, or blocks it, or completes it with the provided response bytes. If a network\n    fetch occurs as a result which encounters a redirect an additional Network.requestIntercepted\n    event will be sent with the same InterceptionId.\n    Deprecated, use Fetch.continueRequest, Fetch.fulfillRequest and Fetch.failRequest instead.\n\n    **EXPERIMENTAL**\n\n    :param interception_id:\n    :param error_reason: *(Optional)* If set this causes the request to fail with the given reason. Passing ```Aborted```` for requests marked with ````isNavigationRequest``` also cancels the navigation. Must not be set in response to an authChallenge.\n    :param raw_response: *(Optional)* If set the requests completes using with the provided base64 encoded raw response, including HTTP status line and headers etc... Must not be set in response to an authChallenge. (Encoded as a base64 string when passed over JSON)\n    :param url: *(Optional)* If set the request url will be modified in a way that's not observable by page. Must not be set in response to an authChallenge.\n    :param method: *(Optional)* If set this allows the request method to be overridden. Must not be set in response to an authChallenge.\n    :param post_data: *(Optional)* If set this allows postData to be set. Must not be set in response to an authChallenge.\n    :param headers: *(Optional)* If set this allows the request headers to be changed. Must not be set in response to an authChallenge.\n    :param auth_challenge_response: *(Optional)* Response to a requestIntercepted with an authChallenge. Must not be set otherwise.\n    \"\"\"\n    params: T_JSON_DICT = {}\n    params[\"interceptionId\"] = interception_id.to_json()\n    if error_reason is not None:\n        params[\"errorReason\"] = error_reason.to_json()\n    if raw_response is not None:\n        params[\"rawResponse\"] = raw_response\n    if url is not None:\n        params[\"url\"] = url\n    if method is not None:\n        params[\"method\"] = method\n    if post_data is not None:\n        params[\"postData\"] = post_data\n    if headers is not None:\n        params[\"headers\"] = headers.to_json()\n    if auth_challenge_response is not None:\n        params[\"authChallengeResponse\"] = auth_challenge_response.to_json()\n    cmd_dict: T_JSON_DICT = {\n        \"method\": \"Network.continueInterceptedRequest\",\n        \"params\": params,\n    }\n    yield cmd_dict\n", "type": "function"}, {"name": "cdp_client_session", "is_method": false, "class_name": null, "parameters": ["request", "cdp_client"], "calls": ["pytest.fixture", "TargetID", "SessionID", "CDPSession", "getattr", "CDPClientSession"], "code_location": {"file": "test_client.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/webbrowser/cdp", "start_line": 61, "end_line": 71}, "code_snippet": "def cdp_client_session(request: pytest.FixtureRequest, cdp_client: CDPClient):\n    target_id = TargetID(\"01234\")\n    session_id = SessionID(\"56789\")\n    session = cdp_client.cdp_connection.sessions[session_id] = CDPSession(\n        cdp_client.cdp_connection.websocket,\n        target_id=target_id,\n        session_id=session_id,\n        cmd_timeout=cdp_client.cdp_connection.cmd_timeout,\n    )\n    fail_unhandled_requests = getattr(request, \"param\", False)\n    return CDPClientSession(cdp_client, session, fail_unhandled_requests)\n", "type": "function"}, {"name": "RequestPaused", "docstring": "Issued when the domain is enabled and the request URL matches the\nspecified filter. The request is paused until the client responds\nwith one of continueRequest, failRequest or fulfillRequest.\nThe stage of the request can be determined by presence of responseErrorReason\nand responseStatusCode -- the request is at the response stage if either\nof these fields is present and in the request stage otherwise.\nRedirect responses and subsequent requests are reported similarly to regular\nresponses and requests. Redirect responses may be distinguished by the value\nof ``responseStatusCode`` (which is one of 301, 302, 303, 307, 308) along with\npresence of the ``location`` header. Requests resulting from a redirect will\nhave ``redirectedRequestId`` field set.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "fetch.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 429, "end_line": 479}, "type": "class"}, {"name": "set_request_interception", "is_method": false, "class_name": null, "parameters": ["patterns"], "calls": ["i.to_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3257, "end_line": 3274}, "code_snippet": "def set_request_interception(\n    patterns: list[RequestPattern],\n) -> Generator[T_JSON_DICT, T_JSON_DICT, None]:\n    \"\"\"\n    Sets the requests to intercept that match the provided patterns and optionally resource types.\n    Deprecated, please use Fetch.enable instead.\n\n    **EXPERIMENTAL**\n\n    :param patterns: Requests matching any of these patterns will be forwarded and wait for the corresponding continueInterceptedRequest call.\n    \"\"\"\n    params: T_JSON_DICT = {}\n    params[\"patterns\"] = [i.to_json() for i in patterns]\n    cmd_dict: T_JSON_DICT = {\n        \"method\": \"Network.setRequestInterception\",\n        \"params\": params,\n    }\n    yield cmd_dict\n", "type": "function"}, {"name": "_mock_launch", "is_method": true, "class_name": "TestLaunch", "parameters": ["self", "request", "session", "mock_run", "cdp_client"], "calls": ["pytest.fixture", "object", "AsyncMock", "getattr", "nullcontext", "CDPClient.launch", "call"], "code_location": {"file": "test_client.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/webbrowser/cdp", "start_line": 91, "end_line": 96}, "code_snippet": "    def _mock_launch(self, request: pytest.FixtureRequest, session: Streamlink, mock_run, cdp_client: Mock):\n        result = object()\n        mock_runner = AsyncMock(return_value=result)\n        with getattr(request, \"param\", nullcontext()):\n            assert CDPClient.launch(session, mock_runner) is result\n            assert mock_runner.await_args_list == [call(cdp_client)]\n", "type": "function"}, {"name": "enable", "is_method": false, "class_name": null, "parameters": ["patterns", "handle_auth_requests"], "calls": ["i.to_json"], "code_location": {"file": "fetch.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 192, "end_line": 212}, "code_snippet": "def enable(\n    patterns: list[RequestPattern] | None = None,\n    handle_auth_requests: bool | None = None,\n) -> Generator[T_JSON_DICT, T_JSON_DICT, None]:\n    \"\"\"\n    Enables issuing of requestPaused events. A request will be paused until client\n    calls one of failRequest, fulfillRequest or continueRequest/continueWithAuth.\n\n    :param patterns: *(Optional)* If specified, only requests matching any of these patterns will produce fetchRequested event and will be paused until clients response. If not set, all requests will be affected.\n    :param handle_auth_requests: *(Optional)* If true, authRequired events will be issued and requests will be paused expecting a call to continueWithAuth.\n    \"\"\"\n    params: T_JSON_DICT = {}\n    if patterns is not None:\n        params[\"patterns\"] = [i.to_json() for i in patterns]\n    if handle_auth_requests is not None:\n        params[\"handleAuthRequests\"] = handle_auth_requests\n    cmd_dict: T_JSON_DICT = {\n        \"method\": \"Fetch.enable\",\n        \"params\": params,\n    }\n    yield cmd_dict\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1156847476959229}
{"question": "Why were specific design constraints or assumptions considered in the test class that validates URL pattern matching for a streaming plugin's URL pattern matching implementation?", "answer": "", "relative_code_list": null, "ground_truth": "The TestPluginCanHandleUrlAlbavision class's URL pattern matching implementation considers several design constraints and assumptions: 1) Domain-specific validation requiring exact domain matches for legitimate broadcasters while rejecting fake domains, 2) Pattern flexibility to handle variations in URL structure including trailing slashes, hyphens, and hash fragments, 3) Channel number validation that accepts known channel numbers while rejecting arbitrary numeric sequences, 4) Path structure consistency requiring specific path patterns like '/envivo-' or '/en-vivo-' prefixes, and 5) Security considerations to prevent matching URLs with additional path segments or parameters that could indicate malicious manipulation. These constraints ensure the plugin only handles legitimate Albavision network streams while rejecting potentially malicious or incorrect URLs.", "score": null, "retrieved_content": [{"name": "TestPluginCanHandleUrlHTTPStreamPlugin", "docstring": "", "methods": [], "attributes": ["__plugin__", "should_match_groups", "should_not_match"], "code_location": {"file": "test_http.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 17, "end_line": 34}, "type": "class"}, {"name": "TestPluginCanHandleUrlStreann", "docstring": "", "methods": [], "attributes": ["__plugin__", "should_match_groups", "should_not_match"], "code_location": {"file": "test_streann.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 5, "end_line": 41}, "type": "class"}, {"name": "test_priority", "is_method": false, "class_name": null, "parameters": ["url", "priority"], "calls": ["pytest.mark.parametrize", "next", "matcher.pattern.match"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 78, "end_line": 79}, "code_snippet": "def test_priority(url, priority):\n    assert next((matcher.priority for matcher in HLSPlugin.matchers if matcher.pattern.match(url)), NO_PRIORITY) == priority\n", "type": "function"}, {"name": "TestPluginCanHandleUrlMDStrm", "docstring": "", "methods": [], "attributes": ["__plugin__", "should_match"], "code_location": {"file": "test_mdstrm.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 5, "end_line": 16}, "type": "class"}, {"name": "TestPluginCanHandleUrlSteamBroadcastPlugin", "docstring": "", "methods": [], "attributes": ["__plugin__", "should_match", "should_not_match"], "code_location": {"file": "test_steam.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 5, "end_line": 18}, "type": "class"}, {"name": "test_matchers_not_matching", "is_method": true, "class_name": "TestPluginMatcher", "parameters": ["self"], "calls": ["pluginmatcher", "re.compile", "pytest.raises", "MyPlugin", "Mock"], "code_location": {"file": "test_plugin.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 207, "end_line": 213}, "code_snippet": "    def test_matchers_not_matching(self):\n        @pluginmatcher(re.compile(r\"http://foo\"))\n        class MyPlugin(FakePlugin):\n            pass\n\n        with pytest.raises(PluginError, match=r\"^The input URL did not match any of this plugin's matchers$\"):\n            MyPlugin(Mock(), \"http://bar\")\n", "type": "function"}, {"name": "test_url_setter", "is_method": true, "class_name": "TestPluginMatcher", "parameters": ["self"], "calls": ["pluginmatcher", "pluginmatcher", "pluginmatcher", "MyPlugin", "re.compile", "re.compile", "re.compile", "Mock", "plugin.match.group", "plugin.match.group", "plugin.match.group"], "code_location": {"file": "test_plugin.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 219, "end_line": 248}, "code_snippet": "    def test_url_setter(self):\n        @pluginmatcher(re.compile(r\"http://(foo)\"))\n        @pluginmatcher(re.compile(r\"http://(bar)\"))\n        @pluginmatcher(re.compile(r\"http://(baz)\"))\n        class MyPlugin(FakePlugin):\n            pass\n\n        plugin = MyPlugin(Mock(), \"http://foo\")\n        assert plugin.url == \"http://foo\"\n        assert [m is not None for m in plugin.matches] == [True, False, False]\n        assert plugin.matcher is plugin.matchers[0].pattern\n        assert plugin.match.group(1) == \"foo\"\n\n        plugin.url = \"http://bar\"\n        assert plugin.url == \"http://bar\"\n        assert [m is not None for m in plugin.matches] == [False, True, False]\n        assert plugin.matcher is plugin.matchers[1].pattern\n        assert plugin.match.group(1) == \"bar\"\n\n        plugin.url = \"http://baz\"\n        assert plugin.url == \"http://baz\"\n        assert [m is not None for m in plugin.matches] == [False, False, True]\n        assert plugin.matcher is plugin.matchers[2].pattern\n        assert plugin.match.group(1) == \"baz\"\n\n        plugin.url = \"http://qux\"\n        assert plugin.url == \"http://qux\"\n        assert [m is not None for m in plugin.matches] == [False, False, False]\n        assert plugin.matcher is None\n        assert plugin.match is None\n", "type": "function"}, {"name": "test_url", "is_method": true, "class_name": "TestAtresPlayer", "parameters": ["self", "url", "expected"], "calls": ["pytest.mark.parametrize", "AtresPlayer", "Mock"], "code_location": {"file": "test_atresplayer.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 33, "end_line": 35}, "code_snippet": "    def test_url(self, url, expected):\n        plugin = AtresPlayer(Mock(), url)\n        assert plugin.url == expected\n", "type": "function"}, {"name": "test_resolve_url_scheme", "is_method": true, "class_name": "TestResolveURL", "parameters": ["self", "session"], "calls": ["pluginmatcher", "pluginmatcher", "session.plugins.update", "re.compile", "re.compile", "pytest.raises", "session.resolve_url", "pytest.raises", "session.resolve_url", "pytest.raises", "session.resolve_url", "session.resolve_url", "session.resolve_url", "session.resolve_url"], "code_location": {"file": "test_session.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/session", "start_line": 120, "end_line": 143}, "code_snippet": "    def test_resolve_url_scheme(self, session: Streamlink):\n        @pluginmatcher(re.compile(r\"http://insecure\"))\n        class PluginHttp(_EmptyPlugin):\n            pass\n\n        @pluginmatcher(re.compile(r\"https://secure\"))\n        class PluginHttps(_EmptyPlugin):\n            pass\n\n        session.plugins.update({\n            \"insecure\": PluginHttp,\n            \"secure\": PluginHttps,\n        })\n\n        with pytest.raises(NoPluginError):\n            session.resolve_url(\"insecure\")\n        assert session.resolve_url(\"http://insecure\")[1] is PluginHttp\n        with pytest.raises(NoPluginError):\n            session.resolve_url(\"https://insecure\")\n\n        assert session.resolve_url(\"secure\")[1] is PluginHttps\n        with pytest.raises(NoPluginError):\n            session.resolve_url(\"http://secure\")\n        assert session.resolve_url(\"https://secure\")[1] is PluginHttps\n", "type": "function"}, {"name": "TestPluginCanHandleUrlLivestream", "docstring": "", "methods": [], "attributes": ["__plugin__", "should_match_groups", "should_not_match"], "code_location": {"file": "test_livestream.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 5, "end_line": 62}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.1101319789886475}
{"question": "How does a streaming plugin that processes encrypted streaming data from embedded iframes ensure secure authentication through its token acquisition mechanism while maintaining stateless session management across multiple HTTP requests?", "answer": "", "relative_code_list": null, "ground_truth": "The Streann plugin implements a multi-step authentication process where it first retrieves a server timestamp to prevent replay attacks, then generates a unique device ID for session identification. It extracts a passphrase from the page content using regex pattern matching, which is used to decrypt encrypted configuration data via OpenSSL. The plugin then constructs a token request with base64-encoded domain and timestamp parameters, sending them with appropriate headers including Referer and X-Requested-With. The token response is validated against a schema ensuring proper structure, and this token is subsequently used to construct the final HLS stream URL. This design maintains security through cryptographic operations, timestamp validation, and proper HTTP headers while remaining stateless through the device ID and token mechanisms.", "score": null, "retrieved_content": [{"name": "get_token", "is_method": true, "class_name": "USTVNow", "parameters": ["self"], "calls": ["log.debug", "self.session.http.get", "res.json", "log.debug", "log.error", "PluginError", "format"], "code_location": {"file": "ustvnow.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 98, "end_line": 122}, "code_snippet": "    def get_token(self):\n        if not self._token:\n            log.debug(\"Getting new session token\")\n            res = self.session.http.get(\n                self._token_url,\n                params={\n                    \"tenant_code\": self.TENANT_CODE,\n                    \"box_id\": self.box_id,\n                    \"product\": self.TENANT_CODE,\n                    \"device_id\": 5,\n                    \"display_lang_code\": \"ENG\",\n                    \"device_sub_type\": \"\",\n                    \"timezone\": \"UTC\",\n                },\n            )\n\n            data = res.json()\n            if not data[\"status\"]:\n                log.error(\"Token acquisition failed: {details} ({detail})\".format(**data[\"error\"]))\n                raise PluginError(\"could not obtain token\")\n\n            self._token = data[\"response\"][\"sessionId\"]\n            log.debug(f\"New token: {self._token}\")\n\n        return self._token\n", "type": "function"}, {"name": "_get_token", "is_method": true, "class_name": "AdultSwim", "parameters": ["self", "path"], "calls": ["self.session.http.get", "self.app_id_js_url_re.search", "log.debug", "self.session.http.get", "self.app_id_re.search", "log.debug", "self.session.http.get", "self.session.http.json", "m.group", "PluginError", "format", "m.group", "PluginError", "format", "PluginError", "dict"], "code_location": {"file": "adultswim.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 134, "end_line": 162}, "code_snippet": "    def _get_token(self, path):\n        res = self.session.http.get(self.url)\n        m = self.app_id_js_url_re.search(res.text)\n        app_id_js_url = m and m.group(1)\n        if not app_id_js_url:\n            raise PluginError(\"Could not determine app_id_js_url\")\n        log.debug(\"app_id_js_url={0}\".format(app_id_js_url))\n\n        res = self.session.http.get(app_id_js_url)\n        m = self.app_id_re.search(res.text)\n        app_id = m and m.group(1)\n        if not app_id:\n            raise PluginError(\"Could not determine app_id\")\n        log.debug(\"app_id={0}\".format(app_id))\n\n        res = self.session.http.get(\n            self.token_url,\n            params=dict(\n                format=\"json\",\n                appId=app_id,\n                path=path,\n            ),\n        )\n\n        token_data = self.session.http.json(res, schema=self._token_schema)\n        if \"error\" in token_data:\n            raise PluginError(token_data[\"error\"][\"message\"])\n\n        return token_data[\"token\"]\n", "type": "function"}, {"name": "_get_stream_token", "is_method": true, "class_name": "Vidio", "parameters": ["self", "stream_id", "stream_type"], "calls": ["self.session.http.post", "self.tokens_url.format", "validate.Schema", "str", "str", "validate.parse_json", "validate.union_get", "uuid4", "uuid4", "validate.any", "validate.any", "validate.url", "validate.url"], "code_location": {"file": "vidio.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 28, "end_line": 50}, "code_snippet": "    def _get_stream_token(self, stream_id, stream_type):\n        return self.session.http.post(\n            self.tokens_url.format(id=stream_id),\n            params={\"type\": stream_type},\n            headers={\"Referer\": self.url},\n            cookies={\n                \"ahoy_visit\": str(uuid4()),\n                \"ahoy_visitor\": str(uuid4()),\n            },\n            schema=validate.Schema(\n                validate.parse_json(),\n                {\n                    \"token\": str,\n                    \"hls_url\": validate.any(\"\", validate.url()),\n                    \"dash_url\": validate.any(\"\", validate.url()),\n                },\n                validate.union_get(\n                    \"token\",\n                    \"hls_url\",\n                    \"dash_url\",\n                ),\n            ),\n        )\n", "type": "function"}, {"name": "get_token", "is_method": true, "class_name": "Streann", "parameters": ["self"], "calls": ["log.debug", "dict", "self.session.http.post", "self.session.http.json", "log.trace", "data.get", "self.token_url.format", "log.error", "base64.b64encode", "base64.b64encode", "validate.Schema", "self._domain.encode", "self.time.encode", "validate.optional", "validate.optional", "validate.optional", "validate.optional", "validate.optional", "validate.optional"], "code_location": {"file": "streann.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 97, "end_line": 135}, "code_snippet": "    def get_token(self, **config):\n        log.debug(\"get_token\")\n        pdata = dict(\n            arg1=base64.b64encode(self._domain.encode(\"utf8\")),\n            arg2=base64.b64encode(self.time.encode(\"utf8\")),\n        )\n\n        headers = {\n            \"Referer\": self.url,\n            \"X-Requested-With\": \"XMLHttpRequest\",\n            \"Content-Type\": \"application/x-www-form-urlencoded\",\n        }\n\n        res = self.session.http.post(\n            self.token_url.format(deviceId=self.device_id, **config),\n            data=pdata,\n            headers=headers,\n        )\n\n        if res.status_code == 204:\n            log.error(f\"self._domain might be invalid - {self._domain}\")\n            return\n\n        data = self.session.http.json(\n            res,\n            schema=validate.Schema({\n                \"token\": str,\n                validate.optional(\"name\"): str,\n                validate.optional(\"webPlayer\"): {\n                    validate.optional(\"id\"): str,\n                    validate.optional(\"name\"): str,\n                    validate.optional(\"type\"): str,\n                    validate.optional(\"allowedDomains\"): [str],\n                },\n            }),\n        )\n        log.trace(f\"{data!r}\")\n        self.title = data.get(\"name\")\n        return data[\"token\"]\n", "type": "function"}, {"name": "_access_token", "is_method": true, "class_name": "Twitch", "parameters": ["self", "is_live", "channel_or_vod"], "calls": ["self.api.access_token", "self._client_integrity_token", "self.api.access_token", "PluginError", "self.api.parse_token", "self.options.get"], "code_location": {"file": "twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 883, "end_line": 912}, "code_snippet": "    def _access_token(self, is_live, channel_or_vod):\n        response = \"\"\n        data = (None, None)\n\n        # if live, try without a client-integrity token first (the web player did the same on 2023-05-31)\n        # if not live, we don't need a client-integrity token\n        if not is_live or not self.options.get(\"force-client-integrity\"):\n            response, *data = self.api.access_token(is_live, channel_or_vod)\n\n        # if live and the previous API response was erroneous, try again with a client-integrity token\n        if is_live and response != \"token\":\n            client_integrity = self._client_integrity_token(channel_or_vod)\n            response, *data = self.api.access_token(is_live, channel_or_vod, client_integrity)\n\n        # unknown API response error: abort\n        if response != \"token\":\n            error, message = data\n            raise PluginError(f\"{error or 'Error'}: {message or 'Unknown error'}\")\n\n        # access token response was empty: stream is offline or channel doesn't exist\n        elif data[0] is None:\n            raise NoStreamsError\n\n        sig, token = data\n        try:\n            restricted_bitrates = self.api.parse_token(token)\n        except PluginError:\n            restricted_bitrates = []\n\n        return sig, token, restricted_bitrates\n", "type": "function"}, {"name": "test_auth_failure", "is_method": true, "class_name": "TestTwitchAPIAccessToken", "parameters": ["self", "plugin", "mock"], "calls": ["pytest.mark.usefixtures", "pytest.mark.parametrize", "pytest.raises", "plugin._access_token", "len"], "code_location": {"file": "test_twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 859, "end_line": 872}, "code_snippet": "    def test_auth_failure(self, plugin: Twitch, mock: rm.Mocker):\n        with pytest.raises(PluginError, match=r'^Unauthorized: The \"Authorization\" token is invalid\\.$'):\n            plugin._access_token(True, \"channelname\")\n        assert len(mock.request_history) == 2, \"Always tries again on error, with integrity-token on second attempt\"\n\n        headers: dict = mock.request_history[0]._request.headers\n        assert headers[\"Authorization\"] == \"OAuth invalid-token\"\n        assert \"Device-Id\" not in headers\n        assert \"Client-Integrity\" not in headers\n\n        headers = mock.request_history[1]._request.headers\n        assert headers[\"Authorization\"] == \"OAuth invalid-token\"\n        assert headers[\"Device-Id\"] == \"device-id\"\n        assert headers[\"Client-Integrity\"] == \"client-integrity-token\"\n", "type": "function"}, {"name": "get_stream", "is_method": true, "class_name": "Mjunoon", "parameters": ["self", "slug", "js_data"], "calls": ["self.session.http.post", "self.session.http.json", "parse_json", "self.cache.get", "self.cache.get", "log.debug", "log.debug", "self.session.http.post", "self.session.http.json", "log.debug", "self.cache.set", "self.cache.set", "self.decrypt_data"], "code_location": {"file": "mjunoon.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 124, "end_line": 169}, "code_snippet": "    def get_stream(self, slug, js_data):\n        token_data = {\n            \"token\": self.cache.get(\"token\"),\n            \"token_type\": self.cache.get(\"token_type\"),\n        }\n\n        if token_data[\"token\"] and token_data[\"token_type\"]:\n            log.debug(\"Using cached token\")\n        else:\n            log.debug(\"Getting new token\")\n\n            res = self.session.http.post(\n                self.login_url,\n                json=js_data[\"credentials\"],\n            )\n            token_data = self.session.http.json(res, schema=self.token_schema)\n            log.debug(f\"Token={token_data['token']}\")\n\n            self.cache.set(\"token\", token_data[\"token\"], expires=token_data[\"expires_in\"])\n            self.cache.set(\"token_type\", token_data[\"token_type\"], expires=token_data[\"expires_in\"])\n\n        headers = {\"Authorization\": f\"{token_data['token_type']} {token_data['token']}\"}\n        data = {\n            \"slug\": slug,\n            \"type\": js_data[\"type\"],\n        }\n        res = self.session.http.post(\n            self.stream_url,\n            headers=headers,\n            json=data,\n        )\n        encrypted_data = self.session.http.json(\n            res,\n            schema=self.encrypted_data_schema,\n        )\n\n        stream_data = parse_json(\n            self.decrypt_data(js_data[\"cipher_data\"], encrypted_data),\n            schema=self.stream_schema,\n        )\n\n        self.author = stream_data[\"channel_name\"]\n        self.category = stream_data[\"genres\"]\n        self.title = stream_data[\"meta_title\"]\n\n        return stream_data[\"live_stream_url\"]\n", "type": "function"}, {"name": "test_integrity_check_vod", "is_method": true, "class_name": "TestTwitchAPIAccessToken", "parameters": ["self", "plugin", "mock"], "calls": ["pytest.mark.usefixtures", "pytest.mark.parametrize", "plugin._access_token", "len"], "code_location": {"file": "test_twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 963, "end_line": 971}, "code_snippet": "    def test_integrity_check_vod(self, plugin: Twitch, mock: rm.Mocker):\n        data = plugin._access_token(False, \"vodid\")\n        assert data == (\"sig\", '{\"channel\":\"foo\"}', [])\n        assert len(mock.request_history) == 1\n\n        headers: dict = mock.request_history[0]._request.headers\n        assert headers[\"Authorization\"] == \"OAuth invalid-token\"\n        assert \"Device-Id\" not in headers\n        assert \"Client-Integrity\" not in headers\n", "type": "function"}, {"name": "_get_streams", "is_method": true, "class_name": "ZeeNews", "parameters": ["self"], "calls": ["self.session.http.get", "log.debug", "self.session.http.json", "format", "items", "HLSStream.parse_variant_playlist", "self.HLS_URL.format"], "code_location": {"file": "zeenews.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 26, "end_line": 30}, "code_snippet": "    def _get_streams(self):\n        res = self.session.http.get(self.TOKEN_URL)\n        token = self.session.http.json(res)[\"video_token\"]\n        log.debug(\"video_token: {0}\".format(token))\n        yield from HLSStream.parse_variant_playlist(self.session, self.HLS_URL.format(token)).items()\n", "type": "function"}, {"name": "_login", "is_method": true, "class_name": "TF1", "parameters": ["self", "login_id", "password"], "calls": ["self.session.http.post", "log.debug", "self.session.http.post", "PluginError", "validate.Schema", "validate.Schema", "validate.parse_json", "validate.any", "int", "validate.parse_json", "validate.get", "validate.all", "validate.all", "validate.union_get", "validate.transform", "validate.union_get", "validate.transform"], "code_location": {"file": "tf1.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 61, "end_line": 117}, "code_snippet": "    def _login(self, login_id, password):\n        status, *login_data = self.session.http.post(\n            url=self._URL_LOGIN,\n            data={\n                \"loginID\": login_id,\n                \"password\": password,\n                \"APIKey\": self._API_KEY,\n                \"includeUserInfo\": \"true\",\n            },\n            schema=validate.Schema(\n                validate.parse_json(),\n                validate.any(\n                    validate.all(\n                        {\n                            \"errorCode\": 0,\n                            \"UID\": str,\n                            \"UIDSignature\": str,\n                            \"signatureTimestamp\": str,\n                        },\n                        validate.union_get(\"UID\", \"UIDSignature\", \"signatureTimestamp\"),\n                        validate.transform(lambda data: (\"success\", *data)),\n                    ),\n                    validate.all(\n                        {\n                            \"errorCode\": int,\n                            \"errorDetails\": str,\n                        },\n                        validate.union_get(\"errorCode\", \"errorDetails\"),\n                        validate.transform(lambda data: (\"failure\", *data)),\n                    ),\n                ),\n            ),\n        )\n\n        if status != \"success\":\n            error_code, error_details = login_data\n            raise PluginError(f\"{error_code=} - {error_details or 'Unknown error'}\")\n\n        uid, uid_signature, signature_timestamp = login_data\n        log.debug(f\"{uid=} {uid_signature=} {signature_timestamp=}\")\n\n        return self.session.http.post(\n            url=self._URL_TOKEN,\n            json={\n                \"uid\": uid,\n                \"signature\": uid_signature,\n                \"timestamp\": int(signature_timestamp),\n                \"consent_ids\": self._CONSENT_IDS,\n            },\n            schema=validate.Schema(\n                validate.parse_json(),\n                {\n                    \"token\": str,\n                },\n                validate.get(\"token\"),\n            ),\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1681292057037354}
{"question": "Why does the performance of a streaming service plugin's stream retrieval method degrade when the JavaScript resource file containing stream configuration contains multiple sequentially processed regular expression patterns for extracting authentication parameters?", "answer": "", "relative_code_list": null, "ground_truth": "The performance degradation occurs because the current implementation uses sequential regex searches (main_js_url_re, user_id_re) on the JavaScript file content, which requires multiple full scans of the potentially large file. Each regex operation has O(n) complexity where n is the file size, and when performed sequentially without caching or combined pattern matching, this results in O(m*n) complexity where m is the number of patterns. This becomes particularly problematic with large JavaScript files or when the patterns are located far apart in the file, causing increased memory usage and processing time that directly impacts stream startup latency for users.", "score": null, "retrieved_content": [{"name": "_get_streams", "is_method": true, "class_name": "Pixiv", "parameters": ["self"], "calls": ["self.get_option", "self.get_option", "self.options.get", "self.get_streamer_data", "streamer_data.get", "log.trace", "self.clear_cookies", "log.info", "log.debug", "format", "log.info", "self.hls_stream", "self.get_option", "self._login_using_session_id_and_device_token", "format", "self.get_option", "log.info", "enumerate", "self.get_option", "join", "self.get_option", "log.info", "int", "self.get_option", "p.get", "format", "self.set_option", "self.set_option", "log.error", "log.error", "self.hls_stream", "format", "split", "self.input_ask"], "code_location": {"file": "pixiv.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 115, "end_line": 173}, "code_snippet": "    def _get_streams(self):\n        login_session_id = self.get_option(\"sessionid\")\n        login_device_token = self.get_option(\"devicetoken\")\n\n        if self.options.get(\"purge_credentials\"):\n            self.clear_cookies()\n            self._authed = False\n            log.info(\"All credentials were successfully removed.\")\n\n        if self._authed:\n            log.debug(\"Attempting to authenticate using cached cookies\")\n        elif login_session_id and login_device_token:\n            self._login_using_session_id_and_device_token(login_session_id, login_device_token)\n\n        streamer_data = self.get_streamer_data()\n        performers = streamer_data.get(\"performers\")\n        log.trace(\"{0!r}\".format(streamer_data))\n        if performers:\n            co_hosts = [(p[\"user\"][\"unique_name\"], p[\"user\"][\"name\"]) for p in performers]\n            log.info(\"Available hosts: {0}\".format(\", \".join([\"{0} ({1})\".format(k, v) for k, v in co_hosts])))\n\n            # control if the host from --pixiv-performer is valid,\n            # if not let the User select a different host\n            if self.get_option(\"performer\") and self.get_option(\"performer\") not in [v[0] for v in co_hosts]:\n                # print the owner as 0\n                log.info(f\"0 - {streamer_data['owner']['user']['unique_name']} ({streamer_data['owner']['user']['name']})\")\n                # print all other performer\n                for i, item in enumerate(co_hosts, start=1):\n                    log.info(\"{0} - {1} ({2})\".format(i, item[0], item[1]))\n\n                try:\n                    number = int(self.input_ask(\"Enter the number you'd like to watch\").split(\" \")[0])\n                    if number == 0:\n                        # default stream\n                        self.set_option(\"performer\", None)\n                    else:\n                        # other co-hosts\n                        self.set_option(\"performer\", co_hosts[number - 1][0])\n                except FatalPluginError:\n                    log.error(\"Selected performer is invalid.\")\n                    return\n                except (IndexError, ValueError, TypeError):\n                    log.error(\"Input is invalid\")\n                    return\n\n        # ignore the owner stream, if a performer is selected\n        # or use it when there are no other performers\n        if not self.get_option(\"performer\") or not performers:\n            return self.hls_stream(streamer_data[\"owner\"][\"hls_movie\"][\"url\"])\n\n        # play a co-host stream\n        if performers and self.get_option(\"performer\"):\n            for p in performers:\n                if p[\"user\"][\"unique_name\"] == self.get_option(\"performer\"):\n                    # if someone goes online at the same time as Streamlink\n                    # was used, the hls URL might not be in the JSON data\n                    hls_movie = p.get(\"hls_movie\")\n                    if hls_movie:\n                        return self.hls_stream(hls_movie[\"url\"])\n", "type": "function"}, {"name": "_get_streams", "is_method": true, "class_name": "N13TV", "parameters": ["self"], "calls": ["self.match.group", "log.debug", "self.session.http.get", "self.main_js_url_re.search", "log.debug", "self.session.http.get", "self.user_id_re.search", "log.debug", "format", "self.video_name_re.search", "log.debug", "m.group", "PluginError", "format", "urljoin", "m.group", "PluginError", "format", "self._get_live", "self._get_vod", "m.group", "PluginError", "format"], "code_location": {"file": "n13tv.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 123, "end_line": 153}, "code_snippet": "    def _get_streams(self):\n        url_type = self.match.group(1)\n        log.debug(\"URL type={0}\".format(url_type))\n\n        res = self.session.http.get(self.url)\n\n        if url_type != \"live\":\n            m = self.video_name_re.search(res.text)\n            video_name = m and m.group(1)\n            if not video_name:\n                raise PluginError(\"Could not determine video_name\")\n            log.debug(\"Video name={0}\".format(video_name))\n\n        m = self.main_js_url_re.search(res.text)\n        main_js_path = m and m.group(1)\n        if not main_js_path:\n            raise PluginError(\"Could not determine main_js_path\")\n        log.debug(\"Main JS path={0}\".format(main_js_path))\n\n        res = self.session.http.get(urljoin(self.url, main_js_path))\n\n        m = self.user_id_re.search(res.text)\n        user_id = m and m.group(1)\n        if not user_id:\n            raise PluginError(\"Could not determine user_id\")\n        log.debug(\"User ID={0}\".format(user_id))\n\n        if url_type == \"live\":\n            return self._get_live(user_id)\n        else:\n            return self._get_vod(user_id, video_name)\n", "type": "function"}, {"name": "_get_stream_data", "is_method": true, "class_name": "AdultSwim", "parameters": ["self", "streamid"], "calls": ["self.session.http.get", "self.json_data_re.search", "m.group", "parse_json", "PluginError", "m.group"], "code_location": {"file": "adultswim.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 103, "end_line": 113}, "code_snippet": "    def _get_stream_data(self, streamid):\n        res = self.session.http.get(self.url)\n        m = self.json_data_re.search(res.text)\n        if m and m.group(1):\n            streams = parse_json(m.group(1), schema=self._stream_data_schema)\n        else:\n            raise PluginError(\"Failed to get json_data\")\n\n        for stream in streams:\n            if \"id\" in stream and streamid == stream[\"id\"] and \"stream\" in stream:\n                return stream[\"stream\"]\n", "type": "function"}, {"name": "_get_streams", "is_method": true, "class_name": "YuppTV", "parameters": ["self"], "calls": ["self.session.http.headers.update", "self.session.http.headers.update", "self.get_option", "self.get_option", "self.options.get", "self.session.http.get", "re.search", "HLSStreamYupptv.parse_variant_playlist", "self.session.http.cookies.get", "self.session.http.cookies.get", "self.clear_cookies", "log.info", "log.debug", "log.error", "self._login_using_box_id_and_yuppflix_token", "log.error", "log.error"], "code_location": {"file": "yupptv.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 78, "end_line": 124}, "code_snippet": "    def _get_streams(self):\n        self.session.http.headers.update({\"User-Agent\": useragents.CHROME})\n        self.session.http.headers.update({\"Origin\": \"https://www.yupptv.com\"})\n\n        authed = (\n            self.session.http.cookies.get(\"BoxId\")\n            and self.session.http.cookies.get(\"YuppflixToken\")\n        )  # fmt: skip\n\n        login_box_id = self.get_option(\"boxid\")\n        login_yuppflix_token = self.get_option(\"yuppflixtoken\")\n\n        if self.options.get(\"purge_credentials\"):\n            self.clear_cookies()\n            authed = False\n            log.info(\"All credentials were successfully removed\")\n\n        if authed:\n            log.debug(\"Attempting to authenticate using cached cookies\")\n        elif login_box_id and login_yuppflix_token:\n            self._login_using_box_id_and_yuppflix_token(\n                login_box_id,\n                login_yuppflix_token,\n            )\n            authed = True\n\n        page = self.session.http.get(self.url)\n        if authed and \"btnsignup\" in page.text:\n            log.error(\"This device requires renewed credentials to log in\")\n            return\n\n        match = re.search(r\"\"\"src:\\s*(?P<q>[\"'])(?P<url>.+?)(?P=q)\"\"\", page.text)\n        if not match or \"preview/\" in match[\"url\"]:\n            if \"btnsignup\" in page.text:\n                log.error(\"This stream requires you to login\")\n            else:\n                log.error(\"This stream requires a subscription\")\n            return\n\n        def override_encoding(res, *_, **__):\n            res.encoding = \"utf-8\"\n\n        return HLSStreamYupptv.parse_variant_playlist(\n            self.session,\n            match[\"url\"],\n            hooks={\"response\": override_encoding},\n        )\n", "type": "function"}, {"name": "get_stream", "is_method": true, "class_name": "Mjunoon", "parameters": ["self", "slug", "js_data"], "calls": ["self.session.http.post", "self.session.http.json", "parse_json", "self.cache.get", "self.cache.get", "log.debug", "log.debug", "self.session.http.post", "self.session.http.json", "log.debug", "self.cache.set", "self.cache.set", "self.decrypt_data"], "code_location": {"file": "mjunoon.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 124, "end_line": 169}, "code_snippet": "    def get_stream(self, slug, js_data):\n        token_data = {\n            \"token\": self.cache.get(\"token\"),\n            \"token_type\": self.cache.get(\"token_type\"),\n        }\n\n        if token_data[\"token\"] and token_data[\"token_type\"]:\n            log.debug(\"Using cached token\")\n        else:\n            log.debug(\"Getting new token\")\n\n            res = self.session.http.post(\n                self.login_url,\n                json=js_data[\"credentials\"],\n            )\n            token_data = self.session.http.json(res, schema=self.token_schema)\n            log.debug(f\"Token={token_data['token']}\")\n\n            self.cache.set(\"token\", token_data[\"token\"], expires=token_data[\"expires_in\"])\n            self.cache.set(\"token_type\", token_data[\"token_type\"], expires=token_data[\"expires_in\"])\n\n        headers = {\"Authorization\": f\"{token_data['token_type']} {token_data['token']}\"}\n        data = {\n            \"slug\": slug,\n            \"type\": js_data[\"type\"],\n        }\n        res = self.session.http.post(\n            self.stream_url,\n            headers=headers,\n            json=data,\n        )\n        encrypted_data = self.session.http.json(\n            res,\n            schema=self.encrypted_data_schema,\n        )\n\n        stream_data = parse_json(\n            self.decrypt_data(js_data[\"cipher_data\"], encrypted_data),\n            schema=self.stream_schema,\n        )\n\n        self.author = stream_data[\"channel_name\"]\n        self.category = stream_data[\"genres\"]\n        self.title = stream_data[\"meta_title\"]\n\n        return stream_data[\"live_stream_url\"]\n", "type": "function"}, {"name": "_get_streams", "is_method": true, "class_name": "WWENetwork", "parameters": ["self"], "calls": ["self._login", "self._get_streams_content", "self.get_option", "self.get_option", "self.match.groups"], "code_location": {"file": "wwenetwork.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 167, "end_line": 169}, "code_snippet": "    def _get_streams(self):\n        if token := self._login(self.get_option(\"email\"), self.get_option(\"password\")):\n            return self._get_streams_content(*self.match.groups(), token)\n", "type": "function"}, {"name": "_streams_brightcove_js", "is_method": true, "class_name": "BFMTV", "parameters": ["self", "root"], "calls": ["re.compile", "validate.Schema", "validate.Schema", "self._brightcove", "validate.xml_findall", "validate.filter", "validate.get", "validate.transform", "re.compile", "validate.union_get", "schema_brightcove_js.validate", "log.debug", "self.session.http.get", "urljoin", "re_js_src.search", "elem.attrib.get"], "code_location": {"file": "bfmtv.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 52, "end_line": 72}, "code_snippet": "    def _streams_brightcove_js(self, root):\n        re_js_src = re.compile(r\"^[\\w/]+/main\\.\\w+\\.js$\")\n        schema_brightcove_js = validate.Schema(\n            validate.xml_findall(r\".//script[@src]\"),\n            validate.filter(lambda elem: re_js_src.search(elem.attrib.get(\"src\")) is not None),\n            validate.get(0),\n            str,\n            validate.transform(lambda src: urljoin(self.url, src)),\n        )\n        schema_brightcove_js2 = validate.Schema(\n            re.compile(r\"\"\"i\\?\\([A-Z]=\"[^\"]+\",y=\"(?P<video_id>\\d+).*\"data-account\"\\s*:\\s*\"(?P<account_id>\\d+)\"\"\"),\n            validate.union_get(\"account_id\", \"video_id\"),\n        )\n        try:\n            js_url = schema_brightcove_js.validate(root)\n            log.debug(f\"JS URL: {js_url}\")\n            account_id, video_id = self.session.http.get(js_url, schema=schema_brightcove_js2)\n        except PluginError:\n            return\n\n        return self._brightcove(account_id, video_id)\n", "type": "function"}, {"name": "_get_streams", "is_method": true, "class_name": "RTPPlay", "parameters": ["self"], "calls": ["self.session.http.headers.update", "re.compile", "self.session.http.get", "HLSStream.parse_variant_playlist", "validate.Schema", "validate.transform", "validate.any", "validate.all", "validate.all", "validate.all", "next", "validate.get", "validate.any", "validate.get", "validate.parse_json", "validate.transform", "validate.url", "validate.get", "validate.parse_json", "validate.transform", "validate.transform", "validate.url", "reversed", "validate.url", "list", "unquote", "unquote", "decode", "re_m3u8.finditer", "join", "join", "b64decode"], "code_location": {"file": "rtpplay.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 21, "end_line": 73}, "code_snippet": "    def _get_streams(self):\n        self.session.http.headers.update({\n            \"User-Agent\": useragents.CHROME,\n            \"Referer\": self.url,\n        })\n\n        re_m3u8 = re.compile(\n            r\"\"\"\n                hls\\s*:\\s*(?:\n                    (?P<q>[\"'])(?P<string>.*?)(?P=q)\n                    |\n                    decodeURIComponent\\s*\\((?P<obfuscated>\\[.*?])\\.join\\(\n                    |\n                    atob\\s*\\(\\s*decodeURIComponent\\s*\\((?P<obfuscated_b64>\\[.*?])\\.join\\(\n                )\n            \"\"\",\n            re.VERBOSE,\n        )\n\n        hls_url = self.session.http.get(\n            self.url,\n            schema=validate.Schema(\n                validate.transform(lambda text: next(reversed(list(re_m3u8.finditer(text))), None)),\n                validate.any(\n                    None,\n                    validate.all(\n                        validate.get(\"string\"),\n                        str,\n                        validate.any(\n                            \"\",\n                            validate.url(),\n                        ),\n                    ),\n                    validate.all(\n                        validate.get(\"obfuscated\"),\n                        str,\n                        validate.parse_json(),\n                        validate.transform(lambda arr: unquote(\"\".join(arr))),\n                        validate.url(),\n                    ),\n                    validate.all(\n                        validate.get(\"obfuscated_b64\"),\n                        str,\n                        validate.parse_json(),\n                        validate.transform(lambda arr: unquote(\"\".join(arr))),\n                        validate.transform(lambda b64: b64decode(b64).decode(\"utf-8\")),\n                        validate.url(),\n                    ),\n                ),\n            ),\n        )\n        if hls_url:\n            return HLSStream.parse_variant_playlist(self.session, hls_url)\n", "type": "function"}, {"name": "_get_streams", "is_method": true, "class_name": "Mjunoon", "parameters": ["self"], "calls": ["self.match.group", "log.debug", "self.get_data", "log.debug", "self.get_stream", "log.debug", "HLSStream.parse_variant_playlist"], "code_location": {"file": "mjunoon.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 171, "end_line": 184}, "code_snippet": "    def _get_streams(self):\n        slug = self.match.group(1)\n        log.debug(f\"Slug={slug}\")\n\n        js_data = self.get_data()\n        if not js_data:\n            return\n\n        log.debug(f\"JS data={js_data}\")\n\n        hls_url = self.get_stream(slug, js_data)\n        log.debug(f\"HLS URL={hls_url}\")\n\n        return HLSStream.parse_variant_playlist(self.session, hls_url)\n", "type": "function"}, {"name": "_get_streams", "is_method": true, "class_name": "Streann", "parameters": ["self"], "calls": ["self.session.http.headers.update", "base64.b64decode", "self.passphrase", "self.session.http.get", "attrib.get", "self.get_option", "log.error", "urlparse", "log.debug", "decrypt_openssl", "parse_qsd", "log.trace", "self.get_token", "self.stream_url.format", "log.debug", "HLSStream.parse_variant_playlist", "urlparse", "log.error", "urlparse", "params.decode", "format", "validate.Schema", "self.get_option", "validate.parse_html", "validate.xml_findall", "validate.filter", "urlparse", "elem.attrib.get"], "code_location": {"file": "streann.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 137, "end_line": 176}, "code_snippet": "    def _get_streams(self):\n        if not self.matches[\"streann\"]:\n            self._domain = urlparse(self.url).netloc\n            iframes = self.session.http.get(\n                self.url,\n                schema=validate.Schema(\n                    validate.parse_html(),\n                    validate.xml_findall(\".//iframe[@src]\"),\n                    validate.filter(lambda elem: urlparse(elem.attrib.get(\"src\")).netloc == \"ott.streann.com\"),\n                ),\n            )\n            if not iframes:\n                log.error(\"Could not find 'ott.streann.com' iframe\")\n                return\n            self.url = iframes[0].attrib.get(\"src\")\n\n        if not self._domain and self.get_option(\"url\"):\n            self._domain = urlparse(self.get_option(\"url\")).netloc\n\n        if self._domain is None:\n            log.error(\"Missing source URL, use --streann-url\")\n            return\n\n        self.session.http.headers.update({\"Referer\": self.url})\n        # Get the query string\n        encrypted_data = urlparse(self.url).query\n        data = base64.b64decode(encrypted_data)\n        # and decrypt it\n        passphrase = self.passphrase()\n        if passphrase:\n            log.debug(\"Found passphrase\")\n            params = decrypt_openssl(data, passphrase)\n            config = parse_qsd(params.decode(\"utf8\"))\n            log.trace(f\"config: {config!r}\")\n            token = self.get_token(**config)\n            if not token:\n                return\n            hls_url = self.stream_url.format(time=self.time, deviceId=self.device_id, token=token, **config)\n            log.debug(\"URL={0}\".format(hls_url))\n            return HLSStream.parse_variant_playlist(self.session, hls_url, acceptable_status=(200, 403, 404, 500))\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1604506969451904}
{"question": "Why does the frame metadata storage class facilitate accurate frame rendering and synchronization within the browser debugging protocol's screen capture functionality?", "answer": "", "relative_code_list": null, "ground_truth": "The ScreencastFrameMetadata class serves to encapsulate critical spatial, temporal, and device-specific metadata required for accurately rendering and synchronizing individual frames in a screencast session. It provides essential information including DIP-based device dimensions (device_width, device_height), page scaling factors (page_scale_factor), scroll offsets in CSS pixels (scroll_offset_x, scroll_offset_y), top offset positioning (offset_top), and optional timestamp data (timestamp) that enables proper frame alignment, viewport calculation, and temporal coordination between the browser's rendering engine and the consuming application. The class's JSON serialization/deserialization methods (to_json, from_json) ensure seamless interoperability with the Chrome DevTools Protocol by maintaining proper data type conversion and handling optional timestamp fields through the network.TimeSinceEpoch type.", "score": null, "retrieved_content": [{"name": "ScreencastFrameMetadata", "docstring": "Screencast frame metadata.", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 840, "end_line": 887}, "type": "class"}, {"name": "ScreencastFrame", "docstring": "**EXPERIMENTAL**\n\nCompressed image data requested by the ``startScreencast``.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3880, "end_line": 3899}, "type": "class"}, {"name": "Frame", "docstring": "Information about the Frame on the page.", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 549, "end_line": 644}, "type": "class"}, {"name": "FrameResource", "docstring": "Information about the Resource on the page.", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 648, "end_line": 698}, "type": "class"}, {"name": "FrameTree", "docstring": "Information about the Frame hierarchy.", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 733, "end_line": 755}, "type": "class"}, {"name": "FrameAttached", "docstring": "Fired when frame has been attached to its parent.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3394, "end_line": 3411}, "type": "class"}, {"name": "from_json", "is_method": true, "class_name": "ScreencastFrame", "parameters": ["cls", "json"], "calls": ["cls", "str", "ScreencastFrameMetadata.from_json", "int"], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3894, "end_line": 3899}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> ScreencastFrame:\n        return cls(\n            data=str(json[\"data\"]),\n            metadata=ScreencastFrameMetadata.from_json(json[\"metadata\"]),\n            session_id=int(json[\"sessionId\"]),\n        )\n", "type": "function"}, {"name": "FrameResourceTree", "docstring": "Information about the Frame hierarchy along with their cached resources.", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 702, "end_line": 729}, "type": "class"}, {"name": "IFrameStreamInfo", "docstring": "", "methods": [], "attributes": [], "code_location": {"file": "segment.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/hls", "start_line": 116, "end_line": 121}, "type": "class"}, {"name": "from_json", "is_method": true, "class_name": "ScreencastFrameMetadata", "parameters": ["cls", "json"], "calls": ["cls", "float", "float", "float", "float", "float", "float", "network.TimeSinceEpoch.from_json"], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 878, "end_line": 887}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> ScreencastFrameMetadata:\n        return cls(\n            offset_top=float(json[\"offsetTop\"]),\n            page_scale_factor=float(json[\"pageScaleFactor\"]),\n            device_width=float(json[\"deviceWidth\"]),\n            device_height=float(json[\"deviceHeight\"]),\n            scroll_offset_x=float(json[\"scrollOffsetX\"]),\n            scroll_offset_y=float(json[\"scrollOffsetY\"]),\n            timestamp=network.TimeSinceEpoch.from_json(json[\"timestamp\"]) if \"timestamp\" in json else None,\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1708984375}
{"question": "Why might specific performance bottlenecks arise from the stream multiplexer instantiation pattern when handling multiple concurrent adaptive streaming protocol streams with mixed video and audio representations?", "answer": "", "relative_code_list": null, "ground_truth": "The FFMPEGMuxer instantiation pattern in the DASH stream handling could create performance bottlenecks in several ways: 1) Each concurrent stream with mixed representations requires a separate FFMPEGMuxer instance, leading to increased memory usage and process overhead. 2) The synchronous nature of muxer creation during stream opening could cause blocking operations that delay stream initialization. 3) Multiple concurrent muxer instances may compete for system resources like CPU and I/O bandwidth. 4) The lack of muxer pooling or reuse mechanism means each stream open operation incurs the full initialization cost. 5) Under high load conditions, the cumulative overhead of multiple FFMPEG processes could degrade overall system performance and increase latency in stream delivery.", "score": null, "retrieved_content": [{"name": "MuxedHLSStream", "docstring": "Muxes multiple HLS video and audio streams into one output stream.", "methods": ["__init__", "to_manifest_url"], "attributes": ["__shortname__"], "code_location": {"file": "hls.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/hls", "start_line": 572, "end_line": 635}, "type": "class"}, {"name": "test_stream_open_video_audio", "is_method": true, "class_name": "TestDASHStreamOpen", "parameters": ["self", "session", "timestamp", "muxer", "reader"], "calls": ["Mock", "Mock", "Mock", "Mock", "DASHStream", "stream.open", "Mock", "call", "call", "call"], "code_location": {"file": "test_dash.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/dash", "start_line": 393, "end_line": 411}, "code_snippet": "    def test_stream_open_video_audio(self, session: Streamlink, timestamp: datetime, muxer: Mock, reader: Mock):\n        rep_video = Mock(ident=(None, None, \"1\"), mimeType=\"video/mp4\")\n        rep_audio = Mock(ident=(None, None, \"2\"), mimeType=\"audio/mp3\", lang=\"en\")\n\n        mock_reader_video = Mock()\n        mock_reader_audio = Mock()\n        readers = {rep_video: mock_reader_video, rep_audio: mock_reader_audio}\n        reader.side_effect = lambda _stream, _representation, _timestamp, *_, **__: readers[_representation]\n\n        stream = DASHStream(session, Mock(), rep_video, rep_audio)\n        stream.open()\n\n        assert reader.call_args_list == [\n            call(stream, rep_video, timestamp, name=\"video\"),\n            call(stream, rep_audio, timestamp, name=\"audio\"),\n        ]\n        assert mock_reader_video.open.call_count == 1\n        assert mock_reader_audio.open.call_count == 1\n        assert muxer.call_args_list == [call(session, mock_reader_video, mock_reader_audio, copyts=True)]\n", "type": "function"}, {"name": "test_multiple", "is_method": true, "class_name": "TestHlsExtAudio", "parameters": ["self", "session", "stream"], "calls": ["pytest.mark.parametrize", "isinstance", "pytest.param", "pytest.param", "pytest.param", "pytest.param"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 1194, "end_line": 1200}, "code_snippet": "    def test_multiple(self, session: Streamlink, stream: MuxedHLSStream):\n        assert isinstance(stream, MuxedHLSStream)\n        assert [substream.url for substream in stream.substreams] == [\n            \"http://mocked/path/playlist.m3u8\",\n            \"http://mocked/path/en.m3u8\",\n            \"http://mocked/path/es.m3u8\",\n        ]\n", "type": "function"}, {"name": "_select_streams", "is_method": true, "class_name": "SVTPlay", "parameters": ["self", "videos", "subtitles"], "calls": ["stream_priorities.items", "HLSStream.parse_variant_playlist", "DASHStream.parse_manifest", "self.session.get_option", "HTTPStream", "MuxedStream", "dash_streams.items"], "code_location": {"file": "svtplay.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 112, "end_line": 149}, "code_snippet": "    def _select_streams(self, videos, subtitles):\n        # the goal is to have streams with the widest range of qualities/substreams and highest bitrate at the top\n        stream_priorities = {\n            \"dashhbbtv\": DASHStream,  # DASH AVC\n            \"dash-hbbtv-avc\": DASHStream,  # DASH AVC\n            \"dash-avc\": DASHStream,  # DASH AVC\n            \"dash-full\": DASHStream,  # DASH AVC\n            \"dash\": DASHStream,  # DASH AVC\n            \"hlswebvtt\": HLSStream,  # HLS with subtitles\n            \"hls-cmaf-live-vtt\": HLSStream,  # HLS with subtitles\n            \"hls-ts-avc\": HLSStream,  # HLS with MPEG-TS\n            \"hls-ts-full\": HLSStream,  # HLS with MPEG-TS\n            \"hls\": HLSStream,  # HLS with MPEG-TS\n            \"hls-cmaf-live\": HLSStream,  # HLS with fMP4\n            \"hls-cmaf-full\": HLSStream,  # HLS with fMP4\n            \"dash-hbbtv-hevc\": DASHStream,  # DASH HEVC (low prio, because of potential user decoder issues)\n            \"hls-ts-lb-full\": HLSStream,  # low bitrate\n            \"hls-cmaf-lb-full\": HLSStream,  # low bitrate\n            \"dash-lb-full\": DASHStream,  # low bitrate\n        }\n\n        for fmt, streamtype in stream_priorities.items():\n            if fmt not in videos:\n                continue\n\n            if streamtype is HLSStream:\n                return HLSStream.parse_variant_playlist(self.session, videos[fmt], name_fmt=\"{pixels}_{bitrate}\")\n\n            if streamtype is DASHStream:\n                subtitlestreams = {}\n                if self.session.get_option(\"mux-subtitles\") and \"webvtt\" in subtitles:\n                    subtitlestreams[\"webvtt\"] = HTTPStream(self.session, subtitles[\"webvtt\"])\n\n                dash_streams = DASHStream.parse_manifest(self.session, videos[fmt])\n                if not subtitlestreams:\n                    return dash_streams\n\n                return {q: MuxedStream(self.session, s, subtitles=subtitlestreams) for q, s in dash_streams.items()}\n", "type": "function"}, {"name": "test_audio_multi", "is_method": true, "class_name": "TestDASHStreamParseManifest", "parameters": ["self", "session", "mpd"], "calls": ["Mock", "Mock", "DASHStream.parse_manifest", "sorted", "sorted", "call", "streams.keys", "Mock", "Mock", "Mock", "Mock", "Mock"], "code_location": {"file": "test_dash.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/dash", "start_line": 173, "end_line": 187}, "code_snippet": "    def test_audio_multi(self, session: Streamlink, mpd: Mock):\n        adaptationset = Mock(\n            contentProtections=None,\n            representations=[\n                Mock(id=\"1\", contentProtections=None, mimeType=\"video/mp4\", height=720),\n                Mock(id=\"2\", contentProtections=None, mimeType=\"video/mp4\", height=1080),\n                Mock(id=\"3\", contentProtections=None, mimeType=\"audio/aac\", bandwidth=128.0, lang=\"en\"),\n                Mock(id=\"4\", contentProtections=None, mimeType=\"audio/aac\", bandwidth=256.0, lang=\"en\"),\n            ],\n        )\n        mpd.return_value = Mock(periods=[Mock(adaptationSets=[adaptationset])])\n\n        streams = DASHStream.parse_manifest(session, \"http://test/manifest.mpd\")\n        assert mpd.call_args_list == [call(ANY, url=\"http://test/manifest.mpd\", base_url=\"http://test/manifest.mpd\")]\n        assert sorted(streams.keys()) == sorted([\"720p+a128k\", \"1080p+a128k\", \"720p+a256k\", \"1080p+a256k\"])\n", "type": "function"}, {"name": "test_duplicated_resolutions_sorted_bandwidth", "is_method": true, "class_name": "TestDASHStreamParseManifest", "parameters": ["self", "session", "mpd"], "calls": ["Mock", "Mock", "DASHStream.parse_manifest", "getattr", "pytest.approx", "getattr", "pytest.approx", "getattr", "pytest.approx", "call", "Mock", "Mock", "Mock", "Mock"], "code_location": {"file": "test_dash.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/dash", "start_line": 280, "end_line": 295}, "code_snippet": "    def test_duplicated_resolutions_sorted_bandwidth(self, session: Streamlink, mpd: Mock):\n        adaptationset = Mock(\n            contentProtections=None,\n            representations=[\n                Mock(id=\"1\", contentProtections=None, mimeType=\"video/mp4\", height=1080, bandwidth=64.0),\n                Mock(id=\"2\", contentProtections=None, mimeType=\"video/mp4\", height=1080, bandwidth=128.0),\n                Mock(id=\"3\", contentProtections=None, mimeType=\"video/mp4\", height=1080, bandwidth=32.0),\n            ],\n        )\n        mpd.return_value = Mock(periods=[Mock(adaptationSets=[adaptationset])])\n\n        streams = DASHStream.parse_manifest(session, \"http://test/manifest.mpd\")\n        assert mpd.call_args_list == [call(ANY, url=\"http://test/manifest.mpd\", base_url=\"http://test/manifest.mpd\")]\n        assert getattr(streams[\"1080p\"].video_representation, \"bandwidth\", None) == pytest.approx(128.0)\n        assert getattr(streams[\"1080p_alt\"].video_representation, \"bandwidth\", None) == pytest.approx(64.0)\n        assert getattr(streams[\"1080p_alt2\"].video_representation, \"bandwidth\", None) == pytest.approx(32.0)\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "MuxedHLSStream", "parameters": ["self", "session", "video", "audio", "hlsstream", "multivariant", "force_restart", "ffmpeg_options"], "calls": ["maps.extend", "__init__", "isinstance", "TStream", "tracks.extend", "tracks.append", "enumerate", "super", "range", "len"], "code_location": {"file": "hls.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/hls", "start_line": 579, "end_line": 627}, "code_snippet": "    def __init__(\n        self,\n        session: Streamlink,\n        video: str,\n        audio: str | list[str],\n        hlsstream: type[TMuxedHLSStream_co] | None = None,\n        multivariant: M3U8 | None = None,\n        force_restart: bool = False,\n        ffmpeg_options: Mapping[str, Any] | None = None,\n        **kwargs,\n    ):\n        \"\"\"\n        :param session: Streamlink session instance\n        :param video: Video stream URL\n        :param audio: Audio stream URL or list of URLs\n        :param hlsstream: The :class:`HLSStream` class of each sub-stream\n        :param url_master: The URL of the HLS playlist's multivariant playlist (deprecated)\n        :param multivariant: The parsed multivariant playlist\n        :param force_restart: Start from the beginning after reaching the playlist's end\n        :param ffmpeg_options: Additional keyword arguments passed to :class:`ffmpegmux.FFMPEGMuxer`\n        :param kwargs: Additional keyword arguments passed to :class:`HLSStream`\n        \"\"\"\n\n        tracks = [video]\n        maps = [\"0:v?\", \"0:a?\"]\n        if audio:\n            if isinstance(audio, list):\n                tracks.extend(audio)\n            else:\n                tracks.append(audio)\n        maps.extend(f\"{i}:a\" for i in range(1, len(tracks)))\n\n        # https://github.com/python/mypy/issues/18017\n        TStream: type[TMuxedHLSStream_co] = hlsstream if hlsstream is not None else HLSStream  # type: ignore[assignment]\n        substreams = [\n            TStream(\n                session,\n                url,\n                multivariant=multivariant,\n                force_restart=force_restart,\n                name=None if idx == 0 else \"audio\",\n                **kwargs,\n            )\n            for idx, url in enumerate(tracks)\n        ]\n        ffmpeg_options = ffmpeg_options or {}\n\n        super().__init__(session, *substreams, format=\"mpegts\", maps=maps, **ffmpeg_options)\n        self.multivariant = multivariant if multivariant and multivariant.is_master else None\n", "type": "function"}, {"name": "test_repr", "is_method": false, "class_name": null, "parameters": ["session"], "calls": ["HLSStream", "M3U8", "HLSStream", "repr", "repr"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 100, "end_line": 107}, "code_snippet": "def test_repr(session: Streamlink):\n    stream = HLSStream(session, \"https://foo.bar/playlist.m3u8\")\n    assert repr(stream) == \"<HLSStream ['hls', 'https://foo.bar/playlist.m3u8']>\"\n\n    multivariant: M3U8[HLSSegment, HLSPlaylist] = M3U8(\"https://foo.bar/master.m3u8\")\n    multivariant.is_master = True\n    stream = HLSStream(session, \"https://foo.bar/playlist.m3u8\", multivariant=multivariant)\n    assert repr(stream) == \"<HLSStream ['hls', 'https://foo.bar/playlist.m3u8', 'https://foo.bar/master.m3u8']>\"\n", "type": "function"}, {"name": "test_substream_names", "is_method": true, "class_name": "TestHlsExtAudio", "parameters": ["self", "session", "stream"], "calls": ["pytest.mark.parametrize", "isinstance", "pytest.param"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 1257, "end_line": 1264}, "code_snippet": "    def test_substream_names(self, session: Streamlink, stream: MuxedHLSStream):\n        assert isinstance(stream, MuxedHLSStream)\n        assert [substream.name for substream in stream.substreams] == [\n            None,\n            \"audio\",\n            \"audio\",\n            \"audio\",\n        ]\n", "type": "function"}, {"name": "test_duplicated_resolutions", "is_method": true, "class_name": "TestDASHStreamParseManifest", "parameters": ["self", "session", "mpd"], "calls": ["Mock", "Mock", "DASHStream.parse_manifest", "sorted", "sorted", "call", "streams.keys", "Mock", "Mock", "Mock", "Mock", "Mock"], "code_location": {"file": "test_dash.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/dash", "start_line": 263, "end_line": 277}, "code_snippet": "    def test_duplicated_resolutions(self, session: Streamlink, mpd: Mock):\n        adaptationset = Mock(\n            contentProtections=None,\n            representations=[\n                Mock(id=\"1\", contentProtections=None, mimeType=\"video/mp4\", height=1080, bandwidth=128.0),\n                Mock(id=\"2\", contentProtections=None, mimeType=\"video/mp4\", height=1080, bandwidth=64.0),\n                Mock(id=\"3\", contentProtections=None, mimeType=\"video/mp4\", height=1080, bandwidth=32.0),\n                Mock(id=\"4\", contentProtections=None, mimeType=\"video/mp4\", height=720),\n            ],\n        )\n        mpd.return_value = Mock(periods=[Mock(adaptationSets=[adaptationset])])\n\n        streams = DASHStream.parse_manifest(session, \"http://test/manifest.mpd\")\n        assert mpd.call_args_list == [call(ANY, url=\"http://test/manifest.mpd\", base_url=\"http://test/manifest.mpd\")]\n        assert sorted(streams.keys()) == sorted([\"720p\", \"1080p\", \"1080p_alt\", \"1080p_alt2\"])\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1795716285705566}
{"question": "Where in the control flow of a validation function that applies multiple key path lookups to the same input value is nested dictionary traversal handled when multiple key paths are provided?", "answer": "", "relative_code_list": null, "ground_truth": "The validate.union_get function processes multiple key path tuples sequentially, where each tuple represents a nested path to traverse in the input dictionary. For each path like ('foo', 'bar'), the control flow recursively navigates through the nested dictionary structure, accessing each key in sequence. If any key in the path is missing, it typically raises a ValidationError, breaking the normal control flow. When all paths are successfully traversed, the function returns a tuple containing the extracted values from each path, maintaining the order of the input paths.", "score": null, "retrieved_content": [{"name": "test_nested_failure", "is_method": true, "class_name": "TestGetItemSchema", "parameters": ["self"], "calls": ["assert_validationerror", "pytest.raises", "validate.validate", "validate.get"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 701, "end_line": 711}, "code_snippet": "    def test_nested_failure(self):\n        dictionary = {\"foo\": {\"bar\": {\"baz\": \"qux\"}}}\n        with pytest.raises(ValidationError) as cm:\n            validate.validate(validate.get((\"foo\", \"qux\", \"baz\"), default=\"default\"), dictionary)\n        assert_validationerror(\n            cm.value,\n            \"\"\"\n                ValidationError(GetItemSchema):\n                  Item 'qux' was not found in object {'bar': {'baz': 'qux'}}\n            \"\"\",\n        )\n", "type": "function"}, {"name": "test_nested", "is_method": true, "class_name": "TestGetItemSchema", "parameters": ["self"], "calls": ["validate.validate", "validate.get"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 693, "end_line": 695}, "code_snippet": "    def test_nested(self):\n        dictionary = {\"foo\": {\"bar\": {\"baz\": \"qux\"}}}\n        assert validate.validate(validate.get((\"foo\", \"bar\", \"baz\")), dictionary) == \"qux\"\n", "type": "function"}, {"name": "test_nested", "is_method": true, "class_name": "TestUnionGetSchema", "parameters": ["self"], "calls": ["validate.validate", "validate.union_get"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 912, "end_line": 919}, "code_snippet": "    def test_nested(self):\n        assert validate.validate(\n            validate.union_get(\n                (\"foo\", \"bar\"),\n                (\"baz\", \"qux\"),\n            ),\n            {\"foo\": {\"bar\": 1}, \"baz\": {\"qux\": 2}},\n        ) == (1, 2)\n", "type": "function"}, {"name": "test_nested_default", "is_method": true, "class_name": "TestGetItemSchema", "parameters": ["self"], "calls": ["validate.validate", "validate.get"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 697, "end_line": 699}, "code_snippet": "    def test_nested_default(self):\n        dictionary = {\"foo\": {\"bar\": {\"baz\": \"qux\"}}}\n        assert validate.validate(validate.get((\"foo\", \"bar\", \"qux\"), default=\"default\"), dictionary) == \"default\"\n", "type": "function"}, {"name": "_validate_getitemschema", "is_method": false, "class_name": null, "parameters": ["schema", "value"], "calls": ["iselement", "ValidationError", "type", "ValidationError", "len", "repr", "repr", "repr", "repr"], "code_location": {"file": "_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/validate", "start_line": 255, "end_line": 283}, "code_snippet": "def _validate_getitemschema(schema: GetItemSchema, value):\n    item = schema.item if type(schema.item) is tuple and not schema.strict else (schema.item,)\n    idx = 0\n    key = None\n    try:\n        for key in item:\n            if iselement(value):\n                value = value.attrib[key]\n            else:\n                value = value[key]\n            idx += 1\n        return value\n    except (KeyError, IndexError):\n        # only return default value on last item in nested lookup\n        if idx < len(item) - 1:\n            raise ValidationError(\n                \"Item {key} was not found in object {value}\",\n                key=repr(key),\n                value=repr(value),\n                schema=GetItemSchema,\n            ) from None\n        return schema.default\n    except (TypeError, AttributeError) as err:\n        raise ValidationError(\n            \"Could not get key {key} from object {value}\",\n            key=repr(key),\n            value=repr(value),\n            schema=GetItemSchema,\n        ) from err\n", "type": "function"}, {"name": "test_strict", "is_method": true, "class_name": "TestGetItemSchema", "parameters": ["self"], "calls": ["validate.validate", "validate.get"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 713, "end_line": 718}, "code_snippet": "    def test_strict(self):\n        dictionary = {\n            (\"foo\", \"bar\", \"baz\"): \"foo-bar-baz\",\n            \"foo\": {\"bar\": {\"baz\": \"qux\"}},\n        }\n        assert validate.validate(validate.get((\"foo\", \"bar\", \"baz\"), strict=True), dictionary) == \"foo-bar-baz\"\n", "type": "function"}, {"name": "test_multiple_nested", "is_method": true, "class_name": "TestValidationError", "parameters": ["self"], "calls": ["ValidationError", "assert_validationerror", "ValidationError", "ValidationError"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 1615, "end_line": 1636}, "code_snippet": "    def test_multiple_nested(self):\n        err = ValidationError(\n            \"a\",\n            ValidationError(\"b\", \"c\"),\n            \"d\",\n            ValidationError(\"e\"),\n            \"f\",\n        )\n        assert_validationerror(\n            err,\n            \"\"\"\n                ValidationError:\n                  a\n                  ValidationError:\n                    b\n                    c\n                  d\n                  ValidationError:\n                    e\n                  f\n            \"\"\",\n        )\n", "type": "function"}, {"name": "test_nested_failure", "is_method": true, "class_name": "TestSchema", "parameters": ["self", "schema_nested"], "calls": ["assert_validationerror", "pytest.raises", "schema_nested.validate"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 63, "end_line": 72}, "code_snippet": "    def test_nested_failure(self, schema_nested: validate.Schema):\n        with pytest.raises(PluginError) as cm:\n            schema_nested.validate(\"bar\")\n        assert_validationerror(\n            cm.value,\n            \"\"\"\n                Unable to validate result: ValidationError(equality):\n                  'bar' does not equal 'foo'\n            \"\"\",\n        )\n", "type": "function"}, {"name": "test_multiple_nested_context", "is_method": true, "class_name": "TestValidationError", "parameters": ["self"], "calls": ["ValidationError", "ValidationError", "ValidationError", "ValidationError", "ValidationError", "ValidationError", "ValidationError", "assert_validationerror"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 1656, "end_line": 1688}, "code_snippet": "    def test_multiple_nested_context(self):\n        errAB = ValidationError(\"a\", \"b\")\n        errC = ValidationError(\"c\")\n        errDE = ValidationError(\"d\", \"e\")\n        errF = ValidationError(\"f\")\n        errG = ValidationError(\"g\")\n        errHI = ValidationError(\"h\", \"i\")\n        errCF = ValidationError(errC, errF)\n        errAB.__cause__ = errCF\n        errC.__cause__ = errDE\n        errF.__cause__ = errG\n        errCF.__cause__ = errHI\n        assert_validationerror(\n            errAB,\n            \"\"\"\n                ValidationError:\n                  a\n                  b\n                  Context:\n                    ValidationError:\n                      c\n                      Context:\n                        d\n                        e\n                    ValidationError:\n                      f\n                      Context:\n                        g\n                    Context:\n                      h\n                      i\n            \"\"\",\n        )\n", "type": "function"}, {"name": "test_failure_notfound", "is_method": true, "class_name": "TestDict", "parameters": ["self"], "calls": ["assert_validationerror", "pytest.raises", "validate.validate"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 257, "end_line": 266}, "code_snippet": "    def test_failure_notfound(self):\n        with pytest.raises(ValidationError) as cm:\n            validate.validate({\"foo\": \"bar\"}, {\"baz\": \"qux\"})\n        assert_validationerror(\n            cm.value,\n            \"\"\"\n                ValidationError(dict):\n                  Key 'foo' not found in {'baz': 'qux'}\n            \"\"\",\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.1743624210357666}
{"question": "Why is the domain code generator class used in generating type-safe Python modules from protocol domain definitions?", "answer": "", "relative_code_list": null, "ground_truth": "The CdpDomain class serves as the central orchestrator for generating type-safe Python modules from Chrome DevTools Protocol (CDP) domain definitions. Its purpose is to parse JSON domain specifications containing metadata, types, commands, and events, then transform them into properly structured Python code with correct type annotations and cross-domain dependencies. The class intelligently computes actual import dependencies by analyzing type references across all domain elements (types, commands, events) rather than relying on the CDP's declared dependencies, ensuring the generated modules are both functionally correct and type-safe. It handles the complete code generation process including module header formatting, proper import statements, and sequential generation of all domain components while maintaining cross-domain reference integrity.", "score": null, "retrieved_content": [{"name": "generate_code", "is_method": true, "class_name": "CdpDomain", "parameters": ["self", "ref", "package"], "calls": ["self.generate_imports", "MODULE_HEADER.format", "itertools.chain", "join", "iter", "iter", "iter", "item.generate_code"], "code_location": {"file": "generate-cdp.py", "path": "/data3/pwh/swebench-repos/streamlink/script", "start_line": 868, "end_line": 885}, "code_snippet": "    def generate_code(self, ref: str, package: str) -> str:\n        \"\"\"Generate the Python module code for a given CDP domain.\"\"\"\n        exp = \" (experimental)\" if self.experimental else \"\"\n        imports = self.generate_imports(package)\n        code = MODULE_HEADER.format(\n            ref=ref,\n            domain=self.domain,\n            experimental=exp,\n            imports=imports,\n        )\n        item_iter: Iterator[CdpEvent | CdpCommand | CdpType] = itertools.chain(\n            iter(self.types),\n            iter(self.commands),\n            iter(self.events),\n        )\n        code += \"\\n\\n\\n\".join(item.generate_code() for item in item_iter)\n        code += \"\\n\"\n        return code\n", "type": "function"}, {"name": "CdpDomain", "docstring": "A CDP domain contains metadata, types, commands, and events.", "methods": ["module", "from_json", "generate_code", "get_imports", "generate_imports"], "attributes": [], "code_location": {"file": "generate-cdp.py", "path": "/data3/pwh/swebench-repos/streamlink/script", "start_line": 834, "end_line": 920}, "type": "class"}, {"name": "module", "is_method": true, "class_name": "CdpDomain", "parameters": ["self"], "calls": ["snake_case"], "code_location": {"file": "generate-cdp.py", "path": "/data3/pwh/swebench-repos/streamlink/script", "start_line": 846, "end_line": 848}, "code_snippet": "    def module(self):\n        \"\"\"The name of the Python module for this CDP domain.\"\"\"\n        return snake_case(self.domain)\n", "type": "function"}, {"name": "generate_class_code", "is_method": true, "class_name": "CdpType", "parameters": ["self"], "calls": ["dedent", "docstring", "list", "props.sort", "join", "dedent", "indent", "indent", "dedent", "indent", "indent", "indent", "p.generate_to_json", "join", "indent", "p.generate_from_json", "from_jsons.append", "join", "indent", "operator.attrgetter", "indent", "p.generate_decl"], "code_location": {"file": "generate-cdp.py", "path": "/data3/pwh/swebench-repos/streamlink/script", "start_line": 472, "end_line": 523}, "code_snippet": "    def generate_class_code(self) -> str:\n        \"\"\"\n        Generate a class type.\n\n        Top-level types that are defined as a CDP ``object`` are turned into Python\n        dataclasses.\n        \"\"\"\n        # children = set()\n        code = dedent(f\"\"\"\\\n            @dataclass\n            class {self.id}:\\n\"\"\")\n        doc = docstring(self.description)\n        if doc:\n            code += indent(doc, 4) + \"\\n\"\n\n        # Emit property declarations. These are sorted so that optional\n        # properties come after required properties, which is required to make\n        # the dataclass constructor work.\n        props = list(self.properties)\n        props.sort(key=operator.attrgetter(\"optional\"))\n        code += \"\\n\\n\".join(indent(p.generate_decl(), 4) for p in props)\n        code += \"\\n\\n\"\n\n        # Emit to_json() method. The properties are sorted in the same order as\n        # above for readability.\n        def_to_json = dedent(\"\"\"\\\n            def to_json(self) -> T_JSON_DICT:\n                json: T_JSON_DICT = {}\n        \"\"\")\n        assigns = (p.generate_to_json(dict_=\"json\") for p in props)\n        def_to_json += indent(\"\\n\".join(assigns), 4)\n        def_to_json += \"\\n\"\n        def_to_json += indent(\"return json\", 4)\n        code += indent(def_to_json, 4) + \"\\n\\n\"\n\n        # Emit from_json() method. The properties are sorted in the same order\n        # as above for readability.\n        def_from_json = dedent(f\"\"\"\\\n            @classmethod\n            def from_json(cls, json: T_JSON_DICT) -> {self.id}:\n                return cls(\n        \"\"\")\n        from_jsons = []\n        for p in props:\n            from_json = p.generate_from_json(dict_=\"json\")\n            from_jsons.append(f\"{p.py_name}={from_json},\")\n        def_from_json += indent(\"\\n\".join(from_jsons), 8)\n        def_from_json += \"\\n\"\n        def_from_json += indent(\")\", 4)\n        code += indent(def_from_json, 4)\n\n        return code\n", "type": "function"}, {"name": "generate_util", "is_method": false, "class_name": null, "parameters": ["util_path", "ref"], "calls": ["util_path.open", "util_file.write", "UTIL.format"], "code_location": {"file": "generate-cdp.py", "path": "/data3/pwh/swebench-repos/streamlink/script", "start_line": 941, "end_line": 944}, "code_snippet": "def generate_util(util_path: Path, ref: str):\n    \"\"\"Generate a ``util.py`` that is imported by the domain module files.\"\"\"\n    with util_path.open(\"w\", encoding=\"utf-8\") as util_file:\n        util_file.write(UTIL.format(ref=ref))\n", "type": "function"}, {"name": "generate_init", "is_method": false, "class_name": null, "parameters": ["init_path", "ref", "package", "domains"], "calls": ["init_path.open", "init_file.write", "sorted", "INIT_HEADER.format", "init_file.write"], "code_location": {"file": "generate-cdp.py", "path": "/data3/pwh/swebench-repos/streamlink/script", "start_line": 933, "end_line": 938}, "code_snippet": "def generate_init(init_path: Path, ref: str, package: str, domains: list[CdpDomain]):\n    \"\"\"Generate an ``__init__.py`` that exports the specified modules.\"\"\"\n    with init_path.open(\"w\", encoding=\"utf-8\") as init_file:\n        init_file.write(INIT_HEADER.format(ref=ref))\n        for module in sorted([domain.module for domain in domains] + [\"util\"]):\n            init_file.write(f\"import {package}.{module} as {module}\\n\")\n", "type": "function"}, {"name": "generate_code", "is_method": true, "class_name": "CdpType", "parameters": ["self"], "calls": ["logger.debug", "self.generate_enum_code", "self.generate_class_code", "self.generate_primitive_code"], "code_location": {"file": "generate-cdp.py", "path": "/data3/pwh/swebench-repos/streamlink/script", "start_line": 394, "end_line": 402}, "code_snippet": "    def generate_code(self) -> str:\n        \"\"\"Generate Python code for this type.\"\"\"\n        logger.debug(f\"Generating type {self.id}: {self.type}\")\n        if self.enum:\n            return self.generate_enum_code()\n        elif self.properties:\n            return self.generate_class_code()\n        else:\n            return self.generate_primitive_code()\n", "type": "function"}, {"name": "ProtocolHandler", "docstring": "", "methods": ["to_json", "from_json"], "attributes": [], "code_location": {"file": "page.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 1438, "end_line": 1454}, "type": "class"}, {"name": "generate_code", "is_method": true, "class_name": "CdpCommand", "parameters": ["self"], "calls": ["sorted", "join", "indent", "indent", "indent", "indent", "indent", "len", "indent", "len", "generate_doc", "rstrip", "indent", "indent", "p.generate_to_json", "join", "indent", "len", "len", "join", "operator.attrgetter", "join", "p.generate_doc", "len", "join", "indent", "docstring", "len", "generate_return", "indent", "join", "indent", "indent", "p.generate_code", "rstrip", "len", "enumerate", "r.generate_return", "r.generate_doc"], "code_location": {"file": "generate-cdp.py", "path": "/data3/pwh/swebench-repos/streamlink/script", "start_line": 672, "end_line": 749}, "code_snippet": "    def generate_code(self) -> str:\n        \"\"\"Generate code for a CDP command.\"\"\"\n        # Generate the function header\n        if len(self.returns) == 0:\n            ret_type = \"None\"\n        elif len(self.returns) == 1:\n            ret_type = self.returns[0].py_annotation\n        else:\n            nested_types = \", \".join(r.py_annotation for r in self.returns)\n            ret_type = f\"tuple[{nested_types}]\"\n        ret_type = f\"Generator[T_JSON_DICT, T_JSON_DICT, {ret_type}]\"\n\n        code = \"\"\n\n        code += f\"def {self.py_name}(\"\n        ret = f\") -> {ret_type}:\\n\"\n\n        parameters = sorted(self.parameters, key=operator.attrgetter(\"optional\"))\n\n        if parameters:\n            # FIX order of parameters: optional parameters MUST come last\n            params = [f\"{p.generate_code()},\" for p in parameters]\n            code += \"\\n\"\n            code += indent(\"\\n\".join(params), 4)\n            code += \"\\n\"\n            code += ret\n        else:\n            code += ret\n\n        # Generate the docstring\n        doc = \"\"\n        if self.description:\n            doc = self.description\n        if self.experimental:\n            doc += \"\\n\\n**EXPERIMENTAL**\"\n        if parameters and doc:\n            doc += \"\\n\\n\"\n        elif not parameters and self.returns:\n            doc += \"\\n\"\n        doc += \"\\n\".join(p.generate_doc() for p in parameters)\n        if len(self.returns) == 1:\n            doc += \"\\n\"\n            ret_doc = self.returns[0].generate_doc()\n            doc += f\":returns: {ret_doc}\".rstrip()\n        elif len(self.returns) > 1:\n            doc += \"\\n\"\n            doc += \":returns: A tuple with the following items:\\n\\n\"\n            ret_docs = \"\\n\".join(f\"{i}. **{r.name}** - {r.generate_doc()}\".rstrip() for i, r in enumerate(self.returns))\n            doc += indent(ret_docs, 4)\n        if doc:\n            code += indent(docstring(doc), 4)\n\n        # Generate the function body\n        if parameters:\n            code += \"\\n\"\n            code += indent(\"params: T_JSON_DICT = {}\", 4)\n            code += \"\\n\"\n        assigns = (p.generate_to_json(dict_=\"params\", use_self=False) for p in parameters)\n        code += indent(\"\\n\".join(assigns), 4)\n        code += \"\\n\"\n        code += indent(\"cmd_dict: T_JSON_DICT = {\\n\", 4)\n        code += indent(f'\"method\": \"{self.domain}.{self.name}\",\\n', 8)\n        if parameters:\n            code += indent('\"params\": params,\\n', 8)\n        code += indent(\"}\\n\", 4)\n        code += indent(f\"{'json = ' if len(self.returns) else ''}yield cmd_dict\", 4)\n        if len(self.returns) == 0:\n            pass\n        elif len(self.returns) == 1:\n            ret = self.returns[0].generate_return(dict_=\"json\")\n            code += indent(f\"\\nreturn {ret}\", 4)\n        else:\n            ret = \"\\nreturn (\\n\"\n            expr = \"\\n\".join(f\"{r.generate_return(dict_='json')},\" for r in self.returns)\n            ret += indent(expr, 4)\n            ret += \"\\n)\"\n            code += indent(ret, 4)\n        return code\n", "type": "function"}, {"name": "generate_code", "is_method": true, "class_name": "CdpEvent", "parameters": ["self"], "calls": ["dedent", "indent", "dedent", "indent", "join", "indent", "indent", "indent", "join", "docstring", "p.generate_decl", "p.generate_from_json"], "code_location": {"file": "generate-cdp.py", "path": "/data3/pwh/swebench-repos/streamlink/script", "start_line": 790, "end_line": 820}, "code_snippet": "    def generate_code(self) -> str:\n        \"\"\"Generate code for a CDP event.\"\"\"\n        code = dedent(f\"\"\"\\\n            @event_class(\\\"{self.domain}.{self.name}\\\")\n            @dataclass\n            class {self.py_name}:\"\"\")\n\n        code += \"\\n\"\n        desc = \"\"\n        if self.description or self.experimental:\n            if self.experimental:\n                desc += \"**EXPERIMENTAL**\\n\\n\"\n\n            if self.description:\n                desc += self.description\n\n            code += indent(docstring(desc), 4)\n            code += \"\\n\"\n        code += indent(\"\\n\".join(p.generate_decl() for p in self.parameters), 4)\n        code += \"\\n\\n\"\n        def_from_json = dedent(f\"\"\"\\\n            @classmethod\n            def from_json(cls, json: T_JSON_DICT) -> {self.py_name}:\n                return cls(\n        \"\"\")\n        code += indent(def_from_json, 4)\n        from_json = \"\\n\".join(f\"{p.generate_from_json(dict_='json')},\" for p in self.parameters)\n        code += indent(from_json, 12)\n        code += \"\\n\"\n        code += indent(\")\", 8)\n        return code\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.2151978015899658}
{"question": "Why does the method responsible for retrieving master playlist files with enforced UTF-8 encoding serve a specific architectural role in the HLS streaming pipeline, distinguishing it from generic HTTP fetching mechanisms?", "answer": "", "relative_code_list": null, "ground_truth": "The _fetch_variant_playlist method serves as the specialized entry point for retrieving HLS variant playlists (master playlists) within the streaming pipeline, specifically designed to handle the UTF-8 encoding requirement mandated by the HLS specification (RFC 8216) for playlist files. Unlike generic HTTP fetchers, it enforces proper text encoding at the protocol level, ensures OSError exception handling appropriate for network-level playlist retrieval failures, and integrates with the session's HTTP layer while preserving all request arguments. This method establishes the foundation for proper M3U8 parsing by guaranteeing that playlist content is delivered with correct character encoding, preventing parsing errors that could occur with auto-detected or incorrect encodings in generic HTTP responses.", "score": null, "retrieved_content": [{"name": "playlist", "is_method": true, "class_name": "TestRTPPlay", "parameters": ["self"], "calls": ["pytest.fixture", "text", "fd.read"], "code_location": {"file": "test_rtpplay.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 56, "end_line": 58}, "code_snippet": "    def playlist(self):\n        with text(\"hls/test_master.m3u8\") as fd:\n            return fd.read()\n", "type": "function"}, {"name": "test_is_master", "is_method": true, "class_name": "TestHlsPlaylistParseErrors", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.await_close", "Mock", "self.await_read", "Playlist", "call", "call", "FakePlaylist"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 1120, "end_line": 1128}, "code_snippet": "    def test_is_master(self, mock_log):\n        self.subject([Playlist()])\n        assert self.await_read(read_all=True) == b\"\"\n        self.await_close()\n        assert self.thread.reader.buffer.closed, \"Closes the stream on initial playlist parsing error\"\n        assert mock_log.debug.mock_calls == [call(\"Reloading playlist\")]\n        assert mock_log.error.mock_calls == [\n            call(f\"Attempted to play a variant playlist, use 'hls://{self.stream.url}' instead\"),\n        ]\n", "type": "function"}, {"name": "_fetch_playlist", "is_method": true, "class_name": "HLSStreamWorker", "parameters": ["self"], "calls": ["self.session.http.get"], "code_location": {"file": "hls.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/hls", "start_line": 343, "end_line": 352}, "code_snippet": "    def _fetch_playlist(self) -> Response:\n        res = self.session.http.get(\n            self.stream.url,\n            exception=StreamError,\n            retries=self.reload_attempts,\n            **self.reader.request_params,\n        )\n        res.encoding = \"utf-8\"\n\n        return res\n", "type": "function"}, {"name": "test_hls_stream_master", "is_method": false, "class_name": null, "parameters": ["session", "common_args"], "calls": ["M3U8", "HLSStream", "stream.to_url", "stream.to_manifest_url"], "code_location": {"file": "test_stream_to_url.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream", "start_line": 64, "end_line": 69}, "code_snippet": "def test_hls_stream_master(session, common_args):\n    multivariant = M3U8(\"http://host/master.m3u8?foo=bar\")\n    multivariant.is_master = True\n    stream = HLSStream(session, \"http://host/stream.m3u8?foo=bar\", multivariant=multivariant, **common_args)\n    assert stream.to_url() == \"http://host/stream.m3u8?foo=bar&queryparamkey=queryparamval\"\n    assert stream.to_manifest_url() == \"http://host/master.m3u8?foo=bar&queryparamkey=queryparamval\"\n", "type": "function"}, {"name": "stream", "is_method": true, "class_name": "TestHlsExtAudio", "parameters": ["self", "session"], "calls": ["pytest.fixture", "HLSStream.parse_variant_playlist"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 1153, "end_line": 1157}, "code_snippet": "    def stream(self, session: Streamlink):\n        streams = HLSStream.parse_variant_playlist(session, \"http://mocked/path/master.m3u8\")\n        assert \"video\" in streams\n\n        return streams[\"video\"]\n", "type": "function"}, {"name": "test_hls_encrypted_aes128_key_uri_override", "is_method": true, "class_name": "TestHLSStreamEncrypted", "parameters": ["self"], "calls": ["self.gen_key", "bytes", "self.gen_key", "self.subject", "self.await_write", "self.await_read", "self.await_close", "self.content", "self.called", "self.called", "last_request._request.headers.get", "Playlist", "Playlist", "ord", "range", "SegmentEnc", "SegmentEnc", "range", "range", "self.get_mock"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 850, "end_line": 872}, "code_snippet": "    def test_hls_encrypted_aes128_key_uri_override(self):\n        aesKey, aesIv, key = self.gen_key(uri=\"http://real-mocked/{namespace}/encryption.key?foo=bar\")\n        aesKeyInvalid = bytes(ord(aesKey[i : i + 1]) ^ 0xFF for i in range(16))\n        _, __, key_invalid = self.gen_key(aesKeyInvalid, aesIv, uri=\"http://mocked/{namespace}/encryption.key?foo=bar\")\n\n        # noinspection PyTypeChecker\n        segments = self.subject(\n            [\n                Playlist(0, [key_invalid] + [SegmentEnc(num, aesKey, aesIv) for num in range(4)]),\n                Playlist(4, [key_invalid] + [SegmentEnc(num, aesKey, aesIv) for num in range(4, 8)], end=True),\n            ],\n            options={\"hls-segment-key-uri\": \"{scheme}://real-{netloc}{path}?{query}\"},\n        )\n\n        self.await_write(3 + 4)\n        data = self.await_read(read_all=True)\n        self.await_close()\n\n        expected = self.content(segments, prop=\"content_plain\", cond=lambda s: s.num >= 1)\n        assert data == expected, \"Decrypts stream from custom key\"\n        assert not self.called(key_invalid), \"Skips encryption key\"\n        assert self.called(key, once=True), \"Downloads custom encryption key\"\n        assert self.get_mock(key).last_request._request.headers.get(\"X-FOO\") == \"BAR\"\n", "type": "function"}, {"name": "_fetch_playlist", "is_method": true, "class_name": "FilmOnHLSStreamWorker", "parameters": ["self"], "calls": ["_fetch_playlist", "super", "log.debug"], "code_location": {"file": "filmon.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 32, "end_line": 41}, "code_snippet": "    def _fetch_playlist(self):\n        try:\n            return super()._fetch_playlist()\n        except StreamError as err:\n            # noinspection PyUnresolvedReferences\n            if err.err.response.status_code in (403, 502):\n                self.stream.watch_timeout = 0\n                self.playlist_reload_time = 0\n                log.debug(f\"Force-reloading the channel playlist on error: {err}\")\n            raise err\n", "type": "function"}, {"name": "test_hls_encrypted_aes128_with_plaintext_map", "is_method": true, "class_name": "TestHLSStreamEncrypted", "parameters": ["self"], "calls": ["self.gen_key", "TagMap", "TagMap", "self.mock", "self.mock", "self.subject", "self.await_write", "self.await_read", "self.await_close", "self.url", "self.url", "self.id", "self.id", "Playlist", "Playlist", "SegmentEnc", "SegmentEnc", "range", "range"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 825, "end_line": 848}, "code_snippet": "    def test_hls_encrypted_aes128_with_plaintext_map(self):\n        aesKey, aesIv, key = self.gen_key()\n        map1 = TagMap(1, namespace=self.id())\n        map2 = TagMap(2, namespace=self.id())\n        self.mock(\"GET\", self.url(map1), content=map1.content)\n        self.mock(\"GET\", self.url(map2), content=map2.content)\n\n        segments = self.subject([\n            Playlist(0, [map1, key] + [SegmentEnc(num, aesKey, aesIv) for num in range(2)]),\n            Playlist(2, [map2, key] + [SegmentEnc(num, aesKey, aesIv) for num in range(2, 4)], end=True),\n        ])\n\n        self.await_write(1 + 2 + 1 + 2)  # 1 map, 2 segments, 1 map, 2 segments\n        data = self.await_read(read_all=True)\n        self.await_close()\n\n        assert data == (\n            map1.content\n            + segments[0].content_plain\n            + segments[1].content_plain\n            + map2.content\n            + segments[2].content_plain\n            + segments[3].content_plain\n        )\n", "type": "function"}, {"name": "test_variant_playlist", "is_method": true, "class_name": "TestHLSVariantPlaylist", "parameters": ["self", "request", "streams"], "calls": ["pytest.mark.parametrize", "all", "all", "next", "list", "iter", "repr", "streams.keys", "isinstance", "streams.values", "streams.values", "streams.values"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 123, "end_line": 133}, "code_snippet": "    def test_variant_playlist(self, request: pytest.FixtureRequest, streams: dict[str, HLSStream]):\n        assert list(streams.keys()) == [\"720p\", \"720p_alt\", \"480p\", \"360p\", \"160p\", \"1080p (source)\", \"90k\"]\n        assert all(isinstance(stream, HLSStream) for stream in streams.values())\n        assert all(stream.multivariant is not None and stream.multivariant.is_master for stream in streams.values())\n\n        base = f\"http://mocked/{request.node.originalname}\"\n        stream = next(iter(streams.values()))\n        assert repr(stream) == f\"<HLSStream ['hls', '{base}/720p.m3u8', '{base}/master.m3u8']>\"\n\n        assert stream.multivariant is not None\n        assert stream.multivariant.uri == f\"{base}/master.m3u8\"\n", "type": "function"}, {"name": "ChzzkHLSStreamWorker", "docstring": "Custom HLS stream worker that adds __bgda__ query parameter to segment URLs", "methods": ["process_segments"], "attributes": [], "code_location": {"file": "chzzk.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 25, "end_line": 41}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.2108793258666992}
{"question": "Why is the plugin discovery and metadata extraction class used in the custom documentation generation extension for extensible plugin components?", "answer": "", "relative_code_list": null, "ground_truth": "The PluginFinder class is designed to systematically discover, parse, and extract metadata from Streamlink plugin files for Sphinx documentation generation. It locates all valid plugin modules in the streamlink.plugins package (excluding common_ prefixed and protocol plugins), parses each plugin's source code to extract metadata from docstring-like comments using regular expressions, builds AST representations to analyze plugin structure and arguments, and provides a generator interface to yield complete PluginMetadata objects for documentation rendering, while handling parsing errors gracefully through ExtensionError exceptions.", "score": null, "retrieved_content": [{"name": "PluginMetadata", "docstring": "", "methods": ["__init__", "set", "generate"], "attributes": [], "code_location": {"file": "ext_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/docs/sphinxext", "start_line": 177, "end_line": 209}, "type": "class"}, {"name": "PluginMeta", "docstring": "", "methods": ["__init__"], "attributes": [], "code_location": {"file": "plugin.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugin", "start_line": 245, "end_line": 249}, "type": "class"}, {"name": "PluginsDirective", "docstring": "", "methods": ["generate", "run"], "attributes": [], "code_location": {"file": "ext_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/docs/sphinxext", "start_line": 262, "end_line": 275}, "type": "class"}, {"name": "PluginFinder", "docstring": "", "methods": ["__init__", "get_plugins", "_parse_plugin"], "attributes": ["_re_metadata_item"], "code_location": {"file": "ext_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/docs/sphinxext", "start_line": 212, "end_line": 259}, "type": "class"}, {"name": "TestPluginMetadata", "docstring": "", "methods": ["metadata_keys_all", "metadata_keys_required", "metadata_keys_repeat", "metadata_keys_no_repeat", "tokeninfo", "metadata_items", "metadata_keys", "metadata_dict", "test_no_unknown", "test_required", "test_order", "test_repeat", "test_no_repeat", "test_key_url", "test_key_type", "test_key_metadata"], "attributes": [], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 105, "end_line": 211}, "type": "class"}, {"name": "_Plugin", "docstring": "", "methods": ["_get_streams"], "attributes": [], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/session", "start_line": 42, "end_line": 44}, "type": "class"}, {"name": "PluginVisitor", "docstring": "", "methods": ["__init__", "generic_visit", "visit_Module", "visit_Assign", "visit_ClassDef"], "attributes": [], "code_location": {"file": "plugins_json.py", "path": "/data3/pwh/swebench-repos/streamlink/build_backend", "start_line": 386, "end_line": 433}, "type": "class"}, {"name": "Plugin", "docstring": "", "methods": [], "attributes": [], "code_location": {"file": "plugins_json.py", "path": "/data3/pwh/swebench-repos/streamlink/build_backend", "start_line": 72, "end_line": 74}, "type": "class"}, {"name": "PluginMatcher", "docstring": "", "methods": [], "attributes": [], "code_location": {"file": "plugins_json.py", "path": "/data3/pwh/swebench-repos/streamlink/build_backend", "start_line": 38, "end_line": 47}, "type": "class"}, {"name": "PluginOnGithub", "docstring": "", "methods": ["__init__", "generate"], "attributes": ["url_source", "url_issues"], "code_location": {"file": "ext_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/docs/sphinxext", "start_line": 92, "end_line": 102}, "type": "class"}], "retrieved_count": 10, "cost_time": 1.218414068222046}
{"question": "Where is the asynchronous in-memory communication channel implementation for capturing and routing stdout and stderr outputs in the test utility class for simulating process output?", "answer": "", "relative_code_list": null, "ground_truth": "The trio memory channel implementation is in the __init__ method of the FakeProcessOutput class, specifically where trio.open_memory_channel(10) creates the sender and receiver channels, and the _onoutput inner function uses self.onoutput_sender.send_nowait((channel, res)) to capture and route outputs from both stdout and stderr mock methods.", "score": null, "retrieved_content": [{"name": "TestConsoleOutputStream", "docstring": "", "methods": ["_mock_new", "test_supports_status_messages", "test_write", "test_write_status_message", "test_close"], "attributes": [], "code_location": {"file": "test_stream.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli/console", "start_line": 79, "end_line": 148}, "type": "class"}, {"name": "fake_stdout", "is_method": false, "class_name": null, "parameters": ["monkeypatch", "tmp_path"], "calls": ["pytest.fixture", "_create_fd", "monkeypatch.setattr"], "code_location": {"file": "test_file.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli/output", "start_line": 33, "end_line": 37}, "code_snippet": "def fake_stdout(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):\n    # can't use in-memory io.BytesIO, since fd.fileno() is called on Windows\n    with _create_fd(tmp_path, \"stdout\") as fd:\n        monkeypatch.setattr(\"streamlink_cli.output.file.stdout\", fd)\n        yield fd\n", "type": "function"}, {"name": "output", "is_method": true, "class_name": "TestPlayerOutput", "parameters": ["self", "monkeypatch", "player_process"], "calls": ["pytest.fixture", "Mock", "monkeypatch.setattr", "monkeypatch.setattr", "FakePlayerOutput", "output.open", "Mock", "Path", "output.close"], "code_location": {"file": "test_streamrunner.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli", "start_line": 161, "end_line": 171}, "code_snippet": "    def output(self, monkeypatch: pytest.MonkeyPatch, player_process: Mock):\n        mock_popen = Mock(return_value=player_process)\n        monkeypatch.setattr(\"subprocess.Popen\", mock_popen)\n        monkeypatch.setattr(\"streamlink_cli.output.player.sleep\", Mock())\n\n        output = FakePlayerOutput(Path(\"mocked\"))\n        output.open()\n        try:\n            yield output\n        finally:\n            output.close()\n", "type": "function"}, {"name": "stream", "is_method": false, "class_name": null, "parameters": ["buffer"], "calls": ["pytest.fixture", "TextIOWrapper"], "code_location": {"file": "test_stream.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli/console", "start_line": 31, "end_line": 33}, "code_snippet": "def stream(buffer: BytesIO):\n    # stdout/stderr is line buffered, which implies flushes when writing \\n or \\r\n    return TextIOWrapper(buffer, encoding=\"utf-8\", line_buffering=True)\n", "type": "function"}, {"name": "test_stderr", "is_method": true, "class_name": "TestOpen", "parameters": ["self", "session", "popen"], "calls": ["session.options.update", "patch", "FFMPEGMuxer", "streamio.open", "streamio.close"], "code_location": {"file": "test_ffmpegmux.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream", "start_line": 563, "end_line": 572}, "code_snippet": "    def test_stderr(self, session: Streamlink, popen: Mock):\n        session.options.update({\"ffmpeg-verbose\": True})\n        with patch(\"streamlink.stream.ffmpegmux.sys.stderr\") as mock_stderr:\n            streamio = FFMPEGMuxer(session)\n\n            streamio.open()\n            assert popen.call_args_list[0][1][\"stderr\"] is mock_stderr\n\n            streamio.close()\n            assert mock_stderr.close.call_count == 0\n", "type": "function"}, {"name": "FakeProcessOutput", "docstring": "", "methods": ["__init__"], "attributes": [], "code_location": {"file": "test_processoutput.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/utils", "start_line": 40, "end_line": 69}, "type": "class"}, {"name": "get_process", "is_method": false, "class_name": null, "parameters": ["monkeypatch", "_max_test_time"], "calls": ["pytest.fixture", "trio.open_memory_channel", "monkeypatch.setattr", "receiver.receive", "task_status_started", "sender.send_nowait", "trio_run_process"], "code_location": {"file": "test_processoutput.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/utils", "start_line": 83, "end_line": 108}, "code_snippet": "def get_process(monkeypatch: pytest.MonkeyPatch, _max_test_time) -> Callable[[], Awaitable[trio.Process]]:\n    trio_run_process = trio.run_process\n\n    # use a memory channel, so we can wait until the process has launched\n    sender: trio.MemorySendChannel[trio.Process]\n    receiver: trio.MemoryReceiveChannel[trio.Process]\n    sender, receiver = trio.open_memory_channel(1)\n\n    async def get_process() -> trio.Process:\n        return await receiver.receive()\n\n    async def fake_trio_run_process(*args, task_status, **kwargs):\n        task_status_started = task_status.started\n\n        def fake_task_status_started(process: trio.Process):\n            task_status_started(process)\n            sender.send_nowait(process)\n\n        # intercept the task status report\n        task_status.started = fake_task_status_started\n\n        return await trio_run_process(*args, task_status=task_status, **kwargs)\n\n    monkeypatch.setattr(\"trio.run_process\", fake_trio_run_process)\n\n    return get_process\n", "type": "function"}, {"name": "test_streams", "is_method": false, "class_name": null, "parameters": ["console_output", "file_output", "has_file_output", "expected_console_output", "expected_file_output"], "calls": ["pytest.mark.parametrize", "ConsoleOutput", "console.msg", "console.msg_json", "getvalue", "getvalue", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param"], "code_location": {"file": "test_console.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli/console", "start_line": 113, "end_line": 133}, "code_snippet": "def test_streams(\n    console_output: ConsoleOutputStream | None,\n    file_output: TextIO | None,\n    has_file_output: bool,\n    expected_console_output: str,\n    expected_file_output: str,\n):\n    console = ConsoleOutput()\n    assert console.console_output is None\n    assert console.file_output is None\n\n    console.console_output = console_output\n    console.file_output = file_output\n    assert console.console_output is console_output\n    assert console.file_output is (file_output if has_file_output else None)\n\n    console.msg(\"foo\")\n    console.json = True\n    console.msg_json(foo=\"bar\")\n    assert getvalue(console_output) == expected_console_output\n    assert getvalue(file_output) == expected_file_output\n", "type": "function"}, {"name": "_stream_output", "is_method": false, "class_name": null, "parameters": ["monkeypatch"], "calls": ["pytest.fixture", "Mock", "Mock", "Mock", "monkeypatch.setattr", "monkeypatch.setattr", "monkeypatch.setattr"], "code_location": {"file": "test_handle_url.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli/main", "start_line": 53, "end_line": 59}, "code_snippet": "def _stream_output(monkeypatch: pytest.MonkeyPatch):\n    mock_output_stream = Mock()\n    mock_output_stream_http = Mock()\n    mock_output_stream_passthrough = Mock()\n    monkeypatch.setattr(streamlink_cli.main, \"output_stream\", mock_output_stream)\n    monkeypatch.setattr(streamlink_cli.main, \"output_stream_http\", mock_output_stream_http)\n    monkeypatch.setattr(streamlink_cli.main, \"output_stream_passthrough\", mock_output_stream_passthrough)\n", "type": "function"}, {"name": "test_write_stdout", "is_method": false, "class_name": null, "parameters": ["fake_stdout"], "calls": ["FileOutput", "fo.open", "fo.write", "fo.write", "fo.write", "fo.fd.seek", "fo.close", "fo.fd.read"], "code_location": {"file": "test_file.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli/output", "start_line": 92, "end_line": 109}, "code_snippet": "def test_write_stdout(fake_stdout: BufferedRandom):\n    fo = FileOutput(fd=fake_stdout)\n    assert fo.fd is fake_stdout\n    assert fo.filename is None\n    assert fo.record is None\n\n    fo.open()\n    assert fo.opened\n\n    fo.write(b\"foo\")\n    fo.write(b\"bar\")\n    fo.write(b\"baz\")\n    fo.fd.seek(0)\n    assert fo.fd.read() == b\"foobarbaz\"\n\n    fo.close()\n    assert not fo.opened\n    assert not fo.fd.closed\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.32493066787719727}
{"question": "Why does the decorator-based tag parsing approach introduce performance overhead compared to direct function registration in high-frequency adaptive streaming protocol stream processing scenarios?", "answer": "", "relative_code_list": null, "ground_truth": "The decorator-based approach in parse_tag introduces additional function call overhead and attribute setting operations for each decorated parser function. In high-frequency HLS stream processing where thousands of tags per second may need parsing, this overhead accumulates through: 1) Nested function calls (decorator factory pattern), 2) Runtime attribute assignment via setattr for each parser registration, 3) Indirect function lookup through attribute-based dispatch. Compared to direct function registration in a static dictionary or mapping, this dynamic approach can cause 15-25% higher CPU utilization and increased memory footprint due to the additional closure objects and attribute metadata. The performance impact is most significant when processing large manifests with numerous short-lived tag parsing operations, where the decorator setup cost outweighs the parsing execution time itself.", "score": null, "retrieved_content": [{"name": "parse_tag", "is_method": false, "class_name": null, "parameters": ["tag"], "calls": ["setattr"], "code_location": {"file": "m3u8.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/hls", "start_line": 94, "end_line": 100}, "code_snippet": "def parse_tag(tag: str):\n    def decorator(func: Callable[[str], None]) -> Callable[[str], None]:\n        setattr(func, _symbol_tag_parser, tag)\n\n        return func\n\n    return decorator\n", "type": "function"}, {"name": "test_parse_tag_mapping", "is_method": false, "class_name": null, "parameters": [], "calls": ["M3U8Parser", "hasattr", "M3U8ParserSubclass", "hasattr", "M3U8ParserSubclass", "hasattr", "parse_tag", "parse_tag"], "code_location": {"file": "test_m3u8.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 30, "end_line": 59}, "code_snippet": "def test_parse_tag_mapping():\n    class M3U8ParserSubclass(M3U8Parser):\n        @parse_tag(\"EXT-X-VERSION\")\n        def parse_tag_ext_x_version(self):  # pragma: no cover\n            pass\n\n        @parse_tag(\"FOO-BAR\")\n        def parse_tag_foo_bar(self):  # pragma: no cover\n            pass\n\n    parent = M3U8Parser()\n    assert hasattr(parent, \"_TAGS\")\n    assert parent._TAGS[\"EXTINF\"] is M3U8Parser.parse_tag_extinf\n    assert parent._TAGS[\"EXT-X-VERSION\"] is M3U8Parser.parse_tag_ext_x_version\n    assert \"FOO-BAR\" not in parent._TAGS\n\n    childA = M3U8ParserSubclass()\n    assert hasattr(childA, \"_TAGS\")\n    assert childA._TAGS[\"EXTINF\"] is M3U8Parser.parse_tag_extinf\n    assert childA._TAGS[\"EXT-X-VERSION\"] is M3U8ParserSubclass.parse_tag_ext_x_version\n    assert childA._TAGS[\"FOO-BAR\"] is M3U8ParserSubclass.parse_tag_foo_bar\n\n    assert parent._TAGS is not childA._TAGS\n\n    childB = M3U8ParserSubclass()\n    assert hasattr(childB, \"_TAGS\")\n    assert childA._TAGS is childB._TAGS\n\n    assert parent._TAGS[\"EXT-X-VERSION\"].__doc__ is not None\n    assert childA._TAGS[\"EXT-X-VERSION\"].__doc__ is None\n", "type": "function"}, {"name": "parse_tag_ext_x_daterange", "is_method": true, "class_name": "TwitchM3U8Parser", "parameters": ["self", "value"], "calls": ["parse_tag", "parse_tag_ext_x_daterange", "self._is_daterange_ad", "self.m3u8.dateranges_ads.append", "log.trace", "super"], "code_location": {"file": "twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 131, "end_line": 136}, "code_snippet": "    def parse_tag_ext_x_daterange(self, value):\n        super().parse_tag_ext_x_daterange(value)\n        daterange = self.m3u8.dateranges[-1]\n        if self._is_daterange_ad(daterange):\n            self.m3u8.dateranges_ads.append(daterange)\n            log.trace(f\"Ad daterange: {daterange!r}\")  # type: ignore[attr-defined]\n", "type": "function"}, {"name": "parse_tag_ext_x_twitch_prefetch", "is_method": true, "class_name": "TwitchM3U8Parser", "parameters": ["self", "value"], "calls": ["parse_tag", "dataclass_replace", "segments.append", "timedelta", "self._is_segment_ad", "sum", "float", "self.uri", "len"], "code_location": {"file": "twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 93, "end_line": 128}, "code_snippet": "    def parse_tag_ext_x_twitch_prefetch(self, value):\n        segments = self.m3u8.segments\n        if not segments:  # pragma: no cover\n            return\n        last = segments[-1]\n\n        # Use the average duration of all regular segments for the duration of prefetch segments.\n        # This is better than using the duration of the last segment when regular segment durations vary a lot.\n        # In low latency mode, the playlist reload time is the duration of the last segment.\n        duration = last.duration if last.prefetch else sum(segment.duration for segment in segments) / float(len(segments))\n\n        # Use the last duration for extrapolating the start time of the prefetch segment, which is needed for checking\n        # whether it is an ad segment and matches the parsed date ranges or not\n        date = last.date + timedelta(seconds=last.duration)\n\n        # Always treat prefetch segments after a discontinuity as ad segments\n        # (discontinuity tag inserted after last regular segment)\n        # Don't reset discontinuity state: the date extrapolation might be inaccurate,\n        # so all following prefetch segments should be considered an ad after a discontinuity\n        ad = self._discontinuity or self._is_segment_ad(date)\n\n        # Since we don't reset the discontinuity state in prefetch segments for the purpose of ad detection,\n        # set the prefetch segment's discontinuity attribute based on ad transitions\n        discontinuity = ad != last.ad\n\n        segment = dataclass_replace(\n            last,\n            uri=self.uri(value),\n            duration=duration,\n            title=None,\n            discontinuity=discontinuity,\n            date=date,\n            ad=ad,\n            prefetch=True,\n        )\n        segments.append(segment)\n", "type": "function"}, {"name": "test_split_tag", "is_method": false, "class_name": null, "parameters": ["string", "expected"], "calls": ["pytest.mark.parametrize", "M3U8Parser.split_tag"], "code_location": {"file": "test_m3u8.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 72, "end_line": 73}, "code_snippet": "def test_split_tag(string: str, expected: tuple[str, str] | tuple[None, None]):\n    assert M3U8Parser.split_tag(string) == expected\n", "type": "function"}, {"name": "parse_tag_ext_x_prefetch", "is_method": true, "class_name": "KickM3U8Parser", "parameters": ["self", "value"], "calls": ["parse_tag", "dataclass_replace", "segments.append", "sum", "float", "self.uri", "len"], "code_location": {"file": "kick.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 56, "end_line": 74}, "code_snippet": "    def parse_tag_ext_x_prefetch(self, value):\n        segments = self.m3u8.segments\n        if not segments:  # pragma: no cover\n            return\n        last = segments[-1]\n\n        # Use the average duration of all regular segments for the duration of prefetch segments.\n        # This is better than using the duration of the last segment when regular segment durations vary a lot.\n        # In low latency mode, the playlist reload time is the duration of the last segment.\n        duration = last.duration if last.prefetch else sum(segment.duration for segment in segments) / float(len(segments))\n\n        segment = dataclass_replace(\n            last,\n            uri=self.uri(value),\n            duration=duration,\n            title=None,\n            prefetch=True,\n        )\n        segments.append(segment)\n", "type": "function"}, {"name": "test_decorator", "is_method": true, "class_name": "TestPluginMatcher", "parameters": ["self"], "calls": ["pytest.raises", "str", "pluginmatcher", "re.compile"], "code_location": {"file": "test_plugin.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 129, "end_line": 137}, "code_snippet": "    def test_decorator(self):\n        class MyPlugin:\n            pass\n\n        with pytest.raises(TypeError) as cm:\n            # noinspection PyTypeChecker\n            pluginmatcher(re.compile(r\"\"))(MyPlugin)\n\n        assert str(cm.value) == \"MyPlugin is not a Plugin\"\n", "type": "function"}, {"name": "parse_line", "is_method": true, "class_name": "M3U8Parser", "parameters": ["self", "line"], "calls": ["line.startswith", "self.split_tag", "self.get_segment", "self.m3u8.segments.append", "self.uri", "self.get_playlist", "self.m3u8.playlists.append", "self.uri"], "code_location": {"file": "m3u8.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/hls", "start_line": 561, "end_line": 576}, "code_snippet": "    def parse_line(self, line: str) -> None:\n        if line.startswith(\"#\"):\n            tag, value = self.split_tag(line)\n            if not tag or value is None or tag not in self._TAGS:\n                return\n            self._TAGS[tag](self, value)\n\n        elif self._expect_segment:\n            self._expect_segment = False\n            segment = self.get_segment(self.uri(line))\n            self.m3u8.segments.append(segment)\n\n        elif self._expect_playlist:\n            self._expect_playlist = False\n            playlist = self.get_playlist(self.uri(line))\n            self.m3u8.playlists.append(playlist)\n", "type": "function"}, {"name": "parse_ext_x_twitch_live_sequence", "is_method": true, "class_name": "TwitchM3U8Parser", "parameters": ["self", "value"], "calls": ["parse_tag"], "code_location": {"file": "twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 86, "end_line": 90}, "code_snippet": "    def parse_ext_x_twitch_live_sequence(self, value):\n        # Unset discontinuity state if the previous segment was not an ad,\n        # as the following segment won't be an ad\n        if self.m3u8.segments and not self.m3u8.segments[-1].ad:\n            self._discontinuity = False\n", "type": "function"}, {"name": "test_hls_daterange_by_class", "is_method": true, "class_name": "TestTwitchHLSStream", "parameters": ["self"], "calls": ["TagDateRangeAd", "self.subject", "self.await_write", "self.await_read", "all", "Playlist", "self.called", "segments.values", "Segment", "Segment"], "code_location": {"file": "test_twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 227, "end_line": 246}, "code_snippet": "    def test_hls_daterange_by_class(self):\n        daterange = TagDateRangeAd(\n            start=DATETIME_BASE,\n            duration=1,\n            attrid=\"foo\",\n            classname=\"twitch-stitched-ad\",\n            custom=None,\n        )\n\n        segments = self.subject(\n            [\n                Playlist(0, [daterange, Segment(0), Segment(1)], end=True),\n            ],\n            streamoptions={\"low_latency\": False},\n        )\n\n        self.await_write(2)\n        data = self.await_read(read_all=True)\n        assert data == segments[1].content, \"Filters out ad segments\"\n        assert all(self.called(s) for s in segments.values()), \"Downloads all segments\"\n", "type": "function"}], "retrieved_count": 10, "cost_time": 1.322507619857788}
{"question": "Where is the validation of session parameters and headers integration into the adaptive streaming protocol stream handler's JSON representation performed by the test function for validating stream JSON serialization without causing duplicate entries?", "answer": "", "relative_code_list": null, "ground_truth": "The test_dash_stream_url function validates the integration by first preparing a request URL using session.http.prepare_new_request with common_args (which include session parameters and headers), then creating a mock MPD instance with this URL. It initializes a DASHStream with the session, mock MPD, and common_args, and asserts that the stream's __json__ method returns a dictionary with the expected URL (including merged query parameters without duplicates) and headers. This ensures that session and stream parameters are correctly combined and deduplicated in the dataflow to the JSON output.", "score": null, "retrieved_content": [{"name": "test_dash_stream_url", "is_method": false, "class_name": null, "parameters": ["session", "common_args", "expected_headers"], "calls": ["common_args.copy", "args.update", "Mock", "DASHStream", "session.http.prepare_new_request", "stream.__json__"], "code_location": {"file": "test_stream_json.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream", "start_line": 120, "end_line": 134}, "code_snippet": "def test_dash_stream_url(session, common_args, expected_headers):\n    # DASHStream requires an MPD instance as input:\n    # The URL of the MPD instance was already prepared by DASHStream.parse_manifest, so copy this behavior here.\n    # This test verifies that session params, headers, etc. are added to the JSON data, without duplicates.\n    args = common_args.copy()\n    args.update(url=\"http://host/stream.mpd?foo=bar\")\n    url = session.http.prepare_new_request(**args).url\n\n    mpd = Mock(url=url)\n    stream = DASHStream(session, mpd, **common_args)\n    assert stream.__json__() == {\n        \"type\": \"dash\",\n        \"url\": \"http://host/stream.mpd?foo=bar&sessionqueryparamkey=sessionqueryparamval&queryparamkey=queryparamval\",\n        \"headers\": expected_headers,\n    }\n", "type": "function"}, {"name": "test_hls_stream", "is_method": false, "class_name": null, "parameters": ["session", "common_args", "expected_headers"], "calls": ["HLSStream", "stream.__json__"], "code_location": {"file": "test_stream_json.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream", "start_line": 91, "end_line": 97}, "code_snippet": "def test_hls_stream(session, common_args, expected_headers):\n    stream = HLSStream(session, \"http://host/stream.m3u8?foo=bar\", **common_args)\n    assert stream.__json__() == {\n        \"type\": \"hls\",\n        \"url\": \"http://host/stream.m3u8?foo=bar&sessionqueryparamkey=sessionqueryparamval&queryparamkey=queryparamval\",\n        \"headers\": expected_headers,\n    }\n", "type": "function"}, {"name": "test_hls_stream_master", "is_method": false, "class_name": null, "parameters": ["session", "common_args", "expected_headers"], "calls": ["M3U8", "HLSStream", "stream.__json__"], "code_location": {"file": "test_stream_json.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream", "start_line": 100, "end_line": 109}, "code_snippet": "def test_hls_stream_master(session, common_args, expected_headers):\n    multivariant = M3U8(\"http://host/master.m3u8?foo=bar\")\n    multivariant.is_master = True\n    stream = HLSStream(session, \"http://host/stream.m3u8?foo=bar\", multivariant=multivariant, **common_args)\n    assert stream.__json__() == {\n        \"type\": \"hls\",\n        \"url\": \"http://host/stream.m3u8?foo=bar&sessionqueryparamkey=sessionqueryparamval&queryparamkey=queryparamval\",\n        \"master\": \"http://host/master.m3u8?foo=bar&sessionqueryparamkey=sessionqueryparamval&queryparamkey=queryparamval\",\n        \"headers\": expected_headers,\n    }\n", "type": "function"}, {"name": "test_http_stream", "is_method": false, "class_name": null, "parameters": ["session", "common_args", "expected_headers"], "calls": ["HTTPStream", "stream.__json__"], "code_location": {"file": "test_stream_json.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream", "start_line": 80, "end_line": 88}, "code_snippet": "def test_http_stream(session, common_args, expected_headers):\n    stream = HTTPStream(session, \"http://host/path?foo=bar\", **common_args)\n    assert stream.__json__() == {\n        \"type\": \"http\",\n        \"url\": \"http://host/path?foo=bar&sessionqueryparamkey=sessionqueryparamval&queryparamkey=queryparamval\",\n        \"method\": \"GET\",\n        \"body\": None,\n        \"headers\": expected_headers,\n    }\n", "type": "function"}, {"name": "test_base_stream", "is_method": false, "class_name": null, "parameters": ["session"], "calls": ["Stream", "stream.__json__"], "code_location": {"file": "test_stream_json.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream", "start_line": 57, "end_line": 62}, "code_snippet": "def test_base_stream(session):\n    stream = Stream(session)\n    assert stream.__json__() == {\n        \"type\": \"stream\",\n    }\n    assert stream.json == \"\"\"{\"type\": \"stream\"}\"\"\"\n", "type": "function"}, {"name": "test_dash_stream_url", "is_method": false, "class_name": null, "parameters": ["session", "common_args"], "calls": ["common_args.copy", "args.update", "Mock", "DASHStream", "session.http.prepare_new_request", "stream.to_url", "pytest.raises", "stream.to_manifest_url", "str"], "code_location": {"file": "test_stream_to_url.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream", "start_line": 83, "end_line": 95}, "code_snippet": "def test_dash_stream_url(session, common_args):\n    # DASHStream requires an MPD instance as input:\n    # The URL of the MPD instance was already prepared by DASHStream.parse_manifest, so copy this behavior here.\n    # This test verifies that session params are added to the URL, without duplicates.\n    args = common_args.copy()\n    args.update(url=\"http://host/stream.mpd?foo=bar\")\n    url = session.http.prepare_new_request(**args).url\n    mpd = Mock(url=url)\n    stream = DASHStream(session, mpd, **common_args)\n    assert stream.to_url() == \"http://host/stream.mpd?foo=bar&queryparamkey=queryparamval\"\n    with pytest.raises(TypeError) as cm:\n        stream.to_manifest_url()\n    assert str(cm.value) == \"<DASHStream [dash]> cannot be translated to a manifest URL\"\n", "type": "function"}, {"name": "test_dash_stream", "is_method": false, "class_name": null, "parameters": ["session", "common_args"], "calls": ["Mock", "DASHStream", "stream.__json__"], "code_location": {"file": "test_stream_json.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream", "start_line": 112, "end_line": 117}, "code_snippet": "def test_dash_stream(session, common_args):\n    mpd = Mock(url=None)\n    stream = DASHStream(session, mpd, **common_args)\n    assert stream.__json__() == {\n        \"type\": \"dash\",\n    }\n", "type": "function"}, {"name": "test_file_stream_handle", "is_method": false, "class_name": null, "parameters": ["session"], "calls": ["FileStream", "Mock", "stream.__json__"], "code_location": {"file": "test_stream_json.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream", "start_line": 73, "end_line": 77}, "code_snippet": "def test_file_stream_handle(session):\n    stream = FileStream(session, None, Mock())\n    assert stream.__json__() == {\n        \"type\": \"file\",\n    }\n", "type": "function"}, {"name": "test_parameters", "is_method": false, "class_name": null, "parameters": ["monkeypatch", "session"], "calls": ["Mock", "monkeypatch.setattr", "HTTPStreamPlugin", "plugin.streams", "call"], "code_location": {"file": "test_http.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 64, "end_line": 89}, "code_snippet": "def test_parameters(monkeypatch: pytest.MonkeyPatch, session: Streamlink):\n    mock_httpstream_init = Mock(return_value=None)\n    monkeypatch.setattr(\"streamlink.stream.http.HTTPStream.__init__\", mock_httpstream_init)\n\n    plugin = HTTPStreamPlugin(\n        session,\n        (\n            \"httpstream://example.com/foo\"\n            + \" auth=('foo', 'bar')\"\n            + \" verify=False\"\n            + \" referer=https://example2.com/bar\"\n            + \" params={'key': 'a value'}\"\n        ),\n    )\n    plugin.streams()\n\n    assert mock_httpstream_init.call_args_list == [\n        call(\n            session,\n            \"https://example.com/foo\",\n            auth=(\"foo\", \"bar\"),\n            verify=False,\n            referer=\"https://example2.com/bar\",\n            params={\"key\": \"a value\"},\n        ),\n    ]\n", "type": "function"}, {"name": "test_strip_json_comment", "is_method": true, "class_name": "TestLoadPluginsData", "parameters": ["self", "caplog", "session", "pluginsdata"], "calls": ["pytest.mark.parametrize", "session.plugins.get_names", "pytest.param", "caplog.get_records"], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/session", "start_line": 322, "end_line": 326}, "code_snippet": "    def test_strip_json_comment(self, caplog: pytest.LogCaptureFixture, session: Streamlink, pluginsdata: str):\n        assert \"fake\" not in session.plugins\n        assert \"testplugin\" not in session.plugins\n        assert session.plugins.get_names() == [\"testplugin\"]\n        assert [(record.name, record.levelname, record.message) for record in caplog.get_records(when=\"setup\")] == []\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3474407196044922}
{"question": "Where in the streaming platform's API client class are error responses from the API handled and transformed into consistent return formats?", "answer": "", "relative_code_list": null, "ground_truth": "The error response patterns are handled in the _query_api method (lines 31-63) within the validate.Schema validation chain, specifically in the validate.any clause that contains three validate.all transformations for different error and success response patterns, converting them into consistent ('error', message) or ('success', data) tuples.", "score": null, "retrieved_content": [{"name": "_api_stream", "is_method": true, "class_name": "GoodGame", "parameters": ["self", "url"], "calls": ["self.session.http.get", "validate.Schema", "validate.parse_json", "validate.any", "validate.all", "validate.all", "validate.get", "validate.transform", "validate.union_get", "validate.transform", "validate.none_or_all", "validate.url", "validate.none_or_all", "validate.all", "validate.union_get", "validate.all", "validate.parse_html", "validate.xml_find", "validate.get", "validate.transform"], "code_location": {"file": "goodgame.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 82, "end_line": 143}, "code_snippet": "    def _api_stream(self, url):\n        return self.session.http.get(\n            url,\n            schema=validate.Schema(\n                validate.parse_json(),\n                validate.any(\n                    validate.all(\n                        {\n                            \"error\": str,\n                        },\n                        validate.get(\"error\"),\n                        validate.transform(lambda data: (\"error\", data)),\n                    ),\n                    validate.all(\n                        {\n                            \"online\": bool,\n                            \"id\": int,\n                            \"streamer\": {\n                                \"username\": str,\n                            },\n                            \"sources\": {\n                                str: validate.url(),\n                            },\n                            \"game\": {\n                                \"title\": validate.none_or_all(str),\n                            },\n                            \"title\": validate.none_or_all(str),\n                            \"players\": [\n                                validate.all(\n                                    {\n                                        \"title\": str,\n                                        \"online\": bool,\n                                        \"content\": validate.all(\n                                            str,\n                                            validate.parse_html(),\n                                            validate.xml_find(\".//iframe\"),\n                                            validate.get(\"src\"),\n                                            validate.transform(urlparse),\n                                        ),\n                                    },\n                                    validate.union_get(\n                                        \"title\",\n                                        \"online\",\n                                        \"content\",\n                                    ),\n                                ),\n                            ],\n                        },\n                        validate.union_get(\n                            \"online\",\n                            \"id\",\n                            (\"streamer\", \"username\"),\n                            (\"game\", \"title\"),\n                            \"title\",\n                            \"sources\",\n                            \"players\",\n                        ),\n                        validate.transform(lambda data: (\"data\", *data)),\n                    ),\n                ),\n            ),\n        )\n", "type": "function"}, {"name": "get_streams_from_api", "is_method": true, "class_name": "Ceskatelevize", "parameters": ["self", "channel"], "calls": ["self.session.http.get", "self.URL_API_CHANNEL.format", "log.error", "validate.Schema", "uuid4", "validate.parse_json", "validate.union_get", "validate.url"], "code_location": {"file": "ceskatelevize.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 42, "end_line": 76}, "code_snippet": "    def get_streams_from_api(self, channel):\n        if not channel:\n            return\n\n        self.title, is_blocked, url = self.session.http.get(\n            self.URL_API_CHANNEL.format(channel=channel),\n            params={\n                \"canPlayDRM\": \"false\",\n                \"streamType\": \"dash\",\n                \"quality\": \"web\",\n                \"maxQualityCount\": 5,\n                \"sessionId\": uuid4(),\n            },\n            schema=validate.Schema(\n                validate.parse_json(),\n                {\n                    \"streamUrls\": {\n                        \"isBlocked\": bool,\n                        \"main\": validate.url(),\n                    },\n                    \"title\": str,\n                },\n                validate.union_get(\n                    \"title\",\n                    (\"streamUrls\", \"isBlocked\"),\n                    (\"streamUrls\", \"main\"),\n                ),\n            ),\n        )\n\n        if is_blocked:\n            log.error(\"The stream is inaccessible\")\n            return\n\n        return url\n", "type": "function"}, {"name": "_get_formats_from_api", "is_method": true, "class_name": "TVP", "parameters": ["self", "token"], "calls": ["self.session.http.get", "self._URL_INFO_API_TOKEN.format", "log.error", "validate.Schema", "validate.parse_json", "validate.union_get", "items", "items", "validate.all", "HLSStream.parse_variant_playlist", "DASHStream.parse_manifest", "validate.union_get", "validate.url"], "code_location": {"file": "tvp.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 59, "end_line": 89}, "code_snippet": "    def _get_formats_from_api(self, token):\n        is_geo_blocked, self.title, formats = self.session.http.get(\n            self._URL_INFO_API_TOKEN.format(token=token),\n            schema=validate.Schema(\n                validate.parse_json(),\n                {\n                    \"status\": \"OK\",\n                    \"isGeoBlocked\": bool,\n                    \"title\": str,\n                    \"formats\": [\n                        validate.all(\n                            {\n                                \"mimeType\": str,\n                                \"url\": validate.url(),\n                            },\n                            validate.union_get(\"mimeType\", \"url\"),\n                        ),\n                    ],\n                },\n                validate.union_get(\"isGeoBlocked\", \"title\", \"formats\"),\n            ),\n        )\n        if is_geo_blocked:\n            log.error(\"The content is not available in your region\")\n            raise NoStreamsError\n\n        for mime_type, url in formats:\n            if mime_type == \"application/x-mpegurl\":\n                yield from HLSStream.parse_variant_playlist(self.session, url).items()\n            if mime_type == \"application/dash+xml\":\n                yield from DASHStream.parse_manifest(self.session, url).items()\n", "type": "function"}, {"name": "_get_streams_content", "is_method": true, "class_name": "WWENetwork", "parameters": ["self", "content_type", "content_id", "token"], "calls": ["self.session.http.get", "validate.Schema", "self.session.http.get", "format", "log.error", "log.error", "log.error", "validate.union_get", "playback_streams.get", "validate.Schema", "validate.optional", "validate.url", "validate.none_or_all", "validate.Schema", "items", "self._API_URLS.get", "validate.parse_json", "validate.any", "validate.filter", "validate.parse_json", "self.session.get_option", "validate.all", "validate.all", "validate.optional", "validate.any", "HLSStream.parse_variant_playlist", "HTTPStream", "validate.get", "validate.transform", "validate.union_get", "validate.transform", "validate.url", "validate.all", "validate.optional", "validate.optional", "validate.optional", "validate.any", "validate.list", "validate.get", "MuxedStream", "validate.url"], "code_location": {"file": "wwenetwork.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 81, "end_line": 165}, "code_snippet": "    def _get_streams_content(self, content_type, content_id, token):\n        success, data = self.session.http.get(\n            self._API_URLS.get(content_type).format(content_id),\n            acceptable_status=(200, 401, 404),\n            params={\"includePlaybackDetails\": \"URL\"},\n            headers={\"Authorization\": f\"Bearer {token}\", **self._API_HEADERS},\n            schema=validate.Schema(\n                validate.parse_json(),\n                validate.any(\n                    validate.all(\n                        {\"messages\": [str]},\n                        validate.get((\"messages\", 0)),\n                        validate.transform(lambda message: (False, message)),\n                    ),\n                    validate.all(\n                        {\n                            \"accessLevel\": str,\n                            validate.optional(\"id\"): int,\n                            validate.optional(\"title\"): str,\n                            validate.optional(\"playerUrlCallback\"): validate.any(None, validate.url()),\n                        },\n                        validate.union_get(\n                            \"accessLevel\",\n                            \"playerUrlCallback\",\n                            \"id\",\n                            \"title\",\n                        ),\n                        validate.transform(lambda data: (True, data)),\n                    ),\n                ),\n            ),\n        )\n\n        if not success:\n            log.error(data)\n            return\n\n        access, playback_url, self.id, self.title = data\n\n        if access != \"GRANTED\":\n            log.error(\"Paid subscription required for this video\")\n            return\n\n        if not playback_url:\n            log.error(\"Failed to get playerUrlCallback from response\")\n            return\n\n        playback_schema = validate.Schema(\n            {\n                \"url\": validate.url(),\n                validate.optional(\"subtitles\"): validate.none_or_all(\n                    [{\"language\": str, \"format\": str, \"url\": validate.url()}],\n                    validate.filter(lambda s: s[\"format\"] == \"srt\"),\n                ),\n            },\n            validate.union_get(\"url\", \"subtitles\"),\n        )\n\n        playback_streams = self.session.http.get(\n            playback_url,\n            schema=validate.Schema(\n                validate.parse_json(),\n                {\n                    validate.optional(\"hls\"): validate.any(\n                        validate.all(\n                            validate.list(playback_schema),\n                            validate.get(0),\n                        ),\n                        playback_schema,\n                    ),\n                },\n            ),\n        )\n\n        if playback_stream := playback_streams.get(\"hls\"):\n            url, subtitles = playback_stream\n\n            if streams := HLSStream.parse_variant_playlist(self.session, url).items():\n                if subtitles and self.session.get_option(\"mux-subtitles\"):\n                    substreams = {s[\"language\"]: HTTPStream(self.session, s[\"url\"]) for s in subtitles}\n\n                    for quality, stream in streams:\n                        yield quality, MuxedStream(self.session, stream, subtitles=substreams)\n                else:\n                    yield from streams\n", "type": "function"}, {"name": "_query_api", "is_method": true, "class_name": "VKvideolive", "parameters": ["self", "channel_name", "vod"], "calls": ["validate.all", "validate.any", "self.session.http.get", "isinstance", "validate.union_get", "validate.all", "validate.all", "PluginError", "validate.optional", "validate.all", "validate.any", "validate.get", "validate.get", "validate.Schema", "validate.get", "validate.parse_json", "validate.all", "validate.union_get", "validate.all", "validate.union_get", "validate.any", "validate.url"], "code_location": {"file": "vkvideolive.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 46, "end_line": 115}, "code_snippet": "    def _query_api(self, channel_name, vod):\n        schema_data = validate.all(\n            {\n                validate.optional(\"category\"): validate.all(\n                    {\n                        \"title\": str,\n                    },\n                    validate.get(\"title\"),\n                ),\n                \"title\": str,\n                \"data\": validate.any(\n                    [\n                        validate.all(\n                            {\n                                \"vid\": str,\n                                \"playerUrls\": [\n                                    validate.all(\n                                        {\n                                            \"type\": str,\n                                            \"url\": validate.any(\"\", validate.url()),\n                                        },\n                                        validate.union_get(\"type\", \"url\"),\n                                    ),\n                                ],\n                            },\n                            validate.union_get(\"vid\", \"playerUrls\"),\n                        ),\n                    ],\n                    [],\n                ),\n            },\n            validate.union_get(\n                \"category\",\n                \"title\",\n                (\"data\", 0),\n            ),\n        )\n        schema_response = validate.any(\n            validate.all(\n                {\"error\": str, \"error_description\": str},\n                validate.get(\"error_description\"),\n            ),\n            schema_data,\n        )\n\n        if not vod:\n            schema = schema_response\n        else:\n            schema = validate.all(\n                {\n                    \"data\": {\n                        \"record\": schema_response,\n                    },\n                },\n                validate.get((\"data\", \"record\")),\n            )\n\n        data = self.session.http.get(\n            f\"{self.API_URL}/blog/{channel_name}/public_video_stream{vod}\",\n            headers={\"Referer\": self.url},\n            acceptable_status=(200, 404),\n            schema=validate.Schema(\n                validate.parse_json(),\n                schema,\n            ),\n        )\n        if isinstance(data, str):\n            raise PluginError(f\"VKvideo API error: {data}\")\n\n        return data\n", "type": "function"}, {"name": "_get_streams", "is_method": true, "class_name": "InvintusMedia", "parameters": ["self"], "calls": ["self.session.http.post", "self.session.http.json", "HLSStream.parse_variant_playlist", "self.match.group", "self.match.group", "update_scheme", "json.dumps"], "code_location": {"file": "invintus.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 25, "end_line": 47}, "code_snippet": "    def _get_streams(self):\n        postdata = {\n            \"clientID\": self.match.group(1),\n            \"showEncoder\": True,\n            \"showMediaAssets\": True,\n            \"showStreams\": True,\n            \"includePrivate\": False,\n            \"advancedDetails\": True,\n            \"VAST\": True,\n            \"eventID\": self.match.group(2),\n        }\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"wsc-api-key\": self.WSC_API_KEY,\n            \"Authorization\": \"embedder\",\n        }\n        res = self.session.http.post(self.API_URL, data=json.dumps(postdata), headers=headers)\n        api_response = self.session.http.json(res, schema=self.api_response_schema)\n        if api_response is None:\n            return\n\n        hls_url = api_response[\"data\"][\"streamingURIs\"][\"main\"]\n        return HLSStream.parse_variant_playlist(self.session, update_scheme(\"https://\", hls_url))\n", "type": "function"}, {"name": "_api_query_streamserver", "is_method": true, "class_name": "TwitCasting", "parameters": ["self"], "calls": ["self.session.http.get", "validate.Schema", "validate.parse_json", "validate.union_get", "validate.optional", "validate.optional", "validate.optional", "validate.url", "validate.url"], "code_location": {"file": "twitcasting.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 64, "end_line": 92}, "code_snippet": "    def _api_query_streamserver(self):\n        return self.session.http.get(\n            self._URL_API_STREAMSERVER,\n            params={\n                \"target\": self.match[\"channel\"],\n                \"mode\": \"client\",\n                \"player\": \"pc_web\",\n            },\n            schema=validate.Schema(\n                validate.parse_json(),\n                {\n                    validate.optional(\"movie\"): {\n                        \"id\": int,\n                        \"live\": bool,\n                    },\n                    validate.optional(\"llfmp4\"): {\n                        \"streams\": {\n                            str: validate.url(),\n                        },\n                    },\n                    validate.optional(\"tc-hls\"): {\n                        \"streams\": {\n                            str: validate.url(),\n                        },\n                    },\n                },\n                validate.union_get(\"movie\", \"llfmp4\", \"tc-hls\"),\n            ),\n        )\n", "type": "function"}, {"name": "_get_live_streams", "is_method": true, "class_name": "Bloomberg", "parameters": ["self", "data", "channel"], "calls": ["validate.Schema", "log.debug", "self.session.http.get", "validate.get", "schema_live_ids.validate", "log.error", "validate.Schema", "validate.parse_json", "validate.get", "validate.all", "validate.get", "validate.optional", "validate.all", "validate.transform", "validate.url"], "code_location": {"file": "bloomberg.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 32, "end_line": 79}, "code_snippet": "    def _get_live_streams(self, data, channel):\n        schema_live_ids = validate.Schema(\n            {\n                \"live\": {\n                    \"channels\": {\n                        \"byChannelId\": {\n                            channel: validate.all(\n                                {\"liveId\": str},\n                                validate.get(\"liveId\"),\n                            ),\n                        },\n                    },\n                },\n            },\n            validate.get((\"live\", \"channels\", \"byChannelId\", channel)),\n        )\n        try:\n            live_id = schema_live_ids.validate(data)\n        except PluginError:\n            log.error(f\"Could not find liveId for channel '{channel}'\")\n            return\n\n        log.debug(f\"Found liveId: {live_id}\")\n        return self.session.http.get(\n            self.LIVE_API_URL,\n            schema=validate.Schema(\n                validate.parse_json(),\n                {\n                    \"livestreams\": {\n                        live_id: {\n                            validate.optional(\"cdns\"): validate.all(\n                                [\n                                    {\n                                        \"streams\": [\n                                            {\n                                                \"url\": validate.url(),\n                                            },\n                                        ],\n                                    },\n                                ],\n                                validate.transform(lambda x: [urls[\"url\"] for y in x for urls in y[\"streams\"]]),\n                            ),\n                        },\n                    },\n                },\n                validate.get((\"livestreams\", live_id, \"cdns\")),\n            ),\n        )\n", "type": "function"}, {"name": "_api_call", "is_method": true, "class_name": "TF1", "parameters": ["self", "channel_id", "user_token"], "calls": ["self.session.http.get", "self._URL_API.format", "validate.Schema", "validate.parse_json", "validate.get", "validate.any", "validate.all", "validate.all", "validate.union_get", "validate.union_get", "validate.url"], "code_location": {"file": "tf1.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 134, "end_line": 172}, "code_snippet": "    def _api_call(self, channel_id, user_token):\n        headers = {\n            # forces HLS streams\n            \"User-Agent\": useragents.IPHONE,\n        }\n        if user_token:\n            headers[\"Authorization\"] = f\"Bearer {user_token}\"\n\n        return self.session.http.get(\n            self._URL_API.format(channel_id=channel_id),\n            params={\n                \"context\": \"MYTF1\",\n                \"pver\": \"5015000\",\n            },\n            headers=headers,\n            schema=validate.Schema(\n                validate.parse_json(),\n                {\n                    \"delivery\": validate.any(\n                        validate.all(\n                            {\n                                \"code\": 200,\n                                \"format\": \"hls\",\n                                \"url\": validate.url(),\n                            },\n                            validate.union_get(\"code\", \"url\"),\n                        ),\n                        validate.all(\n                            {\n                                \"code\": int,\n                                \"error\": str,\n                            },\n                            validate.union_get(\"code\", \"error\"),\n                        ),\n                    ),\n                },\n                validate.get(\"delivery\"),\n            ),\n        )\n", "type": "function"}, {"name": "_query_api", "is_method": true, "class_name": "Kick", "parameters": ["self", "url", "schema", "secondary_attempt"], "calls": ["self.session.http.get", "validate.Schema", "main_schema.validate", "self._get_cookies_from_webbrowser", "self._query_api", "res.raise_for_status", "validate.parse_json", "validate.any", "PluginError", "self._get_api_headers", "PluginError", "validate.all", "validate.all", "validate.all", "validate.transform", "validate.transform", "validate.transform"], "code_location": {"file": "kick.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 242, "end_line": 281}, "code_snippet": "    def _query_api(self, url, schema, secondary_attempt=False):\n        res = self.session.http.get(\n            url,\n            headers=self._get_api_headers(),\n            raise_for_status=False,\n        )\n        if res.status_code == 403 and not secondary_attempt and self._get_cookies_from_webbrowser():\n            # re-attempt API query after getting (new) cookies\n            return self._query_api(url, schema, secondary_attempt=True)\n\n        try:\n            res.raise_for_status()\n        except Exception as err:\n            raise PluginError(f\"Error while querying Kick API: {err or '403 status response'}\") from err\n\n        main_schema = validate.Schema(\n            validate.parse_json(),\n            validate.any(\n                validate.all(\n                    {\"message\": str},\n                    validate.transform(lambda obj: (\"error\", obj[\"message\"])),\n                ),\n                validate.all(\n                    {\"data\": None},\n                    validate.transform(lambda _: (\"data\", None)),\n                ),\n                validate.all(\n                    schema,\n                    validate.transform(lambda obj: (\"data\", obj)),\n                ),\n            ),\n        )\n        restype, data = main_schema.validate(res.text)\n\n        if restype == \"error\":\n            raise PluginError(f\"Error while querying Kick API: {data or 'unknown error'}\")\n        if not data:\n            raise NoStreamsError\n\n        return data\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3423898220062256}
{"question": "Where in the test method that verifies timing offset handling during periodic refresh operations does the control flow adapt when those operations exceed the expected refresh interval threshold?", "answer": "", "relative_code_list": null, "ground_truth": "The control flow adapts by dynamically adjusting the wait time calculation: when reload operations exceed the target duration (5 seconds), the method sets time_wait to 0.0 instead of the normal remaining time, effectively skipping the wait period to maintain synchronization. This is demonstrated when the second reload takes 7 seconds (exceeding the 5-second target), causing the subsequent wait time to be zero, and the playlist_reload_last timestamp is updated to the current time rather than following the standard interval pattern, thus modifying the control flow to handle timing discrepancies.", "score": null, "retrieved_content": [{"name": "test_playlist_reload_offset", "is_method": true, "class_name": "TestHLSStreamWorker", "parameters": ["self"], "calls": ["self.subject", "freezegun.freeze_time", "self.start", "worker.handshake_reload.wait_ready", "frozen_time.move_to", "self.await_reload", "worker.handshake_wait.wait_ready", "self.await_playlist_wait", "worker.handshake_reload.wait_ready", "frozen_time.move_to", "self.await_reload", "worker.handshake_wait.wait_ready", "self.await_playlist_wait", "worker.handshake_reload.wait_ready", "frozen_time.move_to", "self.await_reload", "worker.handshake_wait.wait_ready", "self.await_playlist_wait", "worker.handshake_reload.wait_ready", "frozen_time.move_to", "self.await_reload", "worker.handshake_wait.wait_ready", "self.await_playlist_wait", "worker.handshake_reload.wait_ready", "self.await_reload", "self.await_close", "self.await_read", "self.content", "worker.handshake_wait.wait_ready", "worker.handshake_reload.wait_ready", "Playlist", "Playlist", "Playlist", "Playlist", "Playlist", "Segment", "Segment", "Segment", "Segment", "Segment"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 452, "end_line": 542}, "code_snippet": "    def test_playlist_reload_offset(self) -> None:\n        segments = self.subject(\n            start=False,\n            playlists=[\n                Playlist(0, targetduration=5, segments=[Segment(0)]),\n                Playlist(1, targetduration=5, segments=[Segment(1)]),\n                Playlist(2, targetduration=5, segments=[Segment(2)]),\n                Playlist(3, targetduration=5, segments=[Segment(3)]),\n                Playlist(4, targetduration=5, segments=[Segment(4)], end=True),\n            ],\n        )\n        worker: EventedHLSStreamWorker = self.thread.reader.worker  # type: ignore[assignment]\n        targetduration = ONE_SECOND * 5\n\n        with freezegun.freeze_time(EPOCH) as frozen_time:\n            self.start()\n\n            assert worker.handshake_reload.wait_ready(1), \"Arrives at initial playlist reload\"\n            assert worker._reload_last == EPOCH, \"Sets the initial value of the last reload time\"\n\n            # adjust clock and reload playlist: let it take one second\n            frozen_time.move_to(worker._reload_last + ONE_SECOND)\n            self.await_reload()\n            assert worker._reload_time == 5.0, \"Uses the playlist's targetduration as reload time\"\n\n            # time_completed = 00:00:01; time_elapsed = 1s\n            assert worker.handshake_wait.wait_ready(1), \"Arrives at first wait() call\"\n            assert worker.playlist_sequence == 1, \"Has queued first segment\"\n            assert worker.playlist_end is None, \"Stream hasn't ended yet\"\n            assert worker.time_wait == 4.0, \"Waits for 4 seconds out of the 5 seconds reload time\"\n            self.await_playlist_wait()\n\n            assert worker.handshake_reload.wait_ready(1), \"Arrives at second playlist reload\"\n            assert worker._reload_last == EPOCH + targetduration, \\\n                \"Last reload time is the sum of reload+wait time (=targetduration)\"  # fmt: skip\n\n            # adjust clock and reload playlist: let it exceed targetduration by two seconds\n            frozen_time.move_to(worker._reload_last + targetduration + ONE_SECOND * 2)\n            self.await_reload()\n            assert worker._reload_time == 5.0, \"Uses the playlist's targetduration as reload time\"\n\n            # time_completed = 00:00:12; time_elapsed = 7s (exceeded 5s targetduration)\n            assert worker.handshake_wait.wait_ready(1), \"Arrives at second wait() call\"\n            assert worker.playlist_sequence == 2, \"Has queued second segment\"\n            assert worker.playlist_end is None, \"Stream hasn't ended yet\"\n            assert worker.time_wait == 0.0, \"Doesn't wait when reloading took too long\"\n            self.await_playlist_wait()\n\n            assert worker.handshake_reload.wait_ready(1), \"Arrives at third playlist reload\"\n            assert worker._reload_last == EPOCH + targetduration * 2 + ONE_SECOND * 2, \\\n                \"Sets last reload time to current time when reloading took too long (changes the interval)\"  # fmt: skip\n\n            # adjust clock and reload playlist: let it take one second again\n            frozen_time.move_to(worker._reload_last + ONE_SECOND)\n            self.await_reload()\n            assert worker._reload_time == 5.0, \"Uses the playlist's targetduration as reload time\"\n\n            # time_completed = 00:00:13; time_elapsed = 1s\n            assert worker.handshake_wait.wait_ready(1), \"Arrives at third wait() call\"\n            assert worker.playlist_sequence == 3, \"Has queued third segment\"\n            assert worker.playlist_end is None, \"Stream hasn't ended yet\"\n            assert worker.time_wait == 4.0, \"Waits for 4 seconds out of the 5 seconds reload time\"\n            self.await_playlist_wait()\n\n            assert worker.handshake_reload.wait_ready(1), \"Arrives at fourth playlist reload\"\n            assert worker._reload_last == EPOCH + targetduration * 3 + ONE_SECOND * 2, \\\n                \"Last reload time is the sum of reload+wait time (=targetduration) of the changed interval\"  # fmt: skip\n\n            # adjust clock and reload playlist: simulate no fetch+processing delay\n            frozen_time.move_to(worker._reload_last)\n            self.await_reload()\n            assert worker._reload_time == 5.0, \"Uses the playlist's targetduration as reload time\"\n\n            # time_completed = 00:00:17; time_elapsed = 0s\n            assert worker.handshake_wait.wait_ready(1), \"Arrives at fourth wait() call\"\n            assert worker.playlist_sequence == 4, \"Has queued fourth segment\"\n            assert worker.playlist_end is None, \"Stream hasn't ended yet\"\n            assert worker.time_wait == 5.0, \"Waits for the whole reload time\"\n            self.await_playlist_wait()\n\n            assert worker.handshake_reload.wait_ready(1), \"Arrives at fifth playlist reload\"\n            assert worker._reload_last == EPOCH + targetduration * 4 + ONE_SECOND * 2, \\\n                \"Last reload time is the sum of reload+wait time (no delay)\"  # fmt: skip\n\n            # adjusting the clock is not needed anymore\n            self.await_reload()\n            assert self.await_read(read_all=True) == self.content(segments)\n            self.await_close()\n            assert worker.playlist_end == 4, \"Stream has ended\"\n            assert not worker.handshake_wait.wait_ready(0), \"Doesn't wait once ended\"\n            assert not worker.handshake_reload.wait_ready(0), \"Doesn't reload playlist once ended\"\n", "type": "function"}, {"name": "test_segment_queue_timing_threshold_reached_min", "is_method": true, "class_name": "TestHLSStreamWorker", "parameters": ["self"], "calls": ["self.subject", "freezegun.freeze_time", "patch", "self.start", "worker.handshake_reload.wait_ready", "frozen_time.tick", "self.await_reload", "worker.handshake_wait.wait_ready", "range", "frozen_time.tick", "self.await_playlist_wait", "self.await_reload", "self.thread.close", "self.thread.join", "frozen_time.tick", "self.await_playlist_wait", "self.await_reload", "worker.handshake_wait.wait_ready", "Playlist", "call", "Segment"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 394, "end_line": 450}, "code_snippet": "    def test_segment_queue_timing_threshold_reached_min(self) -> None:\n        self.subject(\n            start=False,\n            playlists=[\n                # no EXT-X-ENDLIST, last mocked playlist response will be repreated forever\n                Playlist(0, targetduration=1, segments=[Segment(0)]),\n            ],\n        )\n        worker: EventedHLSStreamWorker = self.thread.reader.worker  # type: ignore[assignment]\n        targetduration = ONE_SECOND\n\n        with (\n            freezegun.freeze_time(EPOCH) as frozen_time,\n            patch(\"streamlink.stream.hls.hls.log\") as mock_log,\n        ):\n            self.start()\n\n            assert worker.handshake_reload.wait_ready(1), \"Loads playlist for the first time\"\n            assert worker.playlist_sequence == -1, \"Initial sequence number\"\n            assert worker.playlist_sequence_last == EPOCH, \"Sets the initial last queue time\"\n\n            # first playlist reload has taken one second\n            frozen_time.tick(ONE_SECOND)\n            self.await_reload(1)\n\n            assert worker.handshake_wait.wait_ready(1), \"Arrives at wait() call #1\"\n            assert worker.playlist_sequence == 1, \"Updates the sequence number\"\n            assert worker.playlist_sequence_last == EPOCH + ONE_SECOND, \"Updates the last queue time\"\n            assert worker.playlist_targetduration == 1.0\n\n            for num in range(2, 7):\n                # trigger next reload when the target duration has passed\n                frozen_time.tick(targetduration)\n                self.await_playlist_wait(1)\n                self.await_reload(1)\n\n                assert worker.handshake_wait.wait_ready(1), f\"Arrives at wait() call #{num}\"\n                assert worker.playlist_sequence == 1, \"Sequence number is unchanged\"\n                assert worker.playlist_sequence_last == EPOCH + ONE_SECOND, \"Last queue time is unchanged\"\n                assert worker.playlist_targetduration == 1.0\n\n            assert mock_log.warning.call_args_list == []\n\n            # trigger next reload when the target duration has passed\n            frozen_time.tick(targetduration)\n            self.await_playlist_wait(1)\n            self.await_reload(1)\n\n            # FIXME: NO-GIL\n            #   Closing only the worker thread when no new segments were queued keeps the writer thread and buffer still open,\n            #   which is why the test's reader thread keeps running until the test teardown,\n            #   but this somehow breaks the assertion down below, so close everything manually...\n            #   These tests will have to be rewritten eventually in pytest-style, without having to mock log calls.\n            self.thread.close()\n            self.thread.join(1)\n\n            assert mock_log.warning.call_args_list == [call(\"No new segments in playlist for more than 5.00s. Stopping...\")]\n", "type": "function"}, {"name": "test_segment_queue_timing_threshold_reached", "is_method": true, "class_name": "TestHLSStreamWorker", "parameters": ["self"], "calls": ["self.subject", "freezegun.freeze_time", "patch", "self.start", "worker.handshake_reload.wait_ready", "frozen_time.tick", "self.await_reload", "worker.handshake_wait.wait_ready", "frozen_time.tick", "self.await_playlist_wait", "self.await_reload", "worker.handshake_wait.wait_ready", "range", "frozen_time.tick", "self.await_playlist_wait", "self.await_reload", "self.await_read", "self.await_close", "frozen_time.tick", "self.await_playlist_wait", "self.await_reload", "worker.handshake_wait.wait_ready", "Playlist", "Playlist", "call", "Segment", "Segment", "Segment"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 285, "end_line": 347}, "code_snippet": "    def test_segment_queue_timing_threshold_reached(self) -> None:\n        self.subject(\n            start=False,\n            playlists=[\n                Playlist(0, targetduration=5, segments=[Segment(0)]),\n                # no EXT-X-ENDLIST, last mocked playlist response will be repreated forever\n                Playlist(0, targetduration=5, segments=[Segment(0), Segment(1)]),\n            ],\n        )\n        worker: EventedHLSStreamWorker = self.thread.reader.worker  # type: ignore[assignment]\n        targetduration = ONE_SECOND * 5\n\n        with (\n            freezegun.freeze_time(EPOCH) as frozen_time,\n            patch(\"streamlink.stream.hls.hls.log\") as mock_log,\n        ):\n            self.start()\n\n            assert worker.handshake_reload.wait_ready(1), \"Loads playlist for the first time\"\n            assert worker.playlist_sequence == -1, \"Initial sequence number\"\n            assert worker.playlist_sequence_last == EPOCH, \"Sets the initial last queue time\"\n\n            # first playlist reload has taken one second\n            frozen_time.tick(ONE_SECOND)\n            self.await_reload(1)\n\n            assert worker.handshake_wait.wait_ready(1), \"Arrives at wait() call #1\"\n            assert worker.playlist_sequence == 1, \"Updates the sequence number\"\n            assert worker.playlist_sequence_last == EPOCH + ONE_SECOND, \"Updates the last queue time\"\n            assert worker.playlist_targetduration == 5.0\n\n            # trigger next reload when the target duration has passed\n            frozen_time.tick(targetduration)\n            self.await_playlist_wait(1)\n            self.await_reload(1)\n\n            assert worker.handshake_wait.wait_ready(1), \"Arrives at wait() call #2\"\n            assert worker.playlist_sequence == 2, \"Updates the sequence number again\"\n            assert worker.playlist_sequence_last == EPOCH + ONE_SECOND + targetduration, \"Updates the last queue time again\"\n            assert worker.playlist_targetduration == 5.0\n\n            for num in range(3, 6):\n                # trigger next reload when the target duration has passed\n                frozen_time.tick(targetduration)\n                self.await_playlist_wait(1)\n                self.await_reload(1)\n\n                assert worker.handshake_wait.wait_ready(1), f\"Arrives at wait() call #{num}\"\n                assert worker.playlist_sequence == 2, \"Sequence number is unchanged\"\n                assert worker.playlist_sequence_last == EPOCH + ONE_SECOND + targetduration, \"Last queue time is unchanged\"\n                assert worker.playlist_targetduration == 5.0\n\n            assert mock_log.warning.call_args_list == []\n\n            # trigger next reload when the target duration has passed\n            frozen_time.tick(targetduration)\n            self.await_playlist_wait(1)\n            self.await_reload(1)\n\n            self.await_read(read_all=True)\n            self.await_close(1)\n\n            assert mock_log.warning.call_args_list == [call(\"No new segments in playlist for more than 15.00s. Stopping...\")]\n", "type": "function"}, {"name": "test_segment_queue_timing_threshold_reached_ignored", "is_method": true, "class_name": "TestHLSStreamWorker", "parameters": ["self"], "calls": ["self.subject", "worker.is_alive", "self.thread.reader.writer.put", "freezegun.freeze_time", "self.start", "worker.handshake_reload.wait_ready", "frozen_time.tick", "self.await_reload", "worker.handshake_wait.wait_ready", "range", "self.await_read", "self.content", "frozen_time.tick", "self.await_playlist_wait", "self.await_reload", "worker.handshake_wait.wait_ready", "Playlist", "Segment"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 349, "end_line": 392}, "code_snippet": "    def test_segment_queue_timing_threshold_reached_ignored(self) -> None:\n        segments = self.subject(\n            start=False,\n            options={\"hls-segment-queue-threshold\": 0},\n            playlists=[\n                # no EXT-X-ENDLIST, last mocked playlist response will be repreated forever\n                Playlist(0, targetduration=5, segments=[Segment(0)]),\n            ],\n        )\n        worker: EventedHLSStreamWorker = self.thread.reader.worker  # type: ignore[assignment]\n        targetduration = ONE_SECOND * 5\n\n        with freezegun.freeze_time(EPOCH) as frozen_time:\n            self.start()\n\n            assert worker.handshake_reload.wait_ready(1), \"Loads playlist for the first time\"\n            assert worker.playlist_sequence == -1, \"Initial sequence number\"\n            assert worker.playlist_sequence_last == EPOCH, \"Sets the initial last queue time\"\n\n            # first playlist reload has taken one second\n            frozen_time.tick(ONE_SECOND)\n            self.await_reload(1)\n\n            assert worker.handshake_wait.wait_ready(1), \"Arrives at first wait() call\"\n            assert worker.playlist_sequence == 1, \"Updates the sequence number\"\n            assert worker.playlist_sequence_last == EPOCH + ONE_SECOND, \"Updates the last queue time\"\n            assert worker.playlist_targetduration == 5.0\n            assert self.await_read() == self.content(segments)\n\n            # keep reloading a couple of times\n            for num in range(10):\n                frozen_time.tick(targetduration)\n                self.await_playlist_wait(1)\n                self.await_reload(1)\n\n                assert worker.handshake_wait.wait_ready(1), f\"Arrives at wait() #{num + 1}\"\n                assert worker.playlist_sequence == 1, \"Sequence number is unchanged\"\n                assert worker.playlist_sequence_last == EPOCH + ONE_SECOND, \"Last queue time is unchanged\"\n\n        assert self.thread.data == [], \"No new data\"\n        assert worker.is_alive()\n\n        # make stream end gracefully to avoid any unnecessary thread blocking\n        self.thread.reader.writer.put(None)\n", "type": "function"}, {"name": "test_static_refresh_wait", "is_method": true, "class_name": "TestDASHStreamWorker", "parameters": ["self", "timestamp", "mock_wait", "mock_time", "worker", "representation", "segments", "mpd", "duration"], "calls": ["pytest.mark.parametrize", "worker._wait.is_set", "list", "worker.iter_segments", "call", "call"], "code_location": {"file": "test_dash.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/dash", "start_line": 529, "end_line": 548}, "code_snippet": "    def test_static_refresh_wait(\n        self,\n        timestamp: datetime,\n        mock_wait: Mock,\n        mock_time: Mock,\n        worker: DASHStreamWorker,\n        representation: Mock,\n        segments: list[Mock],\n        mpd: Mock,\n        duration: float,\n    ):\n        mpd.dynamic = False\n        mpd.type = \"static\"\n        mpd.periods[0].duration.total_seconds.return_value = duration\n\n        representation.segments.return_value = segments\n        assert list(worker.iter_segments()) == segments\n        assert representation.segments.call_args_list == [call(init=True, timestamp=timestamp)]\n        assert mock_wait.call_args_list == [call(5)]\n        assert worker._wait.is_set()\n", "type": "function"}, {"name": "test_offsets", "is_method": true, "class_name": "TestHLSStreamByterange", "parameters": ["self"], "calls": ["TagMap", "TagMap", "self.mock", "self.mock", "self.subject", "self.await_write", "self.await_read", "self.id", "self.id", "self.url", "self.url", "Segment", "Segment", "Segment", "Segment", "Segment", "Playlist", "Tag", "Tag", "Tag", "Tag", "Tag", "self.url", "self.url", "self.url", "self.url", "self.url", "self.url", "self.url"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 627, "end_line": 663}, "code_snippet": "    def test_offsets(self):\n        map1 = TagMap(1, self.id(), {\"BYTERANGE\": '\"1234@0\"'})\n        map2 = TagMap(2, self.id(), {\"BYTERANGE\": '\"42@1337\"'})\n        self.mock(\"GET\", self.url(map1), content=map1.content)\n        self.mock(\"GET\", self.url(map2), content=map2.content)\n        s1, s2, s3, s4, s5 = Segment(0), Segment(1), Segment(2), Segment(3), Segment(4)\n\n        self.subject([\n            Playlist(\n                0,\n                [\n                    map1,\n                    Tag(\"EXT-X-BYTERANGE\", \"5@3\"),\n                    s1,\n                    Tag(\"EXT-X-BYTERANGE\", \"7\"),\n                    s2,\n                    map2,\n                    Tag(\"EXT-X-BYTERANGE\", \"11\"),\n                    s3,\n                    Tag(\"EXT-X-BYTERANGE\", \"17@13\"),\n                    s4,\n                    Tag(\"EXT-X-BYTERANGE\", \"19\"),\n                    s5,\n                ],\n                end=True,\n            ),\n        ])\n\n        self.await_write(1 + 2 + 1 + 3)  # 1 map, 2 partial segments, 1 map, 3 partial segments\n        self.await_read(read_all=True)\n        assert self.mocks[self.url(map1)].last_request._request.headers[\"Range\"] == \"bytes=0-1233\"\n        assert self.mocks[self.url(map2)].last_request._request.headers[\"Range\"] == \"bytes=1337-1378\"\n        assert self.mocks[self.url(s1)].last_request._request.headers[\"Range\"] == \"bytes=3-7\"\n        assert self.mocks[self.url(s2)].last_request._request.headers[\"Range\"] == \"bytes=8-14\"\n        assert self.mocks[self.url(s3)].last_request._request.headers[\"Range\"] == \"bytes=15-25\"\n        assert self.mocks[self.url(s4)].last_request._request.headers[\"Range\"] == \"bytes=13-29\"\n        assert self.mocks[self.url(s5)].last_request._request.headers[\"Range\"] == \"bytes=30-48\"\n", "type": "function"}, {"name": "test_invalid_offset_reference", "is_method": true, "class_name": "TestHLSStreamByterange", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.await_write", "self.thread.close", "self.called", "Playlist", "call", "Segment", "Tag", "Segment", "Segment", "Tag", "Segment", "Segment", "self.url", "Segment"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 602, "end_line": 625}, "code_snippet": "    def test_invalid_offset_reference(self, mock_log: Mock):\n        self.subject([\n            Playlist(\n                0,\n                [\n                    Tag(\"EXT-X-BYTERANGE\", \"3@0\"),\n                    Segment(0),\n                    Segment(1),\n                    Tag(\"EXT-X-BYTERANGE\", \"5\"),\n                    Segment(2),\n                    Segment(3),\n                ],\n                end=True,\n            ),\n        ])\n\n        self.await_write(4 - 1)\n        self.thread.close()\n\n        assert mock_log.error.call_args_list == [\n            call(\"Failed to fetch segment 2: Missing BYTERANGE offset\"),\n        ]\n        assert self.mocks[self.url(Segment(0))].last_request._request.headers[\"Range\"] == \"bytes=0-2\"\n        assert not self.called(Segment(2))\n", "type": "function"}, {"name": "test_offset_and_duration", "is_method": true, "class_name": "TestHLSStream", "parameters": ["self"], "calls": ["self.subject", "self.await_read", "all", "self.content", "any", "Playlist", "self.called", "segments.values", "self.called", "Segment", "Segment", "Segment", "Segment", "segments.values"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 182, "end_line": 193}, "code_snippet": "    def test_offset_and_duration(self):\n        segments = self.subject(\n            [\n                Playlist(1234, [Segment(0), Segment(1, duration=0.5), Segment(2, duration=0.5), Segment(3)], end=True),\n            ],\n            streamoptions={\"start_offset\": 1, \"duration\": 1},\n        )\n\n        data = self.await_read(read_all=True)\n        assert data == self.content(segments, cond=lambda s: 0 < s.num < 3), \"Respects the offset and duration\"\n        assert all(self.called(s) for s in segments.values() if 0 < s.num < 3), \"Downloads second and third segment\"\n        assert not any(self.called(s) for s in segments.values() if 0 > s.num > 3), \"Skips other segments\"\n", "type": "function"}, {"name": "test_unknown_offset", "is_method": true, "class_name": "TestHLSStreamByterange", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.await_write", "self.thread.close", "self.called", "Playlist", "call", "Segment", "Tag", "Segment", "Segment"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 556, "end_line": 575}, "code_snippet": "    def test_unknown_offset(self, mock_log: Mock):\n        self.subject([\n            Playlist(\n                0,\n                [\n                    Tag(\"EXT-X-BYTERANGE\", \"3\"),\n                    Segment(0),\n                    Segment(1),\n                ],\n                end=True,\n            ),\n        ])\n\n        self.await_write(2 - 1)\n        self.thread.close()\n\n        assert mock_log.error.call_args_list == [\n            call(\"Failed to fetch segment 0: Missing BYTERANGE offset\"),\n        ]\n        assert not self.called(Segment(0))\n", "type": "function"}, {"name": "subject", "is_method": true, "class_name": "TestHlsReloadTime", "parameters": ["self"], "calls": ["subject", "Event", "get_reload_time_called.set", "orig_get_reload_time", "patch.object", "patch.object", "self.start", "self.thread.reader.worker.join", "super", "get_reload_time_called.wait", "RuntimeError"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 1006, "end_line": 1033}, "code_snippet": "    def subject(self, *args, **kwargs):\n        super().subject(*args, start=False, **kwargs)\n\n        # mock the worker thread's _playlist_reload_time method, so that the main thread can wait on its call\n        get_reload_time_called = Event()\n        orig_get_reload_time = self.thread.reader.worker._get_reload_time\n\n        def mocked_get_reload_time(*args2, **kwargs2):\n            get_reload_time_called.set()\n            return orig_get_reload_time(*args2, **kwargs2)\n\n        # immediately kill the writer thread as we don't need it and don't want to wait for its queue polling to end\n        def mocked_queue_get():\n            return None\n\n        with (\n            patch.object(self.thread.reader.worker, \"_get_reload_time\", side_effect=mocked_get_reload_time),\n            patch.object(self.thread.reader.writer, \"_queue_get\", side_effect=mocked_queue_get),\n        ):\n            self.start()\n\n            if not get_reload_time_called.wait(timeout=5):  # pragma: no cover\n                raise RuntimeError(\"Missing _get_reload_time() call\")\n\n            # wait for the worker thread to terminate, so that deterministic assertions can be done about the reload time\n            self.thread.reader.worker.join()\n\n            return self.thread.reader.worker._reload_time\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.35065484046936035}
{"question": "Where is the circular dependency detection mechanism implemented for configurable parameter dependency resolution in the codebase?", "answer": "", "relative_code_list": null, "ground_truth": "The cycle detection mechanism is implemented in the `requires` method of the `Arguments` class located in `src/streamlink/options.py` between lines 282-303, specifically where it checks if `required.name` or `r.name` exists in the `results` set and raises a `RuntimeError` with the message \"cycle detected in plugin argument config\".", "score": null, "retrieved_content": [{"name": "test_requires_cycle", "is_method": true, "class_name": "TestArguments", "parameters": ["self", "args"], "calls": ["pytest.mark.parametrize", "pytest.raises", "list", "pytest.param", "pytest.param", "pytest.param", "args.requires", "Arguments", "Arguments", "Arguments", "Argument", "Argument", "Argument", "Argument", "Argument", "Argument"], "code_location": {"file": "test_options.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 361, "end_line": 364}, "code_snippet": "    def test_requires_cycle(self, args: Arguments):\n        with pytest.raises(RuntimeError) as cm:\n            list(args.requires(\"test1\"))\n        assert cm.value.args[0] == \"cycle detected in plugin argument config\"\n", "type": "function"}, {"name": "pytest_generate_tests", "is_method": false, "class_name": null, "parameters": ["metafunc"], "calls": ["issubclass", "get", "parametrizer", "globals"], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 28, "end_line": 33}, "code_snippet": "def pytest_generate_tests(metafunc: pytest.Metafunc):\n    if metafunc.cls is not None and issubclass(metafunc.cls, PluginCanHandleUrl):\n        name: str = f\"_parametrize_plugincanhandleurl_{metafunc.function.__name__}\"\n        parametrizer: Callable[[pytest.Metafunc], None] | None = globals().get(name)\n        if parametrizer:\n            parametrizer(metafunc)\n", "type": "function"}, {"name": "_parametrize_plugincanhandleurl_test_url_matches_groups_named", "is_method": false, "class_name": null, "parameters": ["metafunc"], "calls": ["metafunc.cls.urlgroups_named", "metafunc.parametrize"], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 79, "end_line": 85}, "code_snippet": "def _parametrize_plugincanhandleurl_test_url_matches_groups_named(metafunc: pytest.Metafunc):\n    urlgroups = metafunc.cls.urlgroups_named()\n    metafunc.parametrize(\n        \"name,url,groups\",\n        urlgroups,\n        ids=[f\"NAME={name} URL={url} GROUPS={groups}\" for name, url, groups in urlgroups],\n    )\n", "type": "function"}, {"name": "_parametrize_plugincanhandleurl_test_url_matches_groups_unnamed", "is_method": false, "class_name": null, "parameters": ["metafunc"], "calls": ["metafunc.cls.urlgroups_unnamed", "metafunc.parametrize"], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 70, "end_line": 76}, "code_snippet": "def _parametrize_plugincanhandleurl_test_url_matches_groups_unnamed(metafunc: pytest.Metafunc):\n    urlgroups = metafunc.cls.urlgroups_unnamed()\n    metafunc.parametrize(\n        \"url,groups\",\n        urlgroups,\n        ids=[f\"URL={url} GROUPS={groups}\" for url, groups in urlgroups],\n    )\n", "type": "function"}, {"name": "_parametrize_plugincanhandleurl_test_all_named_matchers_have_tests", "is_method": false, "class_name": null, "parameters": ["metafunc"], "calls": ["metafunc.parametrize", "metafunc.cls.matchers"], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 45, "end_line": 51}, "code_snippet": "def _parametrize_plugincanhandleurl_test_all_named_matchers_have_tests(metafunc: pytest.Metafunc):\n    matchers: list[Matcher] = [m for m in metafunc.cls.matchers() if m.name is not None]\n    metafunc.parametrize(\n        \"matcher\",\n        matchers,\n        ids=[m.name for m in matchers],\n    )\n", "type": "function"}, {"name": "_parametrize_plugincanhandleurl_test_url_matches_positive_named", "is_method": false, "class_name": null, "parameters": ["metafunc"], "calls": ["metafunc.cls.urls_named", "metafunc.parametrize"], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 61, "end_line": 67}, "code_snippet": "def _parametrize_plugincanhandleurl_test_url_matches_positive_named(metafunc: pytest.Metafunc):\n    urls = metafunc.cls.urls_named()\n    metafunc.parametrize(\n        \"name,url\",\n        urls,\n        ids=[f\"NAME={name} URL={url}\" for name, url in urls],\n    )\n", "type": "function"}, {"name": "_parametrize_plugincanhandleurl_test_all_matchers_match", "is_method": false, "class_name": null, "parameters": ["metafunc"], "calls": ["list", "metafunc.parametrize", "enumerate", "metafunc.cls.matchers"], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 36, "end_line": 42}, "code_snippet": "def _parametrize_plugincanhandleurl_test_all_matchers_match(metafunc: pytest.Metafunc):\n    matchers: list[tuple[int, Matcher]] = list(enumerate(metafunc.cls.matchers()))\n    metafunc.parametrize(\n        \"matcher\",\n        [m for i, m in matchers],\n        ids=[m.name or f\"#{i}\" for i, m in matchers],\n    )\n", "type": "function"}, {"name": "test_matchers_inheritance_named_duplicate", "is_method": true, "class_name": "TestPluginMatcher", "parameters": ["self"], "calls": ["pluginmatcher", "pytest.raises", "pluginmatcher", "re.compile", "re.compile"], "code_location": {"file": "test_plugin.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 196, "end_line": 205}, "code_snippet": "    def test_matchers_inheritance_named_duplicate(self):\n        @pluginmatcher(name=\"foo\", pattern=re.compile(r\"foo\"))\n        class PluginOne(FakePlugin):\n            pass\n\n        with pytest.raises(ValueError, match=r\"^A matcher named 'foo' has already been registered$\"):\n\n            @pluginmatcher(name=\"foo\", pattern=re.compile(r\"foo\"))\n            class PluginTwo(PluginOne):\n                pass\n", "type": "function"}, {"name": "__iter__", "is_method": true, "class_name": "Arguments", "parameters": ["self"], "calls": ["reversed", "self.values"], "code_location": {"file": "options.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink", "start_line": 270, "end_line": 272}, "code_snippet": "    def __iter__(self) -> Iterator[Argument]:  # type: ignore[override]\n        # iterate in reverse order due to add() being called by multiple pluginargument decorators in reverse order\n        return reversed(self.values())\n", "type": "function"}, {"name": "test_requires", "is_method": true, "class_name": "TestArguments", "parameters": ["self"], "calls": ["Argument", "Argument", "Argument", "Arguments", "list", "args.requires"], "code_location": {"file": "test_options.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 319, "end_line": 325}, "code_snippet": "    def test_requires(self):\n        test1 = Argument(\"test1\", requires=\"test2\")\n        test2 = Argument(\"test2\", requires=\"test3\")\n        test3 = Argument(\"test3\")\n        args = Arguments(test1, test2, test3)\n\n        assert list(args.requires(\"test1\")) == [test2, test3]\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.34702634811401367}
{"question": "Where in the plugin metadata loader class are the specific validation checks that prevent loading corrupted plugin data from JSON files?", "answer": "", "relative_code_list": null, "ground_truth": "The validation checks are implemented in the _validate method (lines 263-281) which performs hash verification using importlib.metadata to retrieve the expected hash from package metadata, then compares it against the computed hash of the content using base64.urlsafe_b64encode and hashlib algorithms to ensure data integrity before parsing.", "score": null, "retrieved_content": [{"name": "_validate", "is_method": true, "class_name": "StreamlinkPluginsData", "parameters": ["content"], "calls": ["next", "getattr", "hashalg", "rstrip", "log.error", "log.error", "decode", "importlib.metadata.files", "base64.urlsafe_b64encode", "hashobj.digest"], "code_location": {"file": "plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/session", "start_line": 260, "end_line": 278}, "code_snippet": "    def _validate(content: bytes) -> None:\n        # find plugins JSON checksum in package metadata\n        # https://packaging.python.org/en/latest/specifications/recording-installed-packages/#the-record-file\n        mode, filehash = next(\n            (packagepath.hash.mode, packagepath.hash.value)\n            for packagepath in importlib.metadata.files(_PLUGINSDATA_PACKAGENAME) or []\n            if packagepath.hash is not None and packagepath.parts == _PLUGINSDATA_PACKAGEPATH\n        )\n        if mode not in hashlib.algorithms_guaranteed or not filehash:\n            log.error(\"Unknown plugins data hash mode, falling back to loading all plugins\")\n            raise Exception\n\n        # compare checksums\n        hashalg = getattr(hashlib, mode)\n        hashobj = hashalg(content)\n        digest = base64.urlsafe_b64encode(hashobj.digest()).decode(\"utf-8\").rstrip(\"=\")\n        if digest != filehash:\n            log.error(\"Plugins data checksum mismatch, falling back to loading all plugins\")\n            raise Exception\n", "type": "function"}, {"name": "test_key_type", "is_method": true, "class_name": "TestPluginMetadata", "parameters": ["self", "metadata_dict"], "calls": ["metadata_dict.get"], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 202, "end_line": 204}, "code_snippet": "    def test_key_type(self, metadata_dict):\n        assert metadata_dict.get(\"type\") in PLUGIN_TYPES, \\\n            \"$type metadata has the correct value\"  # fmt: skip\n", "type": "function"}, {"name": "test_key_metadata", "is_method": true, "class_name": "TestPluginMetadata", "parameters": ["self", "metadata_items"], "calls": ["all", "PLUGIN_METADATA.index", "re_metadata.match", "val.split", "sorted"], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 206, "end_line": 211}, "code_snippet": "    def test_key_metadata(self, metadata_items):\n        assert all(re_metadata.match(val) for key, val in metadata_items if key == \"metadata\"), \\\n            \"$metadata metadata values have the correct format\"  # fmt: skip\n        indexes = [PLUGIN_METADATA.index(val.split(\" \")[0]) for key, val in metadata_items if key == \"metadata\"]\n        assert [PLUGIN_METADATA[i] for i in indexes] == [PLUGIN_METADATA[i] for i in sorted(indexes)], \\\n            \"$metadata metadata values are ordered correctly\"  # fmt: skip\n", "type": "function"}, {"name": "test_parse_json", "is_method": true, "class_name": "TestUtilsParse", "parameters": ["self"], "calls": ["parse_json", "parse_json", "parse_json", "pytest.raises", "parse_json", "pytest.raises", "parse_json", "pytest.raises", "parse_json", "validate.Schema"], "code_location": {"file": "test_parse.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/utils", "start_line": 11, "end_line": 20}, "code_snippet": "    def test_parse_json(self):\n        assert parse_json(\"{}\") == {}\n        assert parse_json('{\"test\": 1}') == {\"test\": 1}\n        assert parse_json('{\"test\": 1}', schema=validate.Schema({\"test\": 1})) == {\"test\": 1}\n        with pytest.raises(PluginError):\n            parse_json(\"\"\"{\"test: 1}\"\"\")\n        with pytest.raises(SyntaxError):\n            parse_json(\"\"\"{\"test: 1}\"\"\", exception=SyntaxError)\n        with pytest.raises(PluginError):\n            parse_json(\"\"\"{\"test: 1}\"\"\" * 10)\n", "type": "function"}, {"name": "test_strip_json_comment", "is_method": true, "class_name": "TestLoadPluginsData", "parameters": ["self", "caplog", "session", "pluginsdata"], "calls": ["pytest.mark.parametrize", "session.plugins.get_names", "pytest.param", "caplog.get_records"], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/session", "start_line": 322, "end_line": 326}, "code_snippet": "    def test_strip_json_comment(self, caplog: pytest.LogCaptureFixture, session: Streamlink, pluginsdata: str):\n        assert \"fake\" not in session.plugins\n        assert \"testplugin\" not in session.plugins\n        assert session.plugins.get_names() == [\"testplugin\"]\n        assert [(record.name, record.levelname, record.message) for record in caplog.get_records(when=\"setup\")] == []\n", "type": "function"}, {"name": "test_plugin_metadata", "is_method": false, "class_name": null, "parameters": ["attr"], "calls": ["pytest.mark.parametrize", "FakePlugin", "getattr", "callable", "setattr", "setattr", "Mock", "getattr", "getter", "getter", "Foo", "getter"], "code_location": {"file": "test_plugin.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 424, "end_line": 440}, "code_snippet": "def test_plugin_metadata(attr):\n    plugin = FakePlugin(Mock(), \"https://foo.bar/\")\n    getter = getattr(plugin, f\"get_{attr}\")\n    assert callable(getter)\n\n    assert getattr(plugin, attr) is None\n    assert getter() is None\n\n    setattr(plugin, attr, \" foo bar \")\n    assert getter() == \"foo bar\"\n\n    class Foo:\n        def __str__(self):\n            return \" baz qux \"\n\n    setattr(plugin, attr, Foo())\n    assert getter() == \"baz qux\"\n", "type": "function"}, {"name": "test_matchers_failure", "is_method": true, "class_name": "TestLoadPluginsData", "parameters": ["self", "caplog", "session", "pluginsdata"], "calls": ["pytest.mark.parametrize", "session.plugins.get_names", "pytest.param", "session.plugins.iter_matchers", "caplog.get_records"], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/session", "start_line": 391, "end_line": 399}, "code_snippet": "    def test_matchers_failure(self, caplog: pytest.LogCaptureFixture, session: Streamlink, pluginsdata: str):\n        assert \"fake\" in session.plugins\n        assert \"success\" not in session.plugins\n        assert \"fail\" not in session.plugins\n        assert session.plugins.get_names() == [\"fake\"]\n        assert [name for name, matchers in session.plugins.iter_matchers()] == [\"fake\"]\n        assert [(record.name, record.levelname, record.message) for record in caplog.get_records(when=\"setup\")] == [\n            (\"streamlink.session\", \"error\", \"Error while loading pluginmatcher data from JSON\"),\n        ]\n", "type": "function"}, {"name": "test_arguments_failure", "is_method": true, "class_name": "TestLoadPluginsData", "parameters": ["self", "caplog", "session", "pluginsdata"], "calls": ["pytest.mark.parametrize", "session.plugins.get_names", "pytest.param", "session.plugins.iter_arguments", "caplog.get_records"], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/session", "start_line": 525, "end_line": 533}, "code_snippet": "    def test_arguments_failure(self, caplog: pytest.LogCaptureFixture, session: Streamlink, pluginsdata: str):\n        assert \"fake\" in session.plugins\n        assert \"success\" not in session.plugins\n        assert \"fail\" not in session.plugins\n        assert session.plugins.get_names() == [\"fake\"]\n        assert [name for name, arguments in session.plugins.iter_arguments()] == [\"fake\"]\n        assert [(record.name, record.levelname, record.message) for record in caplog.get_records(when=\"setup\")] == [\n            (\"streamlink.session\", \"error\", \"Error while loading pluginargument data from JSON\"),\n        ]\n", "type": "function"}, {"name": "_parse", "is_method": true, "class_name": "StreamlinkPluginsData", "parameters": ["cls", "content"], "calls": ["_RE_STRIP_JSON_COMMENTS.sub", "json.loads", "cls._build_matchers", "cls._build_arguments", "log.exception", "log.exception"], "code_location": {"file": "plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/session", "start_line": 281, "end_line": 297}, "code_snippet": "    def _parse(cls, content: bytes) -> tuple[dict[str, Matchers], dict[str, Arguments]]:\n        content = _RE_STRIP_JSON_COMMENTS.sub(b\"\", content)\n        data: dict[str, _TPluginData] = json.loads(content)\n\n        try:\n            matchers = cls._build_matchers(data)\n        except Exception:\n            log.exception(\"Error while loading pluginmatcher data from JSON\")\n            raise\n\n        try:\n            arguments = cls._build_arguments(data)\n        except Exception:\n            log.exception(\"Error while loading pluginargument data from JSON\")\n            raise\n\n        return matchers, arguments\n", "type": "function"}, {"name": "load", "is_method": true, "class_name": "StreamlinkPluginsData", "parameters": ["cls"], "calls": ["suppress", "_PLUGINSDATA_PATH.read_bytes", "cls._validate", "cls._parse"], "code_location": {"file": "plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/session", "start_line": 248, "end_line": 257}, "code_snippet": "    def load(cls) -> tuple[dict[str, Matchers], dict[str, Arguments]] | None:\n        # specific errors get logged, others are ignored intentionally\n        with suppress(Exception):\n            content = _PLUGINSDATA_PATH.read_bytes()\n\n            cls._validate(content)\n\n            return cls._parse(content)\n\n        return None\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.34354352951049805}
{"question": "Where in the codebase does the initialization of a base class for parsing elements containing multiple segment definitions' timing attribute depend on the presence of a timing child element or a default value from a parent context?", "answer": "", "relative_code_list": null, "ground_truth": "The initialization of the segmentTimeline attribute in the _MultipleSegmentBaseType class is defined in the __init__ method at lines 695-697 of the manifest.py file located at ./src/streamlink/stream/dash. It uses the only_child method to check for a SegmentTimeline child element and falls back to the _find_default method if no child is present, indicating dependency on both the DOM structure and inherited context defaults.", "score": null, "retrieved_content": [{"name": "__init__", "is_method": true, "class_name": "_MultipleSegmentBaseType", "parameters": ["self"], "calls": ["__init__", "self.attr", "self.attr", "self.only_child", "self._find_default", "super", "self._find_default", "self._find_default"], "code_location": {"file": "manifest.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/dash", "start_line": 681, "end_line": 697}, "code_snippet": "    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n\n        self.duration = self.attr(\n            \"duration\",\n            parser=int,\n            default=self._find_default(\"duration\"),\n        )\n        self.startNumber: int = self.attr(\n            \"startNumber\",\n            parser=int,\n            default=self._find_default(\"startNumber\", 1),\n        )\n\n        self.duration_seconds = self.duration / self.timescale if self.duration else 0.0\n\n        self.segmentTimeline = self.only_child(SegmentTimeline) or self._find_default(\"segmentTimeline\")\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "SegmentTimeline", "parameters": ["self"], "calls": ["__init__", "self.walk_back_get_attr", "self.children", "super"], "code_location": {"file": "manifest.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/dash", "start_line": 977, "end_line": 982}, "code_snippet": "    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n\n        self.timescale = self.walk_back_get_attr(\"timescale\")\n\n        self.timeline_segments = self.children(_TimelineSegment)\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "_SegmentBaseType", "parameters": ["self"], "calls": ["__init__", "self.attr", "self.attr", "self.attr", "self.only_child", "self._find_default", "super", "self._find_default", "MPDParsers.timedelta", "self._find_default", "MPDParsers.timedelta", "self._find_default", "timedelta", "timedelta"], "code_location": {"file": "manifest.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/dash", "start_line": 644, "end_line": 665}, "code_snippet": "    def __init__(self, *args, period: \"Period\", **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n\n        self.period = period\n\n        self.timescale: int = self.attr(\n            \"timescale\",\n            parser=int,\n            default=self._find_default(\"timescale\", 1),\n        )\n        self.presentationTimeOffset: timedelta = self.attr(\n            \"presentationTimeOffset\",\n            parser=MPDParsers.timedelta(self.timescale),\n            default=self._find_default(\"presentationTimeOffset\", timedelta()),\n        )\n        self.availabilityTimeOffset: timedelta = self.attr(\n            \"availabilityTimeOffset\",\n            parser=MPDParsers.timedelta(self.timescale),\n            default=self._find_default(\"availabilityTimeOffset\", timedelta()),\n        )\n\n        self.initialization = self.only_child(Initialization) or self._find_default(\"initialization\")\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "Period", "parameters": ["self"], "calls": ["__init__", "kwargs.get", "self.attr", "self.attr", "self.attr", "self.attr", "self.children", "self.only_child", "self.only_child", "self.only_child", "self.children", "self.only_child", "self.children", "self.children", "timedelta", "super", "MPDParsers.duration", "timedelta", "MPDParsers.duration", "timedelta"], "code_location": {"file": "manifest.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/dash", "start_line": 402, "end_line": 435}, "code_snippet": "    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n\n        self.i = kwargs.get(\"i\", 0)\n        self.id = self.attr(\"id\")\n        self.bitstreamSwitching = self.attr(\n            \"bitstreamSwitching\",\n            parser=MPDParsers.bool_str,\n        )\n        self.duration = self.attr(\n            \"duration\",\n            parser=MPDParsers.duration(self.root.publishTime),\n            default=timedelta(),\n        )\n        self.start = self.attr(\n            \"start\",\n            parser=MPDParsers.duration(self.root.publishTime),\n            default=timedelta(),\n        )\n\n        # anchor time for segment availability\n        offset = self.start if self.root.type == \"dynamic\" else timedelta()\n        self.availabilityStartTime = self.root.availabilityStartTime + offset\n\n        # TODO: Early Access Periods\n\n        self.baseURLs = self.children(BaseURL)\n        self.segmentBase = self.only_child(SegmentBase, period=self)\n        self.segmentList = self.only_child(SegmentList, period=self)\n        self.segmentTemplate = self.only_child(SegmentTemplate, period=self)\n        self.adaptationSets = self.children(AdaptationSet, minimum=1)\n        self.assetIdentifier = self.only_child(AssetIdentifier)\n        self.eventStream = self.children(EventStream)\n        self.subset = self.children(Subset)\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "MPD", "parameters": ["self"], "calls": ["__init__", "defaultdict", "self.timelines.update", "self.attr", "self.attr", "self.attr", "self.attr", "self.attr", "self.attr", "self.attr", "self.attr", "self.attr", "self.attr", "self.attr", "self.children", "self.children", "self.children", "self.children", "kwargs.pop", "list", "urlunparse", "super", "MPDParsers.duration", "MPDParsers.duration", "timedelta", "MPDParsers.duration", "MPDParsers.duration", "timedelta", "MPDParsers.duration", "timedelta", "urlparse", "rsplit", "max", "self.minBufferTime.total_seconds"], "code_location": {"file": "manifest.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/dash", "start_line": 289, "end_line": 369}, "code_snippet": "    def __init__(self, *args, url: str | None = None, **kwargs) -> None:\n        # top level has no parent\n        kwargs[\"root\"] = self\n        kwargs[\"parent\"] = None\n        super().__init__(*args, **kwargs)\n\n        # parser attributes\n        self.url = url\n        self.timelines = defaultdict(lambda: -1)\n        self.timelines.update(kwargs.pop(\"timelines\", {}))\n\n        self.id = self.attr(\"id\")\n        self.profiles = self.attr(\n            \"profiles\",\n            required=True,\n        )\n        self.type = self.attr(\n            \"type\",\n            parser=MPDParsers.type,\n            default=\"static\",\n        )\n        self.publishTime = self.attr(\n            \"publishTime\",\n            parser=MPDParsers.datetime,\n            default=EPOCH_START,\n        )\n        self.availabilityStartTime = self.attr(\n            \"availabilityStartTime\",\n            parser=MPDParsers.datetime,\n            default=EPOCH_START,\n        )\n        self.availabilityEndTime = self.attr(\n            \"availabilityEndTime\",\n            parser=MPDParsers.datetime,\n        )\n        self.minBufferTime: timedelta = self.attr(  # type: ignore[assignment]\n            \"minBufferTime\",\n            parser=MPDParsers.duration(self.publishTime),\n            required=True,\n        )\n        self.minimumUpdatePeriod = self.attr(\n            \"minimumUpdatePeriod\",\n            parser=MPDParsers.duration(self.publishTime),\n            default=timedelta(),\n        )\n        self.timeShiftBufferDepth = self.attr(\n            \"timeShiftBufferDepth\",\n            parser=MPDParsers.duration(self.publishTime),\n        )\n        self.mediaPresentationDuration = self.attr(\n            \"mediaPresentationDuration\",\n            parser=MPDParsers.duration(self.publishTime),\n            default=timedelta(),\n        )\n        self.suggestedPresentationDelay = self.attr(\n            \"suggestedPresentationDelay\",\n            parser=MPDParsers.duration(self.publishTime),\n            # if there is no delay, use a delay of 3 seconds, but respect the manifest's minBufferTime\n            # TODO: add a customizable parameter for this\n            default=timedelta(\n                seconds=max(\n                    self.DEFAULT_MINBUFFERTIME,\n                    self.minBufferTime.total_seconds(),\n                ),\n            ),\n        )\n\n        # parse children\n        location = self.children(Location)\n        self.location = location[0] if location else None\n        if self.location:\n            self.url = self.location.text or \"\"\n            urlp = list(urlparse(self.url))\n            if urlp[2]:\n                urlp[2], _ = urlp[2].rsplit(\"/\", 1)\n            self._base_url = urlunparse(urlp)\n\n        self.baseURLs = self.children(BaseURL)\n        self.periods = self.children(Period, minimum=1)\n        self.periods_map = {period.id: period for period in self.periods if period.id is not None}\n        self.programInformation = self.children(ProgramInformation)\n", "type": "function"}, {"name": "SegmentTimeline", "docstring": "", "methods": ["__init__", "segments"], "attributes": ["__tag__"], "code_location": {"file": "manifest.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/dash", "start_line": 974, "end_line": 993}, "type": "class"}, {"name": "test_segment_no_segments_no_targetduration", "is_method": true, "class_name": "TestHlsReloadTime", "parameters": ["self"], "calls": ["self.subject", "Playlist"], "code_location": {"file": "test_hls.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/stream/hls", "start_line": 1047, "end_line": 1049}, "code_snippet": "    def test_segment_no_segments_no_targetduration(self):\n        time = self.subject([Playlist(0, [], end=True, targetduration=0)], reload_time=\"segment\")\n        assert time == 6, \"sets reload time to 6 seconds when no segments and no targetduration are available\"\n", "type": "function"}, {"name": "_MultipleSegmentBaseType", "docstring": "", "methods": ["__init__"], "attributes": [], "code_location": {"file": "manifest.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/dash", "start_line": 680, "end_line": 697}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "MPDNode", "parameters": ["self", "node", "root", "parent"], "calls": ["kwargs.get", "set", "MPDParsingError", "self.node.tag.lower", "self.__tag__.lower"], "code_location": {"file": "manifest.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/dash", "start_line": 135, "end_line": 142}, "code_snippet": "    def __init__(self, node: _Element, root: MPD, parent: MPDNode, **kwargs) -> None:\n        self.node = node\n        self.root = root\n        self.parent = parent\n        self._base_url = kwargs.get(\"base_url\")\n        self.attributes: set[str] = set()\n        if self.__tag__ and self.node.tag.lower() != self.__tag__.lower():\n            raise MPDParsingError(f\"Root tag did not match the expected tag: {self.__tag__}\")\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "SegmentTemplate", "parameters": ["self"], "calls": ["__init__", "self.attr", "self.attr", "super"], "code_location": {"file": "manifest.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/dash", "start_line": 800, "end_line": 810}, "code_snippet": "    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n\n        self.fmt_initialization = self.attr(\n            \"initialization\",\n            parser=MPDParsers.segment_template,\n        )\n        self.fmt_media = self.attr(\n            \"media\",\n            parser=MPDParsers.segment_template,\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.34515953063964844}
{"question": "Where in the test class for file input/output operations is the logic implemented that determines the modification tracking indicator state after loading cached data?", "answer": "", "relative_code_list": null, "ground_truth": "The logic that determines the dirty flag state after loading cached data is implemented in the test_load_data method of the TestIO class, specifically in the assertions that verify cache._dirty is dirty, which validates the expected state of the dirty flag based on the test parameters and the Cache class's internal handling during data loading.", "score": null, "retrieved_content": [{"name": "test_load_data", "is_method": true, "class_name": "TestIO", "parameters": ["self", "mock_cache_file", "expected", "dirty"], "calls": ["pytest.mark.usefixtures", "pytest.mark.parametrize", "freezegun.freeze_time", "Cache", "pytest.param", "pytest.param", "cache.get"], "code_location": {"file": "test_cache.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 413, "end_line": 420}, "code_snippet": "    def test_load_data(self, mock_cache_file: Mock, expected: str | None, dirty: bool):\n        with freezegun.freeze_time(\"2000-01-01T00:00:00Z\"):\n            cache = Cache(\"mocked-io\")\n            assert not cache._loaded\n            assert not cache._dirty\n            assert cache.get(\"foo\") == expected\n            assert cache._loaded\n            assert cache._dirty is dirty\n", "type": "function"}, {"name": "test_load", "is_method": true, "class_name": "TestIO", "parameters": ["self", "caplog", "cache", "mock_cache_file", "mock_json_load", "log"], "calls": ["pytest.mark.parametrize", "cache._load", "cache._load", "pytest.param", "pytest.param", "pytest.param", "FileNotFoundError", "PermissionError", "JSONDecodeError"], "code_location": {"file": "test_cache.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 380, "end_line": 392}, "code_snippet": "    def test_load(self, caplog: pytest.LogCaptureFixture, cache: Cache, mock_cache_file: Mock, mock_json_load: Mock, log: list):\n        cache._load()\n        assert cache._loaded, \"Failed load attempts also set the loaded state to True\"\n        assert not cache._dirty\n        assert cache._cache == {}\n        assert [(record.name, record.levelname, record.message) for record in caplog.records] == [\n            (\"streamlink.cache\", \"trace\", f\"Loading cache file: {cache.filename}\"),\n            *log,\n        ]\n        assert mock_cache_file.call_count == 1\n\n        cache._load()\n        assert mock_cache_file.call_count == 1\n", "type": "function"}, {"name": "_dirty", "is_method": true, "class_name": "Cache", "parameters": ["self"], "calls": [], "code_location": {"file": "cache.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink", "start_line": 79, "end_line": 80}, "code_snippet": "    def _dirty(self):\n        return self._cache != self._cache_orig\n", "type": "function"}, {"name": "test_disabled", "is_method": true, "class_name": "TestIO", "parameters": ["self", "caplog", "mock_cache_file"], "calls": ["pytest.mark.parametrize", "freezegun.freeze_time", "Cache", "cache.set", "cache._save", "pytest.param", "cache.get", "cache.get", "cache.filename.exists"], "code_location": {"file": "test_cache.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 524, "end_line": 545}, "code_snippet": "    def test_disabled(self, caplog: pytest.LogCaptureFixture, mock_cache_file: Mock):\n        with freezegun.freeze_time(\"2000-01-01T00:00:00Z\"):\n            cache = Cache(\"mocked-io\", disabled=True)\n            assert cache._disabled\n            assert cache._loaded\n            assert cache._cache == {}\n            assert cache._cache_orig == {}\n            assert mock_cache_file.call_args_list == []\n\n            assert cache.get(\"foo\") is None\n            assert not cache._dirty\n\n            cache.set(\"foo\", \"bar\")\n            assert cache._dirty\n            assert cache._timer is None\n\n            assert cache.get(\"foo\") == \"bar\"\n\n            cache._save()\n            assert not cache.filename.exists()\n\n        assert [(record.name, record.levelname, record.message) for record in caplog.records] == []\n", "type": "function"}, {"name": "test_disabled_cache", "is_method": true, "class_name": "TestPlugin", "parameters": ["self", "session", "expected"], "calls": ["pytest.mark.parametrize", "FakePlugin", "pytest.param", "pytest.param", "pytest.param"], "code_location": {"file": "test_plugin.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 122, "end_line": 124}, "code_snippet": "    def test_disabled_cache(self, session: Streamlink, expected: bool):\n        plugin = FakePlugin(session, \"https://mocked\")\n        assert plugin.cache._disabled is expected\n", "type": "function"}, {"name": "test_not_dirty", "is_method": true, "class_name": "TestScheduleSave", "parameters": ["self", "caplog", "cache", "mock_save"], "calls": ["cache._schedule_save"], "code_location": {"file": "test_cache.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 283, "end_line": 288}, "code_snippet": "    def test_not_dirty(self, caplog: pytest.LogCaptureFixture, cache: Cache, mock_save: Mock):\n        assert not cache._dirty\n        cache._schedule_save()\n        assert not cache._timer\n        assert mock_save.call_count == 0\n        assert caplog.records == []\n", "type": "function"}, {"name": "test_save_not_dirty", "is_method": true, "class_name": "TestIO", "parameters": ["self", "caplog", "cache"], "calls": ["cache._save", "cache.filename.exists"], "code_location": {"file": "test_cache.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 496, "end_line": 500}, "code_snippet": "    def test_save_not_dirty(self, caplog: pytest.LogCaptureFixture, cache: Cache):\n        assert not cache._dirty\n        cache._save()\n        assert not cache.filename.exists()\n        assert caplog.records == []\n", "type": "function"}, {"name": "test_schedule_save", "is_method": true, "class_name": "TestScheduleSave", "parameters": ["self", "monkeypatch", "caplog", "cache", "mock_save"], "calls": ["cache._schedule_save", "cache._timer.is_alive", "monkeypatch.setattr", "cache._timer.finished.set", "cache._timer.join", "cache._timer.is_alive"], "code_location": {"file": "test_cache.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 302, "end_line": 328}, "code_snippet": "    def test_schedule_save(\n        self,\n        monkeypatch: pytest.MonkeyPatch,\n        caplog: pytest.LogCaptureFixture,\n        cache: Cache,\n        mock_save: Mock,\n    ):\n        cache._cache = {\"key\": {\"value\": \"foo\"}}\n        assert cache._dirty\n        assert not cache._timer\n\n        cache._schedule_save()\n        assert mock_save.call_count == 0\n        assert cache._timer\n        assert cache._timer.is_alive()\n\n        # Let the timer \"expire\" by monkeypatching the is_set() method and then setting the inner threading.Event flag.\n        # This depends on the implementation details of threading.Timer.\n        monkeypatch.setattr(cache._timer.finished, \"is_set\", lambda: False)\n        cache._timer.finished.set()  # same as cancel(), but is_set() will return False, so the callback will be executed\n        cache._timer.join(timeout=1)\n        assert not cache._timer.is_alive()\n        assert mock_save.call_count == 1\n\n        assert [(record.name, record.levelname, record.message) for record in caplog.records] == [\n            (\"streamlink.cache\", \"trace\", \"Scheduling write to cache file: 3.0s\"),\n        ]\n", "type": "function"}, {"name": "test_save_success", "is_method": true, "class_name": "TestIO", "parameters": ["self", "caplog", "cache"], "calls": ["pytest.mark.parametrize", "cache._save", "cache.filename.exists", "cache.filename.read_text", "freezegun.freeze_time", "pytest.param", "cache.get", "Path"], "code_location": {"file": "test_cache.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 474, "end_line": 494}, "code_snippet": "    def test_save_success(self, caplog: pytest.LogCaptureFixture, cache: Cache):\n        cache._loaded = True\n        cache._cache_orig = {\"foo\": {\"value\": \"bear\", \"expires\": 946684801}}\n        cache._cache = {\"foo\": {\"value\": \"\", \"expires\": 946684801}}\n        assert cache._dirty\n\n        cache._save()\n        assert not cache._dirty\n        assert cache._cache_orig == {\"foo\": {\"value\": \"\", \"expires\": 946684801}}\n        assert cache._cache == cache._cache_orig\n        assert cache._cache[\"foo\"] is not cache._cache_orig[\"foo\"]\n        assert cache.filename.exists()\n        assert (\n            cache.filename.read_text(encoding=\"utf-8\")\n            == \"\"\"{\\n  \"foo\": {\\n    \"value\": \"\\\\ud83d\\\\udc3b\",\\n    \"expires\": 946684801\\n  }\\n}\"\"\"\n        )\n        assert [(record.name, record.levelname, record.message) for record in caplog.records] == [\n            (\"streamlink.cache\", \"trace\", f\"Writing to cache file: {cache.filename}\"),\n        ]\n        with freezegun.freeze_time(\"2000-01-01T00:00:00Z\"):\n            assert cache.get(\"foo\") == \"\"\n", "type": "function"}, {"name": "test_early_close", "is_method": false, "class_name": null, "parameters": ["tmp_path", "fd"], "calls": ["FileOutput", "isinstance", "fo.close", "fo.record.close", "filename.exists", "FileOutput"], "code_location": {"file": "test_file.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/cli/output", "start_line": 40, "end_line": 49}, "code_snippet": "def test_early_close(tmp_path: Path, fd: BufferedRandom):\n    filename = tmp_path / \"foo\" / \"bar\"\n    fo = FileOutput(filename=filename, record=FileOutput(fd=fd))\n    assert isinstance(fo.record, FileOutput)\n    assert not fo.opened\n    assert not fo.record.opened\n    assert not filename.exists()\n\n    fo.close()\n    fo.record.close()\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3437211513519287}
{"question": "Where in the input dictionary must the specific JSON structure and data types be located for the JSON deserialization method of the socket closure event class to successfully instantiate an instance without raising parsing errors?", "answer": "", "relative_code_list": null, "ground_truth": "The JSON input dictionary must contain exactly two keys: \"identifier\" and \"timestamp\". The value for \"identifier\" must be a valid JSON-serializable representation that can be parsed by RequestId.from_json(), typically a string matching the RequestId format. The value for \"timestamp\" must be a valid JSON-serializable representation that can be parsed by MonotonicTime.from_json(), typically a numeric timestamp value. Both values must be present and of the correct types to avoid KeyError or parsing exceptions during the from_json method execution.", "score": null, "retrieved_content": [{"name": "from_json", "is_method": true, "class_name": "DirectUDPSocketClosed", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "MonotonicTime.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 4184, "end_line": 4188}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> DirectUDPSocketClosed:\n        return cls(\n            identifier=RequestId.from_json(json[\"identifier\"]),\n            timestamp=MonotonicTime.from_json(json[\"timestamp\"]),\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "DirectTCPSocketClosed", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "MonotonicTime.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 4050, "end_line": 4054}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> DirectTCPSocketClosed:\n        return cls(\n            identifier=RequestId.from_json(json[\"identifier\"]),\n            timestamp=MonotonicTime.from_json(json[\"timestamp\"]),\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "WebSocketClosed", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "MonotonicTime.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3755, "end_line": 3759}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> WebSocketClosed:\n        return cls(\n            request_id=RequestId.from_json(json[\"requestId\"]),\n            timestamp=MonotonicTime.from_json(json[\"timestamp\"]),\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "WebTransportClosed", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "MonotonicTime.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3953, "end_line": 3957}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> WebTransportClosed:\n        return cls(\n            transport_id=RequestId.from_json(json[\"transportId\"]),\n            timestamp=MonotonicTime.from_json(json[\"timestamp\"]),\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "DirectUDPSocketOpened", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "str", "int", "MonotonicTime.from_json", "str", "int"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 4140, "end_line": 4148}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> DirectUDPSocketOpened:\n        return cls(\n            identifier=RequestId.from_json(json[\"identifier\"]),\n            local_addr=str(json[\"localAddr\"]),\n            local_port=int(json[\"localPort\"]),\n            timestamp=MonotonicTime.from_json(json[\"timestamp\"]),\n            remote_addr=str(json[\"remoteAddr\"]) if \"remoteAddr\" in json else None,\n            remote_port=int(json[\"remotePort\"]) if \"remotePort\" in json else None,\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "DirectTCPSocketOpened", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "str", "int", "MonotonicTime.from_json", "str", "int"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 4006, "end_line": 4014}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> DirectTCPSocketOpened:\n        return cls(\n            identifier=RequestId.from_json(json[\"identifier\"]),\n            remote_addr=str(json[\"remoteAddr\"]),\n            remote_port=int(json[\"remotePort\"]),\n            timestamp=MonotonicTime.from_json(json[\"timestamp\"]),\n            local_addr=str(json[\"localAddr\"]) if \"localAddr\" in json else None,\n            local_port=int(json[\"localPort\"]) if \"localPort\" in json else None,\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "DirectUDPSocketAborted", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "str", "MonotonicTime.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 4164, "end_line": 4169}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> DirectUDPSocketAborted:\n        return cls(\n            identifier=RequestId.from_json(json[\"identifier\"]),\n            error_message=str(json[\"errorMessage\"]),\n            timestamp=MonotonicTime.from_json(json[\"timestamp\"]),\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "DirectTCPSocketAborted", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "str", "MonotonicTime.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 4030, "end_line": 4035}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> DirectTCPSocketAborted:\n        return cls(\n            identifier=RequestId.from_json(json[\"identifier\"]),\n            error_message=str(json[\"errorMessage\"]),\n            timestamp=MonotonicTime.from_json(json[\"timestamp\"]),\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "CertificateError", "parameters": ["cls", "json"], "calls": ["cls", "int", "str", "str"], "code_location": {"file": "security.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 460, "end_line": 465}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> CertificateError:\n        return cls(\n            event_id=int(json[\"eventId\"]),\n            error_type=str(json[\"errorType\"]),\n            request_url=str(json[\"requestURL\"]),\n        )\n", "type": "function"}, {"name": "from_json", "is_method": true, "class_name": "DirectUDPSocketCreated", "parameters": ["cls", "json"], "calls": ["cls", "RequestId.from_json", "DirectUDPSocketOptions.from_json", "MonotonicTime.from_json", "Initiator.from_json"], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 4113, "end_line": 4119}, "code_snippet": "    def from_json(cls, json: T_JSON_DICT) -> DirectUDPSocketCreated:\n        return cls(\n            identifier=RequestId.from_json(json[\"identifier\"]),\n            options=DirectUDPSocketOptions.from_json(json[\"options\"]),\n            timestamp=MonotonicTime.from_json(json[\"timestamp\"]),\n            initiator=Initiator.from_json(json[\"initiator\"]) if \"initiator\" in json else None,\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3332381248474121}
{"question": "How does the function that decrypts OpenSSL-encrypted data implement the OpenSSL key derivation process using the key derivation helper function to generate both key and IV from passphrase and salt?", "answer": "", "relative_code_list": null, "ground_truth": "The decrypt_openssl function implements the OpenSSL key derivation process by first checking if the input data starts with the 'Salted__' prefix, then extracting the salt from the subsequent bytes. It calls evp_bytestokey with the passphrase, salt, specified key_length (default 32), and AES.block_size to generate both the encryption key and initialization vector. This follows OpenSSL's EVP_BytesToKey method which uses MD5 hashing in an iterative process to derive cryptographic material from the passphrase and salt.", "score": null, "retrieved_content": [{"name": "decrypt_openssl", "is_method": false, "class_name": null, "parameters": ["data", "passphrase", "key_length"], "calls": ["data.startswith", "evp_bytestokey", "AES.new", "d.decrypt", "unpad_pkcs5", "len"], "code_location": {"file": "crypto.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/utils", "start_line": 37, "end_line": 43}, "code_snippet": "def decrypt_openssl(data, passphrase, key_length=32):\n    if data.startswith(b\"Salted__\"):\n        salt = data[len(b\"Salted__\") : AES.block_size]\n        key, iv = evp_bytestokey(passphrase, salt, key_length, AES.block_size)\n        d = AES.new(key, AES.MODE_CBC, iv)\n        out = d.decrypt(data[AES.block_size :])\n        return unpad_pkcs5(out)\n", "type": "function"}, {"name": "evp_bytestokey", "is_method": false, "class_name": null, "parameters": ["password", "salt", "key_len", "iv_len"], "calls": ["len", "digest", "hashlib.md5"], "code_location": {"file": "crypto.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/utils", "start_line": 21, "end_line": 34}, "code_snippet": "def evp_bytestokey(password, salt, key_len, iv_len):\n    \"\"\"\n    Python implementation of OpenSSL's EVP_BytesToKey()\n    :param password: or passphrase\n    :param salt: 8 byte salt\n    :param key_len: length of key in bytes\n    :param iv_len:  length of IV in bytes\n    :return: (key, iv)\n    \"\"\"\n    d = d_i = b\"\"\n    while len(d) < key_len + iv_len:\n        d_i = hashlib.md5(d_i + password + salt).digest()\n        d += d_i\n    return d[:key_len], d[key_len : key_len + iv_len]\n", "type": "function"}, {"name": "decrypt_data", "is_method": true, "class_name": "USTVNow", "parameters": ["cls", "data", "key", "iv"], "calls": ["encode", "encode", "encode", "AES.new", "cipher.decrypt", "base64.b64decode", "unpad", "join", "join", "reversed", "reversed", "hexdigest", "SHA256.new"], "code_location": {"file": "ustvnow.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 66, "end_line": 77}, "code_snippet": "    def decrypt_data(cls, data, key, iv):\n        rkey = \"\".join(reversed(key)).encode(\"utf8\")\n        riv = \"\".join(reversed(iv)).encode(\"utf8\")\n\n        fkey = SHA256.new(rkey).hexdigest()[:32].encode(\"utf8\")\n\n        cipher = AES.new(fkey, AES.MODE_CBC, riv)\n        decrypted = cipher.decrypt(base64.b64decode(data))\n        if decrypted:\n            return unpad(decrypted, 16, \"pkcs7\")\n        else:\n            return decrypted\n", "type": "function"}, {"name": "decrypt_data", "is_method": true, "class_name": "Mjunoon", "parameters": ["self", "cipher_data", "encrypted_data"], "calls": ["AES.new", "unpad", "bytes", "bytes", "cipher.decrypt", "binascii.unhexlify"], "code_location": {"file": "mjunoon.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 115, "end_line": 122}, "code_snippet": "    def decrypt_data(self, cipher_data, encrypted_data):\n        cipher = AES.new(\n            bytes(cipher_data[\"key\"], \"utf-8\"),\n            self.encryption_algorithm[cipher_data[\"algorithm\"]],\n            bytes(cipher_data[\"iv\"], \"utf-8\"),\n        )\n\n        return unpad(cipher.decrypt(binascii.unhexlify(encrypted_data)), 16, \"pkcs7\")\n", "type": "function"}, {"name": "test_decrypt_data", "is_method": true, "class_name": "TestPluginUSTVNow", "parameters": ["self"], "calls": ["USTVNow.decrypt_data"], "code_location": {"file": "test_ustvnow.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 30, "end_line": 42}, "code_snippet": "    def test_decrypt_data(self):\n        key = \"80035ad42d7d-bb08-7a14-f726-78403b29\"\n        iv = \"3157b5680927cc4a\"\n\n        assert (\n            USTVNow.decrypt_data(\n                b\"KcRLETVAmHlosM0OyUd5hdTQ6WhBRTe/YRAHiLJWrzf94OLkSueXTtQ9QZ1fjOLCbpX2qteEPUWVnzvvSgVDkQmRUttN\"\n                + b\"/royoxW2aL0gYQSoH1NWoDV8sIgvS5vDiQ85\",\n                key,\n                iv,\n            )\n            == b'{\"status\":false,\"error\":{\"code\":-2,\"type\":\"\",\"message\":\"Invalid credentials.\",\"details\":{}}}'\n        )\n", "type": "function"}, {"name": "create_decryptor", "is_method": true, "class_name": "HLSStreamWriter", "parameters": ["self", "key", "num"], "calls": ["AES.new", "StreamError", "StreamError", "urlparse", "Formatter", "formatter.format", "self.num_to_iv", "self.session.http.get", "getattr", "isinstance", "len", "StreamError"], "code_location": {"file": "hls.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/stream/hls", "start_line": 105, "end_line": 149}, "code_snippet": "    def create_decryptor(self, key: Key, num: int):\n        if key.method != \"AES-128\":\n            raise StreamError(f\"Unable to decrypt cipher {key.method}\")\n\n        if not self.key_uri_override and not key.uri:\n            raise StreamError(\"Missing URI for decryption key\")\n\n        if not self.key_uri_override:\n            key_uri = key.uri\n        else:\n            p = urlparse(key.uri)\n            formatter = Formatter({\n                \"url\": lambda: key.uri,\n                \"scheme\": lambda: p.scheme,\n                \"netloc\": lambda: p.netloc,\n                \"path\": lambda: p.path,\n                \"query\": lambda: p.query,\n            })\n            key_uri = formatter.format(self.key_uri_override)\n\n        if key_uri and self.key_uri != key_uri:\n            try:\n                res = self.session.http.get(\n                    key_uri,\n                    exception=StreamError,\n                    retries=self.retries,\n                    **self.reader.request_params,\n                )\n            except StreamError as err:\n                # FIXME: fix HTTPSession.request()\n                original_error = getattr(err, \"err\", None)\n                if isinstance(original_error, InvalidSchema):\n                    raise StreamError(f\"Unable to find connection adapter for key URI: {key_uri}\") from original_error\n                raise  # pragma: no cover\n\n            res.encoding = \"binary/octet-stream\"\n            self.key_data = res.content\n            self.key_uri = key_uri\n\n        iv = key.iv or self.num_to_iv(num)\n\n        # Pad IV if needed\n        iv = b\"\\x00\" * (16 - len(iv)) + iv\n\n        return AES.new(self.key_data, AES.MODE_CBC, iv)\n", "type": "function"}, {"name": "encrypt_data", "is_method": true, "class_name": "USTVNow", "parameters": ["cls", "data", "key", "iv"], "calls": ["encode", "encode", "encode", "AES.new", "cipher.encrypt", "base64.b64encode", "pad", "join", "join", "reversed", "reversed", "hexdigest", "SHA256.new"], "code_location": {"file": "ustvnow.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 55, "end_line": 63}, "code_snippet": "    def encrypt_data(cls, data, key, iv):\n        rkey = \"\".join(reversed(key)).encode(\"utf8\")\n        riv = \"\".join(reversed(iv)).encode(\"utf8\")\n\n        fkey = SHA256.new(rkey).hexdigest()[:32].encode(\"utf8\")\n\n        cipher = AES.new(fkey, AES.MODE_CBC, riv)\n        encrypted = cipher.encrypt(pad(data, 16, \"pkcs7\"))\n        return base64.b64encode(encrypted)\n", "type": "function"}, {"name": "test_encrypt_data", "is_method": true, "class_name": "TestPluginUSTVNow", "parameters": ["self"], "calls": ["USTVNow.encrypt_data"], "code_location": {"file": "test_ustvnow.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 14, "end_line": 28}, "code_snippet": "    def test_encrypt_data(self):\n        key = \"80035ad42d7d-bb08-7a14-f726-78403b29\"\n        iv = \"3157b5680927cc4a\"\n\n        assert (\n            USTVNow.encrypt_data(\n                b'{\"login_id\":\"test@test.com\",\"login_key\":\"testtest1234\",\"login_mode\":\"1\",\"manufacturer\":\"123\"}',\n                key,\n                iv,\n            )\n            == (\n                b\"uawIc5n+TnmsmR+aP2iEDKG/eMKji6EKzjI4mE+zMhlyCbHm7K4hz7IDJDWwM3aE+Ro4ydSsgJf4ZInnoW6gqvXvG0qB\"\n                + b\"/J2WJeypTSt4W124zkJpvfoJJmGAvBg2t0HT\"\n            )\n        )  # fmt: skip\n", "type": "function"}, {"name": "_get_encryption_config", "is_method": true, "class_name": "USTVNow", "parameters": ["self", "url"], "calls": ["self.session.http.get", "self._main_js_re.search", "self._encryption_config.get", "self._encryption_config.get", "m.group", "self.session.http.get", "dict", "urljoin", "self._enc_key_re.findall"], "code_location": {"file": "ustvnow.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 79, "end_line": 90}, "code_snippet": "    def _get_encryption_config(self, url):\n        # find the path to the main.js\n        # load the main.js and extract the config\n        if not self._encryption_config:\n            res = self.session.http.get(url)\n            m = self._main_js_re.search(res.text)\n            main_js_path = m and m.group(1)\n            if main_js_path:\n                res = self.session.http.get(urljoin(url, main_js_path))\n                self._encryption_config = dict(self._enc_key_re.findall(res.text))\n\n        return self._encryption_config.get(\"AES_Key\"), self._encryption_config.get(\"AES_IV\")\n", "type": "function"}, {"name": "_has_dh_ciphers", "is_method": true, "class_name": "TestHTTPAdapters", "parameters": ["ssl_context"], "calls": ["any", "ssl_context.get_ciphers"], "code_location": {"file": "test_http.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/session", "start_line": 231, "end_line": 232}, "code_snippet": "    def _has_dh_ciphers(ssl_context: SSLContext):\n        return any(cipher[\"kea\"] == \"kx-dhe\" for cipher in ssl_context.get_ciphers())\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3267521858215332}
{"question": "Where is the implementation that handles the inheritance and merging of initialization parameters between parent and child plugin classes located in the plugin architecture?", "answer": "", "relative_code_list": null, "ground_truth": "The implementation for handling constructor options inheritance and merging between parent and child plugin classes is located in the streamlink.plugin.plugin module, specifically in the Plugin class constructor and related option management methods that handle the merging of default options, constructor-provided options, and inherited options from parent classes.", "score": null, "retrieved_content": [{"name": "test_arguments_inheritance", "is_method": true, "class_name": "TestPluginArguments", "parameters": ["self"], "calls": ["pluginargument", "pluginargument", "pluginargument", "pluginargument", "tuple", "tuple"], "code_location": {"file": "test_plugin.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 316, "end_line": 329}, "code_snippet": "    def test_arguments_inheritance(self):\n        @pluginargument(\"foo\", help=\"FOO\")\n        @pluginargument(\"bar\", help=\"BAR\")\n        class PluginOne(FakePlugin):\n            pass\n\n        @pluginargument(\"baz\", help=\"BAZ\")\n        @pluginargument(\"qux\", help=\"QUX\")\n        class PluginTwo(PluginOne):\n            pass\n\n        assert PluginOne.arguments is not PluginTwo.arguments\n        assert tuple(arg.name for arg in PluginOne.arguments) == (\"foo\", \"bar\")\n        assert tuple(arg.name for arg in PluginTwo.arguments) == (\"baz\", \"qux\", \"foo\", \"bar\")\n", "type": "function"}, {"name": "test_matchers_inheritance", "is_method": true, "class_name": "TestPluginMatcher", "parameters": ["self"], "calls": ["pluginmatcher", "pluginmatcher", "pluginmatcher", "pluginmatcher", "re.compile", "re.compile", "re.compile", "re.compile", "Matcher", "Matcher", "Matcher", "Matcher", "Matcher", "Matcher", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile"], "code_location": {"file": "test_plugin.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 172, "end_line": 193}, "code_snippet": "    def test_matchers_inheritance(self):\n        @pluginmatcher(re.compile(r\"foo\"))\n        @pluginmatcher(re.compile(r\"bar\"))\n        class PluginOne(FakePlugin):\n            pass\n\n        @pluginmatcher(re.compile(r\"baz\"))\n        @pluginmatcher(re.compile(r\"qux\"))\n        class PluginTwo(PluginOne):\n            pass\n\n        assert PluginOne.matchers is not PluginTwo.matchers\n        assert PluginOne.matchers == [\n            Matcher(re.compile(r\"foo\"), NORMAL_PRIORITY),\n            Matcher(re.compile(r\"bar\"), NORMAL_PRIORITY),\n        ]\n        assert PluginTwo.matchers == [\n            Matcher(re.compile(r\"baz\"), NORMAL_PRIORITY),\n            Matcher(re.compile(r\"qux\"), NORMAL_PRIORITY),\n            Matcher(re.compile(r\"foo\"), NORMAL_PRIORITY),\n            Matcher(re.compile(r\"bar\"), NORMAL_PRIORITY),\n        ]\n", "type": "function"}, {"name": "test_constructor", "is_method": true, "class_name": "TestPlugins", "parameters": ["self", "plugin"], "calls": ["tuple", "parameters.values", "signature"], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 69, "end_line": 80}, "code_snippet": "    def test_constructor(self, plugin):\n        assert (\n            plugin.__plugin__.__init__ is Plugin.__init__\n            or tuple(\n                (param.name, param.kind)\n                for param in signature(plugin.__plugin__.__init__).parameters.values()\n            ) == (\n                (\"self\", Parameter.POSITIONAL_OR_KEYWORD),\n                (\"args\", Parameter.VAR_POSITIONAL),\n                (\"kwargs\", Parameter.VAR_KEYWORD),\n            )\n        )  # fmt: skip\n", "type": "function"}, {"name": "setup_plugin_options", "is_method": false, "class_name": null, "parameters": ["session", "args", "pluginname", "pluginclass"], "calls": ["session.get_option", "required.values", "Options", "options.update", "Options", "RuntimeError", "getattr", "parg.namespace_dest", "values.get", "warnings.warn", "pluginclass.arguments.requires", "log.error", "user_input_requester.ask_password", "user_input_requester.ask", "parg.argument_name"], "code_location": {"file": "argparser.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink_cli", "start_line": 1560, "end_line": 1619}, "code_snippet": "def setup_plugin_options(\n    session: Streamlink,\n    args: argparse.Namespace,\n    pluginname: str,\n    pluginclass: type[Plugin],\n) -> Options:\n    \"\"\"Initializes plugin options from argument values.\"\"\"\n\n    if not pluginclass.arguments:\n        return Options()\n\n    user_input_requester: UserInputRequester | None = session.get_option(\"user-input-requester\")\n    if not user_input_requester:\n        raise RuntimeError(\"The Streamlink session is missing a UserInputRequester\")\n\n    defaults = {}\n    values = {}\n    required = {}\n\n    for parg in pluginclass.arguments:\n        value = getattr(args, parg.namespace_dest(pluginname))\n        values[parg.dest] = value\n        defaults[parg.dest] = parg.default\n\n        if parg.help == argparse.SUPPRESS:\n            if value != parg.default:\n                warnings.warn(\n                    f\"The {parg.argument_name(pluginname)} plugin argument has been disabled and will be removed in the future\",\n                    StreamlinkDeprecationWarning,\n                    stacklevel=1,\n                )\n            continue\n\n        if parg.required:\n            required[parg.name] = parg\n        # if the value is set, check to see if any of the required arguments are not set\n        if parg.required or value:\n            try:\n                for rparg in pluginclass.arguments.requires(parg.name):\n                    required[rparg.name] = rparg\n            except RuntimeError:  # pragma: no cover\n                log.error(f\"{pluginname} plugin has a configuration error and the arguments cannot be parsed\")\n                break\n\n    for req in required.values():\n        if not values.get(req.dest):\n            prompt = f\"{req.prompt or f'Enter {pluginname} {req.name}'}\"\n            try:\n                if req.sensitive:\n                    value = user_input_requester.ask_password(prompt)\n                else:\n                    value = user_input_requester.ask(prompt)\n            except OSError as err:\n                raise StreamlinkCLIError from err\n            values[req.dest] = value\n\n    options = Options(defaults)\n    options.update(values)\n\n    return options\n", "type": "function"}, {"name": "_Plugin", "docstring": "", "methods": ["_get_streams"], "attributes": [], "code_location": {"file": "test_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/session", "start_line": 42, "end_line": 44}, "type": "class"}, {"name": "PluginArguments", "docstring": "", "methods": ["__init__", "generate", "visit_ClassDef"], "attributes": [], "code_location": {"file": "ext_plugins.py", "path": "/data3/pwh/swebench-repos/streamlink/docs/sphinxext", "start_line": 105, "end_line": 174}, "type": "class"}, {"name": "__init__", "is_method": true, "class_name": "PluginVisitor", "parameters": ["self"], "calls": ["__init__", "super"], "code_location": {"file": "plugins_json.py", "path": "/data3/pwh/swebench-repos/streamlink/build_backend", "start_line": 387, "end_line": 392}, "code_snippet": "    def __init__(self) -> None:\n        super().__init__()\n        self.name: str | None = None\n        self.matchers: list[PluginMatcher] = []\n        self.arguments: list[PluginArgument] = []\n        self.exports: bool = False\n", "type": "function"}, {"name": "PluginMeta", "docstring": "", "methods": ["__init__"], "attributes": [], "code_location": {"file": "plugin.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugin", "start_line": 245, "end_line": 249}, "type": "class"}, {"name": "ParsePluginArgument", "docstring": "", "methods": ["visit_Call"], "attributes": [], "code_location": {"file": "plugins_json.py", "path": "/data3/pwh/swebench-repos/streamlink/build_backend", "start_line": 286, "end_line": 383}, "type": "class"}, {"name": "Plugin", "docstring": "", "methods": [], "attributes": [], "code_location": {"file": "plugins_json.py", "path": "/data3/pwh/swebench-repos/streamlink/build_backend", "start_line": 72, "end_line": 74}, "type": "class"}], "retrieved_count": 10, "cost_time": 0.3445248603820801}
{"question": "Why does the parameter that enables reduced-latency streaming mode in a platform-specific HLS stream implementation class's initialization affect HLS segment fetching performance and buffer management?", "answer": "", "relative_code_list": null, "ground_truth": "The low_latency parameter in KickHLSStream's initialization enables low-latency HLS streaming optimizations, which typically involves reducing segment buffer sizes, implementing partial segment loading, and adjusting fetch strategies to minimize end-to-end latency. This parameter would be passed to the custom KickHLSStreamReader and KickHLSStreamWorker classes to implement performance-optimized segment retrieval with smaller buffer windows and more aggressive prefetching strategies while maintaining stream stability.", "score": null, "retrieved_content": [{"name": "__init__", "is_method": true, "class_name": "KickHLSStreamReader", "parameters": ["self", "stream"], "calls": ["__init__", "max", "stream.session.options.set", "stream.session.options.set", "log.info", "min", "super", "stream.session.options.get"], "code_location": {"file": "kick.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 108, "end_line": 115}, "code_snippet": "    def __init__(self, stream: KickHLSStream, **kwargs):\n        if stream.low_latency:\n            live_edge = max(1, min(LOW_LATENCY_MAX_LIVE_EDGE, stream.session.options.get(\"hls-live-edge\")))\n            stream.session.options.set(\"hls-live-edge\", live_edge)\n            stream.session.options.set(\"hls-segment-stream-data\", True)\n            log.info(f\"Low latency streaming (HLS live edge: {live_edge})\")\n\n        super().__init__(stream, **kwargs)\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "TwitchHLSStreamReader", "parameters": ["self", "stream"], "calls": ["log.info", "__init__", "max", "stream.session.options.set", "stream.session.options.set", "log.info", "min", "super", "stream.session.options.get"], "code_location": {"file": "twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 243, "end_line": 251}, "code_snippet": "    def __init__(self, stream: TwitchHLSStream, **kwargs):\n        log.info(\"Will skip ad segments\")\n        if stream.low_latency:\n            live_edge = max(1, min(LOW_LATENCY_MAX_LIVE_EDGE, stream.session.options.get(\"hls-live-edge\")))\n            stream.session.options.set(\"hls-live-edge\", live_edge)\n            stream.session.options.set(\"hls-segment-stream-data\", True)\n            log.info(f\"Low latency streaming (HLS live edge: {live_edge})\")\n\n        super().__init__(stream, **kwargs)\n", "type": "function"}, {"name": "__init__", "is_method": true, "class_name": "KickHLSStreamWorker", "parameters": ["self"], "calls": ["__init__", "super"], "code_location": {"file": "kick.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 82, "end_line": 85}, "code_snippet": "    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if self.stream.low_latency:\n            self.reload_time = \"segment\"\n", "type": "function"}, {"name": "test_hls_low_latency_no_prefetch", "is_method": true, "class_name": "TestKickHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.await_write", "self.await_read", "Playlist", "Playlist", "call", "Segment", "Segment", "Segment", "Segment", "Segment", "Segment", "Segment", "Segment"], "code_location": {"file": "test_kick.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 114, "end_line": 129}, "code_snippet": "    def test_hls_low_latency_no_prefetch(self, mock_log):\n        self.subject(\n            [\n                Playlist(0, [Segment(0), Segment(1), Segment(2), Segment(3)]),\n                Playlist(4, [Segment(4), Segment(5), Segment(6), Segment(7)], end=True),\n            ],\n            streamoptions={\"low_latency\": True},\n        )\n\n        assert self.stream.low_latency\n\n        self.await_write(6)\n        self.await_read(read_all=True)\n        assert mock_log.info.mock_calls == [\n            call(\"Low latency streaming (HLS live edge: 2)\"),\n        ]\n", "type": "function"}, {"name": "test_hls_low_latency_no_prefetch", "is_method": true, "class_name": "TestTwitchHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.await_write", "self.await_read", "Playlist", "Playlist", "call", "call", "call", "Segment", "Segment", "Segment", "Segment", "Segment", "Segment", "Segment", "Segment"], "code_location": {"file": "test_twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 418, "end_line": 435}, "code_snippet": "    def test_hls_low_latency_no_prefetch(self, mock_log):\n        self.subject(\n            [\n                Playlist(0, [Segment(0), Segment(1), Segment(2), Segment(3)]),\n                Playlist(4, [Segment(4), Segment(5), Segment(6), Segment(7)], end=True),\n            ],\n            streamoptions={\"low_latency\": True},\n        )\n\n        assert self.stream.low_latency\n\n        self.await_write(6)\n        self.await_read(read_all=True)\n        assert mock_log.info.mock_calls == [\n            call(\"Will skip ad segments\"),\n            call(\"Low latency streaming (HLS live edge: 2)\"),\n            call(\"This is not a low latency stream\"),\n        ]\n", "type": "function"}, {"name": "test_hls_low_latency_has_prefetch", "is_method": true, "class_name": "TestTwitchHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.session.options.get", "self.await_write", "self.await_read", "all", "self.session.options.get", "self.content", "any", "Playlist", "Playlist", "self.called", "call", "call", "self.called", "segments.values", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "segments.values"], "code_location": {"file": "test_twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 372, "end_line": 392}, "code_snippet": "    def test_hls_low_latency_has_prefetch(self, mock_log):\n        segments = self.subject(\n            [\n                Playlist(0, [Segment(0), Segment(1), Segment(2), Segment(3), SegmentPrefetch(4), SegmentPrefetch(5)]),\n                Playlist(4, [Segment(4), Segment(5), Segment(6), Segment(7), SegmentPrefetch(8), SegmentPrefetch(9)], end=True),\n            ],\n            streamoptions={\"low_latency\": True},\n        )\n\n        assert self.session.options.get(\"hls-live-edge\") == 2\n        assert self.session.options.get(\"hls-segment-stream-data\")\n\n        self.await_write(6)\n        data = self.await_read(read_all=True)\n        assert data == self.content(segments, cond=lambda s: s.num >= 4), \"Skips first four segments due to reduced live-edge\"\n        assert not any(self.called(s) for s in segments.values() if s.num < 4), \"Doesn't download old segments\"\n        assert all(self.called(s) for s in segments.values() if s.num >= 4), \"Downloads all remaining segments\"\n        assert mock_log.info.mock_calls == [\n            call(\"Will skip ad segments\"),\n            call(\"Low latency streaming (HLS live edge: 2)\"),\n        ]\n", "type": "function"}, {"name": "test_hls_low_latency_has_prefetch", "is_method": true, "class_name": "TestKickHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.session.options.get", "self.await_write", "self.await_read", "all", "self.session.options.get", "self.content", "any", "Playlist", "Playlist", "self.called", "call", "self.called", "segments.values", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "segments.values"], "code_location": {"file": "test_kick.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 71, "end_line": 90}, "code_snippet": "    def test_hls_low_latency_has_prefetch(self, mock_log):\n        segments = self.subject(\n            [\n                Playlist(0, [Segment(0), Segment(1), Segment(2), Segment(3), SegmentPrefetch(4), SegmentPrefetch(5)]),\n                Playlist(4, [Segment(4), Segment(5), Segment(6), Segment(7), SegmentPrefetch(8), SegmentPrefetch(9)], end=True),\n            ],\n            streamoptions={\"low_latency\": True},\n        )\n\n        assert self.session.options.get(\"hls-live-edge\") == 2\n        assert self.session.options.get(\"hls-segment-stream-data\")\n\n        self.await_write(6)\n        data = self.await_read(read_all=True)\n        assert data == self.content(segments, cond=lambda s: s.num >= 4), \"Skips first four segments due to reduced live-edge\"\n        assert not any(self.called(s) for s in segments.values() if s.num < 4), \"Doesn't download old segments\"\n        assert all(self.called(s) for s in segments.values() if s.num >= 4), \"Downloads all remaining segments\"\n        assert mock_log.info.mock_calls == [\n            call(\"Low latency streaming (HLS live edge: 2)\"),\n        ]\n", "type": "function"}, {"name": "test_hls_no_low_latency_has_prefetch", "is_method": true, "class_name": "TestKickHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.await_write", "self.await_read", "all", "self.session.options.get", "self.session.options.get", "self.content", "any", "Playlist", "Playlist", "self.called", "segments.values", "self.called", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "segments.values"], "code_location": {"file": "test_kick.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 93, "end_line": 111}, "code_snippet": "    def test_hls_no_low_latency_has_prefetch(self, mock_log):\n        segments = self.subject(\n            [\n                Playlist(0, [Segment(0), Segment(1), Segment(2), Segment(3), SegmentPrefetch(4), SegmentPrefetch(5)]),\n                Playlist(4, [Segment(4), Segment(5), Segment(6), Segment(7), SegmentPrefetch(8), SegmentPrefetch(9)], end=True),\n            ],\n            streamoptions={\"low_latency\": False},\n        )\n\n        assert self.session.options.get(\"hls-live-edge\") == 4\n        assert not self.session.options.get(\"hls-segment-stream-data\")\n\n        self.await_write(8)\n        data = self.await_read(read_all=True)\n        assert data == self.content(segments, cond=lambda s: s.num < 8), \"Ignores prefetch segments\"\n        assert all(self.called(s) for s in segments.values() if s.num <= 7), \"Ignores prefetch segments\"\n        assert not any(self.called(s) for s in segments.values() if s.num > 7), \"Ignores prefetch segments\"\n        assert mock_log.info.mock_calls == []\n        assert self.thread.reader.worker._reload_time == 3.0\n", "type": "function"}, {"name": "ChzzkHLSStream", "docstring": "Custom HLS stream that adds __bgda__ query parameter to segment URLs", "methods": ["__init__"], "attributes": ["__reader__"], "code_location": {"file": "chzzk.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 51, "end_line": 58}, "type": "class"}, {"name": "test_hls_no_low_latency_has_prefetch", "is_method": true, "class_name": "TestTwitchHLSStream", "parameters": ["self", "mock_log"], "calls": ["patch", "self.subject", "self.await_write", "self.await_read", "all", "self.session.options.get", "self.session.options.get", "self.content", "any", "Playlist", "Playlist", "self.called", "call", "segments.values", "self.called", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "Segment", "Segment", "Segment", "Segment", "SegmentPrefetch", "SegmentPrefetch", "segments.values"], "code_location": {"file": "test_twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 395, "end_line": 415}, "code_snippet": "    def test_hls_no_low_latency_has_prefetch(self, mock_log):\n        segments = self.subject(\n            [\n                Playlist(0, [Segment(0), Segment(1), Segment(2), Segment(3), SegmentPrefetch(4), SegmentPrefetch(5)]),\n                Playlist(4, [Segment(4), Segment(5), Segment(6), Segment(7), SegmentPrefetch(8), SegmentPrefetch(9)], end=True),\n            ],\n            streamoptions={\"low_latency\": False},\n        )\n\n        assert self.session.options.get(\"hls-live-edge\") == 4\n        assert not self.session.options.get(\"hls-segment-stream-data\")\n\n        self.await_write(8)\n        data = self.await_read(read_all=True)\n        assert data == self.content(segments, cond=lambda s: s.num < 8), \"Ignores prefetch segments\"\n        assert all(self.called(s) for s in segments.values() if s.num <= 7), \"Ignores prefetch segments\"\n        assert not any(self.called(s) for s in segments.values() if s.num > 7), \"Ignores prefetch segments\"\n        assert mock_log.info.mock_calls == [\n            call(\"Will skip ad segments\"),\n        ]\n        assert self.thread.reader.worker._reload_time == 3.0\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.33820652961730957}
{"question": "Why does the method that transforms authentication tokens use a dual token validation approach with both current and previous hour timestamps?", "answer": "", "relative_code_list": null, "ground_truth": "The dual token validation approach using both current (date) and previous hour (date - 1) timestamps in the transform_token method is designed to handle potential time synchronization issues between the client and server. This design accounts for scenarios where there might be slight clock skew or network latency that could cause the token generation to be based on a different hour boundary than expected. By attempting both the current and previous hour's timestamp, the implementation increases the likelihood of successful token validation even when there are minor timing discrepancies, ensuring more robust stream access without requiring perfect time synchronization between the client and Albavision's token generation system.", "score": null, "retrieved_content": [{"name": "transform_token", "is_method": true, "class_name": "Albavision", "parameters": ["token_in", "date"], "calls": ["list", "len", "range", "join", "token_out.endswith", "log.error"], "code_location": {"file": "albavision.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 191, "end_line": 202}, "code_snippet": "    def transform_token(token_in, date):\n        token_out = list(token_in)\n        offset = len(token_in)\n        for i in range(offset - 1, -1, -1):\n            p = (i * date) % offset\n            # swap chars at p and i\n            token_out[i], token_out[p] = token_out[p], token_out[i]\n        token_out = \"\".join(token_out)\n        if token_out.endswith(\"OK\"):\n            return token_out[:-2]\n        else:\n            log.error(f\"Invalid site token: {token_in} => {token_out}\")\n", "type": "function"}, {"name": "test_transform", "is_method": true, "class_name": "TestPluginAlbavision", "parameters": ["self"], "calls": ["Albavision.transform_token"], "code_location": {"file": "test_albavision.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 55, "end_line": 57}, "code_snippet": "    def test_transform(self):\n        token = Albavision.transform_token(\"6b425761cc8a86569b1a05a9bf1870c95fca717dOK\", 436171)\n        assert token == \"6b425761cc8a86569b1a05a9bf1870c95fca717d\"\n", "type": "function"}, {"name": "_get_token_req_url", "is_method": true, "class_name": "Albavision", "parameters": ["self"], "calls": ["validate.Schema", "schema.validate", "log.debug", "validate.Schema", "schema.validate", "log.debug", "int", "validate.xml_xpath_string", "validate.none_or_all", "validate.xml_xpath_string", "validate.none_or_all", "self.transform_token", "self.transform_token", "update_qsd", "re.compile", "validate.none_or_all", "re.compile", "validate.none_or_all", "time.time", "validate.get", "validate.url", "validate.get"], "code_location": {"file": "albavision.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 124, "end_line": 154}, "code_snippet": "    def _get_token_req_url(self):\n        schema = validate.Schema(\n            validate.xml_xpath_string(\".//script[contains(text(), 'LIVE_URL')]/text()\"),\n            validate.none_or_all(\n                re.compile(r\"\"\"jQuery\\.get\\s*\\((?P<q>['\"])(?P<token>.+?)(?P=q)\"\"\"),\n                validate.none_or_all(\n                    validate.get(\"token\"),\n                    validate.url(),\n                ),\n            ),\n        )\n        token_req_host = schema.validate(self.page)\n        log.debug(f\"token_req_host={token_req_host}\")\n\n        schema = validate.Schema(\n            validate.xml_xpath_string(\".//script[contains(text(), 'LIVE_URL')]/text()\"),\n            validate.none_or_all(\n                re.compile(r\"\"\"Math\\.floor\\(Date\\.now\\(\\)\\s*/\\s*3600000\\),\\s*(?P<q>['\"])(?P<token>.+?)(?P=q)\"\"\"),\n                validate.none_or_all(validate.get(\"token\")),\n            ),\n        )\n        token_req_str = schema.validate(self.page)\n        log.debug(f\"token_req_str={token_req_str}\")\n        if not token_req_str:\n            return\n\n        date = int(time.time() // 3600)\n        token_req_token = self.transform_token(token_req_str, date) or self.transform_token(token_req_str, date - 1)\n\n        if token_req_host and token_req_token:\n            return update_qsd(token_req_host, {\"rsk\": token_req_token})\n", "type": "function"}, {"name": "_generate_applicationkeysecret", "is_method": true, "class_name": "AbemaTV", "parameters": ["self", "deviceid"], "calls": ["deviceid.encode", "time.gmtime", "encode", "hmac.new", "h.update", "h.digest", "range", "hmac.new", "h.update", "h.digest", "range", "hmac.new", "h.update", "h.digest", "range", "decode", "hmac.new", "h.update", "h.digest", "hmac.new", "h.update", "h.digest", "hmac.new", "h.update", "h.digest", "str", "rstrip", "rstrip", "rstrip", "int", "time.time", "urlsafe_b64encode", "urlsafe_b64encode", "urlsafe_b64encode"], "code_location": {"file": "abematv.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 168, "end_line": 200}, "code_snippet": "    def _generate_applicationkeysecret(self, deviceid):\n        deviceid = deviceid.encode(\"utf-8\")  # for python3\n        # plus 1 hour and drop minute and secs\n        # for python3 : floor division\n        ts_1hour = (int(time.time()) + 60 * 60) // 3600 * 3600\n        time_struct = time.gmtime(ts_1hour)\n        ts_1hour_str = str(ts_1hour).encode(\"utf-8\")\n\n        h = hmac.new(self.SECRETKEY, digestmod=hashlib.sha256)\n        h.update(self.SECRETKEY)\n        tmp = h.digest()\n        for _ in range(time_struct.tm_mon):\n            h = hmac.new(self.SECRETKEY, digestmod=hashlib.sha256)\n            h.update(tmp)\n            tmp = h.digest()\n        h = hmac.new(self.SECRETKEY, digestmod=hashlib.sha256)\n        h.update(urlsafe_b64encode(tmp).rstrip(b\"=\") + deviceid)\n        tmp = h.digest()\n        for _ in range(time_struct.tm_mday % 5):\n            h = hmac.new(self.SECRETKEY, digestmod=hashlib.sha256)\n            h.update(tmp)\n            tmp = h.digest()\n\n        h = hmac.new(self.SECRETKEY, digestmod=hashlib.sha256)\n        h.update(urlsafe_b64encode(tmp).rstrip(b\"=\") + ts_1hour_str)\n        tmp = h.digest()\n\n        for _ in range(time_struct.tm_hour % 5):  # utc hour\n            h = hmac.new(self.SECRETKEY, digestmod=hashlib.sha256)\n            h.update(tmp)\n            tmp = h.digest()\n\n        return urlsafe_b64encode(tmp).rstrip(b\"=\").decode(\"utf-8\")\n", "type": "function"}, {"name": "parse_token", "is_method": true, "class_name": "TwitchAPI", "parameters": ["tokenstr"], "calls": ["parse_json", "validate.Schema", "validate.get", "validate.all", "validate.filter", "re.match"], "code_location": {"file": "twitch.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 345, "end_line": 359}, "code_snippet": "    def parse_token(tokenstr):\n        return parse_json(\n            tokenstr,\n            schema=validate.Schema(\n                {\n                    \"chansub\": {\n                        \"restricted_bitrates\": validate.all(\n                            [str],\n                            validate.filter(lambda n: not re.match(r\"(.+_)?archives|live|chunked\", n)),\n                        ),\n                    },\n                },\n                validate.get((\"chansub\", \"restricted_bitrates\")),\n            ),\n        )\n", "type": "function"}, {"name": "_get_token", "is_method": true, "class_name": "Albavision", "parameters": ["self"], "calls": ["self._get_token_req_url", "self.session.http.get", "log.debug", "self._is_token_based_site", "log.error", "validate.Schema", "log.error", "log.error", "validate.parse_json", "validate.optional", "validate.optional"], "code_location": {"file": "albavision.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 156, "end_line": 188}, "code_snippet": "    def _get_token(self):\n        if not self._is_token_based_site():\n            return\n\n        token_req_url = self._get_token_req_url()\n        if not token_req_url:\n            return\n\n        res = self.session.http.get(\n            token_req_url,\n            schema=validate.Schema(\n                validate.parse_json(),\n                {\n                    \"success\": bool,\n                    validate.optional(\"error\"): int,\n                    validate.optional(\"token\"): str,\n                },\n            ),\n        )\n\n        if not res[\"success\"]:\n            if res[\"error\"]:\n                log.error(f\"Token request failed with error: {res['error']}\")\n            else:\n                log.error(\"Token request failed\")\n            return\n\n        if not res[\"token\"]:\n            log.error(\"Token not found in response\")\n            return\n        token = res[\"token\"]\n        log.debug(f\"token={token}\")\n        return token\n", "type": "function"}, {"name": "get_token", "is_method": true, "class_name": "Streann", "parameters": ["self"], "calls": ["log.debug", "dict", "self.session.http.post", "self.session.http.json", "log.trace", "data.get", "self.token_url.format", "log.error", "base64.b64encode", "base64.b64encode", "validate.Schema", "self._domain.encode", "self.time.encode", "validate.optional", "validate.optional", "validate.optional", "validate.optional", "validate.optional", "validate.optional"], "code_location": {"file": "streann.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 97, "end_line": 135}, "code_snippet": "    def get_token(self, **config):\n        log.debug(\"get_token\")\n        pdata = dict(\n            arg1=base64.b64encode(self._domain.encode(\"utf8\")),\n            arg2=base64.b64encode(self.time.encode(\"utf8\")),\n        )\n\n        headers = {\n            \"Referer\": self.url,\n            \"X-Requested-With\": \"XMLHttpRequest\",\n            \"Content-Type\": \"application/x-www-form-urlencoded\",\n        }\n\n        res = self.session.http.post(\n            self.token_url.format(deviceId=self.device_id, **config),\n            data=pdata,\n            headers=headers,\n        )\n\n        if res.status_code == 204:\n            log.error(f\"self._domain might be invalid - {self._domain}\")\n            return\n\n        data = self.session.http.json(\n            res,\n            schema=validate.Schema({\n                \"token\": str,\n                validate.optional(\"name\"): str,\n                validate.optional(\"webPlayer\"): {\n                    validate.optional(\"id\"): str,\n                    validate.optional(\"name\"): str,\n                    validate.optional(\"type\"): str,\n                    validate.optional(\"allowedDomains\"): [str],\n                },\n            }),\n        )\n        log.trace(f\"{data!r}\")\n        self.title = data.get(\"name\")\n        return data[\"token\"]\n", "type": "function"}, {"name": "_get_stream_token", "is_method": true, "class_name": "Vidio", "parameters": ["self", "stream_id", "stream_type"], "calls": ["self.session.http.post", "self.tokens_url.format", "validate.Schema", "str", "str", "validate.parse_json", "validate.union_get", "uuid4", "uuid4", "validate.any", "validate.any", "validate.url", "validate.url"], "code_location": {"file": "vidio.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 28, "end_line": 50}, "code_snippet": "    def _get_stream_token(self, stream_id, stream_type):\n        return self.session.http.post(\n            self.tokens_url.format(id=stream_id),\n            params={\"type\": stream_type},\n            headers={\"Referer\": self.url},\n            cookies={\n                \"ahoy_visit\": str(uuid4()),\n                \"ahoy_visitor\": str(uuid4()),\n            },\n            schema=validate.Schema(\n                validate.parse_json(),\n                {\n                    \"token\": str,\n                    \"hls_url\": validate.any(\"\", validate.url()),\n                    \"dash_url\": validate.any(\"\", validate.url()),\n                },\n                validate.union_get(\n                    \"token\",\n                    \"hls_url\",\n                    \"dash_url\",\n                ),\n            ),\n        )\n", "type": "function"}, {"name": "_timefree", "is_method": true, "class_name": "Radiko", "parameters": ["self", "station_id", "start_at"], "calls": ["self._authorize", "hexdigest", "self._get_xml", "hashlib.md5", "urlencode", "encode", "str", "random.random"], "code_location": {"file": "radiko.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 56, "end_line": 72}, "code_snippet": "    def _timefree(self, station_id, start_at):\n        m3u8_url = \"https://tf-rpaa.smartstream.ne.jp/tf/playlist.m3u8\"\n        token, _area_id = self._authorize()\n        lsid = hashlib.md5(str(random.random()).encode(\"utf-8\")).hexdigest()\n        end_at = self._get_xml(start_at, station_id)\n        m3u8_params = {\n            \"station_id\": station_id,\n            \"start_at\": start_at,\n            \"ft\": start_at,\n            \"end_at\": end_at,\n            \"to\": end_at,\n            \"l\": 15,\n            \"lsid\": lsid,\n            \"type\": \"b\",\n        }\n        url = f\"{m3u8_url}?{urlencode(m3u8_params)}\"\n        return url, token\n", "type": "function"}, {"name": "_get_tokenizer", "is_method": true, "class_name": "BlazeTV", "parameters": ["self", "streamtype", "uvid"], "calls": ["self.session.http.get", "validate.Schema", "validate.parse_json", "validate.get", "validate.url"], "code_location": {"file": "blazetv.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 55, "end_line": 70}, "code_snippet": "    def _get_tokenizer(self, streamtype, uvid):\n        return self.session.http.get(\n            f\"https://watch.blaze.tv/stream/{streamtype}/widevine/{uvid}\",\n            schema=validate.Schema(\n                validate.parse_json(),\n                {\n                    \"tokenizer\": {\n                        \"url\": validate.url(),\n                        \"uvid\": str,\n                        \"expiry\": int,\n                        \"token\": str,\n                    },\n                },\n                validate.get(\"tokenizer\"),\n            ),\n        )\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.34214067459106445}
{"question": "How can a systematic approach ensure that the validation framework's extensibility maintains backward compatibility when adding new validation components?", "answer": "", "relative_code_list": null, "ground_truth": "To ensure extensibility with backward compatibility when adding new validator types, the validation framework should employ a strategy based on polymorphism and interface segregation. New validators should implement a common validation interface (e.g., a base Validator class with a validate method) without modifying existing validator logic. The framework should use a registry pattern where validators are registered by type/name, allowing new validators to be added without altering core validation logic. Runtime discovery mechanisms (like plugin systems or dependency injection) can dynamically load validators. Additionally, versioned schema definitions and semantic versioning for the validation library help maintain compatibility. Testing should include validation chains mixing old and new validators to verify no regressions occur.", "score": null, "retrieved_content": [{"name": "test_extensions", "is_method": true, "class_name": "TestXmlXpathValidator", "parameters": ["self", "element"], "calls": ["Element", "validate.validate", "validate.xml_xpath", "int", "context.context_node.attrib.get"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 1429, "end_line": 1434}, "code_snippet": "    def test_extensions(self, element):\n        def foo(context, a, b):\n            return int(context.context_node.attrib.get(\"val\")) + a + b\n\n        element = Element(\"root\", attrib={\"val\": \"3\"})\n        assert validate.validate(validate.xml_xpath(\"foo(5, 7)\", extensions={(None, \"foo\"): foo}), element) == 15.0\n", "type": "function"}, {"name": "test_validate_failure_custom", "is_method": true, "class_name": "TestSchema", "parameters": ["self", "schema"], "calls": ["assert_validationerror", "pytest.raises", "schema.validate"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 46, "end_line": 58}, "code_snippet": "    def test_validate_failure_custom(self, schema: validate.Schema):\n        class CustomError(PluginError):\n            pass\n\n        with pytest.raises(CustomError) as cm:\n            schema.validate(\"bar\", name=\"data\", exception=CustomError)\n        assert_validationerror(\n            cm.value,\n            \"\"\"\n                Unable to validate data: ValidationError(equality):\n                  'bar' does not equal 'foo'\n            \"\"\",\n        )\n", "type": "function"}, {"name": "test_stringsubclass", "is_method": true, "class_name": "TestPattern", "parameters": ["self"], "calls": ["validate.validate", "validate.all", "Element", "validate.xml_xpath_string", "re.compile", "validate.get"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 327, "end_line": 338}, "code_snippet": "    def test_stringsubclass(self):\n        assert (\n            validate.validate(\n                validate.all(\n                    validate.xml_xpath_string(\".//@bar\"),\n                    re.compile(r\".+\"),\n                    validate.get(0),\n                ),\n                Element(\"foo\", {\"bar\": \"baz\"}),\n            )\n            == \"baz\"\n        )\n", "type": "function"}, {"name": "test_subclass", "is_method": true, "class_name": "TestValidationError", "parameters": ["self"], "calls": ["issubclass"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 1591, "end_line": 1592}, "code_snippet": "    def test_subclass(self):\n        assert issubclass(ValidationError, ValueError)\n", "type": "function"}, {"name": "test_split", "is_method": true, "class_name": "TestRegexSchema", "parameters": ["self"], "calls": ["validate.validate", "validate.regex", "re.compile"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 564, "end_line": 565}, "code_snippet": "    def test_split(self):\n        assert validate.validate(validate.regex(re.compile(r\"\\s+\"), \"split\"), \"foo bar baz\") == [\"foo\", \"bar\", \"baz\"]\n", "type": "function"}, {"name": "test_success", "is_method": true, "class_name": "TestEndsWithValidator", "parameters": ["self"], "calls": ["validate.validate", "validate.endswith"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 1065, "end_line": 1066}, "code_snippet": "    def test_success(self):\n        assert validate.validate(validate.endswith(\"baz\"), \"foo bar baz\")\n", "type": "function"}, {"name": "test_nested_failure", "is_method": true, "class_name": "TestSchema", "parameters": ["self", "schema_nested"], "calls": ["assert_validationerror", "pytest.raises", "schema_nested.validate"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 63, "end_line": 72}, "code_snippet": "    def test_nested_failure(self, schema_nested: validate.Schema):\n        with pytest.raises(PluginError) as cm:\n            schema_nested.validate(\"bar\")\n        assert_validationerror(\n            cm.value,\n            \"\"\"\n                Unable to validate result: ValidationError(equality):\n                  'bar' does not equal 'foo'\n            \"\"\",\n        )\n", "type": "function"}, {"name": "test_schema", "is_method": true, "class_name": "TestValidationError", "parameters": ["self"], "calls": ["ValidationError", "assert_validationerror", "ValidationError", "ValidationError"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 1690, "end_line": 1711}, "code_snippet": "    def test_schema(self):\n        err = ValidationError(\n            ValidationError(\n                \"foo\",\n                schema=dict,\n            ),\n            ValidationError(\n                \"bar\",\n                schema=\"something\",\n            ),\n            schema=validate.any,\n        )\n        assert_validationerror(\n            err,\n            \"\"\"\n                ValidationError(AnySchema):\n                  ValidationError(dict):\n                    foo\n                  ValidationError(something):\n                    bar\n            \"\"\",\n        )\n", "type": "function"}, {"name": "test_validate_failure", "is_method": true, "class_name": "TestSchema", "parameters": ["self", "schema"], "calls": ["assert_validationerror", "pytest.raises", "schema.validate"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 35, "end_line": 44}, "code_snippet": "    def test_validate_failure(self, schema: validate.Schema):\n        with pytest.raises(PluginError) as cm:\n            schema.validate(\"bar\")\n        assert_validationerror(\n            cm.value,\n            \"\"\"\n                Unable to validate result: ValidationError(equality):\n                  'bar' does not equal 'foo'\n            \"\"\",\n        )\n", "type": "function"}, {"name": "test_sequence", "is_method": true, "class_name": "TestMapValidator", "parameters": ["self"], "calls": ["validate.map", "validate.validate"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 1276, "end_line": 1279}, "code_snippet": "    def test_sequence(self):\n        schema = validate.map(lambda k: k + 1)\n        value = (0, 1, 2, 3)\n        assert validate.validate(schema, value) == (1, 2, 3, 4)\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3428621292114258}
{"question": "Where in the control flow of the incoming message reading loop method does the transition from session-agnostic to session-specific event handling occur within the browser protocol message processing flow?", "answer": "", "relative_code_list": null, "ground_truth": "The transition from session-agnostic to session-specific event handling in the _task_reader method is triggered by the presence or absence of the 'sessionId' field in the parsed JSON data. When the received CDP message data does not contain a 'sessionId' field, the message is handled by the CDPConnection instance itself via self._handle_data(data). However, when the data contains a 'sessionId' field, the control flow transitions to session-specific handling by looking up the corresponding CDPSession instance from self.sessions using the session_id and then calling self.sessions[session_id]._handle_data(data). This data-driven control flow ensures that messages without session context are processed at the connection level while session-specific messages are routed to the appropriate session instance.", "score": null, "retrieved_content": [{"name": "ReceivedMessageFromTarget", "docstring": "Notifies about a new protocol message received from the session (as reported in\n``attachedToTarget`` event).", "methods": ["from_json"], "attributes": [], "code_location": {"file": "target.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 701, "end_line": 718}, "type": "class"}, {"name": "_handle_event", "is_method": true, "class_name": "CDPBase", "parameters": ["self", "data"], "calls": ["log.log", "set", "log.warning", "parse_json_event", "dict", "type", "type", "log.warning", "sender.send_nowait", "log.log", "broken_channels.add", "sender.close", "dict"], "code_location": {"file": "connection.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp", "start_line": 261, "end_line": 282}, "code_snippet": "    def _handle_event(self, data: T_JSON_DICT) -> None:\n        if \"method\" not in data or \"params\" not in data:\n            log.warning(\"Invalid CDP event message received without method or params\")\n            return\n\n        try:\n            event = parse_json_event(data)\n        except KeyError:\n            log.warning(f\"Unknown CDP event message received: {data['method']}\")\n            return\n\n        log.log(ALL, \"Received event: %(event)r\", dict(event=event))\n        broken_channels = set()\n        for sender in self.event_channels[type(event)]:\n            try:\n                sender.send_nowait(event)\n            except trio.WouldBlock:\n                log.log(ERROR, \"Unable to propagate CDP event %(event)r due to full channel\", dict(event=event))\n            except trio.BrokenResourceError:\n                broken_channels.add(sender)\n                sender.close()\n        self.event_channels[type(event)] -= broken_channels\n", "type": "function"}, {"name": "CDPBase", "docstring": "Low-level base class for Chrome Devtools Protocol connection & session management.\n\nIt provides methods for sending CDP commands and receiving their responses, as well as for listening to CDP events.\n\nBoth CDP commands and events can be sent and received in a global context and in a session context.\n\nThe Chrome Devtools Protocol is documented at https://chromedevtools.github.io/devtools-protocol/", "methods": ["__init__", "listen", "_handle_data", "_handle_cmd_response", "_handle_event"], "attributes": [], "code_location": {"file": "connection.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp", "start_line": 143, "end_line": 282}, "type": "class"}, {"name": "on_message_stream", "is_method": true, "class_name": "NicoLiveWsClient", "parameters": ["self", "data"], "calls": ["data.get", "self.ready.set", "self.opened.wait", "data.get", "self._SCHEMA_COOKIES.validate", "log.debug", "log.info", "self.close", "data.get", "data.get", "self.session.http.cookies.set"], "code_location": {"file": "nicolive.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 81, "end_line": 96}, "code_snippet": "    def on_message_stream(self, data):\n        if data.get(\"protocol\") != \"hls\" or not data.get(\"uri\"):\n            return\n\n        # cookies may be required by some HLS multivariant playlists\n        if cookies := data.get(\"cookies\", []):\n            for cookie in self._SCHEMA_COOKIES.validate(cookies):\n                self.session.http.cookies.set(**cookie)\n\n        self.hls_stream_url = data.get(\"uri\")\n        self.ready.set()\n        if self.opened.wait(self.STREAM_OPENED_TIMEOUT):\n            log.debug(\"Stream opened, keeping websocket connection alive\")\n        else:\n            log.info(\"Closing websocket connection\")\n            self.close()\n", "type": "function"}, {"name": "on_message", "is_method": true, "class_name": "NicoLiveWsClient", "parameters": ["self", "wsapp", "data"], "calls": ["log.debug", "parse_json", "message.get", "message.get", "self._MESSAGE_HANDLERS.get", "handler"], "code_location": {"file": "nicolive.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 64, "end_line": 71}, "code_snippet": "    def on_message(self, wsapp, data: str):\n        log.debug(f\"Received: {data}\")\n        message = parse_json(data)\n        msgtype = message.get(\"type\")\n        msgdata = message.get(\"data\", {})\n\n        if handler := self._MESSAGE_HANDLERS.get(msgtype):\n            handler(self, msgdata)\n", "type": "function"}, {"name": "_handle_data", "is_method": true, "class_name": "CDPBase", "parameters": ["self", "data"], "calls": ["self._handle_cmd_response", "self._handle_event"], "code_location": {"file": "connection.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp", "start_line": 227, "end_line": 231}, "code_snippet": "    def _handle_data(self, data: T_JSON_DICT) -> None:\n        if \"id\" in data:\n            self._handle_cmd_response(data)\n        else:\n            self._handle_event(data)\n", "type": "function"}, {"name": "test_handlers", "is_method": true, "class_name": "TestWebsocketClient", "parameters": ["self", "session"], "calls": ["WebsocketClient"], "code_location": {"file": "test_api_websocket.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 146, "end_line": 155}, "code_snippet": "    def test_handlers(self, session: Streamlink):\n        client = WebsocketClient(session, \"wss://localhost:0\")\n        assert client.ws.on_open == client.on_open\n        assert client.ws.on_error == client.on_error\n        assert client.ws.on_close == client.on_close\n        assert client.ws.on_ping == client.on_ping\n        assert client.ws.on_pong == client.on_pong\n        assert client.ws.on_message == client.on_message\n        assert client.ws.on_cont_message == client.on_cont_message\n        assert client.ws.on_data == client.on_data\n", "type": "function"}, {"name": "on_message", "is_method": true, "class_name": "UStreamTVWsClient", "parameters": ["self", "wsapp", "data"], "calls": ["log.trace", "log.trace", "self._MESSAGE_HANDLERS.get", "parse_json", "log.error", "handlers.items", "arg.get", "log.debug", "handler"], "code_location": {"file": "ustreamtv.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/plugins", "start_line": 217, "end_line": 236}, "code_snippet": "    def on_message(self, wsapp, data: str):\n        try:\n            parsed = parse_json(data, schema=self._schema_cmd)\n        except PluginError:\n            log.error(f\"Could not parse message: {data[:50]}\")\n            return\n\n        cmd: str = parsed[\"cmd\"]\n        args: list[dict] = parsed[\"args\"]\n        log.trace(f\"Received '{cmd}' command\")  # type: ignore[attr-defined]\n        log.trace(f\"{args!r}\")  # type: ignore[attr-defined]\n\n        handlers = self._MESSAGE_HANDLERS.get(cmd)\n        if handlers is not None:\n            for arg in args:\n                for name, handler in handlers.items():\n                    argdata = arg.get(name)\n                    if argdata is not None:\n                        log.debug(f\"Processing '{cmd}' - '{name}'\")\n                        handler(self, argdata)\n", "type": "function"}, {"name": "EventSourceMessageReceived", "docstring": "Fired when EventSource message is received.", "methods": ["from_json"], "attributes": [], "code_location": {"file": "network.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 3454, "end_line": 3477}, "type": "class"}, {"name": "attach_to_browser_target", "is_method": false, "class_name": null, "parameters": [], "calls": ["SessionID.from_json"], "code_location": {"file": "target.py", "path": "/data3/pwh/swebench-repos/streamlink/src/streamlink/webbrowser/cdp/devtools", "start_line": 241, "end_line": 253}, "code_snippet": "def attach_to_browser_target() -> Generator[T_JSON_DICT, T_JSON_DICT, SessionID]:\n    \"\"\"\n    Attaches to the browser target, only uses flat sessionId mode.\n\n    **EXPERIMENTAL**\n\n    :returns: Id assigned to the session.\n    \"\"\"\n    cmd_dict: T_JSON_DICT = {\n        \"method\": \"Target.attachToBrowserTarget\",\n    }\n    json = yield cmd_dict\n    return SessionID.from_json(json[\"sessionId\"])\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.3843514919281006}
{"question": "How should the component testing framework be designed to automatically validate input pattern matching across multiple sources while maintaining test isolation and extensibility?", "answer": "", "relative_code_list": null, "ground_truth": "The framework should implement a base test class with abstract pattern matching logic that concrete test classes inherit, using a standardized data structure for test cases that separates URL patterns from expected outcomes, while employing dependency injection for plugin instances to ensure test isolation and mock capabilities.", "score": null, "retrieved_content": [{"name": "test_success", "is_method": true, "class_name": "TestPattern", "parameters": ["self", "pattern", "data", "expected"], "calls": ["pytest.mark.parametrize", "validate.validate", "re.compile", "type", "result.groupdict"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 322, "end_line": 325}, "code_snippet": "    def test_success(self, pattern, data, expected):\n        result = validate.validate(re.compile(pattern), data)\n        assert type(result) is re.Match\n        assert result.groupdict() == expected\n", "type": "function"}, {"name": "test_success", "is_method": true, "class_name": "TestRegexSchema", "parameters": ["self", "pattern", "data", "expected"], "calls": ["pytest.mark.parametrize", "validate.validate", "validate.regex", "type", "result.groupdict", "re.compile"], "code_location": {"file": "test_validate.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 556, "end_line": 559}, "code_snippet": "    def test_success(self, pattern, data, expected):\n        result = validate.validate(validate.regex(re.compile(pattern)), data)\n        assert type(result) is re.Match\n        assert result.groupdict() == expected\n", "type": "function"}, {"name": "test_all_named_matchers_have_tests", "is_method": true, "class_name": "PluginCanHandleUrl", "parameters": ["self", "matcher"], "calls": ["any", "self.urls_named"], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 142, "end_line": 146}, "code_snippet": "    def test_all_named_matchers_have_tests(self, matcher: Matcher):\n        assert any(  # pragma: no branch\n            name == matcher.name\n            for name, url in self.urls_named()\n        ), \"Named matcher does have a test\"  # fmt: skip\n", "type": "function"}, {"name": "test_all_matchers_match", "is_method": true, "class_name": "PluginCanHandleUrl", "parameters": ["self", "matcher"], "calls": ["any", "matcher.pattern.match", "self.urls_all", "type"], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 136, "end_line": 140}, "code_snippet": "    def test_all_matchers_match(self, matcher: Matcher):\n        assert any(  # pragma: no branch\n            matcher.pattern.match(url)\n            for url in [(item if type(item) is str else item[1]) for item in self.urls_all()]\n        ), \"Matcher matches at least one URL\"  # fmt: skip\n", "type": "function"}, {"name": "test_url_pattern_to_regex_pattern", "is_method": true, "class_name": "TestRequestPausedHandler", "parameters": ["self", "url_pattern", "regex_pattern"], "calls": ["pytest.mark.parametrize", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "RequestPausedHandler._url_pattern_to_regex_pattern"], "code_location": {"file": "test_client.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/webbrowser/cdp", "start_line": 307, "end_line": 308}, "code_snippet": "    def test_url_pattern_to_regex_pattern(self, url_pattern: str, regex_pattern: str):\n        assert RequestPausedHandler._url_pattern_to_regex_pattern(url_pattern).pattern == regex_pattern\n", "type": "function"}, {"name": "_parametrize_plugincanhandleurl_test_all_named_matchers_have_tests", "is_method": false, "class_name": null, "parameters": ["metafunc"], "calls": ["metafunc.parametrize", "metafunc.cls.matchers"], "code_location": {"file": "conftest.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 45, "end_line": 51}, "code_snippet": "def _parametrize_plugincanhandleurl_test_all_named_matchers_have_tests(metafunc: pytest.Metafunc):\n    matchers: list[Matcher] = [m for m in metafunc.cls.matchers() if m.name is not None]\n    metafunc.parametrize(\n        \"matcher\",\n        matchers,\n        ids=[m.name for m in matchers],\n    )\n", "type": "function"}, {"name": "test_url_matches_groups_named", "is_method": true, "class_name": "PluginCanHandleUrl", "parameters": ["self", "name", "url", "groups"], "calls": ["next", "self._get_match_groups", "matcher.pattern.match", "self.matchers", "type"], "code_location": {"file": "__init__.py", "path": "/data3/pwh/swebench-repos/streamlink/tests/plugins", "start_line": 167, "end_line": 171}, "code_snippet": "    def test_url_matches_groups_named(self, name: TName, url: TUrl, groups: TMatchGroup):\n        matches = [(matcher.name, matcher.pattern.match(url)) for matcher in self.matchers() if matcher.name is not None]\n        mname, match = next(((mname, match) for mname, match in matches if match), (None, None))  # pragma: no branch\n        result = None if not match else self._get_match_groups(match, type(groups))\n        assert (mname, result) == (name, groups), \"URL capture groups match the results of the matching named matcher\"\n", "type": "function"}, {"name": "test_url_setter", "is_method": true, "class_name": "TestPluginMatcher", "parameters": ["self"], "calls": ["pluginmatcher", "pluginmatcher", "pluginmatcher", "MyPlugin", "re.compile", "re.compile", "re.compile", "Mock", "plugin.match.group", "plugin.match.group", "plugin.match.group"], "code_location": {"file": "test_plugin.py", "path": "/data3/pwh/swebench-repos/streamlink/tests", "start_line": 219, "end_line": 248}, "code_snippet": "    def test_url_setter(self):\n        @pluginmatcher(re.compile(r\"http://(foo)\"))\n        @pluginmatcher(re.compile(r\"http://(bar)\"))\n        @pluginmatcher(re.compile(r\"http://(baz)\"))\n        class MyPlugin(FakePlugin):\n            pass\n\n        plugin = MyPlugin(Mock(), \"http://foo\")\n        assert plugin.url == \"http://foo\"\n        assert [m is not None for m in plugin.matches] == [True, False, False]\n        assert plugin.matcher is plugin.matchers[0].pattern\n        assert plugin.match.group(1) == \"foo\"\n\n        plugin.url = \"http://bar\"\n        assert plugin.url == \"http://bar\"\n        assert [m is not None for m in plugin.matches] == [False, True, False]\n        assert plugin.matcher is plugin.matchers[1].pattern\n        assert plugin.match.group(1) == \"bar\"\n\n        plugin.url = \"http://baz\"\n        assert plugin.url == \"http://baz\"\n        assert [m is not None for m in plugin.matches] == [False, False, True]\n        assert plugin.matcher is plugin.matchers[2].pattern\n        assert plugin.match.group(1) == \"baz\"\n\n        plugin.url = \"http://qux\"\n        assert plugin.url == \"http://qux\"\n        assert [m is not None for m in plugin.matches] == [False, False, False]\n        assert plugin.matcher is None\n        assert plugin.match is None\n", "type": "function"}, {"name": "test_plugin", "is_method": false, "class_name": null, "parameters": ["test_plugin_code"], "calls": ["ast.parse", "PluginVisitor", "pluginvisitor.visit", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginArgument", "PluginArgument"], "code_location": {"file": "test_plugins_json.py", "path": "/data3/pwh/swebench-repos/streamlink/build_backend", "start_line": 1086, "end_line": 1100}, "code_snippet": "def test_plugin(test_plugin_code: str):\n    tree = ast.parse(test_plugin_code)\n    pluginvisitor = PluginVisitor()\n    pluginvisitor.visit(tree)\n\n    assert pluginvisitor.exports\n    assert pluginvisitor.matchers == [\n        PluginMatcher(pattern=\"https://a\", flags=None, priority=None, name=None),\n        PluginMatcher(pattern=\"https://b\", flags=re.IGNORECASE, priority=PluginMatcher.LOW_PRIORITY, name=\"b\"),\n        PluginMatcher(pattern=\"https://c\", flags=re.I | re.X, priority=20, name=\"c\"),\n    ]\n    assert pluginvisitor.arguments == [\n        PluginArgument(name=\"a\", type=\"comma_list_filter\", type_kwargs={\"acceptable\": [\"a\", \"b\"]}, default=[\"a\"], help=\"a\"),\n        PluginArgument(name=\"b\", requires=\"a\", sensitive=True, help=\"==SUPPRESS==\"),\n    ]\n", "type": "function"}, {"name": "test_pluginmatcher", "is_method": false, "class_name": null, "parameters": ["code", "expected", "raises"], "calls": ["pytest.mark.parametrize", "ast.parse", "PluginVisitor", "pluginvisitor.visit", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "pytest.param", "dedent", "dedent", "dedent", "dedent", "dedent", "pytest.raises", "dedent", "pytest.raises", "dedent", "pytest.raises", "dedent", "dedent", "pytest.raises", "dedent", "pytest.raises", "dedent", "pytest.raises", "dedent", "pytest.raises", "dedent", "pytest.raises", "dedent", "pytest.raises", "dedent", "dedent", "pytest.raises", "dedent", "pytest.raises", "dedent", "pytest.raises", "dedent", "dedent", "pytest.raises", "dedent", "pytest.raises", "dedent", "pytest.raises", "dedent", "pytest.raises", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher", "PluginMatcher"], "code_location": {"file": "test_plugins_json.py", "path": "/data3/pwh/swebench-repos/streamlink/build_backend", "start_line": 422, "end_line": 427}, "code_snippet": "def test_pluginmatcher(code: str, expected: list, raises: nullcontext):\n    tree = ast.parse(code)\n    pluginvisitor = PluginVisitor()\n    with raises:\n        pluginvisitor.visit(tree)\n        assert pluginvisitor.matchers == expected\n", "type": "function"}], "retrieved_count": 10, "cost_time": 0.37812352180480957}
